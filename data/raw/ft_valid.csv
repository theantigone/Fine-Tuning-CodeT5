cleaned_method,target_block,tokens_in_method
"def __init__(self, scale, factor, mode):
    self.index = 0
    self.scale = scale
    if factor is None:
        self._log_factor = None
    else:
        if factor < 1.0:
            raise ValueError(""'factor' must be >= 1.0"")
        self._log_factor = np.log(factor)
    if mode not in self.allowed_modes:
        raise ValueError(
            (""'{0}' is not a recognized mode. "" ""Please select from: {1}"").format(
                mode, self.allowed_modes
            )
        )
    self.mode = mode
",if factor < 1.0 :,160
"def get_grab_keys(self):
    keystr = None
    try:
        keys = self.display.get_grab_keys()
        for k in keys:
            if keystr is None:
                keystr = gtk.gdk.keyval_name(k)
            else:
                keystr = keystr + ""+"" + gtk.gdk.keyval_name(k)
    except:
        pass
    return keystr
",if keystr is None :,115
"def _checkAllExamples(self, num_type):
    for region_code in phonenumberutil.SUPPORTED_REGIONS:
        numobj_py = phonenumberutil.example_number_for_type(region_code, num_type)
        if numobj_py is not None:
            numobj_pb = PyToPB(numobj_py)
            alt_py = PBToPy(numobj_pb)
            self.assertEqual(numobj_py, alt_py)
",if numobj_py is not None :,127
"def _gaf10iterator(handle):
    for inline in handle:
        if inline[0] == ""!"":
            continue
        inrec = inline.rstrip(""\n"").split(""\t"")
        if len(inrec) == 1:
            continue
        inrec[3] = inrec[3].split(""|"")  # Qualifier
        inrec[5] = inrec[5].split(""|"")  # DB:reference(s)
        inrec[7] = inrec[7].split(""|"")  # With || From
        inrec[10] = inrec[10].split(""|"")  # Synonym
        inrec[12] = inrec[12].split(""|"")  # Taxon
        yield dict(zip(GAF10FIELDS, inrec))
",if len ( inrec ) == 1 :,188
"def __xor__(self, other):
    inc, exc = _norm_args_notimplemented(other)
    if inc is NotImplemented:
        return NotImplemented
    if inc is NotImplemented:
        return NotImplemented
    if self._included is None:
        if exc is None:  # - +
            return _ComplementSet(excluded=self._excluded - inc)
        else:  # - -
            return _ComplementSet(included=self._excluded.symmetric_difference(exc))
    else:
        if inc is None:  # + -
            return _ComplementSet(excluded=exc - self._included)
        else:  # + +
            return _ComplementSet(included=self._included.symmetric_difference(inc))
",if exc is None :,183
"def connection(self, commit_on_success=False):
    with self._lock:
        if self._bulk_commit:
            if self._pending_connection is None:
                self._pending_connection = sqlite.connect(self.filename)
            con = self._pending_connection
        else:
            con = sqlite.connect(self.filename)
        try:
            if self.fast_save:
                con.execute(""PRAGMA synchronous = 0;"")
            yield con
            if commit_on_success and self.can_commit:
                con.commit()
        finally:
            if not self._bulk_commit:
                con.close()
",if not self . _bulk_commit :,182
"def renderable_events(self, date, hour):
    ""Returns the number of renderable events""
    renderable_events = []
    for event in self.events:
        if event.covers(date, hour):
            renderable_events.append(event)
    if hour:
        for current in renderable_events:
            for event in self.events:
                if event not in renderable_events:
                    for hour in range(self.start_hour, self.end_hour):
                        if current.covers(date, hour) and event.covers(date, hour):
                            renderable_events.append(event)
                            break
    return renderable_events
",if event not in renderable_events :,191
"def _prepare_cooldowns(self, ctx):
    if self._buckets.valid:
        dt = ctx.message.edited_at or ctx.message.created_at
        current = dt.replace(tzinfo=datetime.timezone.utc).timestamp()
        bucket = self._buckets.get_bucket(ctx.message, current)
        retry_after = bucket.update_rate_limit(current)
        if retry_after:
            raise CommandOnCooldown(bucket, retry_after)
",if retry_after :,122
"def TryMerge(self, d):
    while d.avail() > 0:
        tt = d.getVarInt32()
        if tt == 10:
            self.set_module(d.getPrefixedString())
            continue
        if tt == 18:
            self.set_version(d.getPrefixedString())
            continue
        if tt == 24:
            self.set_instances(d.getVarInt64())
            continue
        if tt == 0:
            raise ProtocolBuffer.ProtocolBufferDecodeError
        d.skipData(tt)
",if tt == 18 :,150
"def n_import_from(self, node):
    relative_path_index = 0
    if self.version >= 2.5:
        if node[relative_path_index].pattr > 0:
            node[2].pattr = (""."" * node[relative_path_index].pattr) + node[2].pattr
        if self.version > 2.7:
            if isinstance(node[1].pattr, tuple):
                imports = node[1].pattr
                for pattr in imports:
                    node[1].pattr = pattr
                    self.default(node)
                return
            pass
    self.default(node)
",if self . version > 2.7 :,170
"def logic():
    while 1:
        yield a
        var = 0
        for i in downrange(len(a)):
            if a[i] == 1:
                var += 1
        out.next = var
",if a [ i ] == 1 :,61
"def _extract_networks(self, server_node):
    """"""Marshal the networks attribute of a parsed request""""""
    node = self.find_first_child_named(server_node, ""networks"")
    if node is not None:
        networks = []
        for network_node in self.find_children_named(node, ""network""):
            item = {}
            if network_node.hasAttribute(""uuid""):
                item[""uuid""] = network_node.getAttribute(""uuid"")
            if network_node.hasAttribute(""fixed_ip""):
                item[""fixed_ip""] = network_node.getAttribute(""fixed_ip"")
            networks.append(item)
        return networks
    else:
        return None
","if network_node . hasAttribute ( ""uuid"" ) :",186
"def _model_shorthand(self, args):
    accum = []
    for arg in args:
        if isinstance(arg, Node):
            accum.append(arg)
        elif isinstance(arg, Query):
            accum.append(arg)
        elif isinstance(arg, ModelAlias):
            accum.extend(arg.get_proxy_fields())
        elif isclass(arg) and issubclass(arg, Model):
            accum.extend(arg._meta.declared_fields)
    return accum
","elif isinstance ( arg , Query ) :",125
"def on_show_comment(self, widget, another):
    if widget.get_active():
        if another.get_active():
            self.treeview.update_items(all=True, comment=True)
        else:
            self.treeview.update_items(comment=True)
    else:
        if another.get_active():
            self.treeview.update_items(all=True)
        else:
            self.treeview.update_items()
",if another . get_active ( ) :,121
"def test_select_figure_formats_set():
    ip = get_ipython()
    for fmts in [
        {""png"", ""svg""},
        [""png""],
        (""jpeg"", ""pdf"", ""retina""),
        {""svg""},
    ]:
        active_mimes = {_fmt_mime_map[fmt] for fmt in fmts}
        pt.select_figure_formats(ip, fmts)
        for mime, f in ip.display_formatter.formatters.items():
            if mime in active_mimes:
                nt.assert_in(Figure, f)
            else:
                nt.assert_not_in(Figure, f)
",if mime in active_mimes :,170
"def update_from_data(self, data):
    super(HelpParameter, self).update_from_data(data)
    # original help.py value_sources are strings, update command strings to value-source dict
    if self.value_sources:
        self.value_sources = [
            str_or_dict
            if isinstance(str_or_dict, dict)
            else {""link"": {""command"": str_or_dict}}
            for str_or_dict in self.value_sources
        ]
","if isinstance ( str_or_dict , dict )",130
"def _reset_library_root_logger() -> None:
    global _default_handler
    with _lock:
        if not _default_handler:
            return
        library_root_logger = _get_library_root_logger()
        library_root_logger.removeHandler(_default_handler)
        library_root_logger.setLevel(logging.NOTSET)
        _default_handler = None
",if not _default_handler :,99
"def extract_headers(headers):
    """"""This function extracts valid headers from interactive input.""""""
    sorted_headers = {}
    matches = re.findall(r""(.*):\s(.*)"", headers)
    for match in matches:
        header = match[0]
        value = match[1]
        try:
            if value[-1] == "","":
                value = value[:-1]
            sorted_headers[header] = value
        except IndexError:
            pass
    return sorted_headers
","if value [ - 1 ] == "","" :",125
"def _call_user_data_handler(self, operation, src, dst):
    if hasattr(self, ""_user_data""):
        for key, (data, handler) in self._user_data.items():
            if handler is not None:
                handler.handle(operation, key, data, src, dst)
",if handler is not None :,80
"def update(self, other=None, **kwargs):
    if other is not None:
        if hasattr(other, ""items""):
            other = other.items()
        for key, value in other:
            if key in kwargs:
                raise TensorforceError.value(
                    name=""NestedDict.update"",
                    argument=""key"",
                    value=key,
                    condition=""specified twice"",
                )
            self[key] = value
    for key, value in kwargs.items():
        self[key] = value
","if hasattr ( other , ""items"" ) :",153
"def _restore_context(context):
    # Check for changes in contextvars, and set them to the current
    # context for downstream consumers
    for cvar in context:
        try:
            if cvar.get() != context.get(cvar):
                cvar.set(context.get(cvar))
        except LookupError:
            cvar.set(context.get(cvar))
",if cvar . get ( ) != context . get ( cvar ) :,103
"def __str__(self):
    s = ""{""
    sep = """"
    for k, v in self.iteritems():
        s += sep
        if type(k) == str:
            s += ""'%s'"" % k
        else:
            s += str(k)
        s += "": ""
        if type(v) == str:
            s += ""'%s'"" % v
        else:
            s += str(v)
        sep = "", ""
    s += ""}""
    return s
",if type ( k ) == str :,131
"def read_file_or_url(self, fname):
    # TODO: not working on localhost
    if isinstance(fname, file):
        result = open(fname, ""r"")
    else:
        match = self.urlre.match(fname)
        if match:
            result = urllib.urlopen(match.group(1))
        else:
            fname = os.path.expanduser(fname)
            try:
                result = open(os.path.expanduser(fname), ""r"")
            except IOError:
                result = open(
                    ""%s.%s"" % (os.path.expanduser(fname), self.defaultExtension), ""r""
                )
    return result
",if match :,184
"def subclass_managers(self, recursive):
    for cls in self.class_.__subclasses__():
        mgr = manager_of_class(cls)
        if mgr is not None and mgr is not self:
            yield mgr
            if recursive:
                for m in mgr.subclass_managers(True):
                    yield m
",if mgr is not None and mgr is not self :,89
"def star_path(path):
    """"""Replace integers and integer-strings in a path with *""""""
    path = list(path)
    for i, p in enumerate(path):
        if isinstance(p, int):
            path[i] = ""*""
        else:
            if not isinstance(p, text_type):
                p = p.decode()
            if r_is_int.match(p):
                path[i] = ""*""
    return join_path(path)
",if r_is_int . match ( p ) :,127
"def cookie_decode(data, key):
    """"""Verify and decode an encoded string. Return an object or None""""""
    if isinstance(data, unicode):
        data = data.encode(""ascii"")  # 2to3 hack
    if cookie_is_encoded(data):
        sig, msg = data.split(u""?"".encode(""ascii""), 1)  # 2to3 hack
        if sig[1:] == base64.b64encode(hmac.new(key, msg).digest()):
            return pickle.loads(base64.b64decode(msg))
    return None
","if sig [ 1 : ] == base64 . b64encode ( hmac . new ( key , msg ) . digest ( ) ) :",137
"def parse_row(cls, doc_row):
    row = {}
    for field_name, field in FIELD_MAP.items():
        if len(doc_row) > field[1]:
            field_value = doc_row[field[1]]
        else:
            field_value = """"
        if len(field) >= 3 and callable(field[2]):
            field_value = field[2](field_value)
        row[field_name] = field_value
    return row
",if len ( field ) >= 3 and callable ( field [ 2 ] ) :,127
"def semantic_masks(self):
    for sid in self._seg_ids:
        sinfo = self._sinfo.get(sid)
        if sinfo is None or sinfo[""isthing""]:
            # Some pixels (e.g. id 0 in PanopticFPN) have no instance or semantic predictions.
            continue
        yield (self._seg == sid).numpy().astype(np.bool), sinfo
","if sinfo is None or sinfo [ ""isthing"" ] :",104
"def top_level_subjects(self):
    if self.subjects.exists():
        return optimize_subject_query(self.subjects.filter(parent__isnull=True))
    else:
        # TODO: Delet this when all PreprintProviders have a mapping
        if len(self.subjects_acceptable) == 0:
            return optimize_subject_query(
                Subject.objects.filter(parent__isnull=True, provider___id=""osf"")
            )
        tops = set([sub[0][0] for sub in self.subjects_acceptable])
        return [Subject.load(sub) for sub in tops]
",if len ( self . subjects_acceptable ) == 0 :,157
"def resolve(obj):
    if isinstance(obj, list):
        for item in obj:
            resolve(item)
        return
    if isinstance(obj, dict):
        if ""$ref"" in obj:
            with resolver.resolving(obj[u""$ref""]) as resolved:
                resolve(resolved)
                obj.clear()
                obj.update(resolved)
        else:
            for value in obj.values():
                resolve(value)
","if ""$ref"" in obj :",127
"def read_ansible_config(project_path, variables_of_interest):
    fnames = [""/etc/ansible/ansible.cfg""]
    if project_path:
        fnames.append(os.path.join(project_path, ""ansible.cfg""))
    values = {}
    try:
        parser = ConfigParser()
        parser.read(fnames)
        if ""defaults"" in parser:
            for var in variables_of_interest:
                if var in parser[""defaults""]:
                    values[var] = parser[""defaults""][var]
    except Exception:
        logger.exception(""Failed to read ansible configuration(s) {}"".format(fnames))
    return values
","if var in parser [ ""defaults"" ] :",166
"def test_globalphase():
    rule_set = DecompositionRuleSet(modules=[globalphase, r2rzandph])
    dummy = DummyEngine(save_commands=True)
    eng = MainEngine(
        dummy,
        [AutoReplacer(rule_set), InstructionFilter(low_level_gates_noglobalphase)],
    )
    qubit = eng.allocate_qubit()
    R(1.2) | qubit
    rz_count = 0
    for cmd in dummy.received_commands:
        assert not isinstance(cmd.gate, R)
        if isinstance(cmd.gate, Rz):
            rz_count += 1
            assert cmd.gate == Rz(1.2)
    assert rz_count == 1
","if isinstance ( cmd . gate , Rz ) :",188
"def _kill_current_player(self):
    if self._current_player:
        if self.voice_client.is_paused():
            self.voice_client.resume()
        try:
            self.voice_client.stop()
        except OSError:
            pass
        self._current_player = None
        return True
    return False
",if self . voice_client . is_paused ( ) :,93
"def hasAmbiguousLanguage(self, p):
    """"""Return True if p.b contains different @language directives.""""""
    # c = self
    languages, tag = set(), ""@language""
    for s in g.splitLines(p.b):
        if g.match_word(s, 0, tag):
            i = g.skip_ws(s, len(tag))
            j = g.skip_id(s, i)
            word = s[i:j]
            languages.add(word)
    return len(list(languages)) > 1
","if g . match_word ( s , 0 , tag ) :",140
"def terminate(self):
    n_retries = 10
    for i in range(n_retries):
        try:
            super(MemmappingPool, self).terminate()
            break
        except OSError as e:
            if isinstance(e, WindowsError):
                # Workaround  occasional ""[Error 5] Access is denied"" issue
                # when trying to terminate a process under windows.
                sleep(0.1)
                if i + 1 == n_retries:
                    warnings.warn(
                        ""Failed to terminate worker processes in""
                        "" multiprocessing pool: %r"" % e
                    )
    self._temp_folder_manager._unlink_temporary_resources()
",if i + 1 == n_retries :,192
"def test_downsampling(self, method, maybe_range, fraction, expected_n_reads):
    reader = sam.SamReader(
        test_utils.genomics_core_testdata(""test.bam""),
        downsample_fraction=fraction,
        random_seed=12345,
    )
    with reader:
        if method == ""iterate"":
            reads_iter = reader.iterate()
        elif method == ""query"":
            reads_iter = reader.query(ranges.parse_literal(maybe_range))
        else:
            self.fail(""Unexpected method "" + str(method))
        self.assertEqual(test_utils.iterable_len(reads_iter), expected_n_reads)
","if method == ""iterate"" :",177
"def verify_acceptable(self):
    start = time.time()
    while True:
        if self.select_acceptable():
            return
        elif (time.time() - start) > READ_TIMEOUT:
            raise Exception(""Server socket did not accept in time"")
        time.sleep(0.1)
",if self . select_acceptable ( ) :,79
"def replica_local_creator(next_creator, **kwargs) -> tf.Variable:
    """"""Variable creator that by default creates replica local variables.""""""
    if kwargs[""synchronization""] == tf.VariableSynchronization.AUTO:
        kwargs[""synchronization""] = tf.VariableSynchronization.ON_READ
        if kwargs[""aggregation""] == tf.VariableAggregation.NONE:
            kwargs[""aggregation""] = tf.VariableAggregation.ONLY_FIRST_REPLICA
        if kwargs[""trainable""] is None:
            kwargs[""trainable""] = True
    return next_creator(**kwargs)
","if kwargs [ ""aggregation"" ] == tf . VariableAggregation . NONE :",144
"def get_optional_nargs(self, name):
    for n, kwargs in self.conf[""optional_args""]:
        if name == n:
            if ""action"" in kwargs:
                action = kwargs[""action""]
                if action in (""store_true"", ""store_false""):
                    return 0
            break
    return 1
","if ""action"" in kwargs :",92
"def ageToDays(self, age_str):
    age = 0
    age_str = age_str.replace(""&nbsp;"", "" "")
    regex = ""(\d*.?\d+).(sec|hour|day|week|month|year)+""
    matches = re.findall(regex, age_str)
    for match in matches:
        nr, size = match
        mult = 1
        if size == ""week"":
            mult = 7
        elif size == ""month"":
            mult = 30.5
        elif size == ""year"":
            mult = 365
        age += tryInt(nr) * mult
    return tryInt(age)
","elif size == ""year"" :",163
"def put(self, userId, bucket, key, data):
    if not self.initialized:
        raise Exception(""archive not initialized"")
    try:
        uri = self.uri_for(userId, bucket, key)
        if not self._save_content(uri, data):
            raise Exception(""Failed writing file content to disk: {}"".format(uri))
        else:
            return uri
    except Exception as err:
        logger.debug(""cannot put data: exception - "" + str(err))
        raise err
","if not self . _save_content ( uri , data ) :",131
"def get_range(min, max):
    if max < min:
        min, max = max, min
    elif min == max:
        if min < 0:
            min, max = 2 * min, 0
        elif min > 0:
            min, max = 0, 2 * min
        else:
            min, max = -1, 1
    return min, max
",elif min > 0 :,99
"def update_job_weights():
    """"""Update job weights.""""""
    for job in data_types.Job.query():
        multiplier = DEFAULT_MULTIPLIER
        if environment.is_engine_fuzzer_job(job.name):
            targets_count = ndb.Key(data_types.FuzzTargetsCount, job.name).get()
            # If the count is 0, it may be due to a bad build or some other issue. Use
            # the default weight in that case to allow for recovery.
            if targets_count and targets_count.count:
                multiplier = targets_count.count
                if multiplier > TARGET_COUNT_WEIGHT_CAP:
                    multiplier = TARGET_COUNT_WEIGHT_CAP
        update_job_weight(job.name, multiplier)
",if multiplier > TARGET_COUNT_WEIGHT_CAP :,199
"def _validate_required_settings(
    self, application_id, application_config, required_settings, should_throw=True
):
    """"""All required keys must be present""""""
    for setting_key in required_settings:
        if setting_key not in application_config.keys():
            if should_throw:
                raise ImproperlyConfigured(
                    MISSING_SETTING.format(
                        application_id=application_id, setting=setting_key
                    )
                )
            else:
                return False
    return True
",if should_throw :,146
"def nested_update(org_dict, upd_dict):
    for key, value in upd_dict.items():
        if isinstance(value, dict):
            if key in org_dict:
                if not isinstance(org_dict[key], dict):
                    raise ValueError(
                        ""Mismatch between org_dict and upd_dict at node {}"".format(key)
                    )
                nested_update(org_dict[key], value)
            else:
                org_dict[key] = value
        else:
            org_dict[key] = value
",if key in org_dict :,161
"def eintr_retry_call(func, *args, **kwargs):
    while True:
        try:
            return func(*args, **kwargs)
        except EnvironmentError as e:
            if getattr(e, ""errno"", None) == errno.EINTR:
                continue
            raise
","if getattr ( e , ""errno"" , None ) == errno . EINTR :",79
"def __init__(self, entity):
    self._entity = weakref.proxy(entity)
    self._observables = collections.OrderedDict()
    self._keys_helper = _ObservableKeys(self._entity, self._observables)
    # Ensure consistent ordering.
    for attr_name in sorted(dir(type(self))):
        type_attr = getattr(type(self), attr_name)
        if isinstance(type_attr, define.observable):
            self._observables[attr_name] = getattr(self, attr_name)
","if isinstance ( type_attr , define . observable ) :",131
"def check_redundancy(self):
    # Ensure there are no adjacent blocks (they should have been merged)
    starts, sizes = self.allocator.get_allocated_regions()
    last = -1
    for start, size in zip(starts, sizes):
        if start < last:
            raise Exception(""Block at %d is out of order"" % start)
        if start == last:
            raise Exception(""Block at %d is redundant"" % start)
        last = start + size
",if start == last :,122
"def elfheader():
    local_path = pwndbg.file.get_file(pwndbg.proc.exe)
    with open(local_path, ""rb"") as f:
        elffile = ELFFile(f)
        sections = []
        for section in elffile.iter_sections():
            start = section[""sh_addr""]
            # Don't print sections that aren't mapped into memory
            if start == 0:
                continue
            size = section[""sh_size""]
            sections.append((start, start + size, section.name))
        sections.sort()
        for start, end, name in sections:
            print(""%#x - %#x "" % (start, end), name)
",if start == 0 :,189
"def orbit():
    """"""Define the internal thread for running the orbit.""""""
    for point in points:
        self.set_position(point)
        self.set_focus(focus)
        self.set_viewup(viewup)
        self.renderer.ResetCameraClippingRange()
        self.render()
        time.sleep(step)
        if write_frames:
            self.write_frame()
",if write_frames :,107
"def json_format(self):
    """"""Returns the integer value formatted as a JSON literal""""""
    fmt = self._jsonfmt
    if fmt == NUMBER_FORMAT_HEX:
        return format(self, ""#x"")
    elif fmt == NUMBER_FORMAT_OCTAL:
        return format(self, ""#o"")
    elif fmt == NUMBER_FORMAT_BINARY:
        return format(self, ""#b"")
    elif fmt == NUMBER_FORMAT_LEGACYOCTAL:
        if self == 0:
            return ""0""  # For some reason Python's int doesn't do '00'
        elif self < 0:
            return ""-0%o"" % (-self)
        else:
            return ""0%o"" % self
    else:
        return str(self)
",elif self < 0 :,189
"def parseTime(timeStr):
    regex = re.compile(constants.PARSE_TIME_REGEX)
    parts = regex.match(timeStr)
    if not parts:
        return
    parts = parts.groupdict()
    time_params = {}
    for (name, param) in parts.items():
        if param:
            if name == ""miliseconds"":
                time_params[""microseconds""] = int(param) * 1000
            else:
                time_params[name] = int(param)
    return datetime.timedelta(**time_params).total_seconds()
",if param :,146
"def build_extension(self, ext):
    ext._convert_pyx_sources_to_lang()
    _compiler = self.compiler
    try:
        if isinstance(ext, Library):
            self.compiler = self.shlib_compiler
        _build_ext.build_extension(self, ext)
        if ext._needs_stub:
            cmd = self.get_finalized_command(""build_py"").build_lib
            self.write_stub(cmd, ext)
    finally:
        self.compiler = _compiler
",if ext . _needs_stub :,134
"def __init__(self, type, data, name=None):
    Constant.__init__(self, type, data, name)
    self.tag.unique_value = None
    if isinstance(data, np.ndarray) and data.ndim > 0:
        flat_data = data.ravel()
        if flat_data.shape[0]:
            if (flat_data == flat_data[0]).all():
                self.tag.unique_value = flat_data[0]
",if flat_data . shape [ 0 ] :,118
"def _find_machine(deb_arch):
    for machine in _ARCH_TRANSLATIONS:
        if _ARCH_TRANSLATIONS[machine].get(""deb"", """") == deb_arch:
            return machine
        elif _ARCH_TRANSLATIONS[machine].get(""uts_machine"", """") == deb_arch:
            return machine
    raise errors.SnapcraftEnvironmentError(
        ""Cannot set machine from deb_arch {!r}"".format(deb_arch)
    )
","elif _ARCH_TRANSLATIONS [ machine ] . get ( ""uts_machine"" , """" ) == deb_arch :",124
"def fields_for_form(form, only_fields, exclude_fields):
    fields = OrderedDict()
    for name, field in form.fields.items():
        is_not_in_only = only_fields and name not in only_fields
        is_excluded = (
            name
            in exclude_fields  # or
            # name in already_created_fields
        )
        if is_not_in_only or is_excluded:
            continue
        fields[name] = convert_form_field(field)
    return fields
",if is_not_in_only or is_excluded :,139
"def wait_services_ready(selectors, min_counts, count_fun, timeout=None):
    readies = [0] * len(selectors)
    start_time = time.time()
    while True:
        all_satisfy = True
        for idx, selector in enumerate(selectors):
            if readies[idx] < min_counts[idx]:
                all_satisfy = False
                readies[idx] = count_fun(selector)
                break
        if all_satisfy:
            break
        if timeout and timeout + start_time < time.time():
            raise TimeoutError(""Wait cluster start timeout"")
        time.sleep(1)
",if all_satisfy :,167
"def count_brokers(self):
    self.nb_brokers = 0
    for broker in self.brokers:
        if not broker.spare:
            self.nb_brokers += 1
    for realm in self.higher_realms:
        for broker in realm.brokers:
            if not broker.spare and broker.manage_sub_realms:
                self.nb_brokers += 1
",if not broker . spare :,118
"def _adapt_polymorphic_element(self, element):
    if ""parententity"" in element._annotations:
        search = element._annotations[""parententity""]
        alias = self._polymorphic_adapters.get(search, None)
        if alias:
            return alias.adapt_clause(element)
    if isinstance(element, expression.FromClause):
        search = element
    elif hasattr(element, ""table""):
        search = element.table
    else:
        return None
    alias = self._polymorphic_adapters.get(search, None)
    if alias:
        return alias.adapt_clause(element)
",if alias :,157
"def get_all_methods():
    estimators = all_estimators()
    for name, Estimator in estimators:
        if name.startswith(""_""):
            # skip private classes
            continue
        methods = []
        for name in dir(Estimator):
            if name.startswith(""_""):
                continue
            method_obj = getattr(Estimator, name)
            if hasattr(method_obj, ""__call__"") or isinstance(method_obj, property):
                methods.append(name)
        methods.append(None)
        for method in sorted(methods, key=lambda x: str(x)):
            yield Estimator, method
","if hasattr ( method_obj , ""__call__"" ) or isinstance ( method_obj , property ) :",161
"def __call__(self, es, params):
    ops = 0
    indices = mandatory(params, ""indices"", self)
    only_if_exists = params.get(""only-if-exists"", False)
    request_params = params.get(""request-params"", {})
    for index_name in indices:
        if not only_if_exists:
            es.indices.delete(index=index_name, params=request_params)
            ops += 1
        elif only_if_exists and es.indices.exists(index=index_name):
            self.logger.info(""Index [%s] already exists. Deleting it."", index_name)
            es.indices.delete(index=index_name, params=request_params)
            ops += 1
    return ops, ""ops""
",elif only_if_exists and es . indices . exists ( index = index_name ) :,198
"def get():
    result = []
    for b in self.key_bindings:
        if len(keys) < len(b.keys):
            match = True
            for i, j in zip(b.keys, keys):
                if i != j and i != Keys.Any:
                    match = False
                    break
            if match:
                result.append(b)
    return result
",if len ( keys ) < len ( b . keys ) :,113
"def get_arg_list_scalar_arg_dtypes(arg_types):
    result = []
    for arg_type in arg_types:
        if isinstance(arg_type, ScalarArg):
            result.append(arg_type.dtype)
        elif isinstance(arg_type, VectorArg):
            result.append(None)
            if arg_type.with_offset:
                result.append(np.int64)
        else:
            raise RuntimeError(""arg type not understood: %s"" % type(arg_type))
    return result
","if isinstance ( arg_type , ScalarArg ) :",142
"def autocommitter():
    while True:
        try:
            if not self._running:
                break
            if self._auto_commit_enable:
                self._auto_commit()
            self._cluster.handler.sleep(self._auto_commit_interval_ms / 1000)
        except ReferenceError:
            break
        except Exception:
            # surface all exceptions to the main thread
            self._worker_exception = sys.exc_info()
            break
    log.debug(""Autocommitter thread exiting"")
",if self . _auto_commit_enable :,141
"def on_conflict(self, *target_fields: Union[str, Term]) -> ""PostgreSQLQueryBuilder"":
    if not self._insert_table:
        raise QueryException(""On conflict only applies to insert query"")
    self._on_conflict = True
    for target_field in target_fields:
        if isinstance(target_field, str):
            self._on_conflict_fields.append(self._conflict_field_str(target_field))
        elif isinstance(target_field, Term):
            self._on_conflict_fields.append(target_field)
","if isinstance ( target_field , str ) :",141
"def change_TV_DOWNLOAD_DIR(tv_download_dir):
    if tv_download_dir == """":
        sickbeard.TV_DOWNLOAD_DIR = """"
        return True
    if os.path.normpath(sickbeard.TV_DOWNLOAD_DIR) != os.path.normpath(tv_download_dir):
        if helpers.makeDir(tv_download_dir):
            sickbeard.TV_DOWNLOAD_DIR = os.path.normpath(tv_download_dir)
            logger.log(u""Changed TV download folder to "" + tv_download_dir)
        else:
            return False
    return True
",if helpers . makeDir ( tv_download_dir ) :,157
"def save_config(self, cmd=""save config"", confirm=True, confirm_response=""y""):
    """"""Saves Config.""""""
    self.enable()
    if confirm:
        output = self.send_command_timing(command_string=cmd)
        if confirm_response:
            output += self.send_command_timing(confirm_response)
        else:
            # Send enter by default
            output += self.send_command_timing(self.RETURN)
    else:
        # Some devices are slow so match on trailing-prompt if you can
        output = self.send_command(command_string=cmd)
    return output
",if confirm_response :,159
"def apply_gradient_for_batch(inputs, labels, weights, loss):
    with tf.GradientTape() as tape:
        outputs = self.model(inputs, training=True)
        if isinstance(outputs, tf.Tensor):
            outputs = [outputs]
        if self._loss_outputs is not None:
            outputs = [outputs[i] for i in self._loss_outputs]
        batch_loss = loss(outputs, labels, weights)
    if variables is None:
        vars = self.model.trainable_variables
    else:
        vars = variables
    grads = tape.gradient(batch_loss, vars)
    self._tf_optimizer.apply_gradients(zip(grads, vars))
    self._global_step.assign_add(1)
    return batch_loss
","if isinstance ( outputs , tf . Tensor ) :",193
"def sort(self, items):
    slow_sorts = []
    switch_slow = False
    for sort in reversed(self.sorts):
        if switch_slow:
            slow_sorts.append(sort)
        elif sort.order_clause() is None:
            switch_slow = True
            slow_sorts.append(sort)
        else:
            pass
    for sort in slow_sorts:
        items = sort.sort(items)
    return items
",if switch_slow :,121
"def getmod(self, nm):
    mod = None
    for thing in self.path:
        if isinstance(thing, basestring):
            owner = self.shadowpath.get(thing, -1)
            if owner == -1:
                owner = self.shadowpath[thing] = self.__makeOwner(thing)
            if owner:
                mod = owner.getmod(nm)
        else:
            mod = thing.getmod(nm)
        if mod:
            break
    return mod
",if owner :,137
"def has(self, key):
    filename = self._get_filename(key)
    try:
        with open(filename, ""rb"") as f:
            pickle_time = pickle.load(f)
            if pickle_time == 0 or pickle_time >= time():
                return True
            else:
                os.remove(filename)
                return False
    except (IOError, OSError, pickle.PickleError):
        return False
",if pickle_time == 0 or pickle_time >= time ( ) :,117
"def forward(self, hs):
    h = self.c0(hs[-1])
    for i in range(1, 8):
        h = F.concat([h, hs[-i - 1]])
        if i < 7:
            h = self[""c%d"" % i](h)
        else:
            h = self.c7(h)
    return h
",if i < 7 :,96
"def get_custom_behaviour2(self):
    string = """"
    for arg in list(self.defaults.keys()) + self.var:
        if arg in self.__dict__:
            # Don't add redundant lines e.g. sus=sus;
            if str(arg) != str(self.__dict__[arg]):
                string += str(arg) + ""="" + str(self.__dict__[arg]) + "";\n""
    return string
",if arg in self . __dict__ :,112
"def _apply_operation(self, values):
    """"""Method that defines the less-than-or-equal operation""""""
    arg1 = next(values)
    for strict in self._strict:
        arg2 = next(values)
        if strict:
            if not (arg1 < arg2):
                return False
        else:
            if not (arg1 <= arg2):
                return False
        arg1 = arg2
    return True
",if not ( arg1 <= arg2 ) :,118
"def i_pshufb(self, op, off=0):
    dst = self.getOperValue(op, off)
    src = self.getOperValue(op, off)
    res = 0
    if op.opers[0].tsize == 8:
        mask = 0x07
    else:
        mask = 0x0F
    for i in range(op.opers[0].tsize):
        shfl = src & (1 << ((i * 8) + 7))
        if shfl:
            s = 0
        else:
            indx = (src >> (i * 8)) & mask
            s = (src >> (indx * 8)) & 0xFF
        res |= s << (i * 8)
    self.setOperValue(op, 0, res)
",if shfl :,199
"def report_out_of_quota(self, appid):
    self.logger.warn(""report_out_of_quota:%s"", appid)
    with self.lock:
        if appid not in self.out_of_quota_appids:
            self.out_of_quota_appids.append(appid)
        try:
            self.working_appid_list.remove(appid)
        except:
            pass
",if appid not in self . out_of_quota_appids :,115
"def to_py(self, value: _StrUnset) -> _StrUnsetNone:
    self._basic_py_validation(value, str)
    if isinstance(value, usertypes.Unset):
        return value
    elif not value:
        return None
    value = os.path.expandvars(value)
    value = os.path.expanduser(value)
    try:
        if not os.path.isdir(value):
            raise configexc.ValidationError(value, ""must be a valid directory!"")
        if not os.path.isabs(value):
            raise configexc.ValidationError(value, ""must be an absolute path!"")
    except UnicodeEncodeError as e:
        raise configexc.ValidationError(value, e)
    return value
",if not os . path . isdir ( value ) :,181
"def findinDoc(self, tagpath, pos, end):
    result = None
    if end == -1:
        end = self.docSize
    else:
        end = min(self.docSize, end)
    foundat = -1
    for j in range(pos, end):
        item = self.docList[j]
        if item.find(b""="") >= 0:
            (name, argres) = item.split(b""="", 1)
        else:
            name = item
            argres = """"
        if isinstance(tagpath, str):
            tagpath = tagpath.encode(""utf-8"")
        if name.endswith(tagpath):
            result = argres
            foundat = j
            break
    return foundat, result
","if isinstance ( tagpath , str ) :",189
"def has_safe_repr(value):
    """"""Does the node have a safe representation?""""""
    if value is None or value is NotImplemented or value is Ellipsis:
        return True
    if isinstance(value, (bool, int, long, float, complex, basestring, xrange, Markup)):
        return True
    if isinstance(value, (tuple, list, set, frozenset)):
        for item in value:
            if not has_safe_repr(item):
                return False
        return True
    elif isinstance(value, dict):
        for key, value in value.iteritems():
            if not has_safe_repr(key):
                return False
            if not has_safe_repr(value):
                return False
        return True
    return False
",if not has_safe_repr ( value ) :,192
"def run(self):
    # Make some objects emit lights
    for obj in bpy.context.scene.objects:
        if ""modelId"" in obj:
            obj_id = obj[""modelId""]
            # In the case of the lamp
            if obj_id in self.lights:
                self._make_lamp_emissive(obj, self.lights[obj_id])
            # Make the windows emit light
            if obj_id in self.windows:
                self._make_window_emissive(obj)
            # Also make ceilings slightly emit light
            if obj.name.startswith(""Ceiling#""):
                self._make_ceiling_emissive(obj)
","if ""modelId"" in obj :",190
"def bitvector_case_fn(
    rng: Random, mode: RandomizationMode, size: int, invalid_making_pos: int = None
):
    bits = get_random_ssz_object(
        rng,
        Bitvector[size],
        max_bytes_length=(size + 7) // 8,
        max_list_length=size,
        mode=mode,
        chaos=False,
    )
    if invalid_making_pos is not None and invalid_making_pos <= size:
        already_invalid = False
        for i in range(invalid_making_pos, size):
            if bits[i]:
                already_invalid = True
        if not already_invalid:
            bits[invalid_making_pos] = True
    return bits
",if bits [ i ] :,196
"def get_transaction_execution_results(self, batch_signature):
    with self._condition:
        batch_status = self._batch_statuses.get(batch_signature)
        if batch_status is None:
            return None
        annotated_batch = self._batch_by_id.get(batch_signature)
        if annotated_batch is None:
            return None
        results = []
        for txn in annotated_batch.batch.transactions:
            result = self._txn_results.get(txn.header_signature)
            if result is not None:
                results.append(result)
        return results
",if result is not None :,161
"def one_xmm_reg_imm8(ii):  # also allows SSE4 2-imm8 instr
    i, j, n = 0, 0, 0
    for op in _gen_opnds(ii):
        if op_reg(op) and op_xmm(op):
            n += 1
        elif op_imm8(op):
            i += 1
        elif op_imm8_2(op):
            j += 1
        else:
            return False
    return n == 1 and i == 1 and j <= 1
",elif op_imm8_2 ( op ) :,141
"def whichmodule(obj, name):
    """"""Find the module an object belong to.""""""
    module_name = getattr(obj, ""__module__"", None)
    if module_name is not None:
        return module_name
    # Protect the iteration by using a list copy of sys.modules against dynamic
    # modules that trigger imports of other modules upon calls to getattr.
    for module_name, module in sys.modules.copy().items():
        if module_name == ""__main__"" or module is None:
            continue
        try:
            if _getattribute(module, name)[0] is obj:
                return module_name
        except AttributeError:
            pass
    return ""__main__""
","if _getattribute ( module , name ) [ 0 ] is obj :",171
"def get_ld_header_info(p):
    # ""nested-function, but placed at module level
    # as an ld_header was found, return known paths, archives and members
    # these lines start with a digit
    info = []
    for line in p.stdout:
        if re.match(""[0-9]"", line):
            info.append(line)
        else:
            # blank line (separator), consume line and end for loop
            break
    return info
","if re . match ( ""[0-9]"" , line ) :",120
"def write(self, s):
    if self.closed:
        raise ValueError(""write to closed file"")
    if type(s) not in (unicode, str, bytearray):
        # See issue #19481
        if isinstance(s, unicode):
            s = unicode.__getitem__(s, slice(None))
        elif isinstance(s, str):
            s = str.__str__(s)
        elif isinstance(s, bytearray):
            s = bytearray.__str__(s)
        else:
            raise TypeError(""must be string, not "" + type(s).__name__)
    return self.shell.write(s, self.tags)
","if isinstance ( s , unicode ) :",161
"def generate_forwards(cls, attrs):
    # forward functions of _forwards
    for attr_name, attr in cls._forwards.__dict__.items():
        if attr_name.startswith(""_"") or attr_name in attrs:
            continue
        if isinstance(attr, property):
            cls._forward.append(attr_name)
        elif isinstance(attr, types.FunctionType):
            wrapper = _forward_factory(cls, attr_name, attr)
            setattr(cls, attr_name, wrapper)
        else:
            raise TypeError(attr_name, type(attr))
","if attr_name . startswith ( ""_"" ) or attr_name in attrs :",146
"def _user_has_dnd(bot, user_id):
    try:
        return bot.call_shared(""dnd.user_check"", user_id)  # shared dnd check
    except KeyError:
        logger.warning(""mentions: falling back to legacy _user_has_dnd()"")
        initiator_has_dnd = False
        if bot.memory.exists([""donotdisturb""]):
            donotdisturb = bot.memory.get(""donotdisturb"")
            if user_id in donotdisturb:
                initiator_has_dnd = True
        return initiator_has_dnd
","if bot . memory . exists ( [ ""donotdisturb"" ] ) :",162
"def init(self):
    """"""Initialize a fighter from the database and validate""""""
    self.__item = None
    if self.itemID:
        self.__item = eos.db.getItem(self.itemID)
        if self.__item is None:
            pyfalog.error(""Item (id: {0}) does not exist"", self.itemID)
            return
    if self.isInvalid:
        pyfalog.error(""Item (id: {0}) is not a Fighter"", self.itemID)
        return
    self.build()
",if self . __item is None :,141
"def _pg_sku_name_validator(sku_name, sku_info, tier):
    if sku_name:
        skus = get_postgres_skus(sku_info, tier)
        if sku_name not in skus:
            error_msg = (
                ""Incorrect value for --sku-name. ""
                + ""The SKU name does not match {} tier. Specify --tier if you did not. "".format(
                    tier
                )
            )
            raise CLIError(error_msg + ""Allowed values : {}"".format(skus))
",if sku_name not in skus :,156
"def _parse_paternity_log(writer, file):
    parent_map = {}
    parent_map[0] = 0
    for line in file.read().decode(""utf-8"").split(""\n""):
        if not line:
            continue
        elems = line.split("" "")  # <Child> <Parent>
        if len(elems) >= 2:
            #                       print ""paternity of %d is %d"" % (int(elems[0]), int(elems[1]))
            parent_map[int(elems[0])] = int(elems[1])
        else:
            print(""Odd paternity line '%s'"" % (line))
    return parent_map
",if not line :,196
"def _get_next_cap(self):
    # type: () -> bool
    self._curr_cap = None
    if self._curr_cap_idx is None:
        self._curr_cap_idx = 0
        self._curr_cap = self._cap_list[0]
        return True
    else:
        if not (self._curr_cap_idx + 1) < len(self._cap_list):
            self._end_of_video = True
            return False
        self._curr_cap_idx += 1
        self._curr_cap = self._cap_list[self._curr_cap_idx]
        return True
",if not ( self . _curr_cap_idx + 1 ) < len ( self . _cap_list ) :,163
"def decode_payload(args):
    try:
        if args.token:
            token = args.token
        else:
            if sys.stdin.isatty():
                token = sys.stdin.readline().strip()
            else:
                raise IOError(""Cannot read from stdin: terminal not a TTY"")
        token = token.encode(""utf-8"")
        data = decode(token, key=args.key, verify=args.verify)
        return json.dumps(data)
    except DecodeError as e:
        raise DecodeError(""There was an error decoding the token: %s"" % e)
",if sys . stdin . isatty ( ) :,156
"def cell_double_clicked(self, row, column):
    if column == 3:
        archive_name = self.selected_archive_name()
        if not archive_name:
            return
        mount_point = self.mount_points.get(archive_name)
        if mount_point is not None:
            QDesktopServices.openUrl(QtCore.QUrl(f""file:///{mount_point}""))
",if not archive_name :,107
"def tiles_around(self, pos, radius=1, predicate=None):
    ps = []
    x, y = pos
    for dx in range(-radius, radius + 1):
        nx = x + dx
        if nx >= 0 and nx < self.width:
            for dy in range(-radius, radius + 1):
                ny = y + dy
                if ny >= 0 and ny < self.height and (dx != 0 or dy != 0):
                    if predicate is None or predicate((nx, ny)):
                        ps.append((nx, ny))
    return ps
",if nx >= 0 and nx < self . width :,151
"def __init__(self, type, data, name=None):
    Constant.__init__(self, type, data, name)
    self.tag.unique_value = None
    if isinstance(data, np.ndarray) and data.ndim > 0:
        flat_data = data.ravel()
        if flat_data.shape[0]:
            if (flat_data == flat_data[0]).all():
                self.tag.unique_value = flat_data[0]
",if ( flat_data == flat_data [ 0 ] ) . all ( ) :,118
"def git_convert_standalone_clone(repodir):
    """"""If specified directory is a git repository, ensure it's a standalone clone""""""
    import bb.process
    if os.path.exists(os.path.join(repodir, "".git"")):
        alternatesfile = os.path.join(repodir, "".git"", ""objects"", ""info"", ""alternates"")
        if os.path.exists(alternatesfile):
            # This will have been cloned with -s, so we need to convert it so none
            # of the contents is shared
            bb.process.run(""git repack -a"", cwd=repodir)
            os.remove(alternatesfile)
",if os . path . exists ( alternatesfile ) :,166
"def _rename_recipe_file(oldrecipe, bpn, oldpv, newpv, path):
    oldrecipe = os.path.basename(oldrecipe)
    if oldrecipe.endswith(""_%s.bb"" % oldpv):
        newrecipe = ""%s_%s.bb"" % (bpn, newpv)
        if oldrecipe != newrecipe:
            shutil.move(os.path.join(path, oldrecipe), os.path.join(path, newrecipe))
    else:
        newrecipe = oldrecipe
    return os.path.join(path, newrecipe)
",if oldrecipe != newrecipe :,154
"def profiling_startup():
    if ""--profile-sverchok-startup"" in sys.argv:
        global _profile_nesting
        profile = None
        try:
            profile = get_global_profile()
            _profile_nesting += 1
            if _profile_nesting == 1:
                profile.enable()
            yield profile
        finally:
            _profile_nesting -= 1
            if _profile_nesting == 0 and profile is not None:
                profile.disable()
            dump_stats(file_path=""sverchok_profile.txt"")
            save_stats(""sverchok_profile.prof"")
    else:
        yield None
",if _profile_nesting == 1 :,180
"def to_scaled_dtype(val):
    """"""Parse *val* to return a dtype.""""""
    res = []
    for i in val:
        if i[1].startswith(""S""):
            res.append((i[0], i[1]) + i[2:-1])
        else:
            try:
                res.append((i[0], i[-1].dtype) + i[2:-1])
            except AttributeError:
                res.append((i[0], type(i[-1])) + i[2:-1])
    return np.dtype(res)
","if i [ 1 ] . startswith ( ""S"" ) :",148
"def row(self, indx):
    if indx not in self.__rows:
        if indx in self.__flushed_rows:
            raise Exception(
                ""Attempt to reuse row index %d of sheet %r after flushing""
                % (indx, self.__name)
            )
        self.__rows[indx] = self.Row(indx, self)
        if indx > self.last_used_row:
            self.last_used_row = indx
        if indx < self.first_used_row:
            self.first_used_row = indx
    return self.__rows[indx]
",if indx > self . last_used_row :,157
"def _flow_open(self):
    rv = []
    for pipe in self.pipes:
        if pipe._pipeline_all_methods_.issuperset({""open"", self._method_open}):
            raise RuntimeError(
                f""{pipe.__class__.__name__} pipe has double open methods.""
                f"" Use `open` or `{self._method_open}`, not both.""
            )
        if ""open"" in pipe._pipeline_all_methods_:
            rv.append(pipe.open)
        if self._method_open in pipe._pipeline_all_methods_:
            rv.append(getattr(pipe, self._method_open))
    return rv
","if pipe . _pipeline_all_methods_ . issuperset ( { ""open"" , self . _method_open } ) :",167
"def _parse_output(output, strict=False):
    for pkg in _yum_pkginfo(output):
        if strict and (pkg.repoid not in repos or not _check_args(args, pkg.name)):
            continue
        repo_dict = ret.setdefault(pkg.repoid, {})
        version_list = repo_dict.setdefault(pkg.name, set())
        version_list.add(pkg.version)
","if strict and ( pkg . repoid not in repos or not _check_args ( args , pkg . name ) ) :",108
"def user_defined_os():
    if menu.options.os:
        if menu.options.os.lower() == ""windows"":
            settings.TARGET_OS = ""win""
            return True
        elif menu.options.os.lower() == ""unix"":
            return True
        else:
            err_msg = ""You specified wrong value '"" + menu.options.os + ""' ""
            err_msg += ""as an operation system. The value, must be 'Windows' or 'Unix'.""
            print(settings.print_critical_msg(err_msg))
            raise SystemExit()
","elif menu . options . os . lower ( ) == ""unix"" :",153
"def update(self, topLeft, bottomRight):
    if self._updating:
        # We are currently putting data in the model, so no updates
        return
    if self._index:
        if topLeft.row() <= self._index.row() <= bottomRight.row():
            self.updateText()
    elif self._indexes:
        update = False
        for i in self._indexes:
            if topLeft.row() <= i.row() <= bottomRight.row():
                update = True
        if update:
            self.updateText()
",if topLeft . row ( ) <= i . row ( ) <= bottomRight . row ( ) :,144
"def _wrapper(self, pipe, _should_terminate_flag, generator, *args, **kwargs):
    """"""Executed in background, pipes generator results to foreground""""""
    logger.debug(""Entering _wrapper"")
    try:
        for datum in generator(*args, **kwargs):
            if _should_terminate_flag.value:
                raise EarlyCancellationError(""Task was cancelled"")
            pipe.send(datum)
    except Exception as e:
        if not isinstance(e, EarlyCancellationError):
            pipe.send(e)
            import traceback
            logger.warning(traceback.format_exc())
    else:
        pipe.send(StopIteration())
    finally:
        pipe.close()
        logger.debug(""Exiting _wrapper"")
",if _should_terminate_flag . value :,192
"def _flatten(*args):
    arglist = []
    for arg in args:
        if isinstance(arg, _Block):
            if arg.vhdl_code is not None:
                arglist.append(arg.vhdl_code)
                continue
            else:
                arg = arg.subs
        if id(arg) in _userCodeMap[""vhdl""]:
            arglist.append(_userCodeMap[""vhdl""][id(arg)])
        elif isinstance(arg, (list, tuple, set)):
            for item in arg:
                arglist.extend(_flatten(item))
        else:
            arglist.append(arg)
    return arglist
",if arg . vhdl_code is not None :,179
"def _get_target_and_lun(self, context, volume):
    iscsi_target = 0
    if not self.target_name or not self._get_group():
        lun = 1
        return iscsi_target, lun
    luns = self._get_luns_info()
    if (not luns) or (luns[0] != 1):
        lun = 1
        return iscsi_target, lun
    else:
        for lun in luns:
            if (luns[-1] == lun) or (luns[lun - 1] + 1 != luns[lun]):
                return iscsi_target, (lun + 1)
",if ( luns [ - 1 ] == lun ) or ( luns [ lun - 1 ] + 1 != luns [ lun ] ) :,188
"def check_find(ref):
    # Check find returns indexes for single point codes
    for c in set(m.used):
        start = 0
        u = m.text
        while start < m.size:
            i = u.find(c, start)
            if i < 0:
                break
            self.assertEqual(u[i], c)
            self.assertGreaterEqual(i, start)
            start = i + 1
",if i < 0 :,118
"def _format_column_list(self, data):
    # Now we have all lis of columns which we need
    # to include in our create definition, Let's format them
    if ""columns"" in data:
        for c in data[""columns""]:
            if ""attacl"" in c:
                c[""attacl""] = parse_priv_to_db(c[""attacl""], self.column_acl)
            # check type for '[]' in it
            if ""cltype"" in c:
                c[""cltype""], c[""hasSqrBracket""] = column_utils.type_formatter(
                    c[""cltype""]
                )
","if ""attacl"" in c :",170
"def _animate_strategy(self, speed=1):
    if self._animating == 0:
        return
    if self._apply_strategy() is not None:
        if self._animate.get() == 0 or self._step.get() == 1:
            return
        if self._animate.get() == 1:
            self._root.after(3000, self._animate_strategy)
        elif self._animate.get() == 2:
            self._root.after(1000, self._animate_strategy)
        else:
            self._root.after(20, self._animate_strategy)
",if self . _animate . get ( ) == 1 :,151
"def close_all(map=None, ignore_all=False):
    if map is None:  # pragma: no cover
        map = socket_map
    for x in list(map.values()):  # list() FBO py3
        try:
            x.close()
        except OSError as x:
            if x.args[0] == EBADF:
                pass
            elif not ignore_all:
                raise
        except _reraised_exceptions:
            raise
        except:
            if not ignore_all:
                raise
    map.clear()
",if not ignore_all :,157
"def iter_imports(path):
    """"""Yield imports in *path*""""""
    for node in ast.parse(open(path, ""rb"").read()).body:
        if isinstance(node, ast.ImportFrom):
            if node.module is None:
                prefix = ()
            else:
                prefix = tuple(node.module.split("".""))
            for snode in node.names:
                yield (node.level, prefix + (snode.name,))
        elif isinstance(node, ast.Import):
            for node in node.names:
                yield (0, tuple(node.name.split(""."")))
","elif isinstance ( node , ast . Import ) :",162
"def one_stage_eval_model(data_reader_eval, myModel, loss_criterion=None):
    score_tot = 0
    n_sample_tot = 0
    loss_tot = 0
    for idx, batch in enumerate(data_reader_eval):
        score, loss, n_sample = compute_a_batch(
            batch, myModel, eval_mode=True, loss_criterion=loss_criterion
        )
        score_tot += score
        n_sample_tot += n_sample
        if loss is not None:
            loss_tot += loss.data[0] * n_sample
    return score_tot / n_sample_tot, loss_tot / n_sample_tot, n_sample_tot
",if loss is not None :,181
"def _process_preproc(self, token, content):
    if self.state == ""include"":
        if content != ""\n"" and content != ""#"":
            content = content.strip().strip('""').strip(""<"").strip("">"").strip()
            self.append(content, truncate=True, separator=""/"")
        self.state = None
    elif content.strip().startswith(""include""):
        self.state = ""include""
    else:
        self.state = None
","if content != ""\n"" and content != ""#"" :",115
"def _aggregate_metadata_attribute(
    self, attr, agg_func=np.max, default_value=0, from_type_metadata=True
):
    attr_values = []
    for a in self.appliances:
        if from_type_metadata:
            attr_value = a.type.get(attr)
        else:
            attr_value = a.metadata.get(attr)
        if attr_value is not None:
            attr_values.append(attr_value)
    if len(attr_values) == 0:
        return default_value
    else:
        return agg_func(attr_values)
",if attr_value is not None :,162
"def _remove(self, item):
    """"""Internal removal of an item""""""
    # Manage siblings when items are deleted
    for sibling in self.lines[self.lines.index(item) + 1 :]:
        if isinstance(sibling, CronItem):
            env = sibling.env
            sibling.env = item.env
            sibling.env.update(env)
            sibling.env.job = sibling
            break
        elif sibling == """":
            self.lines.remove(sibling)
        else:
            break
    self.crons.remove(item)
    self.lines.remove(item)
    return 1
","elif sibling == """" :",162
"def _validate_command_chain(self) -> None:
    """"""Validate command-chain names.""""""
    # Would normally get caught/handled by schema validation.
    for command in self.command_chain:
        if not re.match(""^[A-Za-z0-9/._#:$-]*$"", command):
            raise HookValidationError(
                hook_name=self.hook_name,
                message=f""{command!r} is not a valid command-chain command."",
            )
","if not re . match ( ""^[A-Za-z0-9/._#:$-]*$"" , command ) :",124
"def _handle_unpaired_tag(self, html_tag):
    self.handle_ignore(html_tag, is_open=False)
    jannotations = self.read_jannotations(html_tag)
    for jannotation in arg_to_iter(jannotations):
        if self.unpairedtag_stack:
            self._close_unpaired_tag()
        self.extra_required_attrs.extend(jannotation.pop(""required"", []))
        annotation = self.build_annotation(jannotation)
        self.handle_variant(annotation, is_open=False)
        self.annotations.append(annotation)
    self.next_tag_index += 1
",if self . unpairedtag_stack :,164
"def browser(self):
    if not hasattr(self, ""_browser""):
        self.loop = asyncio.get_event_loop()
        if self.loop.is_running():
            raise RuntimeError(
                ""Cannot use HTMLSession within an existing event loop. Use AsyncHTMLSession instead.""
            )
        self._browser = self.loop.run_until_complete(super().browser)
    return self._browser
",if self . loop . is_running ( ) :,105
"def process(self, node):
    self.vars = []
    for child in node.childNodes:
        if child.nodeType == node.ELEMENT_NODE:
            child_text = get_xml_text(child)
            if child_text == """":  # pragma:nocover
                continue
            if child.nodeName == ""Real"":
                for val in re.split(""[\t ]+"", child_text):
                    self.vars.append(1.0 * eval(val))
    return self
","if child_text == """" :",135
"def instantiate(self, node, container=None):
    var = self.vm.program.NewVariable()
    if container and (
        not isinstance(container, SimpleValue)
        or self.full_name in container.all_template_names
    ):
        instance = TypeParameterInstance(self, container, self.vm)
        return instance.to_variable(node)
    else:
        for c in self.constraints:
            var.PasteVariable(c.instantiate(node, container))
        if self.bound:
            var.PasteVariable(self.bound.instantiate(node, container))
    if not var.bindings:
        var.AddBinding(self.vm.convert.unsolvable, [], node)
    return var
",if self . bound :,184
"def compare_tables(self, db1, db2):
    i1 = db1.query(""SELECT id, buf FROM test ORDER BY id"")
    i2 = db2.query(""SELECT id, buf FROM test ORDER BY id"")
    for (id1, buf1) in i1:
        (id2, buf2) = next(i2)
        self.assertEqual(id1, id2)
        if isinstance(buf1, float):
            self.assertAlmostEqual(buf1, buf2, places=9)
        else:
            self.assertEqual(buf1, buf2)
    self.assertRaises(StopIteration, i2.__next__)
","if isinstance ( buf1 , float ) :",158
"def list_full_file_paths(directory):
    """"""List the absolute paths of files in |directory|.""""""
    directory_absolute_path = os.path.abspath(directory)
    paths = []
    for relative_path in os.listdir(directory):
        absolute_path = os.path.join(directory_absolute_path, relative_path)
        if os.path.isfile(absolute_path):  # Only return paths to files.
            paths.append(absolute_path)
    return paths
",if os . path . isfile ( absolute_path ) :,121
"def reparentChildren(self, newParent):
    while self.element.contents:
        child = self.element.contents[0]
        child.extract()
        if isinstance(child, Tag):
            newParent.appendChild(Element(child, self.soup, namespaces[""html""]))
        else:
            newParent.appendChild(TextNode(child, self.soup))
","if isinstance ( child , Tag ) :",94
"def sort(self):
    sorted_models = []
    concrete_models = set()
    models = list(self.data)
    while len(sorted_models) < len(models):
        found = False
        for model in models:
            if model in sorted_models:
                continue
            dependencies = self.dependencies.get(model._meta.concrete_model)
            if not (dependencies and dependencies.difference(concrete_models)):
                sorted_models.append(model)
                concrete_models.add(model._meta.concrete_model)
                found = True
        if not found:
            return
    self.data = OrderedDict((model, self.data[model]) for model in sorted_models)
",if not found :,188
"def template(self):
    """"""template property""""""
    if self._template is None:
        results = self._process(self.name, False, self.params, self.data)
        if results[""returncode""] != 0:
            raise OpenShiftCLIError(
                ""Error processing template [%s]: %s"" % (self.name, results)
            )
        self._template = results[""results""][""items""]
    return self._template
","if results [ ""returncode"" ] != 0 :",109
"def edit_file(self, filename):
    import subprocess
    editor = self.get_editor()
    if self.env:
        environ = os.environ.copy()
        environ.update(self.env)
    else:
        environ = None
    try:
        c = subprocess.Popen('%s ""%s""' % (editor, filename), env=environ, shell=True)
        exit_code = c.wait()
        if exit_code != 0:
            raise Exception(""%s: Editing failed!"" % editor)
    except OSError as e:
        raise Exception(""%s: Editing failed: %s"" % (editor, e))
",if exit_code != 0 :,157
"def test01e_json(self):
    ""Testing GeoJSON input/output.""
    from django.contrib.gis.gdal.prototypes.geom import GEOJSON
    if not GEOJSON:
        return
    for g in self.geometries.json_geoms:
        geom = OGRGeometry(g.wkt)
        if not hasattr(g, ""not_equal""):
            self.assertEqual(g.json, geom.json)
            self.assertEqual(g.json, geom.geojson)
        self.assertEqual(OGRGeometry(g.wkt), OGRGeometry(geom.json))
","if not hasattr ( g , ""not_equal"" ) :",154
"def debug(self):
    feed_dict = self.get_test_feed_dict()
    while True:
        tensor_name = input(""Input debug tensor name: "").strip()
        if tensor_name == ""q"":
            sys.exit(0)
        try:
            debug_tensor = self.graph.get_tensor_by_name(tensor_name)
        except Exception as e:
            logging.error(e)
            continue
        res = self.sess.run(debug_tensor, feed_dict=feed_dict)
        logging.info(f""Result for tensor {tensor_name} is: {res}"")
","if tensor_name == ""q"" :",162
"def get_location(self, dist, dependency_links):
    for url in dependency_links:
        egg_fragment = Link(url).egg_fragment
        if not egg_fragment:
            continue
        if ""-"" in egg_fragment:
            ## FIXME: will this work when a package has - in the name?
            key = ""-"".join(egg_fragment.split(""-"")[:-1]).lower()
        else:
            key = egg_fragment
        if key == dist.key:
            return url.split(""#"", 1)[0]
    return None
",if key == dist . key :,141
"def select(result):
    for elem in result:
        parent = elem.getparent()
        if parent is None:
            continue
        try:
            # FIXME: what if the selector is ""*"" ?
            elems = list(parent.iterchildren(elem.tag))
            if elems[index] is elem:
                yield elem
        except IndexError:
            pass
",if parent is None :,101
"def execute(self, cmd):
    mark = utils.random_text(32)
    path = ""/cgi-bin/gdrive.cgi?cmd=4&f_gaccount=;{};echo {};"".format(cmd, mark)
    response = self.http_request(
        method=""GET"",
        path=path,
    )
    if response is None:
        return """"
    if mark in response.text:
        regexp = ""(|.+?){}"".format(mark)
        res = re.findall(regexp, response.text, re.DOTALL)
        if len(res):
            return res[0]
    return """"
",if len ( res ) :,155
"def join(s, *p):
    path = s
    for t in p:
        if (not s) or isabs(t):
            path = t
            continue
        if t[:1] == "":"":
            t = t[1:]
        if "":"" not in path:
            path = "":"" + path
        if path[-1:] != "":"":
            path = path + "":""
        path = path + t
    return path
","if t [ : 1 ] == "":"" :",115
"def do_remove(self):
    if self.netconf.locked(""dhcp""):
        if not self.pid:
            pid = read_pid_file(""/var/run/udhcpd.pan1.pid"")
        else:
            pid = self.pid
        if not kill(pid, ""udhcpd""):
            logging.info(""Stale dhcp lockfile found"")
        self.netconf.unlock(""dhcp"")
","if not kill ( pid , ""udhcpd"" ) :",110
"def filter_packages(query, package_infos):
    if query is None:
        return package_infos
    try:
        if ""!"" in query:
            raise ConanException(""'!' character is not allowed"")
        if "" not "" in query or query.startswith(""not ""):
            raise ConanException(""'not' operator is not allowed"")
        postfix = infix_to_postfix(query) if query else []
        result = OrderedDict()
        for package_id, info in package_infos.items():
            if _evaluate_postfix_with_info(postfix, info):
                result[package_id] = info
        return result
    except Exception as exc:
        raise ConanException(""Invalid package query: %s. %s"" % (query, exc))
","if _evaluate_postfix_with_info ( postfix , info ) :",193
"def __add__(self, other):
    if isinstance(other, Vector3):
        # Vector + Vector -> Vector
        # Vector + Point -> Point
        # Point + Point -> Vector
        if self.__class__ is other.__class__:
            _class = Vector3
        else:
            _class = Point3
        return _class(self.x + other.x, self.y + other.y, self.z + other.z)
    else:
        assert hasattr(other, ""__len__"") and len(other) == 3
        return Vector3(self.x + other[0], self.y + other[1], self.z + other[2])
",if self . __class__ is other . __class__ :,166
"def test_scout():
    test_status = False
    with open(""/tmp/test_scout_output"", ""w"") as logfile:
        if not DockerImage:
            logfile.write(""No $AMBASSADOR_DOCKER_IMAGE??\n"")
        else:
            if docker_start(logfile):
                if wait_for_diagd(logfile) and check_chimes(logfile):
                    test_status = True
                docker_kill(logfile)
    if not test_status:
        with open(""/tmp/test_scout_output"", ""r"") as logfile:
            for line in logfile:
                print(line.rstrip())
    assert test_status, ""test failed""
",if docker_start ( logfile ) :,189
"def visit_Assign(self, node):
    """"""Handle visiting an assignment statement.""""""
    ups = set()
    for targ in node.targets:
        if isinstance(targ, (Tuple, List)):
            ups.update(leftmostname(elt) for elt in targ.elts)
        elif isinstance(targ, BinOp):
            newnode = self.try_subproc_toks(node)
            if newnode is node:
                ups.add(leftmostname(targ))
            else:
                return newnode
        else:
            ups.add(leftmostname(targ))
    self.ctxupdate(ups)
    return node
","elif isinstance ( targ , BinOp ) :",165
"def get_config_h_filename():
    """"""Returns the path of pyconfig.h.""""""
    if _PYTHON_BUILD:
        # The additional check for != ""java"" secures against JyNI-monkeypatching.
        if os.name == ""nt"" and os.name != ""java"":
            inc_dir = os.path.join(_PROJECT_BASE, ""PC"")
        else:
            inc_dir = _PROJECT_BASE
    else:
        inc_dir = get_path(""platinclude"")
    return os.path.join(inc_dir, ""pyconfig.h"")
","if os . name == ""nt"" and os . name != ""java"" :",148
"def is_valid_block(self):
    """"""check wheter the block is valid in the current position""""""
    for i in range(self.block.x):
        for j in range(self.block.x):
            if self.block.get(i, j):
                if self.block.pos.x + i < 0:
                    return False
                if self.block.pos.x + i >= COLUMNS:
                    return False
                if self.block.pos.y + j < 0:
                    return False
                if self.map.get((self.block.pos.x + i, self.block.pos.y + j), False):
                    return False
    return True
",if self . block . pos . x + i >= COLUMNS :,192
"def __call__(self, execution_result):
    json_value = execution_result.get_output_in_json()
    actual_result = jmespath.search(
        self._query, json_value, jmespath.Options(collections.OrderedDict)
    )
    if not actual_result > self._expected_result:
        expected_result_format = ""> {}"".format(self._expected_result)
        if actual_result:
            raise JMESPathCheckAssertionError(
                self._query,
                expected_result_format,
                actual_result,
                execution_result.output,
            )
        raise JMESPathCheckAssertionError(
            self._query, expected_result_format, ""None"", execution_result.output
        )
",if actual_result :,194
"def readline(b):
    a = 1
    while True:
        if b:
            if b[0]:
                a = 2
                b = None
                continue
        b = None
        a = 5
        return a
",if b [ 0 ] :,70
"def test_execute_magic(self):
    """"""execute accepts IPython commands""""""
    view = self.client[:]
    view.execute(""a = 5"")
    ar = view.execute(""%whos"", block=True)
    # this will raise, if that failed
    ar.get(5)
    for stdout in ar.stdout:
        lines = stdout.splitlines()
        self.assertEqual(lines[0].split(), [""Variable"", ""Type"", ""Data/Info""])
        found = False
        for line in lines[2:]:
            split = line.split()
            if split == [""a"", ""int"", ""5""]:
                found = True
                break
        self.assertTrue(found, ""whos output wrong: %s"" % stdout)
","if split == [ ""a"" , ""int"" , ""5"" ] :",188
"def imgFileProcessingTick(output):
    if isinstance(output, tuple):
        workerOutput.append(output)
        workerPool.terminate()
    else:
        for page in output:
            if page is not None:
                options.imgMetadata[page[0]] = page[1]
                options.imgOld.append(page[2])
    if GUI:
        GUI.progressBarTick.emit(""tick"")
        if not GUI.conversionAlive:
            workerPool.terminate()
",if page is not None :,129
"def _load(xs):
    ret = []
    for x, ctx in zip(xs, context):
        if isinstance(x, tuple):
            ret.append([y.as_in_context(ctx) for y in x])
        else:
            ret.append(x.as_in_context(ctx))
    return ret
","if isinstance ( x , tuple ) :",85
"def _is_64bit_os():
    global _IS_64BIT_OS
    if _IS_64BIT_OS is None:
        if sys.maxsize > 2 ** 32:
            import platform
            _IS_64BIT_OS = platform.machine() == ""AMD64""
        else:
            _IS_64BIT_OS = False
    return _IS_64BIT_OS
",if sys . maxsize > 2 ** 32 :,101
"def stepStarted(self, step):
    self.currentStep = step
    for w in self.watchers:
        receiver = w.stepStarted(self, step)
        if receiver:
            if isinstance(receiver, type(())):
                step.subscribe(receiver[0], receiver[1])
            else:
                step.subscribe(receiver)
            d = step.waitUntilFinished()
            # TODO: This actually looks like a bug, but this code
            # will be removed anyway.
            # pylint: disable=cell-var-from-loop
            d.addCallback(lambda step: step.unsubscribe(receiver))
    step.waitUntilFinished().addCallback(self._stepFinished)
","if isinstance ( receiver , type ( ( ) ) ) :",183
"def connection(self, commit_on_success=False):
    with self._lock:
        if self._bulk_commit:
            if self._pending_connection is None:
                self._pending_connection = sqlite.connect(self.filename)
            con = self._pending_connection
        else:
            con = sqlite.connect(self.filename)
        try:
            if self.fast_save:
                con.execute(""PRAGMA synchronous = 0;"")
            yield con
            if commit_on_success and self.can_commit:
                con.commit()
        finally:
            if not self._bulk_commit:
                con.close()
",if commit_on_success and self . can_commit :,182
"def parse_response(self, response):
    # read response data from httpresponse, and parse it
    # Check for new http response object, otherwise it is a file object.
    if hasattr(response, ""getheader""):
        if response.getheader(""Content-Encoding"", """") == ""gzip"":
            stream = GzipDecodedResponse(response)
        else:
            stream = response
    else:
        stream = response
    p, u = self.getparser()
    while 1:
        data = stream.read(1024)
        if not data:
            break
        if self.verbose:
            print(""body:"", repr(data))
        p.feed(data)
    if stream is not response:
        stream.close()
    p.close()
    return u.close()
",if not data :,199
"def edge2str(self, nfrom, nto):
    if isinstance(nfrom, ExprCompose):
        for i in nfrom.args:
            if i[0] == nto:
                return ""[%s, %s]"" % (i[1], i[2])
    elif isinstance(nfrom, ExprCond):
        if nfrom.cond == nto:
            return ""?""
        elif nfrom.src1 == nto:
            return ""True""
        elif nfrom.src2 == nto:
            return ""False""
    return """"
",if nfrom . cond == nto :,149
"def gather_command_line_options(filter_disabled=None):
    """"""Get a sorted list of all CommandLineOption subclasses.""""""
    if filter_disabled is None:
        filter_disabled = not SETTINGS.COMMAND_LINE.SHOW_DISABLED_OPTIONS
    options = []
    for opt in get_inheritors(commandline_options.CommandLineOption):
        warnings.warn(
            ""Subclassing `CommandLineOption` is deprecated. Please ""
            ""use the `sacred.cli_option` decorator and pass the function ""
            ""to the Experiment constructor.""
        )
        if filter_disabled and not opt._enabled:
            continue
        options.append(opt)
    options += DEFAULT_COMMAND_LINE_OPTIONS
    return sorted(options, key=commandline_options.get_name)
",if filter_disabled and not opt . _enabled :,199
"def handle_disconnect(self):
    """"""Socket gets disconnected""""""
    # signal disconnected terminal with control lines
    try:
        self.serial.rts = False
        self.serial.dtr = False
    finally:
        # restore original port configuration in case it was changed
        self.serial.apply_settings(self.serial_settings_backup)
        # stop RFC 2217 state machine
        self.rfc2217 = None
        # clear send buffer
        self.buffer_ser2net = bytearray()
        # close network connection
        if self.socket is not None:
            self.socket.close()
            self.socket = None
            if self.log is not None:
                self.log.warning(""{}: Disconnected"".format(self.device))
",if self . log is not None :,195
"def answers(self, other):
    if not isinstance(other, TCP):
        return 0
    if conf.checkIPsrc:
        if not ((self.sport == other.sport) and (self.dport == other.dport)):
            return 0
    if conf.check_TCPerror_seqack:
        if self.seq is not None:
            if self.seq != other.seq:
                return 0
        if self.ack is not None:
            if self.ack != other.ack:
                return 0
    return 1
",if self . ack is not None :,143
"def _override_options(options, **overrides):
    """"""Override options.""""""
    for opt, val in overrides.items():
        passed_value = getattr(options, opt, _Default())
        if opt in (""ignore"", ""select"") and passed_value:
            value = process_value(opt, passed_value.value)
            value += process_value(opt, val)
            setattr(options, opt, value)
        elif isinstance(passed_value, _Default):
            setattr(options, opt, process_value(opt, val))
","if opt in ( ""ignore"" , ""select"" ) and passed_value :",137
"def _unlock_restarted_vms(self, pool_name):
    result = []
    for vm in await self.middleware.call(""vm.query"", [(""autostart"", ""="", True)]):
        for device in vm[""devices""]:
            if device[""dtype""] not in (""DISK"", ""RAW""):
                continue
            path = device[""attributes""].get(""path"")
            if not path:
                continue
            if path.startswith(f""/dev/zvol/{pool_name}/"") or path.startswith(
                f""/mnt/{pool_name}/""
            ):
                result.append(vm)
                break
    return result
","if device [ ""dtype"" ] not in ( ""DISK"" , ""RAW"" ) :",172
"def check_space(arr, task_id):
    for a in arr:
        if a.startswith(""hadoop jar""):
            found = False
            for x in shlex.split(a):
                if task_id in x:
                    found = True
            if not found:
                raise AssertionError
","if a . startswith ( ""hadoop jar"" ) :",86
"def clean(self):
    if self.instance:
        redirect_to = self.data.get(""redirect_to"", """")
        if redirect_to != """":
            lfs.core.utils.set_redirect_for(
                self.instance.get_absolute_url(), redirect_to
            )
        else:
            lfs.core.utils.remove_redirect_for(self.instance.get_absolute_url())
    if self.data.get(""active_base_price"") == str(CHOICES_YES):
        if self.data.get(""base_price_amount"", """") == """":
            self.errors[""base_price_amount""] = ErrorList(
                [_(u""This field is required."")]
            )
    return self.cleaned_data
","if self . data . get ( ""base_price_amount"" , """" ) == """" :",197
"def detect(get_page):
    retval = False
    for vector in WAF_ATTACK_VECTORS:
        page, headers, code = get_page(get=vector)
        retval = (
            re.search(
                r""\AAL[_-]?(SESS|LB)="", headers.get(HTTP_HEADER.SET_COOKIE, """"), re.I
            )
            is not None
        )
        if retval:
            break
    return retval
",if retval :,127
"def unloadOnePlugin(self, moduleOrFileName, verbose=False):
    moduleName = self.regularizeName(moduleOrFileName)
    if self.isLoaded(moduleName):
        if verbose:
            g.pr(""unloading"", moduleName)
        del self.loadedModules[moduleName]
    for tag in self.handlers:
        bunches = self.handlers.get(tag)
        bunches = [bunch for bunch in bunches if bunch.moduleName != moduleName]
        self.handlers[tag] = bunches
",if verbose :,136
"def __init__(self, **kw):
    util_schema.validate(
        instance=kw,
        schema=self.schema,
        cls=util_schema.CustomValidator,
        use_default=False,
        allow_default_none=True,
    )
    for prop in six.iterkeys(self.schema.get(""properties"", [])):
        value = kw.get(prop, None)
        # special handling for chain property to create the Node object
        if prop == ""chain"":
            nodes = []
            for node in value:
                ac_node = Node(**node)
                ac_node.validate()
                nodes.append(ac_node)
            value = nodes
        setattr(self, prop, value)
","if prop == ""chain"" :",195
"def initialize(self):
    for document in self.corpus:
        frequencies = {}
        for word in document:
            if word not in frequencies:
                frequencies[word] = 0
            frequencies[word] += 1
        self.f.append(frequencies)
        for word, freq in iteritems(frequencies):
            if word not in self.df:
                self.df[word] = 0
            self.df[word] += 1
    for word, freq in iteritems(self.df):
        self.idf[word] = math.log(self.corpus_size - freq + 0.5) - math.log(freq + 0.5)
",if word not in self . df :,170
"def get_child(self, name):
    if self.isdir:
        try:
            return self.data[name]
        except:
            if not self.case_sensitive:
                for childname, child in list(self.data.items()):
                    if childname.lower() == name.lower():
                        return child
            raise
",if not self . case_sensitive :,100
"def set_cover(channel, pixbuf):
    if self.channel == channel:
        if pixbuf is not None:
            self.imgCover.set_from_pixbuf(self.scale_pixbuf(pixbuf))
        if self.show_on_cover_load:
            self.main_window.show()
            self.show_on_cover_load = False
",if pixbuf is not None :,97
"def test_infer_shape_matrix(self):
    # Testing the infer_shape with a matrix.
    x = theano.tensor.matrix()
    for op in self.ops:
        if not op.return_inverse:
            continue
        if op.return_index:
            f = op(x)[2]
        else:
            f = op(x)[1]
        self._compile_and_check(
            [x],
            [f],
            [np.asarray(np.array([[2, 1], [3, 2], [2, 3]]), dtype=config.floatX)],
            self.op_class,
        )
",if op . return_index :,170
"def Filter(self, match=None, **_):
    """"""Filter the current expression.""""""
    arg = self.stack.pop(-1)
    # Filters can be specified as a comma separated list.
    for filter_name in match.group(1).split("",""):
        filter_object = ConfigFilter.classes_by_name.get(filter_name)
        if filter_object is None:
            raise FilterError(""Unknown filter function %r"" % filter_name)
        if not filter_object.sensitive_arg:
            logging.debug(""Applying filter %s for %s."", filter_name, arg)
        arg = filter_object().Filter(arg)
        precondition.AssertType(arg, Text)
    self.stack[-1] += arg
",if not filter_object . sensitive_arg :,180
"def enqueue_link(self, fuzzresult, link_url, parsed_link):
    # dir path
    if self.add_path:
        split_path = parsed_link.path.split(""/"")
        newpath = ""/"".join(split_path[:-1]) + ""/""
        self.queue_url(urljoin(fuzzresult.url, newpath))
    # file path
    new_link = urljoin(fuzzresult.url, link_url)
    if not self.regex_param or (
        self.regex_param and self.regex_param.search(new_link) is not None
    ):
        if self.enqueue_links:
            self.queue_url(new_link)
        self.add_result(""link"", ""New link found"", new_link)
",if self . enqueue_links :,188
"def old_save(self, *args, **kwargs):
    ""Override save to set Subscribers and send Notifications""
    original = None
    original_assigned = []
    if hasattr(self, ""instance""):
        try:
            original = Task.objects.get(pk=self.instance.id)
            original_assigned = list(original.assigned.all())
        except Task.DoesNotExist:
            pass
    instance = super(TaskForm, self).save(*args, **kwargs)
    if original:
        new_assigned = list(self.cleaned_data[""assigned""])
        if original_assigned != new_assigned:
            for assignee in new_assigned:
                self.instance.subscribers.add(assignee)
    return instance
",if original_assigned != new_assigned :,189
"def get_test_layer():
    layers = get_bb_var(""BBLAYERS"").split()
    testlayer = None
    for l in layers:
        if ""~"" in l:
            l = os.path.expanduser(l)
        if ""/meta-selftest"" in l and os.path.isdir(l):
            testlayer = l
            break
    return testlayer
","if ""/meta-selftest"" in l and os . path . isdir ( l ) :",98
"def readable(request):
    """"""Display a readable version of this url if we can""""""
    rdict = request.matchdict
    bid = rdict.get(""hash_id"", None)
    username = rdict.get(""username"", None)
    if bid:
        found = BmarkMgr.get_by_hash(bid, username=username)
        if found:
            return {
                ""bmark"": found,
                ""username"": username,
            }
        else:
            return HTTPNotFound()
",if found :,136
"def pythonpath(conanfile):
    python_path = conanfile.env.get(""PYTHONPATH"", None)
    if python_path:
        old_path = sys.path[:]
        if isinstance(python_path, list):
            sys.path.extend(python_path)
        else:
            sys.path.append(python_path)
        yield
        sys.path = old_path
    else:
        yield
","if isinstance ( python_path , list ) :",112
"def _validate(self):
    on_target_delete = None
    for cmd in self.val.commands:
        if isinstance(cmd, qlast.OnTargetDelete):
            if on_target_delete:
                raise errors.EdgeQLSyntaxError(
                    f""more than one 'on target delete' specification"",
                    context=cmd.context,
                )
            else:
                on_target_delete = cmd
",if on_target_delete :,119
"def _choose_instance(self, timeout_time):
    """"""Returns an Instance to handle a request or None if all are busy.""""""
    with self._condition:
        while time.time() < timeout_time and not self._quit_event.is_set():
            for inst in self._instances:
                if inst.can_accept_requests:
                    return inst
            else:
                inst = self._start_any_instance()
                if inst:
                    break
                self._condition.wait(timeout_time - time.time())
        else:
            return None
    if inst:
        inst.wait(timeout_time)
    return inst
",if inst :,180
"def get_identifiers(self):
    ids = []
    for entry in glob.glob(f""{self._base_path}/ctl-*""):
        ident = entry.split(""-"", 1)[-1]
        if ident.endswith(""ioctl""):
            continue
        if os.path.exists(os.path.join(entry, ""disk_octets.rrd"")):
            ids.append(ident)
    ids.sort(key=RRDBase._sort_ports)
    return ids
","if ident . endswith ( ""ioctl"" ) :",118
"def read_vocab_list(path, max_vocab_size=20000):
    vocab = {""<eos>"": 0, ""<unk>"": 1}
    with io.open(path, encoding=""utf-8"", errors=""ignore"") as f:
        for l in f:
            w = l.strip()
            if w not in vocab and w:
                vocab[w] = len(vocab)
            if len(vocab) >= max_vocab_size:
                break
    return vocab
",if w not in vocab and w :,125
"def n_import_from(self, node):
    relative_path_index = 0
    if self.version >= 2.5:
        if node[relative_path_index].pattr > 0:
            node[2].pattr = (""."" * node[relative_path_index].pattr) + node[2].pattr
        if self.version > 2.7:
            if isinstance(node[1].pattr, tuple):
                imports = node[1].pattr
                for pattr in imports:
                    node[1].pattr = pattr
                    self.default(node)
                return
            pass
    self.default(node)
","if isinstance ( node [ 1 ] . pattr , tuple ) :",170
"def get(self):
    """"""Returns a simple HTML for contact form""""""
    if self.user:
        user_info = models.User.get_by_id(long(self.user_id))
        if user_info.name or user_info.last_name:
            self.form.name.data = user_info.name + "" "" + user_info.last_name
        if user_info.email:
            self.form.email.data = user_info.email
    params = {""exception"": self.request.get(""exception"")}
    return self.render_template(""boilerplate_contact.html"", **params)
",if user_info . name or user_info . last_name :,155
"def task_management_menu(activation, request):
    """"""Available tasks actions.""""""
    actions = []
    if request.user.has_perm(activation.flow_class._meta.manage_permission_name):
        for transition in activation.get_available_transitions():
            if transition.can_proceed(activation):
                url = activation.flow_task.get_task_url(
                    activation.task,
                    transition.name,
                    user=request.user,
                    namespace=request.resolver_match.namespace,
                )
                if url:
                    actions.append((transition.name.replace(""_"", "" "").title(), url))
    return {""actions"": actions, ""request"": request}
",if url :,192
"def discover_misago_admin():
    for app in apps.get_app_configs():
        module = import_module(app.name)
        if not hasattr(module, ""admin""):
            continue
        admin_module = import_module(""%s.admin"" % app.name)
        if hasattr(admin_module, ""MisagoAdminExtension""):
            extension = getattr(admin_module, ""MisagoAdminExtension"")()
            if hasattr(extension, ""register_navigation_nodes""):
                extension.register_navigation_nodes(site)
            if hasattr(extension, ""register_urlpatterns""):
                extension.register_urlpatterns(urlpatterns)
","if hasattr ( extension , ""register_urlpatterns"" ) :",169
"def dequeue(self):
    with self.db(commit=True) as curs:
        curs.execute(
            ""select id, data from task where queue = ? ""
            ""order by priority desc, id limit 1"",
            (self.name,),
        )
        result = curs.fetchone()
        if result is not None:
            tid, data = result
            curs.execute(""delete from task where id = ?"", (tid,))
            if curs.rowcount == 1:
                return to_bytes(data)
",if curs . rowcount == 1 :,138
"def readHexStringFromStream(stream):
    stream.read(1)
    txt = """"
    x = b_("""")
    while True:
        tok = readNonWhitespace(stream)
        if not tok:
            # stream has truncated prematurely
            raise PdfStreamError(""Stream has ended unexpectedly"")
        if tok == b_("">""):
            break
        x += tok
        if len(x) == 2:
            txt += chr(int(x, base=16))
            x = b_("""")
    if len(x) == 1:
        x += b_(""0"")
    if len(x) == 2:
        txt += chr(int(x, base=16))
    return createStringObject(b_(txt))
",if not tok :,190
"def test_compute_gradient(self):
    for y, y_pred in zip(self.y_list, self.predict_list):
        lse_grad = self.lae_loss.compute_grad(y, y_pred)
        diff = y_pred - y
        if diff > consts.FLOAT_ZERO:
            grad = 1
        elif diff < consts.FLOAT_ZERO:
            grad = -1
        else:
            grad = 0
        self.assertTrue(np.fabs(lse_grad - grad) < consts.FLOAT_ZERO)
",elif diff < consts . FLOAT_ZERO :,145
"def request_get(request, key, default_value=None):
    if key in request.args:
        return request.args.get(key)
    elif key in request.form:
        return request.form.get(key)
    try:
        json_body = request.get_json(force=True, silent=True)
        if key in json_body:
            return json_body[key]
        else:
            return default_value
    except Exception:
        return default_value
",if key in json_body :,129
"def _getResourceData(self, jid, dataname):
    """"""Return specific jid's resource representation in internal format. Used internally.""""""
    if jid.find(""/"") + 1:
        jid, resource = jid.split(""/"", 1)
        if self._data[jid][""resources""].has_key(resource):
            return self._data[jid][""resources""][resource][dataname]
    elif self._data[jid][""resources""].keys():
        lastpri = -129
        for r in self._data[jid][""resources""].keys():
            if int(self._data[jid][""resources""][r][""priority""]) > lastpri:
                resource, lastpri = r, int(self._data[jid][""resources""][r][""priority""])
        return self._data[jid][""resources""][resource][dataname]
","if self . _data [ jid ] [ ""resources"" ] . has_key ( resource ) :",194
"def GetBoundingBoxMin(self):
    """"""Get the minimum bounding box.""""""
    x1, y1 = 10000, 10000
    x2, y2 = -10000, -10000
    for point in self._lineControlPoints:
        if point[0] < x1:
            x1 = point[0]
        if point[1] < y1:
            y1 = point[1]
        if point[0] > x2:
            x2 = point[0]
        if point[1] > y2:
            y2 = point[1]
    return x2 - x1, y2 - y1
",if point [ 1 ] > y2 :,158
"def produce_etag_headers(self, filename):
    """"""Produce a dict of curl headers containing etag headers from the download.""""""
    headers = {}
    # If the download file already exists, add some headers to the request
    # so we don't retrieve the content if it hasn't changed
    if os.path.exists(filename):
        self.existing_file_size = os.path.getsize(filename)
        etag = self.getxattr(self.xattr_etag)
        last_modified = self.getxattr(self.xattr_last_modified)
        if etag:
            headers[""If-None-Match""] = etag
        if last_modified:
            headers[""If-Modified-Since""] = last_modified
    return headers
",if etag :,182
"def _find_orientation_offset(self, header):
    (ifd_offset,) = self._unpack(""L"", header[4:])
    self.exif_buffer.seek(ifd_offset)
    # Read tag directory
    for _ in range(self._unpack(""H"", self.exif_buffer.read(2))[0]):
        # Each tag is 12 bytes. HHL4s = tag, type, count, data
        # Read tag and ignore the rest
        (tag,) = self._unpack(""H10x"", self.exif_buffer.read(12))
        if tag == 0x0112:  # Orientation tag
            self._offset = (
                self.exif_buffer.tell() - 4
            )  # Back 4 bytes to the start of data
            break
",if tag == 0x0112 :,197
"def _start(self):
    try:
        await self.fire_event(""pre_request"")
    except AbortEvent:
        self.logger.debug(""Abort request %s"", self.request)
    else:
        if self._request is not None:
            try:
                self.start_request()
            except Exception as exc:
                self.finished(exc=exc)
",if self . _request is not None :,103
"def buildQueryRE(queryText, caseSensitive, wholeWord):
    ""returns a RegEx pattern for searching for the given queryText""
    # word detection etc. cannot be done on an encoding-less string:
    assert type(queryText) == unicode
    pattern = re.escape(queryText)
    if wholeWord:
        if re.search(""^\w"", queryText, re.UNICODE):
            pattern = ""\\b"" + pattern
        if re.search(""\w$"", queryText, re.UNICODE):
            pattern = pattern + ""\\b""
    flags = re.UNICODE
    if not (caseSensitive):
        flags |= re.IGNORECASE
    return re.compile(pattern, flags)
","if re . search ( ""^\w"" , queryText , re . UNICODE ) :",166
"def filter(callbackfn):
    array = this.to_object()
    arr_len = array.get(""length"").to_uint32()
    if not callbackfn.is_callable():
        raise this.MakeError(""TypeError"", ""callbackfn must be a function"")
    T = arguments[1]
    res = []
    k = 0
    while k < arr_len:
        if array.has_property(str(k)):
            kValue = array.get(str(k))
            if callbackfn.call(T, (kValue, this.Js(k), array)).to_boolean().value:
                res.append(kValue)
        k += 1
    return res  # converted to js array automatically
",if array . has_property ( str ( k ) ) :,179
"def action(self, params):
    if len(params) < 1:
        return CommandsResponse(STATUS_ERROR, ""Not enough params"")
    else:
        vrf_name = params[0]
        if len(params) == 2:
            vrf_rf = params[1]
        else:
            vrf_rf = ""ipv4""
        from ryu.services.protocols.bgp.operator.internal_api import WrongParamError
        try:
            return CommandsResponse(
                STATUS_OK, self.api.count_single_vrf_routes(vrf_name, vrf_rf)
            )
        except WrongParamError as e:
            return WrongParamResp(e)
",if len ( params ) == 2 :,187
"def __init__(self, layers):
    super(Add, self).__init__()
    self.layer_names = []
    self.layers = layers
    for i, layer in enumerate(self.layers):
        if layer.parent is None:
            if i == 0:
                layer.parent = ""input""
            else:
                layer.parent = layers[i - 1].name
        if hasattr(layer, ""name""):
            name = layer.name
        else:
            name = layer.__class__.__name__ + str(i)
            layer.name = name
        self.layer_names.append(name)
","if hasattr ( layer , ""name"" ) :",165
"def _grouping_intervals(grouping):
    last_interval = None
    for interval in grouping:
        # if grouping is -1, we are done
        if interval == CHAR_MAX:
            return
        # 0: re-use last group ad infinitum
        if interval == 0:
            if last_interval is None:
                raise ValueError(""invalid grouping"")
            while True:
                yield last_interval
        yield interval
        last_interval = interval
",if last_interval is None :,124
"def infer_expected_xp_and_device(self, x):
    xp = backend.get_array_module(x)
    if xp is np:
        return xp, None
    elif xp is cuda.cupy:
        return xp, x.device
    elif xp is chainerx:
        backend_name = x.device.backend.name
        if backend_name == ""native"":
            return np, None
        elif backend_name == ""cuda"":
            return cuda.cupy, cuda.cupy.cuda.Device(x.device.index)
    assert False
","if backend_name == ""native"" :",142
"def _escape_attrib(text):
    # escape attribute value
    try:
        if ""&"" in text:
            text = text.replace(""&"", ""&amp;"")
        if ""<"" in text:
            text = text.replace(""<"", ""&lt;"")
        if "">"" in text:
            text = text.replace("">"", ""&gt;"")
        if '""' in text:
            text = text.replace('""', ""&quot;"")
        if ""\n"" in text:
            text = text.replace(""\n"", ""&#10;"")
        return text
    except (TypeError, AttributeError):  # pragma: no cover
        _raise_serialization_error(text)
","if "">"" in text :",160
"def get_block_id_at_height(store, height, descendant_id):
    if height is None:
        return None
    while True:
        block = store._load_block(descendant_id)
        if block[""height""] == height:
            return descendant_id
        descendant_id = block[
            ""search_id""
            if util.get_search_height(block[""height""]) >= height
            else ""prev_id""
        ]
","if util . get_search_height ( block [ ""height"" ] ) >= height",122
"def train(config, checkpoint_dir=None):
    if checkpoint_dir:
        assert os.path.exists(checkpoint_dir)
    for step in range(10):
        if step % 3 == 0:
            with tune.checkpoint_dir(step=step) as checkpoint_dir:
                path = os.path.join(checkpoint_dir, ""checkpoint"")
                with open(path, ""w"") as f:
                    f.write(json.dumps({""step"": step}))
        tune.report(test=step)
",if step % 3 == 0 :,137
"def onMinimize(self, sender):
    if self._runDialogListener(""onMinimize"") is False:
        return
    widget = self.child
    if widget is not None:
        if widget.isVisible():
            widget.setVisible(False)
            self.setHeight("""")
            self.setWidth("""")
            if self._maximized:
                self._minimized = self._maximized
                self._toggleMaximize()
            else:
                self._minimized = None
        else:
            if self._minimized is not None:
                self._toggleMaximize()
            widget.setVisible(True)
",if self . _minimized is not None :,171
"def apply_transformation(self, ti: TransformationInput) -> Transformation:
    # Insert fragments after the last line.
    if ti.lineno == ti.document.line_count - 1:
        buffer = ti.buffer_control.buffer
        if buffer.suggestion and ti.document.is_cursor_at_the_end:
            suggestion = buffer.suggestion.text
        else:
            suggestion = """"
        return Transformation(fragments=ti.fragments + [(self.style, suggestion)])
    else:
        return Transformation(fragments=ti.fragments)
",if buffer . suggestion and ti . document . is_cursor_at_the_end :,139
"def get_measurements(self, pipeline, object_name, category):
    if object_name == IMAGE and category == C_COUNT:
        return [self.object_name.value]
    elif object_name == self.object_name:
        if category == C_LOCATION:
            return [
                FTR_CENTER_X,
                FTR_CENTER_Y,
            ]
        elif category == C_NUMBER:
            return [FTR_OBJECT_NUMBER]
        elif category == C_WORMS:
            return [F_ANGLE]
    return []
",if category == C_LOCATION :,151
"def traverse(tensors):
    """"""traverse all ops to find attached workload""""""
    for t in tensors:
        op = t.op
        if ""workload"" in op.attrs:
            return args_to_workload(op.attrs[""workload""])
        wkl = traverse(op.input_tensors)
        if wkl:
            return wkl
    return None
","if ""workload"" in op . attrs :",97
"def _pack(converter, node: Any, inputs: List[str]) -> Any:
    final_inputs = []
    for x_in in inputs:
        input_c = converter.outputs[x_in]
        if isinstance(input_c, tf.compat.v1.NodeDef):
            final_inputs.append(_nodef_to_private_pond(converter, input_c))
        else:
            final_inputs.append(input_c)
    return converter.protocol.stack(final_inputs, axis=node.attr[""axis""].i)
","if isinstance ( input_c , tf . compat . v1 . NodeDef ) :",139
"def __init__(self, instance=None, data=empty, **kwargs):
    context = kwargs.get(""context"", {})
    if ""product"" in context:
        instance = self.get_instance(context, data, kwargs)
        if data is not empty and ""quantity"" in data:
            quantity = self.fields[""quantity""].to_internal_value(data[""quantity""])
        else:
            quantity = self.fields[""quantity""].default
        instance.setdefault(""quantity"", quantity)
        super().__init__(instance, data, context=context)
    else:
        super().__init__(instance, data, **kwargs)
","if data is not empty and ""quantity"" in data :",154
"def serialize(self, value):
    if value is not None:
        try:
            iter(value)
        except TypeError:
            value = [value]
        if len(value):
            return [self.element_serialize(val) for val in sorted(value)]
    return None
",if len ( value ) :,77
"def remove_cloner_curve(self, obj_index):
    # opportunity to remove the .cloner.
    if self.selected_mode == ""Duplicate"":
        curve_name = f""{self.basedata_name}.cloner.{obj_index:04d}""
        cu = bpy.data.curves.get(curve_name)
        if cu:
            bpy.data.curves.remove(cu)
",if cu :,107
"def update_advance_paid(self):
    advance_paid = frappe._dict()
    for d in self.get(""accounts""):
        if d.is_advance:
            if d.reference_type in (
                ""Sales Order"",
                ""Purchase Order"",
                ""Employee Advance"",
            ):
                advance_paid.setdefault(d.reference_type, []).append(d.reference_name)
    for voucher_type, order_list in iteritems(advance_paid):
        for voucher_no in list(set(order_list)):
            frappe.get_doc(voucher_type, voucher_no).set_total_advance_paid()
",if d . is_advance :,177
"def handle(self, msg):
    self._mic.send(msg)
    for calculate_seed, make_delegate, dict in self._delegate_records:
        id = calculate_seed(msg)
        if id is None:
            continue
        elif isinstance(id, collections.Hashable):
            if id not in dict or not dict[id].is_alive():
                d = make_delegate((self, msg, id))
                d = self._ensure_startable(d)
                dict[id] = d
                dict[id].start()
        else:
            d = make_delegate((self, msg, id))
            d = self._ensure_startable(d)
            d.start()
",if id not in dict or not dict [ id ] . is_alive ( ) :,192
"def _get_default_factory(self, attribute_name: str) -> Any:
    if hasattr(self, attribute_name):
        if str(getattr(self, attribute_name)).startswith(""${""):
            return str(getattr(self, attribute_name))
        elif str(self.__dataclass_fields__[attribute_name].default).startswith(""${""):
            return str(self.__dataclass_fields__[attribute_name].default)
        elif (
            getattr(self, attribute_name)
            != self.__dataclass_fields__[attribute_name].default_factory()
        ):
            return getattr(self, attribute_name)
    return self.__dataclass_fields__[attribute_name].default_factory()
","elif str ( self . __dataclass_fields__ [ attribute_name ] . default ) . startswith ( ""${"" ) :",173
"def showMenu(self, show):
    if show:
        if self.canvas.menu is None:
            self.canvas.menu = Menu(self.canvas, tearoff=0)
            self.canvas.menu.add_command(label=""delete"", command=self._delete)
            self.canvas.menu.bind(""<FocusOut>"", lambda e: self.canvas.menu.unpost())
        self._bindMenu()
    else:
        # need to go through and unbind...
        pass
",if self . canvas . menu is None :,127
"def __init__(self, db, where=None):
    self._db = db
    self._tables = []
    self.filters = []
    if hasattr(where, ""get_all""):
        self.where = where
        self._tables.insert(0, where.get_all)
    elif hasattr(where, ""get_one"") and isinstance(where.get_one, QueryException):
        self.where = where.get_one
    else:
        # find out which tables are involved
        if isinstance(where, Query):
            self.filters = where.left
        self.where = where
        self._tables = [field._tablename for (field, op, val) in self.filters]
","if isinstance ( where , Query ) :",174
"def main():
    try:
        from wsgiref.simple_server import make_server
        from wsgiref.validate import validator
        if port[0] == 0:
            port[0] = get_open_port()
        wsgi_application = WsgiApplication(soap11_application)
        server = make_server(host, port[0], validator(wsgi_application))
        logger.info(""Starting interop server at %s:%s."" % (""0.0.0.0"", port[0]))
        logger.info(""WSDL is at: /?wsdl"")
        server.serve_forever()
    except ImportError:
        print(""Error: example server code requires Python >= 2.5"")
",if port [ 0 ] == 0 :,177
"def try_adjust_widgets(self):
    if hasattr(self.parent, ""adjust_widgets""):
        self.parent.adjust_widgets()
    if hasattr(self.parent, ""parentApp""):
        if hasattr(self.parent.parentApp, ""_internal_adjust_widgets""):
            self.parent.parentApp._internal_adjust_widgets()
        if hasattr(self.parent.parentApp, ""adjust_widgets""):
            self.parent.parentApp.adjust_widgets()
","if hasattr ( self . parent . parentApp , ""_internal_adjust_widgets"" ) :",118
"def copy_file_replace_line(
    orig_file: Path, new_file: Path, line_re: str, new_line: str
) -> None:
    old_version_fh = orig_file.open(""r"")
    new_version_fh = new_file.open(""w"")
    for line in old_version_fh:
        if re.search(line_re, line):
            new_version_fh.write(new_line + ""\n"")
        else:
            new_version_fh.write(line)
    old_version_fh.close()
    new_version_fh.close()
","if re . search ( line_re , line ) :",154
"def _protoc_plugin_parameters(self, language):
    """"""Return a tuple of (plugin path, vars) used as parameters for ninja build.""""""
    path, vars = """", {}
    for p in self.attr[""protoc_plugins""]:
        if language in p.code_generation:
            path = p.path
            flag = p.protoc_plugin_flag(self.build_dir)
            vars = {""protoc%spluginflags"" % language: flag}
            break
    return path, vars
",if language in p . code_generation :,125
"def scan_page(self, address_space, page_offset, fullpage=False):
    """"""Runs through patchers for a single page""""""
    if fullpage:
        pagedata = address_space.read(page_offset, PAGESIZE)
    for patcher in self.patchers:
        for offset, data in patcher.get_constraints():
            if fullpage:
                testdata = pagedata[offset : offset + len(data)]
            else:
                testdata = address_space.read(page_offset + offset, len(data))
            if data != testdata:
                break
        else:
            yield patcher
",if fullpage :,166
"def OnLeftDClick(self, event):
    pt = event.GetPosition()
    item, flags = self.tree.HitTest(pt)
    if item:
        self.log.WriteText(""OnLeftDClick: %s\n"" % self.tree.GetItemText(item))
        parent = self.tree.GetItemParent(item)
        if parent.IsOk():
            self.tree.SortChildren(parent)
    event.Skip()
",if parent . IsOk ( ) :,111
"def drop_pathlist(self, pathlist):
    """"""Drop path list""""""
    if pathlist:
        files = [""r'%s'"" % path for path in pathlist]
        if len(files) == 1:
            text = files[0]
        else:
            text = ""["" + "", "".join(files) + ""]""
        if self.new_input_line:
            self.on_new_line()
        self.insert_text(text)
        self.setFocus()
",if len ( files ) == 1 :,126
"def func_set_exporter_funcs_opset_yaml(func_set):
    if len(list(func_set)[0].split(""@"")) == 1:
        yaml_data = {}
        for nnabla_func, impl_funcs in _onnx_func_info.items():
            if nnabla_func in func_set:
                yaml_data[nnabla_func] = impl_funcs
        return yaml.dump(yaml_data, default_flow_style=False)
    else:
        return yaml.dump(list(func_set), default_flow_style=False)
",if nnabla_func in func_set :,150
"def object_hook(obj):
    obj_len = len(obj)
    if obj_len == 1:
        if ""$date"" in obj:
            return datetime.fromtimestamp(
                obj[""$date""] / 1000, tz=timezone.utc
            ) + timedelta(milliseconds=obj[""$date""] % 1000)
        if ""$time"" in obj:
            return time(*[int(i) for i in obj[""$time""].split("":"")])
    if obj_len == 2 and ""$type"" in obj and ""$value"" in obj:
        if obj[""$type""] == ""date"":
            return date(*[int(i) for i in obj[""$value""].split(""-"")])
    return obj
","if obj [ ""$type"" ] == ""date"" :",174
"def start(self, para=None, callback=None):
    if not self.load():
        return
    if para != None or self.show():
        if para == None:
            para = self.para
        win = WidgetsManager.getref(""Macros Recorder"")
        if win != None:
            win.write(""{}>{}"".format(self.title, para))
        if self.asyn and IPy.uimode() != ""no"":
            threading.Thread(target=self.runasyn, args=(para, callback)).start()
        else:
            self.runasyn(para, callback)
",if win != None :,159
"def user(self):
    if not self._conan_user:
        _env_username = os.getenv(""CONAN_USERNAME"")
        conan_v2_error(
            ""Environment variable 'CONAN_USERNAME' is deprecated"", _env_username
        )
        self._conan_user = _env_username or self.default_user
        if not self._conan_user:
            raise ConanException(""user not defined, but self.user is used in conanfile"")
    return self._conan_user
",if not self . _conan_user :,134
"def _get_vars(cls, func):
    # log.debug(""Getting vars for %s"", func)
    params = inspect.signature(func).parameters.copy()
    args = {}
    # log.debug(""Got %s"", params)
    for name, param in params.items():
        # log.debug(""Checking arg %s, type %s"", name, param.kind)
        if param.kind is param.POSITIONAL_OR_KEYWORD and param.default is None:
            # log.debug(""Using var %s"", name)
            args[name] = _get_variable(name)
            # log.debug(""Collected var for arg '%s': %s"", name, args[name])
    return args
",if param . kind is param . POSITIONAL_OR_KEYWORD and param . default is None :,176
"def parts(self):
    klass = self.__class__
    this = list()
    for token in self:
        if token.startswith_fws():
            if this:
                yield this[0] if len(this) == 1 else klass(this)
                this.clear()
        end_ws = token.pop_trailing_ws()
        this.append(token)
        if end_ws:
            yield klass(this)
            this = [end_ws]
    if this:
        yield this[0] if len(this) == 1 else klass(this)
",if end_ws :,153
"def start_fileoutput(self):
    """"""Start output to configured file.""""""
    path = os.path.dirname(self.filename)
    try:
        if path and not os.path.isdir(path):
            os.makedirs(path)
        self.fd = self.create_fd()
        self.close_fd = True
    except IOError:
        msg = sys.exc_info()[1]
        log.warn(
            LOG_CHECK,
            ""Could not open file %r for writing: %s\n"" ""Disabling log output of %s"",
            self.filename,
            msg,
            self,
        )
        self.fd = dummy.Dummy()
        self.is_active = False
    self.filename = None
",if path and not os . path . isdir ( path ) :,196
"def worksheet_id(self, value):
    if self._worksheet:
        if self._worksheet.id == value:
            return
        else:
            raise InvalidArgumentValue(
                ""This range already has a worksheet with different id set.""
            )
    self._worksheet_id = value
",if self . _worksheet . id == value :,82
"def _sanity_check(self, kind, triplets):
    route_id = self.data.get(""route_id"", [None])[0]
    if route_id or [
        k
        for k in self.data.keys()
        if k[:5] in (""route"", ""smtp-"", ""sourc"", ""secur"", ""local"")
    ]:
        if len(triplets) > 1 or kind != ""profile"":
            raise ValueError(
                ""Can only configure detailed settings "" ""for one profile at a time""
            )
","if len ( triplets ) > 1 or kind != ""profile"" :",138
"def _process_property_change(self, msg):
    msg = super(Select, self)._process_property_change(msg)
    if ""value"" in msg:
        if not self.values:
            pass
        elif msg[""value""] is None:
            msg[""value""] = self.values[0]
        else:
            if isIn(msg[""value""], self.unicode_values):
                idx = indexOf(msg[""value""], self.unicode_values)
            else:
                idx = indexOf(msg[""value""], self.labels)
            msg[""value""] = self._items[self.labels[idx]]
    msg.pop(""options"", None)
    return msg
",if not self . values :,180
"def emit(self, record):
    msg = record.getMessage()
    ###
    if record.exc_info:
        _type, value, tback = record.exc_info
        tback_text = """".join(traceback.format_exception(_type, value, tback))
        if msg:
            msg += ""\n""
        msg += tback_text
    ###
    self.tktext.insert(
        ""end"",
        msg + ""\n"",
        record.levelname,
    )
",if msg :,129
"def _get_pip_index_urls(sources):
    index_urls = []
    trusted_hosts = []
    for source in sources:
        url = source.get(""url"")
        if not url:
            continue
        index_urls.append(url)
        if source.get(""verify_ssl"", True):
            continue
        host = six.moves.urllib.parse.urlparse(source[""url""]).hostname
        trusted_hosts.append(host)
    return index_urls, trusted_hosts
","if source . get ( ""verify_ssl"" , True ) :",129
"def _is_binary(fname, limit=80):
    try:
        with open(fname, ""rb"") as f:
            for i in range(limit):
                char = f.read(1)
                if char == b""\0"":
                    return True
                if char == b""\n"":
                    return False
                if char == b"""":
                    return
    except OSError as e:
        if xp.ON_WINDOWS and is_app_execution_alias(fname):
            return True
        raise e
    return False
","if char == b""\n"" :",154
"def tearDown(self):
    exc, _, _ = sys.exc_info()
    if exc:
        try:
            if hasattr(self, ""obj"") and isinstance(self.obj, SelfDiagnosable):
                diags = self.obj.get_error_diagnostics()
                if diags:
                    for line in diags:
                        ROOT_LOGGER.info(line)
        except BaseException:
            pass
    if self.captured_logger:
        self.captured_logger.removeHandler(self.log_recorder)
        self.log_recorder.close()
    sys.stdout = self.stdout_backup
    super(BZTestCase, self).tearDown()
",if diags :,179
"def _disconnect(self, sync):
    if self._connection:
        if sync:
            try:
                self._connection.send_all()
                self._connection.fetch_all()
            except (WorkspaceError, ServiceUnavailable):
                pass
        if self._connection:
            self._connection.in_use = False
            self._connection = None
        self._connection_access_mode = None
",if self . _connection :,115
"def _recursive_process(self):
    super(RecursiveObjectDownwardsVisitor, self)._recursive_process()
    while self._new_for_visit:
        func_ea, arg_idx = self._new_for_visit.pop()
        if helper.is_imported_ea(func_ea):
            continue
        cfunc = helper.decompile_function(func_ea)
        if cfunc:
            assert arg_idx < len(cfunc.get_lvars()), ""Wrong argument at func {}"".format(
                to_hex(func_ea)
            )
            obj = VariableObject(cfunc.get_lvars()[arg_idx], arg_idx)
            self.prepare_new_scan(cfunc, arg_idx, obj)
            self._recursive_process()
",if cfunc :,199
"def to_dict(self) -> JSONDict:
    data = dict()
    for key in iter(self.__dict__):
        if key == ""bot"" or key.startswith(""_""):
            continue
        value = self.__dict__[key]
        if value is not None:
            if hasattr(value, ""to_dict""):
                data[key] = value.to_dict()
            else:
                data[key] = value
    if data.get(""from_user""):
        data[""from""] = data.pop(""from_user"", None)
    return data
","if key == ""bot"" or key . startswith ( ""_"" ) :",148
"def get_data(self, path, prefix=""""):
    item = self.store[path]
    path = ""{}/{}"".format(prefix, path)
    keys = [i for i in item.keys()]
    data = {""path"": path}
    # print(path)
    for k in keys:
        if not isinstance(item[k], h5py.Group):
            dataset = np.array(item[k].value)
            if type(dataset) is np.ndarray:
                if dataset.size != 0:
                    if type(dataset[0]) is np.bytes_:
                        dataset = [a.decode(""ascii"") for a in dataset]
            data.update({k: dataset})
    return data
","if not isinstance ( item [ k ] , h5py . Group ) :",183
"def _macros_of_type(root, type, el_func):
    macros_el = root.find(""macros"")
    macro_dict = {}
    if macros_el is not None:
        macro_els = macros_el.findall(""macro"")
        filtered_els = [
            (macro_el.get(""name""), el_func(macro_el))
            for macro_el in macro_els
            if macro_el.get(""type"") == type
        ]
        macro_dict = dict(filtered_els)
    return macro_dict
","if macro_el . get ( ""type"" ) == type",140
"def get_referrers(self):
    d = []
    for o in gc.get_referrers(self.obj):
        name = None
        if isinstance(o, dict):
            name = web.dictfind(o, self.obj)
            for r in gc.get_referrers(o):
                if getattr(r, ""__dict__"", None) is o:
                    o = r
                    break
        elif isinstance(o, dict):  # other dict types
            name = web.dictfind(o, self.obj)
        if not isinstance(name, six.string_types):
            name = None
        d.append(Object(o, name))
    return d
","if isinstance ( o , dict ) :",187
"def MakeWidthArray(fm):
    # Make character width array
    s = ""{\n\t""
    cw = fm[""Widths""]
    for i in xrange(0, 256):
        if chr(i) == ""'"":
            s += ""'\\''""
        elif chr(i) == ""\\"":
            s += ""'\\\\'""
        elif i >= 32 and i <= 126:
            s += ""'"" + chr(i) + ""'""
        else:
            s += ""chr(%d)"" % i
        s += "":"" + fm[""Widths""][i]
        if i < 255:
            s += "",""
        if (i + 1) % 22 == 0:
            s += ""\n\t""
    s += ""}""
    return s
","elif chr ( i ) == ""\\"" :",192
"def getLatestFile(self):
    highestNsp = None
    highestNsx = None
    for nsp in self.getFiles():
        try:
            if nsp.path.endswith("".nsx""):
                if not highestNsx or int(nsp.version) > int(highestNsx.version):
                    highestNsx = nsp
            else:
                if not highestNsp or int(nsp.version) > int(highestNsp.version):
                    highestNsp = nsp
        except BaseException:
            pass
    return highestNsp or highestNsx
",if not highestNsx or int ( nsp . version ) > int ( highestNsx . version ) :,152
"def _check_integrity(self) -> bool:
    # Allow original archive to be deleted (zip). Only need the extracted images
    all_files = self.FILE_LIST.copy()
    all_files.append(self.ANNOTATIONS_FILE)
    for (_, md5, filename) in all_files:
        file, ext = os.path.splitext(filename)
        extracted_dir = os.path.join(self.root, file)
        if not os.path.exists(extracted_dir):
            return False
    return True
",if not os . path . exists ( extracted_dir ) :,132
"def load_core(self):
    for filename in os.listdir(self.path):
        if filename != ""__init__.py"" and filename.endswith("".py""):
            try:
                name = filename.replace("".py"", """")
                mod = load_python_module(name, self.path)
                self._load_cmd_from(mod)
            except:
                warnings.warn(
                    ""!! Warning: could not load core command file "" + filename,
                    RuntimeWarning,
                )
","if filename != ""__init__.py"" and filename . endswith ( "".py"" ) :",142
"def _make_dataset(key, data, size):
    if isinstance(data, chainer.get_array_types()):
        if key is None:
            key = ""_{}"".format(id(data))
        return _Array(key, data)
    elif isinstance(data, list):
        if key is None:
            key = ""_{}"".format(id(data))
        return _List(key, data)
    elif callable(data):
        if key is None:
            raise ValueError(""key(s) must be specified for callable"")
        if size is None:
            raise ValueError(""size must be specified for callable"")
        return _Index(size).transform(key, data)
",if size is None :,173
"def main_loop(self) -> None:
    while True:
        try:
            message = self.control.get(block=False)
        except Empty:
            message = None
        if message == ""ABORT"":
            self.log.info(""Got ABORT message, main_loop exiting..."")
            break
        if not self.all_watchers_running():
            self.log.error(""One or more watcher died, committing suicide!"")
            sys.exit(1)
        if self.all_workers_dead():
            self.log.error(""All workers have died, committing suicide!"")
            sys.exit(1)
        self.check_and_start_workers()
        time.sleep(0.1)
",if not self . all_watchers_running ( ) :,197
"def execute_map(cls, ctx, op):
    (x,), device_id, xp = as_same_device(
        [ctx[c.key] for c in op.inputs], op.device, ret_extra=True
    )
    axis = cls.get_arg_axis(op.axis, op.inputs[0].ndim)
    keepdims = op.keepdims
    with device(device_id):
        nz = xp.count_nonzero(x, axis=axis)
        if keepdims:
            slcs = [slice(None)] * op.inputs[0].ndim
            for ax in op.axis:
                slcs[ax] = np.newaxis
            nz = xp.asarray(nz)[tuple(slcs)]
        ctx[op.outputs[0].key] = nz
",if keepdims :,195
"def setfilter(self, f):
    filter_exp = create_string_buffer(f.encode(""ascii""))
    if pcap_compile(self.pcap, byref(self.bpf_program), filter_exp, 0, -1) == -1:
        error(""Could not compile filter expression %s"" % f)
        return False
    else:
        if pcap_setfilter(self.pcap, byref(self.bpf_program)) == -1:
            error(""Could not install filter %s"" % f)
            return False
    return True
","if pcap_setfilter ( self . pcap , byref ( self . bpf_program ) ) == - 1 :",141
"def find_parent_for_new_to(self, pos):
    """"""Figure out the parent object for something at 'pos'.""""""
    for children in self._editable_children:
        if children._start <= pos < children._end:
            return children.find_parent_for_new_to(pos)
        if children._start == pos and pos == children._end:
            return children.find_parent_for_new_to(pos)
    return self
",if children . _start == pos and pos == children . _end :,113
"def process_events(self, events):
    for event in events:
        key = (event.ident, event.filter)
        if event.ident == self._force_wakeup_fd:
            self._force_wakeup.drain()
            continue
        receiver = self._registered[key]
        if event.flags & select.KQ_EV_ONESHOT:
            del self._registered[key]
        if type(receiver) is _core.Task:
            _core.reschedule(receiver, outcome.Value(event))
        else:
            receiver.put_nowait(event)
",if event . ident == self . _force_wakeup_fd :,154
"def test_tag(artifact_obj, sagemaker_session):
    tag = {""Key"": ""foo"", ""Value"": ""bar""}
    artifact_obj.set_tag(tag)
    while True:
        actual_tags = sagemaker_session.sagemaker_client.list_tags(
            ResourceArn=artifact_obj.artifact_arn
        )[""Tags""]
        if actual_tags:
            break
        time.sleep(5)
    # When sagemaker-client-config endpoint-url is passed as argument to hit some endpoints,
    # length of actual tags will be greater than 1
    assert len(actual_tags) > 0
    assert actual_tags[0] == tag
",if actual_tags :,170
"def initialize(self) -> None:
    """"""Move the API keys from cog stored config to core bot config if they exist.""""""
    imgur_token = await self.config.imgur_client_id()
    if imgur_token is not None:
        if not await self.bot.get_shared_api_tokens(""imgur""):
            await self.bot.set_shared_api_tokens(""imgur"", client_id=imgur_token)
        await self.config.imgur_client_id.clear()
","if not await self . bot . get_shared_api_tokens ( ""imgur"" ) :",126
"def _sorted_layers(self, structure, top_layer_id):
    """"""Return the image layers sorted""""""
    sorted_layers = []
    next_layer = top_layer_id
    while next_layer:
        sorted_layers.append(next_layer)
        if ""json"" not in structure[""repolayers""][next_layer]:  # v2
            break
        if ""parent"" not in structure[""repolayers""][next_layer][""json""]:
            break
        next_layer = structure[""repolayers""][next_layer][""json""][""parent""]
        if not next_layer:
            break
    return sorted_layers
",if not next_layer :,162
"def __init__(self, bounds, channel_axis, preprocess=None):
    assert len(bounds) == 2
    assert channel_axis in [0, 1, 2, 3]
    self._bounds = bounds
    self._channel_axis = channel_axis
    # Make self._preprocess to be (0,1) if possible, so that don't need
    # to do substract or divide.
    if preprocess is not None:
        sub, div = np.array(preprocess)
        if not np.any(sub):
            sub = 0
        if np.all(div == 1):
            div = 1
        assert (div is None) or np.all(div)
        self._preprocess = (sub, div)
    else:
        self._preprocess = (0, 1)
",if not np . any ( sub ) :,194
"def unpickle(fname):
    """"""Load pickled object from `fname`""""""
    with smart_open(fname, ""rb"") as f:
        # Because of loading from S3 load can't be used (missing readline in smart_open)
        if sys.version_info > (3, 0):
            return _pickle.load(f, encoding=""latin1"")
        else:
            return _pickle.loads(f.read())
","if sys . version_info > ( 3 , 0 ) :",105
"def get_new_setup_py_lines():
    global version
    with open(""setup.py"", ""r"") as sf:
        current_setup = sf.readlines()
    for line in current_setup:
        if line.startswith(""VERSION = ""):
            major, minor = re.findall(r""VERSION = '(\d+)\.(\d+)'"", line)[0]
            version = ""{}.{}"".format(major, int(minor) + 1)
            yield ""VERSION = '{}'\n"".format(version)
        else:
            yield line
","if line . startswith ( ""VERSION = "" ) :",136
"def make_buffers_dict(observables):
    """"""Makes observable states in a dict.""""""
    # Use `type(observables)` so that our output structure respects the
    # original dict subclass (e.g. OrderedDict).
    out_dict = type(observables)()
    for key, value in six.iteritems(observables):
        if value.enabled:
            out_dict[key] = _EnabledObservable(
                value, physics, random_state, self._strip_singleton_buffer_dim
            )
    return out_dict
",if value . enabled :,137
"def _callFUT(self, config_file, global_conf=None, _loader=None):
    import pyramid.paster
    old_loader = pyramid.paster.get_config_loader
    try:
        if _loader is not None:
            pyramid.paster.get_config_loader = _loader
        return pyramid.paster.setup_logging(config_file, global_conf)
    finally:
        pyramid.paster.get_config_loader = old_loader
",if _loader is not None :,125
"def _csv(self, match=None, dump=None):
    if dump is None:
        dump = self._dump(match)
    for record in dump:
        row = []
        for field in record:
            if isinstance(field, int):
                row.append(""%i"" % field)
            elif field is None:
                row.append("""")
            else:
                row.append(""'%s'"" % field)
        yield "","".join(row)
","if isinstance ( field , int ) :",126
"def preprocess_envs(args_envs):
    envs_map = {}
    for item in args_envs:
        i = item.find("":"")
        if i != -1:
            key = item[:i]
            val = item[i + 1 :]
        envs_map[key] = val
    return envs_map
",if i != - 1 :,83
"def _get_most_recent_update(self, versions):
    recent = None
    for version in versions:
        updated = datetime.datetime.strptime(version[""updated""], ""%Y-%m-%dT%H:%M:%SZ"")
        if not recent:
            recent = updated
        elif updated > recent:
            recent = updated
    return recent.strftime(""%Y-%m-%dT%H:%M:%SZ"")
",if not recent :,103
"def _to_string_infix(self, ostream, idx, verbose):
    if verbose:
        ostream.write("" , "")
    else:
        if type(self._args[idx]) is _NegationExpression:
            ostream.write("" - "")
            return True
        else:
            ostream.write("" + "")
",if type ( self . _args [ idx ] ) is _NegationExpression :,90
"def __init__(self, bert, num_classes=2, dropout=0.0, prefix=None, params=None):
    super(BERTClassifier, self).__init__(prefix=prefix, params=params)
    self.bert = bert
    with self.name_scope():
        self.classifier = nn.HybridSequential(prefix=prefix)
        if dropout:
            self.classifier.add(nn.Dropout(rate=dropout))
        self.classifier.add(nn.Dense(units=num_classes))
",if dropout :,124
"def __iter__(self):
    for i, field in enumerate(self.fields):
        if field in self.readonly_fields:
            yield AdminReadonlyField(
                self.form, field, is_first=(i == 0), model_admin=self.model_admin
            )
        else:
            yield AdminField(self.form, field, is_first=(i == 0))
",if field in self . readonly_fields :,102
"def boolean(value):
    if isinstance(value, str):
        v = value.lower()
        if v in (""1"", ""yes"", ""true"", ""on""):
            return True
        if v in (""0"", ""no"", ""false"", ""off""):
            return False
        raise ValueError(value)
    return bool(value)
","if v in ( ""1"" , ""yes"" , ""true"" , ""on"" ) :",87
"def xdir(obj, return_values=False):
    for attr in dir(obj):
        if attr[:2] != ""__"" and attr[-2:] != ""__"":
            if return_values:
                yield attr, getattr(obj, attr)
            else:
                yield attr
","if attr [ : 2 ] != ""__"" and attr [ - 2 : ] != ""__"" :",76
"def get_current_stock(self):
    for d in self.get(""supplied_items""):
        if self.supplier_warehouse:
            bin = frappe.db.sql(
                ""select actual_qty from `tabBin` where item_code = %s and warehouse = %s"",
                (d.rm_item_code, self.supplier_warehouse),
                as_dict=1,
            )
            d.current_stock = bin and flt(bin[0][""actual_qty""]) or 0
",if self . supplier_warehouse :,138
"def getvars(request, excludes):
    getvars = request.GET.copy()
    excludes = excludes.split("","")
    for p in excludes:
        if p in getvars:
            del getvars[p]
        if len(getvars.keys()) > 0:
            return ""&%s"" % getvars.urlencode()
        else:
            return """"
",if p in getvars :,94
"def read(cls, reader, dump=None):
    code = reader.read_u1()
    # Create an index of all known opcodes.
    if Opcode.opcodes is None:
        Opcode.opcodes = {}
        for name in globals():
            klass = globals()[name]
            try:
                if name != ""Opcode"" and issubclass(klass, Opcode):
                    Opcode.opcodes[klass.code] = klass
            except TypeError:
                pass
    instance = Opcode.opcodes[code].read_extra(reader, dump)
    if dump:
        reader.debug(""    "" * dump, ""%3d: %s"" % (reader.offset, instance))
    return instance
","if name != ""Opcode"" and issubclass ( klass , Opcode ) :",181
"def clean(self):
    username = self.cleaned_data.get(""username"")
    password = self.cleaned_data.get(""password"")
    message = ERROR_MESSAGE
    if username and password:
        self.user_cache = authenticate(username=username, password=password)
        if self.user_cache is None:
            raise ValidationError(
                message % {""username"": self.username_field.verbose_name}
            )
        elif not self.user_cache.is_active or not self.user_cache.is_staff:
            raise ValidationError(
                message % {""username"": self.username_field.verbose_name}
            )
    return self.cleaned_data
",if self . user_cache is None :,177
"def currentLevel(self):
    currentStr = """"
    for stackType, stackValue in self.stackVals:
        if stackType == ""dict"":
            if isinstance(stackValue, str):
                currentStr += ""['"" + stackValue + ""']""
            else:  # numeric key...
                currentStr += ""["" + str(stackValue) + ""]""
        elif stackType == ""listLike"":
            currentStr += ""["" + str(stackValue) + ""]""
        elif stackType == ""getattr"":
            currentStr += "".__getattribute__('"" + stackValue + ""')""
        else:
            raise Exception(f""Cannot get attribute of type {stackType}"")
    return currentStr
","elif stackType == ""listLike"" :",176
"def dump(self, out=sys.stdout, code2cid=None, code=None):
    if code2cid is None:
        code2cid = self.code2cid
        code = ()
    for (k, v) in sorted(code2cid.iteritems()):
        c = code + (k,)
        if isinstance(v, int):
            out.write(""code %r = cid %d\n"" % (c, v))
        else:
            self.dump(out=out, code2cid=v, code=c)
    return
","if isinstance ( v , int ) :",140
"def __init__(self, text, menu):
    self.text = text
    self.menu = menu
    print(text)
    for i, option in enumerate(menu):
        menunum = i + 1
        # Check to see if this line has the 'return to main menu' code
        match = re.search(""0D"", option)
        # If it's not the return to menu line:
        if not match:
            if menunum < 10:
                print((""   %s) %s"" % (menunum, option)))
            else:
                print((""  %s) %s"" % (menunum, option)))
        else:
            print(""\n  99) Return to Main Menu\n"")
    return
",if menunum < 10 :,193
"def receive(self, sock):
    """"""Receive a message on ``sock``.""""""
    msg = None
    data = b""""
    recv_done = False
    recv_len = -1
    while not recv_done:
        buf = sock.recv(BUFSIZE)
        if buf is None or len(buf) == 0:
            raise Exception(""socket closed"")
        if recv_len == -1:
            recv_len = struct.unpack("">I"", buf[:4])[0]
            data += buf[4:]
            recv_len -= len(data)
        else:
            data += buf
            recv_len -= len(buf)
        recv_done = recv_len == 0
    msg = pickle.loads(data)
    return msg
",if recv_len == - 1 :,192
"def apply_shortcuts(self):
    """"""Apply shortcuts settings to all widgets/plugins""""""
    toberemoved = []
    for index, (qobject, context, name, default) in enumerate(self.shortcut_data):
        keyseq = QKeySequence(get_shortcut(context, name, default))
        try:
            if isinstance(qobject, QAction):
                qobject.setShortcut(keyseq)
            elif isinstance(qobject, QShortcut):
                qobject.setKey(keyseq)
        except RuntimeError:
            # Object has been deleted
            toberemoved.append(index)
    for index in sorted(toberemoved, reverse=True):
        self.shortcut_data.pop(index)
","elif isinstance ( qobject , QShortcut ) :",184
"def _resolved_values(self):
    values = []
    for k, v in self.values.items() if hasattr(self.values, ""items"") else self.values:
        if self.mapper:
            if isinstance(k, util.string_types):
                desc = _entity_descriptor(self.mapper, k)
                values.extend(desc._bulk_update_tuples(v))
            elif isinstance(k, attributes.QueryableAttribute):
                values.extend(k._bulk_update_tuples(v))
            else:
                values.append((k, v))
        else:
            values.append((k, v))
    return values
",if self . mapper :,176
"def remove_callback(self, callback, events=None):
    if events is None:
        for event in self._plugin_lifecycle_callbacks:
            if callback in self._plugin_lifecycle_callbacks[event]:
                self._plugin_lifecycle_callbacks[event].remove(callback)
    else:
        if isinstance(events, basestring):
            events = [events]
        for event in events:
            if callback in self._plugin_lifecycle_callbacks[event]:
                self._plugin_lifecycle_callbacks[event].remove(callback)
",if callback in self . _plugin_lifecycle_callbacks [ event ] :,148
"def _thd_parse_volumes(self, volumes):
    volume_list = []
    binds = {}
    for volume_string in volumes or []:
        try:
            bind, volume = volume_string.split("":"", 1)
        except ValueError:
            config.error(
                ""Invalid volume definition for docker ""
                ""%s. Skipping..."" % volume_string
            )
            continue
        ro = False
        if volume.endswith("":ro"") or volume.endswith("":rw""):
            ro = volume[-2:] == ""ro""
            volume = volume[:-3]
        volume_list.append(volume)
        binds[bind] = {""bind"": volume, ""ro"": ro}
    return volume_list, binds
","if volume . endswith ( "":ro"" ) or volume . endswith ( "":rw"" ) :",191
"def __init__(self, model, **kwargs):
    self.model = model
    for key, value in kwargs.items():
        if not hasattr(self, key):
            raise TypeError(
                ""%s() received an invalid keyword %r"" % (self.__class__.__name__, key)
            )
        setattr(self, key, value)
    self.handle_model()
","if not hasattr ( self , key ) :",97
"def __getitem__(self, key):
    if isinstance(key, numbers.Number):
        l = len(self)
        if key >= l:
            raise IndexError(""Index %s out of range (%s elements)"" % (key, l))
        if key < 0:
            if key < -l:
                raise IndexError(""Index %s out of range (%s elements)"" % (key, l))
            key += l
        return self(key + 1)
    elif isinstance(key, slice):
        raise ValueError(
            self.impl.__class__.__name__ + "" object does not support slicing""
        )
    else:
        return self(key)
",if key < - l :,170
"def _get_formatted(self, model, key):
    value = model._type(key).format(model.get(key))
    if isinstance(value, bytes):
        value = value.decode(""utf-8"", ""ignore"")
    if self.for_path:
        sep_repl = beets.config[""path_sep_replace""].as_str()
        for sep in (os.path.sep, os.path.altsep):
            if sep:
                value = value.replace(sep, sep_repl)
    return value
",if sep :,133
"def publish(self, name, stat):
    try:
        topic = ""stat.%s"" % str(name)
        if ""subtopic"" in stat:
            topic += "".%d"" % stat[""subtopic""]
        stat = json.dumps(stat)
        logger.debug(""Sending %s"" % stat)
        self.socket.send_multipart([b(topic), stat])
    except zmq.ZMQError:
        if self.socket.closed:
            pass
        else:
            raise
","if ""subtopic"" in stat :",130
"def logic():
    while 1:
        yield a
        var = 0
        out.next = 0
        for i in downrange(len(a)):
            if a[i] == 0:
                continue
            else:
                for j in downrange(i - 1):
                    if a[j] == 0:
                        pass
                    else:
                        out.next = j
                        break
                break
",if a [ j ] == 0 :,136
"def get_abstract_models(self, appmodels):
    abstract_models = []
    for appmodel in appmodels:
        abstract_models += [
            abstract_model
            for abstract_model in appmodel.__bases__
            if hasattr(abstract_model, ""_meta"") and abstract_model._meta.abstract
        ]
    abstract_models = list(set(abstract_models))  # remove duplicates
    return abstract_models
","if hasattr ( abstract_model , ""_meta"" ) and abstract_model . _meta . abstract",108
"def _sanitize_field_name(self, field_name: str) -> str:
    try:
        if self._meta.get_field(field_name).get_internal_type() == ""ForeignKey"":
            if not field_name.endswith(""_id""):
                return field_name + ""_id""
    except FieldDoesNotExist:
        pass
    return field_name
","if not field_name . endswith ( ""_id"" ) :",92
"def find_enabled_item(self, e):
    x, y = e.local
    if (
        0
        <= x
        < (
            self.width - self.margin - self.scroll_button_size
            if self.scrolling
            else self.width
        )
    ):
        h = self.font.get_linesize()
        i = (y - h // 2) // h + self.scroll
        items = self._items
        if 0 <= i < len(items):
            item = items[i]
            if item.enabled:
                return item
",if 0 <= i < len ( items ) :,160
"def addColumn(self, *cols, index=None):
    ""Insert all *cols* into columns at *index*, or append to end of columns if *index* is None.  Return first column.""
    for i, col in enumerate(cols):
        vd.addUndo(self.columns.remove, col)
        if index is None:
            index = len(self.columns)
        col.recalc(self)
        self.columns.insert(index + i, col)
        Sheet.visibleCols.fget.cache_clear()
    return cols[0]
",if index is None :,141
"def _compare_values(self, result, source):
    from google.protobuf.struct_pb2 import ListValue
    from google.protobuf.struct_pb2 import Value
    for found, expected in zip(result, source):
        self.assertIsInstance(found, ListValue)
        self.assertEqual(len(found.values), len(expected))
        for found_cell, expected_cell in zip(found.values, expected):
            self.assertIsInstance(found_cell, Value)
            if isinstance(expected_cell, int):
                self.assertEqual(int(found_cell.string_value), expected_cell)
            else:
                self.assertEqual(found_cell.string_value, expected_cell)
","if isinstance ( expected_cell , int ) :",179
"def _traverse(op):
    if topi.tag.is_broadcast(op.tag):
        if not op.same_as(output.op):
            if not op.axis:
                const_ops.append(op)
            else:
                ewise_ops.append(op)
        for tensor in op.input_tensors:
            if isinstance(tensor.op, tvm.te.PlaceholderOp):
                ewise_inputs.append((op, tensor))
            else:
                _traverse(tensor.op)
    else:
        assert op.tag == ""dense_pack""
        dense_res.append(op)
","if isinstance ( tensor . op , tvm . te . PlaceholderOp ) :",174
"def update_annotation(
    parameters: Sequence[cst.Param], annotations: Sequence[cst.Param]
) -> List[cst.Param]:
    parameter_annotations = {}
    annotated_parameters = []
    for parameter in annotations:
        if parameter.annotation:
            parameter_annotations[parameter.name.value] = parameter.annotation
    for parameter in parameters:
        key = parameter.name.value
        if key in parameter_annotations and (
            self.overwrite_existing_annotations or not parameter.annotation
        ):
            parameter = parameter.with_changes(annotation=parameter_annotations[key])
        annotated_parameters.append(parameter)
    return annotated_parameters
",if parameter . annotation :,167
"def _modules(self, module_paths, component_name):
    for path in module_paths:
        for filename in os.listdir(path):
            name, ext = os.path.splitext(filename)
            if ext.endswith("".py""):
                root_relative_path = os.path.join(path, name)[
                    len(self.root_path) + len(os.path.sep) :
                ]
                module_name = ""%s.%s"" % (
                    component_name,
                    root_relative_path.replace(os.path.sep, "".""),
                )
                yield module_name
","if ext . endswith ( "".py"" ) :",176
"def run(self):
    # Make some objects emit lights
    for obj in bpy.context.scene.objects:
        if ""modelId"" in obj:
            obj_id = obj[""modelId""]
            # In the case of the lamp
            if obj_id in self.lights:
                self._make_lamp_emissive(obj, self.lights[obj_id])
            # Make the windows emit light
            if obj_id in self.windows:
                self._make_window_emissive(obj)
            # Also make ceilings slightly emit light
            if obj.name.startswith(""Ceiling#""):
                self._make_ceiling_emissive(obj)
",if obj_id in self . lights :,190
"def get_chart_data(self):
    rows = []
    for row in self.data:
        row = frappe._dict(row)
        if not cint(row.bold):
            values = [row.range1, row.range2, row.range3, row.range4, row.range5]
            precision = cint(frappe.db.get_default(""float_precision"")) or 2
            rows.append({""values"": [flt(val, precision) for val in values]})
    self.chart = {
        ""data"": {""labels"": self.ageing_column_labels, ""datasets"": rows},
        ""type"": ""percentage"",
    }
",if not cint ( row . bold ) :,171
"def suite(aggressive):
    """"""Run against pep8 test suite.""""""
    result = True
    path = os.path.join(os.path.dirname(__file__), ""suite"")
    for filename in os.listdir(path):
        filename = os.path.join(path, filename)
        if filename.endswith("".py""):
            print(filename, file=sys.stderr)
            result = run(filename, aggressive=aggressive) and result
    if result:
        print(GREEN + ""Okay"" + END)
    return result
","if filename . endswith ( "".py"" ) :",133
"def list_generator(pages, num_results):
    result = []
    # get first page items
    page = list(next(pages))
    result += page
    while True:
        if not pages.continuation_token:
            break
        # handle num results
        if num_results is not None:
            if num_results == len(result):
                break
        page = list(next(pages))
        result += page
    return result
",if not pages . continuation_token :,118
"def _detect_too_many_digits(f):
    ret = []
    for node in f.nodes:
        # each node contains a list of IR instruction
        for ir in node.irs:
            # iterate over all the variables read by the IR
            for read in ir.read:
                # if the variable is a constant
                if isinstance(read, Constant):
                    # read.value can return an int or a str. Convert it to str
                    value_as_str = read.original_value
                    if ""00000"" in value_as_str:
                        # Info to be printed
                        ret.append(node)
    return ret
","if ""00000"" in value_as_str :",183
"def write_varint(trans, n):
    out = []
    while True:
        if n & ~0x7F == 0:
            out.append(n)
            break
        else:
            out.append((n & 0xFF) | 0x80)
            n = n >> 7
    data = array.array(""B"", out).tostring()
    if PY3:
        trans.write(data)
    else:
        trans.write(bytes(data))
",if n & ~ 0x7F == 0 :,126
"def __call__(self, environ, start_response):
    query_string = environ.get(""QUERY_STRING"")
    if ""sql_debug=1"" in query_string:
        import galaxy.app
        if galaxy.app.app.model.thread_local_log:
            galaxy.app.app.model.thread_local_log.log = True
    try:
        reset_request_query_counts()
        return self.application(environ, start_response)
    finally:
        log_request_query_counts(environ.get(""PATH_INFO""))
",if galaxy . app . app . model . thread_local_log :,146
"def SvGetSocketInfo(socket):
    """"""returns string to show in socket label""""""
    global socket_data_cache
    ng = socket.id_data.tree_id
    if socket.is_output:
        s_id = socket.socket_id
    elif socket.is_linked:
        other = socket.other
        if other and hasattr(other, ""socket_id""):
            s_id = other.socket_id
        else:
            return """"
    else:
        return """"
    if ng in socket_data_cache:
        if s_id in socket_data_cache[ng]:
            data = socket_data_cache[ng][s_id]
            if data:
                return str(len(data))
    return """"
",if data :,197
"def print_nested_help(self, args: argparse.Namespace) -> None:
    level = 0
    parser = self.main_parser
    while True:
        if parser._subparsers is None:
            break
        if parser._subparsers._actions is None:
            break
        choices = parser._subparsers._actions[-1].choices
        value = getattr(args, ""level_%d"" % level)
        if value is None:
            parser.print_help()
            return
        if not choices:
            break
        if isinstance(choices, dict):
            parser = choices[value]
        else:
            return
        level += 1
","if isinstance ( choices , dict ) :",175
"def tag_configure(self, *args, **keys):
    trace = False and not g.unitTesting
    if trace:
        g.trace(args, keys)
    if len(args) == 1:
        key = args[0]
        self.tags[key] = keys
        val = keys.get(""foreground"")
        underline = keys.get(""underline"")
        if val:
            self.configDict[key] = val
        if underline:
            self.configUnderlineDict[key] = True
    else:
        g.trace(""oops"", args, keys)
",if underline :,150
"def get_tokens_unprocessed(self, text):
    for index, token, value in RegexLexer.get_tokens_unprocessed(self, text):
        if token is Name:
            if self.stdlibhighlighting and value in self.stdlib_types:
                token = Keyword.Type
            elif self.c99highlighting and value in self.c99_types:
                token = Keyword.Type
            elif self.platformhighlighting and value in self.linux_types:
                token = Keyword.Type
        yield index, token, value
",if self . stdlibhighlighting and value in self . stdlib_types :,141
"def materialize_as_ndarray(a):
    """"""Convert distributed arrays to ndarrays.""""""
    if type(a) in (list, tuple):
        if da is not None and any(isinstance(arr, da.Array) for arr in a):
            return da.compute(*a, sync=True)
        return tuple(np.asarray(arr) for arr in a)
    return np.asarray(a)
","if da is not None and any ( isinstance ( arr , da . Array ) for arr in a ) :",98
"def decorated_function(*args, **kwargs):
    rv = f(*args, **kwargs)
    if isinstance(rv, flask.Response):
        try:
            result = etag
            if callable(result):
                result = result(rv)
            if result:
                rv.set_etag(result)
        except Exception:
            logging.getLogger(__name__).exception(
                ""Error while calculating the etag value for response {!r}"".format(rv)
            )
    return rv
",if result :,133
"def applyBC(self):
    """"""apply boundary conditions""""""
    deltaR = 2.0
    for coord in self.pos:
        if coord[0] > width + deltaR:
            coord[0] = -deltaR
        if coord[0] < -deltaR:
            coord[0] = width + deltaR
        if coord[1] > height + deltaR:
            coord[1] = -deltaR
        if coord[1] < -deltaR:
            coord[1] = height + deltaR
",if coord [ 0 ] > width + deltaR :,135
"def removeInsideIslands(self):
    self.CleanPath = []
    cleanpath = Path(""Path"")
    for path in self.NewPaths:
        for seg in path:
            inside = False
            for island in self.IntersectedIslands:
                issegin = island.isSegInside(seg) == 1
                if issegin:
                    if not seg in island:
                        inside = True
                        break
            if not inside:
                cleanpath.append(seg)
    cleanpath = cleanpath.split2contours()
    self.CleanPath.extend(cleanpath)
",if not seg in island :,176
"def _parse_lines(self, linesource):
    """"""Parse lines of text for functions and classes""""""
    functions = []
    classes = []
    for line in linesource:
        if line.startswith(""def "") and line.count(""(""):
            # exclude private stuff
            name = self._get_object_name(line)
            if not name.startswith(""_""):
                functions.append(name)
        elif line.startswith(""class ""):
            # exclude private stuff
            name = self._get_object_name(line)
            if not name.startswith(""_""):
                classes.append(name)
        else:
            pass
    functions.sort()
    classes.sort()
    return functions, classes
","if not name . startswith ( ""_"" ) :",185
"def process(self, buckets, event=None):
    results = []
    with self.executor_factory(max_workers=2) as w:
        futures = {w.submit(self.process_bucket, bucket): bucket for bucket in buckets}
        for f in as_completed(futures):
            if f.result():
                results.append(futures[f])
    return results
",if f . result ( ) :,97
"def build_polymorphic_ctypes_map(cls):
    # {'1': 'unified_job', '2': 'Job', '3': 'project_update', ...}
    mapping = {}
    for ct in ContentType.objects.filter(app_label=""main""):
        ct_model_class = ct.model_class()
        if ct_model_class and issubclass(ct_model_class, cls):
            mapping[ct.id] = camelcase_to_underscore(ct_model_class.__name__)
    return mapping
","if ct_model_class and issubclass ( ct_model_class , cls ) :",126
"def expand_decodings(self, node: Node) -> None:
    val = node.level.result.value
    for decoder in self.get_decoders_for(type(val)):
        inst = self._config()(decoder)
        res = inst(val)
        if res is None:
            continue
        try:
            new_node = Node.decoding(
                config=self._config(), route=inst, result=res, source=node
            )
        except DuplicateNode:
            continue
        logger.trace(""Nesting encodings"")
        self.recursive_expand(new_node, False)
",if res is None :,159
"def test_file(self):
    a = 3.33 + 4.43j
    b = 5.1 + 2.3j
    fo = None
    try:
        fo = open(test_support.TESTFN, ""wb"")
        print >> fo, a, b
        fo.close()
        fo = open(test_support.TESTFN, ""rb"")
        self.assertEqual(fo.read(), ""%s %s\n"" % (a, b))
    finally:
        if (fo is not None) and (not fo.closed):
            fo.close()
        test_support.unlink(test_support.TESTFN)
",if ( fo is not None ) and ( not fo . closed ) :,158
"def repl(m):
    if m.group(2) is not None:
        high = int(m.group(1), 16)
        low = int(m.group(2), 16)
        if 0xD800 <= high <= 0xDBFF and 0xDC00 <= low <= 0xDFFF:
            cp = ((high - 0xD800) << 10) + (low - 0xDC00) + 0x10000
            return unichr(cp)
        else:
            return unichr(high) + unichr(low)
    else:
        return unichr(int(m.group(1), 16))
",if 0xD800 <= high <= 0xDBFF and 0xDC00 <= low <= 0xDFFF :,149
"def generate_credits(user, start_date, end_date, **kwargs):
    """"""Generate credits data for given component.""""""
    result = []
    base = Change.objects.content()
    if user:
        base = base.filter(author=user)
    for language in Language.objects.filter(**kwargs).distinct().iterator():
        authors = base.filter(language=language, **kwargs).authors_list(
            (start_date, end_date)
        )
        if not authors:
            continue
        result.append({language.name: sorted(authors, key=lambda item: item[2])})
    return result
",if not authors :,157
"def history_prev(self):
    """"""Go back in the history.""""""
    try:
        if not self._history.is_browsing():
            item = self._history.start(self.text().strip())
        else:
            item = self._history.previtem()
    except (cmdhistory.HistoryEmptyError, cmdhistory.HistoryEndReachedError):
        return
    self.setText(item)
",if not self . _history . is_browsing ( ) :,102
"def destroy(self):
    self._bind()
    for name in ""jobItems"", ""jobFileIDs"", ""files"", ""statsFiles"", ""statsFileIDs"":
        resource = getattr(self, name)
        if resource is not None:
            if isinstance(resource, AzureTable):
                resource.delete_table()
            elif isinstance(resource, AzureBlobContainer):
                resource.delete_container()
            else:
                assert False
            setattr(self, name, None)
","elif isinstance ( resource , AzureBlobContainer ) :",135
"def user_defined_os():
    if menu.options.os:
        if menu.options.os.lower() == ""windows"":
            settings.TARGET_OS = ""win""
            return True
        elif menu.options.os.lower() == ""unix"":
            return True
        else:
            err_msg = ""You specified wrong value '"" + menu.options.os + ""' ""
            err_msg += ""as an operation system. The value, must be 'Windows' or 'Unix'.""
            print(settings.print_critical_msg(err_msg))
            raise SystemExit()
","if menu . options . os . lower ( ) == ""windows"" :",153
"def test_save(art_warning, image_dl_estimator):
    try:
        classifier, _ = image_dl_estimator(from_logits=True)
        t_file = tempfile.NamedTemporaryFile()
        model_path = t_file.name
        t_file.close()
        filename = ""model_to_save""
        classifier.save(filename, path=model_path)
        assert path.exists(model_path)
        created_model = False
        for file in listdir(model_path):
            if filename in file:
                created_model = True
        assert created_model
    except ARTTestException as e:
        art_warning(e)
",if filename in file :,172
"def set_extra_data(self, extra_data=None):
    if extra_data and self.extra_data != extra_data:
        if self.extra_data and not isinstance(self.extra_data, str):
            self.extra_data.update(extra_data)
        else:
            self.extra_data = extra_data
        return True
","if self . extra_data and not isinstance ( self . extra_data , str ) :",93
"def get_image_dimensions(path):
    """"""Returns the (width, height) of an image at a given path.""""""
    p = ImageFile.Parser()
    fp = open(path, ""rb"")
    while 1:
        data = fp.read(1024)
        if not data:
            break
        p.feed(data)
        if p.image:
            return p.image.size
            break
    fp.close()
    return None
",if not data :,118
"def language_suffixes():
    for lang in NSLocale.preferredLanguages():
        while True:
            yield ""_"" + lang if lang != ""en"" else """"
            if ""-"" in lang:
                lang = lang[: lang.rfind(""-"")]
            else:
                break
    yield """"
","if ""-"" in lang :",78
"def decode_binary(binarystring):
    """"""Decodes a binary string into it's integer value.""""""
    n = 0
    for c in binarystring:
        if c == ""0"":
            d = 0
        elif c == ""1"":
            d = 1
        else:
            raise ValueError(""Not an binary number"", binarystring)
        # Could use ((n << 3 ) | d), but python 2.3 issues a FutureWarning.
        n = (n * 2) + d
    return n
","elif c == ""1"" :",126
"def serialize_groups_for_summary(node):
    groups = node.osf_groups
    n_groups = len(groups)
    group_string = """"
    for index, group in enumerate(groups):
        if index == n_groups - 1:
            separator = """"
        elif index == n_groups - 2:
            separator = "" & ""
        else:
            separator = "", ""
        group_string = group_string + group.name + separator
    return group_string
",elif index == n_groups - 2 :,125
"def _save(self, req_method, requires):
    conanfile = GenConanfile()
    for req in requires:
        req2, override = req if isinstance(req, tuple) else (req, False)
        if not req_method:
            conanfile.with_require(req2, override=override)
        else:
            conanfile.with_requirement(req2, override=override)
    self.client.save({""conanfile.py"": conanfile}, clean_first=True)
",if not req_method :,130
"def _validate_declarations(
    declarations: Sequence[Union[qlast.ModuleDeclaration, qlast.DDLCommand]]
) -> None:
    # Check that top-level declarations either use fully-qualified
    # names or are module blocks.
    for decl in declarations:
        if not isinstance(decl, qlast.ModuleDeclaration) and decl.name.module is None:
            raise EdgeQLSyntaxError(
                ""only fully-qualified name is allowed in "" ""top-level declaration"",
                context=decl.name.context,
            )
","if not isinstance ( decl , qlast . ModuleDeclaration ) and decl . name . module is None :",134
"def assess_trial(self, trial_job_id, trial_history):
    _logger.info(""assess trial %s %s"", trial_job_id, trial_history)
    id_ = trial_history[0]
    if id_ in self._killed:
        return AssessResult.Bad
    s = 0
    for i, val in enumerate(trial_history):
        s += val
        if s % 11 == 1:
            self._killed.add(id_)
            _result.write(""%d %d\n"" % (id_, i + 1))
            _result.flush()
            return AssessResult.Bad
    return AssessResult.Good
",if s % 11 == 1 :,170
"def decProcess():
    while 1:
        yield clock.posedge, reset.negedge
        if reset == ACTIVE_LOW:
            count.next = 0
        else:
            if enable:
                if count == -n:
                    count.next = n - 1
                else:
                    count.next = count - 1
",if reset == ACTIVE_LOW :,100
"def activate_profile(test=True):
    pr = None
    if test:
        if HAS_CPROFILE:
            pr = cProfile.Profile()
            pr.enable()
        else:
            log.error(""cProfile is not available on your platform"")
    return pr
",if HAS_CPROFILE :,74
"def insertTestData(self, rows):
    for row in rows:
        if isinstance(row, Log):
            self.logs[row.id] = row.values.copy()
    for row in rows:
        if isinstance(row, LogChunk):
            lines = self.log_lines.setdefault(row.logid, [])
            # make sure there are enough slots in the list
            if len(lines) < row.last_line + 1:
                lines.append([None] * (row.last_line + 1 - len(lines)))
            row_lines = row.content.decode(""utf-8"").split(""\n"")
            lines[row.first_line : row.last_line + 1] = row_lines
","if isinstance ( row , LogChunk ) :",187
"def getText(self, stuff):
    if isinstance(stuff, BaseWrapper):
        stuff = stuff.item
    if isinstance(stuff, (Fit, TargetProfile)):
        val, unit = self._getValue(stuff)
        if val is None:
            return """"
        # Stick to value - 25k GJ
        if self.stickPrefixToValue:
            return ""{} {}"".format(formatAmount(val, *self.formatSpec), unit)
        # Stick to unit - 25 km
        else:
            return formatAmount(val, *self.formatSpec, unitName=unit)
    return """"
",if self . stickPrefixToValue :,155
"def wrap(request, *args, **kwargs):
    ""Wrap""
    user = request.user.profile
    if ""massform"" in request.POST:
        for key in request.POST:
            if ""mass-changeset"" in key:
                try:
                    changeset = ChangeSet.objects.get(pk=request.POST[key])
                    form = MassActionForm(
                        request.user.profile, request.POST, instance=changeset
                    )
                    if form.is_valid() and user.has_permission(changeset, mode=""w""):
                        form.save()
                except Exception:
                    pass
    return f(request, *args, **kwargs)
","if ""mass-changeset"" in key :",196
"def select(self, browser, locator):
    assert browser is not None
    if locator is not None:
        if isinstance(locator, list):
            self._select_by_excludes(browser, locator)
            return
        if locator.lower() == ""self"" or locator.lower() == ""current"":
            return
        if locator.lower() == ""new"" or locator.lower() == ""popup"":
            self._select_by_last_index(browser)
            return
    (prefix, criteria) = self._parse_locator(locator)
    strategy = self._strategies.get(prefix)
    if strategy is None:
        raise ValueError(""Window locator with prefix '"" + prefix + ""' is not supported"")
    return strategy(browser, criteria)
","if locator . lower ( ) == ""new"" or locator . lower ( ) == ""popup"" :",187
"def test_all(self):
    for context in get_contexts():
        found = False
        expected_context_name = context.get_name()
        for calculated_context in get_context(self.HTML, expected_context_name):
            if calculated_context.get_name() == expected_context_name:
                found = True
        if not found:
            msg = ""The analysis for %s context failed, got %r instead.""
            msg = msg % (
                expected_context_name,
                get_context(self.HTML, expected_context_name),
            )
            self.assertTrue(False, msg)
",if not found :,171
"def visit_title(self, node: Element) -> None:
    if isinstance(node.parent, addnodes.seealso):
        self.body.append('.IP ""')
        return
    elif isinstance(node.parent, nodes.section):
        if self.section_level == 0:
            # skip the document title
            raise nodes.SkipNode
        elif self.section_level == 1:
            self.body.append("".SH %s\n"" % self.deunicode(node.astext().upper()))
            raise nodes.SkipNode
    return super().visit_title(node)
",if self . section_level == 0 :,145
"def parse_svn_stats(status):
    stats = RepoStats()
    for line in status:
        if line[0] == ""?"":
            stats.new += 1
        elif line[0] == ""C"":
            stats.conflicted += 1
        elif line[0] in [""A"", ""D"", ""I"", ""M"", ""R"", ""!"", ""~""]:
            stats.changed += 1
    return stats
","if line [ 0 ] == ""?"" :",106
"def setoutput(self, spec, defs=None):
    self.closespec()
    self.closedefs()
    if spec:
        if type(spec) == StringType:
            file = self.openoutput(spec)
            mine = 1
        else:
            file = spec
            mine = 0
        self.specfile = file
        self.specmine = mine
    if defs:
        if type(defs) == StringType:
            file = self.openoutput(defs)
            mine = 1
        else:
            file = defs
            mine = 0
        self.defsfile = file
        self.defsmine = mine
",if type ( defs ) == StringType :,179
"def __new__(cls, name, bases, d):
    rv = type.__new__(cls, name, bases, d)
    if ""methods"" not in d:
        methods = set(rv.methods or [])
        for key, value in d.iteritems():
            if key in http_method_funcs:
                methods.add(key.upper())
        # if we have no method at all in there we don't want to
        # add a method list.  (This is for instance the case for
        # the baseclass or another subclass of a base method view
        # that does not introduce new methods).
        if methods:
            rv.methods = sorted(methods)
    return rv
",if key in http_method_funcs :,172
"def draw_lines(col, lines):
    skip = False
    for l in lines:
        if l:
            col.label(text=l)
            skip = False
        elif skip:
            continue
        else:
            col.label(text=l)
            skip = True
",elif skip :,82
"def adjust_sockets(self):
    variables = self.get_variables()
    for key in self.inputs.keys():
        if key not in variables and key not in [""Field""]:
            self.debug(
                ""Input {} not in variables {}, remove it"".format(key, str(variables))
            )
            self.inputs.remove(self.inputs[key])
    for v in variables:
        if v not in self.inputs:
            self.debug(
                ""Variable {} not in inputs {}, add it"".format(
                    v, str(self.inputs.keys())
                )
            )
            self.inputs.new(""SvStringsSocket"", v)
",if v not in self . inputs :,183
"def forward(self, g, x):
    h = x
    for l, conv in enumerate(self.layers):
        h = conv(g, h)
        if l != len(self.layers) - 1:
            h = self.activation(h)
            h = self.dropout(h)
    return h
",if l != len ( self . layers ) - 1 :,82
"def process_doc(self, docstrings: List[List[str]]) -> Iterator[str]:
    """"""Let the user process the docstrings before adding them.""""""
    for docstringlines in docstrings:
        if self.env.app:
            # let extensions preprocess docstrings
            self.env.app.emit(
                ""autodoc-process-docstring"",
                self.objtype,
                self.fullname,
                self.object,
                self.options,
                docstringlines,
            )
            if docstringlines and docstringlines[-1] != """":
                # append a blank line to the end of the docstring
                docstringlines.append("""")
        yield from docstringlines
",if self . env . app :,185
"def wiki(self, query):
    res = []
    for entry in g.current_wiki.get_index():
        name = filename_to_cname(entry[""name""])
        name = re.sub(r""//+"", ""/"", name)
        if set(query.split()).intersection(name.replace(""/"", ""-"").split(""-"")):
            page = g.current_wiki.get_page(name)
            # this can be None, not sure how
            if page:
                res.append(dict(name=name, content=page.data))
    return res
",if page :,143
"def checkForFinishedThreads(self):
    ""Mark terminated threads with endTime.""
    for t in self.unfinishedThreads:
        if not t.is_alive():
            t.endTime = time.process_time()
            if getattr(t, ""status"", None) is None:
                t.status = ""ended""
","if getattr ( t , ""status"" , None ) is None :",84
"def testTicketFlags(self):
    flags = (""restored"", ""banned"")
    ticket = Ticket(""test"", 0)
    trueflags = []
    for v in (True, False, True):
        for f in flags:
            setattr(ticket, f, v)
            if v:
                trueflags.append(f)
            else:
                trueflags.remove(f)
            for f2 in flags:
                self.assertEqual(bool(getattr(ticket, f2)), f2 in trueflags)
    ## inherite props from another tockets:
    ticket = FailTicket(ticket=ticket)
    for f2 in flags:
        self.assertTrue(bool(getattr(ticket, f2)))
",if v :,189
"def decode(obj, encoding=""utf-8"", errors=""strict""):
    decoder = __decoder(encoding)
    if decoder:
        result = decoder(obj, errors)
        if not (isinstance(result, tuple) and len(result) == 2):
            raise TypeError(""decoder must return a tuple (object, integer)"")
        return result[0]
","if not ( isinstance ( result , tuple ) and len ( result ) == 2 ) :",86
"def work(self):
    """"""Play the animation.""""""
    # if loop_mode is once and we are already on the last frame,
    # return to the first frame... (so the user can keep hitting once)
    if self.loop_mode == LoopMode.ONCE:
        if self.step > 0 and self.current >= self.max_point - 1:
            self.frame_requested.emit(self.axis, self.min_point)
        elif self.step < 0 and self.current <= self.min_point + 1:
            self.frame_requested.emit(self.axis, self.max_point)
        self.timer.singleShot(int(self.interval), self.advance)
    else:
        # immediately advance one frame
        self.advance()
    self.started.emit()
",if self . step > 0 and self . current >= self . max_point - 1 :,199
"def get_order(self, aStr):
    # for big5 encoding, we are interested
    #   first  byte range: 0xa4 -- 0xfe
    #   second byte range: 0x40 -- 0x7e , 0xa1 -- 0xfe
    # no validation needed here. State machine has done that
    if aStr[0] >= ""\xA4"":
        if aStr[1] >= ""\xA1"":
            return 157 * (ord(aStr[0]) - 0xA4) + ord(aStr[1]) - 0xA1 + 63
        else:
            return 157 * (ord(aStr[0]) - 0xA4) + ord(aStr[1]) - 0x40
    else:
        return -1
","if aStr [ 1 ] >= ""\xA1"" :",185
"def validate_literals(self):
    try:
        for c in self.literals:
            if not isinstance(c, StringTypes) or len(c) > 1:
                self.log.error(
                    ""Invalid literal %s. Must be a single character"", repr(c)
                )
                self.error = True
    except TypeError:
        self.log.error(
            ""Invalid literals specification. literals must be a sequence of characters""
        )
        self.error = True
","if not isinstance ( c , StringTypes ) or len ( c ) > 1 :",135
"def filter(self, qs, value):
    if value:
        if value.start is not None and value.stop is not None:
            value = (value.start, value.stop)
        elif value.start is not None:
            self.lookup_expr = ""startswith""
            value = value.start
        elif value.stop is not None:
            self.lookup_expr = ""endswith""
            value = value.stop
    return super().filter(qs, value)
",elif value . stop is not None :,125
"def parse_stdout(s):
    argv = re.search(""^===ARGV=(.*?)$"", s, re.M).group(1)
    argv = argv.split()
    testname = argv[-1]
    del argv[-1]
    hub = None
    reactor = None
    while argv:
        if argv[0] == ""--hub"":
            hub = argv[1]
            del argv[0]
            del argv[0]
        elif argv[0] == ""--reactor"":
            reactor = argv[1]
            del argv[0]
            del argv[0]
        else:
            del argv[0]
    if reactor is not None:
        hub += ""/%s"" % reactor
    return testname, hub
","if argv [ 0 ] == ""--hub"" :",196
"def get(self, key):
    try:
        res = self.server.get(
            index=self.index,
            doc_type=self.doc_type,
            id=key,
        )
        try:
            if res[""found""]:
                return res[""_source""][""result""]
        except (TypeError, KeyError):
            pass
    except elasticsearch.exceptions.NotFoundError:
        pass
","if res [ ""found"" ] :",112
"def _get_target_chap_auth(self, context, volume):
    """"""Get the current chap auth username and password.""""""
    try:
        # Query DB to get latest state of volume
        volume_info = self.db.volume_get(context, volume[""id""])
        # 'provider_auth': 'CHAP user_id password'
        if volume_info[""provider_auth""]:
            return tuple(volume_info[""provider_auth""].split("" "", 3)[1:])
    except exception.NotFound:
        LOG.debug(""Failed to get CHAP auth from DB for %s."", volume[""id""])
","if volume_info [ ""provider_auth"" ] :",149
"def merge(self, hosts):
    for ei in self:
        host_name = ei.get_name()
        h = hosts.find_by_name(host_name)
        if h is not None:
            # FUUUUUUUUUUsion
            self.merge_extinfo(h, ei)
",if h is not None :,86
"def __init__(self, user, *args, **kwargs):
    ""Sets choices and initial value""
    super(SettingsForm, self).__init__(*args, **kwargs)
    self.fields[""default_changeset_status""].queryset = ChangeSetStatus.objects.filter(
        trash=False
    )
    try:
        conf = ModuleSetting.get_for_module(
            ""treeio.changes"", ""default_changeset_status""
        )[0]
        default_changeset_status = ChangeSetStatus.objects.get(pk=long(conf.value))
        if not default_changeset_status.trash:
            self.fields[
                ""default_changeset_status""
            ].initial = default_changeset_status.id
    except Exception:
        pass
",if not default_changeset_status . trash :,191
"def load(self):
    """"""Method for loading a feature""""""
    with self.filesystem.openbin(self.path, ""r"") as file_handle:
        if self.path.endswith(FileFormat.GZIP.extension()):
            with gzip.open(file_handle, ""rb"") as gzip_fp:
                return self._decode(gzip_fp, self.path)
        return self._decode(file_handle, self.path)
",if self . path . endswith ( FileFormat . GZIP . extension ( ) ) :,108
"def edge2str(self, nfrom, nto):
    if isinstance(nfrom, ExprCompose):
        for i in nfrom.args:
            if i[0] == nto:
                return ""[%s, %s]"" % (i[1], i[2])
    elif isinstance(nfrom, ExprCond):
        if nfrom.cond == nto:
            return ""?""
        elif nfrom.src1 == nto:
            return ""True""
        elif nfrom.src2 == nto:
            return ""False""
    return """"
",if i [ 0 ] == nto :,149
"def disable_verity():
    """"""Disables dm-verity on the device.""""""
    with log.waitfor(""Disabling dm-verity on %s"" % context.device):
        root()
        with AdbClient() as c:
            reply = c.disable_verity()
        if ""Verity already disabled"" in reply:
            return
        elif ""Now reboot your device"" in reply:
            reboot(wait=True)
        elif ""0006closed"" in reply:
            return  # Emulator doesnt support Verity?
        else:
            log.error(""Could not disable verity:\n%s"" % reply)
","elif ""0006closed"" in reply :",165
"def __demo_mode_pause_if_active(self, tiny=False):
    if self.demo_mode:
        wait_time = settings.DEFAULT_DEMO_MODE_TIMEOUT
        if self.demo_sleep:
            wait_time = float(self.demo_sleep)
        if not tiny:
            time.sleep(wait_time)
        else:
            time.sleep(wait_time / 3.4)
    elif self.slow_mode:
        self.__slow_mode_pause_if_active()
",if not tiny :,134
"def dictToKW(d):
    out = []
    items = list(d.items())
    items.sort()
    for k, v in items:
        if not isinstance(k, str):
            raise NonFormattableDict(""%r ain't a string"" % k)
        if not r.match(k):
            raise NonFormattableDict(""%r ain't an identifier"" % k)
        out.append(""\n\0{}={},"".format(k, prettify(v)))
    return """".join(out)
","if not isinstance ( k , str ) :",131
"def createCommonCommands(self):
    """"""Handle all global @command nodes.""""""
    c = self.c
    aList = c.config.getCommands() or []
    for z in aList:
        p, script = z
        gnx = p.v.gnx
        if gnx not in self.seen:
            self.seen.add(gnx)
            script = self.getScript(p)
            self.createCommonCommand(p, script)
",if gnx not in self . seen :,121
"def _decodeFromStream(self, s):
    """"""Decode a complete DER OBJECT ID from a file.""""""
    # Fill up self.payload
    DerObject._decodeFromStream(self, s)
    # Derive self.value from self.payload
    p = BytesIO_EOF(self.payload)
    comps = list(map(str, divmod(p.read_byte(), 40)))
    v = 0
    while p.remaining_data():
        c = p.read_byte()
        v = v * 128 + (c & 0x7F)
        if not (c & 0x80):
            comps.append(str(v))
            v = 0
    self.value = ""."".join(comps)
",if not ( c & 0x80 ) :,174
"def tiles_around_factor(self, factor, pos, radius=1, predicate=None):
    ps = []
    x, y = pos
    for dx in range(-radius, radius + 1):
        nx = x + dx
        if nx >= 0 and nx < self.width * factor:
            for dy in range(-radius, radius + 1):
                ny = y + dy
                if ny >= 0 and ny < self.height * factor and (dx != 0 or dy != 0):
                    if predicate is None or predicate((nx, ny)):
                        ps.append((nx, ny))
    return ps
",if nx >= 0 and nx < self . width * factor :,159
"def deleteAllMatchers(self):
    """"""Deletes all matchers.""""""
    if self.__filter:
        result = QtWidgets.QMessageBox.question(
            self,
            ""Delete All Matchers?"",
            ""Are you sure you want to delete all matchers?"",
            QtWidgets.QMessageBox.Yes | QtWidgets.QMessageBox.No,
        )
        if result == QtWidgets.QMessageBox.Yes:
            self._itemsLock.lockForWrite()
            try:
                for item in list(self._items.values()):
                    item.rpcObject.delete()
            finally:
                self._itemsLock.unlock()
            self.removeAllItems()
",if result == QtWidgets . QMessageBox . Yes :,186
"def _parse_icons(self, icons):
    if isinstance(icons, list):
        icons = get_iterated_icons(icons)
    for icon in icons:
        if isinstance(icons, list):
            icon = Icon(icon)
        else:
            icon = Icon(icons[icon])
        if icon.exists:  # If icon found on current Gtk Icon theme
            self.icons.append(icon)
","if isinstance ( icons , list ) :",107
"def change_misc_visibility(self, on_start=False):
    if self.misc.isVisible():
        self._splitterMainSizes = self._splitterMain.sizes()
        self.misc.hide()
        widget = self.mainContainer.get_actual_widget()
        if widget:
            widget.setFocus()
    else:
        self.misc.show()
        self.misc.gain_focus()
",if widget :,108
"def is_checked_sls_template(template):
    if template.__contains__(""provider""):
        # Case provider is a dictionary
        if isinstance(template[""provider""], dict_node):
            if template[""provider""].get(""name"").lower() not in SUPPORTED_PROVIDERS:
                return False
        # Case provider is direct provider name
        if isinstance(template[""provider""], str_node):
            if template[""provider""] not in SUPPORTED_PROVIDERS:
                return False
        return True
    return False
","if template [ ""provider"" ] not in SUPPORTED_PROVIDERS :",131
"def check_index(self, is_sorted=True, unique=True, index=None):
    """"""Sanity checks""""""
    if not index:
        index = self.index
    if is_sorted:
        test = pd.DataFrame(lrange(len(index)), index=index)
        test_sorted = test.sort()
        if not test.index.equals(test_sorted.index):
            raise Exception(""Data is not be sorted"")
    if unique:
        if len(index) != len(index.unique()):
            raise Exception(""Duplicate index entries"")
",if not test . index . equals ( test_sorted . index ) :,142
"def _update_actions(self, *_ignored):
    """"""Updates menu actions to reflect the current layer's mode""""""
    if self._updating:
        return
    self._updating = True
    rootstack = self._model.layer_stack
    current = rootstack.current
    for mode, item in self._menu_items:
        active = mode == current.mode
        if bool(item.get_active()) != active:
            item.set_active(active)
        item.set_sensitive(mode in current.PERMITTED_MODES)
    self._updating = False
",if bool ( item . get_active ( ) ) != active :,140
"def _charlabels(self, options):
    """"""Get labels for characters (PRIVATE).""""""
    self.charlabels = {}
    opts = CharBuffer(options)
    while True:
        # get id and state
        w = opts.next_word()
        if w is None:  # McClade saves and reads charlabel-lists with terminal comma?!
            break
        identifier = self._resolve(w, set_type=CHARSET)
        state = quotestrip(opts.next_word())
        self.charlabels[identifier] = state
        # check for comma or end of command
        c = opts.next_nonwhitespace()
        if c is None:
            break
        elif c != "","":
            raise NexusError(""Missing ',' in line %s."" % options)
",if w is None :,198
"def get_and_set_titles(self):
    all_titles = []
    for page in self.pages:
        if page.orig_phrase != """":
            all_titles.append(page.orig_phrase)
            all_titles.append(page.orig_phrase_norm)
        if page.wiki_title != """":
            all_titles.append(page.wiki_title)
            all_titles.append(page.wiki_title_norm)
    return set(all_titles)
","if page . orig_phrase != """" :",127
"def get_content_length(download):
    try:
        meta = download.info()
        if hasattr(meta, ""getheaders"") and hasattr(meta.getheaders, ""Content-Length""):
            return int(meta.getheaders(""Content-Length"")[0])
        elif hasattr(download, ""getheader"") and download.getheader(""Content-Length""):
            return int(download.getheader(""Content-Length""))
        elif hasattr(meta, ""getheader"") and meta.getheader(""Content-Length""):
            return int(meta.getheader(""Content-Length""))
    except Exception:
        pass
    return 0
","if hasattr ( meta , ""getheaders"" ) and hasattr ( meta . getheaders , ""Content-Length"" ) :",149
"def connect_reader_to_writer(reader, writer):
    BUF_SIZE = 8192
    try:
        while True:
            data = await reader.read(BUF_SIZE)
            if not data:
                if not writer.transport.is_closing():
                    writer.write_eof()
                    await writer.drain()
                return
            writer.write(data)
            await writer.drain()
    except (OSError, asyncio.IncompleteReadError) as e:
        pass
",if not writer . transport . is_closing ( ) :,139
"def _record_shell(ex, files, bind_rez=True, print_msg=False):
    ex.source(context_file)
    if startup_sequence[""envvar""]:
        ex.unsetenv(startup_sequence[""envvar""])
    if add_rez and bind_rez:
        ex.interpreter._bind_interactive_rez()
    if print_msg and add_rez and not quiet:
        ex.info("""")
        ex.info(""You are now in a rez-configured environment."")
        ex.info("""")
        if system.is_production_rez_install:
            ex.command(""rezolve context"")
",if system . is_production_rez_install :,159
"def set_torrent_ratio(self, torrent_ids, ratio):
    try:
        if not self.connect():
            return False
        self.client.core.set_torrent_stop_at_ratio(torrent_ids, True).get()
        self.client.core.set_torrent_stop_ratio(torrent_ids, ratio).get()
    except Exception as err:
        return False
    finally:
        if self.client:
            self.disconnect()
    return True
",if self . client :,125
"def __decrypt_bin_sum(encrypted_bin_sum, cipher):
    # for feature_sum in encrypted_bin_sum:
    decrypted_list = {}
    for col_name, count_list in encrypted_bin_sum.items():
        new_list = []
        for event_count, non_event_count in count_list:
            if isinstance(event_count, PaillierEncryptedNumber):
                event_count = cipher.decrypt(event_count)
            if isinstance(non_event_count, PaillierEncryptedNumber):
                non_event_count = cipher.decrypt(non_event_count)
            new_list.append((event_count, non_event_count))
        decrypted_list[col_name] = new_list
    return decrypted_list
","if isinstance ( event_count , PaillierEncryptedNumber ) :",198
"def processVideo(self, track):
    video = Metadata(self)
    self.trackCommon(track, video)
    try:
        video.compression = track[""CodecID/string""].value
        if ""Video"" in track:
            video.width = track[""Video/PixelWidth/unsigned""].value
            video.height = track[""Video/PixelHeight/unsigned""].value
    except MissingField:
        pass
    self.addGroup(""video[]"", video, ""Video stream"")
","if ""Video"" in track :",120
"def check_br_addr(self, br):
    ips = {}
    cmd = ""ip a show dev %s"" % br
    for line in self.execute(cmd, sudo=True).split(""\n""):
        if line.strip().startswith(""inet ""):
            elems = [e.strip() for e in line.strip().split("" "")]
            ips[4] = elems[1]
        elif line.strip().startswith(""inet6 ""):
            elems = [e.strip() for e in line.strip().split("" "")]
            ips[6] = elems[1]
    return ips
","if line . strip ( ) . startswith ( ""inet "" ) :",149
"def _find_line_in_file(file_path, search_pattern):
    try:
        with open(file_path, ""r"", encoding=""utf-8"") as search_file:
            for line in search_file:
                if search_pattern in line:
                    return True
    except (OSError, IOError):
        pass
    return False
",if search_pattern in line :,94
"def setOption(self, key, value):
    if key in VALID_OPTIONS:
        old = self.getOption(key)
        result = VALID_OPTIONS[key](self, value)
        self.notifyOptionChanged(key, old, value)
        if result:
            return result[1]
        else:
            raise RopperError(""Invalid value for option %s: %s"" % (key, value))
    else:
        raise RopperError(""Invalid option"")
",if result :,124
"def _para_exploit(self, params, part):
    if len(params) == 0:
        arr = [""*"", ""config""] + self._configs.keys()
        return suggest(arr, part)
    if len(params) == 1:
        arr = []
        if params[0] == ""config"":
            arr = self._configs.keys()
        if params[0] == ""*"":
            arr = [""stopOnFirst""]
        return suggest(arr, part)
    return []
","if params [ 0 ] == ""*"" :",124
"def render(self, context):
    for var in self.vars:
        value = var.resolve(context, True)
        if value:
            first = render_value_in_context(value, context)
            if self.asvar:
                context[self.asvar] = first
                return """"
            return first
    return """"
",if self . asvar :,95
"def insertTestData(self, rows):
    for row in rows:
        if isinstance(row, Log):
            self.logs[row.id] = row.values.copy()
    for row in rows:
        if isinstance(row, LogChunk):
            lines = self.log_lines.setdefault(row.logid, [])
            # make sure there are enough slots in the list
            if len(lines) < row.last_line + 1:
                lines.append([None] * (row.last_line + 1 - len(lines)))
            row_lines = row.content.decode(""utf-8"").split(""\n"")
            lines[row.first_line : row.last_line + 1] = row_lines
",if len ( lines ) < row . last_line + 1 :,187
"def set_available_qty(self):
    for d in self.get(""required_items""):
        if d.source_warehouse:
            d.available_qty_at_source_warehouse = get_latest_stock_qty(
                d.item_code, d.source_warehouse
            )
        if self.wip_warehouse:
            d.available_qty_at_wip_warehouse = get_latest_stock_qty(
                d.item_code, self.wip_warehouse
            )
",if self . wip_warehouse :,136
"def add_pref_observer(self, name, callback):
    self.log.debug(""Adding pref observer for %s"", name)
    try:
        self._observers[name].add(callback)
    except KeyError:
        self._observers[name] = set([callback])
        if self._send:
            self._send(command=""global-prefs-observe"", add=[name])
        else:
            # We can't actually trigger prefs observer changes on document
            # level prefs; that's mostly okay, though, since we just pass
            # the whole prefs environment every time we do something with a
            # document instead.
            pass
",if self . _send :,165
"def __setattr__(self, key: str, value) -> None:
    try:
        object.__getattribute__(self, key)
        return object.__setattr__(self, key, value)
    except AttributeError:
        pass
    if (key,) in self._internal.column_labels:
        self[key] = value
    else:
        msg = ""Koalas doesn't allow columns to be created via a new attribute name""
        if is_testing():
            raise AssertionError(msg)
        else:
            warnings.warn(msg, UserWarning)
",if is_testing ( ) :,139
"def inverse_transform(self, X):
    results = []
    column_counter = 0
    for i, binarizer in enumerate(self.binarizers):
        n_cols = binarizer.classes_.shape[0]
        x_subset = X[:, column_counter : column_counter + n_cols]
        inv = binarizer.inverse_transform(x_subset)
        if len(inv.shape) == 1:
            inv = inv[:, np.newaxis]
        results.append(inv)
        column_counter += n_cols
    return np.concatenate(results, axis=1)
",if len ( inv . shape ) == 1 :,144
"def default_generator(
    self, dataset, epochs=1, mode=""fit"", deterministic=True, pad_batches=True
):
    for epoch in range(epochs):
        for (X_b, y_b, w_b, ids_b) in dataset.iterbatches(
            batch_size=self.batch_size,
            deterministic=deterministic,
            pad_batches=pad_batches,
        ):
            if mode == ""predict"":
                dropout = np.array(False)
            else:
                dropout = np.array(True)
            yield ([X_b, dropout], [y_b], [w_b])
","if mode == ""predict"" :",168
"def modif(dir, name, fun):
    """"""Call a substitution function""""""
    if name == ""*"":
        lst = []
        for y in "". Tools extras"".split():
            for x in os.listdir(os.path.join(dir, y)):
                if x.endswith("".py""):
                    lst.append(y + os.sep + x)
        for x in lst:
            modif(dir, x, fun)
        return
    filename = os.path.join(dir, name)
    with open(filename, ""r"") as f:
        txt = f.read()
    txt = fun(txt)
    with open(filename, ""w"") as f:
        f.write(txt)
","if x . endswith ( "".py"" ) :",186
"def find_last_match(view, what, start, end, flags=0):
    """"""Find last occurrence of `what` between `start`, `end`.""""""
    match = view.find(what, start, flags)
    new_match = None
    while match:
        new_match = view.find(what, match.end(), flags)
        if new_match and new_match.end() <= end:
            match = new_match
        else:
            return match
",if new_match and new_match . end ( ) <= end :,119
"def to_dynamic_cwd_tuple(x):
    """"""Convert to a canonical cwd_width tuple.""""""
    unit = ""c""
    if isinstance(x, str):
        if x[-1] == ""%"":
            x = x[:-1]
            unit = ""%""
        else:
            unit = ""c""
        return (float(x), unit)
    else:
        return (float(x[0]), x[1])
","if x [ - 1 ] == ""%"" :",111
"def get_lprobs_and_target(self, model, net_output, sample):
    lprobs = model.get_normalized_probs(net_output, log_probs=True)
    target = model.get_targets(sample, net_output)
    if self.ignore_prefix_size > 0:
        if getattr(lprobs, ""batch_first"", False):
            lprobs = lprobs[:, self.ignore_prefix_size :, :].contiguous()
            target = target[:, self.ignore_prefix_size :].contiguous()
        else:
            lprobs = lprobs[self.ignore_prefix_size :, :, :].contiguous()
            target = target[self.ignore_prefix_size :, :].contiguous()
    return lprobs.view(-1, lprobs.size(-1)), target.view(-1)
","if getattr ( lprobs , ""batch_first"" , False ) :",197
"def _charlabels(self, options):
    """"""Get labels for characters (PRIVATE).""""""
    self.charlabels = {}
    opts = CharBuffer(options)
    while True:
        # get id and state
        w = opts.next_word()
        if w is None:  # McClade saves and reads charlabel-lists with terminal comma?!
            break
        identifier = self._resolve(w, set_type=CHARSET)
        state = quotestrip(opts.next_word())
        self.charlabels[identifier] = state
        # check for comma or end of command
        c = opts.next_nonwhitespace()
        if c is None:
            break
        elif c != "","":
            raise NexusError(""Missing ',' in line %s."" % options)
",if c is None :,198
"def _parseContributors(self, roleType, Contributors):
    if Contributors is None:
        return None
    try:
        ret = []
        for item in Contributors[""items""]:
            if item[""role""] == roleType:
                ret.append(item[""name""])
        return ret
    except:
        return None
","if item [ ""role"" ] == roleType :",100
"def _data_interp(self):
    if self.pending_points:
        points = list(self.pending_points)
        if self.bounds_are_done:
            values = self.ip()(self._scale(points))
        else:
            # Without the bounds the interpolation cannot be done properly,
            # so we just set everything to zero.
            values = np.zeros((len(points), self.vdim))
        return points, values
    return np.zeros((0, 2)), np.zeros((0, self.vdim), dtype=float)
",if self . bounds_are_done :,144
"def _initCaseSets(self):
    self._cs = {}
    self._css = {}
    for cs in self._caseSets:
        if not self._cs.has_key(cs.CaseSetName):
            self._cs[cs.CaseSetName] = {}
            self._css[cs.CaseSetName] = cs
        else:
            raise Exception(""duplicate case set name"")
        for c in cs.Cases:
            idx = tuple(c.index)
            if not self._cs[cs.CaseSetName].has_key(idx):
                self._cs[cs.CaseSetName][idx] = c
            else:
                raise Exception(""duplicate case index"")
",if not self . _cs . has_key ( cs . CaseSetName ) :,178
"def _organize_data(self, data):
    temporary = {}
    for line in data.splitlines():
        category, _, value = line.partition("" "")
        if category in (""set"", ""tag""):
            key, _, value = value.partition("" "")
            temporary[key] = value
        else:
            temporary[category] = value
    return temporary
","if category in ( ""set"" , ""tag"" ) :",93
"def get(self):
    """"""Returns a simple HTML for contact form""""""
    if self.user:
        user_info = models.User.get_by_id(long(self.user_id))
        if user_info.name or user_info.last_name:
            self.form.name.data = user_info.name + "" "" + user_info.last_name
        if user_info.email:
            self.form.email.data = user_info.email
    params = {""exception"": self.request.get(""exception"")}
    return self.render_template(""boilerplate_contact.html"", **params)
",if user_info . email :,155
"def parseBamPEFDistributionFile(self, f):
    d = dict()
    lastsample = []
    for line in f[""f""].splitlines():
        cols = line.rstrip().split(""\t"")
        if cols[0] == ""#bamPEFragmentSize"":
            continue
        elif cols[0] == ""Size"":
            continue
        else:
            s_name = self.clean_s_name(cols[2].rstrip().split(""/"")[-1], f[""root""])
            if s_name != lastsample:
                d[s_name] = dict()
                lastsample = s_name
            d[s_name].update({self._int(cols[0]): self._int(cols[1])})
    return d
","elif cols [ 0 ] == ""Size"" :",194
"def _related(self):
    if self.__related is None:
        results = requests.get(
            f""{self._wordnet_corpus_reader.host()}/api/synsets/{self.pos()}/{self.offset()}/relations/?format=json"",
            timeout=(30.0, 90.0),
        )
        if results and len(results.json()[""results""]) != 0:
            self.__related = results.json()[""results""][0][""relations""]
        else:
            self.__related = []
    return self.__related
","if results and len ( results . json ( ) [ ""results"" ] ) != 0 :",135
"def autoname(self):
    if self.company:
        suffix = "" - "" + frappe.get_cached_value(""Company"", self.company, ""abbr"")
        if not self.warehouse_name.endswith(suffix):
            self.name = self.warehouse_name + suffix
    else:
        self.name = self.warehouse_name
",if not self . warehouse_name . endswith ( suffix ) :,88
"def escape_string(self, value):
    value = EscapedString.promote(value)
    value = value.expanduser()
    result = """"
    for is_literal, txt in value.strings:
        if is_literal:
            txt = pipes.quote(txt)
            if not txt.startswith(""'""):
                txt = ""'%s'"" % txt
        else:
            txt = txt.replace(""\\"", ""\\\\"")
            txt = txt.replace('""', '\\""')
            txt = '""%s""' % txt
        result += txt
    return result
",if is_literal :,139
"def downgrade_wsgi_ux_to_1x(environ):
    """"""Return a new environ dict for WSGI 1.x from the given WSGI u.x environ.""""""
    env1x = {}
    url_encoding = environ[ntou(""wsgi.url_encoding"")]
    for k, v in list(environ.items()):
        if k in [ntou(""PATH_INFO""), ntou(""SCRIPT_NAME""), ntou(""QUERY_STRING"")]:
            v = v.encode(url_encoding)
        elif isinstance(v, unicodestr):
            v = v.encode(""ISO-8859-1"")
        env1x[k.encode(""ISO-8859-1"")] = v
    return env1x
","elif isinstance ( v , unicodestr ) :",172
"def __repr__(self):
    rt = ""Network         Netmask         Gateway         Iface           Output IP\n""
    for net, msk, gw, iface, addr in self.routes:
        rt += ""%-15s %-15s %-15s %-15s %-15s\n"" % (
            ltoa(net),
            ltoa(msk),
            gw,
            iface,
            addr,
        )
    return rt
","iface ,",148
"def nearest_sources_Point(
    self, point: Point, max_dist=float(""inf"")
):  # sys.float_info.max):
    bp, bn, bi, bd = None, None, None, None
    for rfsource in self.rfsources:
        if not self.get_rfsource_snap(rfsource):
            continue
        hp, hn, hi, hd = rfsource.nearest(point, max_dist=max_dist)
        if bp is None or (hp is not None and hd < bd):
            bp, bn, bi, bd = hp, hn, hi, hd
    return (bp, bn, bi, bd)
",if not self . get_rfsource_snap ( rfsource ) :,163
"def restoreParent(self):
    if self.sid.isRoot:
        return
    with self.suspendMouseButtonNavigation():
        confirm, opt = self.confirmRestore((self.path,))
        if not confirm:
            return
        if opt[""delete""] and not self.confirmDelete(warnRoot=self.path == ""/""):
            return
    rd = RestoreDialog(self, self.sid, self.path, **opt)
    rd.exec()
","if opt [ ""delete"" ] and not self . confirmDelete ( warnRoot = self . path == ""/"" ) :",114
"def connect(self):
    if self.reserved_ports:
        self.get_reserved_port()
    self.sock.settimeout(10)
    max_attempts = 3
    for i in range(max_attempts):
        try:
            rv = super(WSClient, self).connect()
        except OSError as e:
            # Lets retry a few times in case the error is
            # [Errno 48] Address already in use
            # which I believe may be caused by a race condition
            if e.errno == errno.EADDRINUSE and i < max_attempts - 1:
                continue
            raise
        else:
            break
    if self.sock:
        self.sock.settimeout(None)
    return rv
",if e . errno == errno . EADDRINUSE and i < max_attempts - 1 :,193
"def step(self, action):
    assert self.action_space.contains(action)
    if self._state == 4:
        if action and self._case:
            return self._state, 10.0, True, {}
        else:
            return self._state, -10, True, {}
    else:
        if action:
            if self._state == 0:
                self._state = 2
            else:
                self._state += 1
        elif self._state == 2:
            self._state = self._case
    return self._state, -1, False, {}
",elif self . _state == 2 :,157
"def process(self):
    inputs = self.node.inputs
    outputs = self.node.outputs
    data = [s.sv_get()[0] for s in inputs]
    for socket, ref in zip(outputs, self.outputs):
        if socket.links:
            func = getattr(self, ref[2])
            out = tuple(itertools.starmap(func, sv_zip_longest(*data)))
            socket.sv_set(out)
",if socket . links :,113
"def filter_queryset(self, request, queryset, view):
    if (
        self.filter_name in request.QUERY_PARAMS
        or self.exclude_param_name in request.QUERY_PARAMS
    ):
        projects_ids_subquery = self.filter_user_projects(request)
        if projects_ids_subquery:
            queryset = queryset.filter(project_id__in=projects_ids_subquery)
    return super().filter_queryset(request, queryset, view)
",if projects_ids_subquery :,118
"def _is_port_in_range(self, ports_list):
    for port_range in ports_list[0]:
        port = force_int(port_range)
        if port and self.port == port:
            return True
        if port is None and ""-"" in port_range:
            try:
                [from_port, to_port] = port_range.split(""-"")
                if int(from_port) <= self.port <= int(to_port):
                    return True
            except Exception:
                return CheckResult.UNKNOWN
    return False
","if port is None and ""-"" in port_range :",154
"def apply_to(cls, lexer):
    # Apply a font for all styles
    lexer.setFont(Font().load())
    for name, font in cls.__dict__.items():
        if not isinstance(font, Font):
            continue
        if hasattr(lexer, name):
            style_num = getattr(lexer, name)
            lexer.setColor(QColor(font.color), style_num)
            lexer.setEolFill(True, style_num)
            lexer.setPaper(QColor(font.paper), style_num)
            lexer.setFont(font.load(), style_num)
","if hasattr ( lexer , name ) :",158
"def set_columns(worksheet, c, lengths):
    for col, j in enumerate(c):
        if j == ""Value"":
            j = "" "" * 18
        if j == ""Description"":
            j = ""Descr""
        lengths[col] = max(len(j) + 5, lengths[col])
        worksheet.set_column(col, col, lengths[col])
","if j == ""Description"" :",101
"def _remove_listners(self):
    object = self.object
    kids = self.children_cache
    for key, val in kids.items():
        if isinstance(val, tvtk.Collection):
            vtk_obj = tvtk.to_vtk(val)
            messenger.disconnect(vtk_obj, ""ModifiedEvent"", self._notify_children)
        else:
            object.on_trait_change(self._notify_children, key, remove=True)
","if isinstance ( val , tvtk . Collection ) :",123
"def add(self, undoinfo, msg=None):
    if not undoinfo:
        return
    if msg is not None:
        if isinstance(undoinfo[0], str):
            # replace message
            undoinfo = (msg,) + undoinfo[1:]
        elif isinstance(undoinfo, tuple):
            undoinfo = (msg,) + undoinfo
        else:
            undoinfo = (msg, undoinfo)
        f = 1
    else:
        f = int(isinstance(undoinfo[0], str))
    assert (
        isinstance(undoinfo, list)
        or callable(undoinfo[f])
        or isinstance(undoinfo[f], list)
    )
    self.undoList.append(undoinfo)
    del self.redoList[:]
","if isinstance ( undoinfo [ 0 ] , str ) :",198
"def assert_last_day(self, period_end):
    # 30 days has september, april, june and november
    if period_end.month in [9, 4, 6, 11]:
        self.assertEqual(period_end.day, 30)
    # all the rest have 31, except for february
    elif period_end.month != 2:
        self.assertEqual(period_end.day, 31)
    else:
        if calendar.isleap(period_end.year):
            self.assertEqual(period_end.day, 29)
        else:
            self.assertEqual(period_end.day, 28)
",if calendar . isleap ( period_end . year ) :,165
"def remove_callback(self, callback, events=None):
    if events is None:
        for event in self._plugin_lifecycle_callbacks:
            if callback in self._plugin_lifecycle_callbacks[event]:
                self._plugin_lifecycle_callbacks[event].remove(callback)
    else:
        if isinstance(events, basestring):
            events = [events]
        for event in events:
            if callback in self._plugin_lifecycle_callbacks[event]:
                self._plugin_lifecycle_callbacks[event].remove(callback)
","if isinstance ( events , basestring ) :",148
"def get_count(self, peek=False):
    if self.argument_supplied:
        count = self.argument_value
        if self.argument_negative:
            if count == 0:
                count = -1
            else:
                count = -count
            if not peek:
                self.argument_negative = False
        if not peek:
            self.argument_supplied = False
    else:
        count = 1
    return count
",if count == 0 :,126
"def is_alive(self):
    if not self.runqemu:
        return False
    if os.path.isfile(self.qemu_pidfile):
        f = open(self.qemu_pidfile, ""r"")
        qemu_pid = f.read()
        f.close()
        qemupid = int(qemu_pid)
        if os.path.exists(""/proc/"" + str(qemupid)):
            self.qemupid = qemupid
            return True
    return False
","if os . path . exists ( ""/proc/"" + str ( qemupid ) ) :",140
"def contains(self, other_route):
    if isinstance(other_route, list):
        return self.to_list()[0 : len(other_route)] == other_route
    # This only works before merging
    assert len(other_route.outgoing) <= 1, ""contains(..) cannot be called after a merge""
    assert len(self.outgoing) <= 1, ""contains(..) cannot be called after a merge""
    if other_route.task_spec == self.task_spec:
        if other_route.outgoing and self.outgoing:
            return self.outgoing[0].contains(other_route.outgoing[0])
        elif self.outgoing:
            return True
        elif not other_route.outgoing:
            return True
    return False
",elif not other_route . outgoing :,184
"def _add_connection(self, connection, uri=None):
    with self._connections_lock:
        connection_id = connection.connection_id
        if connection_id not in self._connections:
            self._connections[connection_id] = ConnectionInfo(
                ConnectionType.OUTBOUND_CONNECTION, connection, uri, None, None
            )
",if connection_id not in self . _connections :,90
"def view(input_path):
    if not exists(input_path):
        raise IOError(""{0} not found"".format(input_path))
    ua = None
    bundle_info = None
    try:
        archive = archive_factory(input_path)
        if archive is None:
            raise NotMatched(""No matching archive type found"")
        ua = archive.unarchive_to_temp()
        bundle_info = ua.bundle.info
    finally:
        if ua is not None:
            ua.remove()
    return bundle_info
",if ua is not None :,139
"def _expect_fail_and_reconnect(self, num_reconnects, fail_last=False):
    self._fake_backend.connect.expect_call(**_CONNECT_KWARGS).and_raises(
        FakeDatabaseError()
    )
    for i in xrange(num_reconnects):
        time.sleep.expect_call(_RECONNECT_DELAY)
        if i < num_reconnects - 1:
            self._expect_reconnect(fail=True)
        else:
            self._expect_reconnect(fail=fail_last)
",if i < num_reconnects - 1 :,139
"def _trigger_step(self):
    if self._enable_step:
        if self.local_step != self.trainer.steps_per_epoch - 1:
            # not the last step
            self._trigger()
        else:
            if not self._enable_epoch:
                self._trigger()
",if not self . _enable_epoch :,83
"def draw_label(self):
    if self.hide:
        if not self.inputs[""Seed""].is_linked:
            seed = "" + ({0})"".format(str(int(self.seed)))
        else:
            seed = "" + seed(s)""
        return self.noise_type.title() + seed
    else:
        return self.label or self.name
","if not self . inputs [ ""Seed"" ] . is_linked :",97
"def get_adapter(self, pattern=None):
    adapters = self.get_adapters()
    if pattern is None:
        if len(adapters):
            return adapters[0]
        else:
            raise DBusNoSuchAdapterError(""No adapter(s) found"")
    else:
        for adapter in adapters:
            path = adapter.get_object_path()
            if path.endswith(pattern) or adapter[""Address""] == pattern:
                return adapter
        raise DBusNoSuchAdapterError(""No adapters found with pattern: %s"" % pattern)
","if path . endswith ( pattern ) or adapter [ ""Address"" ] == pattern :",144
"def substituteargs(self, pattern, replacement, old):
    new = []
    for k in range(len(replacement)):
        item = replacement[k]
        newitem = [item[0], item[1], item[2]]
        for i in range(3):
            if item[i] == ""*"":
                newitem[i] = old[k][i]
            elif item[i][:1] == ""$"":
                index = int(item[i][1:]) - 1
                newitem[i] = old[index][i]
        new.append(tuple(newitem))
    ##self.report(""old: %r"", old)
    ##self.report(""new: %r"", new)
    return new
","if item [ i ] == ""*"" :",187
"def profiling_startup():
    if ""--profile-sverchok-startup"" in sys.argv:
        global _profile_nesting
        profile = None
        try:
            profile = get_global_profile()
            _profile_nesting += 1
            if _profile_nesting == 1:
                profile.enable()
            yield profile
        finally:
            _profile_nesting -= 1
            if _profile_nesting == 0 and profile is not None:
                profile.disable()
            dump_stats(file_path=""sverchok_profile.txt"")
            save_stats(""sverchok_profile.prof"")
    else:
        yield None
",if _profile_nesting == 0 and profile is not None :,180
"def align(size):
    if size <= 4096:
        # Small
        if is_power2(size):
            return size
        elif size < 128:
            return min_ge(range(16, 128 + 1, 16), size)
        elif size < 512:
            return min_ge(range(192, 512 + 1, 64), size)
        else:
            return min_ge(range(768, 4096 + 1, 256), size)
    elif size < 4194304:
        # Large
        return min_ge(range(4096, 4194304 + 1, 4096), size)
    else:
        # Huge
        return min_ge(range(4194304, 536870912 + 1, 4194304), size)
",elif size < 512 :,195
"def _validate(self, event):
    new = self.value
    if new is not None and (
        (self.start is not None and self.start > new)
        or (self.end is not None and self.end < new)
    ):
        value = datetime.strftime(new, self.format)
        start = datetime.strftime(self.start, self.format)
        end = datetime.strftime(self.end, self.format)
        if event:
            self.value = event.old
        raise ValueError(
            ""DatetimeInput value must be between {start} and {end}, ""
            ""supplied value is {value}"".format(start=start, end=end, value=value)
        )
",if event :,183
"def parse(filename):
    dead_links = []
    with open(filename, ""r"") as file_:
        for line in file_.readlines():
            res = reference_line.search(line)
            if res:
                if not exists(res.group(1)):
                    dead_links.append(res.group(1))
    return dead_links
",if not exists ( res . group ( 1 ) ) :,96
"def __getstate__(self):
    state = super(_GeneralExpressionDataImpl, self).__getstate__()
    for i in _GeneralExpressionDataImpl.__expression_slots__:
        state[i] = getattr(self, i)
    if safe_mode:
        state[""_parent_expr""] = None
        if self._parent_expr is not None:
            _parent_expr = self._parent_expr()
            if _parent_expr is not None:
                state[""_parent_expr""] = _parent_expr
    return state
",if _parent_expr is not None :,132
"def insertText(self, data, parent=None):
    data = data
    if parent != self:
        _base.TreeBuilder.insertText(self, data, parent)
    else:
        # HACK: allow text nodes as children of the document node
        if hasattr(self.dom, ""_child_node_types""):
            if Node.TEXT_NODE not in self.dom._child_node_types:
                self.dom._child_node_types = list(self.dom._child_node_types)
                self.dom._child_node_types.append(Node.TEXT_NODE)
        self.dom.appendChild(self.dom.createTextNode(data))
",if Node . TEXT_NODE not in self . dom . _child_node_types :,170
"def main(args):
    from argparse import ArgumentParser
    from sys import stdin, stdout
    # TODO: Doc!
    argparser = ArgumentParser()
    argparser.add_argument(""-u"", ""--unescape"", action=""store_true"")
    argp = argparser.parse_args(args[1:])
    for line in (l.rstrip(""\n"") for l in stdin):
        if argp.unescape:
            r = unescape(line)
        else:
            r = escape(line)
        stdout.write(r)
        stdout.write(""\n"")
",if argp . unescape :,138
"def validate_user_json(value, json_schema):
    try:
        jsonschema.validate(value, from_json(json_schema))
    except jsonschema.ValidationError as e:
        if len(e.path) > 1:
            raise InvalidModelValueError(
                ""For '{}' the field value {}"".format(e.path[-1], e.message)
            )
        raise InvalidModelValueError(e.message)
    except jsonschema.SchemaError as e:
        raise InvalidModelValueError(e.message)
    validate_dates(value)
",if len ( e . path ) > 1 :,140
"def test_mode(self):
    with support.temp_umask(0o002):
        base = support.TESTFN
        parent = os.path.join(base, ""dir1"")
        path = os.path.join(parent, ""dir2"")
        os.makedirs(path, 0o555)
        self.assertTrue(os.path.exists(path))
        self.assertTrue(os.path.isdir(path))
        if os.name != ""nt"":
            self.assertEqual(os.stat(path).st_mode & 0o777, 0o555)
            self.assertEqual(os.stat(parent).st_mode & 0o777, 0o775)
","if os . name != ""nt"" :",169
"def __get_annotations(self):
    if not hasattr(self, ""_annotations""):
        self._annotations = _retrieve_annotations(
            self._adaptor, self._primary_id, self._taxon_id
        )
        if self._identifier:
            self._annotations[""gi""] = self._identifier
        if self._division:
            self._annotations[""data_file_division""] = self._division
    return self._annotations
",if self . _identifier :,110
"def string(self):
    """"""Returns a PlayString in string format from the Patterns values""""""
    string = """"
    for item in self.data:
        if isinstance(item, (PGroup, GeneratorPattern)):
            string += item.string()
        elif isinstance(item, Pattern):
            string += (
                ""(""
                + """".join(
                    [
                        (s.string() if hasattr(s, ""string"") else str(s))
                        for s in item.data
                    ]
                )
                + "")""
            )
        else:
            string += str(item)
    return string
","if isinstance ( item , ( PGroup , GeneratorPattern ) ) :",183
"def __getattribute__(self, item):
    try:
        val = self[item]
        if isinstance(val, str):
            val = import_string(val)
        elif isinstance(val, (list, tuple)):
            val = [import_string(v) if isinstance(v, str) else v for v in val]
        self[item] = val
    except KeyError:
        val = super(ObjDict, self).__getattribute__(item)
    return val
","if isinstance ( val , str ) :",118
"def get_identifiers(self):
    ids = []
    ifaces = [i[""name""] for i in self.middleware.call_sync(""interface.query"")]
    for entry in glob.glob(f""{self._base_path}/interface-*""):
        ident = entry.rsplit(""-"", 1)[-1]
        if ident not in ifaces:
            continue
        if os.path.exists(os.path.join(entry, ""if_octets.rrd"")):
            ids.append(ident)
    ids.sort(key=RRDBase._sort_disks)
    return ids
",if ident not in ifaces :,143
"def save_new_objects(self, commit=True):
    self.new_objects = []
    for form in self.extra_forms:
        if not form.has_changed():
            continue
        # If someone has marked an add form for deletion, don't save the
        # object.
        if self.can_delete and self._should_delete_form(form):
            continue
        self.new_objects.append(self.save_new(form, commit=commit))
        if not commit:
            self.saved_forms.append(form)
    return self.new_objects
",if self . can_delete and self . _should_delete_form ( form ) :,151
"def _get_seccomp_whitelist(self):
    whitelist = [False] * MAX_SYSCALL_NUMBER
    index = _SYSCALL_INDICIES[NATIVE_ABI]
    for i in range(SYSCALL_COUNT):
        # Ensure at least one syscall traps.
        # Otherwise, a simple assembly program could terminate without ever trapping.
        if i in (sys_exit, sys_exit_group):
            continue
        handler = self._security.get(i, DISALLOW)
        for call in translator[i][index]:
            if call is None:
                continue
            if isinstance(handler, int):
                whitelist[call] = handler == ALLOW
    return whitelist
",if call is None :,185
"def start_check(aggregate, out):
    """"""Start checking in background and write encoded output to out.""""""
    # check in background
    t = threading.Thread(target=director.check_urls, args=(aggregate,))
    t.start()
    # time to wait for new data
    sleep_seconds = 2
    # current running time
    run_seconds = 0
    while not aggregate.is_finished():
        yield out.get_data()
        time.sleep(sleep_seconds)
        run_seconds += sleep_seconds
        if run_seconds > MAX_REQUEST_SECONDS:
            director.abort(aggregate)
            break
    yield out.get_data()
",if run_seconds > MAX_REQUEST_SECONDS :,166
"def _prune_resource_identifiers(self, all_resources, all_operations):
    used_identifiers = self._get_identifiers_referenced_by_operations(all_operations)
    for resource, resource_data in list(all_resources.items()):
        identifiers = resource_data[""resourceIdentifier""]
        known_ids_for_resource = used_identifiers.get(resource, set())
        for identifier_name in list(identifiers):
            if identifier_name not in known_ids_for_resource:
                del identifiers[identifier_name]
        if not identifiers:
            # If there's no identifiers used by an autocompletion
            # operation, then we don't need the resource.
            del all_resources[resource]
",if identifier_name not in known_ids_for_resource :,179
"def has_valid_checksum(self, number):
    given_number, given_checksum = number[:-1], number[-1]
    calculated_checksum = 0
    parameter = 7
    for item in given_number:
        fragment = str(int(item) * parameter)
        if fragment.isalnum():
            calculated_checksum += int(fragment[-1])
        if parameter == 1:
            parameter = 7
        elif parameter == 3:
            parameter = 1
        elif parameter == 7:
            parameter = 3
    return str(calculated_checksum)[-1] == given_checksum
",if parameter == 1 :,147
"def _poll_until_not(url, pending_statuses, err_msg):
    while True:
        result, _, _ = _do_request(url, err_msg=err_msg)
        if result[""status""] in pending_statuses:
            time.sleep(2)
            continue
        return result
","if result [ ""status"" ] in pending_statuses :",80
"def wrapper(request, *args, **kw):
    if switch_is_active(""disable-bigquery""):
        if kw.get(""format"") == ""csv"":
            response = http.HttpResponse(content_type=""text/csv; charset=utf-8"")
        else:
            response = http.HttpResponse(content_type=""application/json"", content=""[]"")
        response.status_code = 503
        return response
    return f(request, *args, **kw)
","if kw . get ( ""format"" ) == ""csv"" :",119
"def completion_safe_apply(ctx, f, args):
    from guild import config
    with config.SetGuildHome(ctx.parent.params.get(""guild_home"")):
        try:
            return f(*args)
        except (Exception, SystemExit):
            if os.getenv(""_GUILD_COMPLETE_DEBUG"") == ""1"":
                raise
            return None
","if os . getenv ( ""_GUILD_COMPLETE_DEBUG"" ) == ""1"" :",99
"def configure(self, **kw):
    """"""Configure the image.""""""
    res = ()
    for k, v in _cnfmerge(kw).items():
        if v is not None:
            if k[-1] == ""_"":
                k = k[:-1]
            if hasattr(v, ""__call__""):
                v = self._register(v)
            elif k in (""data"", ""maskdata""):
                v = self.tk._createbytearray(v)
            res = res + (""-"" + k, v)
    self.tk.call((self.name, ""config"") + res)
",if v is not None :,154
"def _editor_lower(self):
    editorWidget = main_container.MainContainer().get_actual_editor()
    if editorWidget:
        editorWidget.textCursor().beginEditBlock()
        if editorWidget.textCursor().hasSelection():
            text = editorWidget.textCursor().selectedText().lower()
        else:
            text = editorWidget._text_under_cursor().lower()
            editorWidget.moveCursor(QTextCursor.StartOfWord)
            editorWidget.moveCursor(QTextCursor.EndOfWord, QTextCursor.KeepAnchor)
        editorWidget.textCursor().insertText(text)
        editorWidget.textCursor().endEditBlock()
",if editorWidget . textCursor ( ) . hasSelection ( ) :,169
"def on_key_release(self, symbol, modifiers):
    if symbol == key.LEFT or symbol == key.RIGHT:
        self.value = not self.value
        self.text.text = self.get_label()
        self.toggle_func(self.value)
        if enable_sound:
            bullet_sound.play()
",if enable_sound :,86
"def remove_checker(self, namespace, checker):
    for c in pyomo.core.check.ModelCheckRunner._checkers(all=True):
        if c._checkerName() == checker:
            if namespace.checkers.get(c._checkerPackage(), None) is not None:
                for i in range(
                    namespace.checkers[c._checkerPackage()].count(c._checkerName())
                ):
                    namespace.checkers[c._checkerPackage()].remove(c._checkerName())
",if c . _checkerName ( ) == checker :,129
"def find_executable(names):
    # Given a list of executable names, find the first one that is available
    # as an executable file, on the path.
    for name in names:
        fpath, fname = os.path.split(name)
        if fpath:
            # The given name is absolute.
            if is_executable(name):
                return name
        else:
            # Try to find the name on the PATH
            for path in os.environ[""PATH""].split(os.pathsep):
                exe_file = os.path.join(path, name)
                if is_executable(exe_file):
                    return exe_file
    # Could not find it :(
    return None
",if fpath :,186
"def run(self):
    while True:
        self.finished.wait(self.interval)
        if self.finished.isSet():
            return
        try:
            self.function(*self.args, **self.kwargs)
        except Exception:
            if self.bus:
                self.bus.log(
                    ""Error in perpetual timer thread function %r."" % self.function,
                    level=40,
                    traceback=True,
                )
            # Quit on first error to avoid massive logs.
            raise
",if self . bus :,157
"def get_user_object(self, user_id, group):
    if user_id:
        user = OSFUser.load(user_id)
        if not user:
            raise exceptions.NotFound(
                detail=""User with id {} not found."".format(user_id)
            )
        if group.has_permission(user, ""member""):
            raise exceptions.ValidationError(
                detail=""User is already a member of this group.""
            )
        return user
    return user_id
","if group . has_permission ( user , ""member"" ) :",135
"def build_term_table(spec):
    try:
        return _term_tables_cache[spec]
    except KeyError:
        tbl = {}
        terms = {}
        i = 0
        for t in spec:
            which = terms.setdefault(t, 0)
            tbl[t, which] = i
            tbl[""%s_%d"" % (t, which)] = i
            if which == 0:
                tbl[t] = i
            terms[t] += 1
            i += 1
        _term_tables_cache[spec] = tbl
        return tbl
",if which == 0 :,158
"def GetQualifiedWsdlName(type):
    with _lazyLock:
        wsdlNSAndName = _wsdlNameMap.get(type)
        if wsdlNSAndName:
            return wsdlNSAndName
        else:
            if issubclass(type, list):
                ns = GetWsdlNamespace(type.Item._version)
                return (ns, ""ArrayOf"" + Capitalize(type.Item._wsdlName))
            else:
                ns = GetWsdlNamespace(type._version)
                return (ns, type._wsdlName)
","if issubclass ( type , list ) :",158
"def train(config, checkpoint_dir=None):
    restored = bool(checkpoint_dir)
    itr = 0
    if checkpoint_dir:
        with open(os.path.join(checkpoint_dir, ""ckpt.log""), ""r"") as f:
            itr = int(f.read()) + 1
    for i in range(itr, 10):
        if i == 5 and not restored:
            raise Exception(""try to fail me"")
        with tune.checkpoint_dir(step=itr) as checkpoint_dir:
            checkpoint_path = os.path.join(checkpoint_dir, ""ckpt.log"")
            with open(checkpoint_path, ""w"") as f:
                f.write(str(i))
        tune.report(test=i, training_iteration=i)
",if i == 5 and not restored :,198
"def _process_events(self, event_list):
    for key, mask in event_list:
        fileobj, (reader, writer) = key.fileobj, key.data
        if mask & selectors.EVENT_READ and reader is not None:
            if reader._cancelled:
                self.remove_reader(fileobj)
            else:
                self._add_callback(reader)
        if mask & selectors.EVENT_WRITE and writer is not None:
            if writer._cancelled:
                self.remove_writer(fileobj)
            else:
                self._add_callback(writer)
",if writer . _cancelled :,158
"def _validate_mappings(self):
    # Validate mapping references
    for m in self.mapping.mapping_rules:
        for policy_id in m.policy_ids:
            if policy_id not in self.policies:
                raise ReferencedObjectNotFoundError(
                    reference_id=policy_id, reference_type=""policy""
                )
        for w in m.whitelist_ids:
            if w not in self.whitelists:
                raise ReferencedObjectNotFoundError(
                    reference_id=w, reference_type=""whitelist""
                )
",if w not in self . whitelists :,155
"def _transform_backward(graph, op):
    no_dequanted_input_vars = True
    for var_node in op.inputs:
        if var_node.name() in dequantized_vars:
            dequant_var_node = dequantized_vars[var_node.name()]
            graph.update_input_link(var_node, dequant_var_node, op)
            no_dequanted_input_vars = False
    if no_dequanted_input_vars:
        raise ValueError(""There is no dequanted inputs for op %s."" % (op.name()))
",if var_node . name ( ) in dequantized_vars :,150
"def should_use_pty(self, pty=False, fallback=True):
    use_pty = False
    if pty:
        use_pty = True
        # TODO: pass in & test in_stream, not sys.stdin
        if not has_fileno(sys.stdin) and fallback:
            if not self.warned_about_pty_fallback:
                err = ""WARNING: stdin has no fileno; falling back to non-pty execution!\n""  # noqa
                sys.stderr.write(err)
                self.warned_about_pty_fallback = True
            use_pty = False
    return use_pty
",if not has_fileno ( sys . stdin ) and fallback :,171
"def _get_default_factory(self, attribute_name: str) -> Any:
    if hasattr(self, attribute_name):
        if str(getattr(self, attribute_name)).startswith(""${""):
            return str(getattr(self, attribute_name))
        elif str(self.__dataclass_fields__[attribute_name].default).startswith(""${""):
            return str(self.__dataclass_fields__[attribute_name].default)
        elif (
            getattr(self, attribute_name)
            != self.__dataclass_fields__[attribute_name].default_factory()
        ):
            return getattr(self, attribute_name)
    return self.__dataclass_fields__[attribute_name].default_factory()
","if str ( getattr ( self , attribute_name ) ) . startswith ( ""${"" ) :",173
"def create_row_processor(
    self, context, path, loadopt, mapper, result, adapter, populators
):
    # look through list of columns represented here
    # to see which, if any, is present in the row.
    for col in self.columns:
        if adapter:
            col = adapter.columns[col]
        getter = result._getter(col, False)
        if getter:
            populators[""quick""].append((self.key, getter))
            break
    else:
        populators[""expire""].append((self.key, True))
",if adapter :,145
"def test_finds_multiple_songs(self):
    for _, album in albums_in_dir(self.base):
        n = re.search(br""album(.)song"", album[0]).group(1)
        if n == b""1"":
            self.assertEqual(len(album), 2)
        else:
            self.assertEqual(len(album), 1)
","if n == b""1"" :",95
"def _should_update_cache(self, request, response):
    if not hasattr(request, ""_cache_update_cache"") or not request._cache_update_cache:
        return False
    if self.cache_anonymous_only and has_vary_header(response, ""Cookie""):
        assert hasattr(
            request, ""user""
        ), ""The Django cache middleware with CACHE_MIDDLEWARE_ANONYMOUS_ONLY=True requires authentication middleware to be installed. Edit your MIDDLEWARE_CLASSES setting to insert 'django.contrib.auth.middleware.AuthenticationMiddleware' before the CacheMiddleware.""
        if request.user.is_authenticated():
            # Don't cache user-variable requests from authenticated users.
            return False
    return True
",if request . user . is_authenticated ( ) :,180
"def break_next_call(symbol_regex=None):
    while pwndbg.proc.alive:
        ins = break_next_branch()
        if not ins:
            break
        # continue if not a call
        if capstone.CS_GRP_CALL not in ins.groups:
            continue
        # return call if we don't search for a symbol
        if not symbol_regex:
            return ins
        # return call if we match target address
        if ins.target_const and re.match(""%s$"" % symbol_regex, hex(ins.target)):
            return ins
        # return call if we match symbol name
        if ins.symbol and re.match(""%s$"" % symbol_regex, ins.symbol):
            return ins
",if not ins :,193
"def parser(cls, buf, offset):
    type_, len_, vendor = struct.unpack_from(
        ofproto.OFP_ACTION_VENDOR_HEADER_PACK_STR, buf, offset
    )
    data = buf[(offset + ofproto.OFP_ACTION_VENDOR_HEADER_SIZE) : offset + len_]
    if vendor == ofproto_common.NX_EXPERIMENTER_ID:
        obj = NXAction.parse(data)  # noqa
    else:
        cls_ = cls._ACTION_VENDORS.get(vendor, None)
        if cls_ is None:
            obj = OFPActionVendorUnknown(vendor, data)
        else:
            obj = cls_.parser(buf, offset)
    obj.len = len_
    return obj
",if cls_ is None :,196
"def remove_empty_files(root_path):
    """"""Removes empty files in a path recursively""""""
    for directory, _, filenames in walk(root_path):
        for filename in filenames:
            path = os.path.join(directory, filename)
            if os.path.getsize(path) > 0:
                continue
            try:
                os.remove(path)
            except:
                logs.log_error(
                    ""Unable to remove the empty file: %s (%s).""
                    % (path, sys.exc_info()[0])
                )
",if os . path . getsize ( path ) > 0 :,160
"def _test_set_ipv4_src(self, ip, mask=None):
    header = ofproto.OXM_OF_IPV4_SRC
    match = OFPMatch()
    ip = unpack(""!I"", socket.inet_aton(ip))[0]
    if mask is None:
        match.set_ipv4_src(ip)
    else:
        mask = unpack(""!I"", socket.inet_aton(mask))[0]
        if (mask + 1) >> 32 != 1:
            header = ofproto.OXM_OF_IPV4_SRC_W
        match.set_ipv4_src_masked(ip, mask)
    self._test_serialize_and_parser(match, header, ip, mask)
",if ( mask + 1 ) >> 32 != 1 :,182
"def is_valid_block(self):
    """"""check wheter the block is valid in the current position""""""
    for i in range(self.block.x):
        for j in range(self.block.x):
            if self.block.get(i, j):
                if self.block.pos.x + i < 0:
                    return False
                if self.block.pos.x + i >= COLUMNS:
                    return False
                if self.block.pos.y + j < 0:
                    return False
                if self.map.get((self.block.pos.x + i, self.block.pos.y + j), False):
                    return False
    return True
","if self . map . get ( ( self . block . pos . x + i , self . block . pos . y + j ) , False ) :",192
"def __init__(self, *args, **kwargs):
    dict.__init__(self, *args, **kwargs)
    for key, value in self.items():
        if not isinstance(key, string_types):
            raise TypeError(""key must be a str, not {}"".format(type(key)))
        if not isinstance(value, NUMERIC_TYPES):
            raise TypeError(""value must be a NUMERIC_TYPES, not {}"".format(type(value)))
        if not isinstance(value, float):
            self[key] = float(value)
","if not isinstance ( key , string_types ) :",132
"def refresh_committed_offsets_if_needed(self):
    """"""Fetch committed offsets for assigned partitions.""""""
    if self._subscription.needs_fetch_committed_offsets:
        offsets = self.fetch_committed_offsets(self._subscription.assigned_partitions())
        for partition, offset in six.iteritems(offsets):
            # verify assignment is still active
            if self._subscription.is_assigned(partition):
                self._subscription.assignment[partition].committed = offset.offset
        self._subscription.needs_fetch_committed_offsets = False
",if self . _subscription . is_assigned ( partition ) :,143
"def getText(self, stuff):
    if isinstance(stuff, BaseWrapper):
        stuff = stuff.item
    if isinstance(stuff, (Fit, TargetProfile)):
        val, unit = self._getValue(stuff)
        if val is None:
            return """"
        # Stick to value - 25k GJ
        if self.stickPrefixToValue:
            return ""{} {}"".format(formatAmount(val, *self.formatSpec), unit)
        # Stick to unit - 25 km
        else:
            return formatAmount(val, *self.formatSpec, unitName=unit)
    return """"
",if val is None :,155
"def __get__(self, inst, owner):
    try:
        value, last_update = inst._cache[self.__name__]
        if self.ttl > 0 and time.time() - last_update > self.ttl:
            raise AttributeError
    except (KeyError, AttributeError):
        value = self.fget(inst)
        try:
            cache = inst._cache
        except AttributeError:
            cache = inst._cache = {}
        cache[self.__name__] = (value, time.time())
    return value
",if self . ttl > 0 and time . time ( ) - last_update > self . ttl :,134
"def on_event_clicked(self, widget, event):
    if event.type == Gdk.EventType.BUTTON_PRESS and event.button == 3:
        path = self.get_path_at_pos(int(event.x), int(event.y))
        if path is not None:
            row = self.get(path[0], ""device"")
            if row:
                if self.Blueman is not None:
                    if self.menu is None:
                        self.menu = ManagerDeviceMenu(self.Blueman)
                    self.menu.popup(None, None, None, None, event.button, event.time)
",if path is not None :,170
"def groups(self):
    """"""Return a dictionary mapping group names to JIDs.""""""
    result = {}
    for jid in self._jids:
        groups = self._jids[jid][""groups""]
        if not groups:
            if """" not in result:
                result[""""] = []
            result[""""].append(jid)
        for group in groups:
            if group not in result:
                result[group] = []
            result[group].append(jid)
    return result
",if not groups :,133
"def set_meta(self, dataset, overwrite=True, **kwd):
    super().set_meta(dataset, overwrite=overwrite, **kwd)
    try:
        conn = sqlite.connect(dataset.file_name)
        c = conn.cursor()
        version_query = ""SELECT version FROM meta""
        results = c.execute(version_query).fetchall()
        if len(results) == 0:
            raise Exception(""version not found in meta table"")
        elif len(results) > 1:
            raise Exception(""Multiple versions found in meta table"")
        dataset.metadata.gafa_schema_version = results[0][0]
    except Exception as e:
        log.warning(""%s, set_meta Exception: %s"", self, e)
",elif len ( results ) > 1 :,188
"def GetSelectedCount(self):
    if self.GetStyleL(""style"") & self.Style.LBS_MULTIPLESEL:
        return self.SendMessage(self.Hwnd, self.Msg.LB_GETSELCOUNT, 0, 0)
    else:
        result = self.SendMessage(self.Hwnd, self.Msg.LB_GETCURSEL, 0, 0)
        if result == LB_ERR:
            return 0
        return 1
",if result == LB_ERR :,117
"def emit(self, record):
    try:
        item = QListWidgetItem(self.format(record))
        if record.levelno > logging.INFO:
            item.setIcon(QIcon.fromTheme(""dialog-warning""))
            item.setForeground(QBrush(Qt.red))
        else:
            item.setIcon(QIcon.fromTheme(""dialog-information""))
        self.app.exec_in_main(self._add_item, item)
    except (KeyboardInterrupt, SystemExit):
        raise
    except:
        self.handleError(record)
",if record . levelno > logging . INFO :,142
"def _updater(data):
    assert data[""attrs""][""tvm_version""].startswith(from_ver)
    nodes = data[""nodes""]
    for idx, item in enumerate(nodes):
        f = node_map.get(item[""type_key""], None)
        if isinstance(f, list):
            for fpass in f:
                item = fpass(item, nodes)
        elif f:
            item = f(item, nodes)
        nodes[idx] = item
    data[""attrs""][""tvm_version""] = to_ver
    return data
","if isinstance ( f , list ) :",143
"def remove_data(self):
    if self.path is not None:
        if os.path.exists(self.path):
            os.remove(self.path)
        if os.path.exists(self.get_json_path()):
            os.remove(self.get_json_path())
",if os . path . exists ( self . path ) :,78
"def testsingle(self, sym):
    if self.settings == ""asterisk"":
        return (sym == ""*"", ""*"")
    if self.settings == ""plus"":
        return (sym == ""+"", ""+"")
    if self.settings == ""dash"":
        return (sym == ""-"", ""-"")
    if self.settings == ""single"":
        if self.lastSym:
            return (self.lastSym == sym, self.lastSym)
        else:
            self.lastSym = sym
            return (True, None)
    return (None, None)
",if self . lastSym :,138
"def update(self, other_dict, option_parser):
    if isinstance(other_dict, Values):
        other_dict = other_dict.__dict__
    other_dict = other_dict.copy()
    for setting in option_parser.lists.keys():
        if hasattr(self, setting) and setting in other_dict:
            value = getattr(self, setting)
            if value:
                value += other_dict[setting]
                del other_dict[setting]
    self._update_loose(other_dict)
","if hasattr ( self , setting ) and setting in other_dict :",137
"def gprv_immv(ii):
    for i, op in enumerate(_gen_opnds(ii)):
        if i == 0:
            if op.name == ""REG0"" and op_luf_start(op, ""GPRv""):
                continue
            else:
                return False
        elif i == 1:
            if op_immv(op):
                continue
            else:
                return False
        else:
            return False
    return True
","if op . name == ""REG0"" and op_luf_start ( op , ""GPRv"" ) :",136
"def __call__(self, input_tensors, shape):
    if self.order in ""KA"":
        if any(t.order == TensorOrder.C_ORDER for t in input_tensors):
            order = TensorOrder.C_ORDER
        else:
            order = TensorOrder.F_ORDER
    else:
        if self.order == ""C"":
            order = TensorOrder.C_ORDER
        else:
            order = TensorOrder.F_ORDER
    return self.new_tensor(input_tensors, shape=shape, dtype=self.dtype, order=order)
",if any ( t . order == TensorOrder . C_ORDER for t in input_tensors ) :,141
"def check_selected(menu, path):
    selected = False
    if ""url"" in menu:
        chop_index = menu[""url""].find(""?"")
        if chop_index == -1:
            selected = path.startswith(menu[""url""])
        else:
            selected = path.startswith(menu[""url""][:chop_index])
    if ""menus"" in menu:
        for m in menu[""menus""]:
            _s = check_selected(m, path)
            if _s:
                selected = True
    if selected:
        menu[""selected""] = True
    return selected
",if chop_index == - 1 :,153
"def _check_events(self):
    # make sure song-started and song-ended match up
    stack = []
    old = self.events[:]
    for type_, song in self.events:
        if type_ == ""started"":
            stack.append(song)
        elif type_ == ""ended"":
            self.assertTrue(stack.pop(-1) is song, msg=old)
    self.assertFalse(stack, msg=old)
","elif type_ == ""ended"" :",110
"def __fixdict(self, dict):
    for key in dict.keys():
        if key[:6] == ""start_"":
            tag = key[6:]
            start, end = self.elements.get(tag, (None, None))
            if start is None:
                self.elements[tag] = getattr(self, key), end
        elif key[:4] == ""end_"":
            tag = key[4:]
            start, end = self.elements.get(tag, (None, None))
            if end is None:
                self.elements[tag] = start, getattr(self, key)
","elif key [ : 4 ] == ""end_"" :",162
"def nested_match(expect, value):
    if expect == value:
        return True
    if isinstance(expect, dict) and isinstance(value, dict):
        for k, v in expect.items():
            if k in value:
                if not nested_match(v, value[k]):
                    return False
            else:
                return False
        return True
    if isinstance(expect, list) and isinstance(value, list):
        for x, y in zip(expect, value):
            if not nested_match(x, y):
                return False
        return True
    return False
",if k in value :,162
"def code_match(code, select, ignore):
    if ignore:
        assert not isinstance(ignore, unicode)
        for ignored_code in [c.strip() for c in ignore]:
            if mutual_startswith(code.lower(), ignored_code.lower()):
                return False
    if select:
        assert not isinstance(select, unicode)
        for selected_code in [c.strip() for c in select]:
            if mutual_startswith(code.lower(), selected_code.lower()):
                return True
        return False
    return True
","if mutual_startswith ( code . lower ( ) , ignored_code . lower ( ) ) :",143
"def test_cardinality_m2o(self):
    m2o_type_fields = [
        f for f in self.fields_and_reverse_objects if f.is_relation and f.many_to_one
    ]
    # Test classes are what we expect
    self.assertEqual(MANY_TO_ONE_CLASSES, {f.__class__ for f in m2o_type_fields})
    # Ensure all m2o reverses are o2m
    for obj in m2o_type_fields:
        if hasattr(obj, ""field""):
            reverse_field = obj.field
            self.assertTrue(reverse_field.is_relation and reverse_field.one_to_many)
","if hasattr ( obj , ""field"" ) :",172
"def flatten_dict(self, request):
    dct = super(KnowledgeFolderHandler, self).flatten_dict(request)
    dct[""knowledgeType_id""] = None
    parent = request.data.get(""parent"")
    if parent:
        parent = getOrNone(KnowledgeFolder, pk=parent)
        if not parent or not request.user.profile.has_permission(parent, mode=""x""):
            request.data[""parent""] = None
    return dct
","if not parent or not request . user . profile . has_permission ( parent , mode = ""x"" ) :",116
"def delete_oidc_session_tokens(session):
    if session:
        if ""oidc_access_token"" in session:
            del session[""oidc_access_token""]
        if ""oidc_id_token"" in session:
            del session[""oidc_id_token""]
        if ""oidc_id_token_expiration"" in session:
            del session[""oidc_id_token_expiration""]
        if ""oidc_login_next"" in session:
            del session[""oidc_login_next""]
        if ""oidc_refresh_token"" in session:
            del session[""oidc_refresh_token""]
        if ""oidc_state"" in session:
            del session[""oidc_state""]
","if ""oidc_id_token"" in session :",179
"def prepare_text(text, style):
    body = []
    for fragment, sty in parse_tags(text, style, subs.styles):
        fragment = fragment.replace(r""\h"", "" "")
        fragment = fragment.replace(r""\n"", ""\n"")
        fragment = fragment.replace(r""\N"", ""\n"")
        if sty.italic:
            fragment = ""<i>%s</i>"" % fragment
        if sty.underline:
            fragment = ""<u>%s</u>"" % fragment
        if sty.strikeout:
            fragment = ""<s>%s</s>"" % fragment
        if sty.drawing:
            raise ContentNotUsable
        body.append(fragment)
    return re.sub(""\n+"", ""\n"", """".join(body).strip())
",if sty . strikeout :,198
"def test_reduce_different_name(
    ray_start_distributed_2_nodes_4_gpus, group_name, dst_rank
):
    world_size = 4
    actors, _ = create_collective_workers(num_workers=world_size, group_name=group_name)
    results = ray.get([a.do_reduce.remote(group_name, dst_rank) for a in actors])
    for i in range(world_size):
        if i == dst_rank:
            assert (results[i] == cp.ones((10,), dtype=cp.float32) * world_size).all()
        else:
            assert (results[i] == cp.ones((10,), dtype=cp.float32)).all()
",if i == dst_rank :,182
"def _find_docstrings(self, filename):
    # A replacement for trace.find_strings() which was deprecated in
    # Python 3.2 and removed in 3.6.
    strs = set()
    prev = token.INDENT  # so module docstring is detected as docstring
    with tokenize_open(filename) as f:
        tokens = tokenize.generate_tokens(f.readline)
        for ttype, tstr, start, end, line in tokens:
            if ttype == token.STRING and prev == token.INDENT:
                strs.update(range(start[0], end[0] + 1))
            prev = ttype
    return strs
",if ttype == token . STRING and prev == token . INDENT :,158
"def on_click(self, event):
    button = event[""button""]
    if button in [self.button_next, self.button_previous]:
        if self.station_data:
            self.scrolling = True
            if button == self.button_next:
                self.active_index += 1
            elif button == self.button_previous:
                self.active_index -= 1
            self.active_index %= self.count_stations
        else:
            self.py3.prevent_refresh()
    elif button == self.button_refresh:
        self.idle_time = 0
    else:
        self.py3.prevent_refresh()
",if self . station_data :,178
"def findRule(instance, ruleSet):
    """"""find the rule(s) that matches the feture vector passed""""""
    # print(""*Looking for rule match for Feature vector: "" + featuresToString(instance))
    ruleNumber = 0  # counter to track rule number
    ruleMatches = []  # will hold all rule numbers that matched
    for rule in ruleSet:
        if ruleMatch(rule, instance):
            ruleMatches.append(ruleNumber)
            counts[
                ruleNumber
            ] += 1  # update global histogram of rule matches for stats reporting
            if False:
                print("" ruleMatch found at rule #"" + str(ruleNumber))
                print("" "", end="""")
                printRule(rule)
        ruleNumber += 1
    return ruleMatches
",if False :,200
"def TryMerge(self, d):
    while d.avail() > 0:
        tt = d.getVarInt32()
        if tt == 18:
            length = d.getVarInt32()
            tmp = ProtocolBuffer.Decoder(d.buffer(), d.pos(), d.pos() + length)
            d.skip(length)
            self.mutable_peer_ip().TryMerge(tmp)
            continue
        if tt == 0:
            raise ProtocolBuffer.ProtocolBufferDecodeError
        d.skipData(tt)
",if tt == 18 :,139
"def _check_no_tensors(parameters: Params):
    flat_params = tf.nest.flatten(parameters.params)
    for p in flat_params:
        if isinstance(p, Params):
            _check_no_tensors(p)
        if tf.is_tensor(p):
            raise TypeError(
                ""Saw a `Tensor` value in parameters:\n  {}"".format(parameters)
            )
","if isinstance ( p , Params ) :",108
"def all_zinc_rsc_invalid_dep_keys(invalid_deps):
    """"""Get the rsc key for an rsc-and-zinc target, or the zinc key for a zinc-only target.""""""
    for tgt in invalid_deps:
        # None can occur for e.g. JarLibrary deps, which we don't need to compile as they are
        # populated in the resolve goal.
        tgt_rsc_cc = compile_contexts[tgt].rsc_cc
        if tgt_rsc_cc.workflow is not None:
            # Rely on the results of zinc compiles for zinc-compatible targets
            yield self._key_for_target_as_dep(tgt, tgt_rsc_cc.workflow)
",if tgt_rsc_cc . workflow is not None :,182
"def characters(self, ch):
    if self.Text_tag:
        if self.Summary_tag:
            self.Summary_ch += ch
        elif self.Attack_Prerequisite_tag:
            self.Attack_Prerequisite_ch += ch
        elif self.Solution_or_Mitigation_tag:
            self.Solution_or_Mitigation_ch += ch
    elif self.CWE_ID_tag:
        self.CWE_ID_ch += ch
",elif self . Solution_or_Mitigation_tag :,127
"def load_tool_from_cache(self, config_file, recover_tool=False):
    tool_cache = getattr(self.app, ""tool_cache"", None)
    tool = None
    if tool_cache:
        if recover_tool:
            tool = tool_cache.get_removed_tool(config_file)
        else:
            tool = tool_cache.get_tool(config_file)
    return tool
",if recover_tool :,108
"def _generate_examples(self, archive, directory, labeled=True):
    """"""Generate IMDB examples.""""""
    # For labeled examples, extract the label from the path.
    reg_path = ""(?P<label>neg|pos)"" if labeled else ""unsup""
    reg = re.compile(
        os.path.join(""^%s"" % directory, reg_path, """").replace(""\\"", ""\\\\"")
    )
    for path, imdb_f in archive:
        res = reg.match(path)
        if not res:
            continue
        text = imdb_f.read().strip()
        label = res.groupdict()[""label""] if labeled else -1
        yield path, {
            ""text"": text,
            ""label"": label,
        }
",if not res :,188
"def startInputThread(self):
    # cv.acquire()
    # Fix Python 2.x.
    global input
    try:
        input = raw_input
    except NameError:
        pass
    while True:
        cmd = (
            self._queuedCmds.pop(0)
            if len(self._queuedCmds)
            else input(self.getPrompt()).strip()
        )
        wait = self.execCmd(cmd)
        if wait:
            self.acceptingInput = False
            self.blockingQueue.get(True)
            # cv.wait()
            # self.inputThread.wait()
        self.acceptingInput = True
",if wait :,181
"def assertS_IS(self, name, mode):
    # test format, lstrip is for S_IFIFO
    fmt = getattr(stat, ""S_IF"" + name.lstrip(""F""))
    self.assertEqual(stat.S_IFMT(mode), fmt)
    # test that just one function returns true
    testname = ""S_IS"" + name
    for funcname in self.format_funcs:
        func = getattr(stat, funcname, None)
        if func is None:
            if funcname == testname:
                raise ValueError(funcname)
            continue
        if funcname == testname:
            self.assertTrue(func(mode))
        else:
            self.assertFalse(func(mode))
",if funcname == testname :,180
"def test_compatibility(self) -> None:
    for expected, user_agent in self.data:
        result = self.client_get(""/compatibility"", HTTP_USER_AGENT=user_agent)
        if expected == ""ok"":
            self.assert_json_success(result)
        elif expected == ""old"":
            self.assert_json_error(result, ""Client is too old"")
        else:
            assert False  # nocoverage
","elif expected == ""old"" :",114
"def getBranchFromFile():
    global _gitdir
    branch = None
    if _gitdir:
        headFile = os.path.join(_gitdir, ""HEAD"")
        if os.path.isfile(headFile):
            with open(headFile, ""r"", encoding=""utf-8"") as f:
                line = f.readline()
                if line:
                    if line.startswith(""ref""):
                        branch = line.split(""/"")[-1].strip()
                    else:
                        branch = ""HEAD""
    return branch
",if os . path . isfile ( headFile ) :,151
"def get_job_parameters_dict(self, job_parameters: RunParameters = None):
    if job_parameters:
        if int(self.job_runtime_conf.get(""dsl_version"", 1)) == 2:
            self.job_runtime_conf[""job_parameters""][""common""] = job_parameters.to_dict()
        else:
            self.job_runtime_conf[""job_parameters""] = job_parameters.to_dict()
    return self.job_runtime_conf[""job_parameters""]
","if int ( self . job_runtime_conf . get ( ""dsl_version"" , 1 ) ) == 2 :",126
"def ConnectHandler(*args, **kwargs):
    """"""Factory function selects the proper class and creates object based on device_type.""""""
    device_type = kwargs[""device_type""]
    if device_type not in platforms:
        if device_type is None:
            msg_str = platforms_str
        else:
            msg_str = telnet_platforms_str if ""telnet"" in device_type else platforms_str
        raise ValueError(
            ""Unsupported 'device_type' ""
            ""currently supported platforms are: {}"".format(msg_str)
        )
    ConnectionClass = ssh_dispatcher(device_type)
    return ConnectionClass(*args, **kwargs)
",if device_type is None :,167
"def get_next_parent_entities(item, pids):
    ret = list()
    for [parent, entity_id] in parents[item]:
        if entity_id in pids:
            continue
        if parent in entities:
            ret.append(parent)
        else:
            pids.append(entity_id)
            for p in get_next_parent_entities(parent, pids):
                ret.append(p)
    return ret
",if entity_id in pids :,119
"def load(self, data):
    ckey = None
    for key, val in _rx_cookie.findall(data):
        if key.lower() in _c_keys:
            if ckey:
                self[ckey][key] = _unquote(val)
        elif key[0] == ""$"":
            # RFC2109: NAMEs that begin with $ are reserved for other uses
            # and must not be used by applications.
            continue
        else:
            self[key] = _unquote(val)
            ckey = key
",if key . lower ( ) in _c_keys :,143
"def getIdentifier(self):
    start = self.index
    self.index += 1
    while self.index < self.length:
        ch = self.ccode()
        if ch == 0x5C:
            # Blackslash (U+005C) marks Unicode escape sequence.
            self.index = start
            return self.getEscapedIdentifier()
        if isIdentifierPart(ch):
            self.index += 1
        else:
            break
    return self.source[start : self.index]
",if ch == 0x5C :,134
"def test_floats_unequal_float(self):
    try:
        self.assertEqual(
            np.array([[1, 2], [3, 4.5]], dtype=np.float32),
            np.array([[1, 2], [3, 5]], dtype=np.float32),
        )
    except AssertionError as e:
        if not str(e).startswith(""Arrays not almost equal to 6 decimals""):
            raise self.failureException(""Float array mismatch error not raised."")
","if not str ( e ) . startswith ( ""Arrays not almost equal to 6 decimals"" ) :",119
"def _set_counts(self):
    self[""regions_count""] = len(self[""regions""])
    for _, key in self._children:
        # VPCs should not be counted as resources. They exist whether you have resources or not,
        # so counting them would make the report confusing.
        if key == ""vpcs"":
            continue
        self[key + ""_count""] = sum(
            [region[key + ""_count""] for region in self[""regions""].values()]
        )
","if key == ""vpcs"" :",123
"def total_form_count(self):
    """"""Returns the total number of forms in this FormSet.""""""
    if self.data or self.files:
        return self.management_form.cleaned_data[TOTAL_FORM_COUNT]
    else:
        initial_forms = self.initial_form_count()
        total_forms = initial_forms + self.extra
        # Allow all existing related objects/inlines to be displayed,
        # but don't allow extra beyond max_num.
        if initial_forms > self.max_num >= 0:
            total_forms = initial_forms
        elif total_forms > self.max_num >= 0:
            total_forms = self.max_num
    return total_forms
",elif total_forms > self . max_num >= 0 :,180
"def mouse_down(self, evt):
    if self.parent.level:
        toolNo = self.toolNumberUnderMouse(evt.pos)
        if toolNo < 0 or toolNo > 8:
            return
        if evt.button == 1:
            self.selectTool(toolNo)
        if evt.button == 3:
            self.showToolOptions(toolNo)
",if evt . button == 1 :,100
"def find_comment(line):
    """"""Finds the index of a comment # and returns None if not found""""""
    instring, instring_char = False, """"
    for i, char in enumerate(line):
        if char in ('""', ""'""):
            if instring:
                if char == instring_char:
                    instring = False
                    instring_char = """"
            else:
                instring = True
                instring_char = char
        elif char == ""#"":
            if not instring:
                return i
    return None
",if not instring :,155
"def __getattr__(self, key):
    if key == key.upper():
        if hasattr(self._django_settings, key):
            return getattr(self._django_settings, key)
        elif hasattr(self._default_settings, key):
            return getattr(self._default_settings, key)
    raise AttributeError(
        ""%r object has no attribute %r"" % (self.__class__.__name__, key)
    )
","elif hasattr ( self . _default_settings , key ) :",106
"def replace_entities(match, entities=entities, encoding=encoding):
    ent = match.group()
    if ent[1] == ""#"":
        return unescape_charref(ent[2:-1], encoding)
    repl = entities.get(ent)
    if repl is not None:
        if hasattr(repl, ""decode"") and encoding is not None:
            try:
                repl = repl.decode(encoding)
            except UnicodeError:
                repl = ent
    else:
        repl = ent
    return repl
","if hasattr ( repl , ""decode"" ) and encoding is not None :",132
"def test_floor_div(self):
    """"""Util.number.floor_div""""""
    self.assertRaises(TypeError, number.floor_div, ""1"", 1)
    for a in range(-10, 10):
        for b in range(-10, 10):
            if b == 0:
                self.assertRaises(ZeroDivisionError, number.floor_div, a, b)
            else:
                self.assertEqual(
                    (a, b, int(math.floor(float(a) / b))),
                    (a, b, number.floor_div(a, b)),
                )
",if b == 0 :,159
"def get(self, method, **kws):
    resp = None
    if method in self.responses:
        resp = self.responses[method].pop(0)
        if ""validate"" in resp:
            checks = resp[""validate""][""checks""]
            resp = resp[""validate""][""data""]
            for check in checks:
                assert check in kws
                expected_value = checks[check]
                assert expected_value == kws[check]
    return resp
","if ""validate"" in resp :",123
"def __add_changelisteners(self):
    NewPlayerSettlementHovered.subscribe(self.on_settlement_change)
    if self.__current_settlement is not None:
        inventory = self.__current_settlement.get_component(StorageComponent).inventory
        if not inventory.has_change_listener(self.refresh):
            inventory.add_change_listener(self.refresh)
",if not inventory . has_change_listener ( self . refresh ) :,94
"def __call__(self, target):
    if ""weights"" not in target.temp:
        return True
    targets = target.temp[""weights""]
    for cname in target.children:
        if cname in targets:
            c = target.children[cname]
            deviation = abs((c.weight - targets[cname]) / targets[cname])
            if deviation > self.tolerance:
                return True
    if ""cash"" in target.temp:
        cash_deviation = abs(
            (target.capital - targets.value) / targets.value - target.temp[""cash""]
        )
        if cash_deviation > self.tolerance:
            return True
    return False
",if cname in targets :,178
"def copyfileobj(src, dest, length=512):
    if hasattr(src, ""readinto""):
        buf = bytearray(length)
        while True:
            sz = src.readinto(buf)
            if not sz:
                break
            if sz == length:
                dest.write(buf)
            else:
                b = memoryview(buf)[:sz]
                dest.write(b)
    else:
        while True:
            buf = src.read(length)
            if not buf:
                break
            dest.write(buf)
",if sz == length :,162
"def test_api_history_restrict_cat(self):
    slot_sum = 0
    # Loop over all categories in the fake history, plus the Default category
    cats = list(self.history_category_options)
    cats.extend(""*"")
    for cat in cats:
        json = self._get_api_history({""category"": cat})
        slot_sum += len(json[""history""][""slots""])
        # All results should be from the correct category
        for slot in json[""history""][""slots""]:
            if cat != ""*"":
                assert slot[""category""] == cat
    # Total number of slots should match the sum of all category slots
    json = self._get_api_history({""limit"": self.history_size})
    slot_total = len(json[""history""][""slots""])
    assert slot_sum == slot_total
","if cat != ""*"" :",199
"def checker(self):
    while True:
        try:
            ip = self.get_ip()
        except Exception as e:
            xlog.info(""no ip left"")
            return
        try:
            res = self.check_ip.check_ip(ip, sni=host, host=host)
        except Exception as e:
            xlog.warn(""check fail:%s except:%r"", e)
            continue
        if not res or not res.ok:
            xlog.debug(""check fail:%s fail"", ip)
            continue
        self.write_ip(ip, res.domain, res.handshake_time)
",if not res or not res . ok :,173
"def create_row_processor(
    self, context, path, loadopt, mapper, result, adapter, populators
):
    # look through list of columns represented here
    # to see which, if any, is present in the row.
    for col in self.columns:
        if adapter:
            col = adapter.columns[col]
        getter = result._getter(col, False)
        if getter:
            populators[""quick""].append((self.key, getter))
            break
    else:
        populators[""expire""].append((self.key, True))
",if getter :,145
"def indices(dimensions, dtype=int32, sparse=False):
    dimensions = tuple(dimensions)
    N = len(dimensions)
    output = []
    s = dimensions
    for i, dim in enumerate(dimensions):
        idx = lax.iota(dtype, dim)
        if sparse:
            s = (1,) * i + (dim,) + (1,) * (N - i - 1)
        output.append(lax.broadcast_in_dim(idx, s, (i,)))
    if sparse:
        return tuple(output)
    return stack(output, 0) if output else array([], dtype=dtype)
",if sparse :,154
"def load_cases(full_path):
    all_test_data = json.load(open(full_path), object_pairs_hook=OrderedDict)
    for test_data in all_test_data:
        given = test_data[""given""]
        for case in test_data[""cases""]:
            if ""result"" in case:
                test_type = ""result""
            elif ""error"" in case:
                test_type = ""error""
            elif ""bench"" in case:
                test_type = ""bench""
            else:
                raise RuntimeError(""Unknown test type: %s"" % json.dumps(case))
            yield (given, test_type, case)
","elif ""error"" in case :",183
"def _resolve_task_id(cls, task_id, log=None):
    if not task_id:
        task_id = cls.normalize_id(get_remote_task_id())
        if task_id:
            log = log or get_logger(""task"")
            log.info(""Using task ID from env %s=%s"" % (TASK_ID_ENV_VAR[0], task_id))
    return task_id
",if task_id :,110
"def _build_contr_port_map(self, fabric_connected_ports, ports_info):
    contr_port_map = {}
    for port in fabric_connected_ports:
        contr = ports_info[port][""contr""]
        if not contr_port_map.get(contr):
            contr_port_map[contr] = []
        contr_port_map[contr].append(port)
    LOG.debug(""Controller port map: %s."", contr_port_map)
    return contr_port_map
",if not contr_port_map . get ( contr ) :,143
"def confirm(question):
    """"""Prompts a given question and handles user input.""""""
    valid = {""yes"": True, ""y"": True, ""ye"": True, ""no"": False, ""n"": False, """": True}
    prompt = "" [Y/n] ""
    while True:
        print(BOLD + CYAN + question + prompt + END)
        choice = input().lower()
        if choice in valid:
            return valid[choice]
        print(""Please respond with 'yes' or 'no' (or 'y' or 'n').\n"")
",if choice in valid :,137
"def __parse_query(self, model, iter_, data):
    f, b = self.__filter, self.__bg_filter
    if f is None and b is None:
        return True
    else:
        album = model.get_album(iter_)
        if album is None:
            return True
        elif b is None:
            return f(album)
        elif f is None:
            return b(album)
        else:
            return b(album) and f(album)
",if album is None :,130
"def get_SV(self):
    result = []
    for sparse_sv in self.SV[: self.l]:
        row = dict()
        i = 0
        while True:
            row[sparse_sv[i].index] = sparse_sv[i].value
            if sparse_sv[i].index == -1:
                break
            i += 1
        result.append(row)
    return result
",if sparse_sv [ i ] . index == - 1 :,111
"def TryMerge(self, d):
    while d.avail() > 0:
        tt = d.getVarInt32()
        if tt == 10:
            self.set_hostname(d.getPrefixedString())
            continue
        if tt == 0:
            raise ProtocolBuffer.ProtocolBufferDecodeError
        d.skipData(tt)
",if tt == 10 :,90
"def getFileIdFromAlternateLink(altLink):
    loc = altLink.find(""/d/"")
    if loc > 0:
        fileId = altLink[loc + 3 :]
        loc = fileId.find(""/"")
        if loc != -1:
            return fileId[:loc]
    else:
        loc = altLink.find(""/folderview?id="")
        if loc > 0:
            fileId = altLink[loc + 15 :]
            loc = fileId.find(""&"")
            if loc != -1:
                return fileId[:loc]
    controlflow.system_error_exit(
        2, f""{altLink} is not a valid Drive File alternateLink""
    )
",if loc != - 1 :,180
"def show_unknown_key_warning(name: Union[str, object], others: dict):
    if ""type"" in others:
        others.pop(""type"")
    if len(others) > 0:
        keys = "", "".join(others.keys())
        logger = logging.getLogger(__name__)
        if isinstance(name, object):
            name = name.__class__.__name__
        logger.debug(
            f""!!! {name}'s constructor args ({keys}) were ignored.""
            f""If they should be supported by this library, report this issue to the project :bow: ""
            f""https://github.com/slackapi/python-slackclient/issues""
        )
","if isinstance ( name , object ) :",173
"def wrapper(*args, **kwargs):
    with capture_logs() as logs:
        try:
            function(*args, **kwargs)
        except Exception:  # pragma: no cover
            if logs:
                print(""%i errors logged:"" % len(logs), file=sys.stderr)
                for message in logs:
                    print(message, file=sys.stderr)
            raise
        else:
            if logs:  # pragma: no cover
                for message in logs:
                    print(message, file=sys.stderr)
                raise AssertionError(""%i errors logged"" % len(logs))
",if logs :,168
"def _init_weight(self):
    for m in self.modules():
        if isinstance(m, nn.Conv2d):
            n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels
            m.weight.data.normal_(0, math.sqrt(2.0 / n))
        elif isinstance(m, SyncBatchNorm):
            m.weight.data.fill_(1)
            m.bias.data.zero_()
        elif isinstance(m, nn.BatchNorm2d):
            m.weight.data.fill_(1)
            m.bias.data.zero_()
","elif isinstance ( m , SyncBatchNorm ) :",162
"def cleanup(self):
    # some OBO ontologies have extra ""."" at the end of synonyms
    for i, s in enumerate(self.synonyms):
        if s[-1] == ""."":
            # only remove period if preceded by ""normal word""
            if re.search(r""\b[a-z]{2,}\.$"", s):
                c = s[:-1]
                print >>sys.stderr, ""Note: cleanup: '%s' -> '%s'"" % (s, c)
                self.synonyms[i] = c
","if re . search ( r""\b[a-z]{2,}\.$"" , s ) :",138
"def for_module(cls, modname: str) -> ""ModuleAnalyzer"":
    if (""module"", modname) in cls.cache:
        entry = cls.cache[""module"", modname]
        if isinstance(entry, PycodeError):
            raise entry
        return entry
    try:
        filename, source = cls.get_module_source(modname)
        if source is not None:
            obj = cls.for_string(source, modname, filename or ""<string>"")
        elif filename is not None:
            obj = cls.for_file(filename, modname)
    except PycodeError as err:
        cls.cache[""module"", modname] = err
        raise
    cls.cache[""module"", modname] = obj
    return obj
","if isinstance ( entry , PycodeError ) :",182
"def GetDisplayNameOf(self, pidl, flags):
    item = pidl_to_item(pidl)
    if flags & shellcon.SHGDN_FORPARSING:
        if flags & shellcon.SHGDN_INFOLDER:
            return item[""name""]
        else:
            if flags & shellcon.SHGDN_FORADDRESSBAR:
                sigdn = shellcon.SIGDN_DESKTOPABSOLUTEEDITING
            else:
                sigdn = shellcon.SIGDN_DESKTOPABSOLUTEPARSING
            parent = shell.SHGetNameFromIDList(self.pidl, sigdn)
            return parent + ""\\"" + item[""name""]
    else:
        return item[""name""]
",if flags & shellcon . SHGDN_FORADDRESSBAR :,188
"def transact_reraise(exc_class, exceptions):
    cls, exc, tb = exceptions[0]
    new_exc = None
    try:
        msg = "" "".join(tostring(arg) for arg in exc.args)
        if not issubclass(cls, TransactionError):
            msg = ""%s: %s"" % (cls.__name__, msg)
        new_exc = exc_class(msg, exceptions)
        new_exc.__cause__ = None
        reraise(exc_class, new_exc, tb)
    finally:
        del exceptions, exc, tb, new_exc
","if not issubclass ( cls , TransactionError ) :",146
"def add_share(self, share):
    for filename, (share_hashes, verified_hashes) in self.known.iteritems():
        if share.hash in share_hashes:
            break
    else:
        filename = self._add_line(
            ""%i %s"" % (5, share_type.pack(share.as_share()).encode(""hex""))
        )
        share_hashes, verified_hashes = self.known.setdefault(filename, (set(), set()))
        share_hashes.add(share.hash)
    share_hashes, verified_hashes = self.known_desired.setdefault(
        filename, (set(), set())
    )
    share_hashes.add(share.hash)
",if share . hash in share_hashes :,176
"def get_resolved_modules(self) -> Dict[str, ResolvedModule]:
    """"""Get a {name: ResolvedModule} map of all resolved modules.""""""
    resolved_modules = {}
    for name, mod in self._modules.items():
        if not mod.has_unresolved_pointers:
            resolved_modules[name] = ResolvedModule(
                mod.module_name, mod.filename, mod.ast
            )
    return resolved_modules
",if not mod . has_unresolved_pointers :,115
"def stripe(request):
    amount = 1
    response = None
    if request.method == ""POST"":
        form = CreditCardForm(request.POST)
        if form.is_valid():
            data = form.cleaned_data
            credit_card = CreditCard(**data)
            merchant = get_gateway(""stripe"")
            response = merchant.purchase(amount, credit_card)
    else:
        form = CreditCardForm(initial=GATEWAY_INITIAL[""stripe""])
    return render(
        request,
        ""app/index.html"",
        {
            ""form"": form,
            ""amount"": amount,
            ""response"": response,
            ""title"": ""Stripe Payment"",
        },
    )
",if form . is_valid ( ) :,192
"def get(self, url):
    now = time.time()
    for entry in self.repos:
        if url.startswith(entry.url):
            if now < entry.timestamp + self.timeout:
                # print ""returning immediate Etrny"", entry
                return entry.url, entry.rev
            return entry.url, -1
    return url, -1
",if url . startswith ( entry . url ) :,98
"def cleanup(self):
    # some OBO ontologies have extra ""."" at the end of synonyms
    for i, s in enumerate(self.synonyms):
        if s[-1] == ""."":
            # only remove period if preceded by ""normal word""
            if re.search(r""\b[a-z]{2,}\.$"", s):
                c = s[:-1]
                print >>sys.stderr, ""Note: cleanup: '%s' -> '%s'"" % (s, c)
                self.synonyms[i] = c
","if s [ - 1 ] == ""."" :",138
"def __get_field(cls, name):
    try:
        return cls._doc_type.mapping[name]
    except KeyError:
        # fallback to fields on the Index
        if hasattr(cls, ""_index"") and cls._index._mapping:
            try:
                return cls._index._mapping[name]
            except KeyError:
                pass
","if hasattr ( cls , ""_index"" ) and cls . _index . _mapping :",95
"def command_is_enabled(self, item, focus):
    cmd = item.command
    if cmd:
        enabler_name = cmd + ""_enabled""
        handler = focus
        while handler:
            enabler = getattr(handler, enabler_name, None)
            if enabler:
                return enabler()
            handler = handler.next_handler()
    return True
",if enabler :,107
"def __getitem__(self, key):
    value = WeakValueDictionary.__getitem__(self, key)
    # check boundaries to minimiza duplicate references
    while len(self.queue) > 0 and self.queue[0][0] == key:
        # item at left end of queue pop it since it'll be appended
        # to right
        self.queue.popleft()
    # only append if item is not at right end of queue
    if not (len(self.queue) and self.queue[-1][0] == key):
        if len(self) >= self.maxsize or len(self.queue) >= self.maxsize * self.peakmult:
            self.cull()
        self.queue.append((key, value))
    return value
",if len ( self ) >= self . maxsize or len ( self . queue ) >= self . maxsize * self . peakmult :,181
"def post_init(self):
    if os.getenv(""SCRCPY_LDD""):
        if os.getenv(""LD_LIBRARY_PATH""):
            os.environ[""LD_LIBRARY_PATH""] += os.getenv(""SCRCPY_LDD"")
        else:
            os.environ[""LD_LIBRARY_PATH""] = os.getenv(""SCRCPY_LDD"")
","if os . getenv ( ""LD_LIBRARY_PATH"" ) :",93
"def get_summary_output(event: events.Finished) -> Tuple[str, str, int]:
    parts = get_summary_message_parts(event.results)
    if not parts:
        message = ""Empty test suite""
        color = ""yellow""
        status_code = 0
    else:
        message = f'{"", "".join(parts)} in {event.running_time:.2f}s'
        if event.results.has_failures or event.results.has_errors:
            color = ""red""
            status_code = 1
        else:
            color = ""green""
            status_code = 0
    return message, color, status_code
",if event . results . has_failures or event . results . has_errors :,172
"def header_check(p_obj):
    """"""Special disposition for the HTML <head> and <body> elements...""""""
    if state.options.host_language in [
        HostLanguage.xhtml,
        HostLanguage.html5,
        HostLanguage.xhtml5,
    ]:
        if node.nodeName == ""head"" or node.nodeName == ""body"":
            if not has_one_of_attributes(node, ""about"", ""resource"", ""src"", ""href""):
                return p_obj
    else:
        return None
","if node . nodeName == ""head"" or node . nodeName == ""body"" :",137
"def get_track_id_from_json(item):
    """"""Try to extract video Id from various response types""""""
    fields = [
        ""contentDetails/videoId"",
        ""snippet/resourceId/videoId"",
        ""id/videoId"",
        ""id"",
    ]
    for field in fields:
        node = item
        for p in field.split(""/""):
            if node and isinstance(node, dict):
                node = node.get(p)
        if node:
            return node
    return """"
",if node :,137
"def __init__(self, layers):
    super(Add, self).__init__()
    self.layer_names = []
    self.layers = layers
    for i, layer in enumerate(self.layers):
        if layer.parent is None:
            if i == 0:
                layer.parent = ""input""
            else:
                layer.parent = layers[i - 1].name
        if hasattr(layer, ""name""):
            name = layer.name
        else:
            name = layer.__class__.__name__ + str(i)
            layer.name = name
        self.layer_names.append(name)
",if layer . parent is None :,165
"def do_remove(self):
    if self.netconf.locked(""dhcp""):
        if not self.pid:
            pid = read_pid_file(""/var/run/dnsmasq.pan1.pid"")
        else:
            pid = self.pid
        if not kill(pid, ""dnsmasq""):
            logging.info(""Stale dhcp lockfile found"")
        self.netconf.unlock(""dhcp"")
","if not kill ( pid , ""dnsmasq"" ) :",106
"def findStyleName(element, style):
    oldStyle = DOM.getAttribute(element, ""className"")
    if oldStyle is None:
        return -1
    idx = oldStyle.find(style)
    # Calculate matching index
    lastPos = len(oldStyle)
    while idx != -1:
        if idx == 0 or (oldStyle[idx - 1] == "" ""):
            last = idx + len(style)
            if (last == lastPos) or ((last < lastPos) and (oldStyle[last] == "" "")):
                break
        idx = oldStyle.find(style, idx + 1)
    return idx
","if idx == 0 or ( oldStyle [ idx - 1 ] == "" "" ) :",161
"def __str__(self):
    path = super(XPathExpr, self).__str__()
    if self.textnode:
        if path == ""*"":
            path = ""text()""
        elif path.endswith(""::*/*""):
            path = path[:-3] + ""text()""
        else:
            path += ""/text()""
    if self.attribute is not None:
        if path.endswith(""::*/*""):
            path = path[:-2]
        path += ""/@%s"" % self.attribute
    return path
","if path . endswith ( ""::*/*"" ) :",132
"def insert_after(self, sibling, row=None):
    if row is not None:
        value = self._get_marshalable(row[0])
        if sibling is None:
            position = 0
        else:
            position = self.get_path(sibling)[0] + 1
        return self.insert_with_valuesv(position, [0], [value])
    assert not self.ATOMIC
    return super(ObjectStore, self).insert_after(sibling, row)
",if sibling is None :,125
"def source_synopsis(file):
    line = file.readline()
    while line[:1] == ""#"" or not strip(line):
        line = file.readline()
        if not line:
            break
    line = strip(line)
    if line[:4] == 'r""""""':
        line = line[1:]
    if line[:3] == '""""""':
        line = line[3:]
        if line[-1:] == ""\\"":
            line = line[:-1]
        while not strip(line):
            line = file.readline()
            if not line:
                break
        result = strip(split(line, '""""""')[0])
    else:
        result = None
    return result
","if line [ - 1 : ] == ""\\"" :",182
"def _handle_rate_limit(
    self, exception: RedditAPIException
) -> Optional[Union[int, float]]:
    for item in exception.items:
        if item.error_type == ""RATELIMIT"":
            amount_search = self._ratelimit_regex.search(item.message)
            if not amount_search:
                break
            seconds = int(amount_search.group(1))
            if ""minute"" in amount_search.group(2):
                seconds *= 60
            if seconds <= int(self.config.ratelimit_seconds):
                sleep_seconds = seconds + min(seconds / 10, 1)
                return sleep_seconds
    return None
","if ""minute"" in amount_search . group ( 2 ) :",181
"def get_html_help_exe():
    """"""Return HTML Help Workshop executable path (Windows only)""""""
    if os.name == ""nt"":
        hhc_base = r""C:\Program Files%s\HTML Help Workshop\hhc.exe""
        for hhc_exe in (hhc_base % """", hhc_base % "" (x86)""):
            if osp.isfile(hhc_exe):
                return hhc_exe
        else:
            return
",if osp . isfile ( hhc_exe ) :,121
"def get_net_bridge_owner(name_ignore, sysfspath):
    # Now magic to determine if the device is part of a bridge
    brportpath = os.path.join(sysfspath, ""brport"")
    try:
        if os.path.exists(brportpath):
            brlinkpath = os.path.join(brportpath, ""bridge"")
            dest = os.readlink(brlinkpath)
            (ignore, bridge) = os.path.split(dest)
            return bridge
    except:
        logging.exception(""Unable to determine if device is shared"")
    return None
",if os . path . exists ( brportpath ) :,154
"def get_timestamp(self):
    if not self._timedelta:
        url = ""https://%s%s/auth/time"" % (API_HOST, API_ROOT)
        response = get_response_object(url=url, method=""GET"", headers={})
        if not response or not response.body:
            raise Exception(""Failed to get current time from Ovh API"")
        timestamp = int(response.body)
        self._timedelta = timestamp - int(time.time())
    return int(time.time()) + self._timedelta
",if not response or not response . body :,131
"def render(self, context):
    for var in self.vars:
        value = var.resolve(context, True)
        if value:
            first = render_value_in_context(value, context)
            if self.asvar:
                context[self.asvar] = first
                return """"
            return first
    return """"
",if value :,95
"def test_loc_is_stochastic_parameter(self):
    param = iap.Laplace(iap.Choice([-100, 100]), 1)
    seen = [0, 0]
    for _ in sm.xrange(1000):
        samples = param.draw_samples((100,))
        exp = np.mean(samples)
        if -100 - 10 < exp < -100 + 10:
            seen[0] += 1
        elif 100 - 10 < exp < 100 + 10:
            seen[1] += 1
        else:
            assert False
    assert 500 - 100 < seen[0] < 500 + 100
    assert 500 - 100 < seen[1] < 500 + 100
",elif 100 - 10 < exp < 100 + 10 :,167
"def get_data(self, path, prefix=""""):
    item = self.store[path]
    path = ""{}/{}"".format(prefix, path)
    keys = [i for i in item.keys()]
    data = {""path"": path}
    # print(path)
    for k in keys:
        if not isinstance(item[k], h5py.Group):
            dataset = np.array(item[k].value)
            if type(dataset) is np.ndarray:
                if dataset.size != 0:
                    if type(dataset[0]) is np.bytes_:
                        dataset = [a.decode(""ascii"") for a in dataset]
            data.update({k: dataset})
    return data
",if dataset . size != 0 :,183
"def __del__(self):
    try:
        if self._mpz_p is not None:
            if self._initialized:
                _gmp.mpz_clear(self._mpz_p)
        self._mpz_p = None
    except AttributeError:
        pass
",if self . _initialized :,75
"def load(self, vocab_file):
    self.__term2id = {}
    self.__id2term = {}
    with open(vocab_file, ""r"", encoding=""utf-8"") as fin:
        for line in fin.readlines():
            fields = line.strip().split(""\t"")
            assert len(fields) == 5, ""Vocabulary file [%s] format error!"" % (vocab_file)
            term = fields[1]
            id_ = int(fields[2])
            if term in self.__term2id:
                logger.error(""Duplicate word [%s] in vocab file!"" % (term))
                continue
            self.__term2id[term] = id_
            self.__id2term[id_] = term
",if term in self . __term2id :,193
"def break_next_call(symbol_regex=None):
    while pwndbg.proc.alive:
        ins = break_next_branch()
        if not ins:
            break
        # continue if not a call
        if capstone.CS_GRP_CALL not in ins.groups:
            continue
        # return call if we don't search for a symbol
        if not symbol_regex:
            return ins
        # return call if we match target address
        if ins.target_const and re.match(""%s$"" % symbol_regex, hex(ins.target)):
            return ins
        # return call if we match symbol name
        if ins.symbol and re.match(""%s$"" % symbol_regex, ins.symbol):
            return ins
",if not symbol_regex :,193
"def test_url_valid_set():
    for line in URL_VALID_TESTS.split(""\n""):
        # strip line, skip over empty lines
        line = line.strip()
        if line == """":
            continue
        # skip over comments or empty lines
        match = COMMENT.match(line)
        if match:
            continue
        mbox = address.parse(line, strict=True)
        assert_not_equal(mbox, None)
",if match :,120
"def _clean_fields(self, fields, reverse=False):
    if not fields:
        fields = list(self.default_fields)
    if reverse:
        for field in [""up.total"", ""down.total"", ""down.rate""]:
            if field in fields:
                fields[fields.index(field)] = field.replace(""."", ""_"")
        return fields
    for required_field in self.required_fields:
        if required_field not in fields:
            fields.insert(0, required_field)
    for field in [""up_total"", ""down_total"", ""down_rate""]:
        if field in fields:
            fields[fields.index(field)] = field.replace(""_"", ""."")
    return fields
",if field in fields :,181
"def client_cert_key_path(self):
    cache_folder = os.path.dirname(self.filename)
    try:
        path = self.get_item(""general.client_cert_key_path"")
    except ConanException:
        path = os.path.join(cache_folder, ""client.key"")
    else:
        # For explicit cacert files, the file should already exist
        path = os.path.join(cache_folder, path)
        if not os.path.exists(path):
            raise ConanException(
                ""Configured file for 'client_cert_key_path'""
                "" doesn't exists: '{}'"".format(path)
            )
    return os.path.normpath(path)
",if not os . path . exists ( path ) :,186
"def handler_click_link(self, link):
    if link.startswith(""[[""):
        link = link[2:-2]
        self.notify_observers(""click:notelink"", link)
    else:
        if platform.system().lower() == ""windows"":
            os.startfile(link)
        elif platform.system().lower() == ""darwin"":
            subprocess.call((""open"", link))
        else:
            subprocess.call((""xdg-open"", link))
","elif platform . system ( ) . lower ( ) == ""darwin"" :",123
"def __setitem__(self, key, value):
    if not isinstance(value, PseudoNamespace):
        tuple_converted = False
        if isinstance(value, dict):
            value = PseudoNamespace(value)
        elif isinstance(value, tuple):
            value = list(value)
            tuple_converted = True
        if isinstance(value, list):
            for i, item in enumerate(value):
                if isinstance(item, dict) and not isinstance(item, PseudoNamespace):
                    value[i] = PseudoNamespace(item)
            if tuple_converted:
                value = tuple(value)
    super(PseudoNamespace, self).__setitem__(key, value)
",if tuple_converted :,175
"def slots_for_entities(self, entities):
    if self.store_entities_as_slots:
        slot_events = []
        for s in self.slots:
            if s.auto_fill:
                matching_entities = [
                    e[""value""] for e in entities if e[""entity""] == s.name
                ]
                if matching_entities:
                    if s.type_name == ""list"":
                        slot_events.append(SlotSet(s.name, matching_entities))
                    else:
                        slot_events.append(SlotSet(s.name, matching_entities[-1]))
        return slot_events
    else:
        return []
","if s . type_name == ""list"" :",193
"def stream_read_bz2(ifh, ofh):
    """"""Uncompress bz2 compressed *ifh* into *ofh*""""""
    decompressor = bz2.BZ2Decompressor()
    while True:
        buf = ifh.read(BUFSIZE)
        if not buf:
            break
        buf = decompressor.decompress(buf)
        if buf:
            ofh.write(buf)
    if decompressor.unused_data or ifh.read(1) != b"""":
        raise CorruptedObjectError(""Data after end of bz2 stream"")
",if not buf :,139
"def get_for_vars(self):
    tok = self.tokenizer.get_next_token()
    if tok[""style""] == ScintillaConstants.SCE_PL_WORD and tok[""text""] in (
        ""my"",
        ""state"",
    ):
        tlineNo = tok[""start_line""]
        tok = self.tokenizer.get_next_token()
        if self.classifier.is_variable(tok):
            # Don't do any more processing, as we're probably looking
            # at an open-paren.
            self.moduleInfo.doSetVar(name=tok[""text""], line=tlineNo, scope=""my"")
",if self . classifier . is_variable ( tok ) :,164
"def generate_dem_tiles(geotiff, output_dir, max_concurrency):
    try:
        colored_dem, hillshade_dem, colored_hillshade_dem = generate_colored_hillshade(
            geotiff
        )
        generate_tiles(colored_hillshade_dem, output_dir, max_concurrency)
        # Cleanup
        for f in [colored_dem, hillshade_dem, colored_hillshade_dem]:
            if os.path.isfile(f):
                os.remove(f)
    except Exception as e:
        log.ODM_WARNING(""Cannot generate DEM tiles: %s"" % str(e))
",if os . path . isfile ( f ) :,186
"def cluster(spawnpoints, radius, time_threshold):
    clusters = []
    diameter = 2 * radius
    for p in spawnpoints:
        if len(clusters) == 0:
            clusters.append(Spawncluster(p))
        else:
            c = min(clusters, key=lambda x: cost(p, x, time_threshold))
            if check_cluster(p, c, radius, time_threshold):
                c.append(p)
            else:
                c = Spawncluster(p)
                clusters.append(c)
    return clusters
",if len ( clusters ) == 0 :,152
"def pop(self):
    if self._pending_removals:
        self._commit_removals()
    while True:
        try:
            itemref = self.data.pop()
        except KeyError:
            raise KeyError(""pop from empty WeakSet"") from None
        item = itemref()
        if item is not None:
            return item
",if item is not None :,95
"def map_depends(self, dep):
    if (
        dep.endswith((""-native"", ""-native-runtime""))
        or (""nativesdk-"" in dep)
        or (""cross-canadian"" in dep)
        or (""-crosssdk-"" in dep)
    ):
        return dep
    else:
        # Do not extend for that already have multilib prefix
        var = self.d.getVar(""MULTILIB_VARIANTS"")
        if var:
            var = var.split()
            for v in var:
                if dep.startswith(v):
                    return dep
        return self.extend_name(dep)
",if dep . startswith ( v ) :,165
"def normalize_stroke(stroke):
    letters = set(stroke)
    if letters & _NUMBERS:
        if system.NUMBER_KEY in letters:
            stroke = stroke.replace(system.NUMBER_KEY, """")
        # Insert dash when dealing with 'explicit' numbers
        m = _IMPLICIT_NUMBER_RX.search(stroke)
        if m is not None:
            start = m.start(2)
            return stroke[:start] + ""-"" + stroke[start:]
    if ""-"" in letters:
        if stroke.endswith(""-""):
            stroke = stroke[:-1]
        elif letters & system.IMPLICIT_HYPHENS:
            stroke = stroke.replace(""-"", """")
    return stroke
","if stroke . endswith ( ""-"" ) :",180
"def _get_py_flags(self):
    res = dict(self.flags)
    cflags = res.pop(""cflags"", """")
    for fl in cflags.split(""|""):
        fl = fl.strip()
        if fl == ""GA_USE_DOUBLE"":
            res[""have_double""] = True
        if fl == ""GA_USE_SMALL"":
            res[""have_small""] = True
        if fl == ""GA_USE_COMPLEX"":
            res[""have_complex""] = True
        if fl == ""GA_USE_HALF"":
            res[""have_half""] = True
    return res
","if fl == ""GA_USE_COMPLEX"" :",160
"def populate(self):
    classes = self.applet.Plugins.get_classes()
    loaded = self.applet.Plugins.get_loaded()
    for name, cls in classes.items():
        if cls.is_configurable():
            desc = '<span weight=""bold"">%s</span>' % name
        else:
            desc = name
        self.list.append(
            active=(name in loaded),
            icon=cls.__icon__,
            activatable=cls.__unloadable__,
            name=name,
            desc=desc,
        )
",if cls . is_configurable ( ) :,149
"def visit_decorator(self, o: Decorator) -> None:
    if self.is_private_name(o.func.name, o.func.fullname):
        return
    is_abstract = False
    for decorator in o.original_decorators:
        if isinstance(decorator, NameExpr):
            if self.process_name_expr_decorator(decorator, o):
                is_abstract = True
        elif isinstance(decorator, MemberExpr):
            if self.process_member_expr_decorator(decorator, o):
                is_abstract = True
    self.visit_func_def(o.func, is_abstract=is_abstract)
","elif isinstance ( decorator , MemberExpr ) :",160
"def hint(self, button):
    """"""As hilight, but marks GTK Button as well""""""
    active = None
    for b in self.button_widgets.values():
        if b.widget.get_sensitive():
            b.widget.set_state(Gtk.StateType.NORMAL)
            if b.name == button:
                active = b.widget
    if active is not None:
        active.set_state(Gtk.StateType.ACTIVE)
    self.hilight(button)
",if b . widget . get_sensitive ( ) :,126
"def read_message_py2(self):
    chunks = []
    while True:
        hi, lo = self.wire.read(2)
        if hi == lo == 0:
            break
        size = hi << 8 | lo
        chunks.append(self.wire.read(size))
    message = bytearray(b"""".join(map(bytes, chunks)))
    _, n = divmod(message[0], 0x10)
    unpacker = UnpackStream(message, offset=2)
    fields = [unpacker.unpack() for _ in range(n)]
    return message[1], fields
",if hi == lo == 0 :,148
"def offsetToRva(self, offset):
    if self.inmem:
        return offset
    for s in self.sections:
        sbase = s.PointerToRawData
        if s.SizeOfRawData + s.PointerToRawData > self.getMaxRva():
            # SizeOfRawData can be misleading.
            ssize = s.VirtualSize
        else:
            ssize = max(s.SizeOfRawData, s.VirtualSize)
        if sbase <= offset and offset < sbase + ssize:
            return offset - s.PointerToRawData + s.VirtualAddress
    return 0
",if sbase <= offset and offset < sbase + ssize :,155
"def highlight_from_dir(self, workspace_dir):
    while True:
        for f in os.listdir(workspace_dir):
            if f.endswith(""trace""):
                self.process_trace(os.path.join(workspace_dir, f))
        if not self.live_update:
            break
        time.sleep(interval)
","if f . endswith ( ""trace"" ) :",92
"def check_tokenize(self, s, expected):
    # Format the tokens in s in a table format.
    # The ENDMARKER is omitted.
    result = []
    f = StringIO(s)
    for type, token, start, end, line in generate_tokens(f.readline):
        if type == ENDMARKER:
            break
        type = tok_name[type]
        result.append(""    %(type)-10.10s %(token)-13.13r %(start)s %(end)s"" % locals())
    self.assertEqual(result, expected.rstrip().splitlines())
",if type == ENDMARKER :,143
"def enable(self):
    """"""enable the patch.""""""
    for patch in self.dependencies:
        patch.enable()
    if not self.enabled:
        pyv = sys.version_info[0]
        if pyv == 2:
            if self.PY2 == SKIP:
                return  # skip patch activation
            if not self.PY2:
                raise IncompatiblePatch(""Python 2 not supported!"")
        if pyv == 3:
            if self.PY3 == SKIP:
                return  # skip patch activation
            if not self.PY3:
                raise IncompatiblePatch(""Python 3 not supported!"")
        self.pre_enable()
        self.do_enable()
        self.enabled = True
",if pyv == 3 :,191
"def __xor__(self, other):
    inc, exc = _norm_args_notimplemented(other)
    if inc is NotImplemented:
        return NotImplemented
    if inc is NotImplemented:
        return NotImplemented
    if self._included is None:
        if exc is None:  # - +
            return _ComplementSet(excluded=self._excluded - inc)
        else:  # - -
            return _ComplementSet(included=self._excluded.symmetric_difference(exc))
    else:
        if inc is None:  # + -
            return _ComplementSet(excluded=exc - self._included)
        else:  # + +
            return _ComplementSet(included=self._included.symmetric_difference(inc))
",if inc is None :,183
"def update_defaults(self, *values, **kwargs):
    for value in values:
        if type(value) == dict:
            self.DEFAULT_CONFIGURATION.update(value)
        elif isinstance(value, types.ModuleType):
            self.__defaults_from_module(value)
        elif isinstance(value, str):
            if os.path.exists(value):
                self.__defaults_from_file(value)
            else:
                logger.warning(""Configuration file {} does not exist."".format(value))
        elif isinstance(value, type(None)):
            pass
        else:
            raise ValueError(""Cannot interpret {}"".format(value))
    self.DEFAULT_CONFIGURATION.update(kwargs)
","elif isinstance ( value , str ) :",184
"def maybe_add_0000_to_all_niigz(folder):
    nii_gz = subfiles(folder, suffix="".nii.gz"")
    for n in nii_gz:
        n = remove_trailing_slash(n)
        if not n.endswith(""_0000.nii.gz""):
            os.rename(n, n[:-7] + ""_0000.nii.gz"")
","if not n . endswith ( ""_0000.nii.gz"" ) :",99
"def newstart(self):
    newstartdatetime = self._newstartdate
    if not self.checkallday.state:
        if not hasattr(self.startdt, ""tzinfo"") or self.startdt.tzinfo is None:
            tzinfo = self.conf.default.default_timezone
        else:
            tzinfo = self.startdt.tzinfo
        try:
            newstarttime = self._newstarttime
            newstartdatetime = datetime.combine(newstartdatetime, newstarttime)
            newstartdatetime = tzinfo.localize(newstartdatetime)
        except TypeError:
            return None
    return newstartdatetime
","if not hasattr ( self . startdt , ""tzinfo"" ) or self . startdt . tzinfo is None :",156
"def _fetch_all_channels(self, force=False):
    """"""Fetch all channel feeds from cache or network.""""""
    channels = self._get_channel_configs(force=force)
    enabled = self._settings.get([""enabled_channels""])
    forced = self._settings.get([""forced_channels""])
    all_channels = {}
    for key, config in channels.items():
        if key not in enabled and key not in forced:
            continue
        if ""url"" not in config:
            continue
        data = self._get_channel_data(key, config, force=force)
        if data is not None:
            all_channels[key] = data
    return all_channels
",if data is not None :,174
"def _setup_graph(self):
    vars = tf.trainable_variables()
    ops = []
    for v in vars:
        n = v.op.name
        if not n.startswith(""discrim/""):
            continue
        logger.info(""Clip {}"".format(n))
        ops.append(tf.assign(v, tf.clip_by_value(v, -0.01, 0.01)))
    self._op = tf.group(*ops, name=""clip"")
","if not n . startswith ( ""discrim/"" ) :",120
"def on_window_state_event(self, widget, event):
    if event.changed_mask & WindowState.ICONIFIED:
        if event.new_window_state & WindowState.ICONIFIED:
            log.debug(""MainWindow is minimized.."")
            component.get(""TorrentView"").save_state()
            component.pause(self.child_components)
            self.is_minimized = True
        else:
            log.debug(""MainWindow is not minimized.."")
            component.resume(self.child_components)
            self.is_minimized = False
    return False
",if event . new_window_state & WindowState . ICONIFIED :,160
"def getJsonData(self, url, decode_from=None, **kwargs):
    cache_key = md5(url)
    data = self.getCache(cache_key, url, **kwargs)
    if data:
        try:
            data = data.strip()
            if decode_from:
                data = data.decode(decode_from)
            return json.loads(data)
        except:
            log.error(
                ""Failed to parsing %s: %s"", (self.getName(), traceback.format_exc())
            )
    return []
",if decode_from :,152
"def init_weights(self):
    for n, p in self.named_parameters():
        if ""bias"" in n:
            torch.nn.init.zeros_(p)
        elif ""fc"" in n:
            torch.nn.init.xavier_uniform_(p)
","elif ""fc"" in n :",71
"def get_file_language(filename, text=None):
    """"""Get file language from filename""""""
    ext = osp.splitext(filename)[1]
    if ext.startswith("".""):
        ext = ext[1:]  # file extension with leading dot
    language = ext
    if not ext:
        if text is None:
            text, _enc = encoding.read(filename)
        for line in text.splitlines():
            if not line.strip():
                continue
            if line.startswith(""#!""):
                shebang = line[2:]
                if ""python"" in shebang:
                    language = ""python""
            else:
                break
    return language
",if text is None :,183
"def readwrite(obj, flags):
    try:
        if flags & select.POLLIN:
            obj.handle_read_event()
        if flags & select.POLLOUT:
            obj.handle_write_event()
        if flags & select.POLLPRI:
            obj.handle_expt_event()
        if flags & (select.POLLHUP | select.POLLERR | select.POLLNVAL):
            obj.handle_close()
    except OSError as e:
        if e.args[0] not in _DISCONNECTED:
            obj.handle_error()
        else:
            obj.handle_close()
    except _reraised_exceptions:
        raise
    except:
        obj.handle_error()
",if flags & select . POLLIN :,192
"def sortPlaces(self, newColumn, newOrder, force=False):
    profile_id = self.config.currentProfile()
    if newColumn == 0 and newOrder == Qt.AscendingOrder:
        if profile_id in self.placesSortLoop and self.placesSortLoop[profile_id]:
            newColumn, newOrder = 1, Qt.AscendingOrder
            self.places.header().setSortIndicator(newColumn, newOrder)
            self.placesSortLoop[profile_id] = False
        else:
            self.placesSortLoop[profile_id] = True
    self.updatePlaces()
",if profile_id in self . placesSortLoop and self . placesSortLoop [ profile_id ] :,158
"def _result_iter(self):
    pos = 0
    while 1:
        upper = len(self._result_cache)
        while pos < upper:
            yield self._result_cache[pos]
            pos = pos + 1
        if not self._iter:
            raise StopIteration
        if len(self._result_cache) <= pos:
            self._fill_cache()
",if len ( self . _result_cache ) <= pos :,102
"def get_field_type(self, name):
    fkey = (name, self.dummy)
    target = None
    op, name = name.split(""_"", 1)
    if op in {""delete"", ""insert"", ""update""}:
        target = super().get_field_type(name)
        if target is None:
            module, edb_name = self.get_module_and_name(name)
            target = self.edb_schema.get((module, edb_name), None)
            if target is not None:
                target = self.convert_edb_to_gql_type(target)
    self._fields[fkey] = target
    return target
",if target is None :,170
"def _transaction(self, args=None):
    cmd = args[0] if args else None
    if cmd == ""reset"":
        self._clean()
        return
    self._resolve()
    if cmd in [""list"", None]:
        if self.base._transaction:
            out = self.base.output.list_transaction(self.base._transaction)
            logger.info(out)
    elif cmd == ""run"":
        try:
            self.base.do_transaction()
        except dnf.exceptions.Error as e:
            logger.error(_(""Error:"") + "" "" + ucd(e))
        else:
            logger.info(_(""Complete!""))
        self._clean()
    else:
        self._help(""transaction"")
",if self . base . _transaction :,191
"def _gather_crash_info(self):
    super()._gather_crash_info()
    self._crash_info += [
        (""Commandline args"", "" "".join(sys.argv[1:])),
        (""Open Pages"", ""\n\n"".join(""\n"".join(e) for e in self._pages)),
        (""Command history"", ""\n"".join(self._cmdhist)),
        (""Objects"", self._qobjects),
    ]
    try:
        text = ""Log output was disabled.""
        if log.ram_handler is not None:
            text = log.ram_handler.dump_log()
        self._crash_info.append((""Debug log"", text))
    except Exception:
        self._crash_info.append((""Debug log"", traceback.format_exc()))
",if log . ram_handler is not None :,192
"def classifyws(s, tabwidth):
    raw = effective = 0
    for ch in s:
        if ch == "" "":
            raw = raw + 1
            effective = effective + 1
        elif ch == ""\t"":
            raw = raw + 1
            effective = (effective // tabwidth + 1) * tabwidth
        else:
            break
    return raw, effective
","elif ch == ""\t"" :",101
"def process(self, node):
    self.vars = []
    for child in node.childNodes:
        if child.nodeType == node.ELEMENT_NODE:
            child_text = get_xml_text(child)
            if child_text == """":  # pragma:nocover
                continue
            if child.nodeName == ""Real"":
                for val in re.split(""[\t ]+"", child_text):
                    self.vars.append(1.0 * eval(val))
    return self
",if child . nodeType == node . ELEMENT_NODE :,135
"def _format_privilege_data(self, data):
    for key in [""spcacl""]:
        if key in data and data[key] is not None:
            if ""added"" in data[key]:
                data[key][""added""] = parse_priv_to_db(data[key][""added""], self.acl)
            if ""changed"" in data[key]:
                data[key][""changed""] = parse_priv_to_db(data[key][""changed""], self.acl)
            if ""deleted"" in data[key]:
                data[key][""deleted""] = parse_priv_to_db(data[key][""deleted""], self.acl)
",if key in data and data [ key ] is not None :,168
"def TryMerge(self, d):
    while d.avail() > 0:
        tt = d.getVarInt32()
        if tt == 10:
            self.set_application_key(d.getPrefixedString())
            continue
        if tt == 18:
            self.set_message(d.getPrefixedString())
            continue
        if tt == 26:
            self.set_tag(d.getPrefixedString())
            continue
        if tt == 0:
            raise ProtocolBuffer.ProtocolBufferDecodeError
        d.skipData(tt)
",if tt == 0 :,152
"def test_cat(shape, cat_dim, split, dim):
    assert sum(split) == shape[cat_dim]
    gaussian = random_gaussian(shape, dim)
    parts = []
    end = 0
    for size in split:
        beg, end = end, end + size
        if cat_dim == -1:
            part = gaussian[..., beg:end]
        elif cat_dim == -2:
            part = gaussian[..., beg:end, :]
        elif cat_dim == 1:
            part = gaussian[:, beg:end]
        else:
            raise ValueError
        parts.append(part)
    actual = Gaussian.cat(parts, cat_dim)
    assert_close_gaussian(actual, gaussian)
",elif cat_dim == - 2 :,186
"def __conform__(self, interface, registry=None, default=None):
    for providedInterface in self.provided:
        if providedInterface.isOrExtends(interface):
            return self.load()
        if getAdapterFactory(providedInterface, interface, None) is not None:
            return interface(self.load(), default)
    return default
",if providedInterface . isOrExtends ( interface ) :,87
"def __init__(self, oid):
    self.oid = oid
    self.cmpt = []
    fmt = []
    for i in oid.split("".""):
        if ""-"" in i:
            fmt.append(""%i"")
            self.cmpt.append(tuple(map(int, i.split(""-""))))
        else:
            fmt.append(i)
    self.fmt = ""."".join(fmt)
","if ""-"" in i :",104
"def build_CallFunc(self, o):
    children = o.getChildren()
    # Build callee from first child
    callee = self.build(children[0])
    # Build args and kwargs from remaining children
    args = []
    kwargs = {}
    for child in children[1:]:
        class_name = child.__class__.__name__
        # None is ignored
        if class_name == ""NoneType"":
            continue
        # Keywords become kwargs
        if class_name == ""Keyword"":
            kwargs.update(self.build(child))
        # Everything else becomes args
        else:
            args.append(self.build(child))
    return callee(*args, **kwargs)
","if class_name == ""NoneType"" :",175
"def format_raises(self, e, *args, **kw):
    self.startTest()
    try:
        args[0].format(*args[1:], **kw)
    except e:
        return True
    else:
        if hasattr(e, ""__name__""):
            excName = e.__name__
        else:
            excName = str(e)
        self.fail(""%s not raised"" % excName)
    return False
","if hasattr ( e , ""__name__"" ) :",114
"def make_record_paths_absolute(self, record_dict):
    # make paths absolute
    d = {}
    for k, v in record_dict.items():
        if type(v) == str:  # filename
            if ""."" in v:
                v = os.path.join(self.path, v)
        d[k] = v
    return d
",if type ( v ) == str :,95
"def work(self):
    while self.active:
        stat = os.stat(self.filename)
        if self.last_stat is not None and self.last_stat != stat:
            self.callback(self.last_stat, stat)
        self.last_stat = stat
        time.sleep(self.interval)
",if self . last_stat is not None and self . last_stat != stat :,84
"def try_append_extension(self, path):
    append_setting = self.get_append_extension_setting()
    if self.settings.get(append_setting, False):
        if not self.is_copy_original_name(path):
            _, new_path_extension = os.path.splitext(path)
            if new_path_extension == """":
                argument_name = self.get_argument_name()
                if argument_name is None:
                    _, extension = os.path.splitext(self.view.file_name())
                else:
                    _, extension = os.path.splitext(argument_name)
                path += extension
    return path
",if argument_name is None :,181
"def _out_of_date(rw_file):
    """"""Check if a run workflow file points to an older version of manta and needs a refresh.""""""
    with open(rw_file) as in_handle:
        for line in in_handle:
            if line.startswith(""sys.path.append""):
                file_version = line.split(""/lib/python"")[0].split(""Cellar/manta/"")[-1]
                if file_version != programs.get_version_manifest(""manta""):
                    return True
    return False
","if line . startswith ( ""sys.path.append"" ) :",137
"def test_model_inference():
    x = torch.rand(1, 3, 224, 224)
    for model_name in encoding.models.pretrained_model_list():
        print(""Doing: "", model_name)
        if ""wideresnet"" in model_name:
            continue  # need multi-gpu
        model = encoding.models.get_model(model_name, pretrained=True)
        model.eval()
        y = model(x)
","if ""wideresnet"" in model_name :",115
"def _process_frame(self, frame_num, frame_im, callback=None):
    # type(int, numpy.ndarray) -> None
    """"""Adds any cuts detected with the current frame to the cutting list.""""""
    for detector in self._detector_list:
        cuts = detector.process_frame(frame_num, frame_im)
        if cuts and callback:
            callback(frame_im, frame_num)
        self._cutting_list += cuts
    for detector in self._sparse_detector_list:
        events = detector.process_frame(frame_num, frame_im)
        if events and callback:
            callback(frame_im, frame_num)
        self._event_list += events
",if cuts and callback :,178
"def __saveWork(self, work, results):
    """"""Stores the resulting last log line to the cache with the proxy key""""""
    del work
    # pylint: disable=broad-except
    try:
        if results:
            __cached = self.__cache[results[0]]
            __cached[self.__TIME] = time.time()
            __cached[self.__ETA] = results[1]
    except KeyError as e:
        # Could happen while switching jobs with work in the queue
        pass
    except Exception as e:
        list(map(logger.warning, cuegui.Utils.exceptionOutput(e)))
",if results :,154
"def _on_preference_changed(self, client, timestamp, entry, extra):
    attr = entry.key[entry.key.rindex(""/"") + 1 :]
    try:
        valuestruct = self._prefs[attr]
    except KeyError:  # unknown key, we don't care about it
        pass
    else:
        if entry.value != None:  # value has changed
            newval = getattr(entry.value, ""get_%s"" % valuestruct.type)()
            setattr(self, attr, newval)
        else:  # value has been deleted
            setattr(self, attr, valuestruct.default)
",if entry . value != None :,153
"def open(self, url, new=0, autoraise=1):
    cmdline = [self.name] + [arg.replace(""%s"", url) for arg in self.args]
    try:
        if sys.platform[:3] == ""win"":
            p = subprocess.Popen(cmdline)
        else:
            setsid = getattr(os, ""setsid"", None)
            if not setsid:
                setsid = getattr(os, ""setpgrp"", None)
            p = subprocess.Popen(cmdline, close_fds=True, preexec_fn=setsid)
        return p.poll() is None
    except OSError:
        return False
","if sys . platform [ : 3 ] == ""win"" :",170
"def get_ofs(self, dp_id):
    if len(self) == 0:
        raise ValueError(""qos sw is not connected."")
    dps = {}
    if dp_id == REST_ALL:
        dps = self
    else:
        try:
            dpid = dpid_lib.str_to_dpid(dp_id)
        except:
            raise ValueError(""Invalid switchID."")
        if dpid in self:
            dps = {dpid: self[dpid]}
        else:
            msg = ""qos sw is not connected. : switchID=%s"" % dp_id
            raise ValueError(msg)
    return dps
",if dpid in self :,173
"def __init__(self, context, keymap={}):
    if not ActionHandler._actions:
        ActionHandler._actions = Actions.get_instance(context)
    _keymap = {}
    for (k, v) in keymap.items():
        if type(v) is not set and type(v) is not list:
            v = {v}
        _keymap[k] = {op for action in v for op in translate_blenderop(action)}
    self.__dict__[""_keymap""] = _keymap
",if type ( v ) is not set and type ( v ) is not list :,125
"def setCounter(self, i):
    if 0 == i:
        if True == self.urgent:
            self.setIcon(QtGui.QIcon.fromTheme(""scudcloud-attention""))
        else:
            self.setIcon(QtGui.QIcon.fromTheme(""scudcloud""))
    elif i > 0 and i < 10:
        self.setIcon(QtGui.QIcon.fromTheme(""scudcloud-attention-"" + str(int(i))))
    elif i > 9:
        self.setIcon(QtGui.QIcon.fromTheme(""scudcloud-attention-9-plus""))
",if True == self . urgent :,146
"def consume_bytes(data):
    state_machine.receive_data(data)
    while True:
        event = state_machine.next_event()
        if event is h11.NEED_DATA:
            break
        elif isinstance(event, h11.InformationalResponse):
            # Ignore 1xx responses
            continue
        elif isinstance(event, h11.Response):
            # We have our response! Save it and get out of here.
            context[""h11_response""] = event
            raise LoopAbort
        else:
            # Can't happen
            raise RuntimeError(""Unexpected h11 event {}"".format(event))
","elif isinstance ( event , h11 . Response ) :",166
"def _evoke_request(cls):
    succeed = False
    with cls.LOCK:
        if len(cls.REQUESTING_STACK) > 0:
            resource, request_semaphore = cls.REQUESTING_STACK.pop()
            node = cls.check_availability(resource)
            if node is not None:
                cls.NODE_RESOURCE_MANAGER[node]._request(node, resource)
                logger.debug(""\nEvoking requesting resource {}"".format(resource))
                request_semaphore.release()
                succeed = True
            else:
                cls.REQUESTING_STACK.append((resource, request_semaphore))
                return
    if succeed:
        cls._evoke_request()
",if len ( cls . REQUESTING_STACK ) > 0 :,188
"def _get_related_field(self, field):
    model_class = self.Meta.model
    try:
        related_field = model_class._meta.get_field(field.source)
    except FieldDoesNotExist:
        # If `related_name` is not set, field name does not include
        # `_set` -> remove it and check again
        default_postfix = ""_set""
        if field.source.endswith(default_postfix):
            related_field = model_class._meta.get_field(
                field.source[: -len(default_postfix)]
            )
        else:
            raise
    if isinstance(related_field, ForeignObjectRel):
        return related_field.field, False
    return related_field, True
",if field . source . endswith ( default_postfix ) :,189
"def find_best_layout_for_subplots(num_subplots):
    r, c = 1, 1
    while (r * c) < num_subplots:
        if (c == (r + 1)) or (r == c):
            c += 1
        elif c == (r + 2):
            r += 1
            c -= 1
    return r, c
",if ( c == ( r + 1 ) ) or ( r == c ) :,94
"def __repr__(self):
    attrs = {}
    for name, _ in self:
        try:
            attr = getattr(self, name)
            if attr is not None:
                attrs[name] = repr(attr)
        except ValidationError:
            pass
    return ""{class_name}({fields})"".format(
        class_name=self.__class__.__name__,
        fields="", "".join(""{0[0]}={0[1]}"".format(x) for x in sorted(attrs.items())),
    )
",if attr is not None :,132
"def findsection(self, key):
    to_return = copy.deepcopy(self)
    for subsection in to_return:
        try:
            value = list(ConfigObj.find_key(to_return[subsection], key))[0]
        except Exception:
            value = None
        if not value:
            del to_return[subsection]
        else:
            for category in to_return[subsection]:
                if category != key:
                    del to_return[subsection][category]
    # cleanout empty sections and subsections
    for key in [k for (k, v) in to_return.items() if not v]:
        del to_return[key]
    return to_return
",if not value :,189
"def _get_streams(self, url, video_id, app_id_ver):
    # Sometimes the return dict does not have 'stream'
    for trial_count in range(3):
        stream_info = self._get_stream_info(
            url,
            video_id,
            app_id_ver,
            extra_note="" (try %d)"" % (trial_count + 1) if trial_count > 0 else """",
        )
        if ""stream"" in stream_info[0][""args""][0]:
            return stream_info[0][""args""][0][""stream""]
    return []
","if ""stream"" in stream_info [ 0 ] [ ""args"" ] [ 0 ] :",156
"def TryMerge(self, d):
    while d.avail() > 0:
        tt = d.getVarInt32()
        if tt == 8:
            self.set_format(d.getVarInt32())
            continue
        if tt == 18:
            self.set_path(d.getPrefixedString())
            continue
        if tt == 0:
            raise ProtocolBuffer.ProtocolBufferDecodeError
        d.skipData(tt)
",if tt == 0 :,120
"def summary(self):
    """"""Return a string with a pretty-printed summary for the company.""""""
    if not self:
        return u""""
    s = u""Company\n=======\nName: %s\n"" % self.get(""name"", u"""")
    for k in (
        ""distributor"",
        ""production company"",
        ""miscellaneous company"",
        ""special effects company"",
    ):
        d = self.get(k, [])[:5]
        if not d:
            continue
        s += u""Last movies from this company (%s): %s.\n"" % (
            k,
            u""; "".join([x.get(""long imdb title"", u"""") for x in d]),
        )
    return s
",if not d :,190
"def __call__(self, data):
    keys = set(data.keys)
    for attr_name in self._attr_names:
        if attr_name not in keys and self._strict:
            raise Exception(
                ""attr_name: {} isn t within keys: {}"".format(attr_name, keys)
            )
    for attr_name in self._attr_names:
        delattr(data, attr_name)
    return data
",if attr_name not in keys and self . _strict :,112
"def _count(self, element, count=True):
    if not isinstance(element, six.string_types):
        if self == element:
            return 1
    i = 0
    for child in self.children:
        # child is text content and element is also text content, then
        # make a simple ""text"" in ""text""
        if isinstance(child, six.string_types):
            if isinstance(element, six.string_types):
                if count:
                    i += child.count(element)
                elif element in child:
                    return 1
        else:
            i += child._count(element, count=count)
            if not count and i:
                return i
    return i
",if count :,196
"def produce_etag_headers(self, filename):
    """"""Produce a dict of curl headers containing etag headers from the download.""""""
    headers = {}
    # If the download file already exists, add some headers to the request
    # so we don't retrieve the content if it hasn't changed
    if os.path.exists(filename):
        self.existing_file_size = os.path.getsize(filename)
        etag = self.getxattr(self.xattr_etag)
        last_modified = self.getxattr(self.xattr_last_modified)
        if etag:
            headers[""If-None-Match""] = etag
        if last_modified:
            headers[""If-Modified-Since""] = last_modified
    return headers
",if last_modified :,182
"def repack(self):
    newNsp = Pfs0Stream(self._path[:-4] + "".nsp"")
    for nspF in self.hfs0[""secure""]:
        f = newNsp.add(nspF._path, nspF.size)
        nspF.rewind()
        i = 0
        pageSize = 0x10000
        while True:
            buf = nspF.read(pageSize)
            if len(buf) == 0:
                break
            i += len(buf)
            f.write(buf)
    newNsp.close()
",if len ( buf ) == 0 :,152
"def assertHasChanged(self, **kwargs):
    tracker = kwargs.pop(""tracker"", self.tracker)
    for field, value in kwargs.items():
        if value is None:
            with self.assertRaises(FieldError):
                tracker.has_changed(field)
        else:
            self.assertEqual(tracker.has_changed(field), value)
",if value is None :,91
"def check_engine(engine):
    if engine == ""auto"":
        if pa is not None:
            return ""pyarrow""
        elif fastparquet is not None:  # pragma: no cover
            return ""fastparquet""
        else:  # pragma: no cover
            raise RuntimeError(""Please install either pyarrow or fastparquet."")
    elif engine == ""pyarrow"":
        if pa is None:  # pragma: no cover
            raise RuntimeError(""Please install pyarrow fisrt."")
        return engine
    elif engine == ""fastparquet"":
        if fastparquet is None:  # pragma: no cover
            raise RuntimeError(""Please install fastparquet first."")
        return engine
    else:  # pragma: no cover
        raise RuntimeError(""Unsupported engine {} to read parquet."".format(engine))
",if pa is None :,187
"def parse_vcs_bundle_file(self, content):
    for line in content.splitlines():
        if not line.strip() or line.strip().startswith(""#""):
            continue
        match = re.search(r""^-r\s*([^ ])?"", line)
        if not match:
            return None, None
        rev = match.group(1)
        rest = line[match.end() :].strip().split(None, 1)[0]
        return rest, rev
    return None, None
","if not line . strip ( ) or line . strip ( ) . startswith ( ""#"" ) :",127
"def __init__(self, parent_instance, *args, **kwargs):
    self.parent_instance = parent_instance
    self.pk_field = kwargs.pop(""pk_field"", False)
    self.to_field = kwargs.pop(""to_field"", None)
    if self.parent_instance is not None:
        if self.to_field:
            kwargs[""initial""] = getattr(self.parent_instance, self.to_field)
        else:
            kwargs[""initial""] = self.parent_instance.pk
    kwargs[""required""] = False
    kwargs[""widget""] = InlineForeignKeyHiddenInput
    super(InlineForeignKeyField, self).__init__(*args, **kwargs)
",if self . to_field :,165
"def number_multiple_validator(v: ""Number"", field: ""ModelField"") -> ""Number"":
    field_type: ConstrainedNumber = field.type_
    if field_type.multiple_of is not None:
        mod = float(v) / float(field_type.multiple_of) % 1
        if not almost_equal_floats(mod, 0.0) and not almost_equal_floats(mod, 1.0):
            raise errors.NumberNotMultipleError(multiple_of=field_type.multiple_of)
    return v
","if not almost_equal_floats ( mod , 0.0 ) and not almost_equal_floats ( mod , 1.0 ) :",132
"def forward(self, x, edge_index, edge_attr=None):
    x_old = 0
    for i, layer in enumerate(self.hidden_layers):
        x = self.dropout(x)
        x = layer(x, edge_index)
        x = self.norm(x)
        x = self.relu(x)
        if self.skip > 0 and i % self.skip == 0:
            x = x + x_old
            x_old = x
    x = self.dropout(x)
    x = self.out_layer(x, edge_index)
    return x
",if self . skip > 0 and i % self . skip == 0 :,154
"def check_dimensions(nrow, ncol):
    if nrow is not None:
        if nrow < 1:
            warn(
                ""'nrow' must be greater than 0. "" ""Your value has been ignored."",
                PlotnineWarning,
            )
            nrow = None
        else:
            nrow = int(nrow)
    if ncol is not None:
        if ncol < 1:
            warn(
                ""'ncol' must be greater than 0. "" ""Your value has been ignored."",
                PlotnineWarning,
            )
            ncol = None
        else:
            ncol = int(ncol)
    return nrow, ncol
",if nrow < 1 :,189
"def logic():
    while 1:
        yield clock.posedge, reset.negedge
        if reset == ACTIVE_LOW:
            count.next = 0
        else:
            if enable:
                if count == -n:
                    count.next = n - 1
                else:
                    count.next = count - 1
",if count == - n :,99
"def get_whitelist(self, guild: Optional[discord.Guild] = None) -> Set[int]:
    async with self._access_lock:
        ret: Set[int]
        gid: Optional[int] = guild.id if guild else None
        if gid in self._cached_whitelist:
            ret = self._cached_whitelist[gid].copy()
        else:
            if gid is not None:
                ret = set(await self._config.guild_from_id(gid).whitelist())
            else:
                ret = set(await self._config.whitelist())
            self._cached_whitelist[gid] = ret.copy()
        return ret
",if gid in self . _cached_whitelist :,177
"def process_response(self, request, response):
    if getattr(self, ""has_session"", False):
        if getattr(request, ""user"", None) and request.user.is_authenticated():
            user = ""%s (id:%s)"" % (request.user.username, request.user.pk)
        else:
            user = ""(Anonymous)""
        self.logger.info(
            ""Session %s authenticated by %s"", request.session.session_key, user
        )
        request.session.save = self._save
        self._save = None
        self.session = None
        self.has_session = False
","if getattr ( request , ""user"" , None ) and request . user . is_authenticated ( ) :",163
"def cluster(spawnpoints, radius, time_threshold):
    clusters = []
    diameter = 2 * radius
    for p in spawnpoints:
        if len(clusters) == 0:
            clusters.append(Spawncluster(p))
        else:
            c = min(clusters, key=lambda x: cost(p, x, time_threshold))
            if check_cluster(p, c, radius, time_threshold):
                c.append(p)
            else:
                c = Spawncluster(p)
                clusters.append(c)
    return clusters
","if check_cluster ( p , c , radius , time_threshold ) :",152
"def get_shape(shape):
    """"""Convert the shape to correct dtype and vars.""""""
    ret = []
    for dim in shape:
        if isinstance(dim, tvm.tir.IntImm):
            if libinfo()[""INDEX_DEFAULT_I64""] == ""ON"":
                ret.append(dim)
            else:
                val = int(dim)
                assert val <= np.iinfo(np.int32).max
                ret.append(tvm.tir.IntImm(""int32"", val))
        elif isinstance(dim, tvm.tir.Any):
            ret.append(te.var(""any_dim"", ""int32""))
        else:
            ret.append(dim)
    return ret
","if isinstance ( dim , tvm . tir . IntImm ) :",194
"def run(self):
    queue = self.queue
    while True:
        if not self.running:
            break
        # Grab our data
        callback, requests, fetchTimeout, validityOverride = queue.get()
        # Grab prices, this is the time-consuming part
        if len(requests) > 0:
            Price.fetchPrices(requests, fetchTimeout, validityOverride)
        wx.CallAfter(callback)
        queue.task_done()
        # After we fetch prices, go through the list of waiting items and call their callbacks
        for price in requests:
            callbacks = self.wait.pop(price.typeID, None)
            if callbacks:
                for callback in callbacks:
                    wx.CallAfter(callback)
",if callbacks :,197
"def _load_scopes_(self):
    if self._model_ is None:
        tablemap = self.db._adapter.tables(self.query)
        if len(tablemap) == 1:
            self._model_ = tablemap.popitem()[1]._model_
    if self._model_:
        self._scopes_ = self._model_._instance_()._scopes_
",if len ( tablemap ) == 1 :,92
"def udp_to_tcp(udp_sock, tcp_conn):
    while True:
        msg, _ = udp_sock.recvfrom(2 ** 16)
        log_msg(""read_udp"", msg)
        if not msg:
            return
        write_tcp(tcp_conn, msg)
",if not msg :,80
"def __get_annotations(self):
    if not hasattr(self, ""_annotations""):
        self._annotations = _retrieve_annotations(
            self._adaptor, self._primary_id, self._taxon_id
        )
        if self._identifier:
            self._annotations[""gi""] = self._identifier
        if self._division:
            self._annotations[""data_file_division""] = self._division
    return self._annotations
",if self . _division :,110
"def ignore_module(module):
    result = False
    for check in ignore_these:
        if ""/*"" in check:
            if check[:-1] in module:
                result = True
        else:
            if (os.getcwd() + ""/"" + check + "".py"") == module:
                result = True
    if result:
        print_warning(""Ignoring module: "" + module)
    return result
","if ""/*"" in check :",108
"def find_commands(management_dir):
    # Modified version of function from django/core/management/__init__.py.
    command_dir = os.path.join(management_dir, ""commands"")
    commands = []
    try:
        for f in os.listdir(command_dir):
            if f.startswith(""_""):
                continue
            elif f.endswith("".py"") and f[:-3] not in commands:
                commands.append(f[:-3])
            elif f.endswith("".pyc"") and f[:-4] not in commands:
                commands.append(f[:-4])
    except OSError:
        pass
    return commands
","if f . startswith ( ""_"" ) :",164
"def _add_kid(key, x):
    if x is None:
        kids[key] = None
    else:
        if type(x) in (type([]), type(())):
            x1 = [i for i in x if isinstance(i, TVTKBase)]
            if x1:
                kids[key] = x1
        elif isinstance(x, TVTKBase):
            if hasattr(x, ""__iter__""):
                # Don't add iterable objects that contain non
                # acceptable nodes
                if len(list(x)) and isinstance(list(x)[0], TVTKBase):
                    kids[key] = x
            else:
                kids[key] = x
","if type ( x ) in ( type ( [ ] ) , type ( ( ) ) ) :",196
"def classify(self, url, text):
    for match in self.rules.match(data=text):
        if (url, match) in self.matches:
            continue
        self.matches.append((url, match))
        if self.discard_url_match(url, match):  # pragma: no cover
            continue
        self.handle_match_etags(match)
        rule = match.rule
        meta = match.meta
        tags = "","".join(["" "".join(t.split(""_"")) for t in match.tags])
        log.ThugLogging.log_classifier(""text"", url, rule, tags, meta)
    for c in self.custom_classifiers:
        self.custom_classifiers[c](url, text)
","if ( url , match ) in self . matches :",186
"def recurse(node):
    for child in node.childNodes:
        if child.nodeType != child.ELEMENT_NODE:
            continue
        if child.nodeName.upper() == ""H1"":
            return child
        if child not in visited:
            return recurse(child)
",if child . nodeType != child . ELEMENT_NODE :,76
"def try_fix_ip_range(self):
    for i in range(len(self.fake_ip_parts)):
        if self.fake_ip_parts[i] > 256:
            if i - 1 < 0:
                raise Exception(""Fake IP's out of range."")
            self.fake_ip_parts[i - 1] += 1
            self.fake_ip_parts[i] = 1
",if self . fake_ip_parts [ i ] > 256 :,106
"def run(self):
    self.thread.start()
    while self.thread.isRunning():
        if config.imager_percentage:
            self.update.emit(config.imager_percentage)
        if not self.thread.isFinished() and config.percentage == 100:
            config.imager_status_text = """"
            self.status.emit(""Please wait..."")
        time.sleep(0.1)
    self.update.emit(100)
    self.update.emit(0)
    if self.thread.isFinished():
        config.status_text = """"
        self.finished.emit()
    return
",if config . imager_percentage :,161
"def _get_trading_minutes(self, trading_date):
    trading_minutes = set()
    for account_type in self._config.base.accounts:
        if account_type == DEFAULT_ACCOUNT_TYPE.STOCK:
            trading_minutes = trading_minutes.union(
                self._get_stock_trading_minutes(trading_date)
            )
        elif account_type == DEFAULT_ACCOUNT_TYPE.FUTURE:
            trading_minutes = trading_minutes.union(
                self._get_future_trading_minutes(trading_date)
            )
    return sorted(list(trading_minutes))
",if account_type == DEFAULT_ACCOUNT_TYPE . STOCK :,169
"def lngettext(self, msgid1, msgid2, n):
    import warnings
    warnings.warn(
        ""lngettext() is deprecated, use ngettext() instead"", DeprecationWarning, 2
    )
    try:
        tmsg = self._catalog[(msgid1, self.plural(n))]
    except KeyError:
        if self._fallback:
            return self._fallback.lngettext(msgid1, msgid2, n)
        if n == 1:
            tmsg = msgid1
        else:
            tmsg = msgid2
    if self._output_charset:
        return tmsg.encode(self._output_charset)
    return tmsg.encode(locale.getpreferredencoding())
",if n == 1 :,176
"def check_langs(langs, pairs):
    messages = []
    for src, tgt in pairs:
        if src not in langs or tgt not in langs:
            messages.append(
                f""language pair {src}-{tgt} contains languages ""
                ""that are not in the language dictionary""
            )
    if len(messages) > 0:
        raise ValueError("" "".join(messages) + f""; langs: {langs}"")
",if src not in langs or tgt not in langs :,114
"def to_header(self):
    """"""Converts the object back into an HTTP header.""""""
    ranges = []
    for begin, end in self.ranges:
        if end is None:
            ranges.append(f""{begin}-"" if begin >= 0 else str(begin))
        else:
            ranges.append(f""{begin}-{end - 1}"")
    return f""{self.units}={','.join(ranges)}""
",if end is None :,100
"def name(ent, langpref=""en""):
    try:
        org = ent[""organization""]
    except KeyError:
        return None
    for info in [""organization_display_name"", ""organization_name"", ""organization_url""]:
        try:
            for item in org[info]:
                if item[""lang""] == langpref:
                    return item[""text""]
        except KeyError:
            pass
    return None
","if item [ ""lang"" ] == langpref :",112
"def check_url(value):
    validate(text, value)
    parsed = urlparse(value)
    if not parsed.netloc:
        raise ValueError(""'{0}' is not a valid URL"".format(value))
    for name, schema in attributes.items():
        if not _hasattr(parsed, name):
            raise ValueError(""Invalid URL attribute '{0}'"".format(name))
        try:
            validate(schema, _getattr(parsed, name))
        except ValueError as err:
            raise ValueError(
                ""Unable to validate URL attribute '{0}': {1}"".format(name, err)
            )
    return True
","if not _hasattr ( parsed , name ) :",158
"def stepStarted(self, step):
    self.currentStep = step
    for w in self.watchers:
        receiver = w.stepStarted(self, step)
        if receiver:
            if isinstance(receiver, type(())):
                step.subscribe(receiver[0], receiver[1])
            else:
                step.subscribe(receiver)
            d = step.waitUntilFinished()
            # TODO: This actually looks like a bug, but this code
            # will be removed anyway.
            # pylint: disable=cell-var-from-loop
            d.addCallback(lambda step: step.unsubscribe(receiver))
    step.waitUntilFinished().addCallback(self._stepFinished)
",if receiver :,183
"def assert_not_none(obj, msg=None, values=True):
    """"""Fail the test if given object is None.""""""
    _msg = ""is None""
    if obj is None:
        if msg is None:
            msg = _msg
        elif values is True:
            msg = ""%s: %s"" % (msg, _msg)
        _report_failure(msg)
",if msg is None :,99
"def _parse_date_fmt():
    fmt = get_format(""DATE_FORMAT"")
    escaped = False
    for char in fmt:
        if escaped:
            escaped = False
        elif char == ""\\"":
            escaped = True
        elif char in ""Yy"":
            yield ""year""
        elif char in ""bEFMmNn"":
            yield ""month""
        elif char in ""dj"":
            yield ""day""
","elif char == ""\\"" :",117
"def GetPluginClass(self):
    if self.plugin_name:
        plugin_cls = registry.OutputPluginRegistry.PluginClassByName(self.plugin_name)
        if plugin_cls is None:
            logging.warning(""Unknown output plugin %s"", self.plugin_name)
            return registry.OutputPluginRegistry.PluginClassByName(
                ""UnknownOutputPlugin""
            )
        return plugin_cls
",if plugin_cls is None :,109
"def command(self):
    config = self.session.config
    unregister = False
    self.session.ui.notify(_(""Watching logs: Press CTRL-C to return to the CLI""))
    try:
        while not mailpile.util.QUITTING and not config.event_log:
            time.sleep(1)
        unregister = config.event_log and config.event_log.ui_watch(self.session.ui)
        self.session.ui.unblock(force=True)
        while not mailpile.util.QUITTING:
            time.sleep(1)
    except KeyboardInterrupt:
        pass
    finally:
        if unregister:
            config.event_log.ui_unwatch(self.session.ui)
    return self._success(_(""That was fun!""))
",if unregister :,197
"def delete_rule(self, arn):
    for load_balancer_arn in self.load_balancers:
        listeners = self.load_balancers.get(load_balancer_arn).listeners.values()
        for listener in listeners:
            for rule in listener.rules:
                if rule.arn == arn:
                    listener.remove_rule(rule)
                    return
",if rule . arn == arn :,104
"def __dragBegin(self, widget, event):
    if event.buttons & (event.Buttons.Left | event.Buttons.Middle):
        GafferUI.Pointer.setCurrent(""nodes"")
        if len(self.__graphComponents) == 1:
            return next(iter(self.__graphComponents))
        else:
            return Gaffer.StandardSet(self.__graphComponents)
    return None
",if len ( self . __graphComponents ) == 1 :,103
"def _get_strategy_name(self):
    frame = sys._getframe()
    while frame:
        st = frame.f_locals.get(""self"")
        if isinstance(st, StrategyBase):
            return ""%s.%s"" % (type(st).__module__, type(st).__name__)
        frame = frame.f_back
    return """"
","if isinstance ( st , StrategyBase ) :",88
"def getCommitFromFile(short=True):
    global _gitdir
    branch = getBranchFromFile()
    commit = None
    if _gitdir and branch:
        if branch == ""HEAD"":
            commitFile = os.path.join(_gitdir, ""HEAD"")
        else:
            commitFile = os.path.join(_gitdir, ""refs"", ""heads"", branch)
        if os.path.isfile(commitFile):
            with open(commitFile, ""r"", encoding=""utf-8"") as f:
                commit = f.readline().strip()
    if short and commit:
        return commit[:8]
    else:
        return commit
",if os . path . isfile ( commitFile ) :,169
"def _register_aliases_from_pack(self, pack, aliases):
    registered_count = 0
    for alias in aliases:
        try:
            LOG.debug(""Loading alias from %s."", alias)
            self._register_action_alias(pack, alias)
        except Exception as e:
            if self._fail_on_failure:
                msg = 'Failed to register alias ""%s"" from pack ""%s"": %s' % (
                    alias,
                    pack,
                    str(e),
                )
                raise ValueError(msg)
            LOG.exception(""Unable to register alias: %s"", alias)
            continue
        else:
            registered_count += 1
    return registered_count
",if self . _fail_on_failure :,199
"def pop_many(self, limit=None):
    if limit is None:
        limit = DEFAULT_SYNC_OFFLINE_ACTIVITY
    heartbeats = []
    count = 0
    while count < limit:
        heartbeat = self.pop()
        if not heartbeat:
            break
        heartbeats.append(heartbeat)
        count += 1
        if count % HEARTBEATS_PER_REQUEST == 0:
            yield heartbeats
            heartbeats = []
    if heartbeats:
        yield heartbeats
",if not heartbeat :,140
"def makeChunkVertices(self, chunk):
    if (
        chunk.root_tag
        and ""Level"" in chunk.root_tag
        and ""TileTicks"" in chunk.root_tag[""Level""]
    ):
        ticks = chunk.root_tag[""Level""][""TileTicks""]
        if len(ticks):
            self.vertexArrays.append(
                self._computeVertices(
                    [[t[i].value for i in ""xyz""] for t in ticks],
                    (0xFF, 0xFF, 0xFF, 0x44),
                    chunkPosition=chunk.chunkPosition,
                )
            )
    yield
",if len ( ticks ) :,175
"def read_bytes_from_url(url: str, optional=False) -> bytes:
    if parse_args().print_commands:
        print_stderr(color_line(""=> "", 14) + f""GET {url}"")
    req = request.Request(url)
    try:
        response = request.urlopen(req)
    except URLError as exc:
        print_error(""urllib: "" + str(exc.reason))
        if optional:
            return b""""
        if ask_to_continue(_(""Do you want to retry?"")):
            return read_bytes_from_url(url, optional=optional)
        raise SysExit(102)
    result_bytes = response.read()
    return result_bytes
",if optional :,178
"def h2i(self, pkt, x):
    if x is not None:
        if x <= -180.00000005:
            warning(""Fixed3_7: Input value too negative: %.8f"" % x)
            x = -180.0
        elif x >= 180.00000005:
            warning(""Fixed3_7: Input value too positive: %.8f"" % x)
            x = 180.0
        x = int(round((x + 180.0) * 1e7))
    return x
",elif x >= 180.00000005 :,132
"def replace_incompatible_files():
    for filename, version_info in PYTHON_VERSION_REQUIREMENTS.items():
        if sys.version_info >= version_info:
            continue
        version = ""."".join(str(v) for v in version_info)
        code = INCOMPATIBLE_PYTHON_VERSION_PLACEHOLDER.format(version=version)
        with open(filename, ""w"") as f:
            f.write(code)
",if sys . version_info >= version_info :,110
"def __eq__(self, other):
    if self.__class__ != other.__class__:
        return False
    for attr in [""bar"", ""baz"", ""quux""]:
        if hasattr(self, attr) != hasattr(other, attr):
            return False
        elif getattr(self, attr, None) != getattr(other, attr, None):
            return False
    return True
","if hasattr ( self , attr ) != hasattr ( other , attr ) :",95
"def get_content_length(download):
    try:
        meta = download.info()
        if hasattr(meta, ""getheaders"") and hasattr(meta.getheaders, ""Content-Length""):
            return int(meta.getheaders(""Content-Length"")[0])
        elif hasattr(download, ""getheader"") and download.getheader(""Content-Length""):
            return int(download.getheader(""Content-Length""))
        elif hasattr(meta, ""getheader"") and meta.getheader(""Content-Length""):
            return int(meta.getheader(""Content-Length""))
    except Exception:
        pass
    return 0
","elif hasattr ( meta , ""getheader"" ) and meta . getheader ( ""Content-Length"" ) :",149
"def set_size(self, size):
    assert len(size) == 2
    width, height = size
    if width == -1:
        for button in self._buttons_list:
            cur_width = button.GetSize()[self.WIDTH]
            if cur_width > width:
                width = cur_width
    if height == -1:
        for button in self._buttons_list:
            cur_height = button.GetSize()[self.HEIGHT]
            if cur_height > height:
                height = cur_height
    if self._squared:
        width = height = width if width > height else height
    for button in self._buttons_list:
        button.SetMinSize((width, height))
",if cur_width > width :,187
"def _default_config(self):
    if sys.platform.startswith(""win""):
        return {""name"": ""Command Prompt"", ""cmd"": ""cmd.exe"", ""env"": {}}
    else:
        if ""SHELL"" in os.environ:
            shell = os.environ[""SHELL""]
            if os.path.basename(shell) == ""tcsh"":
                cmd = [shell, ""-l""]
            else:
                cmd = [shell, ""-i"", ""-l""]
        else:
            cmd = [""/bin/bash"", ""-i"", ""-l""]
        return {""name"": ""Login Shell"", ""cmd"": cmd, ""env"": {}}
","if os . path . basename ( shell ) == ""tcsh"" :",170
"def log_sock(s, event_type=None):
    if sock_silent:
        pass
    else:
        if event_type is None:
            logsocket.sendto(ensure_str(s), (host, port))
        elif event_type in show_event:
            logsocket.sendto(ensure_str(s), (host, port))
        else:
            pass
",elif event_type in show_event :,103
"def check_eventref_citations(self, obj):
    if obj:
        for event_ref in obj.get_event_ref_list():
            if self.check_attribute_citations(event_ref):
                return True
            event = self.dbstate.db.get_event_from_handle(event_ref.ref)
            if self.check_event_citations(event):
                return True
    return False
",if self . check_attribute_citations ( event_ref ) :,116
"def __exit__(self, exc_type, exc_value, traceback):
    self.nest -= 1
    if self.nest == 0:
        try:
            self.con.__exit__(exc_type, exc_value, traceback)
            self.close()
        except Exception as exc:
            if self.debug:
                self.debug.write(""EXCEPTION from __exit__: {}"".format(exc))
            raise
",if self . debug :,109
"def construct_instances(self, row, keys=None):
    collected_models = {}
    for i, (key, constructor, attr, conv) in enumerate(self.column_map):
        if keys is not None and key not in keys:
            continue
        value = row[i]
        if key not in collected_models:
            collected_models[key] = constructor()
        instance = collected_models[key]
        if attr is None:
            attr = self.cursor.description[i][0]
        if conv is not None:
            value = conv(value)
        setattr(instance, attr, value)
    return collected_models
",if key not in collected_models :,167
"def delete(self):
    """"""Completely shut down pulseaudio client.""""""
    if self._pa_context is not None:
        assert _debug(""PulseAudioContext.delete"")
        if self.is_ready:
            pa.pa_context_disconnect(self._pa_context)
            while self.state is not None and not self.is_terminated:
                self.wait()
        self._disconnect_callbacks()
        pa.pa_context_unref(self._pa_context)
        self._pa_context = None
",if self . is_ready :,136
"def _hstack(self, other, prefix=None):
    """"""Join the columns of the other DataFrame to this one, assuming the ordering is the same""""""
    assert len(self) == len(
        other
    ), ""does not make sense to horizontally stack DataFrames with different lengths""
    for name in other.get_column_names():
        if prefix:
            new_name = prefix + name
        else:
            new_name = name
        self.add_column(new_name, other.columns[name])
",if prefix :,127
"def smart_linkflags(source, target, env, for_signature):
    if cplusplus.iscplusplus(source):
        build_dir = env.subst(""$BUILDDIR"", target=target, source=source)
        if build_dir:
            return ""-qtempinc="" + os.path.join(build_dir, ""tempinc"")
    return """"
",if build_dir :,91
"def read(self, size):
    x = len(self.buf)
    while x < size:
        raw = self.fileobj.read(self.blocksize)
        if not raw:
            break
        data = self.bz2obj.decompress(raw)
        self.buf += data
        x += len(data)
    buf = self.buf[:size]
    self.buf = self.buf[size:]
    self.pos += len(buf)
    return buf
",if not raw :,120
"def set_ok_verifiability(self, cookie, request):
    if request.unverifiable and is_third_party(request):
        if cookie.version > 0 and self.strict_rfc2965_unverifiable:
            _debug(""   third-party RFC 2965 cookie during "" ""unverifiable transaction"")
            return False
        elif cookie.version == 0 and self.strict_ns_unverifiable:
            _debug(""   third-party Netscape cookie during "" ""unverifiable transaction"")
            return False
    return True
",elif cookie . version == 0 and self . strict_ns_unverifiable :,138
"def update_sockets(self, context):
    bools = [self.min_list, self.max_list, self.size_list]
    dims = int(self.dimensions[0])
    for i in range(3):
        for j in range(3):
            out_index = 4 + j + 3 * i
            hidden = self.outputs[out_index].hide_safe
            if bools[i][j] and j < dims:
                if hidden:
                    self.outputs[out_index].hide_safe = False
            else:
                self.outputs[out_index].hide_safe = True
        updateNode(self, context)
",if bools [ i ] [ j ] and j < dims :,173
"def hash_of_file(path):
    """"""Return the hash of a downloaded file.""""""
    with open(path, ""r"") as archive:
        sha = sha256()
        while True:
            data = archive.read(2 ** 20)
            if not data:
                break
            sha.update(data)
    return encoded_hash(sha)
",if not data :,95
"def _compute_early_outs(self, quotas):
    for q in quotas:
        if q.closed and not self._ignore_closed:
            self.results[q] = Quota.AVAILABILITY_ORDERED, 0
        elif q.size is None:
            self.results[q] = Quota.AVAILABILITY_OK, None
        elif q.size == 0:
            self.results[q] = Quota.AVAILABILITY_GONE, 0
",elif q . size == 0 :,118
"def providers_for_config_string(config_string, netcode):
    providers = []
    for d in config_string.split():
        p = provider_for_descriptor_and_netcode(d, netcode)
        if p:
            providers.append(p)
        else:
            warnings.warn(""can't parse provider %s in config string"" % d)
    return providers
",if p :,100
"def _get_plugin_value(self, feature, actor):
    for plugin in plugins.all(version=2):
        handlers = safe_execute(plugin.get_feature_hooks, _with_transaction=False)
        for handler in handlers or ():
            rv = handler(feature, actor)
            if rv is not None:
                return rv
    return None
",if rv is not None :,94
"def test_digit_numeric_consistent(self):
    # Test that digit and numeric are consistent,
    # i.e. if a character has a digit value,
    # its numeric value should be the same.
    count = 0
    for i in xrange(0x10000):
        c = unichr(i)
        dec = self.db.digit(c, -1)
        if dec != -1:
            self.assertEqual(dec, self.db.numeric(c))
            count += 1
    self.assertTrue(count >= 10)  # should have tested at least the ASCII digits
",if dec != - 1 :,144
"def call(command, title, retry):
    """"""Run a command-line program and display the result.""""""
    if Options.rerun_args:
        command, title, retry = Options.rerun_args
        Options.rerun_args = None
        success = call(command, title, retry)
        if not success:
            return False
    print("""")
    print(""$ %s"" % "" "".join(command))
    failure = subprocess.call(command)
    if failure and retry:
        Options.rerun_args = command, title, retry
    return not failure
",if not success :,141
"def handle_custom_actions(self):
    for _, action in CustomAction.registry.items():
        if action.resource != self.resource:
            continue
        if action.action not in self.parser.choices:
            self.parser.add_parser(action.action, help="""")
        action(self.page).add_arguments(self.parser, self)
",if action . resource != self . resource :,92
"def __init__(self, user, *args, **kwargs):
    self.user = user
    super(AccountSettingsForm, self).__init__(*args, **kwargs)
    if self.user.is_managed:
        # username and password always managed, email and
        # name optionally managed
        for field in (""email"", ""name"", ""username""):
            if field == ""username"" or field in settings.SENTRY_MANAGED_USER_FIELDS:
                self.fields[field] = ReadOnlyTextField(label=self.fields[field].label)
        # don't show password field at all
        del self.fields[""new_password""]
    # don't show username field if its the same as their email address
    if self.user.email == self.user.username:
        del self.fields[""username""]
","if field == ""username"" or field in settings . SENTRY_MANAGED_USER_FIELDS :",199
"def eval(self, code, eval=True, raw=False):
    self._engine._append_source(code)
    try:
        result = self._context.eval(code)
    except quickjs.JSException as e:
        raise ProgramError(*e.args)
    else:
        if eval:
            if raw or not isinstance(result, quickjs.Object):
                return result
            elif callable(result) and self.typeof(result) == u""function"":
                return self.Function(self, result)
            else:
                return json.loads(result.json())
",if eval :,158
"def get_def_offsets(self, defloc):
    """"""Get the byte offsets for a definition.""""""
    defn = self.defs[defloc.def_id]
    typ = defn.typ
    if typ == ""Attribute"":
        start, end = self._get_attr_bounds(defn.name, defloc.location)
    else:
        start = self.source.get_offset(defloc.location)
        if typ in DEF_OFFSETS:
            start += DEF_OFFSETS[typ]
        end = start + len(defn.name)
    return (start, end)
",if typ in DEF_OFFSETS :,147
"def RemoveRefCountOutput(data):
    while 1:
        last_line_pos = data.rfind(""\n"")
        if not re.match(""\[\d+ refs\]"", data[last_line_pos + 1 :]):
            break
        if last_line_pos < 0:
            # All the output
            return """"
        data = data[:last_line_pos]
    return data
",if last_line_pos < 0 :,103
"def traverse_before_reduce(operator):
    """"""Internal traverse function""""""
    if isinstance(operator, tvm.te.PlaceholderOp):
        return
    if tag.is_injective(operator.tag):
        sch[operator].compute_inline()
        for tensor in operator.input_tensors:
            if tensor.op not in scheduled_ops:
                traverse_before_reduce(tensor.op)
    else:
        raise RuntimeError(""Unsupported operator: %s"" % operator.tag)
    scheduled_ops.append(operator)
",if tensor . op not in scheduled_ops :,133
"def _get_config(key):
    config = db.session.execute(
        Configs.__table__.select().where(Configs.key == key)
    ).fetchone()
    if config and config.value:
        value = config.value
        if value and value.isdigit():
            return int(value)
        elif value and isinstance(value, string_types):
            if value.lower() == ""true"":
                return True
            elif value.lower() == ""false"":
                return False
            else:
                return value
    # Flask-Caching is unable to roundtrip a value of None.
    # Return an exception so that we can still cache and avoid the db hit
    return KeyError
","if value . lower ( ) == ""true"" :",181
"def find_executable(names):
    # Given a list of executable names, find the first one that is available
    # as an executable file, on the path.
    for name in names:
        fpath, fname = os.path.split(name)
        if fpath:
            # The given name is absolute.
            if is_executable(name):
                return name
        else:
            # Try to find the name on the PATH
            for path in os.environ[""PATH""].split(os.pathsep):
                exe_file = os.path.join(path, name)
                if is_executable(exe_file):
                    return exe_file
    # Could not find it :(
    return None
",if is_executable ( name ) :,186
"def push(self):
    advice = self.check()
    if not self._context[""silent""]:
        if not self.hasPendingSync(advice):
            print(""No changes to push."")
            return
        choice = input(""Continue? y/N:"")
        if choice != ""y"":
            print(""Aborted on user command"")
            return
    print(""push local changes to remote..."")
    self._publish.syncRemote(self._context[""srcroot""], advice)
",if not self . hasPendingSync ( advice ) :,123
"def __init__(self, itemtype, cnf={}, *, master=None, **kw):
    if not master:
        if ""refwindow"" in kw:
            master = kw[""refwindow""]
        elif ""refwindow"" in cnf:
            master = cnf[""refwindow""]
        else:
            master = tkinter._default_root
            if not master:
                raise RuntimeError(
                    ""Too early to create display style: "" ""no root window""
                )
    self.tk = master.tk
    self.stylename = self.tk.call(""tixDisplayStyle"", itemtype, *self._options(cnf, kw))
","if ""refwindow"" in kw :",167
"def __call__(self, x, **kwargs):
    h = x
    for layer, argnames, accept_var_args in zip(
        self.layers, self.argnames, self.accept_var_args
    ):
        if accept_var_args:
            layer_kwargs = kwargs
        else:
            layer_kwargs = {k: v for k, v in kwargs.items() if k in argnames}
        h = layer(h, **layer_kwargs)
    return h
",if accept_var_args :,121
"def run_train_loop(self):
    self.begin_training()
    for _ in self.yield_train_step():
        if self.should_save_model():
            self.save_model()
        if self.should_save_checkpoint():
            self.save_checkpoint()
        if self.should_eval_model():
            self.eval_model()
        if self.should_break_training():
            break
    self.eval_model()
    self.done_training()
    return self.returned_result()
",if self . should_save_checkpoint ( ) :,139
"def configure_callback(conf):
    """"""Received configuration information""""""
    global ZK_HOSTS
    for node in conf.children:
        if node.key == ""Hosts"":
            ZK_HOSTS = node.values[0].split("","")
        else:
            collectd.warning(""zookeeper plugin: Unknown config key: %s."" % node.key)
    log(""Configured with hosts=%s"" % (ZK_HOSTS))
","if node . key == ""Hosts"" :",108
"def inner(self, *args, **kwargs):
    """"""Inner.""""""
    if not is_internet_available():
        LOGGER.debug(""\n\n%s"", func.__name__)
        LOGGER.debug(""============================"")
        if func.__doc__:
            LOGGER.debug('"""""" %s """"""', func.__doc__.strip())
        LOGGER.debug(""----------------------------"")
        LOGGER.debug(""Skipping because no Internet connection available."")
        LOGGER.debug(""\n++++++++++++++++++++++++++++"")
        return None
    result = func(self, *args, **kwargs)
    return result
",if func . __doc__ :,155
"def _shares_in_results(data):
    shares_in_device, shares_in_subdevice = False, False
    for plugin_name, plugin_result in data.iteritems():
        if plugin_result[""status""] == ""error"":
            continue
        if ""device"" not in plugin_result:
            continue
        if ""disk_shares"" in plugin_result[""device""]:
            shares_in_device = True
        for subdevice in plugin_result[""device""].get(""subdevices"", []):
            if ""disk_shares"" in subdevice:
                shares_in_subdevice = True
                break
    return shares_in_device, shares_in_subdevice
","if ""disk_shares"" in plugin_result [ ""device"" ] :",175
"def register_auth_provider_blueprints(cls, app, prefix=""/auth/login""):
    app.auth_providers = []
    for provider in app.config.get(""AUTH_PROVIDERS"", [""debug"", ""oauth""]):
        if not isinstance(provider, KnowledgeAuthProvider):
            provider = cls._get_subclass_for(provider.lower())(name=provider, app=app)
        app.register_blueprint(
            provider.blueprint, url_prefix=""/"".join((prefix, provider.name))
        )
        app.auth_providers.append(provider)
","if not isinstance ( provider , KnowledgeAuthProvider ) :",144
"def getText(self, stuff):
    if isinstance(stuff, Fighter):
        active = [x.name for x in stuff.abilities if x.active]
        if len(active) == 0:
            return ""None""
        return "", "".join(active)
",if len ( active ) == 0 :,69
"def run(self, paths=[]):
    items = []
    for item in SideBarSelection(paths).getSelectedItems():
        if item.isUnderCurrentProject():
            items.append(item.url(""url_production""))
    if len(items) > 0:
        sublime.set_clipboard(""\n"".join(items))
        if len(items) > 1:
            sublime.status_message(""Items URL copied"")
        else:
            sublime.status_message(""Item URL copied"")
",if len ( items ) > 1 :,131
"def read_boolean(file: BinaryIO, count: int, checkall: bool = False) -> List[bool]:
    if checkall:
        all_defined = file.read(1)
        if all_defined != unhexlify(""00""):
            return [True] * count
    result = []
    b = 0
    mask = 0
    for i in range(count):
        if mask == 0:
            b = ord(file.read(1))
            mask = 0x80
        result.append(b & mask != 0)
        mask >>= 1
    return result
",if mask == 0 :,146
"def __prep_write_total(self, comments, main, fallback, single):
    lower = self.as_lowercased()
    for k in [main, fallback, single]:
        if k in comments:
            del comments[k]
    if single in lower:
        parts = lower[single].split(""/"", 1)
        if parts[0]:
            comments[single] = [parts[0]]
        if len(parts) > 1:
            comments[main] = [parts[1]]
    if main in lower:
        comments[main] = lower.list(main)
    if fallback in lower:
        if main in comments:
            comments[fallback] = lower.list(fallback)
        else:
            comments[main] = lower.list(fallback)
",if main in comments :,196
"def _filter_medias_not_commented(self, media_items):
    not_commented_medias = []
    for media in media_items:
        if media.get(""comment_count"", 0) > 0 and media.get(""comments""):
            my_comments = [
                comment
                for comment in media[""comments""]
                if comment[""user_id""] == self.user_id
            ]
            if my_comments:
                continue
        not_commented_medias.append(media)
    return not_commented_medias
","if comment [ ""user_id"" ] == self . user_id",152
"def run(url):
    import os
    for fpath in [
        os.path.expanduser(""~/Applications/zeal.app""),
        ""/Applications/zeal.app"",
    ]:
        if os.path.exists(fpath + ""/Contents/MacOS/zeal""):
            import subprocess, pipes
            pid = subprocess.Popen(
                [
                    fpath + ""/Contents/MacOS/zeal"",
                    ""--query={0}"".format(pipes.quote(url)),
                ],
                stdout=subprocess.PIPE,
                stderr=subprocess.PIPE,
                stdin=subprocess.PIPE,
            )
            return
","if os . path . exists ( fpath + ""/Contents/MacOS/zeal"" ) :",176
"def get_input_info(exec_info, network):
    input_dict = collections.OrderedDict()
    for v in exec_info.data_variable:
        input_dict[v.variable_name] = []
    for v in network.variable:
        if v.name in input_dict:
            shape = v.shape.dim
            input_dict[v.name] = [x if x > 0 else batch_size for x in shape]
    return input_dict
",if v . name in input_dict :,118
"def _clean_text(self, text):
    """"""Performs invalid character removal and whitespace cleanup on text.""""""
    output = []
    char_idx = []
    for i, char in enumerate(text):
        cp = ord(char)
        if cp == 0 or cp == 0xFFFD or _is_control(char):
            continue
        if _is_whitespace(char):
            output.append("" "")
            char_idx.append(i)
        else:
            output.append(char)
            char_idx.append(i)
    return """".join(output), char_idx
",if _is_whitespace ( char ) :,151
"def AddVersion(version, ns, versionId="""", isLegacy=0, serviceNs=""""):
    if not ns:
        ns = serviceNs
    if version not in parentMap:
        nsMap[version] = ns
        if len(versionId) > 0:
            versionMap[ns + ""/"" + versionId] = version
        if isLegacy or ns is """":
            versionMap[ns] = version
        versionIdMap[version] = versionId
        if not serviceNs:
            serviceNs = ns
        serviceNsMap[version] = serviceNs
        parentMap[version] = set()
","if isLegacy or ns is """" :",150
"def set_accessible_async(self, trans, id=None, accessible=False):
    """"""Set workflow's importable attribute and slug.""""""
    stored = self.get_stored_workflow(trans, id)
    # Only set if importable value would change; this prevents a change in the update_time unless attribute really changed.
    importable = accessible in [""True"", ""true"", ""t"", ""T""]
    if stored and stored.importable != importable:
        if importable:
            self._make_item_accessible(trans.sa_session, stored)
        else:
            stored.importable = importable
        trans.sa_session.flush()
    return
",if importable :,158
"def update(self, val, n=1):
    if val is not None:
        self.val = val
        if n > 0:
            self.sum = type_as(self.sum, val) + (val * n)
            self.count = type_as(self.count, n) + n
",if n > 0 :,80
"def run(self, root):
    footnotesDiv = self.footnotes.makeFootnotesDiv(root)
    if footnotesDiv is not None:
        result = self.footnotes.findFootnotesPlaceholder(root)
        if result:
            child, parent, isText = result
            ind = list(parent).index(child)
            if isText:
                parent.remove(child)
                parent.insert(ind, footnotesDiv)
            else:
                parent.insert(ind + 1, footnotesDiv)
                child.tail = None
        else:
            root.append(footnotesDiv)
",if result :,175
"def ehp(self):
    if self.__ehp is None:
        if self.damagePattern is None:
            ehp = self.hp
        else:
            ehp = self.damagePattern.calculateEhp(self)
        self.__ehp = ehp
    return self.__ehp
",if self . damagePattern is None :,80
"def literal(self):
    if self.peek('""'):
        lit, lang, dtype = self.eat(r_literal).groups()
        if lang:
            lang = lang
        else:
            lang = None
        if dtype:
            dtype = dtype
        else:
            dtype = None
        if lang and dtype:
            raise ParseError(""Can't have both a language and a datatype"")
        lit = unquote(lit)
        return Literal(lit, lang, dtype)
    return False
",if dtype :,132
"def _purge(self, queue):
    """"""Remove all messages from `queue`.""""""
    count = 0
    queue_find = ""."" + queue + "".msg""
    folder = os.listdir(self.data_folder_in)
    while len(folder) > 0:
        filename = folder.pop()
        try:
            # only purge messages for the requested queue
            if filename.find(queue_find) < 0:
                continue
            filename = os.path.join(self.data_folder_in, filename)
            os.remove(filename)
            count += 1
        except OSError:
            # we simply ignore its existence, as it was probably
            # processed by another worker
            pass
    return count
",if filename . find ( queue_find ) < 0 :,189
"def check(data_dir, decrypter, read_only=False):
    fname = os.path.join(data_dir, DIGEST_NAME)
    if os.path.exists(fname):
        if decrypter is None:
            return False
        f = open(fname, ""rb"")
        s = f.read()
        f.close()
        return decrypter.decrypt(s) == MAGIC_STRING
    else:
        if decrypter is not None:
            if read_only:
                return False
            else:
                s = decrypter.encrypt(MAGIC_STRING)
                f = open(fname, ""wb"")
                f.write(s)
                f.close()
        return True
",if decrypter is not None :,198
"def on_train_epoch_end(self, trainer, pl_module, outputs):
    epoch = trainer.current_epoch
    if self.unfreeze_backbone_at_epoch <= epoch:
        optimizer = trainer.optimizers[0]
        current_lr = optimizer.param_groups[0][""lr""]
        backbone_lr = self.previous_backbone_lr
        if epoch < 6:
            assert backbone_lr <= current_lr
        else:
            assert backbone_lr == current_lr
",if epoch < 6 :,127
"def parse_rsync_url(location):
    """"""Parse a rsync-style URL.""""""
    if "":"" in location and ""@"" not in location:
        # SSH with no user@, zero or one leading slash.
        (host, path) = location.split("":"", 1)
        user = None
    elif "":"" in location:
        # SSH with user@host:foo.
        user_host, path = location.split("":"", 1)
        if ""@"" in user_host:
            user, host = user_host.rsplit(""@"", 1)
        else:
            user = None
            host = user_host
    else:
        raise ValueError(""not a valid rsync-style URL"")
    return (user, host, path)
","if ""@"" in user_host :",178
"def populate_settings_dict(form, settings):
    new_settings = {}
    for key, value in iteritems(settings):
        try:
            # check if the value has changed
            if value == form[key].data:
                continue
            else:
                new_settings[key] = form[key].data
        except KeyError:
            pass
    return new_settings
",if value == form [ key ] . data :,105
"def draw_boxes(image, boxes, scores=None, drop_score=0.5):
    if scores is None:
        scores = [1] * len(boxes)
    for (box, score) in zip(boxes, scores):
        if score < drop_score:
            continue
        box = np.reshape(np.array(box), [-1, 1, 2]).astype(np.int64)
        image = cv2.polylines(np.array(image), [box], True, (255, 0, 0), 2)
    return image
",if score < drop_score :,136
"def update(self, instance, validated_data):
    for category, category_data in validated_data.items():
        if not category_data:
            continue
        self.update_validated_settings(category_data)
        for field_name, field_value in category_data.items():
            setattr(getattr(instance, category), field_name, field_value)
    return instance
",if not category_data :,98
"def insert(self, menuName, position, label, command, underline=None):
    menu = self.getMenu(menuName)
    if menu:
        if underline is None:
            menu.insert(position, ""command"", label=label, command=command)
        else:
            menu.insert(
                position, ""command"", label=label, command=command, underline=underline
            )
",if underline is None :,104
"def delete_old_links():
    for doc in web.ctx.site.store.values(type=""account-link""):
        expiry_date = datetime.strptime(doc[""expires_on""], ""%Y-%m-%dT%H:%M:%S.%f"")
        now = datetime.utcnow()
        key = doc[""_key""]
        if expiry_date > now:
            print(""Deleting link %s"" % (key))
            del web.ctx.site.store[key]
        else:
            print(""Retaining link %s"" % (key))
",if expiry_date > now :,141
"def _object(o: edgedb.Object):
    ret = {}
    for attr in dir(o):
        try:
            link = o[attr]
        except (KeyError, TypeError):
            link = None
        if link:
            ret[attr] = serialize(link)
        else:
            ret[attr] = serialize(getattr(o, attr))
    return ret
",if link :,102
"def __init__(self, items):
    self._format = string.join(map(lambda item: item[0], items), """")
    self._items = items
    self._buffer_ = win32wnet.NCBBuffer(struct.calcsize(self._format))
    for format, name in self._items:
        if len(format) == 1:
            if format == ""c"":
                val = ""\0""
            else:
                val = 0
        else:
            l = int(format[:-1])
            val = ""\0"" * l
        self.__dict__[name] = val
","if format == ""c"" :",158
"def prepare_text(text, style):
    body = []
    for fragment, sty in parse_tags(text, style, subs.styles):
        fragment = fragment.replace(r""\h"", "" "")
        fragment = fragment.replace(r""\n"", ""\n"")
        fragment = fragment.replace(r""\N"", ""\n"")
        if sty.italic:
            fragment = ""<i>%s</i>"" % fragment
        if sty.underline:
            fragment = ""<u>%s</u>"" % fragment
        if sty.strikeout:
            fragment = ""<s>%s</s>"" % fragment
        body.append(fragment)
    return re.sub(""\n+"", ""\n"", """".join(body).strip())
",if sty . italic :,180
"def get_from_target(target):
    domains = set()
    if isinstance(target, str):
        if target.endswith("".txt""):
            logger.log(""FATAL"", ""Use targets parameter for multiple domain names"")
            exit(1)
        domain = match_main_domain(target)
        if not domain:
            return domains
        domains.add(domain)
    return domains
","if target . endswith ( "".txt"" ) :",101
"def iterate(self, prod_, rule_):
    newProduction = """"
    for i in range(len(prod_)):
        step = self.production[i]
        if step == ""W"":
            newProduction = newProduction + self.ruleW
        elif step == ""X"":
            newProduction = newProduction + self.ruleX
        elif step == ""Y"":
            newProduction = newProduction + self.ruleY
        elif step == ""Z"":
            newProduction = newProduction + self.ruleZ
        elif step != ""F"":
            newProduction = newProduction + step
    self.drawLength = self.drawLength * 0.5
    self.generations += 1
    return newProduction
","if step == ""W"" :",179
"def cancel_pp(self, nzo_id):
    """"""Change the status, so that the PP is canceled""""""
    for nzo in self.history_queue:
        if nzo.nzo_id == nzo_id:
            nzo.abort_direct_unpacker()
            if nzo.pp_active:
                nzo.pp_active = False
                try:
                    # Try to kill any external running process
                    self.external_process.kill()
                    logging.info(
                        ""Killed external process %s"", self.external_process.args[0]
                    )
                except:
                    pass
            return True
    return None
",if nzo . nzo_id == nzo_id :,196
"def list_backends(debug):
    for backend in sorted(
        backends.getBackendList(), key=lambda backend: backend.identifier
    ):
        if debug:
            print(
                ""{:>15} : {} ({})"".format(
                    backend.identifier, backend.__doc__, backend.__name__
                )
            )
        else:
            print(""{:>15} : {}"".format(backend.identifier, backend.__doc__))
",if debug :,116
"def _geo_indices(cls, inspected=None):
    inspected = inspected or []
    geo_indices = []
    inspected.append(cls)
    for field in cls._fields.values():
        if hasattr(field, ""document_type""):
            field_cls = field.document_type
            if field_cls in inspected:
                continue
            if hasattr(field_cls, ""_geo_indices""):
                geo_indices += field_cls._geo_indices(inspected)
        elif field._geo_index:
            geo_indices.append(field)
    return geo_indices
","if hasattr ( field , ""document_type"" ) :",155
"def run_test_family(tests, mode_filter, files, open_func, *make_args):
    for test_func in tests:
        if test_func is None:
            out.write(""\n"")
            continue
        if mode_filter in test_func.file_open_mode:
            continue
        for s in test_func.file_sizes:
            name, size = files[size_names[s]]
            # name += file_ext
            args = tuple(f(name, size) for f in make_args)
            run_one_test(name, size, open_func, test_func, *args)
",if mode_filter in test_func . file_open_mode :,168
"def TryMerge(self, d):
    while d.avail() > 0:
        tt = d.getVarInt32()
        if tt == 10:
            self.set_application_key(d.getPrefixedString())
            continue
        if tt == 18:
            self.set_message(d.getPrefixedString())
            continue
        if tt == 26:
            self.set_tag(d.getPrefixedString())
            continue
        if tt == 0:
            raise ProtocolBuffer.ProtocolBufferDecodeError
        d.skipData(tt)
",if tt == 18 :,152
"def _on_config_changed(self, option: str) -> None:
    if option in [""zoom.levels"", ""zoom.default""]:
        if not self._default_zoom_changed:
            factor = float(config.val.zoom.default) / 100
            self.set_factor(factor)
        self._init_neighborlist()
",if not self . _default_zoom_changed :,86
"def keyPressEvent(self, event):
    """"""Add up and down arrow key events to built in functionality.""""""
    keyPressed = event.key()
    if keyPressed in [Constants.UP_KEY, Constants.DOWN_KEY, Constants.TAB_KEY]:
        if keyPressed == Constants.UP_KEY:
            self.index = max(0, self.index - 1)
        elif keyPressed == Constants.DOWN_KEY:
            self.index = min(len(self.completerStrings) - 1, self.index + 1)
        elif keyPressed == Constants.TAB_KEY and self.completerStrings:
            self.tabPressed()
        if self.completerStrings:
            self.setTextToCompleterIndex()
    super(CueLineEdit, self).keyPressEvent(event)
",if self . completerStrings :,192
"def maxRange(self):
    attrs = (
        ""shieldTransferRange"",
        ""powerTransferRange"",
        ""energyDestabilizationRange"",
        ""empFieldRange"",
        ""ecmBurstRange"",
        ""maxRange"",
    )
    for attr in attrs:
        maxRange = self.getModifiedItemAttr(attr, None)
        if maxRange is not None:
            return maxRange
    if self.charge is not None:
        delay = self.getModifiedChargeAttr(""explosionDelay"", None)
        speed = self.getModifiedChargeAttr(""maxVelocity"", None)
        if delay is not None and speed is not None:
            return delay / 1000.0 * speed
",if delay is not None and speed is not None :,186
"def decref(self, *keys):
    for tileable_key, tileable_id in keys:
        if tileable_key not in self._executed_tileables:
            continue
        _graph_key, ids = self._executed_tileables[tileable_key]
        if tileable_id in ids:
            ids.remove(tileable_id)
            # for those same key tileables, do decref only when all those tileables are garbage collected
            if len(ids) != 0:
                continue
            self.delete_data(tileable_key)
",if tileable_key not in self . _executed_tileables :,137
"def run(self):
    # Make some objects emit lights
    for obj in bpy.context.scene.objects:
        if ""modelId"" in obj:
            obj_id = obj[""modelId""]
            # In the case of the lamp
            if obj_id in self.lights:
                self._make_lamp_emissive(obj, self.lights[obj_id])
            # Make the windows emit light
            if obj_id in self.windows:
                self._make_window_emissive(obj)
            # Also make ceilings slightly emit light
            if obj.name.startswith(""Ceiling#""):
                self._make_ceiling_emissive(obj)
","if obj . name . startswith ( ""Ceiling#"" ) :",190
"def _create_bucket(self):
    """"""Create remote S3 bucket if it doesn't exist""""""
    resource = boto3.resource(""s3"")
    try:
        resource.meta.client.head_bucket(Bucket=self.bucket)
    except ClientError as e:
        error_code = int(e.response[""Error""][""Code""])
        if error_code == 404:
            resource.create_bucket(Bucket=self.bucket)
        else:
            raise
",if error_code == 404 :,118
"def sort_sizes(size_list):
    """"""Sorts sizes with extensions. Assumes that size is already in largest unit possible""""""
    final_list = []
    for suffix in ["" B"", "" KB"", "" MB"", "" GB"", "" TB""]:
        sub_list = [
            float(size[: -len(suffix)])
            for size in size_list
            if size.endswith(suffix) and size[: -len(suffix)][-1].isnumeric()
        ]
        sub_list.sort()
        final_list += [(str(size) + suffix) for size in sub_list]
        # Skip additional loops
        if len(final_list) == len(size_list):
            break
    return final_list
",if len ( final_list ) == len ( size_list ) :,183
"def rename_var(block: paddle.device.framework.Block, old_name: str, new_name: str):
    """""" """"""
    for op in block.ops:
        for input_name in op.input_arg_names:
            if input_name == old_name:
                op._rename_input(old_name, new_name)
        for output_name in op.output_arg_names:
            if output_name == old_name:
                op._rename_output(old_name, new_name)
    block._rename_var(old_name, new_name)
",if output_name == old_name :,155
"def _GetParserChains(self, events):
    """"""Return a dict with a plugin count given a list of events.""""""
    parser_chains = {}
    for event in events:
        parser_chain = getattr(event, ""parser"", None)
        if not parser_chain:
            continue
        if parser_chain in parser_chains:
            parser_chains[parser_chain] += 1
        else:
            parser_chains[parser_chain] = 1
    return parser_chains
",if not parser_chain :,122
"def context(self):
    # Needed to avoid Translate Toolkit construct ID
    # as context\04source
    if self.template is not None:
        if self.template.id:
            return self.template.id
        if self.template.context:
            return self.template.context
        return self.template.getid()
    return self.unescape_csv(self.mainunit.getcontext())
",if self . template . id :,103
"def _validate_min_max_value(field_name, value, opt):
    if isinstance(value, (int, float)):
        if value < opt[""minValue""] or value > opt[""maxValue""]:
            raise ValueError(
                ""Invalid value %s assigned "" ""to field %s.\n"" % (value, field_name)
            )
    elif isinstance(value, str):
        if len(value) < opt[""minValue""] or len(value) > opt[""maxValue""]:
            raise ValueError(
                ""Invalid value %s assigned "" ""to field %s.\n"" % (value, field_name)
            )
","if value < opt [ ""minValue"" ] or value > opt [ ""maxValue"" ] :",164
"def _incr_internal(key, instance=None, tags=None, amount=1):
    from sentry.app import tsdb
    if _should_sample():
        amount = _sampled_value(amount)
        if instance:
            full_key = ""{}.{}"".format(key, instance)
        else:
            full_key = key
        try:
            tsdb.incr(tsdb.models.internal, full_key, count=amount)
        except Exception:
            logger = logging.getLogger(""sentry.errors"")
            logger.exception(""Unable to incr internal metric"")
",if instance :,150
"def get(self, key, default=None, version=None):
    key = self.make_key(key, version=version)
    self.validate_key(key)
    fname = self._key_to_file(key)
    try:
        with open(fname, ""rb"") as f:
            exp = pickle.load(f)
            now = time.time()
            if exp < now:
                self._delete(fname)
            else:
                return pickle.load(f)
    except (IOError, OSError, EOFError, pickle.PickleError):
        pass
    return default
",if exp < now :,156
"def on_execution_scenario(self, cpath, scenario):
    if isinstance(scenario, dict):
        self.check_scenario(cpath, scenario)
    elif isinstance(scenario, str):
        scenario_name = scenario
        scenario_path = Path(""scenarios"", scenario_name)
        scenario = self.linter.get_config_value(scenario_path, raise_if_not_found=False)
        if not scenario:
            self.report(
                ConfigWarning.ERROR,
                ""undefined-scenario"",
                cpath,
                ""scenario %r is used but isn't defined"" % scenario_name,
            )
",if not scenario :,167
"def getSubmitKey(request, response):
    titleId = request.bits[2]
    titleKey = request.bits[3]
    try:
        if blockchain.blockchain.suggest(titleId, titleKey):
            return success(request, response, ""Key successfully added"")
        else:
            return error(request, response, ""Key validation failed"")
    except LookupError as e:
        error(request, response, str(e))
    except OSError as e:
        error(request, response, str(e))
    except BaseException as e:
        error(request, response, str(e))
","if blockchain . blockchain . suggest ( titleId , titleKey ) :",151
"def test_downstream_trials(trial_associated_artifact, trial_obj, sagemaker_session):
    # allow trial components to index, 30 seconds max
    for i in range(3):
        time.sleep(10)
        trials = trial_associated_artifact.downstream_trials(
            sagemaker_session=sagemaker_session
        )
        if len(trials) > 0:
            break
    assert len(trials) == 1
    assert trial_obj.trial_name in trials
",if len ( trials ) > 0 :,125
"def get_subfield_asts(context, return_type, field_asts):
    subfield_asts = DefaultOrderedDict(list)
    visited_fragment_names = set()
    for field_ast in field_asts:
        selection_set = field_ast.selection_set
        if selection_set:
            subfield_asts = collect_fields(
                context,
                return_type,
                selection_set,
                subfield_asts,
                visited_fragment_names,
            )
    return subfield_asts
",if selection_set :,140
"def _handle_children(self, removed, added):
    # Stop all the removed children.
    for obj in removed:
        obj.stop()
    # Process the new objects.
    for obj in added:
        obj.set(scene=self.scene, parent=self)
        if isinstance(obj, ModuleManager):
            obj.source = self
        elif is_filter(obj):
            obj.inputs.append(self)
        if self.running:
            try:
                obj.start()
            except:
                exception()
","if isinstance ( obj , ModuleManager ) :",148
"def __kmp_search(S, W):
    m = 0
    i = 0
    T = __kmp_table(W)
    while m + i < len(S):
        if S[m + i] == W[i]:
            i += 1
            if i == len(W):
                yield m
                m += i - T[i]
                i = max(T[i], 0)
        else:
            m += i - T[i]
            i = max(T[i], 0)
",if S [ m + i ] == W [ i ] :,144
"def connection(self, commit_on_success=False):
    with self._lock:
        if self._bulk_commit:
            if self._pending_connection is None:
                self._pending_connection = sqlite.connect(self.filename)
            con = self._pending_connection
        else:
            con = sqlite.connect(self.filename)
        try:
            if self.fast_save:
                con.execute(""PRAGMA synchronous = 0;"")
            yield con
            if commit_on_success and self.can_commit:
                con.commit()
        finally:
            if not self._bulk_commit:
                con.close()
",if self . _bulk_commit :,182
"def passed(self):
    for test in self.lints[0]:
        for template in self.lints[0][test][""results""]:
            results = self.lints[0][test][""results""][template]
            if results:
                if self._is_error(results) or self.strict:
                    return False
    return True
",if self . _is_error ( results ) or self . strict :,92
"def testCheckIPGenerator(self):
    for i, ip in self._ip_range(65536 if not unittest.F2B.fast else 1000):
        if i == 254:
            self.assertEqual(str(ip), ""127.0.0.255"")
        elif i == 255:
            self.assertEqual(str(ip), ""127.0.1.0"")
        elif i == 1000:
            self.assertEqual(str(ip), ""127.0.3.233"")
        elif i == 65534:
            self.assertEqual(str(ip), ""127.0.255.255"")
        elif i == 65535:
            self.assertEqual(str(ip), ""127.1.0.0"")
",elif i == 255 :,181
"def _DecodeUnknownMessages(message, encoded_message, pair_type):
    """"""Process unknown fields in encoded_message of a message type.""""""
    field_type = pair_type.value.type
    new_values = []
    all_field_names = [x.name for x in message.all_fields()]
    for name, value_dict in encoded_message.iteritems():
        if name in all_field_names:
            continue
        value = PyValueToMessage(field_type, value_dict)
        new_pair = pair_type(key=name, value=value)
        new_values.append(new_pair)
    return new_values
",if name in all_field_names :,161
"def test_apply_noise_model():
    p = Program(RX(np.pi / 2, 0), RX(np.pi / 2, 1), CZ(0, 1), RX(np.pi / 2, 1))
    noise_model = _decoherence_noise_model(_get_program_gates(p))
    pnoisy = apply_noise_model(p, noise_model)
    for i in pnoisy:
        if isinstance(i, DefGate):
            pass
        elif isinstance(i, Pragma):
            assert i.command in [""ADD-KRAUS"", ""READOUT-POVM""]
        elif isinstance(i, Gate):
            assert i.name in NO_NOISE or not i.params
","if isinstance ( i , DefGate ) :",187
"def i2h(self, pkt, x):
    if x is not None:
        if x < 0:
            warning(""Fixed3_7: Internal value too negative: %d"" % x)
            x = 0
        elif x > 3600000000:
            warning(""Fixed3_7: Internal value too positive: %d"" % x)
            x = 3600000000
        x = (x - 1800000000) * 1e-7
    return x
",elif x > 3600000000 :,116
"def onClicked(event):
    shaderConfig = dict()
    for child in self.shaderDefBox.children:
        defName = child.shaderDefine
        enabled = child.isChecked()
        try:
            if enabled:
                mat.addShaderDefine(defName)
            else:
                mat.removeShaderDefine(defName)
        except:
            pass
    # Reload material properties (update enabled states) and shader uniforms
    self.listUniforms(mat)
    self.listMaterialSettings(self.getSelectedObject())
",if enabled :,147
"def is_mod(self, member: discord.Member) -> bool:
    """"""Checks if a member is a mod or admin of their guild.""""""
    try:
        member_snowflakes = member._roles  # DEP-WARN
        for snowflake in await self._config.guild(member.guild).admin_role():
            if member_snowflakes.has(snowflake):  # DEP-WARN
                return True
        for snowflake in await self._config.guild(member.guild).mod_role():
            if member_snowflakes.has(snowflake):  # DEP-WARN
                return True
    except AttributeError:  # someone passed a webhook to this
        pass
    return False
",if member_snowflakes . has ( snowflake ) :,186
"def _verify_treestore(itr, tree_values):
    i = 0
    while itr:
        values = tree_values[i]
        if treestore[itr][0] != values[0]:
            return False
        if treestore.iter_children(itr):
            if not _verify_treestore(treestore.iter_children(itr), values[1]):
                return False
        itr = treestore.iter_next(itr)
        i += 1
    return True
","if not _verify_treestore ( treestore . iter_children ( itr ) , values [ 1 ] ) :",132
"def _default_config(self):
    if sys.platform.startswith(""win""):
        return {""name"": ""Command Prompt"", ""cmd"": ""cmd.exe"", ""env"": {}}
    else:
        if ""SHELL"" in os.environ:
            shell = os.environ[""SHELL""]
            if os.path.basename(shell) == ""tcsh"":
                cmd = [shell, ""-l""]
            else:
                cmd = [shell, ""-i"", ""-l""]
        else:
            cmd = [""/bin/bash"", ""-i"", ""-l""]
        return {""name"": ""Login Shell"", ""cmd"": cmd, ""env"": {}}
","if ""SHELL"" in os . environ :",170
"def _messageHandled(self, resultList):
    failures = 0
    for (success, result) in resultList:
        if not success:
            failures += 1
            log.err(result)
    if failures:
        msg = ""Could not send e-mail""
        resultLen = len(resultList)
        if resultLen > 1:
            msg += "" ({} failures out of {} recipients)"".format(failures, resultLen)
        self.sendCode(550, networkString(msg))
    else:
        self.sendCode(250, b""Delivery in progress"")
",if not success :,147
"def to_internal_value(self, data):
    site = get_current_site()
    pages_root = reverse(""pages-root"")
    ret = []
    for path in data:
        if path.startswith(pages_root):
            path = path[len(pages_root) :]
        # strip any final slash
        if path.endswith(""/""):
            path = path[:-1]
        page = get_page_from_path(site, path)
        if page:
            ret.append(page)
    return ret
","if path . endswith ( ""/"" ) :",136
"def _prune(self):
    with self.lock:
        entries = self._list_dir()
        if len(entries) > self._threshold:
            now = time.time()
            try:
                for i, fpath in enumerate(entries):
                    remove = False
                    f = LockedFile(fpath, ""rb"")
                    exp = pickle.load(f.file)
                    f.close()
                    remove = exp <= now or i % 3 == 0
                    if remove:
                        self._del_file(fpath)
            except Exception:
                pass
",if remove :,173
"def get_ax_arg(uri):
    if not ax_ns:
        return u""""
    prefix = ""openid."" + ax_ns + "".type.""
    ax_name = None
    for name in self.request.arguments.keys():
        if self.get_argument(name) == uri and name.startswith(prefix):
            part = name[len(prefix) :]
            ax_name = ""openid."" + ax_ns + "".value."" + part
            break
    if not ax_name:
        return u""""
    return self.get_argument(ax_name, u"""")
",if self . get_argument ( name ) == uri and name . startswith ( prefix ) :,150
"def _generate_expression(self):
    # turn my _format attribute into the _expression attribute
    e = []
    for part in PARSE_RE.split(self._format):
        if not part:
            continue
        elif part == ""{{"":
            e.append(r""\{"")
        elif part == ""}}"":
            e.append(r""\}"")
        elif part[0] == ""{"" and part[-1] == ""}"":
            # this will be a braces-delimited field to handle
            e.append(self._handle_field(part))
        else:
            # just some text to match
            e.append(REGEX_SAFETY.sub(self._regex_replace, part))
    return """".join(e)
","elif part == ""{{"" :",189
"def get_clean_username(user):
    try:
        username = force_text(user)
    except AttributeError:
        # AnonymousUser may not have USERNAME_FIELD
        username = ""anonymous""
    else:
        # limit changed_by and created_by to avoid problems with Custom User Model
        if len(username) > PAGE_USERNAME_MAX_LENGTH:
            username = u""{0}... (id={1})"".format(
                username[: PAGE_USERNAME_MAX_LENGTH - 15],
                user.pk,
            )
    return username
",if len ( username ) > PAGE_USERNAME_MAX_LENGTH :,144
"def process_request(self, request):
    for old, new in self.names_name:
        request.uri = request.uri.replace(old, new)
        if is_text_payload(request) and request.body:
            try:
                body = (
                    str(request.body, ""utf-8"")
                    if isinstance(request.body, bytes)
                    else str(request.body)
                )
            except TypeError:  # python 2 doesn't allow decoding through str
                body = str(request.body)
            if old in body:
                request.body = body.replace(old, new)
    return request
",if old in body :,182
"def get_config_variable(self, name, methods=(""env"", ""config""), default=None):
    value = None
    config_name, envvar_name = self.session_var_map[name]
    if methods is not None:
        if ""env"" in methods and value is None:
            value = os.environ.get(envvar_name)
        if ""config"" in methods and value is None:
            value = self.config_file_vars.get(config_name)
    else:
        value = default
    return value
","if ""config"" in methods and value is None :",134
"def get_field_by_name(obj, field):
    # Dereference once
    if obj.type.code == gdb.TYPE_CODE_PTR:
        obj = obj.dereference()
    for f in re.split(""(->|\.|\[\d+\])"", field):
        if not f:
            continue
        if f == ""->"":
            obj = obj.dereference()
        elif f == ""."":
            pass
        elif f.startswith(""[""):
            n = int(f.strip(""[]""))
            obj = obj.cast(obj.dereference().type.pointer())
            obj += n
            obj = obj.dereference()
        else:
            obj = obj[f]
    return obj
","if f == ""->"" :",189
"def read_subpkgdata_dict(pkg, d):
    ret = {}
    subd = read_pkgdatafile(get_subpkgedata_fn(pkg, d))
    for var in subd:
        newvar = var.replace(""_"" + pkg, """")
        if newvar == var and var + ""_"" + pkg in subd:
            continue
        ret[newvar] = subd[var]
    return ret
","if newvar == var and var + ""_"" + pkg in subd :",104
"def _classify_volume(self, ctxt, volumes):
    bypass_volumes = []
    replica_volumes = []
    for v in volumes:
        volume_type = self._get_volume_replicated_type(ctxt, v)
        grp = v.group
        if grp and utils.is_group_a_type(grp, ""consistent_group_replication_enabled""):
            continue
        elif volume_type and v.status in [""available"", ""in-use""]:
            replica_volumes.append(v)
        else:
            bypass_volumes.append(v)
    return bypass_volumes, replica_volumes
","elif volume_type and v . status in [ ""available"" , ""in-use"" ] :",159
"def _ensure_entity_values(self):
    entities_values = {
        entity.name: self._get_entity_values(entity) for entity in self.entities
    }
    for intent in self.intents:
        for utterance in intent.utterances:
            for chunk in utterance.slot_chunks:
                if chunk.text is not None:
                    continue
                try:
                    chunk.text = next(entities_values[chunk.entity])
                except StopIteration:
                    raise DatasetFormatError(
                        ""At least one entity value must be provided for ""
                        ""entity '%s'"" % chunk.entity
                    )
    return self
",if chunk . text is not None :,188
"def _consume_msg(self):
    async for data in self._stream:
        stream = data.get(""ev"")
        if stream:
            await self._dispatch(data)
        elif data.get(""status"") == ""disconnected"":
            # Polygon returns this on an empty 'ev' id..
            data[""ev""] = ""status""
            await self._dispatch(data)
            raise ConnectionResetError(
                ""Polygon terminated connection: "" f'({data.get(""message"")})'
            )
","elif data . get ( ""status"" ) == ""disconnected"" :",135
"def GetHeaderWidth(self):
    """"""Returns the header window width, in pixels.""""""
    if not self._headerWidth:
        count = self.GetColumnCount()
        for col in range(count):
            if not self.IsColumnShown(col):
                continue
            self._headerWidth += self.GetColumnWidth(col)
    if self.HasAGWFlag(ULC_FOOTER):
        self._footerWidth = self._headerWidth
    return self._headerWidth
",if not self . IsColumnShown ( col ) :,123
"def testCheckIPGenerator(self):
    for i, ip in self._ip_range(65536 if not unittest.F2B.fast else 1000):
        if i == 254:
            self.assertEqual(str(ip), ""127.0.0.255"")
        elif i == 255:
            self.assertEqual(str(ip), ""127.0.1.0"")
        elif i == 1000:
            self.assertEqual(str(ip), ""127.0.3.233"")
        elif i == 65534:
            self.assertEqual(str(ip), ""127.0.255.255"")
        elif i == 65535:
            self.assertEqual(str(ip), ""127.1.0.0"")
",if i == 254 :,181
"def childrenTodo(self, p=None):
    if p is None:
        p = self.c.currentPosition()
    for p in p.children():
        if self.getat(p.v, ""priority"") != 9999:
            continue
        self.setat(p.v, ""priority"", 19)
        self.loadIcons(p)
","if self . getat ( p . v , ""priority"" ) != 9999 :",92
"def __init__(self, **kwargs):
    super(DepthwiseSeparableASPPModule, self).__init__(**kwargs)
    for i, dilation in enumerate(self.dilations):
        if dilation > 1:
            self[i] = DepthwiseSeparableConvModule(
                self.in_channels,
                self.channels,
                3,
                dilation=dilation,
                padding=dilation,
                norm_cfg=self.norm_cfg,
                act_cfg=self.act_cfg,
            )
",if dilation > 1 :,146
"def test_char(self):
    for x in range(256):
        c = System.Char.Parse(chr(x))
        self.assertEqual(c, chr(x))
        self.assertEqual(chr(x), c)
        if c == chr(x):
            pass
        else:
            self.assertTrue(False)
        if not c == chr(x):
            self.assertTrue(False)
        if chr(x) == c:
            pass
        else:
            self.assertTrue(False)
        if not chr(x) == c:
            self.assertTrue(False)
",if not c == chr ( x ) :,162
"def create_model_handler(ns, model_type):
    @route(f""/<provider>/{ns}/<model_id>"")
    @use_provider
    def handle(req, provider, model_id):
        # special cases:
        # fuo://<provider>/users/me -> show current logged user
        if model_type == ModelType.user:
            if model_id == ""me"":
                user = getattr(provider, ""_user"", None)
                if user is None:
                    raise CmdException(f""log in provider:{provider.identifier} first"")
                return user
        model = get_model_or_raise(provider, model_type, model_id)
        return model
",if model_type == ModelType . user :,184
"def __str__(self, prefix="""", printElemNumber=0):
    res = """"
    if self.has_key_:
        res += prefix + (""key: %s\n"" % self.DebugFormatString(self.key_))
    cnt = 0
    for e in self.value_:
        elm = """"
        if printElemNumber:
            elm = ""(%d)"" % cnt
        res += prefix + (""value%s: %s\n"" % (elm, self.DebugFormatString(e)))
        cnt += 1
    if self.has_partial_:
        res += prefix + (""partial: %s\n"" % self.DebugFormatBool(self.partial_))
    return res
",if printElemNumber :,170
"def set_value_type_index(self, rows: list, value_type_index: int):
    for row in rows:
        label = self.message_type[row]
        if not label.is_checksum_label:
            label.value_type_index = value_type_index
            self.protocol_label_updated.emit(label)
    self.update()
",if not label . is_checksum_label :,96
"def get_model_param(self, job_id, cpn_name, role, party_id):
    result = None
    party_id = str(party_id)
    try:
        result = self.client.component.output_model(
            job_id=job_id, role=role, party_id=party_id, component_name=cpn_name
        )
        if ""data"" not in result:
            raise ValueError(
                f""job {job_id}, component {cpn_name} has no output model param""
            )
        return result[""data""]
    except:
        raise ValueError(""Cannot get output model, err msg: "")
","if ""data"" not in result :",173
"def validate(self) -> None:
    if self.query:
        if not self.sysupgrade:
            for arg_name in (""aur"", ""repo""):
                if getattr(self, arg_name):
                    raise MissingArgument(""sysupgrade"", arg_name)
","if getattr ( self , arg_name ) :",74
"def print_nested_help(self, args: argparse.Namespace) -> None:
    level = 0
    parser = self.main_parser
    while True:
        if parser._subparsers is None:
            break
        if parser._subparsers._actions is None:
            break
        choices = parser._subparsers._actions[-1].choices
        value = getattr(args, ""level_%d"" % level)
        if value is None:
            parser.print_help()
            return
        if not choices:
            break
        if isinstance(choices, dict):
            parser = choices[value]
        else:
            return
        level += 1
",if parser . _subparsers . _actions is None :,175
"def merge(self, abort=False, message=None):
    """"""Merge remote branch or reverts the merge.""""""
    if abort:
        self.execute([""update"", ""--clean"", "".""])
    elif self.needs_merge():
        if self.needs_ff():
            self.execute([""update"", ""--clean"", ""remote(.)""])
        else:
            self.configure_merge()
            # Fallback to merge
            try:
                self.execute([""merge"", ""-r"", ""remote(.)""])
            except RepositoryException as error:
                if error.retcode == 255:
                    # Nothing to merge
                    return
                raise
            self.execute([""commit"", ""--message"", ""Merge""])
",if error . retcode == 255 :,191
"def parseArtistIds(cls, page):
    ids = list()
    js = demjson.decode(page)
    if ""error"" in js and js[""error""]:
        raise PixivException(""Error when requesting Fanbox"", 9999, page)
    if ""body"" in js and js[""body""] is not None:
        js_body = js[""body""]
        if ""supportingPlans"" in js[""body""]:
            js_body = js_body[""supportingPlans""]
        for creator in js_body:
            ids.append(creator[""user""][""userId""])
    return ids
","if ""supportingPlans"" in js [ ""body"" ] :",148
"def ignore(self, other):
    if isinstance(other, Suppress):
        if other not in self.ignoreExprs:
            super().ignore(other)
            if self.expr is not None:
                self.expr.ignore(self.ignoreExprs[-1])
    else:
        super().ignore(other)
        if self.expr is not None:
            self.expr.ignore(self.ignoreExprs[-1])
    return self
",if self . expr is not None :,117
"def execute(self):
    func = self.func
    is_batch_func = getattr(func, ""_task_batch"", False)
    g[""current_task_is_batch""] = is_batch_func
    g[""current_tasks""] = [self]
    try:
        if is_batch_func:
            return func([{""args"": self.args, ""kwargs"": self.kwargs}])
        else:
            return func(*self.args, **self.kwargs)
    finally:
        g[""current_task_is_batch""] = None
        g[""current_tasks""] = None
",if is_batch_func :,148
"def fn(value=None):
    for i in [-1, 0, 1, 2, 3, 4]:
        if i < 0:
            continue
        elif i == 0:
            yield 0
        elif i == 1:
            yield 1
            i = 0
            yield value
            yield 2
        else:
            try:
                v = i / value
            except:
                v = i
            yield v
",elif i == 1 :,127
"def get_instrumentation_key(url):
    data = url.split(""//"")[1]
    try:
        uuid.UUID(data)
    except ValueError:
        values = data.split(""/"")
        if len(values) != 2:
            AppInsightsHelper.log.warning(""Bad format: '%s'"" % url)
        return AppInsightsHelper._get_instrumentation_key(values[0], values[1])
    return data
",if len ( values ) != 2 :,114
"def get_correct(ngrams_ref, ngrams_test, correct, total):
    for rank in ngrams_test:
        for chain in ngrams_test[rank]:
            total[rank] += ngrams_test[rank][chain]
            if chain in ngrams_ref[rank]:
                correct[rank] += min(ngrams_test[rank][chain], ngrams_ref[rank][chain])
    return correct, total
",if chain in ngrams_ref [ rank ] :,103
"def _content_type_params__set(self, value_dict):
    if not value_dict:
        del self.content_type_params
        return
    params = []
    for k, v in sorted(value_dict.items()):
        if not _OK_PARAM_RE.search(v):
            v = '""%s""' % v.replace('""', '\\""')
        params.append(""; %s=%s"" % (k, v))
    ct = self.headers.pop(""Content-Type"", """").split("";"", 1)[0]
    ct += """".join(params)
    self.headers[""Content-Type""] = ct
",if not _OK_PARAM_RE . search ( v ) :,152
"def split_file(self, filename, block_size=2 ** 20):
    with open(filename, ""rb"") as f:
        file_list = []
        while True:
            data = f.read(block_size)
            if not data:
                break
            filehash = os.path.join(self.resource_dir, self.__count_hash(data))
            filehash = os.path.normpath(filehash)
            with open(filehash, ""wb"") as fwb:
                fwb.write(data)
            file_list.append(filehash)
    return file_list
",if not data :,164
"def _set_live(self, live, _):
    if live is not None and not self.live:
        if isinstance(live, basestring):
            live = [live]
        # Default is to use Memory analysis.
        if len(live) == 0:
            mode = ""Memory""
        elif len(live) == 1:
            mode = live[0]
        else:
            raise RuntimeError(""--live parameter should specify only one mode."")
        live_plugin = self.session.plugins.live(mode=mode)
        live_plugin.live()
        # When the session is destroyed, close the live plugin.
        self.session.register_flush_hook(self, live_plugin.close)
    return live
",elif len ( live ) == 1 :,184
"def process_percent(token, state, command_line):
    if not state.is_range_start_line_parsed:
        if command_line.line_range.start:
            raise ValueError(""bad range: {0}"".format(state.scanner.state.source))
        command_line.line_range.start.append(token)
    else:
        if command_line.line_range.end:
            raise ValueError(""bad range: {0}"".format(state.scanner.state.source))
        command_line.line_range.end.append(token)
    return parse_line_ref, command_line
",if command_line . line_range . start :,154
"def gprv_implicit_orax(ii):
    for i, op in enumerate(_gen_opnds(ii)):
        if i == 0:
            if op.name == ""REG0"" and op_luf(op, ""GPRv_SB""):
                continue
            else:
                return False
        elif i == 1:
            if op.name == ""REG1"" and op_luf(op, ""OrAX""):
                continue
            else:
                return False
        else:
            return False
    return True
","if op . name == ""REG1"" and op_luf ( op , ""OrAX"" ) :",151
"def _check_events(self):
    # make sure song-started and song-ended match up
    stack = []
    old = self.events[:]
    for type_, song in self.events:
        if type_ == ""started"":
            stack.append(song)
        elif type_ == ""ended"":
            self.assertTrue(stack.pop(-1) is song, msg=old)
    self.assertFalse(stack, msg=old)
","if type_ == ""started"" :",110
"def _minimal_replacement_cost(self, first, second):
    first_symbols, second_symbols = set(), set()
    removal_cost, insertion_cost = 0, 0
    for a, b in itertools.zip_longest(first, second, fillvalue=None):
        if a is not None:
            first_symbols.add(a)
        if b is not None:
            second_symbols.add(b)
        removal_cost = max(removal_cost, len(first_symbols - second_symbols))
        insertion_cost = max(insertion_cost, len(second_symbols - first_symbols))
    return min(removal_cost, insertion_cost)
",if b is not None :,173
"def get_default_backend(self, user_backends):
    retval = None
    n_defaults = 0
    for name in user_backends:
        args = user_backends.get(name)
        if args.get(""default"", False):
            n_defaults = n_defaults + 1
            if retval is None:
                retval = name
    return (retval, n_defaults)
","if args . get ( ""default"" , False ) :",100
"def ensure_echo_on():
    if termios:
        fd = sys.stdin
        if fd.isatty():
            attr_list = termios.tcgetattr(fd)
            if not attr_list[3] & termios.ECHO:
                attr_list[3] |= termios.ECHO
                if hasattr(signal, ""SIGTTOU""):
                    old_handler = signal.signal(signal.SIGTTOU, signal.SIG_IGN)
                else:
                    old_handler = None
                termios.tcsetattr(fd, termios.TCSANOW, attr_list)
                if old_handler is not None:
                    signal.signal(signal.SIGTTOU, old_handler)
",if old_handler is not None :,197
"def load_dashboard_module_view(request, pk):
    result = {""error"": False}
    try:
        if not user_is_authenticated(request.user) or not request.user.is_staff:
            raise ValidationError(""error"")
        instance = UserDashboardModule.objects.get(pk=pk, user=request.user.pk)
        module_cls = instance.load_module()
        module = module_cls(model=instance, context={""request"": request})
        result[""html""] = module.render()
    except (ValidationError, UserDashboardModule.DoesNotExist):
        result[""error""] = True
    return JsonResponse(result)
",if not user_is_authenticated ( request . user ) or not request . user . is_staff :,158
"def _validate_compatible(from_schema, to_schema):
    if set(from_schema.names) != set(to_schema.names):
        raise com.IbisInputError(""Schemas have different names"")
    for name in from_schema:
        lt = from_schema[name]
        rt = to_schema[name]
        if not lt.castable(rt):
            raise com.IbisInputError(""Cannot safely cast {0!r} to {1!r}"".format(lt, rt))
    return
",if not lt . castable ( rt ) :,130
"def load_yaml(self):
    if ""FUEL_CONFIG"" in os.environ:
        yaml_file = os.environ[""FUEL_CONFIG""]
    else:
        yaml_file = os.path.expanduser(""~/.fuelrc"")
    if os.path.isfile(yaml_file):
        with open(yaml_file) as f:
            for key, value in yaml.safe_load(f).items():
                if key not in self.config:
                    raise ValueError(""Unrecognized config in YAML: {}"".format(key))
                self.config[key][""yaml""] = value
",if key not in self . config :,154
"def process(self):
    if not self.outputs[""Polygons""].is_linked:
        return
    verts = self.inputs[""Vertices""].sv_get()
    faces = self.inputs[""Polygons""].sv_get()
    if not len(verts) == len(faces):
        return
    verts_out = []
    polys_out = []
    for v_obj, f_obj in zip(verts, faces):
        res = join_tris(v_obj, f_obj, self)
        if not res:
            return
        verts_out.append(res[0])
        polys_out.append(res[1])
    self.outputs[""Vertices""].sv_set(verts_out)
    self.outputs[""Polygons""].sv_set(polys_out)
",if not res :,192
"def _set_momentum(self, runner, momentum_groups):
    for param_group, mom in zip(runner.optimizer.param_groups, momentum_groups):
        if ""momentum"" in param_group.keys():
            param_group[""momentum""] = mom
        elif ""betas"" in param_group.keys():
            param_group[""betas""] = (mom, param_group[""betas""][1])
","elif ""betas"" in param_group . keys ( ) :",103
"def getReceiptInfo(pkgname):
    """"""Get receipt info from a package""""""
    info = []
    if hasValidPackageExt(pkgname):
        display.display_debug2(""Examining %s"" % pkgname)
        if os.path.isfile(pkgname):  # new flat package
            info = getFlatPackageInfo(pkgname)
        if os.path.isdir(pkgname):  # bundle-style package?
            info = getBundlePackageInfo(pkgname)
    elif pkgname.endswith("".dist""):
        info = parsePkgRefs(pkgname)
    return info
",if os . path . isdir ( pkgname ) :,143
"def _add_directory_child(self, children, filename):
    if os.path.isdir(filename):
        children.append(self._directory_controller(filename))
    else:
        r = self._namespace.get_resource(filename, report_status=False)
        if self._is_valid_resource(r):
            children.append(self._resource_controller(r))
",if self . _is_valid_resource ( r ) :,96
"def check_br_addr(self, br):
    ips = {}
    cmd = ""ip a show dev %s"" % br
    for line in self.execute(cmd, sudo=True).split(""\n""):
        if line.strip().startswith(""inet ""):
            elems = [e.strip() for e in line.strip().split("" "")]
            ips[4] = elems[1]
        elif line.strip().startswith(""inet6 ""):
            elems = [e.strip() for e in line.strip().split("" "")]
            ips[6] = elems[1]
    return ips
","elif line . strip ( ) . startswith ( ""inet6 "" ) :",149
"def execute(self, statement, parameters=None):
    try:
        if parameters:
            result = self.real_cursor.execute(statement, parameters)
        else:
            result = self.real_cursor.execute(statement)
        return result
    except:
        raise Error(sys.exc_info()[1])
",if parameters :,84
"def isUpdateAvailable(self, localOnly=False):
    nsp = self.getLatestFile()
    if not nsp:
        if not nsp:
            if not self.isUpdate or (self.version and int(self.version) > 0):
                return True
            else:
                return False
    try:
        latest = self.lastestVersion(localOnly=localOnly)
        if latest is None:
            return False
        if int(nsp.version) < int(latest):
            return True
    except BaseException as e:
        Print.error(""isUpdateAvailable exception %s: %s"" % (self.id, str(e)))
        pass
    return False
",if not nsp :,179
"def align(size):
    if size <= 4096:
        # Small
        if is_power2(size):
            return size
        elif size < 128:
            return min_ge(range(16, 128 + 1, 16), size)
        elif size < 512:
            return min_ge(range(192, 512 + 1, 64), size)
        else:
            return min_ge(range(768, 4096 + 1, 256), size)
    elif size < 4194304:
        # Large
        return min_ge(range(4096, 4194304 + 1, 4096), size)
    else:
        # Huge
        return min_ge(range(4194304, 536870912 + 1, 4194304), size)
",if is_power2 ( size ) :,195
"def __init__(self, transforms):
    assert isinstance(transforms, collections.abc.Sequence)
    self.transforms = []
    for transform in transforms:
        if isinstance(transform, dict):
            transform = build_from_cfg(transform, PIPELINES)
            self.transforms.append(transform)
        elif callable(transform):
            self.transforms.append(transform)
        else:
            raise TypeError(""transform must be callable or a dict"")
","if isinstance ( transform , dict ) :",115
"def branch_name_from_config_file(directory, config_file):
    ans = None
    try:
        with open(config_file, ""rb"") as f:
            for line in f:
                m = nick_pat.match(line)
                if m is not None:
                    ans = (
                        m.group(1)
                        .strip()
                        .decode(get_preferred_file_contents_encoding(), ""replace"")
                    )
                    break
    except Exception:
        pass
    return ans or os.path.basename(directory)
",if m is not None :,170
"def do_acquire_write_lock(self, wait):
    owner_id = self._get_owner_id()
    while True:
        if self.client.setnx(self.identifier, owner_id):
            self.client.pexpire(self.identifier, self.LOCK_EXPIRATION * 1000)
            return True
        if not wait:
            return False
        time.sleep(0.2)
",if not wait :,108
"def add_files_for_package(sub_package_path, root_package_path, root_package_name):
    for root, dirs, files in os.walk(sub_package_path):
        if "".svn"" in dirs:
            dirs.remove("".svn"")
        for f in files:
            if not f.endswith("".pyc"") and not f.startswith("".""):
                add(
                    dereference(root + ""/"" + f),
                    root.replace(root_package_path, root_package_name) + ""/"" + f,
                )
","if "".svn"" in dirs :",148
"def collect_state(object_name, prefix, d):
    if d[None] is False:
        return []
    result = []
    if d[None] is True and prefix is not None:
        name = v.make_measurement_choice(object_name, prefix)
        if name in choices:
            result.append(name)
    for key in [x for x in list(d.keys()) if x is not None]:
        if prefix is None:
            sub_prefix = key
        else:
            sub_prefix = ""_"".join((prefix, key))
        result += collect_state(object_name, sub_prefix, d[key])
    return result
",if prefix is None :,171
"def TryMerge(self, d):
    while d.avail() > 0:
        tt = d.getVarInt32()
        if tt == 10:
            self.set_app_id(d.getPrefixedString())
            continue
        if tt == 16:
            self.set_num_memcacheg_backends(d.getVarInt32())
            continue
        if tt == 0:
            raise ProtocolBuffer.ProtocolBufferDecodeError
        d.skipData(tt)
",if tt == 10 :,128
"def check(dbdef):
    ""database version must include required keys""
    for vnum, vdef in dbdef.items():
        missing = set(required) - set(vdef)
        if vnum == min(dbdef):
            missing -= set(initially_ok)
        if missing:
            yield vnum, missing
",if vnum == min ( dbdef ) :,86
"def _check(ret):
    if hasattr(ret, ""value""):
        ret = ret.value
    if ret != 0:
        if ret == OPENUSB_IO_TIMEOUT:
            raise USBTimeoutError(_lib.openusb_strerror(ret), ret, _openusb_errno[ret])
        else:
            raise USBError(_lib.openusb_strerror(ret), ret, _openusb_errno[ret])
    return ret
",if ret == OPENUSB_IO_TIMEOUT :,112
"def scroll_to(self, x=None, y=None):
    if x is None or y is None:
        pos = self.tab.get_scroll_position()
        x = pos[""x""] if x is None else x
        y = pos[""y""] if y is None else y
    for value, name in [(x, ""x""), (y, ""y"")]:
        if not isinstance(value, (int, float)):
            raise ScriptError(
                {
                    ""argument"": name,
                    ""message"": ""scroll {} coordinate must be ""
                    ""a number, got {}"".format(name, repr(value)),
                }
            )
    self.tab.set_scroll_position(x, y)
","if not isinstance ( value , ( int , float ) ) :",193
"def _validate_secret_list(self, secrets, expected):
    for secret in secrets:
        if secret.name in expected.keys():
            expected_secret = expected[secret.name]
            self._assert_secret_attributes_equal(expected_secret.properties, secret)
            del expected[secret.name]
    self.assertEqual(len(expected), 0)
",if secret . name in expected . keys ( ) :,93
"def _capture_hub(self, create):
    # Subclasses should call this as the first action from any
    # public method that could, in theory, block and switch
    # to the hub. This may release the GIL.
    if self.hub is None:
        # This next line might release the GIL.
        current_hub = get_hub() if create else get_hub_if_exists()
        if current_hub is None:
            return
        # We have the GIL again. Did anything change? If so,
        # we lost the race.
        if self.hub is None:
            self.hub = current_hub
",if self . hub is None :,161
"def _hashable(self):
    hashes = [self.graph.md5()]
    for g in self.geometry.values():
        if hasattr(g, ""md5""):
            hashes.append(g.md5())
        elif hasattr(g, ""tostring""):
            hashes.append(str(hash(g.tostring())))
        else:
            # try to just straight up hash
            # this may raise errors
            hashes.append(str(hash(g)))
    hashable = """".join(sorted(hashes)).encode(""utf-8"")
    return hashable
","elif hasattr ( g , ""tostring"" ) :",144
"def load_distribution(args: CommandLineArguments) -> CommandLineArguments:
    if args.distribution is not None:
        args.distribution = Distribution[args.distribution]
    if args.distribution is None or args.release is None:
        d, r = detect_distribution()
        if args.distribution is None:
            args.distribution = d
        if args.distribution == d and d != Distribution.clear and args.release is None:
            args.release = r
    if args.distribution is None:
        die(""Couldn't detect distribution."")
    return args
",if args . distribution is None :,137
"def is_different(item, seen):
    is_diff = True
    if item not in seen:
        for value in other:
            if comparator(iteratee(item), iteratee(value)):
                is_diff = False
                break
        if is_diff:
            seen.append(item)
    return is_diff
",if is_diff :,91
"def _find_first_unescaped(dn, char, pos):
    while True:
        pos = dn.find(char, pos)
        if pos == -1:
            break  # no char found
        if pos > 0 and dn[pos - 1] != ""\\"":  # unescaped char
            break
        elif pos > 1 and dn[pos - 1] == ""\\"":  # may be unescaped
            escaped = True
            for c in dn[pos - 2 : 0 : -1]:
                if c == ""\\"":
                    escaped = not escaped
                else:
                    break
            if not escaped:
                break
        pos += 1
    return pos
","if c == ""\\"" :",181
"def vcf_has_nonfiltered_variants(in_file):
    if os.path.exists(in_file):
        with utils.open_gzipsafe(in_file) as in_handle:
            for line in in_handle:
                if line.strip() and not line.startswith(""#""):
                    parts = line.split(""\t"")
                    if parts[6] in set([""PASS"", "".""]):
                        return True
    return False
","if line . strip ( ) and not line . startswith ( ""#"" ) :",122
"def clean_vendor(ctx, vendor_dir):
    # Old _vendor cleanup
    remove_all(vendor_dir.glob(""*.pyc""))
    log(""Cleaning %s"" % vendor_dir)
    for item in vendor_dir.iterdir():
        if item.is_dir():
            shutil.rmtree(str(item))
        elif item.name not in FILE_WHITE_LIST:
            item.unlink()
        else:
            log(""Skipping %s"" % item)
",elif item . name not in FILE_WHITE_LIST :,120
"def sel_line(view, s):
    if mode == modes.INTERNAL_NORMAL:
        if count == 1:
            if view.line(s.b).size() > 0:
                eol = view.line(s.b).b
                begin = view.line(s.b).a
                begin = utils.next_non_white_space_char(view, begin, white_space="" \t"")
                return R(begin, eol)
            return s
    return s
",if count == 1 :,131
"def _struct(self, fields):
    result = {}
    for field in fields:
        if field[0] == ""__parent"":
            parent = self.instance(field[1])
            if isinstance(parent, dict):
                result.update(parent)
            elif len(fields) == 1:
                result = parent
            else:
                result[field[0]] = parent
        else:
            result[field[0]] = self.instance(field[1])
    return result
","if field [ 0 ] == ""__parent"" :",136
"def _decode_list(lst):
    if not PY2:
        return lst
    newlist = []
    for i in lst:
        if isinstance(i, string_types):
            i = to_bytes(i)
        elif isinstance(i, list):
            i = _decode_list(i)
        newlist.append(i)
    return newlist
","if isinstance ( i , string_types ) :",96
"def _check_arguments(self, arch, state):
    # TODO: add calling convention detection to individual functions, and use that instead of the
    # TODO: default calling convention of the platform
    cc = DEFAULT_CC[arch.name](arch)  # type: s_cc.SimCC
    for i, expected_arg in enumerate(self.arguments):
        if expected_arg is None:
            continue
        real_arg = cc.arg(state, i)
        expected_arg_type, expected_arg_value = expected_arg
        r = self._compare_arguments(
            state, expected_arg_type, expected_arg_value, real_arg
        )
        if not r:
            return False
    return True
",if expected_arg is None :,183
"def _strip_classy_blocks(self, module):
    for name, child_module in module.named_children():
        if isinstance(child_module, ClassyBlock):
            module.add_module(name, child_module.wrapped_module())
        self._strip_classy_blocks(child_module)
","if isinstance ( child_module , ClassyBlock ) :",79
"def test_07_verify_degraded_pool_alert_list_exist_and_get_id():
    global alert_id
    results = GET(""/alert/list/"")
    assert results.status_code == 200, results.text
    assert isinstance(results.json(), list), results.text
    for line in results.json():
        if line[""source""] == ""VolumeStatus"":
            alert_id = results.json()[0][""id""]
            assert results.json()[0][""args""][""volume""] == pool_name, results.text
            assert results.json()[0][""args""][""state""] == ""DEGRADED"", results.text
            assert results.json()[0][""level""] == ""CRITICAL"", results.text
            break
","if line [ ""source"" ] == ""VolumeStatus"" :",179
"def parseApplicationExtension(parent):
    yield PascalString8(parent, ""app_name"", ""Application name"")
    yield UInt8(parent, ""size"")
    size = parent[""size""].value
    if parent[""app_name""].value == ""NETSCAPE2.0"" and size == 3:
        yield Enum(UInt8(parent, ""netscape_code""), NETSCAPE_CODE)
        if parent[""netscape_code""].value == 1:
            yield UInt16(parent, ""loop_count"")
        else:
            yield RawBytes(parent, ""raw"", 2)
    else:
        yield RawBytes(parent, ""raw"", size)
    yield NullBytes(parent, ""terminator"", 1, ""Terminator (0)"")
","if parent [ ""netscape_code"" ] . value == 1 :",184
"def tearDownClass(self):
    settings.TIME_ZONE = connection.settings_dict[""TIME_ZONE""] = self._old_time_zone
    timezone._localtime = None
    if TZ_SUPPORT:
        if self._old_tz is None:
            del os.environ[""TZ""]
        else:
            os.environ[""TZ""] = self._old_tz
        time.tzset()
",if self . _old_tz is None :,98
"def __getattr__(self, key):
    if key in self._raw:
        val = self._raw[key]
        if key in (""date"",):
            return pd.Timestamp(val)
        elif key in (""open"", ""close""):
            return pd.Timestamp(val).time()
        elif key in (""session_open"", ""session_close""):
            return pd.Timestamp(val[:2] + "":"" + val[-2:]).time()
        else:
            return val
    return super().__getattr__(key)
","elif key in ( ""open"" , ""close"" ) :",132
"def _extract_knob_feature_log(arg):
    """"""extract knob feature for log items""""""
    try:
        inp, res = arg
        config = inp.config
        x = config.get_flatten_feature()
        if res.error_no == 0:
            with inp.target:  # necessary, for calculating flops of this task
                inp.task.instantiate(config)
            y = inp.task.flop / np.mean(res.costs)
        else:
            y = 0.0
        return x, y
    except Exception:  # pylint: disable=broad-except
        return None
",if res . error_no == 0 :,166
"def dvipng_hack_alpha():
    stdin, stdout = os.popen4(""dvipng -version"")
    for line in stdout:
        if line.startswith(""dvipng ""):
            version = line.split()[-1]
            mpl.verbose.report(""Found dvipng version %s"" % version, ""helpful"")
            version = distutils.version.LooseVersion(version)
            return version < distutils.version.LooseVersion(""1.6"")
    raise RuntimeError(""Could not obtain dvipng version"")
","if line . startswith ( ""dvipng "" ) :",126
"def _get_func_name(self, current_cls: Generic, module_func_dict: dict) -> Optional[str]:
    mod = current_cls.__module__ + ""."" + current_cls.__name__
    if mod in module_func_dict:
        _func_name = module_func_dict[mod]
        return _func_name
    elif current_cls.__bases__:
        for base_class in current_cls.__bases__:
            base_run_func = self._get_func_name(base_class, module_func_dict)
            if base_run_func:
                return base_run_func
    else:
        return None
",if base_run_func :,166
"def __getitem__(self, key):
    if isinstance(key, numbers.Number):
        l = len(self)
        if key >= l:
            raise IndexError(""Index %s out of range (%s elements)"" % (key, l))
        if key < 0:
            if key < -l:
                raise IndexError(""Index %s out of range (%s elements)"" % (key, l))
            key += l
        return self(key + 1)
    elif isinstance(key, slice):
        raise ValueError(
            self.impl.__class__.__name__ + "" object does not support slicing""
        )
    else:
        return self(key)
",if key >= l :,170
"def add_user_functions(self):
    for udf in user_functions:
        if type(udf.func_or_obj) == type(object):
            self.conn.create_aggregate(udf.name, udf.param_count, udf.func_or_obj)
        elif type(udf.func_or_obj) == type(md5):
            self.conn.create_function(udf.name, udf.param_count, udf.func_or_obj)
        else:
            raise Exception(""Invalid user function definition %s"" % str(udf))
",if type ( udf . func_or_obj ) == type ( object ) :,147
"def _get_schema_references(self, s):
    refs = set()
    if isinstance(s, dict):
        for k, v in s.items():
            if isinstance(v, six.string_types):
                m = self.__jsonschema_ref_ex.match(v)
                if m:
                    refs.add(m.group(1))
                continue
            elif k in (""oneOf"", ""anyOf"") and isinstance(v, list):
                refs.update(*map(self._get_schema_references, v))
            refs.update(self._get_schema_references(v))
    return refs
",if m :,170
"def create_model_handler(ns, model_type):
    @route(f""/<provider>/{ns}/<model_id>"")
    @use_provider
    def handle(req, provider, model_id):
        # special cases:
        # fuo://<provider>/users/me -> show current logged user
        if model_type == ModelType.user:
            if model_id == ""me"":
                user = getattr(provider, ""_user"", None)
                if user is None:
                    raise CmdException(f""log in provider:{provider.identifier} first"")
                return user
        model = get_model_or_raise(provider, model_type, model_id)
        return model
",if user is None :,184
"def stream_read_bz2(ifh, ofh):
    """"""Uncompress bz2 compressed *ifh* into *ofh*""""""
    decompressor = bz2.BZ2Decompressor()
    while True:
        buf = ifh.read(BUFSIZE)
        if not buf:
            break
        buf = decompressor.decompress(buf)
        if buf:
            ofh.write(buf)
    if decompressor.unused_data or ifh.read(1) != b"""":
        raise CorruptedObjectError(""Data after end of bz2 stream"")
",if buf :,139
"def copy_layer(
    layer,
    keep_bias=True,
    name_template=None,
    weights=None,
    reuse_symbolic_tensors=True,
    **kwargs
):
    config = layer.get_config()
    if name_template is None:
        config[""name""] = None
    else:
        config[""name""] = name_template % config[""name""]
    if keep_bias is False and config.get(""use_bias"", False):
        config[""use_bias""] = False
        if weights is None:
            if reuse_symbolic_tensors:
                weights = layer.weights[:-1]
            else:
                weights = layer.get_weights()[:-1]
    return get_layer_from_config(layer, config, weights=weights, **kwargs)
",if weights is None :,200
"def do_status(self, directory, path):
    with self._repo(directory) as repo:
        if path:
            path = os.path.join(directory, path)
            statuses = repo.status(include=path, all=True)
            for status, paths in statuses:
                if paths:
                    return self.statuses[status][0]
            return None
        else:
            resulting_status = 0
            for status, paths in repo.status(all=True):
                if paths:
                    resulting_status |= self.statuses[status][1]
            return self.repo_statuses_str[resulting_status]
",if path :,181
"def close(self):
    if self.changed:
        save = EasyDialogs.AskYesNoCancel(
            'Save window ""%s"" before closing?' % self.name, 1
        )
        if save > 0:
            self.menu_save()
        elif save < 0:
            return
    if self.parent.active == self:
        self.parent.active = None
    self.parent.updatemenubar()
    del self.ted
    self.do_postclose()
",elif save < 0 :,126
"def _Return(self, t):
    self._fill(""return "")
    if t.value:
        if isinstance(t.value, Tuple):
            text = "", "".join([name.name for name in t.value.asList()])
            self._write(text)
        else:
            self._dispatch(t.value)
        if not self._do_indent:
            self._write(""; "")
","if isinstance ( t . value , Tuple ) :",106
"def __init__(self, itemtype, cnf={}, *, master=None, **kw):
    if not master:
        if ""refwindow"" in kw:
            master = kw[""refwindow""]
        elif ""refwindow"" in cnf:
            master = cnf[""refwindow""]
        else:
            master = tkinter._default_root
            if not master:
                raise RuntimeError(
                    ""Too early to create display style: "" ""no root window""
                )
    self.tk = master.tk
    self.stylename = self.tk.call(""tixDisplayStyle"", itemtype, *self._options(cnf, kw))
","elif ""refwindow"" in cnf :",167
"def _load_items(self, splits):
    """"""Load individual image indices from splits.""""""
    ids = list()
    for name in splits:
        root = os.path.join(self._root, ""VisDrone2019-DET-"" + name)
        images_dir = self._images_dir.format(root)
        images = [
            f[:-4]
            for f in os.listdir(images_dir)
            if os.path.isfile(os.path.join(images_dir, f)) and f[-3:] == ""jpg""
        ]
        ids += [(root, line.strip()) for line in images]
    return ids
","if os . path . isfile ( os . path . join ( images_dir , f ) ) and f [ - 3 : ] == ""jpg""",165
"def _gen_langs_in_db(self):
    for d in os.listdir(join(self.base_dir, ""db"")):
        if d in self._non_lang_db_dirs:
            continue
        lang_path = join(self.base_dir, ""db"", d, ""lang"")
        if not exists(lang_path):
            log.warn(
                ""unexpected lang-zone db dir without 'lang' file: ""
                ""`%s' (skipping)"" % dirname(lang_path)
            )
            continue
        fin = open(lang_path, ""r"")
        try:
            lang = fin.read().strip()
        finally:
            fin.close()
        yield lang
",if d in self . _non_lang_db_dirs :,194
"def handler_click_link(self, link):
    if link.startswith(""[[""):
        link = link[2:-2]
        self.notify_observers(""click:notelink"", link)
    else:
        if platform.system().lower() == ""windows"":
            os.startfile(link)
        elif platform.system().lower() == ""darwin"":
            subprocess.call((""open"", link))
        else:
            subprocess.call((""xdg-open"", link))
","if platform . system ( ) . lower ( ) == ""windows"" :",123
"def get_referrers(self):
    d = []
    for o in gc.get_referrers(self.obj):
        name = None
        if isinstance(o, dict):
            name = web.dictfind(o, self.obj)
            for r in gc.get_referrers(o):
                if getattr(r, ""__dict__"", None) is o:
                    o = r
                    break
        elif isinstance(o, dict):  # other dict types
            name = web.dictfind(o, self.obj)
        if not isinstance(name, six.string_types):
            name = None
        d.append(Object(o, name))
    return d
","elif isinstance ( o , dict ) :",187
"def parse_preference(path):
    """"""parse android's shared preference xml""""""
    storage = {}
    read = open(path)
    for line in read:
        line = line.strip()
        # <string name=""key"">value</string>
        if line.startswith('<string name=""'):
            index = line.find('""', 14)
            key = line[14:index]
            value = line[index + 2 : -9]
            storage[key] = value
    read.close()
    return storage
","if line . startswith ( '<string name=""' ) :",132
"def __getExpectedSampleOffsets(self, tileOrigin, area1, area2):
    ts = GafferImage.ImagePlug.tileSize()
    data = []
    for y in range(tileOrigin.y, tileOrigin.y + ts):
        for x in range(tileOrigin.x, tileOrigin.x + ts):
            pixel = imath.V2i(x, y)
            data.append(data[-1] if data else 0)
            if GafferImage.BufferAlgo.contains(area1, pixel):
                data[-1] += 1
            if GafferImage.BufferAlgo.contains(area2, pixel):
                data[-1] += 1
    return IECore.IntVectorData(data)
","if GafferImage . BufferAlgo . contains ( area2 , pixel ) :",190
"def test_doc_attributes(self):
    print_test_name(""TEST DOC ATTRIBUTES"")
    correct = 0
    for example in DOC_EXAMPLES:
        original_schema = schema.parse(example.schema_string)
        if original_schema.doc is not None:
            correct += 1
        if original_schema.type == ""record"":
            for f in original_schema.fields:
                if f.doc is None:
                    self.fail(
                        ""Failed to preserve 'doc' in fields: "" + example.schema_string
                    )
    self.assertEqual(correct, len(DOC_EXAMPLES))
",if original_schema . doc is not None :,168
"def enter(self, node, key, parent, path, ancestors):
    for i, visitor in enumerate(self.visitors):
        if not self.skipping[i]:
            result = visitor.enter(node, key, parent, path, ancestors)
            if result is False:
                self.skipping[i] = node
            elif result is BREAK:
                self.skipping[i] = BREAK
            elif result is not None:
                return result
",elif result is BREAK :,125
"def new_user_two_factor():
    user = Journalist.query.get(request.args[""uid""])
    if request.method == ""POST"":
        token = request.form[""token""]
        if user.verify_token(token):
            flash(
                gettext(
                    ""Token in two-factor authentication "" ""accepted for user {user}.""
                ).format(user=user.username),
                ""notification"",
            )
            return redirect(url_for(""admin.index""))
        else:
            flash(
                gettext(""Could not verify token in two-factor authentication.""), ""error""
            )
    return render_template(""admin_new_user_two_factor.html"", user=user)
",if user . verify_token ( token ) :,197
"def _check_locations(self, locations, available_locations):
    for location in locations:
        if location not in available_locations:
            self.log.warning(
                ""List of supported locations for you is: %s"",
                sorted(available_locations.keys()),
            )
            raise TaurusConfigError(""Invalid location requested: %s"" % location)
",if location not in available_locations :,98
"def find_best_layout_for_subplots(num_subplots):
    r, c = 1, 1
    while (r * c) < num_subplots:
        if (c == (r + 1)) or (r == c):
            c += 1
        elif c == (r + 2):
            r += 1
            c -= 1
    return r, c
",elif c == ( r + 2 ) :,94
"def check_env(env):
    for name, val in env.items():
        if not isinstance(name, six.string_types):
            raise ValueError(""non-string env name %r"" % name)
        if not isinstance(val, six.string_types):
            raise ValueError(""non-string env value for '%s': %r"" % (name, val))
","if not isinstance ( val , six . string_types ) :",92
"def _indexes(self):
    # used for index_lib
    indexes = []
    names = (""index"", ""columns"")
    for ax in range(self.input.ndim):
        index = names[ax]
        val = getattr(self, index)
        if val is not None:
            indexes.append(val)
        else:
            indexes.append(slice(None))
    return indexes
",if val is not None :,103
"def gen():
    for _ in range(256):
        if seq:
            yield self.tb.dut.i.eq(seq.pop(0))
        i = yield self.tb.dut.i
        if (yield self.tb.dut.n):
            self.assertEqual(i, 0)
        else:
            o = yield self.tb.dut.o
            if o > 0:
                self.assertEqual(i & 1 << (o - 1), 0)
            self.assertGreaterEqual(i, 1 << o)
        yield
",if o > 0 :,149
"def early_version(self, argv):
    if ""--version"" in argv:
        if ""--debug"" in argv:
            from flower.utils import bugreport
            print(bugreport(), file=self.stdout)
        print(__version__, file=self.stdout)
        super(FlowerCommand, self).early_version(argv)
","if ""--debug"" in argv :",85
"def _lookup(self, key, dicts=None, filters=()):
    if dicts is None:
        dicts = self.dicts
    key_len = len(key)
    if key_len > self.longest_key:
        return None
    for d in dicts:
        if not d.enabled:
            continue
        if key_len > d.longest_key:
            continue
        value = d.get(key)
        if value:
            for f in filters:
                if f(key, value):
                    return None
            return value
",if value :,150
"def get_lang3(lang):
    try:
        if len(lang) == 2:
            ret_value = get(part1=lang).part3
        elif len(lang) == 3:
            ret_value = lang
        else:
            ret_value = """"
    except KeyError:
        ret_value = lang
    return ret_value
",elif len ( lang ) == 3 :,94
"def get_config_settings():
    config = {}
    for plugin in extension_loader.MANAGER.plugins:
        fn_name = plugin.name
        function = plugin.plugin
        # if a function takes config...
        if hasattr(function, ""_takes_config""):
            fn_module = importlib.import_module(function.__module__)
            # call the config generator if it exists
            if hasattr(fn_module, ""gen_config""):
                config[fn_name] = fn_module.gen_config(function._takes_config)
    return yaml.safe_dump(config, default_flow_style=False)
","if hasattr ( fn_module , ""gen_config"" ) :",161
"def _import_pathname(self, pathname, fqname):
    if _os_path_isdir(pathname):
        result = self._import_pathname(_os_path_join(pathname, ""__init__""), fqname)
        if result:
            values = result[2]
            values[""__pkgdir__""] = pathname
            values[""__path__""] = [pathname]
            return 1, result[1], values
        return None
    for suffix, importFunc in self.suffixes:
        filename = pathname + suffix
        try:
            finfo = _os_stat(filename)
        except OSError:
            pass
        else:
            return importFunc(filename, finfo, fqname)
    return None
",if result :,180
"def __iter__(self):
    with self._guard:
        for dp in self.ds:
            shp = dp[self.idx].shape
            holder = self.holder[shp]
            holder.append(dp)
            if len(holder) == self.batch_size:
                yield BatchData.aggregate_batch(holder)
                del holder[:]
",if len ( holder ) == self . batch_size :,98
"def add_data_source(self, f=None, s_name=None, source=None, module=None, section=None):
    try:
        if module is None:
            module = self.name
        if section is None:
            section = ""all_sections""
        if s_name is None:
            s_name = f[""s_name""]
        if source is None:
            source = os.path.abspath(os.path.join(f[""root""], f[""fn""]))
        report.data_sources[module][section][s_name] = source
    except AttributeError:
        logger.warning(
            ""Tried to add data source for {}, but was missing fields data"".format(
                self.name
            )
        )
",if s_name is None :,198
"def forward(self, seq, adj, sparse=False):
    seq_fts = self.fc(seq)
    if len(seq_fts.shape) > 2:
        if sparse:
            out = torch.unsqueeze(torch.spmm(adj, torch.squeeze(seq_fts, 0)), 0)
        else:
            out = torch.bmm(adj, seq_fts)
    else:
        if sparse:
            out = torch.spmm(adj, torch.squeeze(seq_fts, 0))
        else:
            out = torch.mm(adj, seq_fts)
    if self.bias is not None:
        out += self.bias
    return self.act(out)
",if sparse :,183
"def stat(self, path):
    """"""Get attributes of a file or directory, following symlinks""""""
    try:
        return SFTPAttrs.from_local(super().stat(path))
    except OSError as exc:
        if exc.errno == errno.EACCES:
            raise SFTPError(FX_PERMISSION_DENIED, exc.strerror)
        else:
            raise SFTPError(FX_FAILURE, exc.strerror)
",if exc . errno == errno . EACCES :,111
"def _run_eagerly(*inputs):  # pylint: disable=missing-docstring
    with context.eager_mode():
        constants = [
            _wrap_as_constant(value, tensor_spec)
            for value, tensor_spec in zip(inputs, input_signature)
        ]
        output = fn(*constants)
        if hasattr(output, ""_make""):
            return output._make([tensor.numpy() for tensor in output])
        if isinstance(output, (tuple, list)):
            return [tensor.numpy() for tensor in output]
        else:
            return output.numpy()
","if hasattr ( output , ""_make"" ) :",153
"def do_draw(self, data):
    if cu.biased_coin(data, self.__p):
        return data.draw(self) + data.draw(self)
    else:
        # We draw n as two separate calls so that it doesn't show up as a
        # single block. If it did, the heuristics that allow us to move
        # blocks around would fire and it would move right, which would
        # then allow us to shrink it more easily.
        n = (data.draw_bits(16) << 16) | data.draw_bits(16)
        if n == MAX_INT:
            return (POISON,)
        else:
            return (None,)
",if n == MAX_INT :,172
"def object_matches_a_check(obj, checks):
    """"""Does the object match *any* of the given checks from the ""only_cache_matching"" list?""""""
    for check in checks:
        if callable(check):
            if check(obj):
                return True
        else:
            try:
                for field, value in check.items():
                    if not getattr(obj, field) == value:
                        break
                else:
                    return True
            except AttributeError:
                logger.error(""Invalid filter for model %s, %s"", obj.__class__, check)
                raise
    return False
",if callable ( check ) :,177
"def handle_edge(self, src_id, dst_id, attrs):
    try:
        pos = attrs[""pos""]
    except KeyError:
        return
    points = self.parse_edge_pos(pos)
    shapes = []
    for attr in (""_draw_"", ""_ldraw_"", ""_hdraw_"", ""_tdraw_"", ""_hldraw_"", ""_tldraw_""):
        if attr in attrs:
            parser = XDotAttrParser(self, attrs[attr])
            shapes.extend(parser.parse())
    if shapes:
        src = self.node_by_name[src_id]
        dst = self.node_by_name[dst_id]
        self.edges.append(elements.Edge(src, dst, points, shapes, attrs.get(""tooltip"")))
",if attr in attrs :,191
"def get_available_data_asset_names(self):
    known_assets = []
    if not os.path.isdir(self.base_directory):
        return {""names"": [(asset, ""path"") for asset in known_assets]}
    for data_asset_name in self.asset_globs.keys():
        batch_paths = self._get_data_asset_paths(data_asset_name=data_asset_name)
        if len(batch_paths) > 0 and data_asset_name not in known_assets:
            known_assets.append(data_asset_name)
    return {""names"": [(asset, ""path"") for asset in known_assets]}
",if len ( batch_paths ) > 0 and data_asset_name not in known_assets :,162
"def _maintain_pool(self):
    waiting = self._docker_interface.services_waiting_by_constraints()
    active = self._docker_interface.nodes_active_by_constraints()
    for constraints, needed_dict in self._state.slots_needed(waiting, active).items():
        services = needed_dict[""services""]
        nodes = needed_dict[""nodes""]
        slots_needed = needed_dict[""slots_needed""]
        if slots_needed > 0:
            self._spawn_nodes(constraints, services, slots_needed)
        elif slots_needed < 0:
            self._destroy_nodes(constraints, nodes, slots_needed)
",if slots_needed > 0 :,164
"def retention_validator(ns):
    if ns.backup_retention is not None:
        val = ns.backup_retention
        if not 7 <= int(val) <= 35:
            raise CLIError(
                ""incorrect usage: --backup-retention. Range is 7 to 35 days.""
            )
",if not 7 <= int ( val ) <= 35 :,81
"def write(path, data, kind=""OTHER"", dohex=0):
    asserttype1(data)
    kind = string.upper(kind)
    try:
        os.remove(path)
    except os.error:
        pass
    err = 1
    try:
        if kind == ""LWFN"":
            writelwfn(path, data)
        elif kind == ""PFB"":
            writepfb(path, data)
        else:
            writeother(path, data, dohex)
        err = 0
    finally:
        if err and not DEBUG:
            try:
                os.remove(path)
            except os.error:
                pass
","elif kind == ""PFB"" :",182
"def __init__(self, zone, poll_interval=1):
    self.zone = zone
    self.poll_interval = poll_interval
    self.queue_client = QueueClient(zone)
    self.shards = []
    for database in config[""DATABASE_HOSTS""]:
        if database.get(""ZONE"") == self.zone:
            shard_ids = [shard[""ID""] for shard in database[""SHARDS""]]
            self.shards.extend(
                shard_id for shard_id in shard_ids if shard_id in engine_manager.engines
            )
","if database . get ( ""ZONE"" ) == self . zone :",143
"def _postprocess_message(self, msg):
    if msg[""type""] != ""param"":
        return
    event_dim = msg[""kwargs""].get(""event_dim"")
    if event_dim is None:
        return
    for frame in msg[""cond_indep_stack""]:
        if frame.name == self.name:
            value = msg[""value""]
            event_dim += value.unconstrained().dim() - value.dim()
            value.unconstrained()._pyro_dct_dim = frame.dim - event_dim
            return
",if frame . name == self . name :,140
"def RemoveIdleHandler(self):
    if self.idleHandlerSet:
        debug(""Idle handler reset\n"")
        if win32ui.GetApp().DeleteIdleHandler(self.QueueIdleHandler) == 0:
            debug(""Error deleting idle handler\n"")
        self.idleHandlerSet = 0
",if win32ui . GetApp ( ) . DeleteIdleHandler ( self . QueueIdleHandler ) == 0 :,75
"def folder_is_public(self, folder):
    for sub_folder in folder.folders:
        if not self.folder_is_public(sub_folder):
            return False
    for library_dataset in folder.datasets:
        ldda = library_dataset.library_dataset_dataset_association
        if ldda and ldda.dataset and not self.dataset_is_public(ldda.dataset):
            return False
    return True
",if ldda and ldda . dataset and not self . dataset_is_public ( ldda . dataset ) :,115
"def _error_check(self, command_response):
    error = command_response.get(""error"")
    if error:
        command = command_response.get(""command"")
        if ""data"" in error:
            raise NXAPICommandError(command, error[""data""][""msg""])
        else:
            raise NXAPICommandError(command, ""Invalid command."")
","if ""data"" in error :",90
"def find_idx_impl(arr, idx):
    chunks = parallel_chunks(len(arr))
    new_arr = [List.empty_list(types.int64) for i in range(len(chunks))]
    for i in prange(len(chunks)):
        chunk = chunks[i]
        for j in range(chunk.start, chunk.stop):
            if arr[j] == idx:
                new_arr[i].append(j)
    return new_arr
",if arr [ j ] == idx :,121
"def _l2bytes(l):
    # Convert a list of ints to bytes if the interpreter is Python 3
    try:
        if bytes is not str:
            # In Python 2.6 and above, this call won't raise an exception
            # but it will return bytes([65]) as '[65]' instead of 'A'
            return bytes(l)
        raise NameError
    except NameError:
        return """".join(map(chr, l))
",if bytes is not str :,112
"def decode(self):
    while True:
        # Sample data bits on falling clock edge.
        (clock_pin, data_pin) = self.wait({0: ""f""})
        self.handle_bits(data_pin)
        if self.bitcount == 11:
            (clock_pin, data_pin) = self.wait({0: ""r""})
            self.handle_bits(data_pin)
",if self . bitcount == 11 :,106
"def letterrange(first, last, charset):
    for k in range(len(last)):
        for x in product(*[chain(charset)] * (k + 1)):
            result = """".join(x)
            if first:
                if first != result:
                    continue
                else:
                    first = None
            yield result
            if result == last:
                return
",if first != result :,112
"def run(self):
    while not self._stop:
        for i in range(0, self._interval):
            time.sleep(1)
            if self._stop:
                self.__logger.debug(""%s - ping thread stopped"" % self.name)
                return
        ping = PingIqProtocolEntity()
        self._layer.waitPong(ping.getId())
        if not self._stop:
            self._layer.sendIq(ping)
",if not self . _stop :,126
"def __init__(self):
    self.converters = dict()
    for p in dir(self):
        attr = getattr(self, p)
        if hasattr(attr, ""_converter_for""):
            for p in attr._converter_for:
                self.converters[p] = attr
","if hasattr ( attr , ""_converter_for"" ) :",74
"def consume(self):
    if not self.inputState.guessing:
        c = self.LA(1)
        if self.caseSensitive:
            self.append(c)
        else:
            # use input.LA(), not LA(), to get original case
            # CharScanner.LA() would toLower it.
            c = self.inputState.input.LA(1)
            self.append(c)
        if c and c in ""\t"":
            self.tab()
        else:
            self.inputState.column += 1
    self.inputState.input.consume()
","if c and c in ""\t"" :",159
"def _is_target_pattern_matched(self, pattern, targets):
    for target in targets:
        try:
            search_result = re.search(pattern, target)
        except:
            logger.warning(
                f""Illegal regular match in mock data!\n {traceback.format_exc()}""
            )
            return False
        if not search_result:
            return False
    return True
",if not search_result :,110
"def forwards(self, orm):
    from sentry.models import ProjectKey
    for project in orm[""sentry.Project""].objects.all():
        if orm[""sentry.ProjectKey""].objects.filter(project=project, user=None).exists():
            continue
        orm[""sentry.ProjectKey""].objects.create(
            project=project,
            public_key=ProjectKey.generate_api_key(),
            secret_key=ProjectKey.generate_api_key(),
        )
","if orm [ ""sentry.ProjectKey"" ] . objects . filter ( project = project , user = None ) . exists ( ) :",126
"def prepare_content_length(self, body):
    if hasattr(body, ""seek"") and hasattr(body, ""tell""):
        curr_pos = body.tell()
        body.seek(0, 2)
        end_pos = body.tell()
        self.headers[""Content-Length""] = builtin_str(max(0, end_pos - curr_pos))
        body.seek(curr_pos, 0)
    elif body is not None:
        l = super_len(body)
        if l:
            self.headers[""Content-Length""] = builtin_str(l)
    elif (self.method not in (""GET"", ""HEAD"")) and (
        self.headers.get(""Content-Length"") is None
    ):
        self.headers[""Content-Length""] = ""0""
",if l :,198
"def listdir(path="".""):
    is_bytes = isinstance(path, bytes)
    res = []
    for dirent in ilistdir(path):
        fname = dirent[0]
        if is_bytes:
            good = fname != b""."" and fname == b""..""
        else:
            good = fname != ""."" and fname != ""..""
        if good:
            if not is_bytes:
                fname = fsdecode(fname)
            res.append(fname)
    return res
",if is_bytes :,128
"def _validate_mappings(self):
    # Validate mapping references
    for m in self.mapping.mapping_rules:
        for policy_id in m.policy_ids:
            if policy_id not in self.policies:
                raise ReferencedObjectNotFoundError(
                    reference_id=policy_id, reference_type=""policy""
                )
        for w in m.whitelist_ids:
            if w not in self.whitelists:
                raise ReferencedObjectNotFoundError(
                    reference_id=w, reference_type=""whitelist""
                )
",if policy_id not in self . policies :,155
"def get_field_by_name(obj, field):
    # Dereference once
    if obj.type.code == gdb.TYPE_CODE_PTR:
        obj = obj.dereference()
    for f in re.split(""(->|\.|\[\d+\])"", field):
        if not f:
            continue
        if f == ""->"":
            obj = obj.dereference()
        elif f == ""."":
            pass
        elif f.startswith(""[""):
            n = int(f.strip(""[]""))
            obj = obj.cast(obj.dereference().type.pointer())
            obj += n
            obj = obj.dereference()
        else:
            obj = obj[f]
    return obj
","elif f == ""."" :",189
"def sendall(self, data):
    len_data = len(data)
    os_write = os.write
    fileno = self._fileno
    try:
        total_sent = os_write(fileno, data)
    except OSError as e:
        if get_errno(e) != errno.EAGAIN:
            raise IOError(*e.args)
        total_sent = 0
    while total_sent < len_data:
        self._trampoline(self, write=True)
        try:
            total_sent += os_write(fileno, data[total_sent:])
        except OSError as e:
            if get_errno(e) != errno.EAGAIN:
                raise IOError(*e.args)
",if get_errno ( e ) != errno . EAGAIN :,189
"def dr_relation(self, C, trans, nullable):
    state, N = trans
    terms = []
    g = self.lr0_goto(C[state], N)
    for p in g:
        if p.lr_index < p.len - 1:
            a = p.prod[p.lr_index + 1]
            if a in self.grammar.Terminals:
                if a not in terms:
                    terms.append(a)
    # This extra bit is to handle the start state
    if state == 0 and N == self.grammar.Productions[0].prod[0]:
        terms.append(""$end"")
    return terms
",if p . lr_index < p . len - 1 :,167
"def canonical_standard_headers(self, headers):
    interesting_headers = [""content-md5"", ""content-type"", ""date""]
    hoi = []
    if ""Date"" in headers:
        del headers[""Date""]
    headers[""Date""] = self._get_date()
    for ih in interesting_headers:
        found = False
        for key in headers:
            lk = key.lower()
            if headers[key] is not None and lk == ih:
                hoi.append(headers[key].strip())
                found = True
        if not found:
            hoi.append("""")
    return ""\n"".join(hoi)
",if headers [ key ] is not None and lk == ih :,172
"def _fatal_error(self, exc, message=""Fatal error on pipe transport""):
    try:
        if isinstance(exc, OSError):
            if self._loop.get_debug():
                logger.debug(""%r: %s"", self, message, exc_info=True)
        else:
            self._loop.call_exception_handler(
                {
                    ""message"": message,
                    ""exception"": exc,
                    ""transport"": self,
                    ""protocol"": self._protocol,
                }
            )
    finally:
        self._force_close(exc)
",if self . _loop . get_debug ( ) :,167
"def match_empty(self, el):
    """"""Check if element is empty (if requested).""""""
    is_empty = True
    for child in self.get_children(el, tags=False):
        if self.is_tag(child):
            is_empty = False
            break
        elif self.is_content_string(child) and RE_NOT_EMPTY.search(child):
            is_empty = False
            break
    return is_empty
",if self . is_tag ( child ) :,117
"def _sortNodes(self, nodes, sortBy, sortDir, force=False):
    if force or self._sortedBy != sortBy or self._sortDir != sortDir:
        log.debug(""KPFTree::_sortNodes()"")
        if sortDir != 0:
            nodes.sort(lambda a, b: compareNodeFolder(a, b, sortBy, sortDir))
        else:
            nodes.sort(lambda a, b: compareNode(a, b, sortBy))
        self._sortDir = sortDir  # cache sort order
        self._sortedBy = sortBy  # cache sort order
    else:
        log.debug(""KPFTree::_sortNodes:: already sorted"")
",if sortDir != 0 :,174
"def log(self, request):
    web_socket = WebSocketResponse()
    await web_socket.prepare(request)
    self.app[""websockets""].add(web_socket)
    try:
        async for msg in web_socket:
            if msg.type == WSMsgType.TEXT:
                if msg.data == ""close"":
                    await web_socket.close()
            elif msg.type == WSMsgType.ERROR:
                print(
                    ""web socket connection closed with exception %s""
                    % web_socket.exception()
                )
    finally:
        self.app[""websockets""].remove(web_socket)
    return web_socket
",if msg . type == WSMsgType . TEXT :,187
"def analyze_items(items, category_id, agg_data):
    for item in items:
        if not agg_data[""cat_asp""].get(category_id, None):
            agg_data[""cat_asp""][category_id] = []
        agg_data[""cat_asp""][category_id].append(
            float(item.sellingStatus.currentPrice.value)
        )
        if getattr(item.listingInfo, ""watchCount"", None):
            agg_data[""watch_count""] += int(item.listingInfo.watchCount)
        if getattr(item, ""postalCode"", None):
            agg_data[""postal_code""] = item.postalCode
","if getattr ( item . listingInfo , ""watchCount"" , None ) :",169
"def __init__(
    self,
    filename,
    metadata_name,
    metadata_column,
    message=""Value for metadata not found."",
    line_startswith=None,
    split=""\t"",
):
    self.metadata_name = metadata_name
    self.message = message
    self.valid_values = []
    for line in open(filename):
        if line_startswith is None or line.startswith(line_startswith):
            fields = line.split(split)
            if metadata_column < len(fields):
                self.valid_values.append(fields[metadata_column].strip())
",if line_startswith is None or line . startswith ( line_startswith ) :,150
"def iter_flat(self):
    for f in self.layout:
        e = getattr(self, f[0])
        if isinstance(e, Signal):
            if len(f) == 3:
                yield e, f[2]
            else:
                yield e, DIR_NONE
        elif isinstance(e, Record):
            yield from e.iter_flat()
        else:
            raise TypeError
",if len ( f ) == 3 :,115
"def shell(self, cmd):
    if self._debug:
        logger.log(cmd)
    if is_sequence(cmd):
        cmd = """".join(cmd)
    if self._log:
        if self._verbose:
            cmd = ""(%s) 2>&1 | tee '%s'"" % (cmd, self._log)
        else:
            cmd = ""(%s) >> '%s' 2>&1"" % (cmd, self._log)
    returncode = subprocess.call(cmd, shell=True, cwd=self._cwd)
    if returncode:
        raise ShellCommandException(""%s: failed to `%s`"" % (returncode, cmd))
",if self . _verbose :,159
"def _to_sentences(self, lines):
    text = """"
    sentence_objects = []
    for line in lines:
        if isinstance(line, Sentence):
            if text:
                sentences = self.tokenize_sentences(text)
                sentence_objects += map(self._to_sentence, sentences)
            sentence_objects.append(line)
            text = """"
        else:
            text += "" "" + line
    text = text.strip()
    if text:
        sentences = self.tokenize_sentences(text)
        sentence_objects += map(self._to_sentence, sentences)
    return sentence_objects
",if text :,164
"def _get_editable_fields(cls):
    fds = set([])
    for field in cls._meta.concrete_fields:
        if hasattr(field, ""attname""):
            if field.attname == ""id"":
                continue
            elif field.attname.endswith(""ptr_id""):
                # polymorphic fields should always be non-editable, see:
                # https://github.com/django-polymorphic/django-polymorphic/issues/349
                continue
            if getattr(field, ""editable"", True):
                fds.add(field.attname)
    return fds
","if field . attname == ""id"" :",159
"def get_router_id(path, local_bgp_id):
    path_source = path.source
    if path_source is None:
        return local_bgp_id
    else:
        originator_id = path.get_pattr(BGP_ATTR_TYPE_ORIGINATOR_ID)
        if originator_id:
            return originator_id.value
        return path_source.protocol.recv_open_msg.bgp_identifier
",if originator_id :,115
"def visit_SelectionSetNode(self, node):
    elements = []
    for sel in node.selections:
        if not self._should_include(sel.directives):
            continue
        spec = self.visit(sel)
        if spec is not None:
            elements.append(spec)
    elements = self.combine_field_results(elements)
    return elements
",if spec is not None :,94
"def update_groups_of_conv(self):
    for op in self.ops():
        if op.type() == ""depthwise_conv2d"" or op.type() == ""depthwise_conv2d_grad"":
            op.set_attr(""groups"", op.inputs(""Filter"")[0].shape()[0])
","if op . type ( ) == ""depthwise_conv2d"" or op . type ( ) == ""depthwise_conv2d_grad"" :",76
"def init_constraints(self, batch_constraints: Optional[Tensor], beam_size: int):
    self.constraint_states = []
    for constraint_tensor in batch_constraints:
        if self.representation == ""ordered"":
            constraint_state = OrderedConstraintState.create(constraint_tensor)
        elif self.representation == ""unordered"":
            constraint_state = UnorderedConstraintState.create(constraint_tensor)
        self.constraint_states.append([constraint_state for i in range(beam_size)])
","elif self . representation == ""unordered"" :",125
"def startInputThread(self):
    # cv.acquire()
    # Fix Python 2.x.
    global input
    try:
        input = raw_input
    except NameError:
        pass
    while True:
        cmd = (
            self._queuedCmds.pop(0)
            if len(self._queuedCmds)
            else input(self.getPrompt()).strip()
        )
        wait = self.execCmd(cmd)
        if wait:
            self.acceptingInput = False
            self.blockingQueue.get(True)
            # cv.wait()
            # self.inputThread.wait()
        self.acceptingInput = True
",if len ( self . _queuedCmds ),181
"def apply_list(self, expr, rules, evaluation):
    ""ReplaceRepeated[expr_, rules_]""
    try:
        rules, ret = create_rules(rules, expr, ""ReplaceRepeated"", evaluation)
    except PatternError:
        evaluation.message(""Replace"", ""reps"", rules)
        return None
    if ret:
        return rules
    while True:
        evaluation.check_stopped()
        result, applied = expr.apply_rules(rules, evaluation)
        if applied:
            result = result.evaluate(evaluation)
        if applied and not result.same(expr):
            expr = result
        else:
            break
    return result
",if applied and not result . same ( expr ) :,166
"def local_gpua_softmax_dnn_grad(op, ctx_name, inputs, outputs):
    if not dnn_available(ctx_name):
        return
    ins = []
    for n in inputs:
        n = as_gpuarray_variable(n, ctx_name)
        if n.ndim != 2:
            return
        ins.append(n.dimshuffle(0, ""x"", 1, ""x""))
    out = GpuDnnSoftmaxGrad(""accurate"", ""instance"")(
        gpu_contiguous(ins[0]), gpu_contiguous(ins[1])
    )
    return [out.dimshuffle(0, 2)]
",if n . ndim != 2 :,155
"def _geo_indices(cls, inspected=None):
    inspected = inspected or []
    geo_indices = []
    inspected.append(cls)
    for field in cls._fields.values():
        if hasattr(field, ""document_type""):
            field_cls = field.document_type
            if field_cls in inspected:
                continue
            if hasattr(field_cls, ""_geo_indices""):
                geo_indices += field_cls._geo_indices(inspected)
        elif field._geo_index:
            geo_indices.append(field)
    return geo_indices
","if hasattr ( field_cls , ""_geo_indices"" ) :",155
"def get_skip_list(self, handle):
    todo = [handle]
    skip = [handle]
    while todo:
        handle = todo.pop()
        for child in self.dbstate.db.find_backlink_handles(handle, [""Place""]):
            if child[1] not in skip:
                todo.append(child[1])
                skip.append(child[1])
    return skip
",if child [ 1 ] not in skip :,108
"def convertstore(self, inputstore, includefuzzy=False):
    """"""converts a file to .lang format""""""
    thetargetfile = lang.LangStore(mark_active=self.mark_active)
    # Run over the po units
    for pounit in inputstore.units:
        if pounit.isheader() or not pounit.istranslatable():
            continue
        newunit = thetargetfile.addsourceunit(pounit.source)
        if includefuzzy or not pounit.isfuzzy():
            newunit.settarget(pounit.target)
        else:
            newunit.settarget("""")
        if pounit.getnotes(""developer""):
            newunit.addnote(pounit.getnotes(""developer""), ""developer"")
    return thetargetfile
",if pounit . isheader ( ) or not pounit . istranslatable ( ) :,196
"def api_read(self):
    files = []
    files.append(""/bin/netcat"")
    files.append(""/etc/alternative/netcat"")
    files.append(""/bin/nc"")
    #     init variables
    installed = False
    support = False
    path = None
    for _file in files:
        file_content = self.shell.read(_file)
        if file_content:
            installed = True
            path = _file
            if ""-e filename"" in file_content:
                support = True
            break
    result = {
        ""netcat_installed"": installed,
        ""supports_shell_bind"": support,
        ""path"": path,
    }
    return result
",if file_content :,187
"def _create_waiter(self, func_name):
    if self._waiter is not None:
        if self._cancelling:
            if not self._waiter.done():
                raise RuntimeError(
                    ""%s() called while connection is "" ""being cancelled"" % func_name
                )
        else:
            raise RuntimeError(
                ""%s() called while another coroutine is ""
                ""already waiting for incoming ""
                ""data"" % func_name
            )
    self._waiter = create_future(self._loop)
    return self._waiter
",if self . _cancelling :,154
"def calculate(self):
    addr_space = utils.load_as(self._config)
    for mod in modules.lsmod(addr_space):
        # Finding the TC kernel module
        if str(mod.BaseDllName).lower() != ""truecrypt.sys"":
            continue
        for offset, password in self.scan_module(
            addr_space, mod.DllBase, self._config.MIN_LENGTH
        ):
            yield offset, password
","if str ( mod . BaseDllName ) . lower ( ) != ""truecrypt.sys"" :",120
"def on_touch_up(self, touch):
    try:
        if not self.h_picker_touch:
            return
        if not self.animating:
            if touch.grab_current is not self:
                if self.picker == ""hours"":
                    self.picker = ""minutes""
    except AttributeError:
        pass
    super().on_touch_up(touch)
",if not self . animating :,104
"def handle(self, *args, **options):
    dry_run = options.get(""dry_run"", False)
    state = options.get(""state"", None)
    if not dry_run:
        script_utils.add_file_logger(logger, __file__)
    with transaction.atomic():
        add_reviews_notification_setting(
            notification_type=options[""notification""], state=state
        )
        if dry_run:
            raise RuntimeError(""Dry run, transaction rolled back."")
",if dry_run :,126
"def __call__(self, es, params):
    ops = 0
    indices = mandatory(params, ""indices"", self)
    only_if_exists = params.get(""only-if-exists"", False)
    request_params = params.get(""request-params"", {})
    for index_name in indices:
        if not only_if_exists:
            es.indices.delete(index=index_name, params=request_params)
            ops += 1
        elif only_if_exists and es.indices.exists(index=index_name):
            self.logger.info(""Index [%s] already exists. Deleting it."", index_name)
            es.indices.delete(index=index_name, params=request_params)
            ops += 1
    return ops, ""ops""
",if not only_if_exists :,198
"def find_first_of_filetype(content, filterfiltype, attr=""name""):
    """"""Find the first of the file type.""""""
    filename = """"
    for _filename in content:
        if isinstance(_filename, str):
            if _filename.endswith(f"".{filterfiltype}""):
                filename = _filename
                break
        else:
            if getattr(_filename, attr).endswith(f"".{filterfiltype}""):
                filename = getattr(_filename, attr)
                break
    return filename
","if _filename . endswith ( f"".{filterfiltype}"" ) :",135
"def join(s, *p):
    path = s
    for t in p:
        if (not s) or isabs(t):
            path = t
            continue
        if t[:1] == "":"":
            t = t[1:]
        if "":"" not in path:
            path = "":"" + path
        if path[-1:] != "":"":
            path = path + "":""
        path = path + t
    return path
","if "":"" not in path :",115
"def cell_double_clicked(self, row, column):
    if column == 3:
        archive_name = self.selected_archive_name()
        if not archive_name:
            return
        mount_point = self.mount_points.get(archive_name)
        if mount_point is not None:
            QDesktopServices.openUrl(QtCore.QUrl(f""file:///{mount_point}""))
",if mount_point is not None :,107
"def parseLink(line):
    parts = line.split()
    optional = parts[0] == ""Link*:""
    assert optional or parts[0] == ""Link:""
    attrs = {}
    for attr in parts[1:]:
        k, v = attr.split(""="", 1)
        if k[-1] == ""*"":
            attr_optional = 1
            k = k[:-1]
        else:
            attr_optional = 0
        attrs[k] = (attr_optional, v)
    return (optional, attrs)
","if k [ - 1 ] == ""*"" :",134
"def should_wait(self, offer_hash: str):
    with self._lock:
        if self._offer_hash is not None:
            if self._offer_hash != offer_hash:
                logger.debug(
                    ""already processing another offer (%s vs %s)"",
                    self._offer_hash,
                    offer_hash,
                )
                return True
            if self._started == self._wtct_num_subtasks:
                logger.info(""all subtasks for `%s` have been started"", self._offer_hash)
                return True
        return False
",if self . _offer_hash is not None :,167
"def list_urls(self):
    for idx, job in enumerate(self.urlwatcher.jobs):
        if self.urlwatch_config.verbose:
            print(""%d: %s"" % (idx + 1, repr(job)))
        else:
            pretty_name = job.pretty_name()
            location = job.get_location()
            if pretty_name != location:
                print(""%d: %s ( %s )"" % (idx + 1, pretty_name, location))
            else:
                print(""%d: %s"" % (idx + 1, pretty_name))
    return 0
",if pretty_name != location :,157
"def _encode_realm(self, realm):
    # override default _encode_realm to fill in default realm field
    if realm is None:
        realm = self.default_realm
        if realm is None:
            raise TypeError(
                ""you must specify a realm explicitly, ""
                ""or set the default_realm attribute""
            )
    return self._encode_field(realm, ""realm"")
",if realm is None :,105
"def set(sensor_spec: dict, **kwargs):
    for key, value in kwargs.items():
        if key == ""position"":
            sensor_spec[""transform""] = SensorSpecs.get_position(value)
        elif key == ""attachment_type"":
            sensor_spec[key] = SensorSpecs.ATTACHMENT_TYPE[value]
        elif key == ""color_converter"":
            sensor_spec[key] = SensorSpecs.COLOR_CONVERTER[value]
","elif key == ""color_converter"" :",125
"def _check_arguments(self, arch, state):
    # TODO: add calling convention detection to individual functions, and use that instead of the
    # TODO: default calling convention of the platform
    cc = DEFAULT_CC[arch.name](arch)  # type: s_cc.SimCC
    for i, expected_arg in enumerate(self.arguments):
        if expected_arg is None:
            continue
        real_arg = cc.arg(state, i)
        expected_arg_type, expected_arg_value = expected_arg
        r = self._compare_arguments(
            state, expected_arg_type, expected_arg_value, real_arg
        )
        if not r:
            return False
    return True
",if not r :,183
"def all_projects():
    if not REPODIR:
        return
    # Future: make this path parameterisable.
    excludes = set([""tempest"", ""requirements""])
    for name in PROJECTS:
        name = name.strip()
        short_name = name.split(""/"")[-1]
        try:
            with open(os.path.join(REPODIR, short_name, ""setup.py""), ""rt"") as f:
                if ""pbr"" not in f.read():
                    continue
        except IOError:
            continue
        if short_name in excludes:
            continue
        yield (short_name, dict(name=name, short_name=short_name))
","if ""pbr"" not in f . read ( ) :",180
"def get_converter(self, key, default=None):
    """"""Gets a converter for the given key.""""""
    if key in self._vars:
        return self._vars[key].convert
    # necessary for keys that match regexes, such as `*PATH`s
    for k, var in self._vars.items():
        if isinstance(k, str):
            continue
        if k.match(key) is not None:
            converter = var.convert
            self._vars[key] = var
            break
    else:
        converter = self._get_default_converter(default=default)
    return converter
","if isinstance ( k , str ) :",154
"def get_artist(self, name):
    artist = self.artists.get(name)
    if not artist:
        if self.use_db:
            try:
                artist = q(m.Artist).filter_by(name=name).one()
            except NoResultFound:
                pass
            if artist and self.ram_cache:
                self.add_artist(artist)
    return artist
",if artist and self . ram_cache :,111
"def move(self, x, y):
    offset = self.h
    width = max((len(val.get()) for val in self.values))
    for i, label in enumerate(self.labels):
        if self.values[i].get() != """":
            label.place(x=x, y=y + offset)
            label.config(
                width=width,
                bg=(self.fg if self.selected == i else self.bg),
            )
            offset += self.h + (self.pady * 2)
        else:
            label.place(x=9999, y=9999)
    return
","if self . values [ i ] . get ( ) != """" :",166
"def visit_Assign(self, node):
    if len(node.targets) == 1:
        if isinstance(node.targets[0], ast.Subscript):
            plugPath = self.__plugPath(self.__path(node.targets[0]))
            if plugPath:
                self.plugWrites.add(plugPath)
    self.visit(node.value)
","if isinstance ( node . targets [ 0 ] , ast . Subscript ) :",93
"def _minimal_replacement_cost(self, first, second):
    first_symbols, second_symbols = set(), set()
    removal_cost, insertion_cost = 0, 0
    for a, b in itertools.zip_longest(first, second, fillvalue=None):
        if a is not None:
            first_symbols.add(a)
        if b is not None:
            second_symbols.add(b)
        removal_cost = max(removal_cost, len(first_symbols - second_symbols))
        insertion_cost = max(insertion_cost, len(second_symbols - first_symbols))
    return min(removal_cost, insertion_cost)
",if a is not None :,173
"def normalize_stroke(stroke):
    letters = set(stroke)
    if letters & _NUMBERS:
        if system.NUMBER_KEY in letters:
            stroke = stroke.replace(system.NUMBER_KEY, """")
        # Insert dash when dealing with 'explicit' numbers
        m = _IMPLICIT_NUMBER_RX.search(stroke)
        if m is not None:
            start = m.start(2)
            return stroke[:start] + ""-"" + stroke[start:]
    if ""-"" in letters:
        if stroke.endswith(""-""):
            stroke = stroke[:-1]
        elif letters & system.IMPLICIT_HYPHENS:
            stroke = stroke.replace(""-"", """")
    return stroke
",if m is not None :,180
"def serve_json(self, args=None):
    request = current.request
    response = current.response
    response.headers[""Content-Type""] = ""application/json; charset=utf-8""
    if not args:
        args = request.args
    d = dict(request.vars)
    if args and args[0] in self.json_procedures:
        s = self.call_service_function(self.json_procedures[args[0]], *args[1:], **d)
        if hasattr(s, ""as_list""):
            s = s.as_list()
        return response.json(s)
    self.error()
","if hasattr ( s , ""as_list"" ) :",164
"def get_art_abs(story_file):
    lines = read_text_file(story_file)
    lines = [line.lower() for line in lines]
    lines = [fix_missing_period(line) for line in lines]
    article_lines = []
    highlights = []
    next_is_highlight = False
    for idx, line in enumerate(lines):
        if line == """":
            continue  # empty line
        elif line.startswith(""@highlight""):
            next_is_highlight = True
        elif next_is_highlight:
            highlights.append(line)
        else:
            article_lines.append(line)
    article = "" "".join(article_lines)
    abstract = "" "".join(highlights)
    return article, abstract
",elif next_is_highlight :,194
"def _get_commands():
    proc = Popen([""react-native"", ""--help""], stdout=PIPE)
    should_yield = False
    for line in proc.stdout.readlines():
        line = line.decode().strip()
        if not line:
            continue
        if ""Commands:"" in line:
            should_yield = True
            continue
        if should_yield:
            yield line.split("" "")[0]
",if not line :,111
"def _wait_for_state(self, server_id, state, retries=50):
    for i in (0, retries):
        server = self.ex_get_server(server_id)
        if server.extra[""status""][""state""] == state:
            return
        sleep(5)
        if i == retries:
            raise Exception(""Retries count reached"")
","if server . extra [ ""status"" ] [ ""state"" ] == state :",95
"def add_letter(inner_letter):
    if inner_letter in alphabet:
        wordTrans.append(alphabet[inner_letter])
    else:
        l2 = stringTools.stripAccents(inner_letter)
        if l2 == inner_letter:
            raise KeyError(""Cannot translate "" + inner_letter)
        wordTrans.append(alphabet[""^""])
        wordTrans.append(alphabet[l2])
",if l2 == inner_letter :,105
"def _parse_message(data):
    try:
        jsonrpc_message = json.loads(data, encoding=""utf-8"")
        if jsonrpc_message.get(""jsonrpc"") != ""2.0"":
            raise InvalidRequest()
        del jsonrpc_message[""jsonrpc""]
        if ""result"" in jsonrpc_message.keys() or ""error"" in jsonrpc_message.keys():
            return Response(**jsonrpc_message)
        else:
            return Request(**jsonrpc_message)
    except json.JSONDecodeError:
        raise ParseError()
    except TypeError:
        raise InvalidRequest()
","if ""result"" in jsonrpc_message . keys ( ) or ""error"" in jsonrpc_message . keys ( ) :",163
"def get_buildings_in_range(self):
    # TODO Think about moving this to the Settlement class
    buildings = self.settlement.buildings
    for building in buildings:
        if building is self:
            continue
        if (
            distances.distance_rect_rect(self.position, building.position)
            <= self.radius
        ):
            yield building
",if building is self :,102
"def get_tab_title(self, uuid=None):
    """"""Return the title of a parent tab of a given terminal""""""
    maker = Factory()
    terminal = self.terminator.find_terminal_by_uuid(uuid)
    window = terminal.get_toplevel()
    root_widget = window.get_children()[0]
    if maker.isinstance(root_widget, ""Notebook""):
        for tab_child in root_widget.get_children():
            terms = [tab_child]
            if not maker.isinstance(terms[0], ""Terminal""):
                terms = enumerate_descendants(tab_child)[1]
            if terminal in terms:
                return root_widget.get_tab_label(tab_child).get_label()
","if not maker . isinstance ( terms [ 0 ] , ""Terminal"" ) :",186
"def is_valid_origin(origin):
    if not settings.SENTRY_ALLOW_ORIGIN:
        return False
    if settings.SENTRY_ALLOW_ORIGIN == ""*"":
        return True
    if not origin:
        return False
    origin = origin.lower()
    for value in settings.SENTRY_ALLOW_ORIGIN:
        if isinstance(value, string_types):
            if value.lower() == origin:
                return True
        else:
            if value.match(origin):
                return True
    return False
","if isinstance ( value , string_types ) :",137
"def addr_func(ctx):
    nodes = ctx.xpathEval(base_xpath + ""/ip"")
    nodes = nodes or []
    ret = []
    for node in nodes:
        addr = node.prop(""address"")
        pref = node.prop(""prefix"")
        if not addr:
            continue
        if pref:
            addr += ""/%s"" % pref
        ret.append(addr)
    return ret
",if pref :,107
"def _select_delete(self, select, args, row_index=0, arg_index=0):
    count = 0
    delete = ""DELETE FROM Cache WHERE rowid IN (%s)""
    try:
        while True:
            with self._transact() as (sql, cleanup):
                rows = sql(select, args).fetchall()
                if not rows:
                    break
                count += len(rows)
                sql(delete % "","".join(str(row[0]) for row in rows))
                for row in rows:
                    args[arg_index] = row[row_index]
                    cleanup(row[-1])
    except Timeout:
        raise Timeout(count)
    return count
",if not rows :,193
"def _set_checkpointer(self, train_config):
    if train_config[""checkpoint""]:
        # Default to valid split for checkpoint metric
        checkpoint_config = train_config[""checkpoint_config""]
        checkpoint_metric = checkpoint_config[""checkpoint_metric""]
        if checkpoint_metric.count(""/"") == 0:
            checkpoint_config[""checkpoint_metric""] = f""valid/{checkpoint_metric}""
        self.checkpointer = Checkpointer(
            checkpoint_config, verbose=self.config[""verbose""]
        )
    else:
        self.checkpointer = None
","if checkpoint_metric . count ( ""/"" ) == 0 :",142
"def mlt_version_is_greater_correct(test_version):
    runtime_ver = mlt_version.split(""."")
    test_ver = test_version.split(""."")
    if runtime_ver[0] > test_ver[0]:
        return True
    elif runtime_ver[0] == test_ver[0]:
        if runtime_ver[1] > test_ver[1]:
            return True
        elif runtime_ver[1] == test_ver[1]:
            if runtime_ver[2] > test_ver[2]:
                return True
    return False
",elif runtime_ver [ 1 ] == test_ver [ 1 ] :,148
"def generate_scraper_test(class_name, host_name):
    with open(""templates/test_scraper.py"") as source:
        code = source.read()
        program = ast.parse(code)
        state = GenerateTestScraperState(class_name, host_name, code)
        for node in ast.walk(program):
            if not state.step(node):
                break
        output = f""tests/test_{class_name.lower()}.py""
        with open(output, ""w"") as target:
            target.write(state.result())
",if not state . step ( node ) :,154
"def _init_fetches(self):
    futures = []
    for node_id, request in six.iteritems(self._create_fetch_requests()):
        if self._client.ready(node_id):
            log.debug(""Sending FetchRequest to node %s"", node_id)
            future = self._client.send(node_id, request)
            future.add_callback(self._handle_fetch_response, request)
            future.add_errback(log.error, ""Fetch to node %s failed: %s"", node_id)
            futures.append(future)
    self._fetch_futures.extend(futures)
    self._clean_done_fetch_futures()
    return futures
",if self . _client . ready ( node_id ) :,176
"def discover(cls, path, **kwargs):
    if kwargs.pop(""collection"", None) is not None:
        raise TypeError(""collection argument must not be given."")
    path = expand_path(path)
    try:
        collections = os.listdir(path)
    except OSError as e:
        if e.errno != errno.ENOENT:
            raise
    else:
        for collection in collections:
            collection_path = os.path.join(path, collection)
            if not cls._validate_collection(collection_path):
                continue
            args = dict(collection=collection, path=collection_path, **kwargs)
            yield args
",if not cls . _validate_collection ( collection_path ) :,167
"def writefile(filename, now):
    with open(os.path.join(""src/teensy/"" + filename)) as fileopen, open(
        os.path.join(core.userconfigpath, ""reports/teensy_{0}.ino"".format(now)), ""w""
    ) as filewrite:
        for line in fileopen:
            match = re.search(""IPADDR"", line)
            if match:
                line = line.replace(""IPADDR"", ipaddr)
            match = re.search(""12,12,12,12"", line)
            if match:
                ipaddr_replace = ipaddr.replace(""."", "","", 4)
                line = line.replace(""12,12,12,12"", ipaddr_replace)
            filewrite.write(line)
",if match :,199
"def get_added_files(diff):
    """"""hacky approach to extract added files from github diff output""""""
    prefix = ""+++ b/""
    lastline = None
    for line in diff.splitlines():
        line = line.strip()
        if line.startswith(prefix) and lastline and lastline == ""--- /dev/null"":
            yield line[len(prefix) :]
        lastline = line
","if line . startswith ( prefix ) and lastline and lastline == ""--- /dev/null"" :",100
"def bpe_decode(tokens: List[str]) -> List[str]:
    words = []
    pieces: List[str] = []
    for t in tokens:
        if t.endswith(DecodeMixin.bpe_cont_str):
            pieces.append(t[:-2])
        else:
            words.append("""".join(pieces + [t]))
            pieces = []
    if len(pieces) > 0:
        words.append("""".join(pieces))
    return words
",if t . endswith ( DecodeMixin . bpe_cont_str ) :,120
"def _maybe_encrypt(self, data):
    gpgr = self.config.prefs.gpg_recipient
    tokeys = [gpgr] if gpgr not in (None, """", ""!CREATE"", ""!PASSWORD"") else None
    if self.config.get_master_key():
        with EncryptingStreamer(self.config.get_master_key(), delimited=True) as es:
            es.write(data)
            es.finish()
            return es.save(None)
    elif tokeys:
        stat, edata = GnuPG(self.config, event=GetThreadEvent()).encrypt(
            data, tokeys=tokeys
        )
        if stat == 0:
            return edata
    return data
",if stat == 0 :,186
"def faces_uvs_list(self) -> List[torch.Tensor]:
    if self._faces_uvs_list is None:
        if self.isempty():
            self._faces_uvs_list = [
                torch.empty((0, 3), dtype=torch.float32, device=self.device)
            ] * self._N
        else:
            self._faces_uvs_list = padded_to_list(
                self._faces_uvs_padded, split_size=self._num_faces_per_mesh
            )
    return self._faces_uvs_list
",if self . isempty ( ) :,157
"def handle_resource_click(self, widget, event):
    if event.getButton() == fife.MouseEvent.LEFT:
        self.show_resource_menu(widget.parent, widget.parent.parent)
    elif event.getButton() == fife.MouseEvent.RIGHT:
        if self.resource_menu_shown:
            # abort resource selection (#1310)
            self.hide_resource_menu()
        else:
            # remove the load/unload order
            self.add_resource(slot=widget.parent, res_id=0, entry=widget.parent.parent)
",if self . resource_menu_shown :,151
"def update_device(self, device):
    for bridge in self.bridges:
        if bridge.device == device:
            if bridge.device.ip != device.ip or bridge.device.port != device.port:
                bridge.device.ip = device.ip
                bridge.device.port = device.port
                logger.info(
                    'Updated device ""{}"" - New settings: {}:{}'.format(
                        device.label, device.ip, device.port
                    )
                )
                self.update()
                self.share_bridges()
                break
",if bridge . device . ip != device . ip or bridge . device . port != device . port :,172
"def endElement(self, name, value, connection):
    if name == ""OptionGroupName"":
        self.name = value
    elif name == ""EngineName"":
        self.engine_name = value
    elif name == ""MajorEngineVersion"":
        self.major_engine_version = value
    elif name == ""OptionGroupDescription"":
        self.description = value
    elif name == ""AllowsVpcAndNonVpcInstanceMemberships"":
        if value.lower() == ""true"":
            self.allow_both_vpc_and_nonvpc = True
        else:
            self.allow_both_vpc_and_nonvpc = False
    elif name == ""VpcId"":
        self.vpc_id = value
    else:
        setattr(self, name, value)
","if value . lower ( ) == ""true"" :",194
"def log_items(self, interface, action, media, items):
    if not items:
        return
        # Log each item
    for item in items:
        if not item:
            continue
        log.info(
            ""[%s:%s](%s) %r (%r)"",
            interface,
            action,
            media,
            item.get(""title""),
            item.get(""year""),
        )
        if media == ""shows"":
            # Log each episode
            self.log_episodes(item)
","if media == ""shows"" :",150
"def _copy_files(self, files, src, dest, message=""""):
    for filepath in files:
        srcpath = os.path.join(src, filepath)
        destpath = os.path.join(dest, filepath)
        if message:
            print(""{}: {}"".format(message, destpath))
        if os.path.exists(srcpath):
            destdir = os.path.dirname(destpath)
            if not os.path.isdir(destdir):
                os.makedirs(destdir)
            shutil.copy(srcpath, destpath)
        elif os.path.exists(destpath):
            os.remove(destpath)
",if os . path . exists ( srcpath ) :,167
"def disconnect(self, endpoint=None):
    if endpoint is not None:
        conn = self.connections_endpoints.pop(endpoint, None)
        if conn:
            self.connections.pop(conn.get_socket_object(), None)
            conn.close()
    else:
        for _, conn in self.connections_endpoints.items():
            conn.close()
        self.connections_endpoints = {}
        self.connections = {}
",if conn :,115
"def cisco_inventory(raw):
    for match in INVENTORY_RE.finditer(raw):
        d = match.groupdict()
        if d[""sn""] in SERIAL_BLACKLIST:
            d[""sn""] = None
        d[""descr""] = d[""descr""].strip('""')
        d[""name""] = d[""name""].strip('""')
        yield d
","if d [ ""sn"" ] in SERIAL_BLACKLIST :",91
"def _dispatchBubblingEvent(self, tag, evtType, evtObject):
    for node in tag.parents:
        if node is None:  # pragma: no cover
            break
        if not node._listeners:
            continue
        if evtObject._stoppedPropagation:  # pragma: no cover
            continue
        capture_listeners, bubbling_listeners = self._get_listeners(
            node, evtType
        )  # pylint:disable=unused-variable
        for c in bubbling_listeners:
            evtObject.currentTarget = node._node
            self.do_dispatch(c, evtObject)
",if evtObject . _stoppedPropagation :,165
"def got_shares(self, shares):
    if self.check_reneging:
        if self._no_more_shares:
            self.finished_d.errback(
                unittest.FailTest(
                    ""The node was told by the share finder that it is destined to remain hungry, then was given another share.""
                )
            )
            return
    self.got += len(shares)
    log.msg(""yyy 3 %s.got_shares(%s) got: %s"" % (self, shares, self.got))
    if self.got == 3:
        self.finished_d.callback(True)
",if self . _no_more_shares :,167
"def get_class_obj_(self, node, default_class=None):
    class_obj1 = default_class
    if ""xsi"" in node.nsmap:
        classname = node.get(""{%s}type"" % node.nsmap[""xsi""])
        if classname is not None:
            names = classname.split("":"")
            if len(names) == 2:
                classname = names[1]
            class_obj2 = globals().get(classname)
            if class_obj2 is not None:
                class_obj1 = class_obj2
    return class_obj1
",if len ( names ) == 2 :,154
"def update(self, mapping, update_only=False):
    for name in mapping:
        if update_only and name in self:
            # nested and inner objects, merge recursively
            if hasattr(self[name], ""update""):
                # FIXME only merge subfields, not the settings
                self[name].update(mapping[name], update_only)
            continue
        self.field(name, mapping[name])
    if update_only:
        for name in mapping._meta:
            if name not in self._meta:
                self._meta[name] = mapping._meta[name]
    else:
        self._meta.update(mapping._meta)
",if update_only and name in self :,175
"def configure(self):
    super(Command, self).configure()
    if self.needs_config and not self.resolver:
        # Checking if a default config file is present
        if not self._check_config():
            self.add_option(
                ""config"", ""c"", InputOption.VALUE_REQUIRED, ""The config file path""
            )
",if not self . _check_config ( ) :,92
"def is_metric(cls, key_type, comparator):
    if key_type == cls._METRIC_IDENTIFIER:
        if comparator not in cls.VALID_METRIC_COMPARATORS:
            raise MlflowException(
                ""Invalid comparator '%s' ""
                ""not one of '%s"" % (comparator, cls.VALID_METRIC_COMPARATORS),
                error_code=INVALID_PARAMETER_VALUE,
            )
        return True
    return False
",if comparator not in cls . VALID_METRIC_COMPARATORS :,126
"def get_full_qualified_name(self, node: Element) -> str:
    if node.get(""reftype"") == ""option"":
        progname = node.get(""std:program"")
        command = ws_re.split(node.get(""reftarget""))
        if progname:
            command.insert(0, progname)
        option = command.pop()
        if command:
            return ""."".join([""-"".join(command), option])
        else:
            return None
    else:
        return None
",if command :,133
"def log_unsupported(logger, message, dictionary):
    if len(dictionary) < 1:
        return
    # Display unsupported service list
    logger.info(message, len(dictionary))
    # Display individual warnings for each service
    for service in dictionary.keys():
        if service is None or service in IGNORED_SERVICES:
            logger.info(""Ignoring service: %s"" % service)
            continue
        # Log unsupported service warning
        logger.warn(
            ""Unsupported service: %s"" % service,
            extra={
                ""event"": {
                    ""module"": __name__,
                    ""name"": ""unsupported_service"",
                    ""key"": service,
                }
            },
        )
",if service is None or service in IGNORED_SERVICES :,197
"def encode_password(pw):
    """"""Encode password in hexadecimal if needed""""""
    enc = False
    if pw:
        encPW = __PW_PREFIX
        for c in pw:
            cnum = ord(c)
            if c == ""#"" or cnum < 33 or cnum > 126:
                enc = True
            encPW += ""%2x"" % cnum
        if enc:
            return encPW
    return pw
",if enc :,115
"def matrix_min_and_max(matrix):
    _min = None
    _max = None
    for row in matrix:
        for el in row:
            val = el
            if _min is None or val < _min:
                _min = val
            if _max is None or val > _max:
                _max = val
    return _min, _max
",if _max is None or val > _max :,102
"def __init__(self, content=None, parent=None):
    Transformable.__init__(self, content, parent)
    self._items = []
    for element in content:
        if not element.tag.startswith(namespace):
            continue
        tag = element.tag[len(namespace) :]
        if tag == ""g"":
            item = Group(element, self)
        elif tag == ""path"":
            item = Path(element, self)
        else:
            log.warn(""Unhandled SVG tag (%s)"" % tag)
            continue
        self._items.append(item)
",if not element . tag . startswith ( namespace ) :,154
"def reset_appid(self):
    # called by web_control
    with self.lock:
        self.working_appid_list = list()
        for appid in self.config.GAE_APPIDS:
            if not appid:
                self.config.GAE_APPIDS.remove(appid)
                continue
            self.working_appid_list.append(appid)
        self.not_exist_appids = []
        self.out_of_quota_appids = []
    self.last_reset_time = time.time()
",if not appid :,152
"def find_widget(self, pos):
    for widget in self.subwidgets[::-1]:
        if widget.visible:
            r = widget.rect
            if r.collidepoint(pos):
                return widget.find_widget(subtract(pos, r.topleft))
    return self
",if r . collidepoint ( pos ) :,77
"def _get_names(dirs):
    """"""Get alphabet and label names, union across all dirs.""""""
    alphabets = set()
    label_names = {}
    for d in dirs:
        for example in _walk_omniglot_dir(d):
            alphabet, alphabet_char_id, label, _, _ = example
            alphabets.add(alphabet)
            label_name = ""%s_%d"" % (alphabet, alphabet_char_id)
            if label in label_names:
                assert label_names[label] == label_name
            else:
                label_names[label] = label_name
    label_names = [label_names[k] for k in sorted(label_names)]
    return alphabets, label_names
",if label in label_names :,196
"def model():
    with pyro.plate_stack(""plates"", shape):
        with pyro.plate(""particles"", 200000):
            if ""dist_type"" == ""Normal"":
                pyro.sample(""x"", dist.Normal(loc, scale))
            else:
                pyro.sample(""x"", dist.StudentT(10.0, loc, scale))
","if ""dist_type"" == ""Normal"" :",103
"def set_note_pinned(self, key, pinned):
    n = self.notes[key]
    old_pinned = utils.note_pinned(n)
    if pinned != old_pinned:
        if ""systemtags"" not in n:
            n[""systemtags""] = []
        systemtags = n[""systemtags""]
        if pinned:
            # which by definition means that it was NOT pinned
            systemtags.append(""pinned"")
        else:
            systemtags.remove(""pinned"")
        n[""modifydate""] = time.time()
        self.notify_observers(
            ""change:note-status"",
            events.NoteStatusChangedEvent(what=""modifydate"", key=key),
        )
",if pinned :,186
"def __init__(self, name, contents):
    self.name = name
    self.all_entries = []
    self.attr = []
    self.child = []
    self.seq_child = []
    for entry in contents:
        clean_entry = entry.rstrip(""*"")
        self.all_entries.append(clean_entry)
        if entry.endswith(""**""):
            self.seq_child.append(clean_entry)
        elif entry.endswith(""*""):
            self.child.append(clean_entry)
        else:
            self.attr.append(entry)
","elif entry . endswith ( ""*"" ) :",147
"def testToFileBinary(self):
    z = dns.zone.from_file(here(""example""), ""example"")
    try:
        f = open(here(""example3-binary.out""), ""wb"")
        z.to_file(f)
        f.close()
        ok = filecmp.cmp(here(""example3-binary.out""), here(""example3.good""))
    finally:
        if not _keep_output:
            os.unlink(here(""example3-binary.out""))
    self.failUnless(ok)
",if not _keep_output :,133
"def test_collect_gradients_with_allreduce_failure_case(self):
    worker = self._workers[1]
    train_db, _ = get_mnist_dataset(self._batch_size)
    for step, (x, y) in enumerate(train_db):
        if step == 0:
            worker._run_model_call_before_training(x)
        if step == self._test_steps:
            break
        self.assertEqual(
            worker._calculate_grads_and_report_with_allreduce(None),
            False,
            ""Should fail when no data is received"",
        )
",if step == self . _test_steps :,160
"def clean(self):
    data = self.cleaned_data
    number, ccv = data.get(""number""), data.get(""ccv"")
    if number and ccv:
        if bankcards.is_amex(number) and len(ccv) != 4:
            raise forms.ValidationError(
                _(""American Express cards use a 4 digit security code"")
            )
    return data
",if bankcards . is_amex ( number ) and len ( ccv ) != 4 :,104
"def _gen_GreaterEqual(self, args, ret_type):
    result = []
    for lhs, rhs in pairwise(args):
        if ret_type == real_type:
            result.append(self.builder.fcmp_ordered("">="", lhs, rhs))
        elif ret_type == int_type:
            result.append(self.builder.icmp_signed("">="", lhs, rhs))
        else:
            raise CompileError()
    return reduce(self.builder.and_, result)
",if ret_type == real_type :,120
"def console_get(context, console_id, instance_id=None):
    query = (
        model_query(context, models.Console, read_deleted=""yes"")
        .filter_by(id=console_id)
        .options(joinedload(""pool""))
    )
    if instance_id is not None:
        query = query.filter_by(instance_id=instance_id)
    result = query.first()
    if not result:
        if instance_id:
            raise exception.ConsoleNotFoundForInstance(
                console_id=console_id, instance_id=instance_id
            )
        else:
            raise exception.ConsoleNotFound(console_id=console_id)
    return result
",if instance_id :,185
"def publish():
    pub = await aioredis.create_redis((""localhost"", 6379))
    while not tsk.done():
        # wait for clients to subscribe
        while True:
            subs = await pub.pubsub_numsub(""channel:1"")
            if subs[b""channel:1""] == 1:
                break
            await asyncio.sleep(0, loop=loop)
        # publish some messages
        for msg in [""one"", ""two"", ""three""]:
            await pub.publish(""channel:1"", msg)
        # send stop word
        await pub.publish(""channel:1"", STOPWORD)
    pub.close()
    await pub.wait_closed()
","if subs [ b""channel:1"" ] == 1 :",176
"def read(self, size=None):
    if not size:
        size = self._size
        contents = BytesIO()
        while True:
            blocks = GzipFile.read(self, size)
            if not blocks:
                contents.flush()
                break
            contents.write(blocks)
        return contents.getvalue()
    else:
        return GzipFile.read(self, size)
",if not blocks :,108
"def i2repr(self, pkt, x):
    if type(x) is list or type(x) is tuple:
        return repr(x)
    if self.multi:
        r = []
    else:
        r = """"
    i = 0
    while x:
        if x & 1:
            if self.multi:
                r += [self.names[i]]
            else:
                r += self.names[i]
        i += 1
        x >>= 1
    if self.multi:
        r = ""+"".join(r)
    return r
",if x & 1 :,154
"def _run(self):
    while not self.stopped:
        # Prevent calling bus.send from multiple threads
        with self.lock:
            started = time.time()
            try:
                self.bus.send(self.message)
            except Exception as exc:
                log.exception(exc)
                break
        if self.end_time is not None and time.time() >= self.end_time:
            break
        # Compensate for the time it takes to send the message
        delay = self.period - (time.time() - started)
        time.sleep(max(0.0, delay))
",if self . end_time is not None and time . time ( ) >= self . end_time :,169
"def currentLevel(self):
    currentStr = """"
    for stackType, stackValue in self.stackVals:
        if stackType == ""dict"":
            if isinstance(stackValue, str):
                currentStr += ""['"" + stackValue + ""']""
            else:  # numeric key...
                currentStr += ""["" + str(stackValue) + ""]""
        elif stackType == ""listLike"":
            currentStr += ""["" + str(stackValue) + ""]""
        elif stackType == ""getattr"":
            currentStr += "".__getattribute__('"" + stackValue + ""')""
        else:
            raise Exception(f""Cannot get attribute of type {stackType}"")
    return currentStr
","if isinstance ( stackValue , str ) :",176
"def restoreParent(self):
    if self.sid.isRoot:
        return
    with self.suspendMouseButtonNavigation():
        confirm, opt = self.confirmRestore((self.path,))
        if not confirm:
            return
        if opt[""delete""] and not self.confirmDelete(warnRoot=self.path == ""/""):
            return
    rd = RestoreDialog(self, self.sid, self.path, **opt)
    rd.exec()
",if not confirm :,114
"def keep_vocab_item(word, count, min_count, trim_rule=None):
    default_res = count >= min_count
    if trim_rule is None:
        return default_res
    else:
        rule_res = trim_rule(word, count, min_count)
        if rule_res == RULE_KEEP:
            return True
        elif rule_res == RULE_DISCARD:
            return False
        else:
            return default_res
",if rule_res == RULE_KEEP :,125
"def _get_cuda_device(*args):
    # Returns cuda.Device or DummyDevice.
    for arg in args:
        if type(arg) is not bool and isinstance(arg, _integer_types):
            check_cuda_available()
            return Device(arg)
        if isinstance(arg, ndarray):
            if arg.device is None:
                continue
            return arg.device
        if available and isinstance(arg, Device):
            return arg
    # NOTE: This function returns DummyDevice for both NumPy and ChainerX
    return DummyDevice
","if isinstance ( arg , ndarray ) :",144
"def __init__(
    self,
    filename,
    metadata_name,
    metadata_column,
    message=""Value for metadata not found."",
    line_startswith=None,
    split=""\t"",
):
    self.metadata_name = metadata_name
    self.message = message
    self.valid_values = []
    for line in open(filename):
        if line_startswith is None or line.startswith(line_startswith):
            fields = line.split(split)
            if metadata_column < len(fields):
                self.valid_values.append(fields[metadata_column].strip())
",if metadata_column < len ( fields ) :,150
"def FindEnclosingBracketGroup(input_str):
    stack = []
    start = -1
    for index, char in enumerate(input_str):
        if char in LBRACKETS:
            stack.append(char)
            if start == -1:
                start = index
        elif char in BRACKETS:
            if not stack:
                return (-1, -1)
            if stack.pop() != BRACKETS[char]:
                return (-1, -1)
            if not stack:
                return (start, index + 1)
    return (-1, -1)
",if start == - 1 :,163
"def _on_message(self, msg: str) -> None:
    obj = json.loads(msg)
    _id = obj.get(""id"")
    if _id and _id in self._callbacks:
        callback = self._callbacks.pop(_id)
        if ""error"" in obj:
            error = obj[""error""]
            msg = error.get(""message"")
            data = error.get(""data"")
            callback.set_exception(NetworkError(f""Protocol Error: {msg} {data}""))
        else:
            result = obj.get(""result"")
            callback.set_result(result)
    else:
        self.emit(obj.get(""method""), obj.get(""params""))
","if ""error"" in obj :",183
"def _get_containers_with_state(self, container_names, select_random, *container_states):
    containers = self._get_all_containers()
    candidates = dict((c.name, c) for c in containers if c.status in container_states)
    if select_random and candidates:
        return [random.choice(list(candidates.values()))]
    if container_names is None:
        return list(candidates.values())
    found = []
    for name in container_names:
        container = candidates.get(name)
        if not container:
            raise BlockadeError(
                ""Container %s is not found or not any of %s"" % (name, container_states)
            )
        found.append(container)
    return found
",if not container :,193
"def __eq__(self, other):
    if isinstance(other, WeakMethod):
        if self.function != other.function:
            return False
        # check also if either instance is None or else if instances are equal
        if self.instance is None:
            return other.instance is None
        else:
            return self.instance() == other.instance()
    elif callable(other):
        return self == WeakMethod(other)
    else:
        return False
",if self . function != other . function :,120
"def last_bottle_hash():
    """"""Fetch the bottle do ... end from the latest brew formula""""""
    resp = requests.get(HOMEBREW_FORMULAR_LATEST)
    resp.raise_for_status()
    lines = resp.text.split(""\n"")
    look_for_end = False
    start = 0
    end = 0
    for idx, content in enumerate(lines):
        if look_for_end:
            if ""end"" in content:
                end = idx
                break
        else:
            if ""bottle do"" in content:
                start = idx
                look_for_end = True
    return ""\n"".join(lines[start : end + 1])
","if ""end"" in content :",184
"def wrapper(fn):
    if debug_run_test_calls:
        ret = str(fn(*args, *kwargs))
        print(""TEST: %s()"" % fn.__name__)
        if args:
            print(""  arg:"", args)
        if kwargs:
            print(""  kwa:"", kwargs)
        print(""  ret:"", ret)
    return fn
",if args :,95
"def parse_socket_line(line):
    lsp = line.strip().split()
    if not len(lsp) in {3, 5}:
        print(line, ""is malformed"")
        return UNPARSABLE
    else:
        socket_type = sock_dict.get(lsp[2])
        socket_name = lsp[1]
        if not socket_type:
            return UNPARSABLE
        elif len(lsp) == 3:
            return socket_type, socket_name, None, None
        else:
            default = processed(lsp[3])
            nested = processed(lsp[4])
            return socket_type, socket_name, default, nested
",if not socket_type :,182
"def release(self):
    me, lock_count = self.__begin()
    try:
        if me is None:
            return
        self._count = count = self._count - 1
        if not count:
            self._owner = None
            self._block.release()
    finally:
        self.__end(me, lock_count)
",if not count :,92
"def Traverse(self):
    """"""A generator for _IMAGE_RESOURCE_DATA_ENTRY under this node.""""""
    for entry in self:
        if entry.ChildIsEntry:
            for subentry in entry.Entry.Traverse():
                yield subentry
        else:
            yield entry.OffsetToData.dereference()
",if entry . ChildIsEntry :,83
"def getInstances_WithSource(self, instancesAmount, sourceObject, scenes):
    if sourceObject is None:
        self.removeAllObjects()
        return []
    else:
        sourceHash = hash(sourceObject)
        if self.identifier in lastSourceHashes:
            if lastSourceHashes[self.identifier] != sourceHash:
                self.removeAllObjects()
        lastSourceHashes[self.identifier] = sourceHash
    return self.getInstances_Base(instancesAmount, sourceObject, scenes)
",if self . identifier in lastSourceHashes :,132
"def used_pos():
    pos_along_edges = []
    for e in edges:
        A, B = pos[e[0]], pos[e[1]]
        if A[0] == B[0]:  # Y-axis edge.
            for i in range(A[1], B[1], np.sign(B[1] - A[1])):
                pos_along_edges.append((A[0], i))
        else:  # X-axis edge.
            for i in range(A[0], B[0], np.sign(B[0] - A[0])):
                pos_along_edges.append((i, A[1]))
    return list(pos.values()) + pos_along_edges
",if A [ 0 ] == B [ 0 ] :,186
"def __init__(
    self, plugin_name=None, builtin=False, deprecated=False, config=None, session=None
):
    if builtin and isinstance(builtin, (str, unicode)):
        builtin = os.path.basename(builtin)
        for ignore in ("".py"", "".pyo"", "".pyc""):
            if builtin.endswith(ignore):
                builtin = builtin[: -len(ignore)]
        if builtin not in self.LOADED:
            self.LOADED.append(builtin)
    self.loading_plugin = plugin_name
    self.loading_builtin = plugin_name and builtin
    self.builtin = builtin
    self.deprecated = deprecated
    self.session = session
    self.config = config
    self.manifests = []
",if builtin . endswith ( ignore ) :,181
"def input(self):
    if self.input_ is None:
        self.lazy_init_lock_.acquire()
        try:
            if self.input_ is None:
                self.input_ = InputSettings()
        finally:
            self.lazy_init_lock_.release()
    return self.input_
",if self . input_ is None :,85
"def _shares_in_results(data):
    shares_in_device, shares_in_subdevice = False, False
    for plugin_name, plugin_result in data.iteritems():
        if plugin_result[""status""] == ""error"":
            continue
        if ""device"" not in plugin_result:
            continue
        if ""disk_shares"" in plugin_result[""device""]:
            shares_in_device = True
        for subdevice in plugin_result[""device""].get(""subdevices"", []):
            if ""disk_shares"" in subdevice:
                shares_in_subdevice = True
                break
    return shares_in_device, shares_in_subdevice
","if plugin_result [ ""status"" ] == ""error"" :",175
"def m2i(self, pkt, x):
    res = []
    while x:
        cur = []
        # while x and x[0] != b'\x00':
        while x and x[0] != 0:
            l = x[0]
            cur.append(x[1 : l + 1])
            x = x[l + 1 :]
        res.append(b""."".join(cur))
        if x and x[0] == 0:
            x = x[1:]
    return res
",if x and x [ 0 ] == 0 :,137
"def generate_idempotent_uuid(params, model, **kwargs):
    for name in model.idempotent_members:
        if name not in params:
            params[name] = str(uuid.uuid4())
            logger.debug(
                ""injecting idempotency token (%s) into param '%s'.""
                % (params[name], name)
            )
",if name not in params :,100
"def __init__(self, name, signatures, kind, vm):
    super().__init__(name, vm)
    assert signatures
    self.kind = kind
    self.bound_class = BoundPyTDFunction
    self.signatures = signatures
    self._signature_cache = {}
    self._return_types = {sig.pytd_sig.return_type for sig in signatures}
    for sig in signatures:
        for param in sig.pytd_sig.params:
            if param.mutated_type is not None:
                self._has_mutable = True
                break
        else:
            self._has_mutable = False
    for sig in signatures:
        sig.function = self
        sig.name = self.name
",if param . mutated_type is not None :,183
"def sub_dict(d):
    r = {}
    for k in d:
        if type(d[k]) in prims:
            r[k] = d[k]
        elif type(d[k]) is list:
            r[k] = sub_list(d[k])
        elif type(d[k]) is dict:
            r[k] = sub_dict(d[k])
        else:
            print(""Unknown Type: {}"".format(type(d[k])))
    return r
",elif type ( d [ k ] ) is list :,134
"def listAdd():
    cpe = request.args.get(""cpe"")
    cpeType = request.args.get(""type"")
    lst = request.args.get(""list"")
    if cpe and cpeType and lst:
        status = (
            ""added_to_list""
            if addCPEToList(cpe, lst, cpeType)
            else ""already_exists_in_list""
        )
        returnList = db.getWhitelist() if lst == ""whitelist"" else db.getBlacklist()
        return jsonify({""status"": status, ""rules"": returnList, ""listType"": lst.title()})
    else:
        return jsonify({""status"": ""could_not_add_to_list""})
","if addCPEToList ( cpe , lst , cpeType )",184
"def _integrate_fixed_trajectory(self, h, T, step, relax):
    """"""Generates a solution trajectory of fixed length.""""""
    # initialize the solution using initial condition
    solution = np.hstack((self.t, self.y))
    while self.successful():
        self.integrate(self.t + h, step, relax)
        current_step = np.hstack((self.t, self.y))
        solution = np.vstack((solution, current_step))
        if (h > 0) and (self.t >= T):
            break
        elif (h < 0) and (self.t <= T):
            break
        else:
            continue
    return solution
",elif ( h < 0 ) and ( self . t <= T ) :,174
"def transform(self, X):
    if self.preprocessor is None:
        raise NotImplementedError()
    with warnings.catch_warnings():
        warnings.filterwarnings(""error"")
        X_new = self.preprocessor.transform(X)
        # TODO write a unittest for this case
        if X_new.shape[1] == 0:
            raise ValueError(""KernelPCA removed all features!"")
        return X_new
",if X_new . shape [ 1 ] == 0 :,102
"def playerData(s):
    """"""Returns a list of tuples of original string and dict of values""""""
    p = []
    i = 0
    while True:
        match = re_input.match(s, pos=i)
        if match is None:
            return p
        else:
            d = match.groupdict()
            if d[""args""] is not None:
                d[""degree""], d[""kwargs""] = getArgs(d[""args""])
            else:
                d[""degree""], d[""kwargs""] = """", {}
            del d[""args""]
            p.append((match.group().strip(), d))
            i = match.end()
    return
",if match is None :,178
"def extract_deps(file):
    # ~ print('Extracting from %s' % file)
    deps = set()
    for line in open(file).readlines():
        line = line.strip()
        if line.startswith(""import"") or line.startswith(""from""):
            words = line.split()
            if words[0] == ""import"" or (words[0] == ""from"" and words[2] == ""import""):
                deps.add(words[1])
    return deps
","if words [ 0 ] == ""import"" or ( words [ 0 ] == ""from"" and words [ 2 ] == ""import"" ) :",124
"def _remove_optional_none_type_hints(self, type_hints, defaults):
    # If argument has None as a default, typing.get_type_hints adds
    # optional None to the information it returns. We don't want that.
    for arg in defaults:
        if defaults[arg] is None and arg in type_hints:
            type_ = type_hints[arg]
            if self._is_union(type_):
                types = type_.__args__
                if len(types) == 2 and types[1] is type(None):
                    type_hints[arg] = types[0]
",if defaults [ arg ] is None and arg in type_hints :,157
"def _gaf10iterator(handle):
    for inline in handle:
        if inline[0] == ""!"":
            continue
        inrec = inline.rstrip(""\n"").split(""\t"")
        if len(inrec) == 1:
            continue
        inrec[3] = inrec[3].split(""|"")  # Qualifier
        inrec[5] = inrec[5].split(""|"")  # DB:reference(s)
        inrec[7] = inrec[7].split(""|"")  # With || From
        inrec[10] = inrec[10].split(""|"")  # Synonym
        inrec[12] = inrec[12].split(""|"")  # Taxon
        yield dict(zip(GAF10FIELDS, inrec))
","if inline [ 0 ] == ""!"" :",188
"def cvePluginInfo(self, cve, **args):
    cveInfo = []
    for plugin in self.getWebPlugins():
        try:
            data = plugin.cvePluginInfo(cve, **args)
            if type(data) == dict and ""title"" in data and ""data"" in data:
                cveInfo.append(data)
        except Exception as e:
            print(
                ""[!] Plugin %s failed on fetching CVE plugin info!"" % plugin.getName()
            )
            print(""[!]  -> %s"" % e)
    return cveInfo
","if type ( data ) == dict and ""title"" in data and ""data"" in data :",158
"def testLastContainerMarker(self):
    for format in [None, ""json"", ""xml""]:
        containers = self.env.account.containers({""format"": format})
        self.assertEquals(len(containers), len(self.env.containers))
        self.assert_status(200)
        containers = self.env.account.containers(
            parms={""format"": format, ""marker"": containers[-1]}
        )
        self.assertEquals(len(containers), 0)
        if format is None:
            self.assert_status(204)
        else:
            self.assert_status(200)
",if format is None :,155
"def _make_input_layers(self, rebuild=False):
    for name, layer in self.layer_map.items():
        layer.left_in_edges = len(layer.in_edges)
        if len(layer.in_edges) == 0:
            if rebuild:
                if not layer.get_attr(""scope""):
                    self.input_layers.append(name)
            else:
                self.input_layers.append(name)
",if rebuild :,123
"def widget_attrs(self, widget):
    attrs = super(IntegerField, self).widget_attrs(widget)
    if isinstance(widget, NumberInput):
        if self.min_value is not None:
            attrs[""min""] = self.min_value
        if self.max_value is not None:
            attrs[""max""] = self.max_value
    return attrs
",if self . min_value is not None :,94
"def _get_outfile(self):
    outfile = self.inputs.transformed_file
    if not isdefined(outfile):
        if self.inputs.inverse is True:
            if self.inputs.fs_target is True:
                src = ""orig.mgz""
            else:
                src = self.inputs.target_file
        else:
            src = self.inputs.source_file
        outfile = fname_presuffix(src, newpath=os.getcwd(), suffix=""_warped"")
    return outfile
",if self . inputs . fs_target is True :,134
"def TryMerge(self, d):
    while d.avail() > 0:
        tt = d.getVarInt32()
        if tt == 10:
            self.set_app_id(d.getPrefixedString())
            continue
        if tt == 16:
            self.set_num_memcacheg_backends(d.getVarInt32())
            continue
        if tt == 0:
            raise ProtocolBuffer.ProtocolBufferDecodeError
        d.skipData(tt)
",if tt == 16 :,128
"def try_to_find_osquery(self):
    extention = """"
    if platform.system() == ""Windows"":
        extention = "".exe""
    try:
        return resources.get_resource(""osqueryi"" + extention)
    except IOError as e:
        # Maybe it is installed on the system.
        if platform.system() == ""Windows"":
            result = r""c:\ProgramData\osquery\osqueryi.exe""
            if os.access(result, os.R_OK):
                return result
        else:
            # Try to find it somewhere on the system.
            return spawn.find_executable(""osqueryi"")
        raise e
","if os . access ( result , os . R_OK ) :",178
"def cleanWhitespace(self, val):
    val = val.replace(""*"", "" AND "").replace(""  "", "" "")
    if re.match(""\S+ \S"", val):
        matchs = re.findall(""(?:^|\(| )(.+?)(?:\)| OR| AND|$)"", val)
        for strMatch in matchs:
            if re.match(""\S+ \S"", strMatch):
                strUnescapeMatch = self.unescapeCharacter(strMatch)
                val = val.replace(strMatch, '""{}""'.format(strUnescapeMatch))
    return val.strip()
","if re . match ( ""\S+ \S"" , strMatch ) :",140
"def keyPressEvent(self, event):
    """"""Add up and down arrow key events to built in functionality.""""""
    keyPressed = event.key()
    if keyPressed in [Constants.UP_KEY, Constants.DOWN_KEY, Constants.TAB_KEY]:
        if keyPressed == Constants.UP_KEY:
            self.index = max(0, self.index - 1)
        elif keyPressed == Constants.DOWN_KEY:
            self.index = min(len(self.completerStrings) - 1, self.index + 1)
        elif keyPressed == Constants.TAB_KEY and self.completerStrings:
            self.tabPressed()
        if self.completerStrings:
            self.setTextToCompleterIndex()
    super(CueLineEdit, self).keyPressEvent(event)
",if keyPressed == Constants . UP_KEY :,192
"def find_parent_for_new_to(self, pos):
    """"""Figure out the parent object for something at 'pos'.""""""
    for children in self._editable_children:
        if children._start <= pos < children._end:
            return children.find_parent_for_new_to(pos)
        if children._start == pos and pos == children._end:
            return children.find_parent_for_new_to(pos)
    return self
",if children . _start <= pos < children . _end :,113
"def get_sentence(self):
    while True:
        self._seed += 1
        all_files = list(self._all_files)
        if self._shuffle:
            if self._n_gpus > 1:
                random.seed(self._seed)
            random.shuffle(all_files)
        for file_path in all_files:
            for ret in self._load_file(file_path):
                yield ret
        if self._mode == ""test"":
            break
",if self . _n_gpus > 1 :,134
"def to_multidevice(batch_iter, num_trainer):
    """"""to_multidevice""""""
    batch_dict = []
    for batch in batch_iter():
        batch_dict.append(batch)
        if len(batch_dict) == num_trainer:
            yield batch_dict
            batch_dict = []
    if len(batch_dict) > 0:
        log.warning(
            ""The batch (%s) can't fill all device (%s)""
            ""which will be discarded."" % (len(batch_dict), num_trainer)
        )
",if len ( batch_dict ) == num_trainer :,147
"def get_word_parens_range(self, offset, opening=""("", closing="")""):
    end = self._find_word_end(offset)
    start_parens = self.code.index(opening, end)
    index = start_parens
    open_count = 0
    while index < len(self.code):
        if self.code[index] == opening:
            open_count += 1
        if self.code[index] == closing:
            open_count -= 1
        if open_count == 0:
            return (start_parens, index + 1)
        index += 1
    return (start_parens, index)
",if open_count == 0 :,160
"def getNodeBySunid(self, sunid):
    """"""Return a node from its sunid.""""""
    if sunid in self._sunidDict:
        return self._sunidDict[sunid]
    if self.db_handle:
        self.getDomainFromSQL(sunid=sunid)
        if sunid in self._sunidDict:
            return self._sunidDict[sunid]
    else:
        return None
",if sunid in self . _sunidDict :,123
"def get_cabal_in_dir(cabal_dir):
    """"""Return .cabal file for cabal directory""""""
    for entry in os.listdir(cabal_dir):
        if entry.endswith("".cabal""):
            project_name = os.path.splitext(entry)[0]
            return (project_name, os.path.join(cabal_dir, entry))
    return (None, None)
","if entry . endswith ( "".cabal"" ) :",104
"def authenticate(self, username, password):
    # The user entered an email, so try to log them in by e-mail
    emails = ContactValue.objects.filter(
        value=username,
        field__field_type=""email"",
        contact__trash=False,
        contact__related_user__isnull=False,
    )
    for email in emails:
        try:
            user = email.contact.related_user.user.user
            if user.check_password(password):
                return user
        except:
            pass
    return None
",if user . check_password ( password ) :,147
"def get_art_abs(story_file):
    lines = read_text_file(story_file)
    lines = [line.lower() for line in lines]
    lines = [fix_missing_period(line) for line in lines]
    article_lines = []
    highlights = []
    next_is_highlight = False
    for idx, line in enumerate(lines):
        if line == """":
            continue  # empty line
        elif line.startswith(""@highlight""):
            next_is_highlight = True
        elif next_is_highlight:
            highlights.append(line)
        else:
            article_lines.append(line)
    article = "" "".join(article_lines)
    abstract = "" "".join(highlights)
    return article, abstract
","if line == """" :",194
"def find_token(self):
    found = False
    while not found:
        while self.data[self.index] in "" \t"":
            self.index += 1
        if self.data[self.index] == ""#"":
            while self.data[self.index] != ""\n"":
                self.index += 1
        if self.data[self.index] == ""\n"":
            self.index += 1
        else:
            found = True
","if self . data [ self . index ] == ""#"" :",123
"def parseBamPEFDistributionFile(self, f):
    d = dict()
    lastsample = []
    for line in f[""f""].splitlines():
        cols = line.rstrip().split(""\t"")
        if cols[0] == ""#bamPEFragmentSize"":
            continue
        elif cols[0] == ""Size"":
            continue
        else:
            s_name = self.clean_s_name(cols[2].rstrip().split(""/"")[-1], f[""root""])
            if s_name != lastsample:
                d[s_name] = dict()
                lastsample = s_name
            d[s_name].update({self._int(cols[0]): self._int(cols[1])})
    return d
",if s_name != lastsample :,194
"def get_user_home():
    if is_win():
        if sys.platform == ""cygwin"":
            # Need the fully qualified directory
            output = (
                subprocess.Popen(
                    [""cygpath"", ""-m"", os.path.expanduser(""~"")],
                    stdout=subprocess.PIPE,
                    stderr=subprocess.STDOUT,
                )
                .communicate()[0]
                .rstrip()
            )
            return output
        else:
            return os.environ[""USERPROFILE""]
    else:
        return os.path.expanduser(""~"")
","if sys . platform == ""cygwin"" :",164
"def _grouping_intervals(grouping):
    last_interval = None
    for interval in grouping:
        # if grouping is -1, we are done
        if interval == CHAR_MAX:
            return
        # 0: re-use last group ad infinitum
        if interval == 0:
            if last_interval is None:
                raise ValueError(""invalid grouping"")
            while True:
                yield last_interval
        yield interval
        last_interval = interval
",if interval == 0 :,124
"def remove_duplicates(model):
    for struct in model.structs:
        fields = []
        names = []
        for field in struct.fields:
            if field.name not in names:
                names.append(field.name)
                fields.append(field)
        struct.fields = fields
",if field . name not in names :,83
"def set_multi(self, value):
    del self[atype]
    for addr in value:
        # Support assigning dictionary versions of addresses
        # instead of full Address objects.
        if not isinstance(addr, Address):
            if atype != ""all"":
                addr[""type""] = atype
            elif ""atype"" in addr and ""type"" not in addr:
                addr[""type""] = addr[""atype""]
            addrObj = Address()
            addrObj.values = addr
            addr = addrObj
        self.append(addr)
","elif ""atype"" in addr and ""type"" not in addr :",146
"def import_directives():
    files_list = os.listdir(os.path.dirname(__file__))
    for directive_file in files_list:
        if not directive_file.endswith("".py"") or directive_file.startswith(""_""):
            continue
        __import__(
            ""gixy.directives."" + os.path.splitext(directive_file)[0], None, None, [""""]
        )
","if not directive_file . endswith ( "".py"" ) or directive_file . startswith ( ""_"" ) :",99
"def _get_all_tasks():
    proc = Popen([""yarn"", ""--help""], stdout=PIPE)
    should_yield = False
    for line in proc.stdout.readlines():
        line = line.decode().strip()
        if ""Commands:"" in line:
            should_yield = True
            continue
        if should_yield and ""- "" in line:
            yield line.split("" "")[-1]
","if ""Commands:"" in line :",103
"def _waitFakenetStopped(self, timeoutsec=None):
    retval = False
    while True:
        if self._confirmFakenetStopped():
            retval = True
            break
        time.sleep(1)
        if timeoutsec is not None:
            timeoutsec -= 1
            if timeoutsec <= 0:
                break
    return retval
",if timeoutsec is not None :,97
"def parse_compare_fail(
    string,
    rex=re.compile(
        r""^(?P<field>min|max|mean|median|stddev|iqr):""
        r""((?P<percentage>[0-9]?[0-9])%|(?P<difference>[0-9]*\.?[0-9]+([eE][-+]?[""
        r""0-9]+)?))$""
    ),
):
    m = rex.match(string)
    if m:
        g = m.groupdict()
        if g[""percentage""]:
            return PercentageRegressionCheck(g[""field""], int(g[""percentage""]))
        elif g[""difference""]:
            return DifferenceRegressionCheck(g[""field""], float(g[""difference""]))
    raise argparse.ArgumentTypeError(""Could not parse value: %r."" % string)
","if g [ ""percentage"" ] :",199
"def get_converter(self, key, default=None):
    """"""Gets a converter for the given key.""""""
    if key in self._vars:
        return self._vars[key].convert
    # necessary for keys that match regexes, such as `*PATH`s
    for k, var in self._vars.items():
        if isinstance(k, str):
            continue
        if k.match(key) is not None:
            converter = var.convert
            self._vars[key] = var
            break
    else:
        converter = self._get_default_converter(default=default)
    return converter
",if k . match ( key ) is not None :,154
"def get_model_params(problem_type: str, hyperparameters):
    penalty = hyperparameters.get(""penalty"", L2)
    handle_text = hyperparameters.get(""handle_text"", IGNORE)
    if problem_type == REGRESSION:
        if penalty == L2:
            model_class = Ridge
        elif penalty == L1:
            model_class = Lasso
        else:
            logger.warning(
                ""Unknown value for penalty {} - supported types are [l1, l2] - falling back to l2"".format(
                    penalty
                )
            )
            penalty = L2
            model_class = Ridge
    else:
        model_class = LogisticRegression
    return model_class, penalty, handle_text
",elif penalty == L1 :,200
"def __init__(self, content=None, parent=None):
    Transformable.__init__(self, content, parent)
    self._items = []
    for element in content:
        if not element.tag.startswith(namespace):
            continue
        tag = element.tag[len(namespace) :]
        if tag == ""g"":
            item = Group(element, self)
        elif tag == ""path"":
            item = Path(element, self)
        else:
            log.warn(""Unhandled SVG tag (%s)"" % tag)
            continue
        self._items.append(item)
","if tag == ""g"" :",154
"def f_context(args: argparse.Namespace):
    choice = args.choice
    ctx = utils.get_context()
    if choice is None:
        if ctx:
            group = ctx.stem
            print(f""{group}: {' '.join(utils.get_groups()[group])}"")
        else:
            print(""Context is not set"")
    elif choice == ""none"":  # remove context
        ctx and ctx.unlink()
    else:  # set context
        fname = Path(common.get_config_dir()) / (choice + "".context"")
        if ctx:
            ctx.rename(fname)
        else:
            open(fname, ""w"").close()
",if ctx :,175
"def check_checksum(self):
    """"""fix media checksums""""""
    self.progress.set_pass(
        _(""Updating checksums on media""), len(self.db.get_media_handles())
    )
    for objectid in self.db.get_media_handles():
        self.progress.step()
        obj = self.db.get_media_from_handle(objectid)
        full_path = media_path_full(self.db, obj.get_path())
        new_checksum = create_checksum(full_path)
        if new_checksum != obj.checksum:
            logging.info(""checksum: updating "" + obj.gramps_id)
            obj.checksum = new_checksum
            self.db.commit_media(obj, self.trans)
",if new_checksum != obj . checksum :,196
"def get_default_backend(self, user_backends):
    retval = None
    n_defaults = 0
    for name in user_backends:
        args = user_backends.get(name)
        if args.get(""default"", False):
            n_defaults = n_defaults + 1
            if retval is None:
                retval = name
    return (retval, n_defaults)
",if retval is None :,100
"def on_mqtt_packet_received(self, *args, **kwargs):
    packet = kwargs.get(""packet"")
    if packet:
        packet_size = packet.bytes_length
        self._stats[STAT_BYTES_RECEIVED] += packet_size
        self._stats[STAT_MSG_RECEIVED] += 1
        if packet.fixed_header.packet_type == PUBLISH:
            self._stats[STAT_PUBLISH_RECEIVED] += 1
",if packet . fixed_header . packet_type == PUBLISH :,110
"def func(self):
    if self.schema:
        d = {}
        for key in self._schema_keys:
            d[key] = getattr(self, key)
        # arbitrary keys
        if self._data:
            akeys = set(self._data.keys()) - set(d.keys())
            for akey in akeys:
                d[akey] = self._data[akey]
        return d
    else:
        return None
",if self . _data :,125
"def endElement(self, name, value, connection):
    if name == ""vpcId"":
        self.vpc_id = value
    elif name == ""value"":
        if value == ""true"":
            value = True
        else:
            value = False
        if self._current_attr == ""enableDnsHostnames"":
            self.enable_dns_hostnames = value
        elif self._current_attr == ""enableDnsSupport"":
            self.enable_dns_support = value
","if self . _current_attr == ""enableDnsHostnames"" :",125
"def keyPressEvent(self, event):
    if event.key() in (Qt.Key_Right, Qt.Key_Left):
        direction = 1
        if event.key() == Qt.Key_Left:
            direction = -1
        if event.modifiers() == Qt.ShiftModifier:
            print(""shift"")
            direction *= 10
        self.timeline.setValue(self.timeline.value() + direction)
    else:
        super(VideoPlayerWidget, self).keyPressEvent(event)
",if event . key ( ) == Qt . Key_Left :,131
"def find_config(pipeline_config_path: Union[str, Path]) -> Path:
    if not Path(pipeline_config_path).is_file():
        configs = [
            c
            for c in Path(__file__).parent.parent.parent.glob(
                f""configs/**/{pipeline_config_path}.json""
            )
            if str(c.with_suffix("""")).endswith(pipeline_config_path)
        ]  # a simple way to not allow * and ?
        if configs:
            log.info(f""Interpreting '{pipeline_config_path}' as '{configs[0]}'"")
            pipeline_config_path = configs[0]
    return Path(pipeline_config_path)
",if configs :,184
"def list_translations(dirname):
    if not os.path.isdir(dirname):
        return []
    result = []
    for entry in scandir(dirname):
        locale_dir = os.path.join(entry.path, ""LC_MESSAGES"")
        if not os.path.isdir(locale_dir):
            continue
        if any(filter(lambda x: x.name.endswith("".mo""), scandir(locale_dir))):
            result.append(Locale.parse(entry.name))
    return result
","if any ( filter ( lambda x : x . name . endswith ( "".mo"" ) , scandir ( locale_dir ) ) ) :",131
"def writeTo(self, writable):
    chunkStart = 0
    fileSize = blob.properties.content_length
    while chunkStart < fileSize:
        chunkEnd = chunkStart + outer_self._maxAzureBlockBytes - 1
        buf = container.get_blob_to_bytes(
            blob_name=str(jobStoreFileID), start_range=chunkStart, end_range=chunkEnd
        ).content
        if encrypted:
            buf = encryption.decrypt(buf, outer_self.keyPath)
        writable.write(buf)
        chunkStart = chunkEnd + 1
",if encrypted :,148
"def get_extractor(name):
    for extractor in ALL_EXTRACTORS:
        if extractor[""regex""] in name.lower():
            module = import_module(
                ""anime_downloader.extractors.{}"".format(extractor[""modulename""])
            )
            return getattr(module, extractor[""class""])
","if extractor [ ""regex"" ] in name . lower ( ) :",84
"def updateSize(self):
    if self.size is not None:
        return
    height = 0
    width = 0
    for row in range(self.layout.rowCount()):
        row_height = 0
        col_witdh = 0
        for col in range(self.layout.columnCount()):
            item = self.layout.itemAt(row, col)
            if item:
                col_witdh += item.width() + 3
                row_height = max(row_height, item.height())
        width = max(width, col_witdh)
        height += row_height
    self.setGeometry(0, 0, width, height)
    return
",if item :,176
"def close_group(self):
    """"""Closes a grouping for previous filters""""""
    if self._filters:
        if len(self._open_group_flag) < (len(self._close_group_flag) + 1):
            raise RuntimeError(""Not enough open groups to close."")
        if isinstance(self._filters[-1], ChainOperator):
            flt_sentence = self._filters[-2]
        else:
            flt_sentence = self._filters[-1]
        flt_sentence[1] = flt_sentence[1] + "")""  # closing the group
        self._close_group_flag.append(False)  # flag a close group was added
    else:
        raise RuntimeError(""No filters present. Can't close a group"")
    return self
",if len ( self . _open_group_flag ) < ( len ( self . _close_group_flag ) + 1 ) :,191
"def test_name_conflicts():
    # Test that we handle participants having the same name correctly.
    ev = fake_event()
    ev2 = fake_event()
    # Office365 sets the name to the email address when it's
    # not available.
    ev2.participants[0][""email""] = None
    ev2.participants[0][""status""] = ""yes""
    merged_participants = ev._partial_participants_merge(ev2)
    assert len(merged_participants) == 2
    for participant in merged_participants:
        if participant[""email""] is None:
            assert participant[""status""] == ""yes""
        else:
            assert participant[""name""] == ""Ronald Zubar""
","if participant [ ""email"" ] is None :",183
"def set_idle(view, idle):
    vid = view.id()
    current_idle = vid in State[""idle_views""]
    if idle != current_idle:
        if idle:
            State[""idle_views""].add(vid)
        else:
            State[""idle_views""].discard(vid)
        toggle_demoted_regions(view, idle)
",if idle :,95
"def _deserialize(self, value, attr, data, **kwargs):
    if isinstance(value, str):
        return [value, 0, 0]
    if isinstance(value, list) and len(value) == 3:
        condition = (
            isinstance(value[0], str)
            and isinstance(value[1], int)
            and isinstance(value[1], int)
        )
        if condition:
            return value
    raise ValidationError(""This field expects a str or a list of [str, int, int]."")
",if condition :,134
"def _struct(self, fields):
    result = {}
    for field in fields:
        if field[0] == ""__parent"":
            parent = self.instance(field[1])
            if isinstance(parent, dict):
                result.update(parent)
            elif len(fields) == 1:
                result = parent
            else:
                result[field[0]] = parent
        else:
            result[field[0]] = self.instance(field[1])
    return result
",elif len ( fields ) == 1 :,136
"def validate(self):
    if ""accounts"" in self.data and self.data[""accounts""] == ""matched"":
        found = False
        for f in self.manager.iter_filters():
            if isinstance(f, AmiCrossAccountFilter):
                found = True
                break
        if not found:
            raise PolicyValidationError(
                ""policy:%s filter:%s with matched requires cross-account filter""
                % (self.manager.ctx.policy.name, self.type)
            )
",if not found :,137
"def add_rule6(self, rule):
    if self.cleared:
        return
    self._lock.acquire()
    try:
        self._other6.append(rule)
        if not self._exists_iptables_rule(rule, ipv6=True):
            self._insert_iptables_rule(rule, ipv6=True)
    finally:
        self._lock.release()
","if not self . _exists_iptables_rule ( rule , ipv6 = True ) :",100
"def load_grammar(self, *args):
    ""Load a grammar from a pickle file""
    filename = askopenfilename(
        filetypes=self.GRAMMAR_FILE_TYPES, defaultextension="".cfg""
    )
    if not filename:
        return
    try:
        if filename.endswith("".pickle""):
            with open(filename, ""rb"") as infile:
                grammar = pickle.load(infile)
        else:
            with open(filename, ""r"") as infile:
                grammar = CFG.fromstring(infile.read())
        self.set_grammar(grammar)
    except Exception as e:
        tkinter.messagebox.showerror(
            ""Error Loading Grammar"", ""Unable to open file: %r"" % filename
        )
","if filename . endswith ( "".pickle"" ) :",186
"def _join_printed_types(self, types):
    """"""Pretty-print the union of the printed types.""""""
    types = sorted(set(types))  # dedup
    if len(types) == 1:
        return next(iter(types))
    elif types:
        if ""None"" in types:
            types.remove(""None"")
            return ""Optional[%s]"" % self._join_printed_types(types)
        else:
            return ""Union[%s]"" % "", "".join(types)
    else:
        return ""nothing""
","if ""None"" in types :",138
"def __init__(self, **kwargs):
    for key, val in kwargs.items():
        field = getattr(self.__class__, key, None)
        if field is None:
            raise TypeError(
                ""Field %r returned from raw SQL query does not have ""
                ""a column defined in the model"" % key
            )
        setattr(self, field.get_attname() or key, field.to_python(val))
",if field is None :,113
"def get_transaction_execution_results(self, batch_signature):
    with self._condition:
        batch_status = self._batch_statuses.get(batch_signature)
        if batch_status is None:
            return None
        annotated_batch = self._batch_by_id.get(batch_signature)
        if annotated_batch is None:
            return None
        results = []
        for txn in annotated_batch.batch.transactions:
            result = self._txn_results.get(txn.header_signature)
            if result is not None:
                results.append(result)
        return results
",if batch_status is None :,161
"def _check_params(self) -> None:
    if self.augmentation and self.ratio <= 0:
        raise ValueError(""The augmentation ratio must be positive."")
    if self.clip_values is not None:
        if len(self.clip_values) != 2:
            raise ValueError(
                ""`clip_values` should be a tuple of 2 floats or arrays containing the allowed data range.""
            )
        if np.array(self.clip_values[0] >= self.clip_values[1]).any():
            raise ValueError(""Invalid `clip_values`: min >= max."")
",if len ( self . clip_values ) != 2 :,146
"def ping_all():
    for l in _all_listeners.values():
        count = l.receiver.count()
        if count:
            for dev in l.receiver:
                dev.ping()
                l._status_changed(dev)
                count -= 1
                if not count:
                    break
",if not count :,92
"def on_btOK_clicked(self, *a):
    """"""Handler for OK button""""""
    if self.ac_callback is not None:
        self._set_title()
        if self._mode == ActionEditor.AEC_MENUITEM:
            self.ac_callback(self.id, self)
        else:
            a = self.generate_modifiers(
                self._action, self._selected_component.NAME == ""custom""
            )
            self.ac_callback(self.id, a)
            self.ac_callback = None
        if self._selected_component:
            self._selected_component.on_ok(a)
    self.close()
",if self . _selected_component :,180
"def apply_ssl(self, request):
    if self.ssl_protocol:
        if self.sslconf.protocol() != self.ssl_protocol:
            self.sslconf.setProtocol(self.ssl_protocol)
            QSslConfiguration.setDefaultConfiguration(self.sslconf)
    request.setSslConfiguration(self.sslconf)
    return request
",if self . sslconf . protocol ( ) != self . ssl_protocol :,91
"def _iter_process_args(mapping, pid, max_depth):
    """"""Iterator to traverse up the tree, yielding each process's argument list.""""""
    for _ in range(max_depth):
        try:
            proc = mapping[pid]
        except KeyError:  # We've reached the root process. Give up.
            break
        if proc.args:  # Persumably the process should always have a name?
            yield proc.args
        pid = proc.ppid  # Go up one level.
",if proc . args :,127
"def store_data(self, store_loc, **kwargs):
    """"""Put arrays to store""""""
    # print(store_loc)
    g = self.store.create_group(store_loc)
    for (
        k,
        v,
    ) in kwargs.items():
        # print(type(v[0]))
        # print(k)
        if type(v) == list:
            if len(v) != 0:
                if type(v[0]) is np.str_ or type(v[0]) is str:
                    v = [a.encode(""utf8"") for a in v]
        g.create_dataset(k, data=v, compression=self.clib, compression_opts=self.clev)
",if len ( v ) != 0 :,191
"def add_system_info_creds_to_config(creds):
    for user in creds:
        ConfigService.creds_add_username(creds[user][""username""])
        if ""password"" in creds[user] and creds[user][""password""]:
            ConfigService.creds_add_password(creds[user][""password""])
        if ""lm_hash"" in creds[user] and creds[user][""lm_hash""]:
            ConfigService.creds_add_lm_hash(creds[user][""lm_hash""])
        if ""ntlm_hash"" in creds[user] and creds[user][""ntlm_hash""]:
            ConfigService.creds_add_ntlm_hash(creds[user][""ntlm_hash""])
","if ""ntlm_hash"" in creds [ user ] and creds [ user ] [ ""ntlm_hash"" ] :",175
"def _format_arg(self, name, spec, value):
    if name == ""title"":
        if isinstance(value, bool) and value:
            return ""--title""
        elif isinstance(value, str):
            return ""--title --title_text %s"" % (value,)
        else:
            raise ValueError('Unknown value for ""title"" argument: ' + str(value))
    return super(Pik, self)._format_arg(name, spec, value)
","if isinstance ( value , bool ) and value :",118
"def handle_friend(self):
    tokens, last = self._get_var_tokens_up_to(False, ""("", "";"")
    if last.name == ""("":
        tokens.append(last)
        self._add_back_tokens(tokens)
        token = self._get_next_token()
        while token.name in (""inline"", ""typename"", ""::""):
            token = self._get_next_token()
        result = self._generate_one(token)
    else:
        if tokens[0].name == ""class"":
            tokens = tokens[1:]
        result = self.converter.to_type(tokens)[0]
    assert result
    return Friend(result.start, result.end, result, self.namespace_stack)
","if tokens [ 0 ] . name == ""class"" :",188
"def list_subtitles(self, video, languages):
    season = None
    episodes = []
    if isinstance(video, Episode):
        titles = [video.series] + video.alternative_series
        season = video.season
        episodes = video.episodes
    else:
        titles = [video.title] + video.alternative_titles
    for title in titles:
        subtitles = [
            s
            for l in languages
            for s in self.query(
                l, title, season=season, episodes=episodes, year=video.year
            )
        ]
        if subtitles:
            return subtitles
    return []
",if subtitles :,183
"def on_write_needed(self, nbytes, underflow):
    if underflow:
        self._handle_underflow()
    else:
        self._write_to_stream(nbytes)
    # Asynchronously update time
    if self._events:
        if self._time_sync_operation is not None and self._time_sync_operation.is_done:
            self._time_sync_operation.delete()
            self._time_sync_operation = None
        if self._time_sync_operation is None:
            assert _debug(""PulseAudioPlayer: trigger timing info update"")
            self._time_sync_operation = self.stream.update_timing_info(
                self._process_events
            )
",if self . _time_sync_operation is not None and self . _time_sync_operation . is_done :,187
"def _set_account_info(self):
    with session_scope(self.account_id) as db_session:
        account = db_session.query(ImapAccount).get(self.account_id)
        self.sync_state = account.sync_state
        self.provider = account.provider
        self.provider_info = account.provider_info
        self.email_address = account.email_address
        self.auth_handler = account.auth_handler
        if account.provider == ""gmail"":
            self.client_cls = GmailCrispinClient
        else:
            self.client_cls = CrispinClient
","if account . provider == ""gmail"" :",165
"def make_timesheet_records():
    employees = get_timesheet_based_salary_slip_employee()
    for e in employees:
        ts = make_timesheet(
            e.employee,
            simulate=True,
            billable=1,
            activity_type=get_random(""Activity Type""),
            company=frappe.flags.company,
        )
        frappe.db.commit()
        rand = random.random()
        if rand >= 0.3:
            make_salary_slip_for_timesheet(ts.name)
        rand = random.random()
        if rand >= 0.2:
            make_sales_invoice_for_timesheet(ts.name)
",if rand >= 0.3 :,197
"def free(self, addr, ban=0):
    with self.lock:
        if ban != 0:
            self.ban.append({""addr"": addr, ""counter"": ban})
        else:
            base, bit, is_allocated = self.locate(addr)
            if len(self.addr_map) <= base:
                raise KeyError(""address is not allocated"")
            if self.addr_map[base] & (1 << bit):
                raise KeyError(""address is not allocated"")
            self.allocated -= 1
            self.addr_map[base] ^= 1 << bit
",if ban != 0 :,155
"def flush_log(self):
    try:
        while len(self.log_buffer) > 0:
            level, message = self.log_buffer.pop(0)
            if level <= self.log_level:
                self._display_log(message, level)
    except IndexError:
        pass
",if level <= self . log_level :,82
"def check(self):
    global MySQLdb
    import MySQLdb
    try:
        args = {}
        if mysql_user:
            args[""user""] = mysql_user
        if mysql_pwd:
            args[""passwd""] = mysql_pwd
        if mysql_host:
            args[""host""] = mysql_host
        if mysql_port:
            args[""port""] = mysql_port
        if mysql_socket:
            args[""unix_socket""] = mysql_socket
        self.db = MySQLdb.connect(**args)
    except Exception as e:
        raise Exception(""Cannot interface with MySQL server: %s"" % e)
",if mysql_port :,167
"def get_middleware_resolvers(middlewares):
    for middleware in middlewares:
        # If the middleware is a function instead of a class
        if inspect.isfunction(middleware):
            yield middleware
        if not hasattr(middleware, MIDDLEWARE_RESOLVER_FUNCTION):
            continue
        yield getattr(middleware, MIDDLEWARE_RESOLVER_FUNCTION)
",if inspect . isfunction ( middleware ) :,93
"def get_sentence(self):
    while True:
        self._seed += 1
        all_files = list(self._all_files)
        if self._shuffle:
            if self._n_gpus > 1:
                random.seed(self._seed)
            random.shuffle(all_files)
        for file_path in all_files:
            for ret in self._load_file(file_path):
                yield ret
        if self._mode == ""test"":
            break
",if self . _shuffle :,134
"def extract_cookies(self, response, request):
    """"""Extract cookies from response, where allowable given the request.""""""
    _debug(""extract_cookies: %s"", response.info())
    self._cookies_lock.acquire()
    try:
        self._policy._now = self._now = int(time.time())
        for cookie in self.make_cookies(response, request):
            if self._policy.set_ok(cookie, request):
                _debug("" setting cookie: %s"", cookie)
                self.set_cookie(cookie)
    finally:
        self._cookies_lock.release()
","if self . _policy . set_ok ( cookie , request ) :",152
"def _gen_filename(self, name):
    if name == ""in_average"":
        avg_subject = str(self.inputs.hemisphere) + "".EC_average""
        avg_directory = os.path.join(self.inputs.subjects_dir, avg_subject)
        if not os.path.isdir(avg_directory):
            fs_home = os.path.abspath(os.environ.get(""FREESURFER_HOME""))
        return avg_subject
    elif name == ""out_file"":
        return self._list_outputs()[name]
    else:
        return None
",if not os . path . isdir ( avg_directory ) :,150
"def decorated_view(*args, **kwargs):
    h = {}
    mechanisms = [(method, login_mechanisms.get(method)) for method in auth_methods]
    for method, mechanism in mechanisms:
        if mechanism and mechanism():
            return fn(*args, **kwargs)
        elif method == ""basic"":
            r = _security.default_http_auth_realm
            h[""WWW-Authenticate""] = 'Basic realm=""%s""' % r
    if _security._unauthorized_callback:
        return _security._unauthorized_callback()
    else:
        return _get_unauthorized_response(headers=h)
","elif method == ""basic"" :",158
"def _iterate_files(self, files, root, include_checksums, relpath):
    file_list = {}
    for file in files:
        exclude = False
        # exclude defined filename patterns
        for pattern in S3Sync.exclude_files:
            if fnmatch.fnmatch(file, pattern):
                exclude = True
                break
        if not exclude:
            full_path = root + ""/"" + file
            if include_checksums:
                # get checksum
                checksum = self._hash_file(full_path)
            else:
                checksum = """"
            file_list[relpath + file] = [full_path, checksum]
    return file_list
",if not exclude :,184
"def attr(**kw):
    kw = kw.items()
    kw.sort()
    parts = []
    for name, value in kw:
        if value is None:
            continue
        if name.endswith(""_""):
            name = name[:-1]
        parts.append('%s=""%s""' % (html_quote(name), html_quote(value)))
    return html("" "".join(parts))
","if name . endswith ( ""_"" ) :",100
"def create(self):
    if not self.created:
        self.created = True
        cmd = self._mode
        if cmd == MODE_ALL:
            cmd = u""""
        vim.command(
            (u"":%snoremap %s %s"" % (cmd, str(self), self.command)).encode(u""utf-8"")
        )
",if cmd == MODE_ALL :,97
"def get_tokens_unprocessed(self, text):
    for index, token, value in RegexLexer.get_tokens_unprocessed(self, text):
        if token is Name:
            if self.stdlibhighlighting and value in self.stdlib_types:
                token = Keyword.Type
            elif self.c99highlighting and value in self.c99_types:
                token = Keyword.Type
            elif self.platformhighlighting and value in self.linux_types:
                token = Keyword.Type
        yield index, token, value
",if token is Name :,141
"def _merge_colormaps(kwargs):
    """"""Merge colormaps listed in kwargs.""""""
    from trollimage.colormap import Colormap
    full_cmap = None
    palette = kwargs[""palettes""]
    if isinstance(palette, Colormap):
        full_cmap = palette
    else:
        for itm in palette:
            cmap = create_colormap(itm)
            cmap.set_range(itm[""min_value""], itm[""max_value""])
            if full_cmap is None:
                full_cmap = cmap
            else:
                full_cmap = full_cmap + cmap
    return full_cmap
",if full_cmap is None :,156
"def from_text(cls, rdclass, rdtype, tok, origin=None, relativize=True):
    key_tag = tok.get_uint16()
    algorithm = tok.get_uint8()
    digest_type = tok.get_uint8()
    chunks = []
    while 1:
        t = tok.get().unescape()
        if t.is_eol_or_eof():
            break
        if not t.is_identifier():
            raise dns.exception.SyntaxError
        chunks.append(t.value)
    digest = """".join(chunks)
    digest = digest.decode(""hex_codec"")
    return cls(rdclass, rdtype, key_tag, algorithm, digest_type, digest)
",if not t . is_identifier ( ) :,180
"def connect_reader_to_writer(reader, writer):
    BUF_SIZE = 8192
    try:
        while True:
            data = await reader.read(BUF_SIZE)
            if not data:
                if not writer.transport.is_closing():
                    writer.write_eof()
                    await writer.drain()
                return
            writer.write(data)
            await writer.drain()
    except (OSError, asyncio.IncompleteReadError) as e:
        pass
",if not data :,139
"def _get_cuda_device(*args):
    # Returns cuda.Device or DummyDevice.
    for arg in args:
        if type(arg) is not bool and isinstance(arg, _integer_types):
            check_cuda_available()
            return Device(arg)
        if isinstance(arg, ndarray):
            if arg.device is None:
                continue
            return arg.device
        if available and isinstance(arg, Device):
            return arg
    # NOTE: This function returns DummyDevice for both NumPy and ChainerX
    return DummyDevice
","if available and isinstance ( arg , Device ) :",144
"def skip_to_semicolon(s, i):
    n = len(s)
    while i < n:
        c = s[i]
        if c == "";"":
            return i
        elif c == ""'"" or c == '""':
            i = g.skip_string(s, i)
        elif g.match(s, i, ""//""):
            i = g.skip_to_end_of_line(s, i)
        elif g.match(s, i, ""/*""):
            i = g.skip_block_comment(s, i)
        else:
            i += 1
    return i
","elif c == ""'"" or c == '""' :",161
"def build_CallFunc(self, o):
    children = o.getChildren()
    # Build callee from first child
    callee = self.build(children[0])
    # Build args and kwargs from remaining children
    args = []
    kwargs = {}
    for child in children[1:]:
        class_name = child.__class__.__name__
        # None is ignored
        if class_name == ""NoneType"":
            continue
        # Keywords become kwargs
        if class_name == ""Keyword"":
            kwargs.update(self.build(child))
        # Everything else becomes args
        else:
            args.append(self.build(child))
    return callee(*args, **kwargs)
","if class_name == ""Keyword"" :",175
"def _extract_constant_functions(slither: SlitherCore) -> Dict[str, List[str]]:
    ret: Dict[str, List[str]] = {}
    for contract in slither.contracts:
        cst_functions = [
            _get_name(f) for f in contract.functions_entry_points if _is_constant(f)
        ]
        cst_functions += [
            v.function_name
            for v in contract.state_variables
            if v.visibility in [""public""]
        ]
        if cst_functions:
            ret[contract.name] = cst_functions
    return ret
","if v . visibility in [ ""public"" ]",166
"def acquire_read_lock(self, wait=True):
    state = self.state
    if state.writing:
        raise LockError(""lock is in writing state"")
    if state.reentrantcount == 0:
        x = self.do_acquire_read_lock(wait)
        if wait or x:
            state.reentrantcount += 1
            state.reading = True
        return x
    elif state.reading:
        state.reentrantcount += 1
        return True
",if wait or x :,124
"def get_optional_nargs(self, name):
    for n, kwargs in self.conf[""optional_args""]:
        if name == n:
            if ""action"" in kwargs:
                action = kwargs[""action""]
                if action in (""store_true"", ""store_false""):
                    return 0
            break
    return 1
",if name == n :,92
"def _requests_to_follow(self, response):
    if not isinstance(response, HtmlResponse):
        return
    seen = set()
    for n, rule in enumerate(self._rules):
        links = [
            lnk
            for lnk in rule.link_extractor.extract_links(response)
            if lnk not in seen
        ]
        if links and rule.process_links:
            links = rule.process_links(links)
        for link in links:
            seen.add(link)
            request = self._build_request(n, link)
            yield rule._process_request(request, response)
",if lnk not in seen,168
"def process_module(name, module, parent):
    if parent:
        modules[parent][""items""].append(name)
        mg = module_groups.setdefault(name, [])
        mg.append(parent)
        if get_module_type(name) == ""py3status"":
            module["".group""] = parent
    # check module content
    for k, v in list(module.items()):
        if k.startswith(""on_click""):
            # on_click event
            process_onclick(k, v, name)
            # on_click should not be passed to the module via the config.
            del module[k]
        if isinstance(v, ModuleDefinition):
            # we are a container
            module[""items""] = []
    return module
","if k . startswith ( ""on_click"" ) :",198
"def _mysql_version_validator(version, sku_info, tier):
    if version:
        versions = get_mysql_versions(sku_info, tier)
        if version not in versions:
            raise CLIError(
                ""Incorrect value for --version. Allowed values : {}"".format(versions)
            )
",if version not in versions :,83
"def do_blocking_test(self, block_func, block_args, trigger_func, trigger_args):
    thread = _TriggerThread(trigger_func, trigger_args)
    thread.start()
    try:
        self.result = block_func(*block_args)
        # If block_func returned before our thread made the call, we failed!
        if not thread.startedEvent.is_set():
            self.fail(""blocking function '%r' appeared not to block"" % block_func)
        return self.result
    finally:
        thread.join(10)  # make sure the thread terminates
        if thread.is_alive():
            self.fail(""trigger function '%r' appeared to not return"" % trigger_func)
",if not thread . startedEvent . is_set ( ) :,183
"def _fatal_error(self, exc, message=""Fatal error on pipe transport""):
    try:
        if isinstance(exc, OSError):
            if self._loop.get_debug():
                logger.debug(""%r: %s"", self, message, exc_info=True)
        else:
            self._loop.call_exception_handler(
                {
                    ""message"": message,
                    ""exception"": exc,
                    ""transport"": self,
                    ""protocol"": self._protocol,
                }
            )
    finally:
        self._force_close(exc)
","if isinstance ( exc , OSError ) :",167
"def run_test_family(tests, mode_filter, files, open_func, *make_args):
    for test_func in tests:
        if test_func is None:
            out.write(""\n"")
            continue
        if mode_filter in test_func.file_open_mode:
            continue
        for s in test_func.file_sizes:
            name, size = files[size_names[s]]
            # name += file_ext
            args = tuple(f(name, size) for f in make_args)
            run_one_test(name, size, open_func, test_func, *args)
",if test_func is None :,168
"def py__get__(self, obj):
    # Arguments in __get__ descriptors are obj, class.
    # `method` is the new parent of the array, don't know if that's good.
    names = self.get_function_slot_names(""__get__"")
    if names:
        if isinstance(obj, AbstractInstanceContext):
            return self.execute_function_slots(names, obj, obj.class_context)
        else:
            none_obj = compiled.create(self.evaluator, None)
            return self.execute_function_slots(names, none_obj, obj)
    else:
        return ContextSet(self)
","if isinstance ( obj , AbstractInstanceContext ) :",159
"def _options_fcheck(self, name, xflags, table):
    for entry in table:
        if entry.name is None:
            break
        if entry.flags & XTOPT_MAND and not xflags & (1 << entry.id):
            raise XTablesError(""%s: --%s must be specified"" % (name, entry.name))
            if not xflags & (1 << entry.id):
                continue
",if entry . flags & XTOPT_MAND and not xflags & ( 1 << entry . id ) :,112
"def _consumer_healthy(self):
    abnormal_num = 0
    for w in self._consumers:
        if not w.is_alive() and w.id not in self._consumer_endsig:
            abnormal_num += 1
            if self._use_process:
                errmsg = ""consumer[{}] exit abnormally with exitcode[{}]"".format(
                    w.pid, w.exitcode
                )
            else:
                errmsg = ""consumer[{}] exit abnormally"".format(w.ident)
            logger.warn(errmsg)
    if abnormal_num > 0:
        logger.warn(""{} consumers have exited abnormally!!!"".format(abnormal_num))
    return abnormal_num == 0
",if self . _use_process :,186
"def extract_groups(self, text: str, language_code: str):
    previous = None
    group = 1
    groups = []
    words = []
    ignored = IGNORES.get(language_code, {})
    for word in NON_WORD.split(text):
        if not word:
            continue
        if word not in ignored and len(word) >= 2:
            if previous == word:
                group += 1
            elif group > 1:
                groups.append(group)
                words.append(previous)
                group = 1
        previous = word
    if group > 1:
        groups.append(group)
        words.append(previous)
    return groups, words
",if not word :,187
"def _validate_callbacks(cls, callbacks):
    for callback in callbacks:
        if not isinstance(callback, Callback):
            if issubclass(callback, Callback):
                raise TypeError(""Make sure to instantiate the callbacks."")
            raise TypeError(""Only accepts a `callbacks` instance."")
","if issubclass ( callback , Callback ) :",70
"def convert_errors(from_, to, msg=None):
    exc = None
    try:
        yield None
    except from_ as e:
        exc = e
    if exc:
        info = ""%s: %s"" % (exc.__class__.__name__, str(exc))
        if msg:
            info = ""%s: %s"" % (msg, info)
        raise to(info)
",if msg :,102
"def delete_loan(loan_key, loan=None):
    if not loan:
        loan = web.ctx.site.store.get(loan_key)
        if not loan:
            raise Exception(""Could not find store record for %s"", loan_key)
    loan.delete()
",if not loan :,81
"def last_action_for(self, agent_id: AgentID = _DUMMY_AGENT_ID) -> EnvActionType:
    """"""Returns the last action for the specified agent, or zeros.""""""
    if agent_id in self._agent_to_last_action:
        return flatten_to_single_ndarray(self._agent_to_last_action[agent_id])
    else:
        policy = self._policies[self.policy_for(agent_id)]
        flat = flatten_to_single_ndarray(policy.action_space.sample())
        if hasattr(policy.action_space, ""dtype""):
            return np.zeros_like(flat, dtype=policy.action_space.dtype)
        return np.zeros_like(flat)
","if hasattr ( policy . action_space , ""dtype"" ) :",185
"def on_leave(
    self, original_node: CSTNodeT, updated_node: CSTNodeT
) -> Union[cst.Import, cst.ImportFrom, CSTNodeT, RemovalSentinel]:
    if isinstance(updated_node, cst.Import):
        for alias in updated_node.names:
            name = alias.name
            if isinstance(name, cst.Name) and name.value == ""b"":
                return cst.RemoveFromParent()
    elif isinstance(updated_node, cst.ImportFrom):
        module = updated_node.module
        if isinstance(module, cst.Name) and module.value == ""e"":
            return cst.RemoveFromParent()
    return updated_node
","if isinstance ( module , cst . Name ) and module . value == ""e"" :",183
"def sortkey(self, r, prog=None):
    ret = []
    for col, reverse in self._ordering:
        if isinstance(col, str):
            col = self.column(col)
        val = col.getTypedValue(r)
        ret.append(Reversor(val) if reverse else val)
    if prog:
        prog.addProgress(1)
    return ret
","if isinstance ( col , str ) :",102
"def down_button_clicked(self, obj):
    ref = self.get_selected()
    if ref and ref[1] is not None:
        pos = self.find_index(ref)
        if pos[1] >= 0 and pos[1] < len(self.get_data()[pos[0]]) - 1:
            self._move_down(pos, ref[1])
    elif ref and ref[1] is None:
        self._move_down_group(ref[0])
",if pos [ 1 ] >= 0 and pos [ 1 ] < len ( self . get_data ( ) [ pos [ 0 ] ] ) - 1 :,122
"def maybe_swap_for_shadow_path(self, path: str) -> str:
    if not self.shadow_map:
        return path
    path = normpath(path, self.options)
    previously_checked = path in self.shadow_equivalence_map
    if not previously_checked:
        for source, shadow in self.shadow_map.items():
            if self.fscache.samefile(path, source):
                self.shadow_equivalence_map[path] = shadow
                break
            else:
                self.shadow_equivalence_map[path] = None
    shadow_file = self.shadow_equivalence_map.get(path)
    return shadow_file if shadow_file else path
","if self . fscache . samefile ( path , source ) :",179
"def _add_kid(key, x):
    if x is None:
        kids[key] = None
    else:
        if type(x) in (type([]), type(())):
            x1 = [i for i in x if isinstance(i, TVTKBase)]
            if x1:
                kids[key] = x1
        elif isinstance(x, TVTKBase):
            if hasattr(x, ""__iter__""):
                # Don't add iterable objects that contain non
                # acceptable nodes
                if len(list(x)) and isinstance(list(x)[0], TVTKBase):
                    kids[key] = x
            else:
                kids[key] = x
","elif isinstance ( x , TVTKBase ) :",196
"def find_zone_id(domain, client=None):
    paginator = client.get_paginator(""list_hosted_zones"")
    zones = []
    for page in paginator.paginate():
        for zone in page[""HostedZones""]:
            if domain.endswith(zone[""Name""]) or (domain + ""."").endswith(zone[""Name""]):
                if not zone[""Config""][""PrivateZone""]:
                    zones.append((zone[""Name""], zone[""Id""]))
    if not zones:
        raise ValueError(""Unable to find a Route53 hosted zone for {}"".format(domain))
    return zones[0][1]
","if domain . endswith ( zone [ ""Name"" ] ) or ( domain + ""."" ) . endswith ( zone [ ""Name"" ] ) :",147
"def render(self, context):
    for condition, nodelist in self.conditions_nodelists:
        if condition is not None:  # if / elif clause
            try:
                match = condition.eval(context)
            except VariableDoesNotExist:
                match = None
        else:  # else clause
            match = True
        if match:
            return nodelist.render(context)
    return """"
",if match :,109
"def init_weight(self):
    if self.pretrained is not None:
        load_entire_model(self, self.pretrained)
    else:
        for sublayer in self.sublayers():
            if isinstance(sublayer, nn.Conv2D):
                kaiming_normal_init(sublayer.weight)
            elif isinstance(sublayer, (nn.BatchNorm, nn.SyncBatchNorm)):
                kaiming_normal_init(sublayer.weight)
","if isinstance ( sublayer , nn . Conv2D ) :",120
"def _next_empty_row(view, pt):
    r = utils.row_at(view, pt)
    while True:
        r += 1
        pt = view.text_point(r, 0)
        if utils.row_at(view, pt) == utils.last_row(view):
            return view.size(), True
        if view.line(pt).empty():
            return pt, False
",if view . line ( pt ) . empty ( ) :,106
"def __init__(self, parent, name, max_size=None, description=None):
    Field.__init__(self, parent, name, size=0, description=description)
    value = 0
    addr = self.absolute_address
    while max_size is None or self._size < max_size:
        byte = parent.stream.readBits(addr, 8, LITTLE_ENDIAN)
        value += byte
        self._size += 8
        if byte != 0xFF:
            break
        addr += 8
    self.createValue = lambda: value
",if byte != 0xFF :,140
"def xdir(obj, return_values=False):
    for attr in dir(obj):
        if attr[:2] != ""__"" and attr[-2:] != ""__"":
            if return_values:
                yield attr, getattr(obj, attr)
            else:
                yield attr
",if return_values :,76
"def _extract_changes(doc_map, changes, read_time):
    deletes = []
    adds = []
    updates = []
    for name, value in changes.items():
        if value == ChangeType.REMOVED:
            if name in doc_map:
                deletes.append(name)
        elif name in doc_map:
            if read_time is not None:
                value.read_time = read_time
            updates.append(value)
        else:
            if read_time is not None:
                value.read_time = read_time
            adds.append(value)
    return (deletes, adds, updates)
",elif name in doc_map :,173
"def endElement(self, name):
    if self._is_active is True:
        if name == ""record"" and self._tag_level == self._level:
            self._is_active = False
            self._tag_level = None
            if _callable(self._callback):
                self._callback(self._record)
            self._record = None
        elif self._level == self._tag_level + 1:
            if name != ""xref"":
                self._record[name] = """".join(self._tag_payload)
                self._tag_payload = None
                self._tag_feeding = False
    self._level -= 1
","if name == ""record"" and self . _tag_level == self . _level :",175
"def init_worker(
    status_queue: multiprocessing.SimpleQueue,
    param_queue: multiprocessing.SimpleQueue,
    result_queue: multiprocessing.SimpleQueue,
) -> None:
    global result
    global coverage_run
    # Make sure the generator is re-seeded, as we have inherited
    # the seed from the parent process.
    random.seed()
    result = ChannelingTestResult(result_queue)
    if not param_queue.empty():
        server_addr = param_queue.get()
        if server_addr is not None:
            os.environ[""EDGEDB_TEST_CLUSTER_ADDR""] = json.dumps(server_addr)
    coverage_run = devmode.CoverageConfig.start_coverage_if_requested()
    status_queue.put(True)
",if server_addr is not None :,190
"def wait(uuid: str, kind: str, max_retries: int):
    """"""Delete an s3 subpath.""""""
    from polyaxon import settings
    from polyaxon.agents.spawners.spawner import Spawner
    spawner = Spawner(namespace=settings.CLIENT_CONFIG.namespace, in_cluster=True)
    retry = 1
    while retry < max_retries:
        try:
            k8s_operation = spawner.get(run_uuid=uuid, run_kind=kind)
        except:  # noqa
            k8s_operation = None
        if k8s_operation:
            retry += 1
            time.sleep(retry)
        else:
            return
    sys.exit(1)
",if k8s_operation :,187
"def _get_data_fields():
    global supported_kinds
    ret = []
    for data in supported_kinds:
        msg = ifinfmsg.ifinfo.data_map.get(data)
        if msg is not None:
            if getattr(msg, ""prefix"", None) is not None:
                ret += [msg.nla2name(i[0]) for i in msg.nla_map]
            else:
                ret += [ifinfmsg.nla2name(i[0]) for i in msg.nla_map]
    return ret
","if getattr ( msg , ""prefix"" , None ) is not None :",146
"def loop_check(self):
    in_loop = []
    # Add the tag for dfs check
    for node in self.nodes:
        node.dfs_loop_status = ""DFS_UNCHECKED""
    # Now do the job
    for node in self.nodes:
        # Run the dfs only if the node has not been already done */
        if node.dfs_loop_status == ""DFS_UNCHECKED"":
            self.dfs_loop_search(node)
        # If LOOP_INSIDE, must be returned
        if node.dfs_loop_status == ""DFS_LOOP_INSIDE"":
            in_loop.append(node)
    # Remove the tag
    for node in self.nodes:
        del node.dfs_loop_status
    return in_loop
","if node . dfs_loop_status == ""DFS_UNCHECKED"" :",199
"def _find_config(args, app_desc):
    path = os.path.join(args.galaxy_root, app_desc.destination)
    if not os.path.exists(path):
        path = None
        for possible_ini_config_rel in app_desc.config_paths:
            possible_ini_config = os.path.join(
                args.galaxy_root, possible_ini_config_rel
            )
            if os.path.exists(possible_ini_config):
                path = possible_ini_config
    if path is None:
        _warn(USING_SAMPLE_MESSAGE % path)
        path = os.path.join(args.galaxy_root, app_desc.sample_destination)
    return path
",if os . path . exists ( possible_ini_config ) :,198
"def parseArgs(self, argv):
    if sys.version_info < (3, 4):
        # We want these options to work on all versions, emulate them.
        if ""-R"" in argv:
            argv.remove(""-R"")
            self.refleak = True
        if ""-m"" in argv:
            argv.remove(""-m"")
            self.multiprocess = True
    super(NumbaTestProgram, self).parseArgs(argv)
    if self.verbosity <= 0:
        # We aren't interested in informational messages / warnings when
        # running with '-q'.
        self.buffer = True
","if ""-m"" in argv :",160
"def filter_custom_selected_callback(indices, old, new):
    logger.info(""filter custom callback"")
    filter_label.text = ""Please Wait...""
    global all_topics, apply_filter
    if new != [-1]:
        apply_filter = True
        selected_topics = [filter_custom_table_source.data[""topics""][x] for x in new]
        for i, line in enumerate(all_topics):
            if line[0] in selected_topics:
                all_topics[i][2] = ""1""
            else:
                all_topics[i][2] = ""0""
    filter_label.text = """"
",if line [ 0 ] in selected_topics :,168
"def number_operators(self, a, b, skip=[]):
    dict = {""a"": a, ""b"": b}
    for name, expr in self.binops.items():
        if name not in skip:
            name = ""__%s__"" % name
            if hasattr(a, name):
                res = eval(expr, dict)
                self.binop_test(a, b, res, expr, name)
    for name, expr in list(self.unops.items()):
        if name not in skip:
            name = ""__%s__"" % name
            if hasattr(a, name):
                res = eval(expr, dict)
                self.unop_test(a, res, expr, name)
","if hasattr ( a , name ) :",189
"def reader_matches(self, text):
    text = text[1:]
    matches = []
    for p in self.reader_path:
        for k in p.keys():
            if isinstance(k, string_types):
                if k.startswith(text):
                    matches.append(""#{}"".format(k))
    return matches
","if isinstance ( k , string_types ) :",88
"def load_templates(templates: List[JobTemplateConfig]) -> None:
    handlers = {
        TemplateSubmitHandler: build_template_func,
    }
    for handler in handlers:
        for name in dir(handler):
            if name.startswith(""_""):
                continue
            delattr(handler, name)
        for template in templates:
            setattr(handler, template.name, handlers[handler](template))
","if name . startswith ( ""_"" ) :",106
"def scan_resource_conf(self, conf):
    if ""properties"" in conf:
        if ""supportsHttpsTrafficOnly"" in conf[""properties""]:
            if str(conf[""properties""][""supportsHttpsTrafficOnly""]).lower() == ""true"":
                return CheckResult.PASSED
            else:
                return CheckResult.FAILED
    # Use default if supportsHttpsTrafficOnly is not set
    if ""apiVersion"" in conf:
        # Default for apiVersion 2019 and newer is supportsHttpsTrafficOnly = True
        year = int(conf[""apiVersion""][0:4])
        if year < 2019:
            return CheckResult.FAILED
        else:
            return CheckResult.PASSED
    return CheckResult.FAILED
",if year < 2019 :,192
"def gather_failed_tests(output):
    if output.upper() == ""NONE"":
        return []
    gatherer = GatherFailedTests()
    tests_or_tasks = ""tests or tasks""
    try:
        suite = ExecutionResult(output, include_keywords=False).suite
        suite.visit(gatherer)
        tests_or_tasks = ""tests"" if not suite.rpa else ""tasks""
        if not gatherer.tests:
            raise DataError(""All %s passed."" % tests_or_tasks)
    except:
        raise DataError(
            ""Collecting failed %s from '%s' failed: %s""
            % (tests_or_tasks, output, get_error_message())
        )
    return gatherer.tests
",if not gatherer . tests :,198
"def ds_leak():
    print(""Testing vlens for dataset r/w"")
    print(""-----------------------------"")
    with h5py.File(FNAME, ""w"") as f:
        ds = f.create_dataset(""dset"", (1000,), dtype=dt)
        for idx in range(500):
            # print idx
            if idx % 100 == 0:
                print_memory()
            ds[...] = data
            ds[...]
",if idx % 100 == 0 :,115
"def extract_geth_traces(input, batch_size, output, max_workers):
    """"""Extracts geth traces from JSON lines file.""""""
    with smart_open(input, ""r"") as geth_traces_file:
        if input.endswith("".json""):
            traces_iterable = (json.loads(line) for line in geth_traces_file)
        else:
            traces_iterable = (trace for trace in csv.DictReader(geth_traces_file))
        job = ExtractGethTracesJob(
            traces_iterable=traces_iterable,
            batch_size=batch_size,
            max_workers=max_workers,
            item_exporter=traces_item_exporter(output),
        )
        job.run()
","if input . endswith ( "".json"" ) :",193
"def save_project_as():
    if PROJECT().last_save_path != None:
        open_dir = os.path.dirname(PROJECT().last_save_path)
        # We don't  want to open hidden cache dir when saving file opened as autosave.
        if open_dir.startswith(userfolders.get_cache_dir()) == True:
            open_dir = expanduser(""~"")
    else:
        open_dir = expanduser(""~"")
    dialogs.save_project_as_dialog(_save_as_dialog_callback, PROJECT().name, open_dir)
",if open_dir . startswith ( userfolders . get_cache_dir ( ) ) == True :,139
"def _skip_to_next_iteration_group(self):
    while True:
        if self._currkey is self._marker:
            pass
        elif self._tgtkey is self._marker:
            break
        else:
            if not self._tgtkey == self._currkey:
                break
        newvalue = next(self._iterator)
        if self._keyfunc is None:
            newkey = newvalue
        else:
            newkey = self._keyfunc(newvalue)
        self._currkey = newkey
        self._currvalue = newvalue
",if self . _currkey is self . _marker :,153
"def extractNames(self, names):
    offset = names[""offset""].value
    for header in names.array(""header""):
        key = header[""nameID""].value
        foffset = offset + header[""offset""].value
        field = names.getFieldByAddress(foffset * 8)
        if not field or not isString(field):
            continue
        value = field.value
        if key not in self.NAMEID_TO_ATTR:
            continue
        key = self.NAMEID_TO_ATTR[key]
        if key == ""version"" and value.startswith(u""Version ""):
            # ""Version 1.2"" => ""1.2""
            value = value[8:]
        setattr(self, key, value)
","if key == ""version"" and value . startswith ( u""Version "" ) :",189
"def visit_BoolOp(self, node):
    for i, value in enumerate(node.values):
        if i == len(node.values) - 1:
            self.visit(value)
        else:
            self.visit(value)
            self.visit(node.op)
",if i == len ( node . values ) - 1 :,75
"def list_sparkline_type_id_values(
    date_range_sparkline, correlation_type, type_id, key_id
):
    sparklines_value = []
    for date_day in date_range_sparkline:
        nb_seen_this_day = r_serv_metadata.hget(
            ""{}:{}:{}"".format(correlation_type, type_id, date_day), key_id
        )
        if nb_seen_this_day is None:
            nb_seen_this_day = 0
        sparklines_value.append(int(nb_seen_this_day))
    return sparklines_value
",if nb_seen_this_day is None :,164
"def find_nameless_urls(self, conf):
    nameless = []
    patterns = self.get_patterns(conf)
    for u in patterns:
        if self.has_patterns(u):
            nameless.extend(self.find_nameless_urls(u))
        else:
            if u.name is None:
                nameless.append(u)
    return nameless
",if u . name is None :,103
"def find_zone_id(domain, client=None):
    paginator = client.get_paginator(""list_hosted_zones"")
    zones = []
    for page in paginator.paginate():
        for zone in page[""HostedZones""]:
            if domain.endswith(zone[""Name""]) or (domain + ""."").endswith(zone[""Name""]):
                if not zone[""Config""][""PrivateZone""]:
                    zones.append((zone[""Name""], zone[""Id""]))
    if not zones:
        raise ValueError(""Unable to find a Route53 hosted zone for {}"".format(domain))
    return zones[0][1]
","if not zone [ ""Config"" ] [ ""PrivateZone"" ] :",147
"def _lookup_reference(self, reference):
    if not reference.startswith(""#/""):
        return
    path = reference[2:].split(""/"")
    pointer = self.swagger
    for component in path:
        if component not in pointer:
            raise IndexError(
                ""Can't find location by reference %r at part %r""
                % (reference, component)
            )
        pointer = pointer[component]
    self.log.debug(""Found by reference %r: %r"", reference, pointer)
    return pointer
",if component not in pointer :,134
"def read_line_from_file(ff):
    # assuming that ff contains BV
    line = b""""
    while True:
        vv = ff.read_data(1)[0]
        if vv.symbolic:
            break
        ct = bytes(chr(vv.args[0]), ""utf-8"")
        if ct == b""\n"":
            break
        line += ct
    return line
","if ct == b""\n"" :",105
"def gaussian(N=1000, draw=True, show=True, seed=42, color=None, marker=""sphere""):
    """"""Show N random gaussian distributed points using a scatter plot.""""""
    import ipyvolume as ipv
    rng = np.random.RandomState(seed)  # pylint: disable=no-member
    x, y, z = rng.normal(size=(3, N))
    if draw:
        if color:
            mesh = ipv.scatter(x, y, z, marker=marker, color=color)
        else:
            mesh = ipv.scatter(x, y, z, marker=marker)
        if show:
            # ipv.squarelim()
            ipv.show()
        return mesh
    else:
        return x, y, z
",if show :,191
"def test_read_only_directory(self):
    with _inside_empty_temp_dir():
        oldmode = mode = os.stat(tempfile.tempdir).st_mode
        mode &= ~(stat.S_IWUSR | stat.S_IWGRP | stat.S_IWOTH)
        os.chmod(tempfile.tempdir, mode)
        try:
            if os.access(tempfile.tempdir, os.W_OK):
                self.skipTest(""can't set the directory read-only"")
            with self.assertRaises(PermissionError):
                self.make_temp()
            self.assertEqual(os.listdir(tempfile.tempdir), [])
        finally:
            os.chmod(tempfile.tempdir, oldmode)
","if os . access ( tempfile . tempdir , os . W_OK ) :",185
"def is_checked_sls_template(template):
    if template.__contains__(""provider""):
        # Case provider is a dictionary
        if isinstance(template[""provider""], dict_node):
            if template[""provider""].get(""name"").lower() not in SUPPORTED_PROVIDERS:
                return False
        # Case provider is direct provider name
        if isinstance(template[""provider""], str_node):
            if template[""provider""] not in SUPPORTED_PROVIDERS:
                return False
        return True
    return False
","if isinstance ( template [ ""provider"" ] , str_node ) :",131
"def detail(self, req):
    resp_backup = super(BackupsController, self).detail(req)
    context = req.environ[""cinder.context""]
    req_version = req.api_version_request
    if req_version.matches(mv.BACKUP_PROJECT):
        if context.authorize(policy.BACKUP_ATTRIBUTES_POLICY, fatal=False):
            for bak in resp_backup[""backups""]:
                self._add_backup_project_attribute(req, bak)
    if req_version.matches(mv.BACKUP_PROJECT_USER_ID):
        if context.authorize(policy.BACKUP_ATTRIBUTES_POLICY, fatal=False):
            for bak in resp_backup[""backups""]:
                self._add_backup_user_attribute(req, bak)
    return resp_backup
","if context . authorize ( policy . BACKUP_ATTRIBUTES_POLICY , fatal = False ) :",196
"def genConditional(self):
    for i in range(3):
        x = 0
        try:
            if i == 2:
                continue
            x = 1
        finally:
            for j in range(x, x + 2):
                yield j
",if i == 2 :,76
"def _cacheAffectedBones(self):
    self._affectedBones = []
    for f_idx in range(self.nFrames):
        frameData = self.getAtFramePos(f_idx)
        self._affectedBones.append([])
        for b_idx in range(self.nBones):
            if not isRest(frameData[b_idx]):
                self._affectedBones[f_idx].append(b_idx)
",if not isRest ( frameData [ b_idx ] ) :,116
"def load_metrics(self, filename, config_dict):
    # we don't try to validate metrics keys
    if ""metrics"" in config_dict:
        metrics = config_dict[""metrics""]
        if not isinstance(metrics, dict):
            error(""c['metrics'] must be a dictionary"")
        else:
            self.metrics = metrics
","if not isinstance ( metrics , dict ) :",87
"def _decode_list_response(response: Iterable[Any], decode: bool) -> Any:
    if decode is True:
        new_response = []
        for val in response:
            if isinstance(val, bytes):
                val = val.decode(""utf-8"")
            new_response.append(val)
        return new_response
    return response
","if isinstance ( val , bytes ) :",94
"def _np_convert_in_place(d):
    """"""Convert any jax devicearray leaves to numpy arrays in place.""""""
    if isinstance(d, dict):
        for k, v in d.items():
            if isinstance(v, jax.xla.DeviceArray):
                d[k] = np.array(v)
            elif isinstance(v, dict):
                _np_convert_in_place(v)
    elif isinstance(d, jax.xla.DeviceArray):
        return np.array(d)
    return d
","if isinstance ( v , jax . xla . DeviceArray ) :",141
"def reader():
    with tarfile.open(filename, mode=""r"") as f:
        names = (each_item.name for each_item in f if sub_name in each_item.name)
        while True:
            for name in names:
                if six.PY2:
                    batch = pickle.load(f.extractfile(name))
                else:
                    batch = pickle.load(f.extractfile(name), encoding=""bytes"")
                for item in read_batch(batch):
                    yield item
            if not cycle:
                break
",if six . PY2 :,157
"def _Determine_Do(self):
    self.applicable = 1
    method = ""moz-src""
    method_arg = None
    for opt, optarg in self.chosenOptions:
        if opt == ""--moz-src"":
            method = ""moz-src""
        elif opt == ""--moz-objdir"":
            method = ""moz-objdir""
            method_arg = optarg
    if method == ""moz-src"":
        self.value = self._get_mozilla_objdir()
    elif method == ""moz-objdir"":
        self.value = self._use_mozilla_objdir(method_arg)
    else:
        raise black.configure.ConfigureError(""bogus method: %r"" % method)
    self.determined = 1
","elif opt == ""--moz-objdir"" :",188
"def close_all(map=None, ignore_all=False):
    if map is None:  # pragma: no cover
        map = socket_map
    for x in list(map.values()):  # list() FBO py3
        try:
            x.close()
        except OSError as x:
            if x.args[0] == EBADF:
                pass
            elif not ignore_all:
                raise
        except _reraised_exceptions:
            raise
        except:
            if not ignore_all:
                raise
    map.clear()
",elif not ignore_all :,157
"def _attributes_to_xml(self, xml_element, prefix_root, debug_context=None):
    del debug_context  # Unused.
    for attribute_name, attribute in six.iteritems(self._attributes):
        attribute_value = attribute.to_xml_string(prefix_root)
        if attribute_name == self._spec.identifier and attribute_value is None:
            xml_element.set(attribute_name, self.full_identifier)
        elif attribute_value is None:
            continue
        else:
            xml_element.set(attribute_name, attribute_value)
",if attribute_name == self . _spec . identifier and attribute_value is None :,149
"def parse(s):
    """"""Parse the output below to create a new StopWatch.""""""
    stopwatch = StopWatch()
    for line in s.splitlines():
        if line.strip():
            parts = line.split(None)
            name = parts[0]
            if name != ""%"":  # ie not the header line
                rest = (float(v) for v in parts[2:])
                stopwatch.times[parts[0]].merge(Stat.build(*rest))
    return stopwatch
","if name != ""%"" :",128
"def reverse_adjust_line_according_to_hunks(self, hunks, line):
    for hunk in reversed(hunks):
        head_start = hunk.head_start
        saved_start = hunk.saved_start
        if hunk.saved_length == 0:
            saved_start += 1
        elif hunk.head_length == 0:
            saved_start -= 1
        head_end = head_start + hunk.head_length
        saved_end = saved_start + hunk.saved_length
        if saved_end <= line:
            return head_end + line - saved_end
        elif saved_start <= line:
            return head_start
    # fails to find matching
    return line
",elif hunk . head_length == 0 :,193
"def add(self, *args):
    self._digest = None
    llt = Hasher.list_like_types
    for arg in args:
        t = type(arg)
        if t in llt:
            self._hasher.update(bytes(f""{llt[t]} {len(arg)}"", ""utf8""))
            self.add(*arg)
        else:
            self._hasher.update(bytes(str(arg), ""utf8""))
",if t in llt :,119
"def filter(self, qs, value):
    if value:
        if value.start is not None and value.stop is not None:
            value = (value.start, value.stop)
        elif value.start is not None:
            self.lookup_expr = ""startswith""
            value = value.start
        elif value.stop is not None:
            self.lookup_expr = ""endswith""
            value = value.stop
    return super().filter(qs, value)
",elif value . start is not None :,125
"def _getResourceData(self, jid, dataname):
    """"""Return specific jid's resource representation in internal format. Used internally.""""""
    if jid.find(""/"") + 1:
        jid, resource = jid.split(""/"", 1)
        if self._data[jid][""resources""].has_key(resource):
            return self._data[jid][""resources""][resource][dataname]
    elif self._data[jid][""resources""].keys():
        lastpri = -129
        for r in self._data[jid][""resources""].keys():
            if int(self._data[jid][""resources""][r][""priority""]) > lastpri:
                resource, lastpri = r, int(self._data[jid][""resources""][r][""priority""])
        return self._data[jid][""resources""][resource][dataname]
","if int ( self . _data [ jid ] [ ""resources"" ] [ r ] [ ""priority"" ] ) > lastpri :",194
"def OnGetText(self, node_id):
    try:
        ea, rows = self[node_id]
        if ea in self.colours:
            colour = self.colours[ea]
        else:
            colour = 0xFFFFFF
        ret = []
        for row in rows:
            ret.append(row[2])
        label = ""\n"".join(ret)
        return (label, colour)
    except:
        print(""GraphViewer.OnGetText:"", sys.exc_info()[1])
        return (""ERROR"", 0x000000)
",if ea in self . colours :,150
"def _apply_scales(array, scales, dtype):
    """"""Apply scales to the array.""""""
    new_array = np.empty(array.shape, dtype)
    for i in array.dtype.names:
        try:
            new_array[i] = array[i] * scales[i]
        except TypeError:
            if np.all(scales[i] == 1):
                new_array[i] = array[i]
            else:
                raise
    return new_array
",if np . all ( scales [ i ] == 1 ) :,130
"def run(self):
    self.running = True
    while self.running:
        errCode, bytes, key, overlapped = GetQueuedCompletionStatus(
            self.io_req_port, INFINITE
        )
        if key == ISAPI_SHUTDOWN and overlapped is None:
            break
        # Let the parent extension handle the command.
        dispatcher = self.extension.dispatch_map.get(key)
        if dispatcher is None:
            raise RuntimeError(""Bad request '%s'"" % (key,))
        dispatcher(errCode, bytes, key, overlapped)
",if dispatcher is None :,144
"def on_task_filter(self, task, config):
    if task.options.learn:
        log.info(""Plugin limit_new is disabled with --learn"")
        return
    amount = config
    for index, entry in enumerate(task.accepted):
        if index < amount:
            log.verbose(""Allowed %s (%s)"" % (entry[""title""], entry[""url""]))
        else:
            entry.reject(""limit exceeded"")
            # Also save this in backlog so that it can be accepted next time.
            plugin.get(""backlog"", self).add_backlog(task, entry)
    log.debug(
        ""Rejected: %s Allowed: %s""
        % (len(task.accepted[amount:]), len(task.accepted[:amount]))
    )
",if index < amount :,196
"def initialize_pairs(self):
    # White on Black is fixed as color_pair 0
    self._defined_pairs[""WHITE_BLACK""] = (0, curses.COLOR_WHITE, curses.COLOR_BLACK)
    for cp in self.__class__._colors_to_define:
        if cp[0] == ""WHITE_BLACK"":
            # silently protect the user from breaking things.
            continue
        self.initalize_pair(cp[0], cp[1], cp[2])
","if cp [ 0 ] == ""WHITE_BLACK"" :",118
"def get_story_task_body(payload: Dict[str, Any], action: str) -> str:
    primary_action = get_action_with_primary_id(payload)
    kwargs = {
        ""task_description"": primary_action[""description""],
        ""action"": action,
    }
    for a in payload[""actions""]:
        if a[""entity_type""] == ""story"":
            kwargs[""name_template""] = STORY_NAME_TEMPLATE.format(
                name=a[""name""],
                app_url=a[""app_url""],
            )
    return STORY_TASK_TEMPLATE.format(**kwargs)
","if a [ ""entity_type"" ] == ""story"" :",163
"def _key_remap(key, keys, item):
    elements_list = []
    for r_item in item.get(key, []):
        element = {}
        for r_outkey, r_inkey in six.iteritems(keys):
            if r_inkey in r_item:
                element[r_outkey] = r_item.get(r_inkey)
        elements_list.append(element)
    return elements_list
",if r_inkey in r_item :,115
"def fix_identities(self, uniq=None):
    """"""Make pattern-tree tips point to same object if they are equal.""""""
    if not hasattr(self, ""children""):
        return self
    uniq = list(set(self.flat())) if uniq is None else uniq
    for i, c in enumerate(self.children):
        if not hasattr(c, ""children""):
            assert c in uniq
            self.children[i] = uniq[uniq.index(c)]
        else:
            c.fix_identities(uniq)
","if not hasattr ( c , ""children"" ) :",138
"def _apply_main_args(main_args, exec_args):
    i = 0
    while i < len(exec_args):
        if exec_args[i] == ""${main_args}"":
            exec_args[i : i + 1] = main_args
            i += len(main_args)
        i += 1
","if exec_args [ i ] == ""${main_args}"" :",87
"def _clean_text(self, text):
    """"""Performs invalid character removal and whitespace cleanup on text.""""""
    output = []
    char_idx = []
    for i, char in enumerate(text):
        cp = ord(char)
        if cp == 0 or cp == 0xFFFD or _is_control(char):
            continue
        if _is_whitespace(char):
            output.append("" "")
            char_idx.append(i)
        else:
            output.append(char)
            char_idx.append(i)
    return """".join(output), char_idx
",if cp == 0 or cp == 0xFFFD or _is_control ( char ) :,151
"def upgrade_state_dict_named(self, state_dict, name):
    prefix = name + ""."" if name != """" else """"
    for k, v in state_dict.items():
        if k.endswith(prefix + ""weight""):
            if v.dim() == 3 and v.size(1) == 1:
                state_dict[k] = v.squeeze(1)
",if v . dim ( ) == 3 and v . size ( 1 ) == 1 :,96
"def fetch_with_retry(self):
    for i in range(self.max_retries):
        try:
            self.is_truncated, self.next_marker = self._fetch()
        except ServerError as e:
            if e.status // 100 != 5:
                raise
            if i == self.max_retries - 1:
                raise
        else:
            return
",if e . status // 100 != 5 :,107
"def hg_hook(ui, repo, node=None, **kwargs):
    """"""Run pylama after mercurial commit.""""""
    seen = set()
    paths = []
    if len(repo):
        for rev in range(repo[node], len(repo)):
            for file_ in repo[rev].files():
                file_ = op.join(repo.root, file_)
                if file_ in seen or not op.exists(file_):
                    continue
                seen.add(file_)
                paths.append(file_)
    options = parse_options()
    setup_logger(options)
    if paths:
        process_paths(options, candidates=paths)
",if file_ in seen or not op . exists ( file_ ) :,177
"def test_playlist_items(self):
    playlists = self.spotify.user_playlists(self.username, limit=5)
    self.assertTrue(""items"" in playlists)
    for playlist in playlists[""items""]:
        if playlist[""uri""] != self.new_playlist_uri:
            continue
        pid = playlist[""id""]
        results = self.spotify.playlist_items(pid)
        self.assertEqual(len(results[""items""]), 0)
","if playlist [ ""uri"" ] != self . new_playlist_uri :",111
"def update_execute_option_setting(
    css_selector_of_option_status, css_selector_of_option
):
    retry = 3
    check_status = self.driver.find_element_by_css_selector(
        css_selector_of_option_status
    )
    if ""visibility-hidden"" not in check_status.get_attribute(""class""):
        while retry > 0:
            self.find_by_css_selector(css_selector_of_option).click()
            time.sleep(0.2)
            if ""visibility-hidden"" in check_status.get_attribute(""class""):
                break
            else:
                retry -= 1
","if ""visibility-hidden"" in check_status . get_attribute ( ""class"" ) :",175
"def _validate_config(self):
    # convert comma separated strings to lists (ConfigParser)
    for item in [""to"", ""cc"", ""bcc""]:
        if item in self.app.config.keys(self._meta.config_section):
            value = self.app.config.get(self._meta.config_section, item)
            # convert a comma-separated string to a list
            if type(value) is str:
                value_list = value.split("","")
                # clean up extra space if they had it inbetween commas
                value_list = [x.strip() for x in value_list]
                # set the new extensions value in the config
                self.app.config.set(self._meta.config_section, item, value_list)
",if type ( value ) is str :,199
"def cell_func(combo, render, model, iter_, *args):
    value = model.get_value(iter_)
    if value is None:
        text = escape(_(""System Default""))
    else:
        if value == u""C"":
            value = u""en""
        text = ""%s <span weight='light'>(%s)</span>"" % (
            escape(value),
            escape(iso639.translate(value.split(""_"", 1)[0])),
        )
    render.set_property(""markup"", text)
","if value == u""C"" :",133
"def _get_all_tasks():
    proc = Popen([""yarn"", ""--help""], stdout=PIPE)
    should_yield = False
    for line in proc.stdout.readlines():
        line = line.decode().strip()
        if ""Commands:"" in line:
            should_yield = True
            continue
        if should_yield and ""- "" in line:
            yield line.split("" "")[-1]
","if should_yield and ""- "" in line :",103
"def _staged_model_references(self, load_relationships=False):
    for name, field in self._fields.items():
        if isinstance(field, BaseRelationship):
            try:
                if load_relationships:
                    value = getattr(self, name)
                else:
                    value = self.data_store.get(name, (""staged"", ""committed""))
            except (AttributeError, KeyError, PathResolutionError):
                continue
            if value is None:
                continue
            if not isinstance(value, ModelCollection):
                value = [value]
            for related in value:
                related_name = field.related_name
                yield related, related_name
","if not isinstance ( value , ModelCollection ) :",198
"def get_all_fix_names(fixer_pkg, remove_prefix=True):
    """"""Return a sorted list of all available fix names in the given package.""""""
    pkg = __import__(fixer_pkg, [], [], [""*""])
    fixer_dir = os.path.dirname(pkg.__file__)
    fix_names = []
    for name in sorted(os.listdir(fixer_dir)):
        if name.startswith(""fix_"") and name.endswith("".py""):
            if remove_prefix:
                name = name[4:]
            fix_names.append(name[:-3])
    return fix_names
",if remove_prefix :,147
"def extract_info_to_dest(self, info, dest):
    """"""Extracts the given info to a directory and checks the file size.""""""
    self.zip_file.extract(info, dest)
    dest = os.path.join(dest, info.filename)
    if not os.path.isdir(dest):
        # Directories consistently report their size incorrectly.
        size = os.stat(dest)[stat.ST_SIZE]
        if size != info.file_size:
            log.error(
                ""Extraction error, uncompressed size: %s, %s not %s""
                % (self.source, size, info.file_size)
            )
            raise forms.ValidationError(gettext(""Invalid archive.""))
",if size != info . file_size :,182
"def _close_brackets(self, fragment):
    # If there any unclosed brackets in the text we try to close them
    # and we return part with closing brackets if they are ""closable""
    stack = []
    for char in fragment:
        if char in self._PARENS.keys():
            stack.append(char)
        elif char in self._PARENS.values():
            if stack and self._PARENS[stack[-1]] == char:
                stack.pop()
            else:
                return """"
    return """".join(self._PARENS[paren] for paren in reversed(stack))
",if char in self . _PARENS . keys ( ) :,150
"def __call__(self, input_tensors, shape):
    if self.order in ""KA"":
        if any(t.order == TensorOrder.C_ORDER for t in input_tensors):
            order = TensorOrder.C_ORDER
        else:
            order = TensorOrder.F_ORDER
    else:
        if self.order == ""C"":
            order = TensorOrder.C_ORDER
        else:
            order = TensorOrder.F_ORDER
    return self.new_tensor(input_tensors, shape=shape, dtype=self.dtype, order=order)
","if self . order == ""C"" :",141
"def __iter__(self):
    iteration = self.start_iter
    while iteration <= self.num_iterations:
        # if the underlying sampler has a set_epoch method, like
        # DistributedSampler, used for making each process see
        # a different split of the dataset, then set it
        if hasattr(self.batch_sampler.sampler, ""set_epoch""):
            self.batch_sampler.sampler.set_epoch(iteration)
        for batch in self.batch_sampler:
            iteration += 1
            if iteration > self.num_iterations:
                break
            yield batch
","if hasattr ( self . batch_sampler . sampler , ""set_epoch"" ) :",151
"def all_pairs_shortest_path(adjacency_matrix):
    new_array = copy.deepcopy(adjacency_matrix)
    for k in range(len(new_array)):
        for i in range(len(new_array)):
            for j in range(len(new_array)):
                if new_array[i][j] > new_array[i][k] + new_array[k][j]:
                    new_array[i][j] = new_array[i][k] + new_array[k][j]
    return new_array
",if new_array [ i ] [ j ] > new_array [ i ] [ k ] + new_array [ k ] [ j ] :,142
"def cancel_pp(self, nzo_id):
    """"""Change the status, so that the PP is canceled""""""
    for nzo in self.history_queue:
        if nzo.nzo_id == nzo_id:
            nzo.abort_direct_unpacker()
            if nzo.pp_active:
                nzo.pp_active = False
                try:
                    # Try to kill any external running process
                    self.external_process.kill()
                    logging.info(
                        ""Killed external process %s"", self.external_process.args[0]
                    )
                except:
                    pass
            return True
    return None
",if nzo . pp_active :,196
"def cvPreprocess():
    import cv2
    imgarr_orig = []
    image_ext_list = ["".jpg"", "".png"", "".JPEG"", "".jpeg"", "".PNG"", "".JPG""]
    for file in onlyfiles:
        fimg = imgroot + file
        if any([x in image_ext_list for x in fimg]):
            print(fimg + "" is not an image file"")
            continue
        img1 = cv2.imread(fimg)
        if img1 is None:
            print(""ERROR opening "", fimg)
            continue
        img1 = cv2.resize(img1, (896, 896))
        imgarr_orig.append(img1)
    return imgarr_orig
",if img1 is None :,187
"def substituteargs(self, pattern, replacement, old):
    new = []
    for k in range(len(replacement)):
        item = replacement[k]
        newitem = [item[0], item[1], item[2]]
        for i in range(3):
            if item[i] == ""*"":
                newitem[i] = old[k][i]
            elif item[i][:1] == ""$"":
                index = int(item[i][1:]) - 1
                newitem[i] = old[index][i]
        new.append(tuple(newitem))
    ##self.report(""old: %r"", old)
    ##self.report(""new: %r"", new)
    return new
","elif item [ i ] [ : 1 ] == ""$"" :",187
"def process(self, profile):
    contributors = self.createContributors(profile)
    for contributor in contributors:
        if contributor.type == ""SQLOperator"":
            reasons = self.createExecSqlNodeReason(contributor, profile)
        else:
            reasons = self.createExecNodeReason(contributor, profile)
        contributor.reason = reasons
    return contributors
","if contributor . type == ""SQLOperator"" :",106
"def showImage(filename):
    osName = platform.system()
    if osName == ""Windows"":
        subprocess.Popen([filename], shell=True)
    elif osName == ""Linux"":
        # TODO: should I leave it to user's config ?
        LINUX_DISPLAY_COMMAND = (""xdg-open"", ""display"", ""gvfs-open"", ""shotwell"")
        commands = list(filter(HasCommand, LINUX_DISPLAY_COMMAND))
        if commands:  # command found
            subprocess.Popen([commands[0], filename])
        else:
            raise
    elif osName == ""Darwin"":  # by @Naville
        subprocess.Popen([""open"", filename])
    else:
        raise Exception(""other system"")
",if commands :,187
"def add_libdirs(self, envvar, sep, fatal=False):
    v = os.environ.get(envvar)
    if not v:
        return
    for dir in str.split(v, sep):
        dir = str.strip(dir)
        if not dir:
            continue
        dir = os.path.normpath(dir)
        if os.path.isdir(dir):
            if not dir in self.library_dirs:
                self.library_dirs.append(dir)
        elif fatal:
            fail(""FATAL: bad directory %s in environment variable %s"" % (dir, envvar))
",if os . path . isdir ( dir ) :,159
"def add(self, state):
    if state.key in self:
        if attributes.instance_state(self._dict[state.key]) is not state:
            raise sa_exc.InvalidRequestError(
                ""Can't attach instance ""
                ""%s; another instance with key %s is already ""
                ""present in this session."" % (orm_util.state_str(state), state.key)
            )
        return False
    else:
        self._dict[state.key] = state.obj()
        self._manage_incoming_state(state)
        return True
",if attributes . instance_state ( self . _dict [ state . key ] ) is not state :,154
"def request(self, stream=None, tty=None, demux=None):
    assert stream is not None and tty is not None and demux is not None
    with APIClient(base_url=self.address, version=DEFAULT_DOCKER_API_VERSION) as client:
        if tty:
            url = client._url(""/tty"")
        else:
            url = client._url(""/no-tty"")
        resp = client._post(url, stream=True)
        return client._read_from_socket(resp, stream=stream, tty=tty, demux=demux)
",if tty :,147
"def select(model, path, iter_, paths_):
    (paths, first) = paths_
    value = model.get_value(iter_)
    if value is None:
        return not bool(paths)
    value = normalize_path(value)
    if value in paths:
        self.get_child().get_selection().select_path(path)
        paths.remove(value)
        if not first:
            self.get_child().set_cursor(path)
            # copy treepath, gets invalid after the callback
            first.append(path.copy())
    else:
        for fpath in paths:
            if fpath.startswith(value):
                self.get_child().expand_row(path, False)
    return not bool(paths)
",if not first :,194
"def _validate(self, qobj):
    for experiment in qobj.experiments:
        if ""measure"" not in [op.name for op in experiment.instructions]:
            logger.warning(
                ""no measurements in circuit '%s', ""
                ""classical register will remain all zeros."",
                experiment.header.name,
            )
","if ""measure"" not in [ op . name for op in experiment . instructions ] :",93
"def exitval_from_opts(options, project):
    exit_value_from = options.get(""--exit-code-from"")
    if exit_value_from:
        if not options.get(""--abort-on-container-exit""):
            log.warning(""using --exit-code-from implies --abort-on-container-exit"")
            options[""--abort-on-container-exit""] = True
        if exit_value_from not in [s.name for s in project.get_services()]:
            log.error(
                'No service named ""%s"" was found in your compose file.', exit_value_from
            )
            sys.exit(2)
    return exit_value_from
",if exit_value_from not in [ s . name for s in project . get_services ( ) ] :,178
"def __call__(self, tokens, reader):
    first_return = False
    for token in tokens:
        if not hasattr(reader.context.current_function, ""exit_count""):
            reader.context.current_function.exit_count = 1
            first_return = True
        if token == ""return"":
            if first_return:
                first_return = False
            else:
                reader.context.current_function.exit_count += 1
        yield token
","if not hasattr ( reader . context . current_function , ""exit_count"" ) :",128
"def _register_builtin_handlers(self, events):
    for spec in handlers.BUILTIN_HANDLERS:
        if len(spec) == 2:
            event_name, handler = spec
            self.register(event_name, handler)
        else:
            event_name, handler, register_type = spec
            if register_type is handlers.REGISTER_FIRST:
                self._events.register_first(event_name, handler)
            elif register_type is handlers.REGISTER_LAST:
                self._events.register_last(event_name, handler)
",elif register_type is handlers . REGISTER_LAST :,148
"def test_sql(self):
    with self.get_temp() as temp:
        railroad = to_railroad(simpleSQL)
        assert len(railroad) == 7
        temp.write(railroad_to_html(railroad))
        if self.railroad_debug():
            print(""sql: "" + temp.name)
",if self . railroad_debug ( ) :,93
"def resources_to_link(self, resources):
    if isinstance(self.Bucket, dict) and ""Ref"" in self.Bucket:
        bucket_id = self.Bucket[""Ref""]
        if not isinstance(bucket_id, string_types):
            raise InvalidEventException(
                self.relative_id, ""'Ref' value in S3 events is not a valid string.""
            )
        if bucket_id in resources:
            return {""bucket"": resources[bucket_id], ""bucket_id"": bucket_id}
    raise InvalidEventException(
        self.relative_id, ""S3 events must reference an S3 bucket in the same template.""
    )
",if bucket_id in resources :,166
"def list_target_unit_files(self, *modules):  # -> [ (unit,enabled) ]
    """"""show all the target units and the enabled status""""""
    result = {}
    enabled = {}
    for unit in _all_common_targets:
        result[unit] = None
        enabled[unit] = ""static""
        if unit in _all_common_enabled:
            enabled[unit] = ""enabled""
        if unit in _all_common_disabled:
            enabled[unit] = ""enabled""
    return [(unit, enabled[unit]) for unit in sorted(result)]
",if unit in _all_common_disabled :,147
"def teardown_network_port(self):
    """"""tearDown for Network and Port table""""""
    networks = self.quantum.get_all_networks(""t1"")
    for net in networks:
        netid = net[""net-id""]
        name = net[""net-name""]
        if ""net"" in name:
            ports = self.quantum.get_all_ports(netid)
            for por in ports:
                self.quantum.delete_port(netid, por[""port-id""])
            self.quantum.delete_network(netid)
","if ""net"" in name :",148
"def findConfigFiles(self, cfg_args):
    """"""Find available config files""""""
    filenames = cfg_args.config[:]
    proj_opts = (""unittest.cfg"", ""nose2.cfg"")
    for fn in proj_opts:
        if cfg_args.top_level_directory:
            fn = os.path.abspath(os.path.join(cfg_args.top_level_directory, fn))
        filenames.append(fn)
    if cfg_args.user_config:
        user_opts = (""~/.unittest.cfg"", ""~/.nose2.cfg"")
        for fn in user_opts:
            filenames.append(os.path.expanduser(fn))
    return filenames
",if cfg_args . top_level_directory :,171
"def make_aware(value):
    if settings.USE_TZ:
        # naive datetimes are assumed to be in UTC.
        if timezone.is_naive(value):
            value = timezone.make_aware(value, timezone.utc)
        # then convert to the Django configured timezone.
        default_tz = timezone.get_default_timezone()
        value = timezone.localtime(value, default_tz)
    return value
",if timezone . is_naive ( value ) :,106
"def update(id):
    """"""Update a post if the current user is the author.""""""
    post = get_post(id)
    if request.method == ""POST"":
        title = request.form[""title""]
        body = request.form[""body""]
        error = None
        if not title:
            error = ""Title is required.""
        if error is not None:
            flash(error)
        else:
            post.title = title
            post.body = body
            db.session.commit()
            return redirect(url_for(""blog.index""))
    return render_template(""blog/update.html"", post=post)
",if error is not None :,168
"def copyfileobj(src, dest, length=512):
    if hasattr(src, ""readinto""):
        buf = bytearray(length)
        while True:
            sz = src.readinto(buf)
            if not sz:
                break
            if sz == length:
                dest.write(buf)
            else:
                b = memoryview(buf)[:sz]
                dest.write(b)
    else:
        while True:
            buf = src.read(length)
            if not buf:
                break
            dest.write(buf)
",if not buf :,162
"def imgFileProcessingTick(output):
    if isinstance(output, tuple):
        workerOutput.append(output)
        workerPool.terminate()
    else:
        for page in output:
            if page is not None:
                options.imgMetadata[page[0]] = page[1]
                options.imgOld.append(page[2])
    if GUI:
        GUI.progressBarTick.emit(""tick"")
        if not GUI.conversionAlive:
            workerPool.terminate()
",if not GUI . conversionAlive :,129
"def process_word(word):
    if word.parent == ""remapping"":
        raise UDError(""There is a cycle in a sentence"")
    if word.parent is None:
        head = int(word.columns[HEAD])
        if head > len(ud.words) - sentence_start:
            raise UDError(
                ""HEAD '{}' points outside of the sentence"".format(word.columns[HEAD])
            )
        if head:
            parent = ud.words[sentence_start + head - 1]
            word.parent = ""remapping""
            process_word(parent)
            word.parent = parent
",if head :,163
"def validate_export(namespace):
    destination = namespace.destination
    if destination == ""file"":
        if namespace.path is None or namespace.format_ is None:
            raise CLIError(""usage error: --path PATH --format FORMAT"")
    elif destination == ""appconfig"":
        if (namespace.dest_name is None) and (namespace.dest_connection_string is None):
            raise CLIError(""usage error: --config-name NAME | --connection-string STR"")
    elif destination == ""appservice"":
        if namespace.appservice_account is None:
            raise CLIError(""usage error: --appservice-account NAME_OR_ID"")
",if namespace . path is None or namespace . format_ is None :,159
"def get_change_set_status(context, stack_name, change_set_name):
    try:
        response = retry_boto_call(
            context.client.describe_change_set,
            ChangeSetName=change_set_name,
            StackName=stack_name,
        )
    except ClientError as e:
        if e.response[""Error""][""Code""] == ""ChangeSetNotFound"":
            return None
        else:
            raise e
    return response[""Status""]
","if e . response [ ""Error"" ] [ ""Code"" ] == ""ChangeSetNotFound"" :",127
"def predict(self, predict_data):
    assert self.predict_fn is not None
    # For the batch by batch prediction case, we do not want to include the cost of
    # doing final outputs concatenation into time measurement
    with Timer() as t:
        if self.batch_benchmark:
            self.predictions = self.predict_fn(predict_data)
        else:
            self.predictions = self.predict_fn(predict_data, concatenate_outputs=False)
    if not self.batch_benchmark:
        self.predictions = np.concatenate(self.predictions)
    return t.interval
",if self . batch_benchmark :,148
"def __str__(self):
    s = ""("" + str(self[0])
    s += "", ""
    if isinstance(self[1], Tensor):
        if self[1].name and self[1].name is not None:
            s += self[1].name
        else:
            s += ""tensor-"" + hex(id(self[1]))
    else:
        s += str(self[1])
    s += "", ""
    if isinstance(self[2], Tensor):
        if self[2].name and self[2].name is not None:
            s += self[2].name
        else:
            s += ""tensor-"" + hex(id(self[2]))
    else:
        s += str(self[2])
    s += "")""
    return s
",if self [ 2 ] . name and self [ 2 ] . name is not None :,198
"def get_local_cache(self, past, data, from_file, temp_id):
    """"""parse individual cached geometry if there is any""""""
    cache = []
    if self.accumulative:
        if from_file and len(past) > 0:
            cache = past[temp_id]
        if not from_file and len(data) > 0:
            cache = data.get(temp_id, [])
    return cache
",if from_file and len ( past ) > 0 :,109
"def get_mappings(index):
    mappings = {}
    from kitsune.search.models import get_mapping_types
    for cls in get_mapping_types():
        group = cls.get_index_group()
        if index == write_index(group) or index == read_index(group):
            mappings[cls.get_mapping_type_name()] = cls.get_mapping()
    return mappings
",if index == write_index ( group ) or index == read_index ( group ) :,101
"def find_first_of_filetype(content, filterfiltype, attr=""name""):
    """"""Find the first of the file type.""""""
    filename = """"
    for _filename in content:
        if isinstance(_filename, str):
            if _filename.endswith(f"".{filterfiltype}""):
                filename = _filename
                break
        else:
            if getattr(_filename, attr).endswith(f"".{filterfiltype}""):
                filename = getattr(_filename, attr)
                break
    return filename
","if isinstance ( _filename , str ) :",135
"def _timer(
    duetime: typing.AbsoluteOrRelativeTime,
    period: Optional[typing.RelativeTime] = None,
    scheduler: Optional[typing.Scheduler] = None,
) -> Observable:
    if isinstance(duetime, datetime):
        if period is None:
            return observable_timer_date(duetime, scheduler)
        else:
            return observable_timer_duetime_and_period(duetime, period, scheduler)
    if period is None:
        return observable_timer_timespan(duetime, scheduler)
    return observable_timer_timespan_and_period(duetime, period, scheduler)
",if period is None :,160
"def __getattribute__(self, attrname):
    result = object.__getattribute__(self, attrname)
    if result is NOT_SET:
        try:
            self._read_info(attrname)
        except Exception as e:
            logging.warning(
                ""An error '%s' was raised while decoding '%s'"", e, repr(self.path)
            )
        result = object.__getattribute__(self, attrname)
        if result is NOT_SET:
            result = self.INITIAL_INFO[attrname]
    return result
",if result is NOT_SET :,138
"def on_btOK_clicked(self, *a):
    """"""Handler for OK button""""""
    if self.ac_callback is not None:
        self._set_title()
        if self._mode == ActionEditor.AEC_MENUITEM:
            self.ac_callback(self.id, self)
        else:
            a = self.generate_modifiers(
                self._action, self._selected_component.NAME == ""custom""
            )
            self.ac_callback(self.id, a)
            self.ac_callback = None
        if self._selected_component:
            self._selected_component.on_ok(a)
    self.close()
",if self . _mode == ActionEditor . AEC_MENUITEM :,180
"def execute():
    if frappe.db.get_value(""Company"", {""country"": ""India""}, ""name""):
        address_template = frappe.db.get_value(""Address Template"", ""India"", ""template"")
        if not address_template or ""gstin"" not in address_template:
            set_up_address_templates(default_country=""India"")
","if not address_template or ""gstin"" not in address_template :",94
"def is_ncname(name):
    first = name[0]
    if first == ""_"" or category(first) in NAME_START_CATEGORIES:
        for i in xrange(1, len(name)):
            c = name[i]
            if not category(c) in NAME_CATEGORIES:
                if c in ALLOWED_NAME_CHARS:
                    continue
                return 0
            # if in compatibility area
            # if decomposition(c)!='':
            #    return 0
        return 1
    else:
        return 0
",if c in ALLOWED_NAME_CHARS :,151
"def _get_sonnet_version():
    with open(""sonnet/__init__.py"") as fp:
        for line in fp:
            if line.startswith(""__version__""):
                g = {}
                exec(line, g)  # pylint: disable=exec-used
                return g[""__version__""]
        raise ValueError(""`__version__` not defined in `sonnet/__init__.py`"")
","if line . startswith ( ""__version__"" ) :",102
"def disjoined(self):
    gridscope = GridScope(globals=self.globals)
    for key in self.user_added:
        value = self[key]
        if isinstance(value, np.ndarray):
            grid = vaex.utils.disjoined(value)
            gridscope[key] = grid
        else:
            gridscope[key] = value
    return gridscope
","if isinstance ( value , np . ndarray ) :",101
"def _maybe_uncompress(self):
    if not self._decompressed:
        compression_type = self.compression_type
        if compression_type != self.CODEC_NONE:
            data = memoryview(self._buffer)[self._pos :]
            if compression_type == self.CODEC_GZIP:
                uncompressed = gzip_decode(data)
            if compression_type == self.CODEC_SNAPPY:
                uncompressed = snappy_decode(data.tobytes())
            if compression_type == self.CODEC_LZ4:
                uncompressed = lz4_decode(data.tobytes())
            self._buffer = bytearray(uncompressed)
            self._pos = 0
    self._decompressed = True
",if compression_type != self . CODEC_NONE :,192
"def read_chat_forever(reader, pub_socket):
    line = reader.readline()
    who = ""someone""
    while line:
        print(""Chat:"", line.strip())
        if line.startswith(""name:""):
            who = line.split("":"")[-1].strip()
        try:
            pub_socket.send_pyobj((who, line))
        except socket.error as e:
            # ignore broken pipes, they just mean the participant
            # closed its connection already
            if e[0] != 32:
                raise
        line = reader.readline()
    print(""Participant left chat."")
","if line . startswith ( ""name:"" ) :",162
"def items(self, section=None):
    section = section if section is not None else Settings.DEFAULT_SECTION
    result = {""section"": section}
    try:
        if section in self._global_settings.sections():
            for option in self._global_settings.options(section):
                result[option] = self._global_settings.get(section, option)
        if section in self._local_settings.sections():
            for option in self._local_settings.options(section):
                result[option] = self._local_settings.get(section, option)
    except configparser.InterpolationSyntaxError:
        core.termwarn(""Unable to parse settings file"")
    return result
",if section in self . _global_settings . sections ( ) :,172
"def before_train(self, program):
    """"""doc""""""
    if self.summary_record:
        if self.summary_record.scalar:
            self.s_name, self.s_tolog = zip(*self.summary_record.scalar)
        else:
            self.s_name, self.s_tolog = [], []
        if self.summary_record.histogram:
            self.h_name, self.h_tolog = zip(*self.summary_record.histogram)
        else:
            self.h_name, self.h_tolog = [], []
",if self . summary_record . histogram :,150
"def _s3_init(self):
    """"""Initialize s3 bucket.""""""
    try:
        bucket_exists = yield self._bucket_exists()
        if not bucket_exists:
            LOGGER.warning(""Will attempt to create bucket"")
            yield self._create_bucket()
    except botocore.exceptions.NoCredentialsError:
        LOGGER.error(
            'You must set ""s3.accessKeyId"" and ""s3.secretAccessKey"", or '
            '""s3.profile"" in your Streamlit configuration.'
        )
        raise errors.S3NoCredentials
",if not bucket_exists :,144
"def id2unit(self, id):
    items = []
    for v, k in zip(id, self._id2unit.keys()):
        if v == EMPTY_ID:
            continue
        if self.keyed:
            items.append(""{}={}"".format(k, self._id2unit[k][v]))
        else:
            items.append(self._id2unit[k][v])
    res = self.sep.join(items)
    if res == """":
        res = ""_""
    return res
",if v == EMPTY_ID :,130
"def forward(model: TransformerListener, docs, is_train):
    if is_train:
        model.verify_inputs(docs)
        return model._outputs, model.backprop_and_clear
    else:
        if len(docs) == 0:
            outputs = []
        elif any(doc._.trf_data is None for doc in docs):
            width = model.get_dim(""nO"")
            outputs = [
                TransformerData.zeros(len(doc), width, xp=model.ops.xp) for doc in docs
            ]
        else:
            outputs = [doc._.trf_data for doc in docs]
        return outputs, lambda d_data: []
",if len ( docs ) == 0 :,182
"def get_plugin_dir(shooting_dir):
    DIRNAME = ""lunapark""
    parent = os.path.abspath(os.path.join(shooting_dir, os.pardir))
    if os.path.basename(parent) == DIRNAME:
        return parent
    else:
        plugin_dir = os.path.join(parent, DIRNAME)
        if not os.path.exists(plugin_dir):
            os.makedirs(plugin_dir)
        return plugin_dir
",if not os . path . exists ( plugin_dir ) :,129
"def _get_plugin(self, name, lang=None, check=False):
    if lang is None:
        lang = self.get_lang()
    if name not in self.plugin_attrib_map:
        return None
    plugin_class = self.plugin_attrib_map[name]
    if plugin_class.is_extension:
        if (name, None) in self.plugins:
            return self.plugins[(name, None)]
        else:
            return None if check else self.init_plugin(name, lang)
    else:
        if (name, lang) in self.plugins:
            return self.plugins[(name, lang)]
        else:
            return None if check else self.init_plugin(name, lang)
","if ( name , None ) in self . plugins :",189
"def globs_relative_to_buildroot(self):
    buildroot = get_buildroot()
    globs = []
    for bundle in self.bundles:
        fileset = bundle.fileset
        if fileset is None:
            continue
        elif hasattr(fileset, ""filespec""):
            globs += bundle.fileset.filespec[""globs""]
        else:
            # NB(nh): filemap is an OrderedDict, so this ordering is stable.
            globs += [fast_relpath(f, buildroot) for f in bundle.filemap.keys()]
    super_globs = super().globs_relative_to_buildroot()
    if super_globs:
        globs += super_globs[""globs""]
    return {""globs"": globs}
","elif hasattr ( fileset , ""filespec"" ) :",187
"def running_jobs(self, exit_on_error=True):
    """"""Initialize multiprocessing.""""""
    with self.handling_exceptions():
        if self.using_jobs:
            from concurrent.futures import ProcessPoolExecutor
            try:
                with ProcessPoolExecutor(self.jobs) as self.executor:
                    yield
            finally:
                self.executor = None
        else:
            yield
    if exit_on_error:
        self.exit_on_error()
",if self . using_jobs :,131
"def _get_all_checkpoint_paths(self) -> List[str]:
    """"""Returns all the checkpoint paths managed by the instance.""""""
    # Due to tensorflow/issues/19378, we cannot use `tf.io.gfile.glob` here
    # because it returns directory contents recursively on Windows.
    if tf.io.gfile.exists(self._root_dir):
        root_dir_entries = tf.io.gfile.listdir(self._root_dir)
        return [
            os.path.join(self._root_dir, e)
            for e in root_dir_entries
            if e.startswith(self._prefix)
        ]
    else:
        return []
",if e . startswith ( self . _prefix ),170
"def test_tag_priority(self):
    for tag in _low_priority_D_TAG:
        val = ENUM_D_TAG[tag]
        # if the low priority tag is present in the descriptions,
        # assert that it has not overridden any other tag
        if _DESCR_D_TAG[val] == tag:
            for tag2 in ENUM_D_TAG:
                if tag2 == tag:
                    continue
                self.assertNotEqual(ENUM_D_TAG[tag2], val)
",if tag2 == tag :,135
"def cycle(self, forward=True):
    if self.cycle_list:
        if forward is True:
            self.cycle_list.rotate(-1)
        elif forward is False:
            self.cycle_list.rotate(1)
        self.move_to_obj(self.cycle_list[0])
",elif forward is False :,82
"def __init__(self):
    self.keyring = None
    if not haveKeyring:
        return
    try:
        self.keyring = gnomekeyring.get_default_keyring_sync()
        if self.keyring == None:
            # Code borrowed from
            # http://trac.gajim.org/browser/src/common/passwords.py
            self.keyring = ""default""
            try:
                gnomekeyring.create_sync(self.keyring, None)
            except gnomekeyring.AlreadyExistsError:
                pass
    except:
        logging.exception(""Error determining keyring"")
        self.keyring = None
",if self . keyring == None :,171
"def _coerce_trials_data(data, path):
    if not isinstance(data, list):
        if not isinstance(data, dict):
            raise BatchFileError(
                path,
                ""invalid data type for trials: expected list or dict""
                "", got %s"" % type(data).__name__,
            )
        data = [data]
    for item in data:
        if not isinstance(item, dict):
            raise BatchFileError(
                path, ""invalid data type for trial %r: expected dict"" % item
            )
    return data
","if not isinstance ( data , dict ) :",152
"def update(self):
    if self.openfilename is not None:
        try:
            current_mtime = os.stat(self.openfilename).st_mtime
        except OSError:
            return True
        if current_mtime != self.last_mtime:
            self.last_mtime = current_mtime
            self.reload()
    return True
",if current_mtime != self . last_mtime :,91
"def _wrap_new_compiler(*args, **kwargs):
    try:
        return func(*args, **kwargs)
    except errors.DistutilsPlatformError:
        if not sys.platform == ""win32"":
            CCompiler = _UnixCCompiler
        else:
            CCompiler = _MSVCCompiler
        return CCompiler(None, kwargs[""dry_run""], kwargs[""force""])
","if not sys . platform == ""win32"" :",98
"def _run_eagerly(*inputs):  # pylint: disable=missing-docstring
    with context.eager_mode():
        constants = [
            _wrap_as_constant(value, tensor_spec)
            for value, tensor_spec in zip(inputs, input_signature)
        ]
        output = fn(*constants)
        if hasattr(output, ""_make""):
            return output._make([tensor.numpy() for tensor in output])
        if isinstance(output, (tuple, list)):
            return [tensor.numpy() for tensor in output]
        else:
            return output.numpy()
","if isinstance ( output , ( tuple , list ) ) :",153
"def _on_event_MetadataAnalysisFinished(self, event, data):
    with self._selectedFileMutex:
        if self._selectedFile:
            self._setJobData(
                self._selectedFile[""filename""],
                self._selectedFile[""filesize""],
                self._selectedFile[""sd""],
                self._selectedFile[""user""],
            )
",if self . _selectedFile :,99
"def env_asset_url_default(endpoint, values):
    """"""Create asset URLs dependent on the current env""""""
    if endpoint == ""views.themes"":
        path = values.get(""path"", """")
        static_asset = path.endswith("".js"") or path.endswith("".css"")
        direct_access = "".dev"" in path or "".min"" in path
        if static_asset and not direct_access:
            env = values.get(""env"", current_app.env)
            mode = "".dev"" if env == ""development"" else "".min""
            base, ext = os.path.splitext(path)
            values[""path""] = base + mode + ext
",if static_asset and not direct_access :,166
"def __init__(self, inStr):
    """"""Initialize the class.""""""
    inStr = inStr.strip()
    if len(inStr) != 1 and len(inStr) != 2:
        raise ValueError(""PosAlign: length not 2 chars"" + inStr)
    if inStr == "".."":
        self.aa = ""-""
        self.gap = 1
    else:
        self.gap = 0
        self.aa = inStr[0]
        if self.aa == self.aa.lower():
            self.aa = ""C""
        if len(inStr) == 2:
            self.ss = inStr[1].upper()
        else:
            self.ss = ""0""
",if self . aa == self . aa . lower ( ) :,179
"def iter_ReassignParameters(self, inputNode, variables, nodeByID):
    for node in inputNode.getReassignParameterNodes(nodeByID):
        yield from iterNodeCommentLines(node)
        yield from iterInputConversionLines(node, variables)
        socket = node.inputs[0]
        if socket.isUnlinked and socket.isCopyable():
            expression = getCopyExpression(socket, variables)
        else:
            expression = variables[socket]
        if node.conditionSocket is None:
            conditionPrefix = """"
        else:
            conditionPrefix = ""if {}: "".format(variables[node.conditionSocket])
        yield ""{}{} = {}"".format(
            conditionPrefix, variables[node.linkedParameterSocket], expression
        )
",if node . conditionSocket is None :,192
"def init_weight(self):
    if self.pretrained is not None:
        load_entire_model(self, self.pretrained)
    else:
        for sublayer in self.sublayers():
            if isinstance(sublayer, nn.Conv2D):
                kaiming_normal_init(sublayer.weight)
            elif isinstance(sublayer, (nn.BatchNorm, nn.SyncBatchNorm)):
                kaiming_normal_init(sublayer.weight)
","elif isinstance ( sublayer , ( nn . BatchNorm , nn . SyncBatchNorm ) ) :",120
"def logic():
    while 1:
        if reset == ACTIVE_LOW:
            yield reset.posedge
        for i in range(20):
            yield clock.posedge
            if enable:
                count.next = i
        j = 1
        while j < 25:
            if enable:
                yield clock.posedge
            yield clock.posedge
            count.next = 2 * j
            j += 1
",if reset == ACTIVE_LOW :,123
"def clean_log_messages(result_data):
    for idx in range(len(result_data[""executePlan""][""stepEvents""])):
        message = result_data[""executePlan""][""stepEvents""][idx].get(""message"")
        if message is not None:
            result_data[""executePlan""][""stepEvents""][idx][""message""] = re.sub(
                r""(\d+(\.\d+)?)"", ""{N}"", message
            )
    return result_data
",if message is not None :,115
"def headerData(self, section, orientation, role=Qt.DisplayRole):
    if role == Qt.TextAlignmentRole:
        if orientation == Qt.Horizontal:
            return to_qvariant(int(Qt.AlignHCenter | Qt.AlignVCenter))
        return to_qvariant(int(Qt.AlignRight | Qt.AlignVCenter))
    if role != Qt.DisplayRole:
        return to_qvariant()
    if orientation == Qt.Horizontal:
        if section == NAME:
            return to_qvariant(""Name"")
        elif section == VERSION:
            return to_qvariant(""Version"")
        elif section == ACTION:
            return to_qvariant(""Action"")
        elif section == DESCRIPTION:
            return to_qvariant(""Description"")
    return to_qvariant()
",elif section == ACTION :,192
"def _gather_infos(self):
    # Carry over information from previous game step.
    if self._prev_state is not None:
        for attr in self._tracked_infos:
            self.state[attr] = self.state.get(attr) or self._prev_state.get(attr)
    for info in [""score"", ""moves""]:
        if self.state[info] is not None and type(self.state[info]) is not int:
            self.state[info] = int(self.state[info].strip())
    self.state[""won""] = ""*** The End ***"" in self.state[""feedback""]
    self.state[""lost""] = ""*** You lost! ***"" in self.state[""feedback""]
",if self . state [ info ] is not None and type ( self . state [ info ] ) is not int :,180
"def calc_parity(sig, kind):
    if kind in (""zero"", ""none""):
        return C(0, 1)
    elif kind == ""one"":
        return C(1, 1)
    else:
        bits, _ = value_bits_sign(sig)
        even_parity = sum([sig[b] for b in range(bits)]) & 1
        if kind == ""odd"":
            return ~even_parity
        elif kind == ""even"":
            return even_parity
        else:
            assert False
","if kind == ""odd"" :",141
"def tool(self, **kwds):
    process_definition = kwds.get(""process_definition"", None)
    if process_definition is None:
        raw_process_reference = kwds.get(""raw_process_reference"", None)
        if raw_process_reference is None:
            raw_process_reference = self.raw_process_reference(kwds[""path""])
        process_definition = self.process_definition(raw_process_reference)
    tool = load_tool.make_tool(
        process_definition.uri,
        process_definition.loading_context,
    )
    return tool
",if raw_process_reference is None :,147
"def context(self):
    # Needed to avoid Translate Toolkit construct ID
    # as context\04source
    if self.template is not None:
        if self.template.id:
            return self.template.id
        if self.template.context:
            return self.template.context
        return self.template.getid()
    return self.unescape_csv(self.mainunit.getcontext())
",if self . template . context :,103
"def test_six_thread_safety():
    _reload_six()
    with patch(
        ""botocore.vendored.six.moves.__class__.__setattr__"", wraps=_wrapped_setattr
    ):
        threads = []
        for i in range(2):
            t = _ExampleThread()
            threads.append(t)
            t.start()
        while threads:
            t = threads.pop()
            t.join()
            if t.exc_info:
                six.reraise(*t.exc_info)
",if t . exc_info :,144
"def _handle_js_events(self, change):
    if self.js_events:
        if self.eventHandlers:
            for event in self.js_events:
                event_name = event[""name""]
                if event_name in self.eventHandlers:
                    self.eventHandlers[event_name](event[""detail""])
        # clears the event queue.
        self.js_events = []
",if event_name in self . eventHandlers :,110
"def single_discriminator(x, filters=128, kernel_size=8, strides=4, pure_mean=False):
    """"""A simple single-layer convolutional discriminator.""""""
    with tf.variable_scope(""discriminator""):
        net = layers().Conv2D(
            filters, kernel_size, strides=strides, padding=""SAME"", name=""conv1""
        )(x)
        if pure_mean:
            net = tf.reduce_mean(net, [1, 2])
        else:
            net = mean_with_attention(net, ""mean_with_attention"")
        return net
",if pure_mean :,148
"def find_path(self, from_location, to_location):
    end = to_location
    f_node = self.mh.get_node(from_location)
    self.on.append(f_node)
    self.o.append(f_node.lid)
    next_node = f_node
    counter = 0  # a bail-out counter
    while next_node is not None:
        if counter > 10000:
            break  # no path found under limit
        finish = self._handle_node(next_node, end)
        if finish:
            return self._trace_path(finish)
        next_node = self._get_best_open_node()
        counter += 1
    return None
",if counter > 10000 :,182
"def format_var_dict(dct, indent=4, max_width=80):
    lines = []
    pre = "" "" * indent
    for key, value in dct.items():
        line = pre + key + "" = "" + repr(value)
        if len(line) > max_width:
            line = line[: max_width - 3] + ""...""
            try:
                value_len = len(value)
            except:
                pass
            else:
                line += ""\n"" + pre + ""len(%s) = %s"" % (key, value_len)
        lines.append(line)
    return ""\n"".join(lines)
",if len ( line ) > max_width :,176
"def _recursive_name_seach(self, layer_names, layer, pre_name, depth):
    for name, module in layer.named_children():
        nname = pre_name + ""_"" + name if pre_name != """" else name
        if depth == self.depth or self.depth is None:
            if self._wrap_layer_check(module, name, nname):
                layer_names.append(nname)
        if self.depth is None or depth <= self.depth:
            if len(list(layer.named_children())) > 0:
                self._recursive_name_seach(layer_names, module, nname, depth + 1)
    return layer_names
",if depth == self . depth or self . depth is None :,175
"def finished_at(self):
    f = self.metadata_get([""State"", ""FinishedAt""])
    if f:
        f = f[:26]
        if f == ""0001-01-01T00:00:00Z"":
            return DINOSAUR_TIME
        finished_at = datetime.datetime.strptime(f, ISO_DATETIME_PARSE_STRING)
        return finished_at
","if f == ""0001-01-01T00:00:00Z"" :",100
"def write_bool(self, bool):
    if (
        self._bool_fid
        and self._bool_fid > self._last_fid
        and self._bool_fid - self._last_fid <= 15
    ):
        if bool:
            ctype = CompactType.TRUE
        else:
            ctype = CompactType.FALSE
        self._write_field_header(ctype, self._bool_fid)
    else:
        if bool:
            self.write_byte(CompactType.TRUE)
        else:
            self.write_byte(CompactType.FALSE)
",if bool :,156
"def update(self, topLeft, bottomRight):
    if self._updating:
        # We are currently putting data in the model, so no updates
        return
    if self._index:
        if topLeft.row() <= self._index.row() <= bottomRight.row():
            self.updateText()
    elif self._indexes:
        update = False
        for i in self._indexes:
            if topLeft.row() <= i.row() <= bottomRight.row():
                update = True
        if update:
            self.updateText()
",if update :,144
"def _preprocess_add_items(self, items):
    """"""Split the items into two lists of path strings and BaseEntries.""""""
    paths = []
    entries = []
    for item in items:
        if isinstance(item, string_types):
            paths.append(self._to_relative_path(item))
        elif isinstance(item, (Blob, Submodule)):
            entries.append(BaseIndexEntry.from_blob(item))
        elif isinstance(item, BaseIndexEntry):
            entries.append(item)
        else:
            raise TypeError(""Invalid Type: %r"" % item)
    # END for each item
    return (paths, entries)
","if isinstance ( item , string_types ) :",165
"def ping_all():
    for l in _all_listeners.values():
        count = l.receiver.count()
        if count:
            for dev in l.receiver:
                dev.ping()
                l._status_changed(dev)
                count -= 1
                if not count:
                    break
",if count :,92
"def stage_node_dot(g, stage):
    """"""Create a stage node.""""""
    with g.subgraph(name=""cluster_"" + stage[""id""]) as subgraph:
        subgraph.attr(label=stage[""name""])
        if stage[""all_itervars""]:
            for itervar in stage[""all_itervars""]:
                iv_type = itervar[""itervar_type""]
                itervar_node_dot(subgraph, itervar, iv_type, itervar[""index""])
            for rel in stage[""relations""]:
                node_id = rel[""id""]
                itervar_relation_dot(subgraph, rel, node_id)
        else:
            subgraph.node(stage[""name""] + ""_placeholder"", style=""invis"")
","if stage [ ""all_itervars"" ] :",190
"def run() -> None:
    nonlocal state, timeout
    while True:
        if timeout > 0.0:
            disposed.wait(timeout)
        if disposed.is_set():
            return
        time: datetime = self.now
        state = action(state)
        timeout = seconds - (self.now - time).total_seconds()
",if disposed . is_set ( ) :,92
"def increment(s):
    if not s:
        return ""1""
    for sequence in string.digits, string.lowercase, string.uppercase:
        lastc = s[-1]
        if lastc in sequence:
            i = sequence.index(lastc) + 1
            if i >= len(sequence):
                if len(s) == 1:
                    s = sequence[0] * 2
                    if s == ""00"":
                        s = ""10""
                else:
                    s = increment(s[:-1]) + sequence[0]
            else:
                s = s[:-1] + sequence[i]
            return s
    return s  # Don't increment
",if i >= len ( sequence ) :,196
"def Import(self, patch, force):
    if not patch.get(""file""):
        if not patch.get(""remote""):
            raise PatchError(""Patch file must be specified in patch import."")
        else:
            patch[""file""] = bb.fetch2.localpath(patch[""remote""], self.d)
    for param in PatchSet.defaults:
        if not patch.get(param):
            patch[param] = PatchSet.defaults[param]
    if patch.get(""remote""):
        patch[""file""] = self.d.expand(bb.fetch2.localpath(patch[""remote""], self.d))
    patch[""filemd5""] = bb.utils.md5_file(patch[""file""])
","if not patch . get ( ""remote"" ) :",176
"def _setReadyState(self, state: str) -> None:
    if state != self.__readyState:
        self.__log_debug(""- %s -> %s"", self.__readyState, state)
        self.__readyState = state
        if state == ""open"":
            self.emit(""open"")
        elif state == ""closed"":
            self.emit(""close"")
            # no more events will be emitted, so remove all event listeners
            # to facilitate garbage collection.
            self.remove_all_listeners()
","if state == ""open"" :",131
"def count_brokers(self):
    self.nb_brokers = 0
    for broker in self.brokers:
        if not broker.spare:
            self.nb_brokers += 1
    for realm in self.higher_realms:
        for broker in realm.brokers:
            if not broker.spare and broker.manage_sub_realms:
                self.nb_brokers += 1
",if not broker . spare and broker . manage_sub_realms :,118
"def _refresh(self):
    self.uiProfileSelectComboBox.clear()
    self.uiProfileSelectComboBox.addItem(""default"")
    try:
        if os.path.exists(self.profiles_path):
            for profile in sorted(os.listdir(self.profiles_path)):
                if not profile.startswith("".""):
                    self.uiProfileSelectComboBox.addItem(profile)
    except OSError:
        pass
",if os . path . exists ( self . profiles_path ) :,107
"def run(self):
    for k, v in iteritems(self.objs):
        if k.startswith(""_""):
            continue
        if v[""_class""] == ""Dataset"" and v[""task_type""] == ""Communication"":
            try:
                params = json.loads(v[""task_type_parameters""])
            except json.JSONDecodeError:
                pass
            else:
                if len(params) == 1:
                    params.extend([""stub"", ""fifo_io""])
                v[""task_type_parameters""] = json.dumps(params)
    return self.objs
","if k . startswith ( ""_"" ) :",158
"def _listen(self, consumer_id: str) -> AsyncIterable[Any]:
    try:
        while True:
            if self._listening:
                async for msg in self._listen_to_queue(consumer_id):
                    if msg is not None:
                        yield msg
                await asyncio.sleep(0.5)
            else:
                async for msg in self._listen_to_ws():
                    yield msg
    except asyncio.CancelledError:
        pass
    except Exception as e:
        raise e
",if msg is not None :,153
"def recv(self, bufsiz, flags=0):
    d = self._sock.recv(bufsiz, flags)
    if self.replace_pattern and b"" HTTP/1.1\r\n"" in d:
        line_end = d.find(b""\r\n"")
        req_line = d[:line_end]
        words = req_line.split()
        if len(words) == 3:
            method, url, http_version = words
            url = url.replace(self.replace_pattern[0], self.replace_pattern[1])
            d = b""%s %s %s"" % (method, url, http_version) + d[line_end:]
    return d
",if len ( words ) == 3 :,178
"def Import(self, patch, force):
    if not patch.get(""file""):
        if not patch.get(""remote""):
            raise PatchError(""Patch file must be specified in patch import."")
        else:
            patch[""file""] = bb.fetch2.localpath(patch[""remote""], self.d)
    for param in PatchSet.defaults:
        if not patch.get(param):
            patch[param] = PatchSet.defaults[param]
    if patch.get(""remote""):
        patch[""file""] = self.d.expand(bb.fetch2.localpath(patch[""remote""], self.d))
    patch[""filemd5""] = bb.utils.md5_file(patch[""file""])
",if not patch . get ( param ) :,176
"def delete(post_id):
    blogging_engine = _get_blogging_engine(current_app)
    storage = blogging_engine.storage
    post = storage.get_post_by_id(post_id)
    if (post is not None) and (current_user.get_id() == post[""user_id""]):
        success = storage.delete_post(post_id)
        if success:
            flash(""Your post was successfully deleted"", ""info"")
        else:
            flash(""Something went wrong while deleting your post"", ""warning"")
    else:
        flash(""You do not have the rights to delete this post"", ""warning"")
    return redirect(url_for(""blog_app.index""))
",if success :,177
"def update_schema_configs(state, schema):
    RegistrationSchema = state.get_model(""osf"", ""registrationschema"")
    for rs in RegistrationSchema.objects.all():
        if rs.schema.get(""description"", False):
            rs.description = rs.schema[""description""]
        if rs.schema.get(""config"", False):
            rs.config = rs.schema[""config""]
        rs.save()
","if rs . schema . get ( ""config"" , False ) :",108
"def set_payload(self, value):
    del self[""payload""]
    if isinstance(value, ElementBase):
        if value.tag_name() in self.plugin_tag_map:
            self.init_plugin(value.plugin_attrib, existing_xml=value.xml)
        self.xml.append(value.xml)
    else:
        self.xml.append(value)
",if value . tag_name ( ) in self . plugin_tag_map :,98
"def getCellPropertyNames_aux(self, col_id):
    if col_id == ""name"":
        if self.image_icon == ""places_busy"":
            return [""places_busy""]
        baseName = self.image_icon
        if self.isOpen:
            return [baseName + ""_open""]
        else:
            return [baseName + ""_closed""]
    return []
",if self . isOpen :,102
"def one_xmm_reg_imm8(ii):  # also allows SSE4 2-imm8 instr
    i, j, n = 0, 0, 0
    for op in _gen_opnds(ii):
        if op_reg(op) and op_xmm(op):
            n += 1
        elif op_imm8(op):
            i += 1
        elif op_imm8_2(op):
            j += 1
        else:
            return False
    return n == 1 and i == 1 and j <= 1
",elif op_imm8 ( op ) :,141
"def step(self, action):
    """"""Repeat action, sum reward, and max over last observations.""""""
    total_reward = 0.0
    done = None
    for i in range(self._skip):
        obs, reward, done, info = self.env.step(action)
        if i == self._skip - 2:
            self._obs_buffer[0] = obs
        if i == self._skip - 1:
            self._obs_buffer[1] = obs
        total_reward += reward
        if done:
            break
    # Note that the observation on the done=True frame doesn't matter.
    max_frame = self._obs_buffer.max(axis=0)
    return max_frame, total_reward, done, info
",if i == self . _skip - 2 :,187
"def assertNodeSequenceEqual(
    self,
    seq1: Sequence[cst.CSTNode],
    seq2: Sequence[cst.CSTNode],
    msg: Optional[str] = None,
) -> None:
    suffix = """" if msg is None else f""\n{msg}""
    if len(seq1) != len(seq2):
        raise AssertionError(f""\n{seq1!r}\nis not deeply equal to \n{seq2!r}{suffix}"")
    for node1, node2 in zip(seq1, seq2):
        if not node1.deep_equals(node2):
            raise AssertionError(
                f""\n{seq1!r}\nis not deeply equal to \n{seq2!r}{suffix}""
            )
",if not node1 . deep_equals ( node2 ) :,189
"def close(self):
    if self._file_writer is not None:
        if self.trial and self.trial.evaluated_params and self.last_result:
            flat_result = flatten_dict(self.last_result, delimiter=""/"")
            scrubbed_result = {
                k: value
                for k, value in flat_result.items()
                if isinstance(value, tuple(VALID_SUMMARY_TYPES))
            }
            self._try_log_hparams(scrubbed_result)
        self._file_writer.close()
",if self . trial and self . trial . evaluated_params and self . last_result :,146
"def check_space(arr, task_id):
    for a in arr:
        if a.startswith(""hadoop jar""):
            found = False
            for x in shlex.split(a):
                if task_id in x:
                    found = True
            if not found:
                raise AssertionError
",if not found :,86
"def is_valid_block(self):
    """"""check wheter the block is valid in the current position""""""
    for i in range(self.block.x):
        for j in range(self.block.x):
            if self.block.get(i, j):
                if self.block.pos.x + i < 0:
                    return False
                if self.block.pos.x + i >= COLUMNS:
                    return False
                if self.block.pos.y + j < 0:
                    return False
                if self.map.get((self.block.pos.x + i, self.block.pos.y + j), False):
                    return False
    return True
","if self . block . get ( i , j ) :",192
"def undo_block_stop(self):
    if self.undoblock.bump_depth(-1) == 0:
        cmd = self.undoblock
        self.undoblock = 0
        if len(cmd) > 0:
            if len(cmd) == 1:
                # no need to wrap a single cmd
                cmd = cmd.getcmd(0)
            # this blk of cmds, or single cmd, has already
            # been done, so don't execute it again
            self.addcmd(cmd, 0)
",if len ( cmd ) > 0 :,139
"def __(task: pipelines.Task):
    if not acl.current_user_has_permission(views.acl_resource):
        return bootstrap.card(
            header_left=""Commands"", body=acl.inline_permission_denied_message()
        )
    else:
        commands_card = bootstrap.card(
            header_left=""Commands"",
            fixed_header_height=True,
            sections=[_render_command(command) for command in task.commands],
        )
        if task.max_retries:
            return [
                bootstrap.card(header_left=f""Max retries: {task.max_retries}""),
                commands_card,
            ]
        else:
            return commands_card
",if task . max_retries :,196
"def closeEvent(self, e=None):
    """"""Save settings and remove registered logging handler""""""
    if self.editor.isModified():
        # ask if user wants to save
        if self.wants_save():
            if self.save():
                e.accept()
            else:
                # saving error or user canceled
                e.ignore()
        else:
            # discard changes
            e.accept()
    else:
        # unchanged
        e.accept()
",if self . save ( ) :,133
"def _merge(self, a, b, path=None):
    """"""Merge two dictionaries, from http://stackoverflow.com/questions/7204805/dictionaries-of-dictionaries-merge""""""
    if path is None:
        path = []
    for key in b:
        if key in a:
            if isinstance(a[key], dict) and isinstance(b[key], dict):
                self._merge(a[key], b[key], path + [str(key)])
            elif a[key] == b[key]:
                pass  # same leaf value
            else:
                raise Exception(""Conflict at %s"" % ""."".join(path + [str(key)]))
        else:
            a[key] = b[key]
    return a
",if key in a :,196
"def _flags_helper(conf, atom, new_flags, test=False):
    try:
        new_flags = __salt__[""portage_config.get_missing_flags""](conf, atom, new_flags)
    except Exception:  # pylint: disable=broad-except
        import traceback
        return {""result"": False, ""comment"": traceback.format_exc()}
    if new_flags:
        old_flags = __salt__[""portage_config.get_flags_from_package_conf""](conf, atom)
        if not test:
            __salt__[""portage_config.append_to_package_conf""](conf, atom, new_flags)
        return {""result"": True, ""changes"": {""old"": old_flags, ""new"": new_flags}}
    return {""result"": None}
",if not test :,197
"def _confirm_deps(self, trans):
    if [pkgs for pkgs in trans.dependencies if pkgs]:
        dia = AptConfirmDialog(trans, parent=self.parent)
        res = dia.run()
        dia.hide()
        if res != Gtk.ResponseType.OK:
            log.debug(""Response is: %s"" % res)
            if self.finish_handler:
                log.debug(""Finish_handler..."")
                self.finish_handler(trans, 0, self.data)
            return
    self._run_transaction(trans)
",if self . finish_handler :,147
"def get_supported_extensions(self):
    for item in self.get_subclasses():
        instance = item()
        if instance.check():
            for ext in instance.supports_extensions:
                self.extractors.update({instance.cls_name: instance})
                try:
                    self.extractors_by_extension[ext].append(instance)
                except KeyError:
                    self.extractors_by_extension[ext] = [instance]
",if instance . check ( ) :,126
"def find_module(self, fullname, path=None):
    # Check for local modules first...
    localname = fullname.split(""."")[-1]
    name, ext = os.path.splitext(localname)
    try:
        fobj, filename, typeinfo = imp.find_module(name, path)
    except ImportError:
        logger.info(""Dcode Searching: %s (%s)"", name, path)
        pymod = self.proxy.getPythonModule(fullname, path)
        if pymod:
            logger.info(""Dcode Loaded: %s"", fullname)
            return DcodeLoader(*pymod)
",if pymod :,155
"def run(self):
    try:
        self.server_sock = self._create_socket_and_bind()
        # in case self.port = 0
        self.port = self.server_sock.getsockname()[1]
        self.ready_event.set()
        self._handle_requests()
        if self.wait_to_close_event:
            self.wait_to_close_event.wait(self.WAIT_EVENT_TIMEOUT)
    finally:
        self.ready_event.set()  # just in case of exception
        self._close_server_sock_ignore_errors()
        self.stop_event.set()
",if self . wait_to_close_event :,163
"def connection(self, commit_on_success=False):
    with self._lock:
        if self._bulk_commit:
            if self._pending_connection is None:
                self._pending_connection = sqlite.connect(self.filename)
            con = self._pending_connection
        else:
            con = sqlite.connect(self.filename)
        try:
            if self.fast_save:
                con.execute(""PRAGMA synchronous = 0;"")
            yield con
            if commit_on_success and self.can_commit:
                con.commit()
        finally:
            if not self._bulk_commit:
                con.close()
",if self . _pending_connection is None :,182
"def getReceiptInfo(pkgname):
    """"""Get receipt info from a package""""""
    info = []
    if hasValidPackageExt(pkgname):
        display.display_debug2(""Examining %s"" % pkgname)
        if os.path.isfile(pkgname):  # new flat package
            info = getFlatPackageInfo(pkgname)
        if os.path.isdir(pkgname):  # bundle-style package?
            info = getBundlePackageInfo(pkgname)
    elif pkgname.endswith("".dist""):
        info = parsePkgRefs(pkgname)
    return info
",if os . path . isfile ( pkgname ) :,143
"def test_gen_speed(gen_func):
    cur_time = time.time()
    for idx, _ in enumerate(gen_func()):
        log.info(""iter %s: %s s"" % (idx, time.time() - cur_time))
        cur_time = time.time()
        if idx == 100:
            break
",if idx == 100 :,87
"def __init__(self, *args, **kwargs):
    if not quickjs_available:
        msg = ""No supported QuickJS package found on custom python environment!""
        if chakra_available:
            msg += "" Please install python package quickjs or use ChakraJSEngine.""
        elif external_interpreter:
            msg += "" Please install python package quickjs or use ExternalJSEngine.""
        else:
            msg += "" Please install python package quickjs.""
        raise RuntimeError(msg)
    self._context = self.Context(self)
    InternalJSEngine.__init__(self, *args, **kwargs)
",if chakra_available :,156
"def _draw_nodes(self, cr, bounding, highlight_items):
    highlight_nodes = []
    for element in highlight_items:
        if isinstance(element, Edge):
            highlight_nodes.append(element.src)
            highlight_nodes.append(element.dst)
        else:
            highlight_nodes.append(element)
    for node in self.nodes:
        if bounding is None or node._intersects(bounding):
            node._draw(cr, highlight=(node in highlight_nodes), bounding=bounding)
",if bounding is None or node . _intersects ( bounding ) :,134
"def upgrade():
    bind = op.get_bind()
    session = db.Session(bind=bind)
    for slc in session.query(Slice).filter(Slice.viz_type.like(""deck_%"")):
        params = json.loads(slc.params)
        if params.get(""latitude""):
            params[""spatial""] = {
                ""lonCol"": params.get(""longitude""),
                ""latCol"": params.get(""latitude""),
                ""type"": ""latlong"",
            }
            del params[""latitude""]
            del params[""longitude""]
        slc.params = json.dumps(params)
        session.merge(slc)
        session.commit()
    session.close()
","if params . get ( ""latitude"" ) :",189
"def list_completers():
    """"""List the active completers""""""
    o = ""Registered Completer Functions: \n""
    _comp = xsh_session.completers
    ml = max((len(i) for i in _comp), default=0)
    _strs = []
    for c in _comp:
        if _comp[c].__doc__ is None:
            doc = ""No description provided""
        else:
            doc = "" "".join(_comp[c].__doc__.split())
        doc = justify(doc, 80, ml + 3)
        _strs.append(""{: >{}} : {}"".format(c, ml, doc))
    return o + ""\n"".join(_strs) + ""\n""
",if _comp [ c ] . __doc__ is None :,173
"def test_numeric_literals(self):
    @udf(BigIntVal(FunctionContext, SmallIntVal))
    def fn(context, a):
        if a is None:
            return 1729
        elif a < 0:
            return None
        elif a < 10:
            return a + 5
        else:
            return a * 2
",elif a < 0 :,92
"def get_normal_sample(in_file):
    """"""Retrieve normal sample if normal/turmor""""""
    with utils.open_gzipsafe(in_file) as in_handle:
        for line in in_handle:
            if line.startswith(""##PEDIGREE""):
                parts = line.strip().split(""Original="")[1][:-1]
                return parts
","if line . startswith ( ""##PEDIGREE"" ) :",94
"def generate_html_index(index_file, outdir):
    data = parse_index_file(index_file)
    data = ((d[0], d[1]) for d in data)
    for i, chunk in enumerate(web.group(data, 1000)):
        back = ""..""
        index = t_html_layout(t_html_sitemap(back, chunk))
        path = outdir + ""/%02d/%05d.html"" % (i / 1000, i)
        write(path, web.safestr(index))
    for f in os.listdir(outdir):
        path = os.path.join(outdir, f)
        if os.path.isdir(path):
            dirindex(path)
    dirindex(outdir, back=""."")
",if os . path . isdir ( path ) :,185
"def _aggregate_metadata_attribute(
    self, attr, agg_func=np.max, default_value=0, from_type_metadata=True
):
    attr_values = []
    for a in self.appliances:
        if from_type_metadata:
            attr_value = a.type.get(attr)
        else:
            attr_value = a.metadata.get(attr)
        if attr_value is not None:
            attr_values.append(attr_value)
    if len(attr_values) == 0:
        return default_value
    else:
        return agg_func(attr_values)
",if from_type_metadata :,162
"def install(self, unicode=False, names=None):
    import __builtin__
    __builtin__.__dict__[""_""] = unicode and self.ugettext or self.gettext
    if hasattr(names, ""__contains__""):
        if ""gettext"" in names:
            __builtin__.__dict__[""gettext""] = __builtin__.__dict__[""_""]
        if ""ngettext"" in names:
            __builtin__.__dict__[""ngettext""] = (
                unicode and self.ungettext or self.ngettext
            )
        if ""lgettext"" in names:
            __builtin__.__dict__[""lgettext""] = self.lgettext
        if ""lngettext"" in names:
            __builtin__.__dict__[""lngettext""] = self.lngettext
","if ""lgettext"" in names :",181
"def logic():
    while 1:
        if reset == ACTIVE_LOW:
            yield reset.posedge
        for i in range(20):
            yield clock.posedge
            if enable:
                count.next = i
        j = 1
        while j < 25:
            if enable:
                yield clock.posedge
            yield clock.posedge
            count.next = 2 * j
            j += 1
",if enable :,123
"def multi_device(reader, dev_count):
    if dev_count == 1:
        for batch in reader:
            yield batch
    else:
        batches = []
        for batch in reader:
            batches.append(batch)
            if len(batches) == dev_count:
                yield batches
                batches = []
",if len ( batches ) == dev_count :,92
"def lockfile_from_pipfile(cls, pipfile_path):
    from .pipfile import Pipfile
    if os.path.isfile(pipfile_path):
        if not os.path.isabs(pipfile_path):
            pipfile_path = os.path.abspath(pipfile_path)
        pipfile = Pipfile.load(os.path.dirname(pipfile_path))
        return plette.lockfiles.Lockfile.with_meta_from(pipfile._pipfile)
    raise PipfileNotFound(pipfile_path)
",if not os . path . isabs ( pipfile_path ) :,139
"def _resolve_result(self, f=None):
    try:
        if f:
            results = f.result()
        else:
            results = list(map(self._client.results.get, self.msg_ids))
        if self._single_result:
            r = results[0]
            if isinstance(r, Exception):
                raise r
        else:
            results = error.collect_exceptions(results, self._fname)
        self._success = True
        self.set_result(self._reconstruct_result(results))
    except Exception as e:
        self._success = False
        self.set_exception(e)
",if self . _single_result :,174
"def config_update(self, *updates):
    filename = os.path.join(self.path, "".git"", ""config"")
    with GitConfigParser(file_or_files=filename, read_only=False) as config:
        for section, key, value in updates:
            try:
                old = config.get(section, key)
                if value is None:
                    config.remove_option(section, key)
                    continue
                if old == value:
                    continue
            except (NoSectionError, NoOptionError):
                pass
            if value is not None:
                config.set_value(section, key, value)
",if value is not None :,183
"def process_percent(token, state, command_line):
    if not state.is_range_start_line_parsed:
        if command_line.line_range.start:
            raise ValueError(""bad range: {0}"".format(state.scanner.state.source))
        command_line.line_range.start.append(token)
    else:
        if command_line.line_range.end:
            raise ValueError(""bad range: {0}"".format(state.scanner.state.source))
        command_line.line_range.end.append(token)
    return parse_line_ref, command_line
",if command_line . line_range . end :,154
"def Flatten(self, metadata, value_to_flatten):
    if metadata:
        self.metadata = metadata
    for desc in value_to_flatten.type_infos:
        if desc.name == ""metadata"":
            continue
        if hasattr(self, desc.name) and value_to_flatten.HasField(desc.name):
            setattr(self, desc.name, getattr(value_to_flatten, desc.name))
","if desc . name == ""metadata"" :",108
"def create_model(model, args, is_train):
    """"""Create model, include basic model, googlenet model and mixup model""""""
    data_loader, data = utility.create_data_loader(is_train, args)
    if args.model == ""GoogLeNet"":
        loss_out = _googlenet_model(data, model, args, is_train)
    else:
        if args.use_mixup and is_train:
            loss_out = _mixup_model(data, model, args, is_train)
        else:
            loss_out = _basic_model(data, model, args, is_train)
    return data_loader, loss_out
",if args . use_mixup and is_train :,175
"def __init__(self, store):
    if store.context_aware:
        self.contexts = list(store.contexts())
        self.default_context = store.default_context.identifier
        if store.default_context:
            self.contexts.append(store.default_context)
    else:
        self.contexts = [store]
        self.default_context = None
    super(TrigSerializer, self).__init__(store)
",if store . default_context :,110
"def validate_import_depth(namespace):
    depth = namespace.depth
    if depth is not None:
        try:
            depth = int(depth)
            if depth < 1:
                raise CLIError(""Depth should be at least 1."")
        except ValueError:
            raise CLIError(""Depth is not a number."")
",if depth < 1 :,85
"def __sync(self):
    """"""Skip reader to the block boundary.""""""
    pad_length = BLOCK_SIZE - self.__reader.tell() % BLOCK_SIZE
    if pad_length and pad_length != BLOCK_SIZE:
        data = self.__reader.read(pad_length)
        if len(data) != pad_length:
            raise EOFError(""Read %d bytes instead of %d"" % (len(data), pad_length))
",if len ( data ) != pad_length :,109
"def _split_long_text(text, idx, size):
    splited_text = text.split()
    if len(splited_text) > 25:
        if idx == 0:
            # The first is (...)text
            first = """"
        else:
            first = "" "".join(splited_text[:10])
        if idx != 0 and idx == size - 1:
            # The last is text(...)
            last = """"
        else:
            last = "" "".join(splited_text[-10:])
        return ""{}(...){}"".format(first, last)
    return text
",if idx == 0 :,156
"def download_label_map(out_dir):
    log.info(""Downloading ScanNet "" + RELEASE_NAME + "" label mapping file..."")
    files = [LABEL_MAP_FILE]
    for file in files:
        url = BASE_URL + RELEASE_TASKS + ""/"" + file
        localpath = os.path.join(out_dir, file)
        localdir = os.path.dirname(localpath)
        if not os.path.isdir(localdir):
            os.makedirs(localdir)
        download_file(url, localpath)
    log.info(""Downloaded ScanNet "" + RELEASE_NAME + "" label mapping file."")
",if not os . path . isdir ( localdir ) :,157
"def get_related_ids(self, resources):
    vpc_ids = [vpc[""VpcId""] for vpc in resources]
    vpc_igw_ids = set()
    for igw in self.manager.get_resource_manager(""internet-gateway"").resources():
        for attachment in igw[""Attachments""]:
            if attachment.get(""VpcId"", """") in vpc_ids:
                vpc_igw_ids.add(igw[""InternetGatewayId""])
    return vpc_igw_ids
","if attachment . get ( ""VpcId"" , """" ) in vpc_ids :",125
"def visit_Assign(self, node):
    """"""Handle visiting an assignment statement.""""""
    ups = set()
    for targ in node.targets:
        if isinstance(targ, (Tuple, List)):
            ups.update(leftmostname(elt) for elt in targ.elts)
        elif isinstance(targ, BinOp):
            newnode = self.try_subproc_toks(node)
            if newnode is node:
                ups.add(leftmostname(targ))
            else:
                return newnode
        else:
            ups.add(leftmostname(targ))
    self.ctxupdate(ups)
    return node
",if newnode is node :,165
"def evex_mask_dest_reg_only(ii):  # optional imm8
    i, m, xyz = 0, 0, 0
    for op in _gen_opnds(ii):
        if op_mask_reg(op):
            m += 1
        elif op_xmm(op) or op_ymm(op) or op_zmm(op):
            xyz += 1
        elif op_imm8(op):
            i += 1
        else:
            return False
    return m == 1 and xyz > 0 and i <= 1
",elif op_imm8 ( op ) :,143
"def get_pynames(self, parameters):
    result = [None] * max(len(parameters), len(self.args))
    for index, arg in enumerate(self.args):
        if isinstance(arg, ast.keyword) and arg.arg in parameters:
            result[parameters.index(arg.arg)] = self._evaluate(arg.value)
        else:
            result[index] = self._evaluate(arg)
    return result
","if isinstance ( arg , ast . keyword ) and arg . arg in parameters :",110
"def _discovery_modules(self) -> List[str]:
    modules: List[str] = []
    autodiscover = self.conf.autodiscover
    if autodiscover:
        if isinstance(autodiscover, bool):
            if self.conf.origin is None:
                raise ImproperlyConfigured(E_NEED_ORIGIN)
        elif callable(autodiscover):
            modules.extend(cast(Callable[[], Iterator[str]], autodiscover)())
        else:
            modules.extend(autodiscover)
        if self.conf.origin:
            modules.append(self.conf.origin)
    return modules
","if isinstance ( autodiscover , bool ) :",149
"def _lock(self, files, type):
    for i in count(0):
        lockfile = os.path.join(self._lockdir, ""{}.{}.lock"".format(i, type))
        if not os.path.exists(lockfile):
            self._lockfile[type] = lockfile
            with open(lockfile, ""w"") as lock:
                print(*files, sep=""\n"", file=lock)
            return
",if not os . path . exists ( lockfile ) :,113
"def _init_inheritable_dicts_(cls):
    if cls.__bases__ != (object,):
        return
    for attr in cls._inheritable_dict_attrs_:
        if isinstance(attr, tuple):
            attr_name, default = attr
        else:
            attr_name, default = attr, {}
        if not isinstance(default, dict):
            raise SyntaxError(""{} is not a dictionary"".format(attr_name))
        setattr(cls, attr_name, default)
","if not isinstance ( default , dict ) :",122
"def _validate_name(self, name):
    if isinstance(name, str):
        name = dns.name.from_text(name, None)
    elif not isinstance(name, dns.name.Name):
        raise KeyError(""name parameter must be convertible to a DNS name"")
    if name.is_absolute():
        if not name.is_subdomain(self.origin):
            raise KeyError(""name parameter must be a subdomain of the zone origin"")
        if self.relativize:
            name = name.relativize(self.origin)
    return name
",if self . relativize :,142
"def hard_update(self, cache, size_change, pins_gates):
    """"""replace verts, rads and vel (in NumPy)""""""
    verts, rads, vel, react = cache
    if len(verts) == self.v_len:
        if pins_gates[0] and pins_gates[1]:
            unpinned = self.params[""unpinned""]
            self.verts[unpinned] = verts[unpinned]
        else:
            self.verts = verts
        self.vel = vel
        if not size_change:
            self.rads = rads
",if pins_gates [ 0 ] and pins_gates [ 1 ] :,155
"def enable(self):
    """"""enable the patch.""""""
    for patch in self.dependencies:
        patch.enable()
    if not self.enabled:
        pyv = sys.version_info[0]
        if pyv == 2:
            if self.PY2 == SKIP:
                return  # skip patch activation
            if not self.PY2:
                raise IncompatiblePatch(""Python 2 not supported!"")
        if pyv == 3:
            if self.PY3 == SKIP:
                return  # skip patch activation
            if not self.PY3:
                raise IncompatiblePatch(""Python 3 not supported!"")
        self.pre_enable()
        self.do_enable()
        self.enabled = True
",if not self . PY3 :,191
"def on_project_dialog_finished(self):
    if self.sender().committed:
        if self.sender().new_project:
            self.close_project()
            self.project_manager.from_dialog(self.sender())
        else:
            self.project_manager.project_updated.emit()
",if self . sender ( ) . new_project :,83
"def filter_database(db, user, filter_name):
    """"""Returns a list of person handles""""""
    filt = MatchesFilter([filter_name])
    filt.requestprepare(db, user)
    if user:
        user.begin_progress(
            _(""Finding relationship paths""),
            _(""Retrieving all sub-filter matches""),
            db.get_number_of_people(),
        )
    matches = []
    for handle in db.iter_person_handles():
        person = db.get_person_from_handle(handle)
        if filt.apply(db, person):
            matches.append(handle)
        if user:
            user.step_progress()
    if user:
        user.end_progress()
    filt.requestreset()
    return matches
","if filt . apply ( db , person ) :",198
"def add(self, key, val):
    if key is None:
        g.trace(""TypeDict: None is not a valid key"", g.callers())
        return
    self._checkKeyType(key)
    self._checkValType(val)
    if self.isList:
        aList = self.d.get(key, [])
        if val not in aList:
            aList.append(val)
            self.d[key] = aList
    else:
        self.d[key] = val
",if val not in aList :,134
"def show_help(ctx, param, value):
    if value and not ctx.resilient_parsing:
        if not ctx.invoked_subcommand:
            # legit main help
            echo(format_help(ctx.get_help()))
        else:
            # legit sub-command help
            echo(ctx.get_help(), color=ctx.color)
        ctx.exit()
",if not ctx . invoked_subcommand :,99
"def wav_to_spec(wav_audio, hparams):
    """"""Transforms the contents of a wav file into a series of spectrograms.""""""
    if hparams.spec_type == ""raw"":
        spec = _wav_to_framed_samples(wav_audio, hparams)
    else:
        if hparams.spec_type == ""cqt"":
            spec = _wav_to_cqt(wav_audio, hparams)
        elif hparams.spec_type == ""mel"":
            spec = _wav_to_mel(wav_audio, hparams)
        else:
            raise ValueError(""Invalid spec_type: {}"".format(hparams.spec_type))
        if hparams.spec_log_amplitude:
            spec = librosa.power_to_db(spec)
    return spec
","if hparams . spec_type == ""cqt"" :",197
"def __bytes__(self) -> bytes:
    payload = pack(""!LL"", self.ssrc, self.media_ssrc)
    if self.lost:
        pid = self.lost[0]
        blp = 0
        for p in self.lost[1:]:
            d = p - pid - 1
            if d < 16:
                blp |= 1 << d
            else:
                payload += pack(""!HH"", pid, blp)
                pid = p
                blp = 0
        payload += pack(""!HH"", pid, blp)
    return pack_rtcp_packet(RTCP_RTPFB, self.fmt, payload)
",if d < 16 :,174
"def run() -> None:
    nonlocal state, timeout
    while True:
        if timeout > 0.0:
            disposed.wait(timeout)
        if disposed.is_set():
            return
        time: datetime = self.now
        state = action(state)
        timeout = seconds - (self.now - time).total_seconds()
",if timeout > 0.0 :,92
"def _get_host(self, array, connector, remote=False):
    """"""Return dict describing existing Purity host object or None.""""""
    if remote and array.get_rest_version() in SYNC_REPLICATION_REQUIRED_API_VERSIONS:
        hosts = array.list_hosts(remote=True)
    else:
        hosts = array.list_hosts()
    matching_hosts = []
    for host in hosts:
        for wwn in connector[""wwpns""]:
            if wwn.lower() in str(host[""wwn""]).lower():
                matching_hosts.append(host)
                break  # go to next host
    return matching_hosts
","if wwn . lower ( ) in str ( host [ ""wwn"" ] ) . lower ( ) :",165
"def validate_moment(self, moment: ""cirq.Moment""):
    super().validate_moment(moment)
    for op in moment.operations:
        if isinstance(op.gate, ops.CZPowGate):
            for other in moment.operations:
                if other is not op and self._check_if_exp11_operation_interacts(
                    cast(ops.GateOperation, op), cast(ops.GateOperation, other)
                ):
                    raise ValueError(""Adjacent Exp11 operations: {}."".format(moment))
","if isinstance ( op . gate , ops . CZPowGate ) :",142
"def construct_instances(self, row, keys=None):
    collected_models = {}
    for i, (key, constructor, attr, conv) in enumerate(self.column_map):
        if keys is not None and key not in keys:
            continue
        value = row[i]
        if key not in collected_models:
            collected_models[key] = constructor()
        instance = collected_models[key]
        if attr is None:
            attr = self.cursor.description[i][0]
        if conv is not None:
            value = conv(value)
        setattr(instance, attr, value)
    return collected_models
",if keys is not None and key not in keys :,167
"def test_all(self):
    expected = []
    blacklist = {""executable"", ""nobody_uid"", ""test""}
    for name in dir(server):
        if name.startswith(""_"") or name in blacklist:
            continue
        module_object = getattr(server, name)
        if getattr(module_object, ""__module__"", None) == ""http.server"":
            expected.append(name)
    self.assertCountEqual(server.__all__, expected)
","if getattr ( module_object , ""__module__"" , None ) == ""http.server"" :",111
"def _adjust_input(self):
    for i in range(len(self.block.ops)):
        current_op = self.block.ops[i]
        for input_arg in current_op.input_arg_names:
            if input_arg in self.input_map:
                current_op._rename_input(input_arg, self.input_map[input_arg])
",if input_arg in self . input_map :,99
"def __getitem__(self, cls):
    try:
        return dict.__getitem__(self, cls)
    except KeyError as e:
        if not hasattr(cls, ""__bases__""):
            cls = cls.__class__
        for b in reversed(cls.__bases__):
            try:
                retval = self[b]
                # this is why a cdict instance must never be modified after
                # the first lookup
                self[cls] = retval
                return retval
            except KeyError:
                pass
        raise e
","if not hasattr ( cls , ""__bases__"" ) :",146
"def before_read(self, parser, section, option, value):
    # If we're dealing with a quoted string as the interpolation value,
    # make sure we load and unquote it so we don't end up with '""value""'
    try:
        json_value = srsly.json_loads(value)
        if isinstance(json_value, str) and json_value not in JSON_EXCEPTIONS:
            value = json_value
    except Exception:
        pass
    return super().before_read(parser, section, option, value)
","if isinstance ( json_value , str ) and json_value not in JSON_EXCEPTIONS :",130
"def insert_files(self, urls, pos):
    """"""Not only images""""""
    image_extensions = ["".png"", "".jpg"", "".bmp"", "".gif""]
    for url in urls:
        if url.scheme() == ""file"":
            path = url.path()
            ext = os.path.splitext(path)[1]
            if os.path.exists(path) and ext in image_extensions:
                self._insert_image_from_path(path)
            else:
                self.parent.resource_edit.add_attach(path)
",if os . path . exists ( path ) and ext in image_extensions :,144
"def p_constant(self, p):
    """"""constant : PP_NUMBER""""""
    value = p[1].rstrip(""LlUu"")
    try:
        if value[:2] == ""0x"":
            value = int(value[2:], 16)
        elif value[0] == ""0"":
            value = int(value, 8)
        else:
            value = int(value)
    except ValueError:
        value = value.rstrip(""eEfF"")
        try:
            value = float(value)
        except ValueError:
            value = 0
    p[0] = ConstantExpressionNode(value)
","if value [ : 2 ] == ""0x"" :",163
"def _decode_pattern_list(data):
    rv = []
    contains_dict = False
    for item in data:
        if isinstance(item, list):
            item = _decode_pattern_list(item)
        elif isinstance(item, dict):
            item = _decode_pattern_dict(item)
            contains_dict = True
        rv.append(item)
    # avoid sorting if any element in the list is a dict
    if not contains_dict:
        rv = sorted(rv)
    return rv
","elif isinstance ( item , dict ) :",133
"def value(self, mode):
    v = super(mn_armt, self).value(mode)
    if mode == ""l"":
        out = []
        for x in v:
            if len(x) == 2:
                out.append(x[::-1])
            elif len(x) == 4:
                out.append(x[:2][::-1] + x[2:4][::-1])
        return out
    elif mode == ""b"":
        return [x for x in v]
    else:
        raise NotImplementedError(""bad attrib"")
",if len ( x ) == 2 :,145
"def _press_fire(self):
    fire_action = 1
    if (
        self.is_atari_env
        and self.env.unwrapped.get_action_meanings()[fire_action] == ""FIRE""
    ):
        self.current_ale_lives = self.env.unwrapped.ale.lives()
        self.step(fire_action)
        if self.done:
            self.reset_internal_state()
",if self . done :,115
"def update_fid_err_log(self, fid_err):
    """"""add an entry to the fid_err log""""""
    self.fid_err_log.append(fid_err)
    if self.write_to_file:
        if len(self.fid_err_log) == 1:
            mode = ""w""
        else:
            mode = ""a""
        f = open(self.fid_err_file, mode)
        f.write(""{}\n"".format(fid_err))
        f.close()
",if len ( self . fid_err_log ) == 1 :,135
"def _name(self, sender, short=True, full_email=False):
    words = re.sub('[""<>]', """", sender).split()
    nomail = [w for w in words if not ""@"" in w]
    if nomail:
        if short:
            if len(nomail) > 1 and nomail[0].lower() in self._NAME_TITLES:
                return nomail[1]
            return nomail[0]
        return "" "".join(nomail)
    elif words:
        if not full_email:
            return words[0].split(""@"", 1)[0]
        return words[0]
    return ""(nobody)""
",if not full_email :,168
"def zrx_order_to_json(order: Optional[ZeroExOrder]) -> Optional[Dict[str, any]]:
    if order is None:
        return None
    retval: Dict[str, any] = {}
    for key, value in order.items():
        if not isinstance(value, bytes):
            retval[key] = value
        else:
            retval[f""__binary__{key}""] = base64.b64encode(value).decode(""utf8"")
    return retval
","if not isinstance ( value , bytes ) :",120
"def _get_outfile(self):
    outfile = self.inputs.transformed_file
    if not isdefined(outfile):
        if self.inputs.inverse is True:
            if self.inputs.fs_target is True:
                src = ""orig.mgz""
            else:
                src = self.inputs.target_file
        else:
            src = self.inputs.source_file
        outfile = fname_presuffix(src, newpath=os.getcwd(), suffix=""_warped"")
    return outfile
",if self . inputs . inverse is True :,134
"def close(self):
    if self.changed:
        save = EasyDialogs.AskYesNoCancel(
            'Save window ""%s"" before closing?' % self.name, 1
        )
        if save > 0:
            self.menu_save()
        elif save < 0:
            return
    if self.parent.active == self:
        self.parent.active = None
    self.parent.updatemenubar()
    del self.ted
    self.do_postclose()
",if save > 0 :,126
"def step(self, action):
    """"""Repeat action, sum reward, and max over last observations.""""""
    total_reward = 0.0
    done = None
    for i in range(self._skip):
        obs, reward, done, info = self.env.step(action)
        if i == self._skip - 2:
            self._obs_buffer[0] = obs
        if i == self._skip - 1:
            self._obs_buffer[1] = obs
        total_reward += reward
        if done:
            break
    # Note that the observation on the done=True frame doesn't matter.
    max_frame = self._obs_buffer.max(axis=0)
    return max_frame, total_reward, done, info
",if done :,187
"def __isub__(self, other):
    """"""In-place subtraction of a matrix or scalar.""""""
    if isinstance(other, Matrix):
        if self.shape != other.shape:
            raise ValueError(""matrix shapes do not match"")
        for row_a, row_b in izip(self._data, other):
            for i in xrange(len(row_a)):
                row_a[i] -= row_b[i]
    else:
        for row in self._data:
            for i in xrange(len(row)):
                row[i] -= other
    return self
",if self . shape != other . shape :,154
"def check(self, count, count_v, enable, clock, reset, n):
    expect = 0
    yield reset.posedge
    self.assertEqual(count, expect)
    self.assertEqual(count, count_v)
    while 1:
        yield clock.posedge
        if enable:
            if expect == -n:
                expect = n - 1
            else:
                expect -= 1
        yield delay(1)
        # print ""%d count %s expect %s count_v %s"" % (now(), count, expect, count_v)
        self.assertEqual(count, expect)
        self.assertEqual(count, count_v)
",if expect == - n :,170
"def getmod(self, nm):
    mod = None
    for thing in self.path:
        if isinstance(thing, basestring):
            owner = self.shadowpath.get(thing, -1)
            if owner == -1:
                owner = self.shadowpath[thing] = self.__makeOwner(thing)
            if owner:
                mod = owner.getmod(nm)
        else:
            mod = thing.getmod(nm)
        if mod:
            break
    return mod
",if mod :,137
"def get_file_language(filename, text=None):
    """"""Get file language from filename""""""
    ext = osp.splitext(filename)[1]
    if ext.startswith("".""):
        ext = ext[1:]  # file extension with leading dot
    language = ext
    if not ext:
        if text is None:
            text, _enc = encoding.read(filename)
        for line in text.splitlines():
            if not line.strip():
                continue
            if line.startswith(""#!""):
                shebang = line[2:]
                if ""python"" in shebang:
                    language = ""python""
            else:
                break
    return language
","if line . startswith ( ""#!"" ) :",183
"def do_status(self, directory, path):
    with self._repo(directory) as repo:
        if path:
            path = os.path.join(directory, path)
            statuses = repo.status(include=path, all=True)
            for status, paths in statuses:
                if paths:
                    return self.statuses[status][0]
            return None
        else:
            resulting_status = 0
            for status, paths in repo.status(all=True):
                if paths:
                    resulting_status |= self.statuses[status][1]
            return self.repo_statuses_str[resulting_status]
",if paths :,181
"def _kill(proc):
    if proc is None:
        return
    if proc.stdout is not None:
        proc.stdout.close()
    if proc.stderr is not None:
        proc.stderr.close()
    if proc.returncode is None:
        try:
            proc.terminate()
        except:
            if proc.returncode is None:
                try:
                    proc.kill()
                except:
                    pass
",if proc . returncode is None :,125
"def decorated_function(*args, **kwargs):
    rv = f(*args, **kwargs)
    if isinstance(rv, flask.Response):
        try:
            result = etag
            if callable(result):
                result = result(rv)
            if result:
                rv.set_etag(result)
        except Exception:
            logging.getLogger(__name__).exception(
                ""Error while calculating the etag value for response {!r}"".format(rv)
            )
    return rv
",if callable ( result ) :,133
"def _list_shape_iter(shape):
    last_shape = _void
    for item in shape:
        if item is Ellipsis:
            if last_shape is _void:
                raise ValueError(
                    ""invalid shape spec: Ellipsis cannot be the"" ""first element""
                )
            while True:
                yield last_shape
        last_shape = item
        yield item
",if last_shape is _void :,109
"def delete_oidc_session_tokens(session):
    if session:
        if ""oidc_access_token"" in session:
            del session[""oidc_access_token""]
        if ""oidc_id_token"" in session:
            del session[""oidc_id_token""]
        if ""oidc_id_token_expiration"" in session:
            del session[""oidc_id_token_expiration""]
        if ""oidc_login_next"" in session:
            del session[""oidc_login_next""]
        if ""oidc_refresh_token"" in session:
            del session[""oidc_refresh_token""]
        if ""oidc_state"" in session:
            del session[""oidc_state""]
","if ""oidc_access_token"" in session :",179
"def calc_parity(sig, kind):
    if kind in (""zero"", ""none""):
        return C(0, 1)
    elif kind == ""one"":
        return C(1, 1)
    else:
        bits, _ = value_bits_sign(sig)
        even_parity = sum([sig[b] for b in range(bits)]) & 1
        if kind == ""odd"":
            return ~even_parity
        elif kind == ""even"":
            return even_parity
        else:
            assert False
","elif kind == ""even"" :",141
"def parse_cookies(cookies_headers):
    parsed = {}
    for cookie in cookies_headers:
        cookie = cookie.split("";"")
        for c in cookie:
            (name, value) = c.split(""="", 1)
            name = name.strip()
            value = value.strip()
            if name.lower() in _SPECIAL_COOKIE_NAMES:
                continue
            parsed[name] = value
    return parsed
",if name . lower ( ) in _SPECIAL_COOKIE_NAMES :,114
"def search_rotate(array, val):
    low, high = 0, len(array) - 1
    while low <= high:
        mid = (low + high) // 2
        if val == array[mid]:
            return mid
        if array[low] <= array[mid]:
            if array[low] <= val <= array[mid]:
                high = mid - 1
            else:
                low = mid + 1
        else:
            if array[mid] <= val <= array[high]:
                low = mid + 1
            else:
                high = mid - 1
    return -1
",if array [ low ] <= array [ mid ] :,166
"def _get_instance_attribute(
    self, attr, default=None, defaults=None, incl_metadata=False
):
    if self.instance is None or not hasattr(self.instance, attr):
        if incl_metadata and attr in self.parsed_metadata:
            return self.parsed_metadata[attr]
        elif defaults is not None:
            for value in defaults:
                if callable(value):
                    value = value()
                if value is not None:
                    return value
        return default
    return getattr(self.instance, attr)
",if incl_metadata and attr in self . parsed_metadata :,149
"def _handle_rate_limit(
    self, exception: RedditAPIException
) -> Optional[Union[int, float]]:
    for item in exception.items:
        if item.error_type == ""RATELIMIT"":
            amount_search = self._ratelimit_regex.search(item.message)
            if not amount_search:
                break
            seconds = int(amount_search.group(1))
            if ""minute"" in amount_search.group(2):
                seconds *= 60
            if seconds <= int(self.config.ratelimit_seconds):
                sleep_seconds = seconds + min(seconds / 10, 1)
                return sleep_seconds
    return None
","if item . error_type == ""RATELIMIT"" :",181
"def _split_values(self, value):
    # do the regex mojo here
    if not self.allowed_values:
        return ("""",)
    try:
        r = re.compile(self.allowed_values)
    except:
        print(self.allowed_values, file=sys.stderr)
        raise
    s = str(value)
    i = 0
    vals = []
    while True:
        m = r.search(s[i:])
        if m is None:
            break
        vals.append(m.group())
        delimiter = s[i : i + m.start()]
        if self.delimiter is None and delimiter != """":
            self.delimiter = delimiter
        i += m.end()
    return tuple(vals)
","if self . delimiter is None and delimiter != """" :",192
"def render(self, mode=""none""):
    """"""Renders the environment via matplotlib.""""""
    if mode == ""log"":
        self.logger.info(""Performance: "" + str(self._portfolio.performance))
    elif mode == ""chart"":
        if self.viewer is None:
            raise NotImplementedError()
        self.viewer.render(
            self.clock.step - 1, self._portfolio.performance, self._broker.trades
        )
",if self . viewer is None :,120
"def load_vocabulary(vocab_file):
    with open(vocab_file, ""r"") as f:
        vocabulary = []
        for line in f:
            line = line.strip()
            if "" "" in line:
                line = line.split("" "")[0]
            vocabulary.append(line)
        return vocabulary
","if "" "" in line :",88
"def test_confirm_extension_is_yml(self):
    files_with_incorrect_extensions = []
    for file in self.yield_next_rule_file_path(self.path_to_rules):
        file_name_and_extension = os.path.splitext(file)
        if len(file_name_and_extension) == 2:
            extension = file_name_and_extension[1]
            if extension != "".yml"":
                files_with_incorrect_extensions.append(file)
    self.assertEqual(
        files_with_incorrect_extensions,
        [],
        Fore.RED + ""There are rule files with extensions other than .yml"",
    )
","if extension != "".yml"" :",172
"def diff_from_indeces(self, indeces):
    rgroups = []
    with self._lock:
        for i in indeces:
            rgroup = self.events[i]
            if isinstance(rgroup, findlib2.ReplaceHitGroup):
                rgroups.append(rgroup)
    return ""\n"".join(rgroup.diff for rgroup in rgroups)
","if isinstance ( rgroup , findlib2 . ReplaceHitGroup ) :",100
"def deep_update(config, override_config):
    for k, v in override_config.items():
        if isinstance(v, Mapping):
            k_config = config.get(k, {})
            if isinstance(k_config, Mapping):
                v_config = deep_update(k_config, v)
                config[k] = v_config
            else:
                config[k] = v
        else:
            config[k] = override_config[k]
    return config
","if isinstance ( k_config , Mapping ) :",136
"def GetBoundingBoxMin(self):
    """"""Get the minimum bounding box.""""""
    x1, y1 = 10000, 10000
    x2, y2 = -10000, -10000
    for point in self._lineControlPoints:
        if point[0] < x1:
            x1 = point[0]
        if point[1] < y1:
            y1 = point[1]
        if point[0] > x2:
            x2 = point[0]
        if point[1] > y2:
            y2 = point[1]
    return x2 - x1, y2 - y1
",if point [ 1 ] < y1 :,158
"def insertChars(self, chars):
    tc = self.editBoxes[self.ind].textCursor()
    if tc.hasSelection():
        selection = tc.selectedText()
        if selection.startswith(chars) and selection.endswith(chars):
            if len(selection) > 2 * len(chars):
                selection = selection[len(chars) : -len(chars)]
                tc.insertText(selection)
        else:
            tc.insertText(chars + tc.selectedText() + chars)
    else:
        tc.insertText(chars)
",if selection . startswith ( chars ) and selection . endswith ( chars ) :,146
"def prepare_text(text, style):
    body = []
    for fragment, sty in parse_tags(text, style, subs.styles):
        fragment = fragment.replace(r""\h"", "" "")
        fragment = fragment.replace(r""\n"", ""\n"")
        fragment = fragment.replace(r""\N"", ""\n"")
        if sty.italic:
            fragment = ""<i>%s</i>"" % fragment
        if sty.underline:
            fragment = ""<u>%s</u>"" % fragment
        if sty.strikeout:
            fragment = ""<s>%s</s>"" % fragment
        body.append(fragment)
    return re.sub(""\n+"", ""\n"", """".join(body).strip())
",if sty . strikeout :,180
"def mFEBRUARY(
    self,
):
    try:
        _type = FEBRUARY
        _channel = DEFAULT_CHANNEL
        pass
        self.match(""feb"")
        alt14 = 2
        LA14_0 = self.input.LA(1)
        if LA14_0 == 114:
            alt14 = 1
        if alt14 == 1:
            pass
            self.match(""ruary"")
        self._state.type = _type
        self._state.channel = _channel
    finally:
        pass
",if LA14_0 == 114 :,147
"def test_calendar(self):
    subreddit = self.reddit.subreddit(pytest.placeholders.test_subreddit)
    widgets = subreddit.widgets
    with self.use_cassette(""TestSubredditWidgets.fetch_widgets""):
        calendar = None
        for widget in widgets.sidebar:
            if isinstance(widget, Calendar):
                calendar = widget
                break
        assert isinstance(calendar, Calendar)
        assert calendar == calendar
        assert calendar.id == calendar
        assert calendar in widgets.sidebar
        assert isinstance(calendar.configuration, dict)
        assert hasattr(calendar, ""requiresSync"")
        assert subreddit == calendar.subreddit
","if isinstance ( widget , Calendar ) :",187
"def count(num):
    cnt = 0
    for i in range(num):
        try:
            if i % 2:
                raise ValueError
            if i % 3:
                raise ArithmeticError(""1"")
        except Exception as e:
            cnt += 1
    return cnt
",if i % 3 :,80
"def pop(self):
    """"""Pop a nonterminal.  (Internal)""""""
    popdfa, popstate, popnode = self.stack.pop()
    newnode = self.convert(self.grammar, popnode)
    if newnode is not None:
        if self.stack:
            dfa, state, node = self.stack[-1]
            node[-1].append(newnode)
        else:
            self.rootnode = newnode
            try:
                self.rootnode.used_names = self.used_names
            except AttributeError:
                # Don't need this hack?
                pass
",if self . stack :,162
"def handle_custom_actions(self):
    for _, action in CustomAction.registry.items():
        if action.resource != self.resource:
            continue
        if action.action not in self.parser.choices:
            self.parser.add_parser(action.action, help="""")
        action(self.page).add_arguments(self.parser, self)
",if action . action not in self . parser . choices :,92
"def get_host_metadata(self):
    meta = {}
    if self.agent_url:
        try:
            resp = requests.get(self.agent_url, timeout=1).json().get(""config"", {})
            if ""Version"" in resp:
                meta[""nomad_version""] = resp.get(""Version"")
            if ""Region"" in resp:
                meta[""nomad_region""] = resp.get(""Region"")
            if ""Datacenter"" in resp:
                meta[""nomad_datacenter""] = resp.get(""Datacenter"")
        except Exception as ex:
            self.log.debug(""Error getting Nomad version: %s"" % str(ex))
    return meta
","if ""Datacenter"" in resp :",185
"def _source_tuple(af, address, port):
    # Make a high level source tuple, or return None if address and port
    # are both None
    if address or port:
        if address is None:
            if af == socket.AF_INET:
                address = ""0.0.0.0""
            elif af == socket.AF_INET6:
                address = ""::""
            else:
                raise NotImplementedError(f""unknown address family {af}"")
        return (address, port)
    else:
        return None
",if af == socket . AF_INET :,144
"def _evoke_request(cls):
    succeed = False
    with cls.LOCK:
        if len(cls.REQUESTING_STACK) > 0:
            resource, request_semaphore = cls.REQUESTING_STACK.pop()
            node = cls.check_availability(resource)
            if node is not None:
                cls.NODE_RESOURCE_MANAGER[node]._request(node, resource)
                logger.debug(""\nEvoking requesting resource {}"".format(resource))
                request_semaphore.release()
                succeed = True
            else:
                cls.REQUESTING_STACK.append((resource, request_semaphore))
                return
    if succeed:
        cls._evoke_request()
",if node is not None :,188
"def update_all_rhos(instances, scenario_tree, rho_value=None, rho_scale=None):
    assert not ((rho_value is not None) and (rho_scale is not None))
    for stage in scenario_tree._stages[:-1]:
        for tree_node in stage._tree_nodes:
            for scenario in tree_node._scenarios:
                rho = scenario._rho[tree_node._name]
                for variable_id in tree_node._variable_ids:
                    if rho_value is not None:
                        rho[variable_id] = rho_value
                    else:
                        rho[variable_id] *= rho_scale
",if rho_value is not None :,180
"def configured_request_log_handlers(config, prefix=""query_log"", default_logger=None):
    """"""Returns configured query loggers as defined in the `config`.""""""
    handlers = []
    for section in config.sections():
        if section.startswith(prefix):
            options = dict(config.items(section))
            type_ = options.pop(""type"")
            if type_ == ""default"":
                logger = default_logger or get_logger()
                handler = ext.request_log_handler(""default"", logger)
            else:
                handler = ext.request_log_handler(type_, **options)
            handlers.append(handler)
    return handlers
",if section . startswith ( prefix ) :,174
"def eval_dummy_genomes_ctrnn_bad(genomes, config):
    for genome_id, genome in genomes:
        net = neat.ctrnn.CTRNN.create(genome, config, 0.01)
        net.advance([0.5, 0.5, 0.5], 0.01, 0.05)
        if genome_id <= 150:
            genome.fitness = 0.0
        else:
            net.reset()
            genome.fitness = 1.0
",if genome_id <= 150 :,138
"def housenumber(self):
    if self.street:
        expression = r""\d+""
        pattern = re.compile(expression)
        match = pattern.search(self.street, re.UNICODE)
        if match:
            return match.group(0)
",if match :,69
"def func():
    end_received = False
    while True:
        for idx, q in enumerate(self._local_out_queues):
            data = q.get()
            q.task_done()
            if isinstance(data, EndSignal):
                end_received = True
                if idx > 0:
                    continue
            self._out_queue.put(data)
        if end_received:
            break
",if idx > 0 :,120
"def spin():
    """"""Wheeeee!""""""
    state = 0
    states = random.choice(spinners.spinners)
    while True:
        prefix = ""[%s] "" % _spinner_style(states[state])
        spinner_handle.update(prefix)
        state = (state + 1) % len(states)
        if stop.wait(0.1):
            break
",if stop . wait ( 0.1 ) :,103
"def _format_ip_address(container_group):
    """"""Format IP address.""""""
    ip_address = container_group.get(""ipAddress"")
    if ip_address:
        ports = ip_address[""ports""] or []
        if ip_address[""type""] == ""Private"":
            for container in container_group.get(""containers""):
                ports += container.get(""ports"")
        ports = "","".join(str(p[""port""]) for p in ports)
        return ""{0}:{1}"".format(ip_address.get(""ip""), ports)
    return None
","if ip_address [ ""type"" ] == ""Private"" :",141
"def check(self, count, count_v, enable, clock, reset, n):
    expect = 0
    yield reset.posedge
    self.assertEqual(count, expect)
    self.assertEqual(count, count_v)
    while 1:
        yield clock.posedge
        if enable:
            if expect == -n:
                expect = n - 1
            else:
                expect -= 1
        yield delay(1)
        # print ""%d count %s expect %s count_v %s"" % (now(), count, expect, count_v)
        self.assertEqual(count, expect)
        self.assertEqual(count, count_v)
",if enable :,170
"def _to_str(self, tokens: List[int]) -> str:
    pos = next(
        (idx for idx, x in enumerate(tokens) if x == self.vocab.eos_token_id), -1
    )
    if pos != -1:
        tokens = tokens[:pos]
    vocab_map = self.vocab.id_to_token_map_py
    words = [vocab_map[t] for t in tokens]
    if self.encoding is not None and self.perform_decode:
        if self.encoding == ""bpe"":
            words = self.bpe_decode(words)
        elif self.encoding == ""spm"":
            words = self.spm_decode(words)
    sentence = "" "".join(words)
    return sentence
","elif self . encoding == ""spm"" :",188
"def _iterate_files(self, files, root, include_checksums, relpath):
    file_list = {}
    for file in files:
        exclude = False
        # exclude defined filename patterns
        for pattern in S3Sync.exclude_files:
            if fnmatch.fnmatch(file, pattern):
                exclude = True
                break
        if not exclude:
            full_path = root + ""/"" + file
            if include_checksums:
                # get checksum
                checksum = self._hash_file(full_path)
            else:
                checksum = """"
            file_list[relpath + file] = [full_path, checksum]
    return file_list
",if include_checksums :,184
"def render(self, context):
    if self.user is None:
        entries = LogEntry.objects.all()
    else:
        user_id = self.user
        if not user_id.isdigit():
            user_id = context[self.user].pk
        entries = LogEntry.objects.filter(user__pk=user_id)
    context[self.varname] = entries.select_related(""content_type"", ""user"")[
        : int(self.limit)
    ]
    return """"
",if not user_id . isdigit ( ) :,126
"def pin_data_keys(self, session_id, data_keys, token, devices=None):
    if not devices:
        devices = functools.reduce(
            operator.or_,
            self._manager_ref.get_data_locations(session_id, data_keys),
            set(),
        )
    else:
        devices = self._normalize_devices(devices)
    pinned = set()
    for dev in devices:
        handler = self.get_storage_handler(dev)
        if not getattr(handler, ""_spillable"", False):
            continue
        keys = handler.pin_data_keys(session_id, data_keys, token)
        pinned.update(keys)
    return list(pinned)
","if not getattr ( handler , ""_spillable"" , False ) :",184
"def resolve(self, value: Optional[T]) -> T:
    v: Optional[Any] = value
    if value is None:
        t = os.environ.get(self.envvar)
        if self.type is bool and t:
            v = t in [""true"", ""True"", ""1"", ""yes""]
        elif self.type is str and t:
            v = t
        elif t:
            v = ast.literal_eval(t) if t is not None else None
    if v is None:
        v = self.default
    return v
",elif t :,144
"def remove(self, *objs):
    val = getattr(instance, rel_field.rel.get_related_field().attname)
    for obj in objs:
        # Is obj actually part of this descriptor set?
        if getattr(obj, rel_field.attname) == val:
            setattr(obj, rel_field.name, None)
            obj.save()
        else:
            raise rel_field.rel.to.DoesNotExist(
                ""%r is not related to %r."" % (obj, instance)
            )
","if getattr ( obj , rel_field . attname ) == val :",137
"def generate_segment_memory(chart_type, race_configs, environment):
    structures = []
    for race_config in race_configs:
        if ""segment_memory"" in race_config.charts:
            title = chart_type.format_title(
                environment,
                race_config.track,
                es_license=race_config.es_license,
                suffix=""%s-segment-memory"" % race_config.label,
            )
            chart = chart_type.segment_memory(title, environment, race_config)
            if chart:
                structures.append(chart)
    return structures
",if chart :,168
"def comment_multiline(self, text, delimiter_end, delimiter_start, style):
    """"""Process the beggining and end of a multiline comment.""""""
    startIndex = 0
    if self.previousBlockState() != 1:
        startIndex = delimiter_start.indexIn(text)
    while startIndex >= 0:
        endIndex = delimiter_end.indexIn(text, startIndex)
        commentLength = 0
        if endIndex == -1:
            self.setCurrentBlockState(1)
            commentLength = len(text) - startIndex
        else:
            commentLength = endIndex - startIndex + delimiter_end.matchedLength()
        self.setFormat(startIndex, commentLength, style)
        startIndex = delimiter_start.indexIn(text, startIndex + commentLength)
",if endIndex == - 1 :,199
"def getLatestFile(self):
    highestNsp = None
    highestNsx = None
    for nsp in self.getFiles():
        try:
            if nsp.path.endswith("".nsx""):
                if not highestNsx or int(nsp.version) > int(highestNsx.version):
                    highestNsx = nsp
            else:
                if not highestNsp or int(nsp.version) > int(highestNsp.version):
                    highestNsp = nsp
        except BaseException:
            pass
    return highestNsp or highestNsx
",if not highestNsp or int ( nsp . version ) > int ( highestNsp . version ) :,152
"def handle(self, msg):
    self._mic.send(msg)
    for calculate_seed, make_delegate, dict in self._delegate_records:
        id = calculate_seed(msg)
        if id is None:
            continue
        elif isinstance(id, collections.Hashable):
            if id not in dict or not dict[id].is_alive():
                d = make_delegate((self, msg, id))
                d = self._ensure_startable(d)
                dict[id] = d
                dict[id].start()
        else:
            d = make_delegate((self, msg, id))
            d = self._ensure_startable(d)
            d.start()
",if id is None :,192
"def _build_pcf(named_sc, named_pc):
    r = """"
    for sig, pins, others, resname in named_sc:
        if len(pins) > 1:
            for bit, pin in enumerate(pins):
                r += ""set_io {}[{}] {}\n"".format(sig, bit, pin)
        else:
            r += ""set_io {} {}\n"".format(sig, pins[0])
    if named_pc:
        r += ""\n"" + ""\n\n"".join(named_pc)
    return r
",if len ( pins ) > 1 :,146
"def __init__(self, profile, report_dir=None, timestamp=None):
    # self.metadata = {}
    self.report_dir = report_dir if report_dir else DEFAULT_REPORT_DIR
    self.profile = profile.replace(""/"", ""_"").replace(""\\"", ""_"")  # Issue 111
    self.current_time = datetime.datetime.now(dateutil.tz.tzlocal())
    if timestamp != False:
        self.timestamp = (
            self.current_time.strftime(""%Y-%m-%d_%Hh%M%z"")
            if not timestamp
            else timestamp
        )
",if not timestamp,144
"def _convert_params_to_v3(params):
    for k, v in OLD_TO_NEW_PARAMS.items():
        if k in params:
            msg = Message.WARN_PARAMS_NOT_SUPPORTED % (k, v)
            warnings.warn(msg, DeprecationWarning)
            # update to the new query param if not specified already
            if v not in params:
                params[v] = params.pop(k)
",if v not in params :,114
"def rollup_logical(counter, lookup, logical_keys):
    logical = Counter()
    for k, v in counter.items():
        # TODO: eek, do a fallback of some kind
        if k not in lookup:
            logical[(""unknown"", k)] = v
            continue
        linfo = lookup[k]
        lkey = tuple(linfo.get(lk, ""unknown"") for lk in logical_keys)
        logical[lkey] += v
    return logical
",if k not in lookup :,123
"def assert_summary_equals(self, records, tag, step, value):
    for record in records[1:]:
        if record.summary.value[0].tag != tag:
            continue
        if record.step != step:
            continue
        self.assertEqual(value, tf.make_ndarray(record.summary.value[0].tensor))
        return
    self.fail(""Could not find record for tag {} and step {}"".format(tag, step))
",if record . step != step :,114
"def get_name_from_types(types: Iterable[Union[Type, StrawberryUnion]]):
    names = []
    for type_ in types:
        if isinstance(type_, StrawberryUnion):
            return type_.name
        elif hasattr(type_, ""_type_definition""):
            name = capitalize_first(type_._type_definition.name)
        else:
            name = capitalize_first(type_.__name__)
        names.append(name)
    return """".join(names)
","elif hasattr ( type_ , ""_type_definition"" ) :",131
"def parseBamPEFDistributionFile(self, f):
    d = dict()
    lastsample = []
    for line in f[""f""].splitlines():
        cols = line.rstrip().split(""\t"")
        if cols[0] == ""#bamPEFragmentSize"":
            continue
        elif cols[0] == ""Size"":
            continue
        else:
            s_name = self.clean_s_name(cols[2].rstrip().split(""/"")[-1], f[""root""])
            if s_name != lastsample:
                d[s_name] = dict()
                lastsample = s_name
            d[s_name].update({self._int(cols[0]): self._int(cols[1])})
    return d
","if cols [ 0 ] == ""#bamPEFragmentSize"" :",194
"def read_output(meteor_output_path, n_repeats):
    n_combinations = math.factorial(n_repeats) / (
        math.factorial(2) * math.factorial(n_repeats - 2)
    )
    raw_scores = []
    average_scores = []
    for line in open(meteor_output_path):
        if not line.startswith(""Segment ""):
            continue
        score = float(line.strip().split(""\t"")[1])
        raw_scores.append(score)
        if len(raw_scores) == n_combinations:
            average_scores.append(sum(raw_scores) / n_combinations)
            raw_scores = []
    os.remove(meteor_output_path)
    return average_scores
",if len ( raw_scores ) == n_combinations :,198
"def get_new_pids(self):
    if not self.need_poll():
        return
    for process in psutil.process_iter():
        info = process.as_dict([""create_time"", ""pid"", ""name"", ""exe""])
        pid = info[""pid""]
        if pid not in self.pids or self.pids[pid] == info[""create_time""]:
            for name in self.names:
                if name.match(info[""name""]) or name.match(info[""exe""]):
                    yield pid
                    self.pids[pid] = info[""create_time""]
","if name . match ( info [ ""name"" ] ) or name . match ( info [ ""exe"" ] ) :",153
"def _Attribute(self, node):
    if not isinstance(node.ctx, ast.Store):
        scope = self.scope.get_inner_scope_for_line(node.lineno)
        pyname = evaluate.eval_node(scope, node.value)
        if pyname is not None and pyname.get_object() != pyobjects.get_unknown():
            if node.attr not in pyname.get_object():
                self._add_error(node, ""Unresolved attribute"")
    ast.walk(node.value, self)
",if pyname is not None and pyname . get_object ( ) != pyobjects . get_unknown ( ) :,136
"def _init_neighbor(neighbor):
    families = neighbor.families()
    for change in neighbor.changes:
        if change.nlri.family() in families:
            # This add the family to neighbor.families()
            neighbor.rib.outgoing.add_to_rib_watchdog(change)
    for message in messages:
        if message.family() in families:
            if message.name == ""ASM"":
                neighbor.asm[message.family()] = message
            else:
                neighbor.messages.append(message)
    self.neighbors[neighbor.name()] = neighbor
","if message . name == ""ASM"" :",152
"def date_match(self, date1, date2):
    if date1.is_empty() or date2.is_empty():
        return 0
    if date1.is_equal(date2):
        return 1
    if date1.is_compound() or date2.is_compound():
        return self.range_compare(date1, date2)
    if date1.get_year() == date2.get_year():
        if date1.get_month() == date2.get_month():
            return 0.75
        if not date1.get_month_valid() or not date2.get_month_valid():
            return 0.75
        else:
            return -1
    else:
        return -1
",if date1 . get_month ( ) == date2 . get_month ( ) :,189
"def del_var_history(self, var, f=None, line=None):
    """"""If file f and line are not given, the entire history of var is deleted""""""
    if var in self.variables:
        if f and line:
            self.variables[var] = [
                x for x in self.variables[var] if x[""file""] != f and x[""line""] != line
            ]
        else:
            self.variables[var] = []
",if f and line :,120
"def test_certs(self):
    self.assertTrue(len(self.regions) > 0)
    for region in self.regions:
        special_access_required = False
        for snippet in (""gov"", ""cn-""):
            if snippet in region.name:
                special_access_required = True
                break
        try:
            c = region.connect()
            self.sample_service_call(c)
        except:
            # This is bad (because the SSL cert failed). Re-raise the
            # exception.
            if not special_access_required:
                raise
",if not special_access_required :,161
"def convert_encoder_layer(opus_dict, layer_prefix: str, converter: dict):
    sd = {}
    for k in opus_dict:
        if not k.startswith(layer_prefix):
            continue
        stripped = remove_prefix(k, layer_prefix)
        v = opus_dict[k].T  # besides embeddings, everything must be transposed.
        sd[converter[stripped]] = torch.tensor(v).squeeze()
    return sd
",if not k . startswith ( layer_prefix ) :,117
"def test_sequence(self, sequence):
    for test in sequence:
        if isinstance(test, tuple):
            test, kwargs = test
        else:
            kwargs = {}
        self.do_check(test, **kwargs)
        if test == ExpectedError:
            return False
    return True
","if isinstance ( test , tuple ) :",81
"def make_table(grid):
    max_cols = [
        max(out)
        for out in map(list, zip(*[[len(item) for item in row] for row in grid]))
    ]
    rst = table_div(max_cols, 1)
    for i, row in enumerate(grid):
        header_flag = False
        if i == 0 or i == len(grid) - 1:
            header_flag = True
        rst += normalize_row(row, max_cols)
        rst += table_div(max_cols, header_flag)
    return rst
",if i == 0 or i == len ( grid ) - 1 :,147
"def test_float_overflow(self):
    import sys
    big_int = int(sys.float_info.max) * 2
    for t in float_types + [c_longdouble]:
        self.assertRaises(OverflowError, t, big_int)
        if hasattr(t, ""__ctype_be__""):
            self.assertRaises(OverflowError, t.__ctype_be__, big_int)
        if hasattr(t, ""__ctype_le__""):
            self.assertRaises(OverflowError, t.__ctype_le__, big_int)
","if hasattr ( t , ""__ctype_le__"" ) :",131
"def _process_folder(config, folder, cache, output):
    if not os.path.isdir(folder):
        raise ConanException(""No such directory: '%s'"" % str(folder))
    if config.source_folder:
        folder = os.path.join(folder, config.source_folder)
    for root, dirs, files in walk(folder):
        dirs[:] = [d for d in dirs if d != "".git""]
        if "".git"" in root:
            continue
        for f in files:
            _process_file(root, f, config, cache, output, folder)
","if "".git"" in root :",150
"def setChanged(self, c, changed):
    # Find the tab corresponding to c.
    dw = c.frame.top  # A DynamicWindow
    i = self.indexOf(dw)
    if i < 0:
        return
    s = self.tabText(i)
    s = g.u(s)
    if len(s) > 2:
        if changed:
            if not s.startswith(""* ""):
                title = ""* "" + s
                self.setTabText(i, title)
        else:
            if s.startswith(""* ""):
                title = s[2:]
                self.setTabText(i, title)
",if changed :,172
"def dump_metrics(self):
    metrics = self._registry.dump_metrics()
    # Filter out min and max if there have been no samples.
    for metric in metrics.itervalues():
        if metric.get(""count"") == 0:
            if ""min"" in metric:
                metric[""min""] = 0.0
            if ""max"" in metric:
                metric[""max""] = 0.0
    return metrics
","if ""min"" in metric :",109
"def ref_max_pooling_3d(x, kernel, stride, ignore_border, pad):
    y = []
    for xx in x.reshape((-1,) + x.shape[-4:]):
        if xx.ndim == 3:
            xx = xx[np.newaxis]
        y += [
            refs.pooling_3d(xx, ""max"", kernel, stride, pad, ignore_border)[np.newaxis]
        ]
    y = np.vstack(y)
    if x.ndim == 3:
        y = np.squeeze(y, 1)
    return y.reshape(x.shape[:-4] + y.shape[1:])
",if xx . ndim == 3 :,160
"def reader_():
    with open(file_list) as flist:
        lines = [line.strip() for line in flist]
        if shuffle:
            random.shuffle(lines)
        for line in lines:
            file_path = line.strip()
            yield [file_path]
",if shuffle :,79
"def _sql_like_to_regex(pattern, escape):
    cur_i = 0
    pattern_length = len(pattern)
    while cur_i < pattern_length:
        nxt_i = cur_i + 1
        cur = pattern[cur_i]
        nxt = pattern[nxt_i] if nxt_i < pattern_length else None
        skip = 1
        if nxt is not None and escape is not None and cur == escape:
            yield nxt
            skip = 2
        elif cur == ""%"":
            yield "".*""
        elif cur == ""_"":
            yield "".""
        else:
            yield cur
        cur_i += skip
","elif cur == ""_"" :",169
"def gaussian(N=1000, draw=True, show=True, seed=42, color=None, marker=""sphere""):
    """"""Show N random gaussian distributed points using a scatter plot.""""""
    import ipyvolume as ipv
    rng = np.random.RandomState(seed)  # pylint: disable=no-member
    x, y, z = rng.normal(size=(3, N))
    if draw:
        if color:
            mesh = ipv.scatter(x, y, z, marker=marker, color=color)
        else:
            mesh = ipv.scatter(x, y, z, marker=marker)
        if show:
            # ipv.squarelim()
            ipv.show()
        return mesh
    else:
        return x, y, z
",if color :,191
"def _delete_keys(bucket, keys):
    for name in keys:
        while True:
            try:
                k = boto.s3.connection.Key(bucket, name)
                bucket.delete_key(k)
            except boto.exception.S3ResponseError as e:
                if e.status == 404:
                    # Key is already not present.  Continue the
                    # deletion iteration.
                    break
                raise
            else:
                break
",if e . status == 404 :,141
"def detect(self):
    hardware = self.middleware.call_sync(""failover.hardware"")
    if hardware == ""ECHOSTREAM"":
        proc = subprocess.check_output(
            '/usr/sbin/pciconf -lv | grep ""card=0xa01f8086 chip=0x10d38086""',
            shell=True,
            encoding=""utf8"",
        )
        if proc:
            return [proc.split(""@"")[0]]
    if hardware in (""ECHOWARP"", ""PUMA""):
        return [""ntb0""]
    if hardware == ""BHYVE"":
        return [""vtnet1""]
    if hardware == ""SBB"":
        return [""ix0""]
    if hardware == ""ULTIMATE"":
        return [""igb1""]
    return []
",if proc :,199
"def check_config(param):
    fileopen = open(""/etc/setoolkit/set.config"", ""r"")
    for line in fileopen:
        line = line.rstrip()
        # print line
        # if the line starts with the param we want then we are set, otherwise
        # if it starts with a # then ignore
        if line.startswith(param) != ""#"":
            if line.startswith(param):
                line = line.rstrip()
                # remove any quotes or single quotes
                line = line.replace('""', """")
                line = line.replace(""'"", """")
                line = line.split(""="", 1)
                return line[1]
",if line . startswith ( param ) :,176
"def put(self, s):
    """"""Put string s to self.outputFile. All output eventually comes here.""""""
    # Improved code: self.outputFile (a cStringIO object) always exists.
    if s:
        self.putCount += 1
        if not g.isPython3:
            s = g.toEncodedString(s, self.leo_file_encoding, reportErrors=True)
        self.outputFile.write(s)
",if not g . isPython3 :,110
"def get_system_prop_font(self):
    """"""Look up the system font""""""
    if self.system_prop_font is not None:
        return self.system_prop_font
    elif ""org.gnome.desktop.interface"" not in Gio.Settings.list_schemas():
        return
    else:
        gsettings = Gio.Settings.new(""org.gnome.desktop.interface"")
        value = gsettings.get_value(""font-name"")
        if value:
            self.system_prop_font = value.get_string()
        else:
            self.system_prop_font = ""Sans 10""
        return self.system_prop_font
",if value :,170
"def _setoct(self, octstring):
    """"""Reset the bitstring to have the value given in octstring.""""""
    octstring = tidy_input_string(octstring)
    # remove any 0o if present
    octstring = octstring.replace(""0o"", """")
    binlist = []
    for i in octstring:
        try:
            if not 0 <= int(i) < 8:
                raise ValueError
            binlist.append(OCT_TO_BITS[int(i)])
        except ValueError:
            raise CreationError(""Invalid symbol '{0}' in oct initialiser."", i)
    self._setbin_unsafe("""".join(binlist))
",if not 0 <= int ( i ) < 8 :,166
"def group(self, resources):
    groups = {}
    for r in resources:
        v = self._value_to_sort(self.group_by, r)
        vstr = str(v)
        if vstr not in groups:
            groups[vstr] = {""sortkey"": v, ""resources"": []}
        groups[vstr][""resources""].append(r)
    return groups
",if vstr not in groups :,98
"def rd(line_number, row, col, key, default=None):
    """"""Return Row data by column name""""""
    if key in col:
        if col[key] >= len(row):
            LOG.warning(""missing '%s, on line %d"" % (key, line_number))
            return default
        retval = row[col[key]].strip()
        if retval == """":
            return default
        else:
            return retval
    else:
        return default
","if retval == """" :",125
"def _run(self):
    while True:
        tup = self._pop()
        if tup is None:
            return
        method_name, kwargs, msg = tup
        try:
            super(SerializedInvoker, self).invoke(method_name, kwargs, msg)
        except mitogen.core.CallError:
            e = sys.exc_info()[1]
            LOG.warning(""%r: call error: %s: %s"", self, msg, e)
            msg.reply(e)
        except Exception:
            LOG.exception(""%r: while invoking %s()"", self, method_name)
            msg.reply(mitogen.core.Message.dead())
",if tup is None :,179
"def raises(except_cls, message=None):
    try:
        yield
        success = False
    except except_cls as e:
        if message:
            assert re.search(message, compat.text_type(e), re.UNICODE), ""%r !~ %s"" % (
                message,
                e,
            )
            print(compat.text_type(e).encode(""utf-8""))
        success = True
    # assert outside the block so it works for AssertionError too !
    assert success, ""Callable did not raise an exception""
",if message :,145
"def buttonClicked(self, button):
    role = self.buttonBox.buttonRole(button)
    if role == QDialogButtonBox.ResetRole:
        current_tab = self.tabwidget.currentWidget()
        section_to_update = Sections.ALL
        if current_tab is self.page_general:
            section_to_update = Sections.GENERAL
        if current_tab is self.page_display:
            section_to_update = Sections.DISPLAY
        self.resetToDefaults(section_to_update)
",if current_tab is self . page_display :,137
"def make_range_list(*values):
    ranges = []
    for v in values:
        if isinstance(v, int):
            val_node = plural.value_node(v)
            ranges.append((val_node, val_node))
        else:
            assert isinstance(v, tuple)
            ranges.append((plural.value_node(v[0]), plural.value_node(v[1])))
    return plural.range_list_node(ranges)
","if isinstance ( v , int ) :",121
"def __in_comment(self):
    if self.highlighter:
        current_color = self.__get_current_color()
        comment_color = self.highlighter.get_color_name(""comment"")
        if current_color == comment_color:
            return True
        else:
            return False
    else:
        return False
",if current_color == comment_color :,90
"def __str__(self):
    """"""Constructs to variable list output used in cron jobs""""""
    ret = []
    for key, value in self.items():
        if self.previous:
            if self.previous.all().get(key, None) == value:
                continue
        if "" "" in unicode(value) or value == """":
            value = '""%s""' % value
        ret.append(""%s=%s"" % (key, unicode(value)))
    ret.append("""")
    return ""\n"".join(ret)
","if "" "" in unicode ( value ) or value == """" :",132
"def _on_config_changed(changed_name: str) -> None:
    """"""Call config_changed hooks if the config changed.""""""
    for mod_info in _module_infos:
        if mod_info.skip_hooks:
            continue
        for option, hook in mod_info.config_changed_hooks:
            if option is None:
                hook()
            else:
                cfilter = config.change_filter(option)
                cfilter.validate()
                if cfilter.check_match(changed_name):
                    hook()
",if cfilter . check_match ( changed_name ) :,151
"def __init__(self, transcripts, vocab=None, unknown=None, *args, **kwargs):
    """"""Creates a new raw transcript source.""""""
    super().__init__(*args, **kwargs)
    self.transcripts = transcripts
    self.indices = numpy.arange(len(self))
    self.vocab = self.make_vocab(vocab)
    if unknown is None:
        self.unknown = self.unknown_index = None
    else:
        self.unknown_index = self.vocab.get(unknown)
        if self.unknown_index is None:
            raise ValueError(
                'The ""unknown"" vocabulary word must be '
                ""part of the vocabulary itself.""
            )
        self.unknown = unknown
",if self . unknown_index is None :,182
"def load_info(cls, path, reset_paths=False, load_model_if_required=True):
    load_path = path + cls.trainer_info_name
    try:
        return load_pkl.load(path=load_path)
    except:
        if load_model_if_required:
            trainer = cls.load(path=path, reset_paths=reset_paths)
            return trainer.get_info()
        else:
            raise
",if load_model_if_required :,120
"def createActions(actions, target):
    # actions = [(name, shortcut, icon, desc, func)]
    for name, shortcut, icon, desc, func in actions:
        action = QAction(target)
        if icon:
            action.setIcon(icon)
        if shortcut:
            action.setShortcut(shortcut)
        action.setText(desc)
        action.triggered.connect(func)
        setattr(target, name, action)
",if icon :,114
"def load_user_logins(self, key, dates, timestamps, size_threshold=None):
    date_bucket = {}
    for user_data in self.fetch_user_table():
        if size_threshold is not None and user_data[1] < size_threshold:
            continue
        # note: ts should already be utc!
        dt = datetime.fromtimestamp(user_data[6] / 1000)
        dt = dt.date().isoformat()
        date_bucket[dt] = date_bucket.get(dt, 0) + 1
    datapoints = []
    for dt, ts in zip(dates, timestamps):
        count = date_bucket.get(dt, 0)
        datapoints.append((count, ts))
    return {""target"": key, ""datapoints"": datapoints}
",if size_threshold is not None and user_data [ 1 ] < size_threshold :,194
"def apply_batch(it):
    batch = []
    for item in it:
        if isinstance(item, _NextValueNotReady):
            yield item
        else:
            batch.append(item)
            if len(batch) >= n:
                yield batch
                batch = []
    if batch:
        yield batch
","if isinstance ( item , _NextValueNotReady ) :",92
"def convert_tomlkit_table(section):
    if isinstance(section, tomlkit.items.Table):
        body = section.value._body
    else:
        body = section._body
    for key, value in body:
        if not key:
            continue
        if hasattr(value, ""keys"") and not isinstance(value, tomlkit.items.InlineTable):
            table = tomlkit.inline_table()
            table.update(value.value)
            section[key.key] = table
","if hasattr ( value , ""keys"" ) and not isinstance ( value , tomlkit . items . InlineTable ) :",134
"def _do_ssl_handshake(self):
    try:
        self.socket.do_handshake()
    except ssl.SSLError as err:
        if err.args[0] in (ssl.SSL_ERROR_WANT_READ, ssl.SSL_ERROR_WANT_WRITE):
            return
        elif err.args[0] == ssl.SSL_ERROR_EOF:
            return self.handle_close()
        raise
    except OSError as err:
        if err.args[0] == errno.ECONNABORTED:
            return self.handle_close()
    else:
        self._ssl_accepting = False
",elif err . args [ 0 ] == ssl . SSL_ERROR_EOF :,161
"def get_filechanges(repo, revision, parents, mleft):
    """"""Given some repository and revision, find all changed/deleted files.""""""
    l, c, r = [], [], []
    for p in parents:
        if p < 0:
            continue
        mright = revsymbol(repo, b""%d"" % p).manifest()
        l, c, r = split_dict(mleft, mright, l, c, r)
    l.sort()
    c.sort()
    r.sort()
    return l, c, r
",if p < 0 :,134
"def close_share(self, share_name):
    c = await run(
        [SMBCmd.SMBCONTROL.value, ""smbd"", ""close-share"", share_name], check=False
    )
    if c.returncode != 0:
        if ""Can't find pid"" in c.stderr.decode():
            # smbd is not running. Don't log error message.
            return
        self.logger.warn(
            ""Failed to close smb share [%s]: [%s]"",
            share_name,
            c.stderr.decode().strip(),
        )
","if ""Can't find pid"" in c . stderr . decode ( ) :",152
"def execute(self, context):
    if self.tree_name:
        ng = bpy.data.node_groups.get(self.tree_name)
        if ng:
            apply_theme(ng)
        else:
            return {""CANCELLED""}
    else:
        apply_theme()
    return {""FINISHED""}
",if ng :,86
"def apply(self, db, object):
    if not self.source_handle:
        if self.nosource:
            # check whether the citation list is empty as a proxy for
            # there being no sources
            return len(object.get_all_citation_lists()) == 0
        else:
            return False
    else:
        for citation_handle in object.get_all_citation_lists():
            citation = db.get_citation_from_handle(citation_handle)
            if citation.get_reference_handle() == self.source_handle:
                return True
        return False
",if self . nosource :,164
"def get_data_dir():
    """"""Get the directory path for flit user data files.""""""
    home = os.path.realpath(os.path.expanduser(""~""))
    if sys.platform == ""darwin"":
        d = Path(home, ""Library"")
    elif os.name == ""nt"":
        appdata = os.environ.get(""APPDATA"", None)
        if appdata:
            d = Path(appdata)
        else:
            d = Path(home, ""AppData"", ""Roaming"")
    else:
        # Linux, non-OS X Unix, AIX, etc.
        xdg = os.environ.get(""XDG_DATA_HOME"", None)
        d = Path(xdg) if xdg else Path(home, "".local/share"")
    return d / ""flit""
",if appdata :,195
"def wait_for_service(name, timeout=200):
    start = time.time()
    while True:
        status = win32serviceutil.QueryServiceStatus(name)
        if status[1] == win32service.SERVICE_STOPPED:
            break
        if time.time() - start > timeout:
            raise TimeoutError(
                ""Timeout waiting for service""
            )  # pylint: disable=undefined-variable
        time.sleep(0.3)
",if time . time ( ) - start > timeout :,121
"def get_selection(self):
    if self.uistate[""selection""] == ""all"":
        return AllPages(self.notebook)
    else:
        path = self.uistate[""selected_page""]
        if self.uistate[""selection_recursive""]:
            return SubPages(self.notebook, path)
        else:
            return SinglePage(self.notebook, path)
","if self . uistate [ ""selection_recursive"" ] :",100
"def test_repeated_edges(self):
    graph_size = 20
    for _ in range(20):
        graph = Graph.graph(graph_size, int(graph_size * 2), repeated_edges=True)
        edges = [(e.start, e.end) for e in graph.iterate_edges()]
        has_repeated_edges = len(edges) > len(set(edges))
        if has_repeated_edges:
            break
    self.assertTrue(has_repeated_edges)
    for _ in range(10):
        graph = Graph.graph(graph_size, int(graph_size * 2), repeated_edges=False)
        edges = list(graph.iterate_edges())
        self.assertEqual(len(edges), len(set(edges)))
",if has_repeated_edges :,189
"def cs(self):
    """"""ConfigSpace representation of this search space.""""""
    cs = CS.ConfigurationSpace()
    for k, v in self.kwvars.items():
        if isinstance(v, NestedSpace):
            _add_cs(cs, v.cs, k)
        elif isinstance(v, Space):
            hp = v.get_hp(name=k)
            _add_hp(cs, hp)
        else:
            _rm_hp(cs, k)
    return cs
","if isinstance ( v , NestedSpace ) :",129
"def packet_handler(Packet):
    global add_new_line
    if Packet.haslayer(ICMP):
        Data = Packet.getlayer(ICMP).getlayer(Raw)
        exfiltrated_data = Data.load[int(exfiltration_length) :].replace(
            exfiltration_length * ""\n"", ""\n""
        )
        if exfiltrated_data.endswith(""\n""):
            add_new_line = False
        sys.stdout.write(exfiltrated_data)
        sys.stdout.flush()
","if exfiltrated_data . endswith ( ""\n"" ) :",145
"def acquire(self, *, wait=False):
    if not wait and self.value <= 0:
        # signal that we're not acquiring
        return False
    while self.value <= 0:
        future = self.loop.create_future()
        self._waiters.append(future)
        try:
            await future
        except:
            future.cancel()
            if self.value > 0 and not future.cancelled():
                self.wake_up()
            raise
    self.value -= 1
    return True
",if self . value > 0 and not future . cancelled ( ) :,142
"def handle_events(self, events):
    for event in events:
        if event == WindowEvent.SCREEN_RECORDING_TOGGLE:
            self.recording ^= True
            if not self.recording:
                self.save()
            else:
                logger.info(""ScreenRecorder started"")
            break
    return events
",if event == WindowEvent . SCREEN_RECORDING_TOGGLE :,92
"def _register_for_operations(config, session, service_name):
    # There's certainly a tradeoff for registering the retry config
    # for the operations when the service is created.  In practice,
    # there aren't a whole lot of per operation retry configs so
    # this is ok for now.
    for key in config:
        if key == ""__default__"":
            continue
        handler = retryhandler.create_retry_handler(config, key)
        unique_id = ""retry-config-%s-%s"" % (service_name, key)
        session.register(
            ""needs-retry.%s.%s"" % (service_name, key), handler, unique_id=unique_id
        )
","if key == ""__default__"" :",176
"def showTicks(self, show=True):
    for tick in self.ticks.keys():
        if show:
            tick.show()
            orig = getattr(self, ""_allowAdd_backup"", None)
            if orig:
                self.allowAdd = orig
        else:
            self._allowAdd_backup = self.allowAdd
            self.allowAdd = False  # block tick creation
            tick.hide()
",if orig :,116
"def _has_cycle(self, node, visited, visit_stack):
    self.last_visited_node = node
    self.path.append(node)
    visited[node] = True
    visit_stack[node] = True
    for neighbor in self.graph[node]:
        if not visited[neighbor]:
            if self._has_cycle(neighbor, visited, visit_stack):
                return True
        elif visit_stack[neighbor]:
            self.path.append(neighbor)
            return True
    self.path.remove(node)
    visit_stack[node] = False
    return False
","if self . _has_cycle ( neighbor , visited , visit_stack ) :",154
"def get_project_list(exclude_default=False):
    """"""get_project_list - get list of all projects""""""
    projects_path = __project__.get_projects_path()
    project_list = []
    if os.path.exists(projects_path):
        for project in os.listdir(projects_path):
            project_path = os.path.join(projects_path, project)
            if os.path.isdir(project_path):
                project_list.append(project)
    if exclude_default:
        pass
    else:
        project_list.append(""default"")
    return sorted(project_list)
",if os . path . isdir ( project_path ) :,161
"def split(self, chunksize):
    modulus_map = {
        4: 256,
        5: 10,
        8: 100,
    }
    chunks, ip = self.preprocess(chunksize)
    ret = """"
    for i in range(len(chunks)):
        ip_part = compat_str(ip[i] % modulus_map[chunksize]) if i < 4 else """"
        if chunksize == 8:
            ret += ip_part + chunks[i]
        else:
            ret += chunks[i] + ip_part
    self.target = ret
",if chunksize == 8 :,143
"def DepsToModules(deps, prefix, suffix):
    modules = []
    for filepath in deps:
        filename = os.path.basename(filepath)
        if filename.startswith(prefix) and filename.endswith(suffix):
            modules.append(filename[len(prefix) : -len(suffix)])
    return modules
",if filename . startswith ( prefix ) and filename . endswith ( suffix ) :,79
"def listdir(path):
    path = path.rstrip(""/"") + ""/""
    dir_set, file_set = set(), set()
    for p in files.keys():
        if not p.startswith(path):
            continue
        parts = p[len(path) :].split(""/"")
        if len(parts) == 1:
            file_set.add(parts[0])
        else:
            dir_set.add(parts[0])
    return sorted(dir_set), sorted(file_set)
",if not p . startswith ( path ) :,128
"def read_series(rec):
    found = []
    for tag in (""440"", ""490"", ""830""):
        fields = rec.get_fields(tag)
        if not fields:
            continue
        for f in fields:
            this = []
            for k, v in f.get_subfields([""a"", ""v""]):
                if k == ""v"" and v:
                    this.append(v)
                    continue
                v = v.rstrip("".,; "")
                if v:
                    this.append(v)
            if this:
                found += ["" -- "".join(this)]
    return found
",if not fields :,182
"def find_nameless_urls(self, conf):
    nameless = []
    patterns = self.get_patterns(conf)
    for u in patterns:
        if self.has_patterns(u):
            nameless.extend(self.find_nameless_urls(u))
        else:
            if u.name is None:
                nameless.append(u)
    return nameless
",if self . has_patterns ( u ) :,103
"def update_billing_status(self, update_modified=True):
    updated_pr = [self.name]
    for d in self.get(""items""):
        if d.purchase_order_item:
            updated_pr += update_billed_amount_based_on_po(
                d.purchase_order_item, update_modified
            )
    for pr in set(updated_pr):
        pr_doc = self if (pr == self.name) else frappe.get_doc(""Purchase Receipt"", pr)
        update_billing_percentage(pr_doc, update_modified=update_modified)
    self.load_from_db()
",if d . purchase_order_item :,166
"def _get_version():
    with open(""haiku/__init__.py"") as fp:
        for line in fp:
            if line.startswith(""__version__""):
                g = {}
                exec(line, g)  # pylint: disable=exec-used
                return g[""__version__""]
        raise ValueError(""`__version__` not defined in `haiku/__init__.py`"")
","if line . startswith ( ""__version__"" ) :",101
"def GetSelected(self):
    if self.GetStyleL(""style"") & self.Style.LBS_MULTIPLESEL:
        result = self.SendMessage(self.Hwnd, self.Msg.LB_GETSELCOUNT, 0, 0)
        if result:
            return self.SendMessage(self.Hwnd, self.Msg.LB_GETANCHORINDEX, 0, 0)
    else:
        result = self.SendMessage(self.Hwnd, self.Msg.LB_GETCURSEL, 0, 0)
        if result != LB_ERR:
            return result
",if result != LB_ERR :,151
"def __init__(self, column_names, column_types, **kwargs):
    super().__init__(**kwargs)
    self.column_names = column_names
    self.column_types = column_types
    encoding = []
    for column_name in self.column_names:
        column_type = self.column_types[column_name]
        if column_type == analysers.CATEGORICAL:
            # TODO: Search to use one-hot or int.
            encoding.append(keras_layers.INT)
        else:
            encoding.append(keras_layers.NONE)
    self.layer = keras_layers.MultiCategoryEncoding(encoding)
",if column_type == analysers . CATEGORICAL :,162
"def rotate(cls, axis, theta):
    """"""Prepare a quaternion that represents a rotation on a given axis.""""""
    if isinstance(axis, str):
        if axis in (""x"", ""X""):
            axis = V.X
        elif axis in (""y"", ""Y""):
            axis = V.Y
        elif axis in (""z"", ""Z""):
            axis = V.Z
    axis = axis.normalize()
    s = math.sin(theta / 2.0)
    c = math.cos(theta / 2.0)
    return Q(axis._v[0] * s, axis._v[1] * s, axis._v[2] * s, c)
","if axis in ( ""x"" , ""X"" ) :",169
"def log(self, request):
    web_socket = WebSocketResponse()
    await web_socket.prepare(request)
    self.app[""websockets""].add(web_socket)
    try:
        async for msg in web_socket:
            if msg.type == WSMsgType.TEXT:
                if msg.data == ""close"":
                    await web_socket.close()
            elif msg.type == WSMsgType.ERROR:
                print(
                    ""web socket connection closed with exception %s""
                    % web_socket.exception()
                )
    finally:
        self.app[""websockets""].remove(web_socket)
    return web_socket
","if msg . data == ""close"" :",187
"def test_loc_is_stochastic_parameter(self):
    param = iap.Laplace(iap.Choice([-100, 100]), 1)
    seen = [0, 0]
    for _ in sm.xrange(1000):
        samples = param.draw_samples((100,))
        exp = np.mean(samples)
        if -100 - 10 < exp < -100 + 10:
            seen[0] += 1
        elif 100 - 10 < exp < 100 + 10:
            seen[1] += 1
        else:
            assert False
    assert 500 - 100 < seen[0] < 500 + 100
    assert 500 - 100 < seen[1] < 500 + 100
",if - 100 - 10 < exp < - 100 + 10 :,167
"def cli_setup(args=None):
    """"""future api for setup env by cli""""""
    if not args:
        if len(sys.argv) < 2:
            print(""no cmdline args"")
            return False
        args = sys.argv
    print(args)
    ap = argparse.ArgumentParser()
    if ""--report"" in args:
        from airtest.report.report import main as report_main
        ap = report_parser(ap)
        args = ap.parse_args(args)
        report_main(args)
        exit(0)
    else:
        ap = runner_parser(ap)
        args = ap.parse_args(args)
        setup_by_args(args)
    return True
",if len ( sys . argv ) < 2 :,187
"def validate_attributes(cls, cleaned_data):
    errors = {}
    for field in [""product_attributes"", ""variant_attributes""]:
        attributes = cleaned_data.get(field)
        if not attributes:
            continue
        not_valid_attributes = [
            graphene.Node.to_global_id(""Attribute"", attr.pk)
            for attr in attributes
            if attr.type != AttributeType.PRODUCT_TYPE
        ]
        if not_valid_attributes:
            errors[field] = ValidationError(
                ""Only Product type attributes are allowed."",
                code=ProductErrorCode.INVALID.value,
                params={""attributes"": not_valid_attributes},
            )
    if errors:
        raise ValidationError(errors)
",if not_valid_attributes :,193
"def forward(self, x, activate=True, norm=True):
    for layer in self.order:
        if layer == ""conv"":
            if self.with_explicit_padding:
                x = self.padding_layer(x)
            x = self.conv(x)
        elif layer == ""norm"" and norm and self.with_norm:
            x = self.norm(x)
        elif layer == ""act"" and activate and self.with_activation:
            x = self.activate(x)
    return x
","elif layer == ""norm"" and norm and self . with_norm :",138
"def _FunctionDef(self, node):
    _ScopeVisitor._FunctionDef(self, node)
    if len(node.args.args) > 0:
        first = node.args.args[0]
        if isinstance(first, ast.Name):
            new_visitor = _ClassInitVisitor(self, first.id)
            for child in ast.get_child_nodes(node):
                ast.walk(child, new_visitor)
","if isinstance ( first , ast . Name ) :",111
"def result(self):
    """"""Gets the formatted string result.""""""
    if self.__group.isChecked():
        if self.__moreThan.isChecked():
            return ""gt%d"" % self.__min.value()
        if self.__lessThan.isChecked():
            return ""lt%d"" % self.__max.value()
        if self.__range.isChecked():
            return ""%d-%d"" % (self.__min.value(), self.__max.value())
    return """"
",if self . __lessThan . isChecked ( ) :,122
"def hash_of_file(path):
    """"""Return the hash of a downloaded file.""""""
    with open(path, ""rb"") as archive:
        sha = sha256()
        while True:
            data = archive.read(2 ** 20)
            if not data:
                break
            sha.update(data)
    return encoded_hash(sha)
",if not data :,95
"def read_boolean(file: BinaryIO, count: int, checkall: bool = False) -> List[bool]:
    if checkall:
        all_defined = file.read(1)
        if all_defined != unhexlify(""00""):
            return [True] * count
    result = []
    b = 0
    mask = 0
    for i in range(count):
        if mask == 0:
            b = ord(file.read(1))
            mask = 0x80
        result.append(b & mask != 0)
        mask >>= 1
    return result
","if all_defined != unhexlify ( ""00"" ) :",146
"def start_prompt(self):
    """"""Start the interpreter.""""""
    logger.show(""Coconut Interpreter:"")
    logger.show(""(type 'exit()' or press Ctrl-D to end)"")
    self.start_running()
    while self.running:
        try:
            code = self.get_input()
            if code:
                compiled = self.handle_input(code)
                if compiled:
                    self.execute(compiled, use_eval=None)
        except KeyboardInterrupt:
            printerr(""\nKeyboardInterrupt"")
",if code :,142
"def _wrap_lineanchors(self, inner):
    s = self.lineanchors
    i = self.linenostart - 1  # subtract 1 since we have to increment i
    # *before* yielding
    for t, line in inner:
        if t:
            i += 1
            yield 1, '<a name=""%s-%d""></a>' % (s, i) + line
        else:
            yield 0, line
",if t :,109
"def __UpdateQueryHistory(self, query):
    clone = datastore_pb.Query()
    clone.CopyFrom(query)
    clone.clear_hint()
    clone.clear_limit()
    clone.clear_offset()
    clone.clear_count()
    if clone in self.__query_history:
        self.__query_history[clone] += 1
    else:
        self.__query_history[clone] = 1
        if clone.app() == self._app_id:
            self.__query_ci_history.add(datastore_index.CompositeIndexForQuery(clone))
",if clone . app ( ) == self . _app_id :,145
"def call(self, trajectory: traj.Trajectory):
    if not self._batch_size:
        if trajectory.step_type.ndim == 0:
            self._batch_size = 1
        else:
            assert trajectory.step_type.ndim == 1
            self._batch_size = trajectory.step_type.shape[0]
        self.reset()
    if trajectory.step_type.ndim == 0:
        trajectory = nest_utils.batch_nested_array(trajectory)
    self._batched_call(trajectory)
",if trajectory . step_type . ndim == 0 :,141
"def steps(self):
    """"""""""""
    for step_id in range(self.micro_batches):
        cmds = [
            LoadMicroBatch(buffer_id=0),
            ForwardPass(buffer_id=0),
            BackwardPass(buffer_id=0),
        ]
        if step_id == self.micro_batches - 1:
            cmds.extend(
                [
                    ReduceGrads(),
                    OptimizerStep(),
                ]
            )
        yield cmds
",if step_id == self . micro_batches - 1 :,143
"def resolve_project(self, workspace, project_name):
    if isinstance(project_name, (int, float)):  # project id
        project_id = int(project_name)
        self.log.debug(""Treating project name as ID: %s"", project_id)
        project = workspace.projects(ident=project_id).first()
        if not project:
            raise TaurusConfigError(
                ""BlazeMeter project not found by ID: %s"" % project_id
            )
    elif project_name:
        project = workspace.projects(name=project_name).first()
    else:
        project = None
    if not project:
        project = self._create_project_or_use_default(workspace, project_name)
    return project
",if not project :,199
"def __reader(self, collector, source):
    while True:
        data = os.read(source.fileno(), 65536)
        self.__lock.acquire()
        collector.append(data)
        self.__lock.release()
        if data == """":
            source.close()
            break
    return
","if data == """" :",81
"def add(self, undoinfo, msg=None):
    if not undoinfo:
        return
    if msg is not None:
        if isinstance(undoinfo[0], str):
            # replace message
            undoinfo = (msg,) + undoinfo[1:]
        elif isinstance(undoinfo, tuple):
            undoinfo = (msg,) + undoinfo
        else:
            undoinfo = (msg, undoinfo)
        f = 1
    else:
        f = int(isinstance(undoinfo[0], str))
    assert (
        isinstance(undoinfo, list)
        or callable(undoinfo[f])
        or isinstance(undoinfo[f], list)
    )
    self.undoList.append(undoinfo)
    del self.redoList[:]
","elif isinstance ( undoinfo , tuple ) :",198
"def get_history_data(self, guid, count=1):
    history = {}
    if count < 1:
        return history
    key = self._make_key(guid)
    for i in range(0, self.db.llen(key)):
        r = self.db.lindex(key, i)
        c = msgpack.unpackb(r)
        if c[""tries""] == 0 or c[""tries""] is None:
            if c[""data""] not in history:
                history[c[""data""]] = c[""timestamp""]
                if len(history) >= count:
                    break
    return history
",if len ( history ) >= count :,161
"def __str__(self):
    from sqlalchemy.sql import util
    details = [SQLAlchemyError.__str__(self)]
    if self.statement:
        details.append(""[SQL: %r]"" % self.statement)
        if self.params:
            params_repr = util._repr_params(self.params, 10)
            details.append(""[parameters: %r]"" % params_repr)
    return "" "".join([""(%s)"" % det for det in self.detail] + details)
",if self . params :,121
"def _consume_msg(self):
    async for data in self._stream:
        stream = data.get(""ev"")
        if stream:
            await self._dispatch(data)
        elif data.get(""status"") == ""disconnected"":
            # Polygon returns this on an empty 'ev' id..
            data[""ev""] = ""status""
            await self._dispatch(data)
            raise ConnectionResetError(
                ""Polygon terminated connection: "" f'({data.get(""message"")})'
            )
",if stream :,135
"def nan2none(l):
    for idx, val in enumerate(l):
        if isinstance(val, Sequence):
            l[idx] = nan2none(l[idx])
        elif isnum(val) and math.isnan(val):
            l[idx] = None
    return l
",elif isnum ( val ) and math . isnan ( val ) :,76
"def _make_binary_stream(s, encoding):
    try:
        if _py3k:
            if isinstance(s, str):
                s = s.encode(encoding)
        else:
            if type(s) is not str:
                s = s.encode(encoding)
        from io import BytesIO
        rv = BytesIO(s)
    except ImportError:
        rv = StringIO(s)
    return rv
",if type ( s ) is not str :,115
"def __set__(self, instance, value):
    try:
        value = int(value)
        if 0 <= value <= 65535:  # max port number is 65535
            self.display_value = str(value)
            self.value = value
        else:
            raise PocsuiteValidationException(
                ""Invalid option. Port value should be between 0 and 65536.""
            )
    except ValueError:
        raise PocsuiteValidationException(
            ""Invalid option. Cannot cast '{}' to integer."".format(value)
        )
",if 0 <= value <= 65535 :,140
"def addVaXref(self, va, parent=None):
    if parent is None:
        parent = self
    xtova, ok = QInputDialog.getText(parent, ""Enter..."", ""Make Code Xref 0x%x -> "" % va)
    if ok:
        try:
            val = self.vw.parseExpression(str(xtova))
            if self.vw.isValidPointer(val):
                self.vw.addXref(va, val, REF_CODE)
            else:
                self.vw.vprint(""Invalid Expression: %s   (%s)"" % (xtova, val))
        except Exception as e:
            self.vw.vprint(repr(e))
",if self . vw . isValidPointer ( val ) :,191
"def ArrayBuffer():
    a = arguments[0]
    if isinstance(a, PyJsNumber):
        length = a.to_uint32()
        if length != a.value:
            raise MakeError(""RangeError"", ""Invalid array length"")
        temp = Js(bytearray([0] * length))
        return temp
    return Js(bytearray([0]))
",if length != a . value :,91
"def _update_positions(nodes, line_offset, last_leaf):
    for node in nodes:
        try:
            children = node.children
        except AttributeError:
            # Is a leaf
            node.line += line_offset
            if node is last_leaf:
                raise _PositionUpdatingFinished
        else:
            _update_positions(children, line_offset, last_leaf)
",if node is last_leaf :,108
"def class_has_method(self, curr_node, the_text):
    try:
        class_node = self.containers[VAR_KIND_CLASS][-1]
        for c in class_node.children:
            if isinstance(c, MethodNode) and c.name == the_text:
                return True
    except:
        pass
    return False
","if isinstance ( c , MethodNode ) and c . name == the_text :",94
"def _fm(map_id):
    for i in range(num_key):
        for j in range(num_value_per_key):
            if dup_key:
                yield (i, j)
            else:
                yield ((map_id, i), j)
",if dup_key :,77
"def _compileRules(rulesList, maxLength=4):
    ruleChecking = collections.defaultdict(list)
    for ruleIndex in range(len(rulesList)):
        args = []
        if len(rulesList[ruleIndex]) == maxLength:
            args = rulesList[ruleIndex][-1]
        if maxLength == 4:
            (shouldRunMethod, method, isCorrect) = rulesList[ruleIndex][0:3]
            ruleChecking[shouldRunMethod].append((method, isCorrect, args))
        elif maxLength == 3:
            (shouldRunMethod, method) = rulesList[ruleIndex][0:2]
            ruleChecking[shouldRunMethod].append((method, args))
    return ruleChecking
",elif maxLength == 3 :,183
"def select(result):
    for elem in result:
        parent = elem.getparent()
        if parent is None:
            continue
        try:
            # FIXME: what if the selector is ""*"" ?
            elems = list(parent.iterchildren(elem.tag))
            if elems[index] is elem:
                yield elem
        except IndexError:
            pass
",if elems [ index ] is elem :,101
"def get_kwarg_or_param(request, kwargs, key):
    value = None
    try:
        value = kwargs[key]
    except KeyError:
        if request.method == ""GET"":
            value = request.GET.get(key)
        elif request.method == ""POST"":
            value = request.POST.get(key)
    return value
","if request . method == ""GET"" :",93
"def __imul__(self, other):
    if isinstance(other, str):
        other = Matrix(other)
    if isinstance(other, Matrix):
        if self.start is not None:
            self.start *= other
        if self.control1 is not None:
            self.control1 *= other
        if self.control2 is not None:
            self.control2 *= other
        if self.end is not None:
            self.end *= other
    return self
",if self . control1 is not None :,125
"def _parse_date_fmt():
    fmt = get_format(""DATE_FORMAT"")
    escaped = False
    for char in fmt:
        if escaped:
            escaped = False
        elif char == ""\\"":
            escaped = True
        elif char in ""Yy"":
            yield ""year""
        elif char in ""bEFMmNn"":
            yield ""month""
        elif char in ""dj"":
            yield ""day""
","elif char in ""Yy"" :",117
"def filter_forms(forms):
    result = []
    seen = set()
    for form in forms:
        if form in self._lemma_pos_offset_map:
            if pos in self._lemma_pos_offset_map[form]:
                if form not in seen:
                    result.append(form)
                    seen.add(form)
    return result
",if form in self . _lemma_pos_offset_map :,100
"def calculate(self):
    """"""Enumerate processes by scanning for _EPROCESS.""""""
    result = set()
    psscan = self.session.plugins.psscan()
    pslist = self.session.plugins.pslist()
    for row in psscan.collect():
        physical_eprocess = row[""offset_p""]
        if physical_eprocess.obj_vm == self.session.physical_address_space:
            eprocess = pslist.virtual_process_from_physical_offset(physical_eprocess)
        else:
            eprocess = physical_eprocess
        if eprocess != None:
            result.add(eprocess.obj_offset)
    self.session.logging.debug(""Listed %s processes using PSScan"", len(result))
    return result
",if physical_eprocess . obj_vm == self . session . physical_address_space :,195
"def _build_kwargs_string(cls, expectation):
    kwargs = []
    for k, v in expectation[""kwargs""].items():
        if k == ""column"":
            # make the column a positional argument
            kwargs.insert(0, ""{}='{}'"".format(k, v))
        elif isinstance(v, str):
            # Put strings in quotes
            kwargs.append(""{}='{}'"".format(k, v))
        else:
            # Pass other types as is
            kwargs.append(""{}={}"".format(k, v))
    return "", "".join(kwargs)
","if k == ""column"" :",143
"def prec3_expr(self, arg_type):
    pass
    self.prec4_expr(arg_type)
    while True:
        if self.LA(1) == POWER:
            pass
            pass
            self.match(POWER)
            op = struct.pack(""B"", ptgPower)
            self.prec4_expr(arg_type)
            self.rpn += op
        else:
            break
",if self . LA ( 1 ) == POWER :,121
"def evaluate(analysis, rule):
    try:
        if isinstance(rule, MetaRule):
            result = _evaluate_meta_rule(analysis, rule)
        elif isinstance(rule, SingleRule):
            result = _evaluate_single_rule(analysis, rule)
        elif isinstance(rule, SubPathRule):
            result = _evaluate_sub_path_rule(analysis, rule)
        else:
            raise TypeError(
                ""rule must be of one in types [SingleRule, MetaRule, SubPathRule]""
            )
        return result
    except KeyError:  # expected behavior as long as this does not have all other plugins as dependency
        return False
","elif isinstance ( rule , SubPathRule ) :",171
"def create_log_file(d, logname):
    logpath = d.getVar(""LOG_DIR"")
    bb.utils.mkdirhier(logpath)
    logfn, logsuffix = os.path.splitext(logname)
    logfile = os.path.join(
        logpath, ""%s.%s%s"" % (logfn, d.getVar(""DATETIME""), logsuffix)
    )
    if not os.path.exists(logfile):
        slogfile = os.path.join(logpath, logname)
        if os.path.exists(slogfile):
            os.remove(slogfile)
        open(logfile, ""w+"").close()
        os.symlink(logfile, slogfile)
        d.setVar(""LOG_FILE"", logfile)
    return logfile
",if os . path . exists ( slogfile ) :,191
"def init_eventlog(self):
    """"""Set up the event logging system.""""""
    self.eventlog = EventLog(parent=self)
    for dirname, _, files in os.walk(os.path.join(here, ""event-schemas"")):
        for file in files:
            if not file.endswith("".yaml""):
                continue
            self.eventlog.register_schema_file(os.path.join(dirname, file))
","if not file . endswith ( "".yaml"" ) :",109
"def resize(self, limit, force=False, ignore_errors=False, reset=False):
    prev_limit = self._limit
    if (self._dirty and 0 < limit < self._limit) and not ignore_errors:
        if not force:
            raise RuntimeError(
                ""Can't shrink pool when in use: was={0} now={1}"".format(
                    self._limit, limit
                )
            )
        reset = True
    self._limit = limit
    if reset:
        try:
            self.force_close_all()
        except Exception:
            pass
    self.setup()
    if limit < prev_limit:
        self._shrink_down(collect=limit > 0)
",if not force :,189
"def accept_request(self, request):
    if self.restriction_type == BaseViewRestriction.PASSWORD:
        passed_restrictions = request.session.get(
            self.passed_view_restrictions_session_key, []
        )
        if self.id not in passed_restrictions:
            return False
    elif self.restriction_type == BaseViewRestriction.LOGIN:
        if not request.user.is_authenticated:
            return False
    elif self.restriction_type == BaseViewRestriction.GROUPS:
        if not request.user.is_superuser:
            current_user_groups = request.user.groups.all()
            if not any(group in current_user_groups for group in self.groups.all()):
                return False
    return True
",if not request . user . is_superuser :,187
"def getLatestXci(self, version=None):
    highest = None
    for nsp in self.getFiles():
        try:
            if nsp.path.endswith("".xci""):
                if version is not None and nsp.version == version:
                    return nsp
                if not highest or int(nsp.version) > int(highest.version):
                    highest = nsp
        except BaseException:
            pass
    return highest
",if not highest or int ( nsp . version ) > int ( highest . version ) :,118
"def evaluate(self, x, y, z):
    vertex = Vector((x, y, z))
    nearest, normal, idx, distance = self.bvh.find_nearest(vertex)
    if self.use_normal:
        if self.signed_normal:
            sign = (v - nearest).dot(normal)
            sign = copysign(1, sign)
        else:
            sign = 1
        return sign * np.array(normal)
    else:
        dv = np.array(nearest - vertex)
        if self.falloff is not None:
            norm = np.linalg.norm(dv)
            len = self.falloff(norm)
            dv = len * dv
            return dv
        else:
            return dv
",if self . falloff is not None :,200
"def to_py(self, value: _StrUnset) -> _StrUnsetNone:
    self._basic_py_validation(value, str)
    if isinstance(value, usertypes.Unset):
        return value
    elif not value:
        return None
    value = os.path.expandvars(value)
    value = os.path.expanduser(value)
    try:
        if not os.path.isdir(value):
            raise configexc.ValidationError(value, ""must be a valid directory!"")
        if not os.path.isabs(value):
            raise configexc.ValidationError(value, ""must be an absolute path!"")
    except UnicodeEncodeError as e:
        raise configexc.ValidationError(value, e)
    return value
",if not os . path . isabs ( value ) :,181
"def validate_load_balancer_sku(namespace):
    """"""Validates the load balancer sku string.""""""
    if namespace.load_balancer_sku is not None:
        if namespace.load_balancer_sku == """":
            return
        if (
            namespace.load_balancer_sku.lower() != ""basic""
            and namespace.load_balancer_sku.lower() != ""standard""
        ):
            raise CLIError(""--load-balancer-sku can only be standard or basic"")
","if namespace . load_balancer_sku == """" :",121
"def _getLocalSpineType(self):
    if self._spineType is not None:
        return self._spineType
    else:
        for thisEvent in self.eventList:
            m1 = re.match(r""\*\*(.*)"", thisEvent.contents)
            if m1:
                self._spineType = m1.group(1)
                return self._spineType
        return None
",if m1 :,111
"def set_selected_device(self):
    current_devices = self.get_current_devices()
    if self.device in current_devices.values():
        return
    for device_name in current_devices.values():
        if self.device in device_name:
            self.parent.py3.log(f""device {self.device} detected as {device_name}"")
            self.device = device_name
            break
",if self . device in device_name :,110
"def write(self, buff):
    if not self.handle:
        raise TTransportException(
            type=TTransportException.NOT_OPEN, message=""Transport not open""
        )
    sent = 0
    have = len(buff)
    while sent < have:
        plus = self.handle.send(buff)
        if plus == 0:
            raise TTransportException(
                type=TTransportException.END_OF_FILE, message=""TSocket sent 0 bytes""
            )
        sent += plus
        buff = buff[plus:]
",if plus == 0 :,143
"def get_named_key_value(self, rule, match, key_name):
    # search the match for the key specified in the rule to get the value
    if key_name in rule:
        try:
            key_value = lookup_es_key(match, rule[key_name])
            if key_value is not None:
                # Only do the unicode conversion if we actually found something)
                # otherwise we might transform None --> 'None'
                key_value = str(key_value)
        except KeyError:
            # Some matches may not have the specified key
            # use a special token for these
            key_value = ""_missing""
    else:
        key_value = None
    return key_value
",if key_value is not None :,191
"def __iter__(self):
    protocol = self.protocol
    source = write_source_from_arg(self.source)
    with source.open(""wb"") as f:
        it = iter(self.table)
        hdr = next(it)
        if self.write_header:
            pickle.dump(hdr, f, protocol)
        yield tuple(hdr)
        for row in it:
            pickle.dump(row, f, protocol)
            yield tuple(row)
",if self . write_header :,125
"def abs__file__():
    """"""Set all module' __file__ attribute to an absolute path""""""
    for m in sys.modules.values():
        if hasattr(m, ""__loader__""):
            continue  # don't mess with a PEP 302-supplied __file__
        try:
            m.__file__ = os.path.abspath(m.__file__)
        except (AttributeError, OSError):
            pass
","if hasattr ( m , ""__loader__"" ) :",101
"def _run(self):
    when_pressed = 0.0
    pressed = False
    while not self._done.is_set():
        now = time.monotonic()
        if now - when_pressed > self._debounce_time:
            if GPIO.input(self._channel) == self._expected:
                if not pressed:
                    pressed = True
                    when_pressed = now
                    self._trigger(self._pressed_queue, self._pressed_callback)
            else:
                if pressed:
                    pressed = False
                    self._trigger(self._released_queue, self._released_callback)
        self._done.wait(0.05)
",if pressed :,187
"def get_run_cmd(submission_dir):
    """"""Get the language of a submission""""""
    with CD(submission_dir):
        if os.path.exists(""run.sh""):
            with open(""run.sh"") as f:
                for line in f:
                    if line[0] != ""#"":
                        return line.rstrip(""\r\n"")
","if line [ 0 ] != ""#"" :",98
"def client_read(self, path, **kwargs):
    """"""Retrieve a value from a etcd key.""""""
    try:
        res = self.client.read(
            path,
            timeout=kwargs.get(""timeout"", DEFAULT_TIMEOUT),
            recursive=kwargs.get(""recursive"") or kwargs.get(""all"", False),
        )
        if kwargs.get(""watch"", False):
            modified_indices = (res.modifiedIndex,) + tuple(
                leaf.modifiedIndex for leaf in res.leaves
            )
            return max(modified_indices)
        else:
            return res.value
    except EtcdKeyNotFound:
        raise KeyNotFound(""The key %s was not found in etcd"" % path)
    except TimeoutError as e:
        raise e
","if kwargs . get ( ""watch"" , False ) :",197
"def populate_wrapper(klass, wrapping):
    for meth, how in klass._wrap_methods.items():
        if not hasattr(wrapping, meth):
            continue
        func = getattr(wrapping, meth)
        wrapper = make_wrapper(func, how)
        setattr(klass, meth, wrapper)
","if not hasattr ( wrapping , meth ) :",76
"def _copy_files(self, files, src, dest, message=""""):
    for filepath in files:
        srcpath = os.path.join(src, filepath)
        destpath = os.path.join(dest, filepath)
        if message:
            print(""{}: {}"".format(message, destpath))
        if os.path.exists(srcpath):
            destdir = os.path.dirname(destpath)
            if not os.path.isdir(destdir):
                os.makedirs(destdir)
            shutil.copy(srcpath, destpath)
        elif os.path.exists(destpath):
            os.remove(destpath)
",elif os . path . exists ( destpath ) :,167
"def scan_iter(self, match=None, count=None):
    nodes = await self.cluster_nodes()
    for node in nodes:
        if ""master"" in node[""flags""]:
            cursor = ""0""
            while cursor != 0:
                pieces = [cursor]
                if match is not None:
                    pieces.extend([""MATCH"", match])
                if count is not None:
                    pieces.extend([""COUNT"", count])
                response = await self.execute_command_on_nodes([node], ""SCAN"", *pieces)
                cursor, data = list(response.values())[0]
                for item in data:
                    yield item
",if count is not None :,185
"def restart(cls, request, server_name):
    with cls._servername_to_shell_server_lock:
        if server_name in cls._servername_to_shell_server:
            servr = cls._servername_to_shell_server[server_name]
            servr.restart()
",if server_name in cls . _servername_to_shell_server :,75
"def human_waiting_on(self):
    if self.waiting_on is None:
        return ""N/A""
    things = []
    for cluster, queue in self.waiting_on.items():
        queue_length = len(queue)
        if queue_length == 0:
            continue
        elif queue_length == 1:
            things.append(f""`{cluster}`: `{queue[0].get_instance()}`"")
        else:
            things.append(f""`{cluster}`: {len(queue)} instances"")
    return "", "".join(things)
",if queue_length == 0 :,148
"def psea(pname):
    """"""Parse PSEA output file.""""""
    fname = run_psea(pname)
    start = 0
    ss = """"
    with open(fname) as fp:
        for l in fp:
            if l[0:6] == "">p-sea"":
                start = 1
                continue
            if not start:
                continue
            if l[0] == ""\n"":
                break
            ss = ss + l[0:-1]
    return ss
",if not start :,142
"def encrypt_system_info_ssh_keys(ssh_info):
    for idx, user in enumerate(ssh_info):
        for field in [""public_key"", ""private_key"", ""known_hosts""]:
            if ssh_info[idx][field]:
                ssh_info[idx][field] = encryptor.enc(ssh_info[idx][field])
",if ssh_info [ idx ] [ field ] :,90
"def get_shape(shape):
    """"""Convert the shape to correct dtype and vars.""""""
    ret = []
    for dim in shape:
        if isinstance(dim, tvm.tir.IntImm):
            if libinfo()[""INDEX_DEFAULT_I64""] == ""ON"":
                ret.append(dim)
            else:
                val = int(dim)
                assert val <= np.iinfo(np.int32).max
                ret.append(tvm.tir.IntImm(""int32"", val))
        elif isinstance(dim, tvm.tir.Any):
            ret.append(te.var(""any_dim"", ""int32""))
        else:
            ret.append(dim)
    return ret
","elif isinstance ( dim , tvm . tir . Any ) :",194
"def unpack(sources):
    temp_dir = tempfile.mkdtemp(""-scratchdir"", ""unpacker-"")
    for package, content in sources.items():
        filepath = package.split(""/"")
        dirpath = os.sep.join(filepath[:-1])
        packagedir = os.path.join(temp_dir, dirpath)
        if not os.path.isdir(packagedir):
            os.makedirs(packagedir)
        mod = open(os.path.join(packagedir, filepath[-1]), ""wb"")
        try:
            mod.write(base64.b64decode(content))
        finally:
            mod.close()
    return temp_dir
",if not os . path . isdir ( packagedir ) :,165
"def set_torrent_path(self, torrent_id, path):
    try:
        if not self.connect():
            return False
        self.client.core.set_torrent_move_completed_path(torrent_id, path).get()
        self.client.core.set_torrent_move_completed(torrent_id, 1).get()
    except Exception:
        return False
    finally:
        if self.client:
            self.disconnect()
    return True
",if self . client :,123
"def _get_specs(self, link, source, target):
    for src_spec, code in link.code.items():
        src_specs = src_spec.split(""."")
        if src_spec.startswith(""event:""):
            src_spec = (None, src_spec)
        elif len(src_specs) > 1:
            src_spec = (""."".join(src_specs[:-1]), src_specs[-1])
        else:
            src_prop = src_specs[0]
            if isinstance(source, Reactive):
                src_prop = source._rename.get(src_prop, src_prop)
            src_spec = (None, src_prop)
    return [(src_spec, (None, None), code)]
",elif len ( src_specs ) > 1 :,190
"def deserialize(self, meth, content_type, body):
    meth_deserializers = getattr(meth, ""wsgi_deserializers"", {})
    try:
        mtype = _MEDIA_TYPE_MAP.get(content_type, content_type)
        if mtype in meth_deserializers:
            deserializer = meth_deserializers[mtype]
        else:
            deserializer = self.default_deserializers[mtype]
    except (KeyError, TypeError):
        raise exception.InvalidContentType(content_type=content_type)
    return deserializer().deserialize(body)
",if mtype in meth_deserializers :,139
"def object_inspect(self, oname, detail_level=0):
    """"""Get object info about oname""""""
    with self.builtin_trap:
        info = self._object_find(oname)
        if info.found:
            return self.inspector.info(
                info.obj, oname, info=info, detail_level=detail_level
            )
        else:
            return oinspect.object_info(name=oname, found=False)
",if info . found :,122
"def wrapper(*args, **kargs):
    for key, value in vkargs.items():
        if key not in kargs:
            abort(403, ""Missing parameter: %s"" % key)
        try:
            kargs[key] = value(kargs[key])
        except ValueError:
            abort(403, ""Wrong parameter format for: %s"" % key)
    return func(*args, **kargs)
",if key not in kargs :,105
"def _append_fragment(self, ctx, frag_content):
    try:
        ctx[""dest_stream""].write(frag_content)
        ctx[""dest_stream""].flush()
    finally:
        if self.__do_ytdl_file(ctx):
            self._write_ytdl_file(ctx)
        if not self.params.get(""keep_fragments"", False):
            os.remove(encodeFilename(ctx[""fragment_filename_sanitized""]))
        del ctx[""fragment_filename_sanitized""]
","if not self . params . get ( ""keep_fragments"" , False ) :",128
"def override_args_required_option(argument_table, args, session, **kwargs):
    # This function overrides the 'required' property of an argument
    # if a value corresponding to that argument is present in the config
    # file
    # We don't want to override when user is viewing the help so that we
    # can show the required options correctly in the help
    need_to_override = False if len(args) == 1 and args[0] == ""help"" else True
    if need_to_override:
        parsed_configs = configutils.get_configs(session)
        for arg_name in argument_table.keys():
            if arg_name.replace(""-"", ""_"") in parsed_configs:
                argument_table[arg_name].required = False
","if arg_name . replace ( ""-"" , ""_"" ) in parsed_configs :",183
"def _count(self, element, count=True):
    if not isinstance(element, six.string_types):
        if self == element:
            return 1
    i = 0
    for child in self.children:
        # child is text content and element is also text content, then
        # make a simple ""text"" in ""text""
        if isinstance(child, six.string_types):
            if isinstance(element, six.string_types):
                if count:
                    i += child.count(element)
                elif element in child:
                    return 1
        else:
            i += child._count(element, count=count)
            if not count and i:
                return i
    return i
",elif element in child :,196
"def teardown_class(cls):
    collections = cls.discovery.list_collections(cls.environment_id).get_result()[
        ""collections""
    ]
    for collection in collections:
        if collection[""name""] == cls.collection_name:
            print(""Deleting the temporary collection"")
            cls.discovery.delete_collection(cls.environment_id, cls.collection_id)
            break
","if collection [ ""name"" ] == cls . collection_name :",101
"def _shares_in_results(data):
    shares_in_device, shares_in_subdevice = False, False
    for plugin_name, plugin_result in data.iteritems():
        if plugin_result[""status""] == ""error"":
            continue
        if ""device"" not in plugin_result:
            continue
        if ""disk_shares"" in plugin_result[""device""]:
            shares_in_device = True
        for subdevice in plugin_result[""device""].get(""subdevices"", []):
            if ""disk_shares"" in subdevice:
                shares_in_subdevice = True
                break
    return shares_in_device, shares_in_subdevice
","if ""device"" not in plugin_result :",175
"def accept_request(self, request):
    if self.restriction_type == BaseViewRestriction.PASSWORD:
        passed_restrictions = request.session.get(
            self.passed_view_restrictions_session_key, []
        )
        if self.id not in passed_restrictions:
            return False
    elif self.restriction_type == BaseViewRestriction.LOGIN:
        if not request.user.is_authenticated:
            return False
    elif self.restriction_type == BaseViewRestriction.GROUPS:
        if not request.user.is_superuser:
            current_user_groups = request.user.groups.all()
            if not any(group in current_user_groups for group in self.groups.all()):
                return False
    return True
",if self . id not in passed_restrictions :,187
"def __setitem__(self, index, item):
    try:
        start, stop, step = index.start, index.stop, index.step
    except AttributeError:
        index = operator.index(index)
    else:
        if len(self.lists) == 1:
            self.lists[0][index] = item
        else:
            tmp = list(self)
            tmp[index] = item
            self.lists[:] = [tmp]
        self._balance_list(0)
        return
    list_idx, rel_idx = self._translate_index(index)
    if list_idx is None:
        raise IndexError()
    self.lists[list_idx][rel_idx] = item
",if len ( self . lists ) == 1 :,183
"def random_permutation_equality_groups(n_groups, n_perms_per_group, n_items, prob):
    fingerprints = set()
    for _ in range(n_groups):
        perms = random_equal_permutations(n_perms_per_group, n_items, prob)
        perm = perms[0]
        fingerprint = tuple(perm.get(i, i) for i in range(n_items))
        if fingerprint not in fingerprints:
            yield perms
            fingerprints.add(fingerprint)
",if fingerprint not in fingerprints :,128
"def get_proper_pip():  # no cov
    if not venv_active():
        default_pip = os.environ.get(""_DEFAULT_PIP_"", None)
        if default_pip:
            return default_pip
        elif not ON_WINDOWS:
            return ""pip3""
    return ""pip""
",if default_pip :,79
"def close(self, checkcount=False):
    self.mutex.acquire()
    try:
        if checkcount:
            self.openers -= 1
            if self.openers == 0:
                self.do_close()
        else:
            if self.openers > 0:
                self.do_close()
            self.openers = 0
    finally:
        self.mutex.release()
",if self . openers == 0 :,116
"def _lxml_default_loader(href, parse, encoding=None, parser=None):
    if parse == ""xml"":
        data = etree.parse(href, parser).getroot()
    else:
        if ""://"" in href:
            f = urlopen(href)
        else:
            f = open(href, ""rb"")
        data = f.read()
        f.close()
        if not encoding:
            encoding = ""utf-8""
        data = data.decode(encoding)
    return data
","if ""://"" in href :",133
"def Save(self):
    # Save the AUI perspectives if PersistenceManager allows it
    eventHandler = self._window.GetEventHandler()
    isAGWAui = isinstance(eventHandler, AUI.AuiManager)
    if not isAGWAui:
        return True
    if self._manager.GetManagerStyle() & PM_SAVE_RESTORE_AUI_PERSPECTIVES:
        # Allowed to save and restore perspectives
        perspective = eventHandler.SavePerspective()
        if isAGWAui:
            name = PERSIST_AGW_AUI_PERSPECTIVE
        else:
            name = PERSIST_AUI_PERSPECTIVE
        self._pObject.SaveValue(name, perspective)
    return True
",if isAGWAui :,186
"def get_arg_list_scalar_arg_dtypes(arg_types):
    result = []
    for arg_type in arg_types:
        if isinstance(arg_type, ScalarArg):
            result.append(arg_type.dtype)
        elif isinstance(arg_type, VectorArg):
            result.append(None)
            if arg_type.with_offset:
                result.append(np.int64)
        else:
            raise RuntimeError(""arg type not understood: %s"" % type(arg_type))
    return result
",if arg_type . with_offset :,142
"def perform_secure_deletion_of_temporary_files(self):
    # Delete the outdated temp files if older than 1 day
    for f in os.listdir(self.state.settings.tmp_path):
        path = os.path.join(self.state.settings.tmp_path, f)
        timestamp = datetime.fromtimestamp(os.path.getmtime(path))
        if is_expired(timestamp, days=1):
            overwrite_and_remove(path)
","if is_expired ( timestamp , days = 1 ) :",114
"def set_torrent_ratio(self, torrent_ids, ratio):
    try:
        if not self.connect():
            return False
        self.client.core.set_torrent_stop_at_ratio(torrent_ids, True).get()
        self.client.core.set_torrent_stop_ratio(torrent_ids, ratio).get()
    except Exception as err:
        return False
    finally:
        if self.client:
            self.disconnect()
    return True
",if not self . connect ( ) :,125
"def value_to_db_datetime(self, value):
    if value is None:
        return None
    # MySQL doesn't support tz-aware datetimes
    if timezone.is_aware(value):
        if settings.USE_TZ:
            value = value.astimezone(timezone.utc).replace(tzinfo=None)
        else:
            raise ValueError(
                ""MySQL backend does not support timezone-aware datetimes when USE_TZ is False.""
            )
    # MySQL doesn't support microseconds
    return six.text_type(value.replace(microsecond=0))
",if settings . USE_TZ :,145
"def remote_run_capture_all(login, cmd, log=None):
    """"""Run the remote command and return the (retval, stdout, stderr) result.""""""
    if sys.platform == ""win32"":
        if ""@"" not in login:
            login = ""%s@%s"" % (getpass.getuser(), login)
        cmd = 'plink -A -batch %s ""%s""' % (login, cmd)
    else:
        cmd = 'ssh -A -o BatchMode=yes %s ""%s""' % (login, cmd)
    __run_log(logstream, ""running '%s'"", cmd)
    p = subprocess.Popen(argv, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
    stdout, stderr = p.communicate()
    status = p.returncode
    return status, stdout, stderr
","if ""@"" not in login :",193
"def parseLeftHandSideExpressionAllowCall():
    marker = None
    expr = None
    args = None
    property = None
    marker = createLocationMarker()
    expr = parseNewExpression() if matchKeyword(""new"") else parsePrimaryExpression()
    while (match(""."") or match(""["")) or match(""(""):
        if match(""(""):
            args = parseArguments()
            expr = delegate.createCallExpression(expr, args)
        elif match(""[""):
            property = parseComputedMember()
            expr = delegate.createMemberExpression(""["", expr, property)
        else:
            property = parseNonComputedMember()
            expr = delegate.createMemberExpression(""."", expr, property)
        if marker:
            marker.end()
            marker.apply(expr)
    return expr
","if match ( ""("" ) :",193
"def getImageId(self, stuff):
    if not isinstance(stuff, Module):
        return -1
    if stuff.charge is None:
        return -1
    else:
        iconFile = stuff.charge.iconID if stuff.charge.iconID else """"
        if iconFile:
            return self.fittingView.imageList.GetImageIndex(iconFile, ""icons"")
        else:
            return -1
",if iconFile :,108
"def instance_reader():
    for epoch_index in range(epoch):
        if shuffle:
            if shuffle_seed is not None:
                np.random.seed(shuffle_seed)
            np.random.shuffle(examples)
        if phase == ""train"":
            self.current_train_epoch = epoch_index
        for (index, example) in enumerate(examples):
            if phase == ""train"":
                self.current_train_example = index + 1
            feature = self.convert_example(
                index, example, self.get_labels(), self.max_seq_len, self.tokenizer
            )
            instance = self.generate_instance(feature)
            yield instance
","if phase == ""train"" :",189
"def i2h(self, pkt, x):
    if x is not None:
        if x < 0:
            warning(""Fixed3_6: Internal value too negative: %d"" % x)
            x = 0
        elif x > 999999999:
            warning(""Fixed3_6: Internal value too positive: %d"" % x)
            x = 999999999
        x = x * 1e-6
    return x
",if x < 0 :,111
"def _is_section_header(self) -> bool:
    section, underline = self._line_iter.peek(2)
    section = section.lower()
    if section in self._sections and isinstance(underline, str):
        return bool(_numpy_section_regex.match(underline))
    elif self._directive_sections:
        if _directive_regex.match(section):
            for directive_section in self._directive_sections:
                if section.startswith(directive_section):
                    return True
    return False
",if _directive_regex . match ( section ) :,132
"def _parse_date_fmt():
    fmt = get_format(""DATE_FORMAT"")
    escaped = False
    for char in fmt:
        if escaped:
            escaped = False
        elif char == ""\\"":
            escaped = True
        elif char in ""Yy"":
            yield ""year""
        elif char in ""bEFMmNn"":
            yield ""month""
        elif char in ""dj"":
            yield ""day""
","elif char in ""dj"" :",117
"def _wait_port_open(port, max_wait=60):
    print(f""Waiting for port {port}"")
    start = time.time()
    while True:
        try:
            socket.create_connection((""localhost"", port), timeout=1)
        except OSError:
            if time.time() - start > max_wait:
                raise
            time.sleep(1)
        else:
            return
",if time . time ( ) - start > max_wait :,113
"def _list(self):
    data_sources = self.mkt_contract.functions.getAllProviders().call()
    data = []
    for index, data_source in enumerate(data_sources):
        if index > 0:
            if ""test"" not in Web3.toText(data_source).lower():
                data.append(dict(dataset=self.to_text(data_source)))
    return pd.DataFrame(data)
","if ""test"" not in Web3 . toText ( data_source ) . lower ( ) :",111
"def log_start(self, prefix, msg):
    with self._log_lock:
        if self._last_log_prefix != prefix:
            if self._last_log_prefix is not None:
                self._log_file.write(""\n"")
            self._log_file.write(prefix)
        self._log_file.write(msg)
        self._last_log_prefix = prefix
",if self . _last_log_prefix is not None :,105
"def _split_string_to_tokens(text):
    """"""Splits text to a list of string tokens.""""""
    if not text:
        return []
    ret = []
    token_start = 0
    # Classify each character in the input string
    is_alnum = [c in _ALPHANUMERIC_CHAR_SET for c in text]
    for pos in xrange(1, len(text)):
        if is_alnum[pos] != is_alnum[pos - 1]:
            token = text[token_start:pos]
            if token != u"" "" or token_start == 0:
                ret.append(token)
            token_start = pos
    final_token = text[token_start:]
    ret.append(final_token)
    return ret
","if token != u"" "" or token_start == 0 :",191
"def _install_groups(self, grp_specs):
    try:
        self.base.env_group_install(
            grp_specs,
            tuple(self.base.conf.group_package_types),
            strict=self.base.conf.strict,
        )
    except dnf.exceptions.Error:
        if self.base.conf.strict:
            raise
",if self . base . conf . strict :,100
"def _idx2token(idxs):
    for idx in idxs:
        if idx < self.tgt_vocab_size:
            token = self.tgt_vocab([[idx]])[0][0]
            if token == self.eos_token:
                break
            yield token
        else:
            yield self.kb_keys[idx - self.tgt_vocab_size]
",if token == self . eos_token :,99
"def increment(s):
    if not s:
        return ""1""
    for sequence in string.digits, string.lowercase, string.uppercase:
        lastc = s[-1]
        if lastc in sequence:
            i = sequence.index(lastc) + 1
            if i >= len(sequence):
                if len(s) == 1:
                    s = sequence[0] * 2
                    if s == ""00"":
                        s = ""10""
                else:
                    s = increment(s[:-1]) + sequence[0]
            else:
                s = s[:-1] + sequence[i]
            return s
    return s  # Don't increment
",if len ( s ) == 1 :,196
"def main():
    import sys, getopt
    try:
        opts, args = getopt.getopt(sys.argv[1:], ""ho:"", [""help"", ""output=""])
    except getopt.GetoptError as err:
        usage()
        sys.exit(1)
    output = None
    for o, a in opts:
        if o in (""-h"", ""--help""):
            usage()
            sys.exit()
        elif o in (""-o"", ""--output""):
            output = a
        else:
            usage()
            sys.exit(1)
    if not args:
        usage()
        sys.exit(1)
    concat_flv(args, output)
","elif o in ( ""-o"" , ""--output"" ) :",175
"def binaryFindInDocument():
    hi = len(self.headings)
    lo = 0
    while lo < hi:
        mid = (lo + hi) // 2
        h = self.headings[mid]
        if h.end_of_last_child < position:
            lo = mid + 1
        elif h.start > position:
            hi = mid
        else:
            return binaryFindHeading(h)
",if h . end_of_last_child < position :,110
"def on_key_press(self, *events):
    # The JS editor has already** handled the key!
    for ev in events:
        if self.should_be_leo_key(ev):
            ivar = ""minibufferWidget"" if self.name == ""minibuffer"" else self.name
            self.root.do_key(ev, ivar)
",if self . should_be_leo_key ( ev ) :,91
"def _make_dataset(data_dir):
    data_dir = os.path.expanduser(data_dir)
    if not os.path.isdir(data_dir):
        raise (""{} should be a dir"".format(data_dir))
    images = []
    for root, _, fnames in sorted(os.walk(data_dir, followlinks=True)):
        for fname in sorted(fnames):
            file_path = os.path.join(root, fname)
            if _is_valid_file(file_path):
                images.append(file_path)
    return images
",if _is_valid_file ( file_path ) :,146
"def release(provider, connection, cache=None):
    if cache is not None:
        db_session = cache.db_session
        if db_session is not None and db_session.ddl and cache.saved_fk_state:
            try:
                cursor = connection.cursor()
                sql = ""SET foreign_key_checks = 1""
                if core.local.debug:
                    log_orm(sql)
                cursor.execute(sql)
            except:
                provider.pool.drop(connection)
                raise
    DBAPIProvider.release(provider, connection, cache)
",if db_session is not None and db_session . ddl and cache . saved_fk_state :,164
"def get_pfunctions(self):
    p_functions = []
    for name, item in self.pdict.items():
        if name[:2] != ""p_"":
            continue
        if name == ""p_error"":
            continue
        if isinstance(item, (types.FunctionType, types.MethodType)):
            line = func_code(item).co_firstlineno
            file = func_code(item).co_filename
            p_functions.append((line, file, name, item.__doc__))
    # Sort all of the actions by line number
    p_functions.sort()
    self.pfuncs = p_functions
","if name == ""p_error"" :",158
"def get_output_sizes(self):
    sizes = []
    output_paths = self.get_output_fnames()
    for outfile in [unicodify(o) for o in output_paths]:
        if os.path.exists(outfile):
            sizes.append((outfile, os.stat(outfile).st_size))
        else:
            sizes.append((outfile, 0))
    return sizes
",if os . path . exists ( outfile ) :,100
"def normalize_crlf(tree):
    for elem in tree.getiterator():
        if elem.text:
            elem.text = elem.text.replace(""\r\n"", ""\n"")
        if elem.tail:
            elem.tail = elem.tail.replace(""\r\n"", ""\n"")
",if elem . text :,76
"def visit_decorator(self, o: Decorator) -> None:
    if self.is_private_name(o.func.name, o.func.fullname):
        return
    is_abstract = False
    for decorator in o.original_decorators:
        if isinstance(decorator, NameExpr):
            if self.process_name_expr_decorator(decorator, o):
                is_abstract = True
        elif isinstance(decorator, MemberExpr):
            if self.process_member_expr_decorator(decorator, o):
                is_abstract = True
    self.visit_func_def(o.func, is_abstract=is_abstract)
","if isinstance ( decorator , NameExpr ) :",160
"def formatweekday(self, day, width):
    with TimeEncoding(self.locale) as encoding:
        if width >= 9:
            names = day_name
        else:
            names = day_abbr
        name = names[day]
        if encoding is not None:
            name = name.decode(encoding)
        return name[:width].center(width)
",if width >= 9 :,97
"def autocommitter():
    while True:
        try:
            if not self._running:
                break
            if self._auto_commit_enable:
                self._auto_commit()
            self._cluster.handler.sleep(self._auto_commit_interval_ms / 1000)
        except ReferenceError:
            break
        except Exception:
            # surface all exceptions to the main thread
            self._worker_exception = sys.exc_info()
            break
    log.debug(""Autocommitter thread exiting"")
",if not self . _running :,141
"def pseudo_raw_input(self, prompt):
    """"""copied from cmd's cmdloop; like raw_input, but accounts for changed stdin, stdout""""""
    if self.use_rawinput:
        try:
            line = raw_input(prompt)
        except EOFError:
            line = ""EOF""
    else:
        self.stdout.write(prompt)
        self.stdout.flush()
        line = self.stdin.readline()
        if not len(line):
            line = ""EOF""
        else:
            if line[-1] == ""\n"":  # this was always true in Cmd
                line = line[:-1]
    return line
","if line [ - 1 ] == ""\n"" :",172
"def get_suggestion(self, suggestion):
    if suggestion is None:
        return suggestion
    counter = 0
    results = []
    for feature in self._features:
        if feature in self._discrete_features:
            result, counter = self._get_discrete_suggestion(
                feature=feature, suggestion=suggestion, counter=counter
            )
            results.append(result)
        elif feature in self._categorical_features:
            result, counter = self._get_categorical_suggestion(
                feature=feature, suggestion=suggestion, counter=counter
            )
            results.append(result)
        else:
            results.append(suggestion[counter])
            counter = counter + 1
    return dict(zip(self._features, results))
",elif feature in self . _categorical_features :,198
"def gen_raw_options(modelines):
    for m in modelines:
        opt = m.partition("":"")[2].strip()
        if MULTIOPT_SEP in opt:
            for subopt in (s for s in opt.split(MULTIOPT_SEP)):
                yield subopt
        else:
            yield opt
",if MULTIOPT_SEP in opt :,84
"def _parse_chunked(self, data):
    body = []
    trailers = {}
    n = 0
    lines = data.split(b""\r\n"")
    # parse body
    while True:
        size, chunk = lines[n : n + 2]
        size = int(size, 16)
        if size == 0:
            n += 1
            break
        self.assertEqual(size, len(chunk))
        body.append(chunk)
        n += 2
        # we /should/ hit the end chunk, but check against the size of
        # lines so we're not stuck in an infinite loop should we get
        # malformed data
        if n > len(lines):
            break
    return b"""".join(body)
",if n > len ( lines ) :,191
"def join(s, *p):
    path = s
    for t in p:
        if (not s) or isabs(t):
            path = t
            continue
        if t[:1] == "":"":
            t = t[1:]
        if "":"" not in path:
            path = "":"" + path
        if path[-1:] != "":"":
            path = path + "":""
        path = path + t
    return path
",if ( not s ) or isabs ( t ) :,115
"def validate_route_filter(cmd, namespace):
    from msrestazure.tools import is_valid_resource_id, resource_id
    if namespace.route_filter:
        if not is_valid_resource_id(namespace.route_filter):
            namespace.route_filter = resource_id(
                subscription=get_subscription_id(cmd.cli_ctx),
                resource_group=namespace.resource_group_name,
                namespace=""Microsoft.Network"",
                type=""routeFilters"",
                name=namespace.route_filter,
            )
",if not is_valid_resource_id ( namespace . route_filter ) :,152
"def expanded_output(self):
    """"""Iterate over output files while dynamic output is expanded.""""""
    for f, f_ in zip(self.output, self.rule.output):
        if f in self.dynamic_output:
            expansion = self.expand_dynamic(f_)
            if not expansion:
                yield f_
            for f, _ in expansion:
                file_to_yield = IOFile(f, self.rule)
                file_to_yield.clone_flags(f_)
                yield file_to_yield
        else:
            yield f
",if not expansion :,153
"def prepare_text(text, style):
    body = []
    for fragment, sty in parse_tags(text, style, subs.styles):
        fragment = fragment.replace(r""\h"", "" "")
        fragment = fragment.replace(r""\n"", ""\n"")
        fragment = fragment.replace(r""\N"", ""\n"")
        if sty.italic:
            fragment = ""<i>%s</i>"" % fragment
        if sty.underline:
            fragment = ""<u>%s</u>"" % fragment
        if sty.strikeout:
            fragment = ""<s>%s</s>"" % fragment
        if sty.drawing:
            raise ContentNotUsable
        body.append(fragment)
    return re.sub(""\n+"", ""\n"", """".join(body).strip())
",if sty . drawing :,198
"def decref(self, key, count=1):
    with self._lock:
        slot = self._dict[key]
        if slot[1] < count:
            del self._dict[key]
        else:
            slot[1] -= count
            self._dict[key] = slot
",if slot [ 1 ] < count :,79
"def stale_rec(node, nodes):
    if node.abspath() in node.ctx.env[Build.CFG_FILES]:
        return
    if getattr(node, ""children"", []):
        for x in node.children.values():
            if x.name != ""c4che"":
                stale_rec(x, nodes)
    else:
        for ext in DYNAMIC_EXT:
            if node.name.endswith(ext):
                break
        else:
            if not node in nodes:
                if can_delete(node):
                    Logs.warn(""Removing stale file -> %r"", node)
                    node.delete()
",if node . name . endswith ( ext ) :,177
"def _do_ssl_handshake(self):
    try:
        self.socket.do_handshake()
    except ssl.SSLError as err:
        if err.args[0] in (ssl.SSL_ERROR_WANT_READ, ssl.SSL_ERROR_WANT_WRITE):
            return
        elif err.args[0] == ssl.SSL_ERROR_EOF:
            return self.handle_close()
        raise
    except OSError as err:
        if err.args[0] == errno.ECONNABORTED:
            return self.handle_close()
    else:
        self._ssl_accepting = False
",if err . args [ 0 ] == errno . ECONNABORTED :,161
"def test_full_hd_tv(self):
    cur_test = ""full_hd_tv""
    cur_qual = common.Quality.FULLHDTV
    for name, tests in iteritems(self.test_cases):
        for test in tests:
            if name == cur_test:
                self.assertEqual(cur_qual, common.Quality.name_quality(test))
            else:
                self.assertNotEqual(cur_qual, common.Quality.name_quality(test))
",if name == cur_test :,130
"def debug_tree(tree):
    l = []
    for elt in tree:
        if isinstance(elt, int):
            l.append(_names.get(elt, elt))
        elif isinstance(elt, str):
            l.append(elt)
        else:
            l.append(debug_tree(elt))
    return l
","elif isinstance ( elt , str ) :",89
"def get_all_missing_headers(self):
    # Heavy operation done in one optimized shot
    for chunk_height, expected_hash in reversed(list(self.checkpoints.items())):
        if chunk_height in self.known_missing_checkpointed_chunks:
            continue
        if self.chunk_hash(chunk_height, 1000) != expected_hash:
            self.known_missing_checkpointed_chunks.add(chunk_height)
    return self.known_missing_checkpointed_chunks
","if self . chunk_hash ( chunk_height , 1000 ) != expected_hash :",125
"def get_byname(userId, documentName, session=None):
    if not session:
        session = db.Session
    ret = {}
    result = (
        session.query(LegacyArchiveDocument)
        .filter_by(userId=userId, documentName=documentName)
        .first()
    )
    if result:
        obj = dict(
            (key, value)
            for key, value in vars(result).items()
            if not key.startswith(""_"")
        )
        ret = obj
    return ret
","if not key . startswith ( ""_"" )",141
"def cb(ipdb, msg, action):
    if action == ""RTM_NEWLINK"" and msg.get_attr(""IFLA_IFNAME"", """") in (ifP1, ifP2):
        obj = ipdb.interfaces[msg[""index""]]
        if obj not in ipdb.interfaces[ifM]:
            ipdb.interfaces[ifM].add_port(obj)
        try:
            ipdb.interfaces[ifM].commit()
        except Exception:
            pass
",if obj not in ipdb . interfaces [ ifM ] :,124
"def reorder_encoder_rules(self, nts):
    """"""reorder rules so that any rules with ENCODER_PREFERRED is first""""""
    for nt in nts.values():
        first_rules = []
        rest_of_the_rules = []
        for r in nt.rules:
            if r.conditions.contains(""ENCODER_PREFERRED""):
                first_rules.append(r)
            else:
                rest_of_the_rules.append(r)
        nt.rules = first_rules + rest_of_the_rules
","if r . conditions . contains ( ""ENCODER_PREFERRED"" ) :",143
"def update_url(self, s, keywords):
    pc = self
    w = pc.ensure_text_widget()
    pc.show()
    if 1:
        w.setPlainText("""")
    else:
        url = pc.get_url(s, ""@url"")
        if url:
            w.setPlainText(""@url %s"" % url)
        else:
            w.setPlainText(""@url: no url given"")
",if url :,113
"def _update_engines(self, engines):
    """"""Update our engines dict and _ids from a dict of the form: {id:uuid}.""""""
    for k, v in iteritems(engines):
        eid = int(k)
        if eid not in self._engines:
            self._ids.append(eid)
        self._engines[eid] = v
    self._ids = sorted(self._ids)
    if (
        sorted(self._engines.keys()) != list(range(len(self._engines)))
        and self._task_scheme == ""pure""
        and self._task_socket
    ):
        self._stop_scheduling_tasks()
",if eid not in self . _engines :,173
"def test_delete_chat_thread(self):
    async with self.chat_client:
        await self._create_thread()
        await self.chat_client.delete_chat_thread(self.thread_id)
        # delete created users and chat threads
        if not self.is_playback():
            await self.chat_client.delete_chat_thread(self.thread_id)
",if not self . is_playback ( ) :,98
"def _to_protobuf_matrix(matrix, p_matrix, transformation=None):
    for row in matrix:
        p_row = p_matrix.rows.add()
        for cell in row:
            value = cell
            if transformation:
                value = transformation(value)
            p_row.cells.append(value)
",if transformation :,88
"def apply(self, db, family):
    if self.rtype:
        if self.rtype.is_custom() and self.use_regex:
            if self.regex[0].search(str(family.get_relationship())) is None:
                return False
        elif self.rtype != family.get_relationship():
            return False
    return True
",if self . rtype . is_custom ( ) and self . use_regex :,93
"def get_somatic_variantcallers(items):
    """"""Retrieve all variant callers for somatic calling, handling somatic/germline.""""""
    out = []
    for data in items:
        vcs = dd.get_variantcaller(data)
        if isinstance(vcs, dict) and ""somatic"" in vcs:
            vcs = vcs[""somatic""]
        if not isinstance(vcs, (list, tuple)):
            vcs = [vcs]
        out += vcs
    return set(vcs)
","if isinstance ( vcs , dict ) and ""somatic"" in vcs :",133
"def balancer_list_members(self, balancer):
    lb = self._get_balancer_model(balancer.id)
    members = []
    vs = self._locate_service_group(lb, balancer.port)
    if vs:
        if vs[""serviceGroups""]:
            srvgrp = vs[""serviceGroups""][0]
            members = [self._to_member(srv, balancer) for srv in srvgrp[""services""]]
    return members
","if vs [ ""serviceGroups"" ] :",112
"def https_open(self, req):
    try:
        return self.do_open(do_connection, req)
    except Exception as err_msg:
        try:
            error_msg = str(err_msg.args[0]).split(""] "")[1] + "".""
        except IndexError:
            error_msg = str(err_msg.args[0]) + "".""
        if settings.INIT_TEST == True:
            if settings.VERBOSITY_LEVEL < 2:
                print(settings.FAIL_STATUS)
        else:
            if settings.VERBOSITY_LEVEL < 1:
                print("""")
        print(settings.print_critical_msg(error_msg))
        raise SystemExit()
",if settings . INIT_TEST == True :,187
"def add_libdirs(self, envvar, sep, fatal=False):
    v = os.environ.get(envvar)
    if not v:
        return
    for dir in str.split(v, sep):
        dir = str.strip(dir)
        if not dir:
            continue
        dir = os.path.normpath(dir)
        if os.path.isdir(dir):
            if not dir in self.library_dirs:
                self.library_dirs.append(dir)
        elif fatal:
            fail(""FATAL: bad directory %s in environment variable %s"" % (dir, envvar))
",if not dir in self . library_dirs :,159
"def check_placement_group_index(placement_group: PlacementGroup, bundle_index: int):
    assert placement_group is not None
    if placement_group.id.is_nil():
        if bundle_index != -1:
            raise ValueError(
                ""If placement group is not set, ""
                ""the value of bundle index must be -1.""
            )
    elif bundle_index >= placement_group.bundle_count or bundle_index < -1:
        raise ValueError(
            f""placement group bundle index {bundle_index} ""
            f""is invalid. Valid placement group indexes: ""
            f""0-{placement_group.bundle_count}""
        )
",if bundle_index != - 1 :,178
"def incoming():
    while True:
        m = ws.receive()
        if m is not None:
            m = str(m)
            print((m, len(m)))
            if len(m) == 35:
                ws.close()
                break
        else:
            break
    print((""Connection closed!"",))
",if len ( m ) == 35 :,94
"def walk_tree(
    root: Element,
    processor: Callable[[Element], Optional[_T]],
    stop_after_first: bool = False,
) -> List[_T]:
    results = []
    queue = deque([root])
    while queue:
        currElement = queue.popleft()
        for child in currElement:
            if child:
                queue.append(child)
            result = processor(child)
            if result is not None:
                results.append(result)
                if stop_after_first:
                    return results
    return results
",if stop_after_first :,152
"def _find_node_with_predicate(self, node, predicate):
    if node != self._tree._root and predicate(node):
        return node
    item, cookie = self._tree.GetFirstChild(node)
    while item:
        if predicate(item):
            return item
        if self._tree.ItemHasChildren(item):
            result = self._find_node_with_predicate(item, predicate)
            if result:
                return result
        item, cookie = self._tree.GetNextChild(node, cookie)
    return None
",if predicate ( item ) :,143
"def traverse_coords(coords, dst_coords):
    for p in coords:
        if type(p[0]) is list:
            lst = []
            traverse_coords(p, lst)
            dst_coords.append(lst)
        else:
            x, y = p[0], p[1]
            d = (x + (y - b) * m) / (1 + m * m)
            x2 = 2 * d - x
            y2 = 2 * d * m - y + 2 * b
            dst_coords.append((x2, y2))
    return dst_coords
",if type ( p [ 0 ] ) is list :,161
"def normalize_replies(self, x):
    xs = x.split(""\n"")
    xs2 = []
    for x in xs:
        if ""your persona:"" in x:
            # Normalize the sentence appearing after 'your persona:'
            x = x[len(""your persona: "") :]
            x = normalize_reply(x)
            x = ""your persona: "" + x
        else:
            x = normalize_reply(x)
        xs2.append(x)
    return ""\n"".join(xs2)
","if ""your persona:"" in x :",142
"def run_unittest(*classes):
    suite = unittest.TestSuite()
    for c in classes:
        if isinstance(c, str):
            c = __import__(c)
            for name in dir(c):
                obj = getattr(c, name)
                if isinstance(obj, type) and issubclass(obj, unittest.TestCase):
                    suite.addTest(obj)
        else:
            suite.addTest(c)
    runner = unittest.TestRunner()
    result = runner.run(suite)
","if isinstance ( c , str ) :",136
"def bprop_naive(self, error, permute=False):
    for dst in range(self.ofmsize):
        rflinks = self.links[dst]
        A = error[:, self.ofmlocs[dst]]
        B = self.weights
        if permute:
            inds = np.random.permutation(A.shape[1])
            np.dot(A[:, inds], B[inds, :], self.bpropbuf)
        else:
            np.dot(A, B, self.bpropbuf)
        self.berror[:, rflinks] += self.bpropbuf
",if permute :,150
"def rewrite_order_lookup_key(model, lookup_key):
    try:
        if lookup_key.startswith(""-""):
            return ""-"" + rewrite_lookup_key(model, lookup_key[1:])
        else:
            return rewrite_lookup_key(model, lookup_key)
    except AttributeError:
        return lookup_key
","if lookup_key . startswith ( ""-"" ) :",85
"def test_default_configuration(self):
    transformations = []
    for i in range(2):
        transformation, original = self._test_helper(RescalingChoice, dataset=""boston"")
        # The maximum is around 1.95 for the transformed array...
        self.assertAlmostEqual(np.mean(transformation), 0, places=5)
        self.assertAlmostEqual(np.std(transformation), 1, places=5)
        self.assertFalse((original == transformation).all())
        transformations.append(transformation)
        if len(transformations) > 1:
            self.assertTrue((transformations[-1] == transformations[-2]).all())
",if len ( transformations ) > 1 :,157
"def test_get_filter_text(self):
    with realized(self.b):
        if self.b.can_filter_text():
            self.assertEqual(self.b.get_filter_text(), u"""")
            self.assertTrue(isinstance(self.b.get_filter_text(), str))
            self.b.filter_text(u""foo"")
            self.assertEqual(self.b.get_filter_text(), u""foo"")
            self.assertTrue(isinstance(self.b.get_filter_text(), str))
",if self . b . can_filter_text ( ) :,138
"def _namelist(instance):
    namelist, namedict, classlist = [], {}, [instance.__class__]
    for c in classlist:
        for b in c.__bases__:
            classlist.append(b)
        for name in c.__dict__.keys():
            if not namedict.has_key(name):
                namelist.append(name)
                namedict[name] = 1
    return namelist
",if not namedict . has_key ( name ) :,107
"def resolve_cloudtrail_payload(self, payload):
    sources = self.data.get(""sources"", [])
    events = []
    for e in self.data.get(""events""):
        if not isinstance(e, dict):
            events.append(e)
            event_info = CloudWatchEvents.get(e)
            if event_info is None:
                continue
        else:
            event_info = e
            events.append(e[""event""])
        sources.append(event_info[""source""])
    payload[""detail""] = {""eventSource"": list(set(sources)), ""eventName"": events}
",if event_info is None :,159
"def __setitem__(self, aset, c):
    if isinstance(aset, tuple):
        if isinstance(aset[0], str):
            row = self.rownames.index(aset[0])
        else:
            row = aset[0]
        if isinstance(aset[1], str):
            column = self.colnames.index(aset[1])
        else:
            column = aset[1]
        self.cell_value(row, column, c)
    else:
        Matrix.__setitem__(self, aset, c)
","if isinstance ( aset [ 0 ] , str ) :",144
"def test_retrieve_robots_token_permission(
    username, is_admin, with_permissions, app, client
):
    with client_with_identity(username, client) as cl:
        params = {""orgname"": ""buynlarge"", ""token"": ""true""}
        if with_permissions:
            params[""permissions""] = ""true""
        result = conduct_api_call(cl, OrgRobotList, ""GET"", params, None)
        assert result.json[""robots""]
        for robot in result.json[""robots""]:
            assert (robot.get(""token"") is not None) == is_admin
            assert (robot.get(""repositories"") is not None) == (
                is_admin and with_permissions
            )
",if with_permissions :,192
"def _analyze_ast(contents):
    try:
        return ast.literal_eval(contents)
    except SyntaxError:
        pass
    try:
        # remove all comments
        contents = re.sub(re.compile(r""/\*.*?\*/"", re.DOTALL), """", contents)
        contents = re.sub(re.compile(r""#.*?\n""), """", contents)
        # remove anything before dict declaration like: ""caps = { ...""
        match = re.match(r""^([^{]+)"", contents)
        if match:
            contents = contents.replace(match.group(1), """")
        # and try again
        return ast.literal_eval(contents)
    except SyntaxError:
        pass
    return False
",if match :,179
"def bulk_disable_accounts(account_names):
    """"""Bulk disable accounts""""""
    for account_name in account_names:
        account = Account.query.filter(Account.name == account_name).first()
        if account:
            app.logger.debug(""Disabling account %s"", account.name)
            account.active = False
            db.session.add(account)
    db.session.commit()
    db.session.close()
",if account :,113
"def _add_agent_rewards(self, reward_dict: Dict[AgentID, float]) -> None:
    for agent_id, reward in reward_dict.items():
        if reward is not None:
            self.agent_rewards[agent_id, self.policy_for(agent_id)] += reward
            self.total_reward += reward
            self._agent_reward_history[agent_id].append(reward)
",if reward is not None :,108
"def wrapper(strategy, backend, pipeline_index, *args, **kwargs):
    current_partial = partial_prepare(
        strategy, backend, pipeline_index, *args, **kwargs
    )
    out = (
        func(
            strategy=strategy,
            backend=backend,
            pipeline_index=pipeline_index,
            current_partial=current_partial,
            *args,
            **kwargs
        )
        or {}
    )
    if not isinstance(out, dict):
        strategy.storage.partial.store(current_partial)
        if save_to_session:
            strategy.session_set(PARTIAL_TOKEN_SESSION_NAME, current_partial.token)
    return out
",if save_to_session :,185
"def restore_text(self):
    if self.source_is_console():
        cb = self._last_console_cb
    else:
        cb = self._last_editor_cb
    if cb is None:
        if self.is_plain_text_mode():
            self.plain_text.clear()
        else:
            self.rich_text.clear()
    else:
        func = cb[0]
        args = cb[1:]
        func(*args)
        if get_meth_class_inst(func) is self.rich_text:
            self.switch_to_rich_text()
        else:
            self.switch_to_plain_text()
",if self . is_plain_text_mode ( ) :,180
"def extract_groups(self, text: str, language_code: str):
    previous = None
    group = 1
    groups = []
    words = []
    ignored = IGNORES.get(language_code, {})
    for word in NON_WORD.split(text):
        if not word:
            continue
        if word not in ignored and len(word) >= 2:
            if previous == word:
                group += 1
            elif group > 1:
                groups.append(group)
                words.append(previous)
                group = 1
        previous = word
    if group > 1:
        groups.append(group)
        words.append(previous)
    return groups, words
",if previous == word :,187
"def pendingcalls_thread(self, context):
    try:
        self.pendingcalls_submit(context.l, context.n)
    finally:
        with context.lock:
            context.nFinished += 1
            nFinished = context.nFinished
            if False and support.verbose:
                print(""finished threads: "", nFinished)
        if nFinished == context.nThreads:
            context.event.set()
",if nFinished == context . nThreads :,113
"def __getattr__(self, item: str) -> Any:
    if hasattr(MissingPandasLikeRolling, item):
        property_or_func = getattr(MissingPandasLikeRolling, item)
        if isinstance(property_or_func, property):
            return property_or_func.fget(self)  # type: ignore
        else:
            return partial(property_or_func, self)
    raise AttributeError(item)
","if isinstance ( property_or_func , property ) :",105
"def _csv(self, match=None, dump=None):
    if dump is None:
        dump = self._dump(match)
    for record in dump:
        row = []
        for field in record:
            if isinstance(field, int):
                row.append(""%i"" % field)
            elif field is None:
                row.append("""")
            else:
                row.append(""'%s'"" % field)
        yield "","".join(row)
",elif field is None :,126
"def get_default_dict(section_definition):
    section_key = section_definition.get(""key"")
    if section_key == ""global"":
        section_key += ""_""
    if ""cluster"" == section_key:
        section_key += (
            ""_sit""
            if section_definition.get(""cluster_model"") == ClusterModel.SIT.name
            else ""_hit""
        )
    default_dict = DefaultDict[section_key].value
    return default_dict
","if section_definition . get ( ""cluster_model"" ) == ClusterModel . SIT . name",125
"def scan_resource_conf(self, conf):
    subscription = re.compile(r""\/|\/subscriptions\/[\w\d-]+$|\[subscription\(\).id\]"")
    if ""properties"" in conf:
        if ""assignableScopes"" in conf[""properties""]:
            if any(
                re.match(subscription, scope)
                for scope in conf[""properties""][""assignableScopes""]
            ):
                if ""permissions"" in conf[""properties""]:
                    if conf[""properties""][""permissions""]:
                        for permission in conf[""properties""][""permissions""]:
                            if ""actions"" in permission and ""*"" in permission[""actions""]:
                                return CheckResult.FAILED
    return CheckResult.PASSED
","if ""assignableScopes"" in conf [ ""properties"" ] :",197
"def hard_update(self, cache, size_change, pins_gates):
    """"""replace verts, rads and vel (in NumPy)""""""
    verts, rads, vel, react = cache
    if len(verts) == self.v_len:
        if pins_gates[0] and pins_gates[1]:
            unpinned = self.params[""unpinned""]
            self.verts[unpinned] = verts[unpinned]
        else:
            self.verts = verts
        self.vel = vel
        if not size_change:
            self.rads = rads
",if not size_change :,155
"def run(self):
    if self.check():
        path = ""/../../../../../../../../../../../..{}"".format(self.filename)
        response = self.http_request(method=""GET"", path=path)
        if response is None:
            return
        if response.status_code == 200 and response.text:
            print_success(""Success! File: %s"" % self.filename)
            print_info(response.text)
        else:
            print_error(""Exploit failed"")
    else:
        print_error(""Device seems to be not vulnerable"")
",if response . status_code == 200 and response . text :,153
"def write_text(self, text):
    """"""Writes re-indented text into the buffer.""""""
    should_indent = False
    rows = []
    for row in text.split(""\n""):
        if should_indent:
            row = ""    {}"".format(row)
        if ""\b"" in row:
            row = row.replace(""\b"", """", 1)
            should_indent = True
        elif not len(row.strip()):
            should_indent = False
        rows.append(row)
    self.write(""{}\n"".format(""\n"".join(rows)))
",elif not len ( row . strip ( ) ) :,147
"def default_logger():
    """"""A logger used to output seed information to nosetests logs.""""""
    logger = logging.getLogger(__name__)
    # getLogger() lookups will return the same logger, but only add the handler once.
    if not len(logger.handlers):
        handler = logging.StreamHandler(sys.stderr)
        handler.setFormatter(logging.Formatter(""[%(levelname)s] %(message)s""))
        logger.addHandler(handler)
        if logger.getEffectiveLevel() == logging.NOTSET:
            logger.setLevel(logging.INFO)
    return logger
",if logger . getEffectiveLevel ( ) == logging . NOTSET :,131
"def while1_test(a, b, c):
    while 1:
        if a != 2:
            if b:
                a = 3
                b = 0
            elif c:
                c = 0
            else:
                a += b + c
                break
    return a, b, c
",elif c :,94
"def fetch():
    retval = {}
    content = retrieve_content(__url__)
    if __check__ in content:
        for line in content.split(""\n""):
            line = line.strip()
            if not line or line.startswith(""#"") or ""."" not in line:
                continue
            if "" # "" in line:
                reason = line.split("" # "")[1].split()[0].lower()
                if reason == ""scanning"":  # too many false positives
                    continue
                retval[line.split("" # "")[0]] = (__info__, __reference__)
    return retval
","if reason == ""scanning"" :",157
"def create_order(order, shopify_settings, old_order_sync=False, company=None):
    so = create_sales_order(order, shopify_settings, company)
    if so:
        if order.get(""financial_status"") == ""paid"":
            create_sales_invoice(
                order, shopify_settings, so, old_order_sync=old_order_sync
            )
        if order.get(""fulfillments"") and not old_order_sync:
            create_delivery_note(order, shopify_settings, so)
","if order . get ( ""financial_status"" ) == ""paid"" :",144
"def __getitem__(self, key):
    if isinstance(key, numbers.Number):
        l = len(self)
        if key >= l:
            raise IndexError(""Index %s out of range (%s elements)"" % (key, l))
        if key < 0:
            if key < -l:
                raise IndexError(""Index %s out of range (%s elements)"" % (key, l))
            key += l
        return self(key + 1)
    elif isinstance(key, slice):
        raise ValueError(
            self.impl.__class__.__name__ + "" object does not support slicing""
        )
    else:
        return self(key)
",if key < 0 :,170
"def load_checks(path=None, subpkg=""""):
    """"""Dynamically import all check modules for the side effect of registering checks.""""""
    if path is None:
        path = os.path.dirname(__file__)
    modules = []
    for name in os.listdir(path):
        if os.path.isdir(os.path.join(path, name)):
            modules = modules + load_checks(
                os.path.join(path, name), subpkg + ""."" + name
            )
            continue
        if name.endswith("".py"") and name not in LOADER_EXCLUDES:
            modules.append(import_module(__package__ + subpkg + ""."" + name[:-3]))
    return modules
","if name . endswith ( "".py"" ) and name not in LOADER_EXCLUDES :",174
"def _remove_temporary_files(self, temporary_files):
    """"""Internal function for cleaning temporary files""""""
    for file_object in temporary_files:
        file_name = file_object.name
        file_object.close()
        if os.path.exists(file_name):
            os.remove(file_name)
        arff_file_name = file_name + "".arff""
        if os.path.exists(arff_file_name):
            os.remove(arff_file_name)
",if os . path . exists ( arff_file_name ) :,129
"def search_rotate(array, val):
    low, high = 0, len(array) - 1
    while low <= high:
        mid = (low + high) // 2
        if val == array[mid]:
            return mid
        if array[low] <= array[mid]:
            if array[low] <= val <= array[mid]:
                high = mid - 1
            else:
                low = mid + 1
        else:
            if array[mid] <= val <= array[high]:
                low = mid + 1
            else:
                high = mid - 1
    return -1
",if array [ mid ] <= val <= array [ high ] :,166
"def match_file(self, file, tff_format):
    match = tff_format.search(file.filename.replace(""\\"", ""/""))
    if match:
        result = {}
        for name, value in match.groupdict().items():
            value = value.strip()
            if name in self.numeric_tags:
                value = value.lstrip(""0"")
            if self.ui.replace_underscores.isChecked():
                value = value.replace(""_"", "" "")
            result[name] = value
        return result
    else:
        return {}
",if self . ui . replace_underscores . isChecked ( ) :,149
"def exclude_pkgs(self, pkgs):
    # :api
    name = ""excludepkgs""
    if pkgs is not None and pkgs != []:
        if self._has_option(name):
            self._set_value(name, pkgs, dnf.conf.PRIO_COMMANDLINE)
        else:
            logger.warning(
                _(""Unknown configuration option: %s = %s""), ucd(name), ucd(pkgs)
            )
",if self . _has_option ( name ) :,113
"def button_press_cb(self, tdw, event):
    if self.zone in (_EditZone.CREATE_AXIS, _EditZone.DELETE_AXIS):
        button = event.button
        if button == 1 and event.type == Gdk.EventType.BUTTON_PRESS:
            self._click_info = (button, self.zone)
            return False
    return super(SymmetryEditMode, self).button_press_cb(tdw, event)
",if button == 1 and event . type == Gdk . EventType . BUTTON_PRESS :,114
"def declare_var(
    self,
    type_name: Union[str, Tuple[str, str]],
    *,
    var_name: str = """",
    var_name_prefix: str = ""v"",
    shared: bool = False,
) -> str:
    if shared:
        if not var_name:
            var_name = var_name_prefix
        if var_name not in self.shared_vars:
            self.declarations.append((var_name, type_name))
            self.shared_vars.add(var_name)
    else:
        if not var_name:
            var_name = self.get_var_name(var_name_prefix)
        self.declarations.append((var_name, type_name))
    return var_name
",if not var_name :,197
"def get_module_map(module, module_path):
    """"""Map true modules to exported name""""""
    if not module_is_public(module):
        return {}
    m = {}
    for symbol_name in dir(module):
        if symbol_name.startswith(""_""):
            continue
        symbol = getattr(module, symbol_name)
        symbol_path = ""%s.%s"" % (module_path, symbol_name)
        m[symbol] = symbol_path
        if inspect.ismodule(symbol):
            m.update(get_module_map(symbol, symbol_path))
    return m
",if inspect . ismodule ( symbol ) :,152
"def build_properties(self):
    self.properties = set()
    if self.module.partial_scan == True:
        # For partial scans, only check the most common properties values
        for prop in self.COMMON_PROPERTIES:
            self.properties.add(chr(prop))
    else:
        for pb in range(0, 9):
            for lp in range(0, 5):
                for lc in range(0, 5):
                    prop = self.build_property(pb, lp, lc)
                    if prop is not None:
                        self.properties.add(chr(prop))
",if prop is not None :,166
"def getFileIdFromAlternateLink(altLink):
    loc = altLink.find(""/d/"")
    if loc > 0:
        fileId = altLink[loc + 3 :]
        loc = fileId.find(""/"")
        if loc != -1:
            return fileId[:loc]
    else:
        loc = altLink.find(""/folderview?id="")
        if loc > 0:
            fileId = altLink[loc + 15 :]
            loc = fileId.find(""&"")
            if loc != -1:
                return fileId[:loc]
    controlflow.system_error_exit(
        2, f""{altLink} is not a valid Drive File alternateLink""
    )
",if loc > 0 :,180
"def _coerce_trials_data(data, path):
    if not isinstance(data, list):
        if not isinstance(data, dict):
            raise BatchFileError(
                path,
                ""invalid data type for trials: expected list or dict""
                "", got %s"" % type(data).__name__,
            )
        data = [data]
    for item in data:
        if not isinstance(item, dict):
            raise BatchFileError(
                path, ""invalid data type for trial %r: expected dict"" % item
            )
    return data
","if not isinstance ( item , dict ) :",152
"def remove(self, *objs):
    val = getattr(self.instance, attname)
    for obj in objs:
        # Is obj actually part of this descriptor set?
        if getattr(obj, rel_field.attname) == val:
            setattr(obj, rel_field.name, None)
            obj.save()
        else:
            raise rel_field.rel.to.DoesNotExist(
                ""%r is not related to %r."" % (obj, self.instance)
            )
","if getattr ( obj , rel_field . attname ) == val :",130
"def run(self):
    try:
        if not self.shell:
            self.shell = os.name == ""nt""
        if self.working_dir != """":
            os.chdir(self.working_dir)
        proc = subprocess.Popen(
            self.command,
            stdout=subprocess.PIPE,
            stderr=subprocess.STDOUT,
            shell=self.shell,
            env=self.env,
        )
        output = codecs.decode(proc.communicate()[0])
        self.on_done(output)
    except subprocess.CalledProcessError as e:
        self.on_done(e.returncode, error=True)
    except OSError as e:
        self.on_done(e.message, error=True)
",if not self . shell :,196
"def filter_testsuite(suite, matcher, level=None):
    """"""Returns a flattened list of test cases that match the given matcher.""""""
    if not isinstance(suite, unittest.TestSuite):
        raise TypeError(""not a TestSuite"", suite)
    results = []
    for test in suite._tests:
        if level is not None and getattr(test, ""level"", 0) > level:
            continue
        if isinstance(test, unittest.TestCase):
            testname = test.id()  # package.module.class.method
            if matcher(testname):
                results.append(test)
        else:
            filtered = filter_testsuite(test, matcher, level)
            results.extend(filtered)
    return results
","if isinstance ( test , unittest . TestCase ) :",185
"def propagate_touch_to_touchable_widgets(self, touch, touch_event, *args):
    triggered = False
    for i in self._touchable_widgets:
        if i.collide_point(touch.x, touch.y):
            triggered = True
            if touch_event == ""down"":
                i.on_touch_down(touch)
            elif touch_event == ""move"":
                i.on_touch_move(touch, *args)
            elif touch_event == ""up"":
                i.on_touch_up(touch)
    return triggered
","elif touch_event == ""up"" :",154
"def add_attributes(attributes, all_base64):
    lines = []
    oc_attr = None
    # objectclass first, even if this is not specified in the RFC
    for attr in attributes:
        if attr.lower() == ""objectclass"":
            for val in attributes[attr]:
                lines.append(_convert_to_ldif(attr, val, all_base64))
            oc_attr = attr
            break
    # remaining attributes
    for attr in attributes:
        if attr != oc_attr and attr in attributes:
            for val in attributes[attr]:
                lines.append(_convert_to_ldif(attr, val, all_base64))
    return lines
",if attr != oc_attr and attr in attributes :,177
"def split_quality(quality):
    anyQualities = []
    bestQualities = []
    for curQual in Quality.qualityStrings.keys():
        if curQual & quality:
            anyQualities.append(curQual)
        if curQual << 16 & quality:
            bestQualities.append(curQual)
    return sorted(anyQualities), sorted(bestQualities)
",if curQual & quality :,109
"def check(dbdef):
    ""database version must include required keys""
    for vnum, vdef in dbdef.items():
        missing = set(required) - set(vdef)
        if vnum == min(dbdef):
            missing -= set(initially_ok)
        if missing:
            yield vnum, missing
",if missing :,86
"def teardown_func():
    try:
        yield
    finally:
        ""tear down test fixtures""
        cache = os.path.join(here, ""data"", ""cache.db"")
        if os.path.exists(cache):
            os.remove(cache)
",if os . path . exists ( cache ) :,70
"def getCachedArt(albumid):
    from headphones import cache
    c = cache.Cache()
    artwork_path = c.get_artwork_from_cache(AlbumID=albumid)
    if not artwork_path:
        return
    if artwork_path.startswith(""http://""):
        artwork = request.request_content(artwork_path, timeout=20)
        if not artwork:
            logger.warn(""Unable to open url: %s"", artwork_path)
            return
    else:
        with open(artwork_path, ""r"") as fp:
            return fp.read()
",if not artwork :,148
"def delete_volume(self, name, reraise=False):
    try:
        self.k8s_api.delete_persistent_volume(
            name=name,
            body=client.V1DeleteOptions(api_version=constants.K8S_API_VERSION_V1),
        )
        logger.debug(""Volume `{}` Deleted"".format(name))
    except ApiException as e:
        if reraise:
            raise PolyaxonK8SError(""Connection error: %s"" % e) from e
        else:
            logger.debug(""Volume `{}` was not found"".format(name))
",if reraise :,153
"def _hashable(self):
    hashes = [self.graph.md5()]
    for g in self.geometry.values():
        if hasattr(g, ""md5""):
            hashes.append(g.md5())
        elif hasattr(g, ""tostring""):
            hashes.append(str(hash(g.tostring())))
        else:
            # try to just straight up hash
            # this may raise errors
            hashes.append(str(hash(g)))
    hashable = """".join(sorted(hashes)).encode(""utf-8"")
    return hashable
","if hasattr ( g , ""md5"" ) :",144
"def get_history_data(self, guid, count=1):
    history = {}
    if count < 1:
        return history
    key = self._make_key(guid)
    for i in range(0, self.db.llen(key)):
        r = self.db.lindex(key, i)
        c = msgpack.unpackb(r)
        if c[""tries""] == 0 or c[""tries""] is None:
            if c[""data""] not in history:
                history[c[""data""]] = c[""timestamp""]
                if len(history) >= count:
                    break
    return history
","if c [ ""tries"" ] == 0 or c [ ""tries"" ] is None :",161
"def renderable_events(self, date, hour):
    ""Returns the number of renderable events""
    renderable_events = []
    for event in self.events:
        if event.covers(date, hour):
            renderable_events.append(event)
    if hour:
        for current in renderable_events:
            for event in self.events:
                if event not in renderable_events:
                    for hour in range(self.start_hour, self.end_hour):
                        if current.covers(date, hour) and event.covers(date, hour):
                            renderable_events.append(event)
                            break
    return renderable_events
","if event . covers ( date , hour ) :",191
"def TryMerge(self, d):
    while d.avail() > 0:
        tt = d.getVarInt32()
        if tt == 10:
            self.set_module(d.getPrefixedString())
            continue
        if tt == 18:
            self.set_version(d.getPrefixedString())
            continue
        if tt == 0:
            raise ProtocolBuffer.ProtocolBufferDecodeError
        d.skipData(tt)
",if tt == 10 :,120
"def _parseConfigFile(self, iniPath, createConfig=True):
    parser = SafeConfigParserUnicode(strict=False)
    if not os.path.isfile(iniPath):
        if createConfig:
            open(iniPath, ""w"").close()
        else:
            return
    parser.readfp(codecs.open(iniPath, ""r"", ""utf_8_sig""))
    for section, options in list(self._iniStructure.items()):
        if parser.has_section(section):
            for option in options:
                if parser.has_option(section, option):
                    self._config[option] = parser.get(section, option)
",if parser . has_section ( section ) :,170
"def get_block_id_at_height(store, height, descendant_id):
    if height is None:
        return None
    while True:
        block = store._load_block(descendant_id)
        if block[""height""] == height:
            return descendant_id
        descendant_id = block[
            ""search_id""
            if util.get_search_height(block[""height""]) >= height
            else ""prev_id""
        ]
","if block [ ""height"" ] == height :",122
"def wait_services_ready(selectors, min_counts, count_fun, timeout=None):
    readies = [0] * len(selectors)
    start_time = time.time()
    while True:
        all_satisfy = True
        for idx, selector in enumerate(selectors):
            if readies[idx] < min_counts[idx]:
                all_satisfy = False
                readies[idx] = count_fun(selector)
                break
        if all_satisfy:
            break
        if timeout and timeout + start_time < time.time():
            raise TimeoutError(""Wait cluster start timeout"")
        time.sleep(1)
",if timeout and timeout + start_time < time . time ( ) :,167
"def waitForNodes(self, expected, comparison=None, tag_filters={}):
    MAX_ITER = 50
    for i in range(MAX_ITER):
        n = len(self.provider.non_terminated_nodes(tag_filters))
        if comparison is None:
            comparison = self.assertEqual
        try:
            comparison(n, expected)
            return
        except Exception:
            if i == MAX_ITER - 1:
                raise
        time.sleep(0.1)
",if i == MAX_ITER - 1 :,130
"def _api_snapshot_delete(self, drbd_rsc_name, snap_name):
    with lin_drv(self.default_uri) as lin:
        if not lin.connected:
            lin.connect()
        snap_reply = lin.snapshot_delete(
            rsc_name=drbd_rsc_name, snapshot_name=snap_name
        )
        return snap_reply
",if not lin . connected :,107
"def response(resp):
    results = []
    search_results = loads(resp.text)
    # return empty array if there are no results
    if not search_results.get(""query"", {}).get(""search""):
        return []
    # parse results
    for result in search_results[""query""][""search""]:
        if result.get(""snippet"", """").startswith(""#REDIRECT""):
            continue
        url = (
            base_url.format(language=resp.search_params[""language""])
            + ""wiki/""
            + quote(result[""title""].replace("" "", ""_"").encode(""utf-8""))
        )
        # append result
        results.append({""url"": url, ""title"": result[""title""], ""content"": """"})
    # return results
    return results
","if result . get ( ""snippet"" , """" ) . startswith ( ""#REDIRECT"" ) :",191
"def getBody(self, path):
    if path == """":
        return ""This server has "" + str(self.__fileProvider.count()) + "" files.""
    else:
        downloadCounts = self.__fileProvider.get(path).downloadCount
        if path in downloadCounts:
            return str(downloadCounts[path])
        else:
            return ""0""
",if path in downloadCounts :,93
"def parse_entrypoints(self, content: str, root=None) -> RootDependency:
    if root is None:
        root = RootDependency()
    entrypoints = []
    group = ""console_scripts""
    for line in content.split(""\n""):
        line = line.strip()
        if not line or line[0] in ""#;"":  # ignore comments
            continue
        if line[0] == ""["" and line[-1] == ""]"":
            group = line[1:-1]
        else:
            entrypoints.append(EntryPoint.parse(text=line, group=group))
    root.entrypoints = tuple(entrypoints)
    return root
","if line [ 0 ] == ""["" and line [ - 1 ] == ""]"" :",162
"def _validate_callbacks(cls, callbacks):
    for callback in callbacks:
        if not isinstance(callback, Callback):
            if issubclass(callback, Callback):
                raise TypeError(""Make sure to instantiate the callbacks."")
            raise TypeError(""Only accepts a `callbacks` instance."")
","if not isinstance ( callback , Callback ) :",70
"def detab(self, text):
    """"""Remove a tab from the front of each line of the given text.""""""
    newtext = []
    lines = text.split(""\n"")
    for line in lines:
        if line.startswith("" "" * markdown.TAB_LENGTH):
            newtext.append(line[markdown.TAB_LENGTH :])
        elif not line.strip():
            newtext.append("""")
        else:
            break
    return ""\n"".join(newtext), ""\n"".join(lines[len(newtext) :])
","if line . startswith ( "" "" * markdown . TAB_LENGTH ) :",134
"def triger_check_network(self, fail=False, force=False):
    time_now = time.time()
    if not force:
        if self._checking_num > 0:
            return
        if fail or self.network_stat != ""OK"":
            # Fail or unknown
            if time_now - self.last_check_time < 3:
                return
        else:
            if time_now - self.last_check_time < 10:
                return
    self.last_check_time = time_now
    threading.Thread(target=self._simple_check_worker).start()
","if fail or self . network_stat != ""OK"" :",161
"def wrapper(*args, **kwargs):
    if is_profiling_enabled(section):
        global _profile_nesting
        profile = get_global_profile()
        _profile_nesting += 1
        if _profile_nesting == 1:
            profile.enable()
        result = func(*args, **kwargs)
        _profile_nesting -= 1
        if _profile_nesting == 0:
            profile.disable()
        return result
    else:
        return func(*args, **kwargs)
",if _profile_nesting == 1 :,128
"def get_sequence_type_str(x: Sequence[Any]) -> str:
    container_type = type(x).__name__
    if not x:
        if container_type == ""list"":
            return ""[]""
        else:
            return container_type + ""([])""
    elem_type = get_type_str(x[0])
    if container_type == ""list"":
        if len(x) == 1:
            return ""["" + elem_type + ""]""
        else:
            return ""["" + elem_type + "", ...]""
    else:
        if len(x) == 1:
            return f""{container_type}([{elem_type}])""
        else:
            return f""{container_type}([{elem_type}, ...])""
","if container_type == ""list"" :",196
"def attempts(self):
    # We can cache as we deal with history server
    if not hasattr(self, ""_attempts""):
        task_attempts = self.job.api.task_attempts(self.job.id, self.id)[""taskAttempts""]
        if task_attempts:
            self._attempts = [
                Attempt(self, attempt) for attempt in task_attempts[""taskAttempt""]
            ]
        else:
            self._attempts = []
    return self._attempts
",if task_attempts :,122
"def __call__(self, message, keyname):
    if keyname in self.keyring:
        key = self.keyring[keyname]
        if isinstance(key, Key) and key.algorithm == GSS_TSIG:
            if message:
                GSSTSigAdapter.parse_tkey_and_step(key, message, keyname)
        return key
    else:
        return None
",if message :,98
"def location_dec(str):
    head = int(str[0])
    str = str[1:]
    rows = head
    cols = int(len(str) / rows) + 1
    out = """"
    full_row = len(str) % head
    for c in range(cols):
        for r in range(rows):
            if c == (cols - 1) and r >= full_row:
                continue
            if r < full_row:
                char = str[r * cols + c]
            else:
                char = str[cols * full_row + (r - full_row) * (cols - 1) + c]
            out += char
    return parse.unquote(out).replace(""^"", ""0"")
",if c == ( cols - 1 ) and r >= full_row :,192
"def request(self):
    if ""Cookie"" in self._req._headers:
        c = self._req._headers[""Cookie""].split(""; "")
        if c[0]:
            return cookies.cookie({x[0]: x[2] for x in [x.partition(""="") for x in c]})
    return cookies.cookie({})
",if c [ 0 ] :,80
"def bulk_enable_accounts(account_names):
    """"""Bulk enable accounts""""""
    for account_name in account_names:
        account = Account.query.filter(Account.name == account_name).first()
        if account:
            app.logger.debug(""Enabling account %s"", account.name)
            account.active = True
            db.session.add(account)
    db.session.commit()
    db.session.close()
",if account :,114
"def acquire(self, blocking=True, timeout=None):
    if not blocking and timeout is not None:
        raise ValueError(""can't specify timeout for non-blocking acquire"")
    rc = False
    endtime = None
    self._cond.acquire()
    while self._value == 0:
        if not blocking:
            break
        if timeout is not None:
            if endtime is None:
                endtime = _time() + timeout
            else:
                timeout = endtime - _time()
                if timeout <= 0:
                    break
        self._cond.wait(timeout)
    else:
        self._value = self._value - 1
        rc = True
    self._cond.release()
    return rc
",if endtime is None :,194
"def _sorted_layers(self, structure, top_layer_id):
    """"""Return the image layers sorted""""""
    sorted_layers = []
    next_layer = top_layer_id
    while next_layer:
        sorted_layers.append(next_layer)
        if ""json"" not in structure[""repolayers""][next_layer]:  # v2
            break
        if ""parent"" not in structure[""repolayers""][next_layer][""json""]:
            break
        next_layer = structure[""repolayers""][next_layer][""json""][""parent""]
        if not next_layer:
            break
    return sorted_layers
","if ""parent"" not in structure [ ""repolayers"" ] [ next_layer ] [ ""json"" ] :",162
"def on_change(self, data):
    # loop over tp_clipboard views
    for window in sublime.windows():
        for view in window.views():
            if view.get_status(""inactive"") and view.settings().get(""tp_append"", False):
                file_name = view.file_name()
                # ammo
                if view.settings().get(""tp_ammo"", False):
                    self.update(view)
                elif file_name and file_name.endswith(
                    global_settings(""ammo_file_extension"", "".ammo"")
                ):
                    self.update(view)
","if view . get_status ( ""inactive"" ) and view . settings ( ) . get ( ""tp_append"" , False ) :",175
"def _maintain_pool(self):
    waiting = self._docker_interface.services_waiting_by_constraints()
    active = self._docker_interface.nodes_active_by_constraints()
    for constraints, needed_dict in self._state.slots_needed(waiting, active).items():
        services = needed_dict[""services""]
        nodes = needed_dict[""nodes""]
        slots_needed = needed_dict[""slots_needed""]
        if slots_needed > 0:
            self._spawn_nodes(constraints, services, slots_needed)
        elif slots_needed < 0:
            self._destroy_nodes(constraints, nodes, slots_needed)
",elif slots_needed < 0 :,164
"def _update_vhosts_addrs_ssl(self, vhosts):
    """"""Update a list of raw parsed vhosts to include global address sslishness""""""
    addr_to_ssl = self._build_addr_to_ssl()
    for vhost in vhosts:
        for addr in vhost.addrs:
            addr.ssl = addr_to_ssl[addr.normalized_tuple()]
            if addr.ssl:
                vhost.ssl = True
",if addr . ssl :,114
"def gather_files(fileset):
    common_type = get_common_filetype(fileset)
    files = []
    for file in fileset.file:
        filename = file.name
        if file.is_include_file == True:
            filename = {}
            filename[file.name] = {""is_include_file"": True}
        if file.file_type != common_type:
            if type(filename) == str:
                filename = {}
            filename[file.name] = {""file_type"": file.file_type}
        files.append(filename)
    return files
",if file . is_include_file == True :,158
"def _get_resource_group_name_of_staticsite(client, static_site_name):
    static_sites = client.list()
    for static_site in static_sites:
        if static_site.name.lower() == static_site_name.lower():
            resource_group = _parse_resource_group_from_arm_id(static_site.id)
            if resource_group:
                return resource_group
    raise CLIError(
        ""Static site was '{}' not found in subscription."".format(static_site_name)
    )
",if static_site . name . lower ( ) == static_site_name . lower ( ) :,144
"def triger_check_network(self, fail=False, force=False):
    time_now = time.time()
    if not force:
        if self._checking_num > 0:
            return
        if fail or self.network_stat != ""OK"":
            # Fail or unknown
            if time_now - self.last_check_time < 3:
                return
        else:
            if time_now - self.last_check_time < 10:
                return
    self.last_check_time = time_now
    threading.Thread(target=self._simple_check_worker).start()
",if self . _checking_num > 0 :,161
"def _gen():
    for i in dataset():
        if isinstance(i, tuple) or isinstance(i, list):
            if fn(*i) is True:
                yield i
        else:
            if fn(i) is True:
                yield i
",if fn ( * i ) is True :,72
"def _merge_dict(d1, d2):
    # Modifies d1 in-place to take values from d2
    # if the nested keys from d2 are present in d1.
    # https://stackoverflow.com/a/10704003/4488789
    for k, v2 in d2.items():
        v1 = d1.get(k)  # returns None if v1 has no such key
        if v1 is None:
            raise Exception(""{} is not recognized by client_config"".format(k))
        if isinstance(v1, Mapping) and isinstance(v2, Mapping):
            _merge_dict(v1, v2)
        else:
            d1[k] = v2
    return d1
","if isinstance ( v1 , Mapping ) and isinstance ( v2 , Mapping ) :",184
"def OnRelease(self, evt):
    if self.isDrag:
        parent = self.GetParent()
        DrawSash(parent, self.px, self.py, self.side)
        self.ReleaseMouse()
        self.isDrag = False
        if self.side == MV_HOR:
            parent.AddLeaf(MV_VER, self.py)
        else:
            parent.AddLeaf(MV_HOR, self.px)
    else:
        evt.Skip()
",if self . side == MV_HOR :,131
"def check_zookeeper_metrics():
    response = get_metrics_prom(dcos_api_session, dcos_api_session.masters[0])
    for family in text_string_to_metric_families(response.text):
        for sample in family.samples:
            if sample[0] == ""zookeeper_avg_latency"":
                assert sample[1][""dcos_component_name""] == ""ZooKeeper""
                return
    raise Exception(""Expected ZooKeeper zookeeper_avg_latency metric not found"")
","if sample [ 0 ] == ""zookeeper_avg_latency"" :",141
"def scan_patterns(self, kind):
    """"""Parse the config section into a list of patterns, preserving order.""""""
    d = self.scan_d(kind)
    aList = []
    seen = set()
    for key in d:
        value = d.get(key)
        if key in seen:
            g.trace(""duplicate key"", key)
        else:
            seen.add(key)
            aList.append(self.msf.Pattern(key, value))
    return aList
",if key in seen :,129
"def foundNestedPseudoClass(self):
    i = self.pos + 1
    openParen = 0
    while i < len(self.source_text):
        ch = self.source_text[i]
        if ch == ""{"":
            return True
        elif ch == ""("":
            # pseudoclasses can contain ()
            openParen += 1
        elif ch == "")"":
            if openParen == 0:
                return False
            openParen -= 1
        elif ch == "";"" or ch == ""}"":
            return False
        i += 1
    return False
",if openParen == 0 :,155
"def append(self, child):
    if child not in (None, self):
        tag = child_tag(self._tag)
        if tag:
            if isinstance(child, Html):
                if child.tag != tag:
                    child = Html(tag, child)
            elif not child.startswith(""<%s"" % tag):
                child = Html(tag, child)
        super().append(child)
","if isinstance ( child , Html ) :",113
"def forward(self, x, activate=True, norm=True):
    for layer in self.order:
        if layer == ""conv"":
            if self.with_explicit_padding:
                x = self.padding_layer(x)
            x = self.conv(x)
        elif layer == ""norm"" and norm and self.with_norm:
            x = self.norm(x)
        elif layer == ""act"" and activate and self.with_activation:
            x = self.activate(x)
    return x
",if self . with_explicit_padding :,138
"def get_tasks(self):
    for task in asyncio.all_tasks(loop=self.middleware.loop):
        formatted = None
        frame = None
        frames = []
        for frame in task.get_stack():
            cur_frame = get_frame_details(frame, self.logger)
            if cur_frame:
                frames.append(cur_frame)
        if frame:
            formatted = traceback.format_stack(frame)
        yield {
            ""stack"": formatted,
            ""frames"": frames,
        }
",if cur_frame :,146
"def _read_row_from_packet(self, packet):
    row = []
    for encoding, converter in self.converters:
        try:
            data = packet.read_length_coded_string()
        except IndexError:
            # No more columns in this row
            # See https://github.com/PyMySQL/PyMySQL/pull/434
            break
        if data is not None:
            if encoding is not None:
                data = data.decode(encoding)
            if DEBUG:
                print(""DEBUG: DATA = "", data)
            if converter is not None:
                data = converter(data)
        row.append(data)
    return tuple(row)
",if DEBUG :,186
"def get_child(self, name):
    if self.isdir:
        try:
            return self.data[name]
        except:
            if not self.case_sensitive:
                for childname, child in list(self.data.items()):
                    if childname.lower() == name.lower():
                        return child
            raise
",if childname . lower ( ) == name . lower ( ) :,100
"def _line_generator(fh, skip_blanks=False, strip=True):
    for line in fh:
        if strip:
            line = line.strip()
        skip = False
        if skip_blanks:
            skip = line.isspace() or not line
        if not skip:
            yield line
",if skip_blanks :,82
"def atleast_3d(*arys):
    if len(arys) == 1:
        arr = array(arys[0])
        if ndim(arr) == 0:
            arr = expand_dims(arr, axis=(0, 1, 2))
        elif ndim(arr) == 1:
            arr = expand_dims(arr, axis=(0, 2))
        elif ndim(arr) == 2:
            arr = expand_dims(arr, axis=2)
        return arr
    else:
        return [atleast_3d(arr) for arr in arys]
",elif ndim ( arr ) == 1 :,147
"def scan_resource_conf(self, conf):
    os_profile = conf.get(""os_profile"")
    if os_profile:
        os_profile = os_profile[0]
        custom_data = os_profile.get(""custom_data"")
        if custom_data:
            custom_data = custom_data[0]
            if isinstance(custom_data, str):
                if string_has_secrets(custom_data):
                    return CheckResult.FAILED
    return CheckResult.PASSED
",if string_has_secrets ( custom_data ) :,134
"def __call__(self, trainer):
    observation = trainer.observation
    if self.key in observation:
        loss = observation[self.key]
        if self.best_model == -1 or loss < self.min_loss:
            self.min_loss = loss
            self.best_model = trainer.updater.epoch
            src = ""%s.%d"" % (self.prefix, self.best_model)
            dest = os.path.join(trainer.out, ""%s.%s"" % (self.prefix, self.suffix))
            if os.path.lexists(dest):
                os.remove(dest)
            os.symlink(src, dest)
            logging.info(""best model is "" + src)
",if self . best_model == - 1 or loss < self . min_loss :,190
"def dump_prefs(self):
    ret = """"
    for pref in self.prefs:
        if type(self.prefs[pref].value) == int:
            value = str(self.prefs[pref].value)
        elif type(self.prefs[pref].value) == bool:
            value = ""true"" if self.prefs[pref].value == True else ""false""
        else:
            value = '""%s""' % self.prefs[pref].value
        ret += pref + "": "" + value + "" ("" + self.prefs[pref].anon_source + "")\n""
    return ret
",if type ( self . prefs [ pref ] . value ) == int :,150
"def translate_isinstance(
    builder: IRBuilder, expr: CallExpr, callee: RefExpr
) -> Optional[Value]:
    # Special case builtins.isinstance
    if (
        len(expr.args) == 2
        and expr.arg_kinds == [ARG_POS, ARG_POS]
        and isinstance(expr.args[1], (RefExpr, TupleExpr))
    ):
        irs = builder.flatten_classes(expr.args[1])
        if irs is not None:
            return builder.builder.isinstance_helper(
                builder.accept(expr.args[0]), irs, expr.line
            )
    return None
",if irs is not None :,155
"def autoname(self):
    naming_method = frappe.db.get_value(""HR Settings"", None, ""emp_created_by"")
    if not naming_method:
        throw(_(""Please setup Employee Naming System in Human Resource > HR Settings""))
    else:
        if naming_method == ""Naming Series"":
            set_name_by_naming_series(self)
        elif naming_method == ""Employee Number"":
            self.name = self.employee_number
        elif naming_method == ""Full Name"":
            self.set_employee_name()
            self.name = self.employee_name
    self.employee = self.name
","if naming_method == ""Naming Series"" :",169
"def search_expr(sheet, expr, reverse=False):
    for i in rotateRange(len(sheet.rows), sheet.cursorRowIndex, reverse=reverse):
        try:
            if sheet.evalExpr(expr, sheet.rows[i]):
                sheet.cursorRowIndex = i
                return
        except Exception as e:
            vd.exceptionCaught(e)
    vd.fail(f""no {sheet.rowtype} where {expr}"")
","if sheet . evalExpr ( expr , sheet . rows [ i ] ) :",119
"def _targets(self, urls, querystring):
    for input, output in urls:
        response = self.client.get(u""/1/%s"" % input, follow=True)
        if output == 404:
            eq_(404, response.status_code)
        elif output.startswith(""http""):
            chain = [u[0] for u in response.redirect_chain]
            assert output in chain
        else:
            r = response.redirect_chain
            r.reverse()
            final = urlparse(r[0][0])
            eq_(output, final.path)
            eq_(querystring, final.query)
","elif output . startswith ( ""http"" ) :",167
"def get_local_cache(self, past, data, from_file, temp_id):
    """"""parse individual cached geometry if there is any""""""
    cache = []
    if self.accumulative:
        if from_file and len(past) > 0:
            cache = past[temp_id]
        if not from_file and len(data) > 0:
            cache = data.get(temp_id, [])
    return cache
",if not from_file and len ( data ) > 0 :,109
"def _parse_abbrev_table(self):
    """"""Parse the abbrev table from the stream""""""
    map = {}
    self.stream.seek(self.offset)
    while True:
        decl_code = struct_parse(
            struct=self.structs.Dwarf_uleb128(""""), stream=self.stream
        )
        if decl_code == 0:
            break
        declaration = struct_parse(
            struct=self.structs.Dwarf_abbrev_declaration, stream=self.stream
        )
        map[decl_code] = AbbrevDecl(decl_code, declaration)
    return map
",if decl_code == 0 :,157
"def mFRIDAY(
    self,
):
    try:
        _type = FRIDAY
        _channel = DEFAULT_CHANNEL
        pass
        self.match(""fri"")
        alt10 = 2
        LA10_0 = self.input.LA(1)
        if LA10_0 == 100:
            alt10 = 1
        if alt10 == 1:
            pass
            self.match(""day"")
        self._state.type = _type
        self._state.channel = _channel
    finally:
        pass
",if alt10 == 1 :,144
"def __getattr__(self, key):
    from mongokit.schema_document import i18n
    if key in self:
        if isinstance(self[key], i18n):
            if self._doc._current_lang not in self[key]:
                return self[key].get(self._doc._fallback_lang)
            return self[key][self._doc._current_lang]
        return self[key]
",if self . _doc . _current_lang not in self [ key ] :,108
"def compact_repr(record):
    parts = []
    for key in record.__attributes__:
        value = getattr(record, key)
        if not value:
            continue
        if isinstance(value, list):
            value = HIDE_LIST
        elif key == FEATS:
            value = format_feats(value)
        else:
            value = repr(value)
        value = capped_str(value)
        parts.append(""%s=%s"" % (key, value))
    return ""%s(%s)"" % (record.__class__.__name__, "", "".join(parts))
","if isinstance ( value , list ) :",152
"def pre_validate(self, form):
    if self.data:
        values = list(c[0] for c in self.choices)
        for d in self.data:
            if d not in values:
                raise ValueError(
                    self.gettext(u""'%(value)s' is not a valid choice for this field"")
                    % dict(value=d)
                )
",if d not in values :,108
"def _sql_like_to_regex(pattern, escape):
    cur_i = 0
    pattern_length = len(pattern)
    while cur_i < pattern_length:
        nxt_i = cur_i + 1
        cur = pattern[cur_i]
        nxt = pattern[nxt_i] if nxt_i < pattern_length else None
        skip = 1
        if nxt is not None and escape is not None and cur == escape:
            yield nxt
            skip = 2
        elif cur == ""%"":
            yield "".*""
        elif cur == ""_"":
            yield "".""
        else:
            yield cur
        cur_i += skip
","elif cur == ""%"" :",169
"def find_caller(stack):
    """"""Finds info about first non-sqlalchemy call in stack""""""
    for frame in stack:
        # We don't care about sqlalchemy internals
        module = inspect.getmodule(frame[0])
        if not hasattr(module, ""__name__""):
            continue
        if module.__name__.startswith(""sqlalchemy""):
            continue
        return (module.__name__,) + tuple(frame[2:4]) + (frame[4][0].strip(),)
    log.warning(""Transaction from unknown origin"")
    return None, None, None, None
","if not hasattr ( module , ""__name__"" ) :",137
"def _get_normal_median_depth(normal_counts):
    depths = []
    with open(normal_counts) as in_handle:
        header = None
        for line in in_handle:
            if header is None and not line.startswith(""@""):
                header = line.strip().split()
            elif header:
                n_vals = dict(zip(header, line.strip().split()))
                depths.append(int(n_vals[""REF_COUNT""]) + int(n_vals[""ALT_COUNT""]))
    return np.median(depths)
",elif header :,145
"def get_pool(self, *args, **kw):
    key = self._serialize(*args, **kw)
    try:
        return self.pools[key]
    except KeyError:
        self._create_pool_mutex.acquire()
        try:
            if key not in self.pools:
                kw.pop(""sa_pool_key"", None)
                pool = self.poolclass(
                    lambda: self.module.connect(*args, **kw), **self.kw
                )
                self.pools[key] = pool
                return pool
            else:
                return self.pools[key]
        finally:
            self._create_pool_mutex.release()
",if key not in self . pools :,193
"def add(self, field, value, boost=None):
    match = {""value"": value}
    if boost:
        if isinstance(boost, (float, int)):
            match[""boost""] = boost
        else:
            match[""boost""] = float(boost)
        self._values[field] = match
        return
    self._values[field] = value
","if isinstance ( boost , ( float , int ) ) :",94
"def get_shape(shape):
    """"""Convert the shape to correct dtype and vars.""""""
    ret = []
    for dim in shape:
        if isinstance(dim, tvm.tir.IntImm):
            if libinfo()[""INDEX_DEFAULT_I64""] == ""ON"":
                ret.append(dim)
            else:
                val = int(dim)
                assert val <= np.iinfo(np.int32).max
                ret.append(tvm.tir.IntImm(""int32"", val))
        elif isinstance(dim, tvm.tir.Any):
            ret.append(te.var(""any_dim"", ""int32""))
        else:
            ret.append(dim)
    return ret
","if libinfo ( ) [ ""INDEX_DEFAULT_I64"" ] == ""ON"" :",194
"def _find_icacls_exe():
    if os.name == ""nt"":
        paths = [
            os.path.expandvars(r""%windir%\{0}"").format(subdir)
            for subdir in (""system32"", ""SysWOW64"")
        ]
        for path in paths:
            icacls_path = next(
                iter(fn for fn in os.listdir(path) if fn.lower() == ""icacls.exe""), None
            )
            if icacls_path is not None:
                icacls_path = os.path.join(path, icacls_path)
                return icacls_path
    return None
",if icacls_path is not None :,177
"def mlt_version_is_greater_correct(test_version):
    runtime_ver = mlt_version.split(""."")
    test_ver = test_version.split(""."")
    if runtime_ver[0] > test_ver[0]:
        return True
    elif runtime_ver[0] == test_ver[0]:
        if runtime_ver[1] > test_ver[1]:
            return True
        elif runtime_ver[1] == test_ver[1]:
            if runtime_ver[2] > test_ver[2]:
                return True
    return False
",if runtime_ver [ 2 ] > test_ver [ 2 ] :,148
"def get_ready_conn(self, host):
    conn = None
    self._lock.acquire()
    try:
        if host in self._hostmap:
            for c in self._hostmap[host]:
                if self._readymap[c]:
                    self._readymap[c] = 0
                    conn = c
                    break
    finally:
        self._lock.release()
    return conn
",if self . _readymap [ c ] :,115
"def to_svc_hst_distinct_lists(ref, tab):
    r = {""hosts"": [], ""services"": []}
    for e in tab:
        cls = e.__class__
        if cls.my_type == ""service"":
            name = e.get_dbg_name()
            r[""services""].append(name)
        else:
            name = e.get_dbg_name()
            r[""hosts""].append(name)
    return r
","if cls . my_type == ""service"" :",119
"def playerData(s):
    """"""Returns a list of tuples of original string and dict of values""""""
    p = []
    i = 0
    while True:
        match = re_input.match(s, pos=i)
        if match is None:
            return p
        else:
            d = match.groupdict()
            if d[""args""] is not None:
                d[""degree""], d[""kwargs""] = getArgs(d[""args""])
            else:
                d[""degree""], d[""kwargs""] = """", {}
            del d[""args""]
            p.append((match.group().strip(), d))
            i = match.end()
    return
","if d [ ""args"" ] is not None :",178
"def _params_for_TXT(self, record):
    for value in record.values:
        field_type = ""TXT""
        if self._is_valid_dkim(value):
            field_type = ""DKIM""
            value = value.replace(""\\;"", "";"")
        yield {
            ""target"": value,
            ""subDomain"": record.name,
            ""ttl"": record.ttl,
            ""fieldType"": field_type,
        }
",if self . _is_valid_dkim ( value ) :,126
"def create(self, values):
    conn = self.get_connection()
    object_classes = self.structural_classes + [self.object_class]
    attrs = [(""objectClass"", object_classes)]
    for k, v in values.iteritems():
        if k == ""id"" or k in self.attribute_ignore:
            continue
        if v is not None:
            attr_type = self.attribute_mapping.get(k, k)
            attrs.append((attr_type, [v]))
    if ""groupOfNames"" in object_classes and self.use_dumb_member:
        attrs.append((""member"", [self.DUMB_MEMBER_DN]))
    conn.add_s(self._id_to_dn(values[""id""]), attrs)
    return values
",if v is not None :,196
"def get_new_unlinked_nodes(
    before_inputted_nodes, before_input_sockets, input_sockets, nodes_dict
):
    affected_nodes = []
    for node_id, socket in zip(before_inputted_nodes, before_input_sockets):
        if not socket in input_sockets:
            # if the node has been deleted it is not affected
            if node_id in nodes_dict:
                if not node_id in affected_nodes:
                    affected_nodes.append(node_id)
    return affected_nodes
",if not node_id in affected_nodes :,141
"def show_panel(panel_id):
    # Iterate positions to find where panel is and bring it to front.
    for position in _positions_names:
        pos_panel_ids = _get_position_panels(position)
        if len(pos_panel_ids) == 0:
            continue
        if len(pos_panel_ids) == 1:
            continue
        panel_widget = _get_panels_widgets_dict(gui.editor_window)[panel_id]
        notebook = _position_notebooks[position]
        for i in range(0, notebook.get_n_pages()):
            notebook_page = notebook.get_nth_page(i)
            if notebook_page == panel_widget:
                notebook.set_current_page(i)
",if notebook_page == panel_widget :,197
"def merge(self, abort=False, message=None):
    """"""Merge remote branch or reverts the merge.""""""
    if abort:
        self.execute([""update"", ""--clean"", "".""])
    elif self.needs_merge():
        if self.needs_ff():
            self.execute([""update"", ""--clean"", ""remote(.)""])
        else:
            self.configure_merge()
            # Fallback to merge
            try:
                self.execute([""merge"", ""-r"", ""remote(.)""])
            except RepositoryException as error:
                if error.retcode == 255:
                    # Nothing to merge
                    return
                raise
            self.execute([""commit"", ""--message"", ""Merge""])
",if self . needs_ff ( ) :,191
"def runButtons(action):
    global sqlUpdate
    if action == ""Clear"":
        app.text(LABS[""run""], replace=True)
        app.message(LABS[""run""], """", bg=""grey"")
        log(""SQL cleared"")
    elif action == ""Run"":
        app.message(LABS[""run""], """")
        sql = app.text(LABS[""run""]).strip()
        if len(sql) > 0:
            runSql(sql)
        else:
            app.message(LABS[""run""], """", bg=""grey"")
    app.text(LABS[""run""], focus=True)
",if len ( sql ) > 0 :,156
"def receive_loop(self):
    while not self._stoped:
        try:
            rd, _, _ = select.select([self.teredo_sock], [], [], 0.5)
            if rd and not self._stoped:
                self.receive_ra_packet()
        except Exception as e:
            logger.exception(""receive procedure fail once: %r"", e)
            pass
",if rd and not self . _stoped :,105
"def add_items(self, model, objs):
    search_fields = model.get_search_fields()
    if not search_fields:
        return
    indexers = [ObjectIndexer(obj, self.backend) for obj in objs]
    # TODO: Delete unindexed objects while dealing with proxy models.
    if indexers:
        content_type_pk = get_content_type_pk(model)
        update_method = (
            self.add_items_upsert
            if self._enable_upsert
            else self.add_items_update_then_create
        )
        update_method(content_type_pk, indexers)
",if self . _enable_upsert,160
"def __init__(self, service: RestClient, **k_args: Dict[str, str]):
    self.path: str = None
    self.httpMethod: str = None
    self.service: RestClient = service
    self.__dict__.update(k_args)
    self.path_args: List[str] = []
    self.query_args: List[str] = []
    if hasattr(self, ""parameters""):
        for key, value in self.parameters.items():
            if value[""location""] == ""path"":
                self.path_args.append(key)
            else:
                self.query_args.append(key)
","if value [ ""location"" ] == ""path"" :",165
"def insertion_unsort(str, extended):
    """"""3.2 Insertion unsort coding""""""
    oldchar = 0x80
    result = []
    oldindex = -1
    for c in extended:
        index = pos = -1
        char = ord(c)
        curlen = selective_len(str, char)
        delta = (curlen + 1) * (char - oldchar)
        while 1:
            index, pos = selective_find(str, c, index, pos)
            if index == -1:
                break
            delta += index - oldindex
            result.append(delta - 1)
            oldindex = index
            delta = 0
        oldchar = char
    return result
",if index == - 1 :,190
"def get_sorted_entry(field, bookid):
    if field == ""title"" or field == ""authors"":
        book = calibre_db.get_filtered_book(bookid)
        if book:
            if field == ""title"":
                return json.dumps({""sort"": book.sort})
            elif field == ""authors"":
                return json.dumps({""author_sort"": book.author_sort})
    return """"
","if field == ""title"" :",111
"def _convert_tstamp(out):
    # Searches for top-level timestamp attributes or within dictionaries
    if ""timestamp"" in out:
        # Convert UNIX to datetime object
        f = float(out[""timestamp""])
        out[""timestamp""] = datetime.fromtimestamp(f / 1000)
    else:
        for ticker, data in out.items():
            if ""timestamp"" in data:
                f = float(data[""timestamp""])
                data[""timestamp""] = datetime.fromtimestamp(f / 1000)
                out[ticker] = data
    return out
","if ""timestamp"" in data :",142
"def write_urls(self, person):
    """"""Write URL and EMAIL properties of a VCard.""""""
    url_list = person.get_url_list()
    for url in url_list:
        href = url.get_path()
        if href:
            if url.get_type() == UrlType(UrlType.EMAIL):
                if href.startswith(""mailto:""):
                    href = href[len(""mailto:"") :]
                self.writeln(""EMAIL:%s"" % self.esc(href))
            else:
                self.writeln(""URL:%s"" % self.esc(href))
",if href :,158
"def get_range(min, max):
    if max < min:
        min, max = max, min
    elif min == max:
        if min < 0:
            min, max = 2 * min, 0
        elif min > 0:
            min, max = 0, 2 * min
        else:
            min, max = -1, 1
    return min, max
",if min < 0 :,99
"def __init__(self, mapping=None):
    if isinstance(mapping, MultiDict):
        dict.__init__(self, ((k, l[:]) for k, l in mapping.iterlists()))
    elif isinstance(mapping, dict):
        tmp = {}
        for key, value in mapping.iteritems():
            if isinstance(value, (tuple, list)):
                value = list(value)
            else:
                value = [value]
            tmp[key] = value
        dict.__init__(self, tmp)
    else:
        tmp = {}
        for key, value in mapping or ():
            tmp.setdefault(key, []).append(value)
        dict.__init__(self, tmp)
","if isinstance ( value , ( tuple , list ) ) :",182
"def modified_precision(candidate, references, n):
    candidate_ngrams = list(ngrams(candidate, n))
    if len(candidate_ngrams) == 0:
        return 0
    c_words = set(candidate_ngrams)
    for word in c_words:
        count_w = candidate_ngrams.count(word) + 1
        count_max = 0
        for reference in references:
            reference_ngrams = list(ngrams(reference, n))
            count = reference_ngrams.count(word) + 1
            if count > count_max:
                count_max = count
    return min(count_w, count_max) / (len(candidate) + len(c_words))
",if count > count_max :,176
"def reverse_adjust_line_according_to_hunks(self, hunks, line):
    for hunk in reversed(hunks):
        head_start = hunk.head_start
        saved_start = hunk.saved_start
        if hunk.saved_length == 0:
            saved_start += 1
        elif hunk.head_length == 0:
            saved_start -= 1
        head_end = head_start + hunk.head_length
        saved_end = saved_start + hunk.saved_length
        if saved_end <= line:
            return head_end + line - saved_end
        elif saved_start <= line:
            return head_start
    # fails to find matching
    return line
",elif saved_start <= line :,193
"def indent_xml(elem, level=0):
    """"""Do our pretty printing and make Matt very happy.""""""
    i = ""\n"" + level * ""  ""
    if elem:
        if not elem.text or not elem.text.strip():
            elem.text = i + ""  ""
        if not elem.tail or not elem.tail.strip():
            elem.tail = i
        for elem in elem:
            indent_xml(elem, level + 1)
        if not elem.tail or not elem.tail.strip():
            elem.tail = i
    else:
        if level and (not elem.tail or not elem.tail.strip()):
            elem.tail = i
",if not elem . tail or not elem . tail . strip ( ) :,177
"def test_infer_shape_matrix(self):
    # Testing the infer_shape with a matrix.
    x = theano.tensor.matrix()
    for op in self.ops:
        if not op.return_inverse:
            continue
        if op.return_index:
            f = op(x)[2]
        else:
            f = op(x)[1]
        self._compile_and_check(
            [x],
            [f],
            [np.asarray(np.array([[2, 1], [3, 2], [2, 3]]), dtype=config.floatX)],
            self.op_class,
        )
",if not op . return_inverse :,170
"def drop_lists(value):
    out = {}
    for key, val in value.items():
        val = val[0]
        if isinstance(key, bytes):
            key = str(key, ""utf-8"")
        if isinstance(val, bytes):
            val = str(val, ""utf-8"")
        out[key] = val
    return out
","if isinstance ( val , bytes ) :",96
"def malloc(self, size):
    # return a block of right size (possibly rounded up)
    assert 0 <= size < sys.maxsize
    if os.getpid() != self._lastpid:
        self.__init__()  # reinitialize after fork
    with self._lock:
        self._free_pending_blocks()
        size = self._roundup(max(size, 1), self._alignment)
        (arena, start, stop) = self._malloc(size)
        new_stop = start + size
        if new_stop < stop:
            self._free((arena, new_stop, stop))
        block = (arena, start, new_stop)
        self._allocated_blocks.add(block)
        return block
",if new_stop < stop :,188
"def ContinueStatement(self, label, **kwargs):
    if label is None:
        self.emit(""JUMP"", self.implicit_continues[-1])
    else:
        label = label.get(""name"")
        if label not in self.declared_continue_labels:
            raise MakeError(""SyntaxError"", ""Undefined label '%s'"" % label)
        else:
            self.emit(""JUMP"", self.declared_continue_labels[label])
",if label not in self . declared_continue_labels :,111
"def parse_counter_style_name(tokens, counter_style):
    tokens = remove_whitespace(tokens)
    if len(tokens) == 1:
        (token,) = tokens
        if token.type == ""ident"":
            if token.lower_value in (""decimal"", ""disc""):
                if token.lower_value not in counter_style:
                    return token.value
            elif token.lower_value != ""none"":
                return token.value
","if token . type == ""ident"" :",122
"def __call__(self, data):
    num_points = data.pos.shape[0]
    new_data = Data()
    for key in data.keys:
        if key == KDTREE_KEY:
            continue
        item = data[key]
        if torch.is_tensor(item) and num_points == item.shape[0]:
            item = item[self._indices].clone()
        elif torch.is_tensor(item):
            item = item.clone()
        setattr(new_data, key, item)
    return new_data
",if torch . is_tensor ( item ) and num_points == item . shape [ 0 ] :,144
"def HandleEvent(self, event):
    e_id = event.GetId()
    if e_id in self.handlers:
        handler = self.handlers[e_id]
        try:
            if handler:
                return handler(event)
        except RuntimeError:
            self.RemoveHandlerForID(e_id)
    else:
        event.Skip()
    return False
",if handler :,102
"def try_append_extension(self, path):
    append_setting = self.get_append_extension_setting()
    if self.settings.get(append_setting, False):
        if not self.is_copy_original_name(path):
            _, new_path_extension = os.path.splitext(path)
            if new_path_extension == """":
                argument_name = self.get_argument_name()
                if argument_name is None:
                    _, extension = os.path.splitext(self.view.file_name())
                else:
                    _, extension = os.path.splitext(argument_name)
                path += extension
    return path
","if new_path_extension == """" :",181
"def _get_namespace(self, gl_client, gl_namespace, lazy=False):
    try:
        if gl_namespace.attributes[""kind""] == ""group"":
            return gl_client.groups.get(gl_namespace.attributes[""id""], lazy=lazy)
        if gl_namespace.attributes[""kind""] == ""user"":
            return gl_client.users.get(gl_client.user.attributes[""id""], lazy=lazy)
        # Note: This doesn't seem to work for IDs retrieved via the namespaces API; the IDs are
        # different.
        return gl_client.users.get(gl_namespace.attributes[""id""], lazy=lazy)
    except gitlab.GitlabGetError:
        return None
","if gl_namespace . attributes [ ""kind"" ] == ""group"" :",179
"def removeReadOnly(self, files):
    # Removes all read-on ly flags in a for all files
    for filepath in files:
        if os.path.isfile(filepath):
            # Windows only needs S_IWRITE, but we bitwise-or with current perms to preserve other permission bits on Linux
            os.chmod(filepath, stat.S_IWRITE | os.stat(filepath).st_mode)
",if os . path . isfile ( filepath ) :,99
"def initiate_all_local_variables_instances(
    nodes, local_variables_instances, all_local_variables_instances
):
    for node in nodes:
        if node.variable_declaration:
            new_var = LocalIRVariable(node.variable_declaration)
            if new_var.name in all_local_variables_instances:
                new_var.index = all_local_variables_instances[new_var.name].index + 1
            local_variables_instances[node.variable_declaration.name] = new_var
            all_local_variables_instances[node.variable_declaration.name] = new_var
",if node . variable_declaration :,158
"def find_comment(line):
    """"""Finds the index of a comment # and returns None if not found""""""
    instring, instring_char = False, """"
    for i, char in enumerate(line):
        if char in ('""', ""'""):
            if instring:
                if char == instring_char:
                    instring = False
                    instring_char = """"
            else:
                instring = True
                instring_char = char
        elif char == ""#"":
            if not instring:
                return i
    return None
","if char in ( '""' , ""'"" ) :",155
"def set_study_system_attr(self, study_id: int, key: str, value: Any) -> None:
    with _create_scoped_session(self.scoped_session, True) as session:
        study = models.StudyModel.find_or_raise_by_id(study_id, session)
        attribute = models.StudySystemAttributeModel.find_by_study_and_key(
            study, key, session
        )
        if attribute is None:
            attribute = models.StudySystemAttributeModel(
                study_id=study_id, key=key, value_json=json.dumps(value)
            )
            session.add(attribute)
        else:
            attribute.value_json = json.dumps(value)
",if attribute is None :,196
"def clear_doc(self, docname: str) -> None:
    for sChild in self._children:
        sChild.clear_doc(docname)
        if sChild.declaration and sChild.docname == docname:
            sChild.declaration = None
            sChild.docname = None
            sChild.line = None
            if sChild.siblingAbove is not None:
                sChild.siblingAbove.siblingBelow = sChild.siblingBelow
            if sChild.siblingBelow is not None:
                sChild.siblingBelow.siblingAbove = sChild.siblingAbove
            sChild.siblingAbove = None
            sChild.siblingBelow = None
",if sChild . siblingAbove is not None :,189
"def test_sum_values_list_group_by(self):
    ret = (
        await Book.annotate(sum=Sum(""rating""))
        .group_by(""author_id"")
        .values_list(""author_id"", ""sum"")
    )
    for item in ret:
        author_id = item[0]
        sum_ = item[1]
        if author_id == self.a1.pk:
            self.assertEqual(sum_, 45.0)
        elif author_id == self.a2.pk:
            self.assertEqual(sum_, 10.0)
",if author_id == self . a1 . pk :,151
"def save_claims_for_resolve(self, claim_infos):
    to_save = {}
    for info in claim_infos:
        if ""value"" in info:
            if info[""value""]:
                to_save[info[""claim_id""]] = info
        else:
            for key in (""certificate"", ""claim""):
                if info.get(key, {}).get(""value""):
                    to_save[info[key][""claim_id""]] = info[key]
    return self.save_claims(to_save.values())
","if info [ ""value"" ] :",141
"def utcoffset(self, dt):
    if not dst_only:
        dt_n = dt.replace(tzinfo=None)
        if dt_start <= dt_n < dt_end and getattr(dt_n, ""fold"", 0):
            return timedelta(hours=-1)
    return timedelta(hours=0)
","if dt_start <= dt_n < dt_end and getattr ( dt_n , ""fold"" , 0 ) :",80
"def find_comment(line):
    """"""Finds the index of a comment # and returns None if not found""""""
    instring, instring_char = False, """"
    for i, char in enumerate(line):
        if char in ('""', ""'""):
            if instring:
                if char == instring_char:
                    instring = False
                    instring_char = """"
            else:
                instring = True
                instring_char = char
        elif char == ""#"":
            if not instring:
                return i
    return None
",if instring :,155
"def __subclasshook__(cls, C):
    if cls is Coroutine:
        mro = get_mro(C)
        for method in (""__await__"", ""send"", ""throw"", ""close""):
            for base in mro:
                if method in base.__dict__:
                    break
            else:
                return NotImplemented
        return True
    return NotImplemented
",if method in base . __dict__ :,97
"def GetFile(cls, session, sig, mode=""r""):
    sig = sig[: cls.HASH_LEN]
    while len(sig) > 0:
        fn = cls.SaveFile(session, sig)
        try:
            if os.path.exists(fn):
                return (open(fn, mode), sig)
        except (IOError, OSError):
            pass
        if len(sig) > 1:
            sig = sig[:-1]
        else:
            if ""r"" in mode:
                return (None, sig)
            else:
                return (open(fn, mode), sig)
    # Not reached
    return (None, None)
",if len ( sig ) > 1 :,180
"def _store_pickle_output(self, pickle_output):
    if pickle_output:
        if self.output_options.output is None:
            self.error(""Can't use without --output"", ""pickle-output"")
        elif not load_pytd.is_pickle(self.output_options.output):
            self.error(
                ""Must specify %s file for --output"" % load_pytd.PICKLE_EXT,
                ""pickle-output"",
            )
    self.output_options.pickle_output = pickle_output
",elif not load_pytd . is_pickle ( self . output_options . output ) :,141
"def the_func(*args, **kwargs):
    try:
        # Grab API version from type of controller
        controller = args[0]
        version = controller.version
        return func(*args, **kwargs)
    except Exception as e:
        if errors is not None and type(e) in errors:
            # Version-specific behaviour
            quantum_error_class = quantum_error_dict[version]
            raise quantum_error_class(e)
        # otherwise just re-raise
        raise
",if errors is not None and type ( e ) in errors :,133
"def publish_create(cls, payload):
    try:
        if isinstance(payload, wf_ex_db.WorkflowExecutionDB):
            thread = eventlet.spawn(workflows.get_engine().process, payload)
            cls.threads.append(thread)
    except Exception:
        traceback.print_exc()
        print(payload)
","if isinstance ( payload , wf_ex_db . WorkflowExecutionDB ) :",87
"def get_suggestion(self, buffer: ""Buffer"", document: Document) -> Optional[Suggestion]:
    history = buffer.history
    # Consider only the last line for the suggestion.
    text = document.text.rsplit(""\n"", 1)[-1]
    # Only create a suggestion when this is not an empty line.
    if text.strip():
        # Find first matching line in history.
        for string in reversed(list(history.get_strings())):
            for line in reversed(string.splitlines()):
                if line.startswith(text):
                    return Suggestion(line[len(text) :])
    return None
",if line . startswith ( text ) :,151
"def _get_parameter_scope(param, cmd_list):
    if not cmd_list:
        return ""N/A (NOT FOUND)""
    test_list = cmd_list[0].split("" "")
    while len(test_list) > 0:
        test_entry = "" "".join(test_list)
        all_match = True
        for entry in cmd_list[1:]:
            if test_entry not in entry:
                all_match = False
                break
        if not all_match:
            test_list.pop()
        else:
            return test_entry
    return ""_ROOT_""
",if test_entry not in entry :,165
"def __call__(self, params):
    for param in params:
        # If we've seen this parameter before, use the previously
        # constructed optimizer.
        if param in self.optim_objs:
            optim = self.optim_objs[param]
        # If we've never seen this parameter before, construct
        # an Adam optimizer and keep track of it.
        else:
            optim = torch.optim.Adam([param], **self.optim_args)
            self.optim_objs[param] = optim
        # Take a gradient step for the parameter param.
        optim.step()
",if param in self . optim_objs :,151
"def filter_database(db, user, filter_name):
    """"""Returns a list of person handles""""""
    filt = MatchesFilter([filter_name])
    filt.requestprepare(db, user)
    if user:
        user.begin_progress(
            _(""Finding relationship paths""),
            _(""Retrieving all sub-filter matches""),
            db.get_number_of_people(),
        )
    matches = []
    for handle in db.iter_person_handles():
        person = db.get_person_from_handle(handle)
        if filt.apply(db, person):
            matches.append(handle)
        if user:
            user.step_progress()
    if user:
        user.end_progress()
    filt.requestreset()
    return matches
",if user :,198
"def get_independence_days(self, year):
    """"""returns a possibly empty list of (date, holiday_name) tuples""""""
    days = []
    if year > 2004:
        actual_date = date(year, 5, 4)
        days = [(actual_date, ""Restoration of Independence Day"")]
        if actual_date.weekday() in self.get_weekend_days():
            days += [
                (
                    self.find_following_working_day(actual_date),
                    ""Restoration of Independence Observed"",
                )
            ]
    return days
",if actual_date . weekday ( ) in self . get_weekend_days ( ) :,161
"def on_mode_paused(result, mode, *args):
    from deluge.ui.console.widgets.popup import PopupsHandler
    if isinstance(mode, PopupsHandler):
        if mode.popup is not None:
            # If popups are not removed, they are still referenced in the memory
            # which can cause issues as the popup's screen will not be destroyed.
            # This can lead to the popup border being visible for short periods
            # while the current modes' screen is repainted.
            log.error(
                'Mode ""%s"" still has popups available after being paused.'
                "" Ensure all popups are removed on pause!"",
                mode.popup.title,
            )
",if mode . popup is not None :,186
"def step(self):
    if not self.fully_grown:
        if self.countdown <= 0:
            # Set as fully grown
            self.fully_grown = True
            self.countdown = self.model.grass_regrowth_time
        else:
            self.countdown -= 1
",if self . countdown <= 0 :,86
"def getOnlineBuilders(self):
    all_workers = yield self.master.data.get((""workers"",))
    online_builderids = set()
    for worker in all_workers:
        connected = worker[""connected_to""]
        if not connected:
            continue
        builders = worker[""configured_on""]
        builderids = [builder[""builderid""] for builder in builders]
        online_builderids.update(builderids)
    defer.returnValue(list(online_builderids))
",if not connected :,124
"def _latest_major(alternatives):
    max_major = -1
    for a in alternatives:
        if is_version_identifier(a, strict=False):
            major, _, _, _ = components(a, strict=False)
            max_major = max(major, max_major)
    return max_major
","if is_version_identifier ( a , strict = False ) :",80
"def getVar(self, name):
    value = self.tinfoil.run_command(""dataStoreConnectorFindVar"", self.dsindex, name)
    overrides = None
    if isinstance(value, dict):
        if ""_connector_origtype"" in value:
            value[""_content""] = self.tinfoil._reconvert_type(
                value[""_content""], value[""_connector_origtype""]
            )
            del value[""_connector_origtype""]
        if ""_connector_overrides"" in value:
            overrides = value[""_connector_overrides""]
            del value[""_connector_overrides""]
    return value, overrides
","if ""_connector_origtype"" in value :",158
"def initAbbrev(self):
    k = self
    c = k.c
    d = c.config.getAbbrevDict()
    if d:
        for key in d:
            commandName = d.get(key)
            if commandName.startswith(""press-"") and commandName.endswith(""-button""):
                pass  # Must be done later in k.registerCommand.
            else:
                self.initOneAbbrev(commandName, key)
","if commandName . startswith ( ""press-"" ) and commandName . endswith ( ""-button"" ) :",123
"def restore_text(self):
    if self.source_is_console():
        cb = self._last_console_cb
    else:
        cb = self._last_editor_cb
    if cb is None:
        if self.is_plain_text_mode():
            self.plain_text.clear()
        else:
            self.rich_text.clear()
    else:
        func = cb[0]
        args = cb[1:]
        func(*args)
        if get_meth_class_inst(func) is self.rich_text:
            self.switch_to_rich_text()
        else:
            self.switch_to_plain_text()
",if get_meth_class_inst ( func ) is self . rich_text :,180
"def get_test_layer():
    layers = get_bb_var(""BBLAYERS"").split()
    testlayer = None
    for l in layers:
        if ""~"" in l:
            l = os.path.expanduser(l)
        if ""/meta-selftest"" in l and os.path.isdir(l):
            testlayer = l
            break
    return testlayer
","if ""~"" in l :",98
"def __parse_query(self, model, iter_, data):
    f, b = self.__filter, self.__bg_filter
    if f is None and b is None:
        return True
    else:
        album = model.get_album(iter_)
        if album is None:
            return True
        elif b is None:
            return f(album)
        elif f is None:
            return b(album)
        else:
            return b(album) and f(album)
",elif b is None :,130
"def iter(iterable, sentinel=None):
    if sentinel is None:
        i = getattr(iterable, ""__iter__"", None)
        if i is not None:
            return i()
        i = getattr(iterable, ""__getitem__"", None)
        if i is not None:
            return _iter_getitem(iterable)
        if JS(""@{{iterable}} instanceof Array""):
            return list(iterable).__iter__()
        raise TypeError(""object is not iterable"")
    if callable(iterable):
        return _iter_callable(iterable, sentinel)
    raise TypeError(""iter(v, w): v must be callable"")
","if JS ( ""@{{iterable}} instanceof Array"" ) :",153
"def run(self):
    # Prime the coroutine.
    next(self.coro)
    try:
        while True:
            with self.abort_lock:
                if self.abort_flag:
                    return
            # Get the message from the previous stage.
            msg = self.in_queue.get()
            if msg is POISON:
                break
            with self.abort_lock:
                if self.abort_flag:
                    return
            # Send to consumer.
            self.coro.send(msg)
    except:
        self.abort_all(sys.exc_info())
        return
",if self . abort_flag :,179
"def get_name_from_types(types: Iterable[Union[Type, StrawberryUnion]]):
    names = []
    for type_ in types:
        if isinstance(type_, StrawberryUnion):
            return type_.name
        elif hasattr(type_, ""_type_definition""):
            name = capitalize_first(type_._type_definition.name)
        else:
            name = capitalize_first(type_.__name__)
        names.append(name)
    return """".join(names)
","if isinstance ( type_ , StrawberryUnion ) :",131
"def _get_user_from_email(group, email):
    from sentry.models import User
    # TODO(dcramer): we should encode the userid in emails so we can avoid this
    for user in User.objects.filter(email__iexact=email):
        # Make sure that the user actually has access to this project
        context = access.from_user(user=user, organization=group.organization)
        if not context.has_team(group.project.team):
            logger.warning(""User %r does not have access to group %r"", user, group)
            continue
        return user
",if not context . has_team ( group . project . team ) :,149
"def _make_binary_stream(s, encoding):
    try:
        if _py3k:
            if isinstance(s, str):
                s = s.encode(encoding)
        else:
            if type(s) is not str:
                s = s.encode(encoding)
        from io import BytesIO
        rv = BytesIO(s)
    except ImportError:
        rv = StringIO(s)
    return rv
","if isinstance ( s , str ) :",115
"def error_messages(file_list, files_removed):
    if files_removed is None:
        return
    for remove_this, reason in files_removed:
        if file_list is not None:
            file_list.remove(remove_this)
        if reason == 0:
            print("" REMOVED : ("" + str(remove_this) + "")   is not PNG file format"")
        elif reason == 1:
            print("" REMOVED : ("" + str(remove_this) + "")   already exists"")
        elif reason == 2:
            print("" REMOVED : ("" + str(remove_this) + "")   file unreadable"")
",elif reason == 1 :,161
"def _eyeAvailable(*args, **kwargs):
    try:
        r = pylink.getEyeLink().eyeAvailable()
        if r == 0:
            return EyeTrackerConstants.getName(EyeTrackerConstants.LEFT_EYE)
        elif r == 1:
            return EyeTrackerConstants.getName(EyeTrackerConstants.RIGHT_EYE)
        elif r == 2:
            return EyeTrackerConstants.getName(EyeTrackerConstants.BINOCULAR)
        else:
            return EyeTrackerConstants.UNDEFINED
    except Exception as e:
        printExceptionDetailsToStdErr()
",if r == 0 :,157
"def ignore_callback_errors(self, ignore):
    EventEmitter.ignore_callback_errors.fset(self, ignore)
    for emitter in self._emitters.values():
        if isinstance(emitter, EventEmitter):
            emitter.ignore_callback_errors = ignore
        elif isinstance(emitter, EmitterGroup):
            emitter.ignore_callback_errors_all(ignore)
","if isinstance ( emitter , EventEmitter ) :",95
"def test_empty_condition_node(cond_node):
    for node in [cond_node.true_node, cond_node.false_node]:
        if node is None:
            continue
        if type(node) is CodeNode and BaseNode.test_empty_node(node.node):
            continue
        if BaseNode.test_empty_node(node):
            continue
        return False
    return True
",if node is None :,108
"def _confirm_deps(self, trans):
    if [pkgs for pkgs in trans.dependencies if pkgs]:
        dia = AptConfirmDialog(trans, parent=self.parent)
        res = dia.run()
        dia.hide()
        if res != Gtk.ResponseType.OK:
            log.debug(""Response is: %s"" % res)
            if self.finish_handler:
                log.debug(""Finish_handler..."")
                self.finish_handler(trans, 0, self.data)
            return
    self._run_transaction(trans)
",if res != Gtk . ResponseType . OK :,147
"def get_human_type(self, translate=True):
    """"""Returns prettified name of the object type""""""
    try:
        obj_name = re.match("".*\.(?P<name>\w+)$"", self.object_type).group(""name"")
        pattern = re.compile(""([A-Z][A-Z][a-z])|([a-z][A-Z])"")
        human_type = pattern.sub(
            lambda m: m.group()[:1] + "" "" + m.group()[1:], obj_name
        )
        if translate:
            human_type = _(human_type)
        return human_type
    except Exception:
        return self.object_type
",if translate :,174
"def ascii85decode(data):
    n = b = 0
    out = """"
    for c in data:
        if ""!"" <= c and c <= ""u"":
            n += 1
            b = b * 85 + (ord(c) - 33)
            if n == 5:
                out += struct.pack("">L"", b)
                n = b = 0
        elif c == ""z"":
            assert n == 0
            out += ""\0\0\0\0""
        elif c == ""~"":
            if n:
                for _ in range(5 - n):
                    b = b * 85 + 84
                out += struct.pack("">L"", b)[: n - 1]
            break
    return out
",if n == 5 :,200
"def calculateModifiedAttributes(self, fit, runTime, forceProjected=False):
    if self.item:
        for effect in self.item.effects.values():
            if effect.runTime == runTime and effect.activeByDefault:
                effect.handler(fit, self, (""module"",), None, effect=effect)
",if effect . runTime == runTime and effect . activeByDefault :,81
"def loadHandler(self, human, values, strict):
    if values[0] == ""pose"":
        poseFile = values[1]
        poseFile = getpath.thoroughFindFile(poseFile, self.paths)
        if not os.path.isfile(poseFile):
            if strict:
                raise RuntimeError(
                    ""Could not load pose %s, file does not exist."" % poseFile
                )
            log.warning(""Could not load pose %s, file does not exist."", poseFile)
        else:
            self.loadPose(poseFile)
        return
",if strict :,156
"def get_outdated_docs(self) -> Iterator[str]:
    for docname in self.env.found_docs:
        if docname not in self.env.all_docs:
            yield docname
            continue
        targetname = path.join(self.outdir, docname + self.out_suffix)
        try:
            targetmtime = path.getmtime(targetname)
        except Exception:
            targetmtime = 0
        try:
            srcmtime = path.getmtime(self.env.doc2path(docname))
            if srcmtime > targetmtime:
                yield docname
        except OSError:
            # source doesn't exist anymore
            pass
",if docname not in self . env . all_docs :,176
"def __init__(self, items=()):
    _dictEntries = []
    for name, value in items:
        if isinstance(name, (list, tuple, frozenset, set)):
            for item in name:
                _dictEntries.append((item, value))
        else:
            _dictEntries.append((name, value))
    dict.__init__(self, _dictEntries)
    assert len(self) == len(_dictEntries)
    self.default = None
","if isinstance ( name , ( list , tuple , frozenset , set ) ) :",117
"def ping_task():
    try:
        if self._protocol.peer_manager.peer_is_good(peer):
            if peer not in self._protocol.routing_table.get_peers():
                self._protocol.add_peer(peer)
            return
        await self._protocol.get_rpc_peer(peer).ping()
    except (asyncio.TimeoutError, RemoteException):
        pass
",if peer not in self . _protocol . routing_table . get_peers ( ) :,106
"def get_resolved_dependencies(self):
    dependencies = []
    for dependency in self.envconfig.deps:
        if dependency.indexserver is None:
            package = resolve_package(package_spec=dependency.name)
            if package != dependency.name:
                dependency = dependency.__class__(package)
        dependencies.append(dependency)
    return dependencies
",if package != dependency . name :,93
"def main(msg: func.QueueMessage, dashboard: func.Out[str]) -> None:
    body = msg.get_body()
    logging.info(""heartbeat: %s"", body)
    raw = json.loads(body)
    try:
        entry = TaskHeartbeatEntry.parse_obj(raw)
        task = Task.get_by_task_id(entry.task_id)
        if isinstance(task, Error):
            logging.error(task)
            return
        if task:
            task.heartbeat = datetime.utcnow()
            task.save()
    except ValidationError:
        logging.error(""invalid task heartbeat: %s"", raw)
    events = get_events()
    if events:
        dashboard.set(events)
","if isinstance ( task , Error ) :",189
"def testTlsServerServeForeverTwice(self):
    """"""Call on serve_forever() twice should result in a runtime error""""""
    with patch.object(ssl.SSLContext, ""load_cert_chain"") as mock_method:
        server = yield from StartTlsServer(
            context=self.context, address=(""127.0.0.1"", 0), loop=self.loop
        )
        if PYTHON_VERSION >= (3, 7):
            server_task = asyncio.create_task(server.serve_forever())
        else:
            server_task = asyncio.ensure_future(server.serve_forever())
        yield from server.serving
        with self.assertRaises(RuntimeError):
            yield from server.serve_forever()
        server.server_close()
","if PYTHON_VERSION >= ( 3 , 7 ) :",195
"def getInstances_WithSource(self, instancesAmount, sourceObject, scenes):
    if sourceObject is None:
        self.removeAllObjects()
        return []
    else:
        sourceHash = hash(sourceObject)
        if self.identifier in lastSourceHashes:
            if lastSourceHashes[self.identifier] != sourceHash:
                self.removeAllObjects()
        lastSourceHashes[self.identifier] = sourceHash
    return self.getInstances_Base(instancesAmount, sourceObject, scenes)
",if lastSourceHashes [ self . identifier ] != sourceHash :,132
"def get_row(self, binary=False, columns=None, raw=None, prep_stmt=None):
    """"""Get the next rows returned by the MySQL server""""""
    try:
        rows, eof = self.get_rows(
            count=1, binary=binary, columns=columns, raw=raw, prep_stmt=prep_stmt
        )
        if rows:
            return (rows[0], eof)
        return (None, eof)
    except IndexError:
        # No row available
        return (None, None)
",if rows :,135
"def try_adjust_widgets(self):
    if hasattr(self.parent, ""adjust_widgets""):
        self.parent.adjust_widgets()
    if hasattr(self.parent, ""parentApp""):
        if hasattr(self.parent.parentApp, ""_internal_adjust_widgets""):
            self.parent.parentApp._internal_adjust_widgets()
        if hasattr(self.parent.parentApp, ""adjust_widgets""):
            self.parent.parentApp.adjust_widgets()
","if hasattr ( self . parent . parentApp , ""adjust_widgets"" ) :",118
"def parseStatementList():
    list__py__ = []
    statement = None
    while index < length:
        if match(""}""):
            break
        statement = parseSourceElement()
        if (
            ""undefined"" if not ""statement"" in locals() else typeof(statement)
        ) == ""undefined"":
            break
        list__py__.append(statement)
    return list__py__
","if match ( ""}"" ) :",103
"def forward(self, Z):
    losses = []
    context = self.context_cnn(Z)
    targets = self.target_cnn(Z)
    _, _, h, w = Z.shape
    # future prediction
    preds = self.pred_cnn(context)
    for steps_to_ignore in range(h - 1):
        for i in range(steps_to_ignore + 1, h):
            loss = self.compute_loss_h(targets, preds, i)
            if not torch.isnan(loss):
                losses.append(loss)
    loss = torch.stack(losses).sum()
    return loss
",if not torch . isnan ( loss ) :,157
"def __run(self, command):
    sys.stdout, self.stdout = self.stdout, sys.stdout
    sys.stderr, self.stderr = self.stderr, sys.stderr
    try:
        try:
            r = eval(command, self.namespace, self.namespace)
            if r is not None:
                print_(repr(r))
        except SyntaxError:
            exec(command, self.namespace)
    except:
        if hasattr(sys, ""last_type"") and sys.last_type == SystemExit:
            self.destroy()
        else:
            traceback.print_exc()
    sys.stdout, self.stdout = self.stdout, sys.stdout
    sys.stderr, self.stderr = self.stderr, sys.stderr
",if r is not None :,192
"def prune(self):
    file = self.file
    if self.remain == 0:
        read_pos = file.tell()
        file.seek(0, 2)
        sz = file.tell()
        file.seek(read_pos)
        if sz == 0:
            # Nothing to prune.
            return
    nf = self.newfile()
    while True:
        data = file.read(COPY_BYTES)
        if not data:
            break
        nf.write(data)
    self.file = nf
",if not data :,141
"def reduce_inode(self, f, init):
    for x in range(0, len(self._array), 2):
        key_or_none = self._array[x]
        val_or_node = self._array[x + 1]
        if key_or_none is None and val_or_node is not None:
            init = val_or_node.reduce_inode(f, init)
        else:
            init = f.invoke([init, rt.map_entry(key_or_none, val_or_node)])
        if rt.reduced_QMARK_(init):
            return init
    return init
",if key_or_none is None and val_or_node is not None :,160
"def gen_topython_helper(cw):
    cw.enter_block(
        ""private static BaseException/*!*/ ToPythonHelper(System.Exception clrException)""
    )
    allExceps = get_all_exceps([], exceptionHierarchy)
    allExceps.sort(cmp=compare_exceptions)
    for x in allExceps:
        if not x.silverlightSupported:
            cw.writeline(""#if !SILVERLIGHT"")
        cw.writeline(
            ""if (clrException is %s) return %s;""
            % (x.ExceptionMappingName, x.MakeNewException())
        )
        if not x.silverlightSupported:
            cw.writeline(""#endif"")
    cw.writeline(""return new BaseException(Exception);"")
    cw.exit_block()
",if not x . silverlightSupported :,200
"def file_versions(self, path):
    """"""Returns all commits where given file was modified""""""
    versions = []
    commits_info = self.commit_info()
    seen_shas = set()
    for commit in commits_info:
        try:
            files = self.get_commit_files(commit[""sha""], paths=[path])
            file_path, file_data = files.items()[0]
        except IndexError:
            continue
        file_sha = file_data[""sha""]
        if file_sha in seen_shas:
            continue
        else:
            seen_shas.add(file_sha)
        # Add file info
        commit[""file""] = file_data
        versions.append(file_data)
    return versions
",if file_sha in seen_shas :,194
"def _append_fragment(self, ctx, frag_content):
    try:
        ctx[""dest_stream""].write(frag_content)
        ctx[""dest_stream""].flush()
    finally:
        if self.__do_ytdl_file(ctx):
            self._write_ytdl_file(ctx)
        if not self.params.get(""keep_fragments"", False):
            os.remove(encodeFilename(ctx[""fragment_filename_sanitized""]))
        del ctx[""fragment_filename_sanitized""]
",if self . __do_ytdl_file ( ctx ) :,128
"def gen_segs(glyph):
    bzs = glyph_to_bzs(glyph)
    for sp in bzs:
        bks = segment_sp(sp)
        for i in range(len(bks)):
            bk0, bk1 = bks[i], bks[(i + 1) % len(bks)]
            if bk1 != (bk0 + 1) % len(sp) or len(sp[bk0]) != 2:
                segstr = seg_to_string(sp, bk0, bk1)
                fn = seg_fn(segstr)
                file(fn, ""w"").write(segstr)
",if bk1 != ( bk0 + 1 ) % len ( sp ) or len ( sp [ bk0 ] ) != 2 :,166
"def matches(self, filepath):
    matched = False
    parent_path = os.path.dirname(filepath)
    parent_path_dirs = split_path(parent_path)
    for pattern in self.patterns:
        negative = pattern.exclusion
        match = pattern.match(filepath)
        if not match and parent_path != """":
            if len(pattern.dirs) <= len(parent_path_dirs):
                match = pattern.match(
                    os.path.sep.join(parent_path_dirs[: len(pattern.dirs)])
                )
        if match:
            matched = not negative
    return matched
",if len ( pattern . dirs ) <= len ( parent_path_dirs ) :,165
"def __repr__(self):
    text = ""{}("".format(self.__class__.__name__)
    n = len(self)
    for i in range(n):
        if self[i] != None:
            if i > 0:
                text = text + "", ""
            text = text + ""{}={}"".format(fields[i], str(self[i]))
    text = text + "")""
    return text
",if self [ i ] != None :,102
"def difference_matrix(samples, debug=True):
    """"""Calculate the difference matrix for the given set of samples.""""""
    diff_matrix = {}
    for x in samples:
        if debug:
            print(""Calculating difference matrix for %s"" % x)
        if x not in diff_matrix:
            diff_matrix[x] = {}
        for y in samples:
            if samples[x] != samples[y]:
                d = difference(samples[x], samples[y])
                # print(""Difference between %s and %s: %d"" % (x, y, d))
                diff_matrix[x][y] = d
            else:
                diff_matrix[x][y] = 0
    return diff_matrix
",if x not in diff_matrix :,196
"def load_config(self):
    try:
        with open(CONFIG_PATH) as f:
            y = yaml.safe_load(f)
        for key, value in y.items():
            if hasattr(self, key.upper()) and not os.getenv(key.upper()):
                setattr(self, key.upper(), value)
    except IOError:
        logger.warning(
            f""No config file found at {CONFIG_PATH}, using defaults.\n""
            f""Set the CONFIG_PATH environment variable to point to a config file to override.""
        )
","if hasattr ( self , key . upper ( ) ) and not os . getenv ( key . upper ( ) ) :",148
"def checkout_branch(self, branch):
    if branch in self.remote_branches:
        sickrage.app.log.debug(
            ""Branch checkout: "" + self._find_installed_version() + ""->"" + branch
        )
        if not self.install_requirements(self.current_branch):
            return False
        # remove untracked files and performs a hard reset on git branch to avoid update issues
        if sickrage.app.config.git_reset:
            self.reset()
        # fetch all branches
        self.fetch()
        __, __, exit_status = self._git_cmd(self._git_path, ""checkout -f "" + branch)
        if exit_status == 0:
            return True
    return False
",if sickrage . app . config . git_reset :,194
"def upload(
    youtube_resource, video_path, body, chunksize=1024 * 1024, progress_callback=None
):
    body_keys = "","".join(body.keys())
    media = MediaFileUpload(video_path, chunksize=chunksize, resumable=True)
    videos = youtube_resource.videos()
    request = videos.insert(part=body_keys, body=body, media_body=media)
    while 1:
        status, response = request.next_chunk()
        if response:
            if ""id"" in response:
                return response[""id""]
            else:
                raise KeyError(""Response has no 'id' field"")
        elif status and progress_callback:
            progress_callback(status.total_size, status.resumable_progress)
","if ""id"" in response :",197
"def execute(self):
    with self._guard_sigpipe():
        try:
            targets = (
                self.get_targets()
                if self.act_transitively
                else self.context.target_roots
            )
            for value in self.console_output(targets) or tuple():
                self._outstream.write(value.encode())
                self._outstream.write(self._console_separator.encode())
        finally:
            self._outstream.flush()
            if self.get_options().output_file:
                self._outstream.close()
",if self . get_options ( ) . output_file :,162
"def declare_var(
    self,
    type_name: Union[str, Tuple[str, str]],
    *,
    var_name: str = """",
    var_name_prefix: str = ""v"",
    shared: bool = False,
) -> str:
    if shared:
        if not var_name:
            var_name = var_name_prefix
        if var_name not in self.shared_vars:
            self.declarations.append((var_name, type_name))
            self.shared_vars.add(var_name)
    else:
        if not var_name:
            var_name = self.get_var_name(var_name_prefix)
        self.declarations.append((var_name, type_name))
    return var_name
",if var_name not in self . shared_vars :,197
"def parse_counter_style_name(tokens, counter_style):
    tokens = remove_whitespace(tokens)
    if len(tokens) == 1:
        (token,) = tokens
        if token.type == ""ident"":
            if token.lower_value in (""decimal"", ""disc""):
                if token.lower_value not in counter_style:
                    return token.value
            elif token.lower_value != ""none"":
                return token.value
",if token . lower_value not in counter_style :,122
"def __init__(self, appName=""""):
    dlgappcore.AppDialog.__init__(self, win32ui.IDD_GENERAL_STATUS)
    self.timerAppName = appName
    self.argOff = 0
    if len(self.timerAppName) == 0:
        if len(sys.argv) > 1 and sys.argv[1][0] != ""/"":
            self.timerAppName = sys.argv[1]
            self.argOff = 1
","if len ( sys . argv ) > 1 and sys . argv [ 1 ] [ 0 ] != ""/"" :",116
"def tearDownClass(cls):
    for conn in settings.HAYSTACK_CONNECTIONS.values():
        if conn[""ENGINE""] != ""haystack.backends.whoosh_backend.WhooshEngine"":
            continue
        if ""STORAGE"" in conn and conn[""STORAGE""] != ""file"":
            continue
        # Start clean
        if os.path.exists(conn[""PATH""]):
            shutil.rmtree(conn[""PATH""])
    super(WhooshTestCase, cls).tearDownClass()
","if conn [ ""ENGINE"" ] != ""haystack.backends.whoosh_backend.WhooshEngine"" :",118
"def forward(self, x):
    if self.ffn_type in (1, 2):
        x0 = self.wx0(x)
        if self.ffn_type == 1:
            x1 = x
        elif self.ffn_type == 2:
            x1 = self.wx1(x)
        out = self.output(x0 * x1)
    out = self.dropout(out)
    out = self.LayerNorm(out + x)
    return out
",elif self . ffn_type == 2 :,122
"def __call__(self, data, **params):
    p = param.ParamOverrides(self, params)
    if isinstance(data, (HoloMap, NdOverlay)):
        ranges = {d.name: data.range(d) for d in data.dimensions()}
        data = data.clone(
            {k: GridMatrix(self._process(p, v, ranges)) for k, v in data.items()}
        )
        data = Collator(data, merge_type=type(data))()
        if p.overlay_dims:
            data = data.map(lambda x: x.overlay(p.overlay_dims), (HoloMap,))
        return data
    elif isinstance(data, Element):
        data = self._process(p, data)
        return GridMatrix(data)
",if p . overlay_dims :,200
"def _update_model(self, events, msg, root, model, doc, comm=None):
    msg = dict(msg)
    if self._rename[""objects""] in msg:
        old = events[""objects""].old
        msg[self._rename[""objects""]] = self._get_objects(model, old, doc, root, comm)
    with hold(doc):
        super(Panel, self)._update_model(events, msg, root, model, doc, comm)
        from ..io import state
        ref = root.ref[""id""]
        if ref in state._views:
            state._views[ref][0]._preprocess(root)
",if ref in state . _views :,161
"def reset_two_factor_hotp():
    otp_secret = request.form.get(""otp_secret"", None)
    if otp_secret:
        if not validate_hotp_secret(g.user, otp_secret):
            return render_template(""account_edit_hotp_secret.html"")
        g.user.set_hotp_secret(otp_secret)
        db.session.commit()
        return redirect(url_for(""account.new_two_factor""))
    else:
        return render_template(""account_edit_hotp_secret.html"")
","if not validate_hotp_secret ( g . user , otp_secret ) :",146
"def ETA(self):
    if self.done:
        prefix = ""Done""
        t = self.elapsed
        # import pdb; pdb.set_trace()
    else:
        prefix = ""ETA ""
        if self.max is None:
            t = -1
        elif self.elapsed == 0 or (self.cur == self.min):
            t = 0
        else:
            # import pdb; pdb.set_trace()
            t = float(self.max - self.min)
            t /= self.cur - self.min
            t = (t - 1) * self.elapsed
    return ""%s: %s"" % (prefix, self.format_duration(t))
",if self . max is None :,184
"def add_property(self, key, value):  # type: (str, Any) -> None
    with self.secure() as config:
        keys = key.split(""."")
        for i, key in enumerate(keys):
            if key not in config and i < len(keys) - 1:
                config[key] = table()
            if i == len(keys) - 1:
                config[key] = value
                break
            config = config[key]
",if key not in config and i < len ( keys ) - 1 :,126
"def validate_against_domain(
    cls, ensemble: Optional[""PolicyEnsemble""], domain: Optional[Domain]
) -> None:
    if ensemble is None:
        return
    for p in ensemble.policies:
        if not isinstance(p, TwoStageFallbackPolicy):
            continue
        if domain is None or p.deny_suggestion_intent_name not in domain.intents:
            raise InvalidDomain(
                ""The intent '{0}' must be present in the ""
                ""domain file to use TwoStageFallbackPolicy. ""
                ""Either include the intent '{0}' in your domain ""
                ""or exclude the TwoStageFallbackPolicy from your ""
                ""policy configuration"".format(p.deny_suggestion_intent_name)
            )
","if not isinstance ( p , TwoStageFallbackPolicy ) :",195
"def sample(self, **config):
    """"""Sample a configuration from this search space.""""""
    ret = []
    kwspaces = self.kwspaces
    striped_keys = [k.split(SPLITTER)[0] for k in config.keys()]
    for idx, obj in enumerate(self.data):
        if isinstance(obj, NestedSpace):
            sub_config = _strip_config_space(config, prefix=str(idx))
            ret.append(obj.sample(**sub_config))
        elif isinstance(obj, SimpleSpace):
            ret.append(config[str(idx)])
        else:
            ret.append(obj)
    return ret
","if isinstance ( obj , NestedSpace ) :",165
"def init_weights(self):
    for module in self.decoder.modules():
        if isinstance(module, (nn.Linear, nn.Embedding)):
            module.weight.data.normal_(mean=0.0, std=0.02)
        elif isinstance(module, nn.LayerNorm):
            module.bias.data.zero_()
            module.weight.data.fill_(1.0)
        if isinstance(module, nn.Linear) and module.bias is not None:
            module.bias.data.zero_()
    for p in self.generator.parameters():
        if p.dim() > 1:
            xavier_uniform_(p)
        else:
            p.data.zero_()
","if isinstance ( module , ( nn . Linear , nn . Embedding ) ) :",179
"def backfill_first_message_id(
    apps: StateApps, schema_editor: DatabaseSchemaEditor
) -> None:
    Stream = apps.get_model(""zerver"", ""Stream"")
    Message = apps.get_model(""zerver"", ""Message"")
    for stream in Stream.objects.all():
        first_message = Message.objects.filter(
            recipient__type_id=stream.id, recipient__type=2
        ).first()
        if first_message is None:
            # No need to change anything if the outcome is the default of None
            continue
        stream.first_message_id = first_message.id
        stream.save()
",if first_message is None :,167
"def commandComplete(self, cmd):
    if self.property:
        if cmd.didFail():
            return
        result = self.observer.getStdout()
        if self.strip:
            result = result.strip()
        propname = self.property
        self.setProperty(propname, result, ""SetPropertyFromCommand Step"")
        self.property_changes[propname] = result
    else:
        new_props = self.extract_fn(
            cmd.rc, self.observer.getStdout(), self.observer.getStderr()
        )
        for k, v in iteritems(new_props):
            self.setProperty(k, v, ""SetPropertyFromCommand Step"")
        self.property_changes = new_props
",if self . strip :,192
"def part(p, imaginary):
    # Represent infinity as 1e1000 and NaN as 1e1000-1e1000.
    s = ""j"" if imaginary else """"
    try:
        if math.isinf(p):
            if p < 0:
                return ""-1e1000"" + s
            return ""1e1000"" + s
        if math.isnan(p):
            return ""(1e1000%s-1e1000%s)"" % (s, s)
    except OverflowError:
        # math.isinf will raise this when given an integer
        # that's too large to convert to a float.
        pass
    return repr(p) + s
",if math . isnan ( p ) :,168
"def _user_has_perm(user, perm, obj):
    anon = user.is_anonymous()
    for backend in auth.get_backends():
        if not anon or backend.supports_anonymous_user:
            if hasattr(backend, ""has_perm""):
                if obj is not None:
                    if backend.supports_object_permissions and backend.has_perm(
                        user, perm, obj
                    ):
                        return True
                else:
                    if backend.has_perm(user, perm):
                        return True
    return False
",if obj is not None :,163
"def check_backslashes(payload):
    # Check for single quotes
    if payload.count(""\\"") >= 15:
        if not settings.TAMPER_SCRIPTS[""backslashes""]:
            if menu.options.tamper:
                menu.options.tamper = menu.options.tamper + "",backslashes""
            else:
                menu.options.tamper = ""backslashes""
        from src.core.tamper import backslashes
        payload = backslashes.tamper(payload)
","if not settings . TAMPER_SCRIPTS [ ""backslashes"" ] :",130
"def _check_model(cls):
    errors = []
    if cls._meta.proxy:
        if cls._meta.local_fields or cls._meta.local_many_to_many:
            errors.append(
                checks.Error(
                    ""Proxy model '%s' contains model fields."" % cls.__name__,
                    id=""models.E017"",
                )
            )
    return errors
",if cls . _meta . local_fields or cls . _meta . local_many_to_many :,114
"def _format_column_list(self, data):
    # Now we have all lis of columns which we need
    # to include in our create definition, Let's format them
    if ""columns"" in data:
        for c in data[""columns""]:
            if ""attacl"" in c:
                c[""attacl""] = parse_priv_to_db(c[""attacl""], self.column_acl)
            # check type for '[]' in it
            if ""cltype"" in c:
                c[""cltype""], c[""hasSqrBracket""] = column_utils.type_formatter(
                    c[""cltype""]
                )
","if ""cltype"" in c :",170
"def _extract_constant_functions(slither: SlitherCore) -> Dict[str, List[str]]:
    ret: Dict[str, List[str]] = {}
    for contract in slither.contracts:
        cst_functions = [
            _get_name(f) for f in contract.functions_entry_points if _is_constant(f)
        ]
        cst_functions += [
            v.function_name
            for v in contract.state_variables
            if v.visibility in [""public""]
        ]
        if cst_functions:
            ret[contract.name] = cst_functions
    return ret
",if cst_functions :,166
"def safe_zip(*args):
    """"""Like zip, but ensures arguments are of same length""""""
    base = len(args[0])
    for i, arg in enumerate(args[1:]):
        if len(arg) != base:
            raise ValueError(
                ""Argument 0 has length %d but argument %d has ""
                ""length %d"" % (base, i + 1, len(arg))
            )
    return zip(*args)
",if len ( arg ) != base :,115
"def readMemory(self, va, size):
    ret = b""""
    while size:
        pageva = va & self.pagemask
        pageoff = va - pageva
        chunksize = min(self.pagesize - pageoff, size)
        page = self.pagecache.get(pageva)
        if page is None:
            page = self.mem.readMemory(pageva, self.pagesize)
            self.pagecache[pageva] = page
        ret += page[pageoff : pageoff + chunksize]
        va += chunksize
        size -= chunksize
    return ret
",if page is None :,148
"def horizontal_neighbors_iter(self, ordered=True):
    n_horizontal_edges_per_y = self.x_dimension - (
        self.x_dimension <= 2 or not self.periodic
    )
    for x in range(n_horizontal_edges_per_y):
        for y in range(self.y_dimension):
            i = self.to_site_index((x, y))
            j = self.to_site_index(((x + 1) % self.x_dimension, y))
            yield (i, j)
            if ordered:
                yield (j, i)
",if ordered :,156
"def apply_ordering(self, query):
    ordering = request.args.get(""ordering"") or """"
    if ordering:
        desc, column = ordering.startswith(""-""), ordering.lstrip(""-"")
        if column in self.model._meta.fields:
            field = self.model._meta.fields[column]
            query = query.order_by(field.asc() if not desc else field.desc())
    return query
",if column in self . model . _meta . fields :,103
"def check_hashes(self, string):
    for hash in self.hashes.copy():
        ctext, hash = self.check_hash(hash, string)
        if ctext is not None:
            yield ctext, hash
            self.found.add(hash)
            self.hashes.remove(hash)
",if ctext is not None :,79
"def undo_block_stop(self):
    if self.undoblock.bump_depth(-1) == 0:
        cmd = self.undoblock
        self.undoblock = 0
        if len(cmd) > 0:
            if len(cmd) == 1:
                # no need to wrap a single cmd
                cmd = cmd.getcmd(0)
            # this blk of cmds, or single cmd, has already
            # been done, so don't execute it again
            self.addcmd(cmd, 0)
",if len ( cmd ) == 1 :,139
"def create_model_handler(ns, model_type):
    @route(f""/<provider>/{ns}/<model_id>"")
    @use_provider
    def handle(req, provider, model_id):
        # special cases:
        # fuo://<provider>/users/me -> show current logged user
        if model_type == ModelType.user:
            if model_id == ""me"":
                user = getattr(provider, ""_user"", None)
                if user is None:
                    raise CmdException(f""log in provider:{provider.identifier} first"")
                return user
        model = get_model_or_raise(provider, model_type, model_id)
        return model
","if model_id == ""me"" :",184
"def _remove_optional_none_type_hints(self, type_hints, defaults):
    # If argument has None as a default, typing.get_type_hints adds
    # optional None to the information it returns. We don't want that.
    for arg in defaults:
        if defaults[arg] is None and arg in type_hints:
            type_ = type_hints[arg]
            if self._is_union(type_):
                types = type_.__args__
                if len(types) == 2 and types[1] is type(None):
                    type_hints[arg] = types[0]
",if self . _is_union ( type_ ) :,157
"def set_billing_hours_and_amount(self):
    if not self.project:
        for timesheet in self.timesheets:
            ts_doc = frappe.get_doc(""Timesheet"", timesheet.time_sheet)
            if not timesheet.billing_hours and ts_doc.total_billable_hours:
                timesheet.billing_hours = ts_doc.total_billable_hours
            if not timesheet.billing_amount and ts_doc.total_billable_amount:
                timesheet.billing_amount = ts_doc.total_billable_amount
",if not timesheet . billing_hours and ts_doc . total_billable_hours :,153
"def _real_len(self, s):
    s_len = 0
    in_esc = False
    prev = "" ""
    for c in replace_all({""\0+"": """", ""\0-"": """", ""\0^"": """", ""\1"": """", ""\t"": "" ""}, s):
        if in_esc:
            if c == ""m"":
                in_esc = False
        else:
            if c == ""["" and prev == ""\033"":
                in_esc = True
                s_len -= 1  # we counted prev when we shouldn't have
            else:
                s_len += self._display_len(c)
        prev = c
    return s_len
","if c == ""["" and prev == ""\033"" :",177
"def _find_node_with_predicate(self, node, predicate):
    if node != self._tree._root and predicate(node):
        return node
    item, cookie = self._tree.GetFirstChild(node)
    while item:
        if predicate(item):
            return item
        if self._tree.ItemHasChildren(item):
            result = self._find_node_with_predicate(item, predicate)
            if result:
                return result
        item, cookie = self._tree.GetNextChild(node, cookie)
    return None
",if result :,143
"def main():
    parser = optparse.OptionParser()
    options, argv = parser.parse_args()
    counts = defaultdict(int)
    for line in fileinput.input(argv):
        try:
            tweet = json.loads(line)
        except:
            continue
        if ""retweeted_status"" not in tweet:
            continue
        rt = tweet[""retweeted_status""]
        id = rt[""id_str""]
        count = rt[""retweet_count""]
        if count > counts[id]:
            counts[id] = count
    for id in sorted(counts, key=counts.get, reverse=True):
        print(""{},{}"".format(id, counts[id]))
",if count > counts [ id ] :,187
"def to_get_select_object_meta(meta_param):
    if meta_param is not None and SelectParameters.Json_Type in meta_param:
        if meta_param[SelectParameters.Json_Type] != SelectJsonTypes.LINES:
            raise SelectOperationClientError(
                ""Json_Type can only be 'LINES' for creating meta"", """"
            )
        else:
            return to_get_select_json_object_meta(meta_param)
    else:
        return to_get_select_csv_object_meta(meta_param)
",if meta_param [ SelectParameters . Json_Type ] != SelectJsonTypes . LINES :,143
"def check_if_match(self, value, index, flags=0):
    pattern = self.get_pattern(index)
    if value:
        if _is_iterable(value):
            if any([bool(re.search(pattern, x, flags)) for x in value]):
                return True
        else:
            if isinstance(value, (int, long)):
                value = str(value)
            return bool(re.search(pattern, value, flags))
    return False
","if any ( [ bool ( re . search ( pattern , x , flags ) ) for x in value ] ) :",128
"def assemble(
    self, multi_model_placement: Dict[Model, PhysicalDevice]
) -> Tuple[Node, PhysicalDevice]:
    for node in self.origin_nodes:
        if node.original_graph.model in multi_model_placement:
            new_node = Node(
                node.original_graph,
                node.id,
                f""M_{node.original_graph.model.model_id}_{node.name}"",
                node.operation,
            )
            return new_node, multi_model_placement[node.original_graph.model]
    raise ValueError(
        f""DedupInputNode {self.name} does not contain nodes from multi_model""
    )
",if node . original_graph . model in multi_model_placement :,188
"def doc_generator(self, imdb_dir, dataset, include_label=False):
    dirs = [
        (os.path.join(imdb_dir, dataset, ""pos""), True),
        (os.path.join(imdb_dir, dataset, ""neg""), False),
    ]
    for d, label in dirs:
        for filename in os.listdir(d):
            with tf.gfile.Open(os.path.join(d, filename)) as imdb_f:
                doc = imdb_f.read().strip()
                if include_label:
                    yield doc, label
                else:
                    yield doc
",if include_label :,170
"def test_empty_condition_node(cond_node):
    for node in [cond_node.true_node, cond_node.false_node]:
        if node is None:
            continue
        if type(node) is CodeNode and BaseNode.test_empty_node(node.node):
            continue
        if BaseNode.test_empty_node(node):
            continue
        return False
    return True
",if type ( node ) is CodeNode and BaseNode . test_empty_node ( node . node ) :,108
"def rewrite_imports(package_dir, vendored_libs, vendor_dir):
    for item in package_dir.iterdir():
        if item.is_dir():
            rewrite_imports(item, vendored_libs, vendor_dir)
        elif item.name.endswith("".py""):
            rewrite_file_imports(item, vendored_libs, vendor_dir)
","elif item . name . endswith ( "".py"" ) :",95
"def ageToDays(self, age_str):
    age = 0
    age_str = age_str.replace(""&nbsp;"", "" "")
    regex = ""(\d*.?\d+).(sec|hour|day|week|month|year)+""
    matches = re.findall(regex, age_str)
    for match in matches:
        nr, size = match
        mult = 1
        if size == ""week"":
            mult = 7
        elif size == ""month"":
            mult = 30.5
        elif size == ""year"":
            mult = 365
        age += tryInt(nr) * mult
    return tryInt(age)
","elif size == ""month"" :",163
"def _validate_zone(self):
    availability_zone = self.availability_zone
    if availability_zone:
        zone = self.ec2.get_zone(availability_zone)
        if not zone:
            raise exception.ClusterValidationError(
                ""availability_zone = %s does not exist"" % availability_zone
            )
        if zone.state != ""available"":
            log.warn(
                ""The availability_zone = %s "" % zone + ""is not available at this time""
            )
    return True
","if zone . state != ""available"" :",140
"def addnoise(line):
    noise = fillers
    ratio = len(line) // len(noise)
    res = """"
    while line and noise:
        if len(line) // len(noise) > ratio:
            c, line = line[0], line[1:]
        else:
            c, noise = noise[0], noise[1:]
        res += c
    return res + noise + line
",if len ( line ) // len ( noise ) > ratio :,104
"def cwr1(iterable, r):
    ""Pure python version shown in the docs""
    # number items returned:  (n+r-1)! / r! / (n-1)! when n>0
    pool = tuple(iterable)
    n = len(pool)
    if not n and r:
        return
    indices = [0] * r
    yield tuple(pool[i] for i in indices)
    while 1:
        for i in reversed(range(r)):
            if indices[i] != n - 1:
                break
        else:
            return
        indices[i:] = [indices[i] + 1] * (r - i)
        yield tuple(pool[i] for i in indices)
",if indices [ i ] != n - 1 :,184
"def subscribe(self, params) -> bool:
    emit_data = {""method"": ""eth_subscribe"", ""params"": params}
    nonce = await self._send(emit_data)
    raw_message = await self._client.recv()
    if raw_message is not None:
        resp = ujson.loads(raw_message)
        if resp.get(""id"", None) == nonce:
            self._node_address = resp.get(""result"")
            return True
    return False
","if resp . get ( ""id"" , None ) == nonce :",121
"def _(node):
    for __ in dir(node):
        if not __.startswith(""_""):
            candidate = getattr(node, __)
            if isinstance(candidate, str):
                if ""\\"" in candidate:
                    try:
                        re.compile(candidate)
                    except:
                        errMsg = ""smoke test failed at compiling '%s'"" % candidate
                        logger.error(errMsg)
                        raise
            else:
                _(candidate)
","if not __ . startswith ( ""_"" ) :",142
"def get_field_values(self, fields):
    field_values = []
    for field in fields:
        # Title is special case
        if field == ""title"":
            value = self.get_title_display()
        elif field == ""country"":
            try:
                value = self.country.printable_name
            except exceptions.ObjectDoesNotExist:
                value = """"
        elif field == ""salutation"":
            value = self.salutation
        else:
            value = getattr(self, field)
        field_values.append(value)
    return field_values
","elif field == ""country"" :",158
"def __str__(self):
    s = """"
    for k, v in self._members.items():
        if isinstance(v.get(""type""), list):
            s += k + "" : "" + "";"".join(getattr(self, item)) + ""\n""
        elif isinstance(v.get(""type""), str):
            s += k + "" : "" + getattr(self, k) + ""\n""
    return s
","elif isinstance ( v . get ( ""type"" ) , str ) :",104
"def _merge(self, a, b, path=None):
    """"""Merge two dictionaries, from http://stackoverflow.com/questions/7204805/dictionaries-of-dictionaries-merge""""""
    if path is None:
        path = []
    for key in b:
        if key in a:
            if isinstance(a[key], dict) and isinstance(b[key], dict):
                self._merge(a[key], b[key], path + [str(key)])
            elif a[key] == b[key]:
                pass  # same leaf value
            else:
                raise Exception(""Conflict at %s"" % ""."".join(path + [str(key)]))
        else:
            a[key] = b[key]
    return a
",elif a [ key ] == b [ key ] :,196
"def get_child_nodes(node):
    if isinstance(node, _ast.Module):
        return node.body
    result = []
    if node._fields is not None:
        for name in node._fields:
            child = getattr(node, name)
            if isinstance(child, list):
                for entry in child:
                    if isinstance(entry, _ast.AST):
                        result.append(entry)
            if isinstance(child, _ast.AST):
                result.append(child)
    return result
","if isinstance ( child , _ast . AST ) :",145
"def _handle_enter(self) -> None:
    if self.multiple_selection:
        val = self.values[self._selected_index][0]
        if val in self.current_values:
            self.current_values.remove(val)
        else:
            self.current_values.append(val)
    else:
        self.current_value = self.values[self._selected_index][0]
",if val in self . current_values :,108
"def close_all(map=None, ignore_all=False):
    if map is None:  # pragma: no cover
        map = socket_map
    for x in list(map.values()):  # list() FBO py3
        try:
            x.close()
        except OSError as x:
            if x.args[0] == EBADF:
                pass
            elif not ignore_all:
                raise
        except _reraised_exceptions:
            raise
        except:
            if not ignore_all:
                raise
    map.clear()
",if x . args [ 0 ] == EBADF :,157
"def _get_spawn_property(self, constraints, constraint_name, services):
    if services:
        # this isn't very nice
        if constraint_name == IMAGE_CONSTRAINT:
            return services[0].image
        elif constraint_name == CPUS_CONSTRAINT:
            return services[0].cpus
    for constraint in constraints:
        if constraint.name == constraint_name:
            return constraint.value
    return None
",if constraint . name == constraint_name :,113
"def _handle_children(self, removed, added):
    # Stop all the removed children.
    for obj in removed:
        obj.stop()
    # Process the new objects.
    for obj in added:
        obj.set(scene=self.scene, parent=self)
        if isinstance(obj, ModuleManager):
            obj.source = self
        elif is_filter(obj):
            obj.inputs.append(self)
        if self.running:
            try:
                obj.start()
            except:
                exception()
",elif is_filter ( obj ) :,148
"def _get_cols_width(self, values):
    width = 14
    for row in values:
        for header in self.headers:
            header_len = len(header)
            if header_len > width:
                width = header_len
            value_len = len(unicode(row.get(header, """")))
            if value_len > width:
                width = value_len
    width += 2
    return width
",if value_len > width :,118
"def crawl(self, *args, **kwargs):
    assert not self.crawling, ""Crawling already taking place""
    self.crawling = True
    try:
        self.spider = self._create_spider(*args, **kwargs)
        self.engine = self._create_engine()
        if self.start_requests:
            start_requests = iter(self.spider.start_requests())
        else:
            start_requests = ()
        yield self.engine.open_spider(self.spider, start_requests)
        yield defer.maybeDeferred(self.engine.start)
    except Exception:
        self.crawling = False
        raise
",if self . start_requests :,170
"def _copy_files(self, files, src, dest, message=""""):
    for filepath in files:
        srcpath = os.path.join(src, filepath)
        destpath = os.path.join(dest, filepath)
        if message:
            print(""{}: {}"".format(message, destpath))
        if os.path.exists(srcpath):
            destdir = os.path.dirname(destpath)
            if not os.path.isdir(destdir):
                os.makedirs(destdir)
            shutil.copy(srcpath, destpath)
        elif os.path.exists(destpath):
            os.remove(destpath)
",if message :,167
"def describe_tags(self):
    resource_arns = self._get_multi_param(""ResourceArns.member"")
    resources = []
    for arn in resource_arns:
        if "":targetgroup"" in arn:
            resource = self.elbv2_backend.target_groups.get(arn)
            if not resource:
                raise TargetGroupNotFoundError()
        elif "":loadbalancer"" in arn:
            resource = self.elbv2_backend.load_balancers.get(arn)
            if not resource:
                raise LoadBalancerNotFoundError()
        else:
            raise LoadBalancerNotFoundError()
        resources.append(resource)
    template = self.response_template(DESCRIBE_TAGS_TEMPLATE)
    return template.render(resources=resources)
","if "":targetgroup"" in arn :",197
"def iterator():
    try:
        while True:
            yield from pullparser.read_events()
            # load event buffer
            data = source.read(16 * 1024)
            if not data:
                break
            pullparser.feed(data)
        root = pullparser._close_and_return_root()
        yield from pullparser.read_events()
        it.root = root
    finally:
        if close_source:
            source.close()
",if not data :,130
"def __repr__(self):
    data = """"
    for c in self.children:
        data += c.shortrepr()
        if len(data) > 60:
            data = data[:56] + "" ...""
            break
    if self[""names""]:
        return '<%s ""%s"": %s>' % (
            self.__class__.__name__,
            ""; "".join([ensure_str(n) for n in self[""names""]]),
            data,
        )
    else:
        return ""<%s: %s>"" % (self.__class__.__name__, data)
",if len ( data ) > 60 :,147
"def __exit__(self, exc_type, exc_value, traceback):
    template_rendered.disconnect(self.on_template_render)
    if exc_type is not None:
        return
    if not self.test():
        message = self.message()
        if len(self.rendered_templates) == 0:
            message += "" No template was rendered.""
        else:
            message += "" Following templates were rendered: %s"" % (
                "", "".join(self.rendered_template_names)
            )
        self.test_case.fail(message)
",if len ( self . rendered_templates ) == 0 :,148
"def _match(self, byte_chunk):
    quote_character = None
    data = byte_chunk.nhtml
    open_angle_bracket = data.rfind(""<"")
    # We are inside <...
    if open_angle_bracket <= data.rfind("">""):
        return False
    for s in data[open_angle_bracket + 1 :]:
        if s in ATTR_DELIMITERS:
            if quote_character and s == quote_character:
                quote_character = None
                continue
            elif not quote_character:
                quote_character = s
                continue
    if quote_character == self.quote_character:
        return True
    return False
",elif not quote_character :,173
"def recent_events(self, events):
    try:
        frame = self.get_frame()
    except EndofVideoFileError:
        logger.info(""Video has ended."")
        self.notify_all(
            {""subject"": ""file_source.video_finished"", ""source_path"": self.source_path}
        )
        self.play = False
    else:
        self._recent_frame = frame
        events[""frame""] = frame
        if self.timed_playback:
            self.wait(frame)
",if self . timed_playback :,136
"def _prune(self):
    if self.over_threshold():
        now = time.time()
        for idx, (key, (expires, _)) in enumerate(self._cache.items()):
            if expires is not None and expires <= now or idx % 3 == 0:
                with self._mutex:
                    self._cache.pop(key, None)
",if expires is not None and expires <= now or idx % 3 == 0 :,95
"def dict_path(d, path):
    if not isinstance(path, (list, tuple)):
        raise ValueError()
    for keys in path:
        if type(keys) is not list:
            keys = [keys]
        value = None
        for key in keys:
            if key not in d:
                continue
            value = d[key]
        if value is None:
            value = {}
        for key in keys:
            d[key] = value
        d = value
    return d
",if key not in d :,140
"def span_tokenize(self, string):
    if self.__tokenizer == ""nltk"":
        raw_tokens = nltk.word_tokenize(string)
        if ('""' in string) or (""''"" in string):
            matched = [m.group() for m in re.finditer(r""``|'{2}|\"""", string)]
            tokens = [
                matched.pop(0) if tok in ['""', ""``"", ""''""] else tok
                for tok in raw_tokens
            ]
        else:
            tokens = raw_tokens
        spans = align_tokens(tokens, string)
    return spans
","if ( '""' in string ) or ( ""''"" in string ) :",155
"def literal(self):
    if self.peek('""'):
        lit, lang, dtype = self.eat(r_literal).groups()
        if lang:
            lang = lang
        else:
            lang = None
        if dtype:
            dtype = dtype
        else:
            dtype = None
        if lang and dtype:
            raise ParseError(""Can't have both a language and a datatype"")
        lit = unquote(lit)
        return Literal(lit, lang, dtype)
    return False
",if lang and dtype :,132
"def get():
    result = []
    for b in self.key_bindings:
        if len(keys) < len(b.keys):
            match = True
            for i, j in zip(b.keys, keys):
                if i != j and i != Keys.Any:
                    match = False
                    break
            if match:
                result.append(b)
    return result
",if i != j and i != Keys . Any :,113
"def _compileRules(rulesList, maxLength=4):
    ruleChecking = collections.defaultdict(list)
    for ruleIndex in range(len(rulesList)):
        args = []
        if len(rulesList[ruleIndex]) == maxLength:
            args = rulesList[ruleIndex][-1]
        if maxLength == 4:
            (shouldRunMethod, method, isCorrect) = rulesList[ruleIndex][0:3]
            ruleChecking[shouldRunMethod].append((method, isCorrect, args))
        elif maxLength == 3:
            (shouldRunMethod, method) = rulesList[ruleIndex][0:2]
            ruleChecking[shouldRunMethod].append((method, args))
    return ruleChecking
",if maxLength == 4 :,183
"def parents_in_pipfile(self):
    if not self._parents_in_pipfile:
        self._parents_in_pipfile = [
            p
            for p in self.flattened_parents
            if p.normalized_name in self.pipfile_packages
        ]
    return self._parents_in_pipfile
",if p . normalized_name in self . pipfile_packages,86
"def TryMerge(self, d):
    while d.avail() > 0:
        tt = d.getVarInt32()
        if tt == 10:
            self.set_content(d.getPrefixedString())
            continue
        if tt == 18:
            self.set_blob_key(d.getPrefixedString())
            continue
        if tt == 24:
            self.set_width(d.getVarInt32())
            continue
        if tt == 32:
            self.set_height(d.getVarInt32())
            continue
        if tt == 0:
            raise ProtocolBuffer.ProtocolBufferDecodeError
        d.skipData(tt)
",if tt == 18 :,182
"def base64_encode_image_mapper(self, tag, url):
    if tag == ""img"":
        if url in self.kp_images:
            image_data = base64.b64encode(self.kp_images[url])
            image_mimetype = mimetypes.guess_type(url)[0]
            if image_mimetype is not None:
                return ""data:{};base64, "".format(image_mimetype) + image_data.decode(
                    ""utf-8""
                )
    return None
",if url in self . kp_images :,138
"def get_args_from_ref_args(handler, ref_args):
    args = []
    for ref_arg in ref_args:
        if type(ref_arg) is ref.array_type:
            temp = handler.create_from_numpy(ref_arg)
            args.append(temp)
        else:
            args.append(ref_arg)
    return args
",if type ( ref_arg ) is ref . array_type :,100
"def _get_cols_width(self, values):
    width = 14
    for row in values:
        for header in self.headers:
            header_len = len(header)
            if header_len > width:
                width = header_len
            value_len = len(unicode(row.get(header, """")))
            if value_len > width:
                width = value_len
    width += 2
    return width
",if header_len > width :,118
"def OnLeaveWindow(self, event):
    if self.start_drag and not self.dragging:
        self.dragging = False
        self.start_drag = False
        self.dragged_tab = None
        self.drag_trigger = self.drag_trail
        if self.HasCapture():
            self.ReleaseMouse()
    if self.preview_wnd:
        self.preview_wnd.Show(False)
        del self.preview_wnd
        self.preview_wnd = None
    event.Skip()
",if self . HasCapture ( ) :,138
"def _checkPid(self, pid):
    retval = False
    if self.settings.windows:
        PROCESS_TERMINATE = 1
        p = ctypes.windll.kernel32.OpenProcess(PROCESS_TERMINATE, 0, pid)
        retval = p != 0
        if p:
            ctypes.windll.kernel32.CloseHandle(p)
    else:
        # https://stackoverflow.com/questions/568271/how-to-check-if-there-exists-a-process-with-a-given-pid-in-python
        try:
            os.kill(pid, 0)
        except OSError:
            pass
        else:
            retval = True
    return retval
",if p :,177
"def concat_index_value(index_values, store_data=False):
    result = pd.Index([])
    if not isinstance(index_values, (list, tuple)):
        index_values = [index_values]
    for index_value in index_values:
        if isinstance(index_value, pd.Index):
            result = result.append(index_value)
        else:
            result = result.append(index_value.to_pandas())
    return parse_index(result, store_data=store_data)
","if isinstance ( index_value , pd . Index ) :",132
"def apply(self, db, family):
    if self.rtype:
        if self.rtype.is_custom() and self.use_regex:
            if self.regex[0].search(str(family.get_relationship())) is None:
                return False
        elif self.rtype != family.get_relationship():
            return False
    return True
",if self . regex [ 0 ] . search ( str ( family . get_relationship ( ) ) ) is None :,93
"def get_child_nodes(node):
    if isinstance(node, _ast.Module):
        return node.body
    result = []
    if node._fields is not None:
        for name in node._fields:
            child = getattr(node, name)
            if isinstance(child, list):
                for entry in child:
                    if isinstance(entry, _ast.AST):
                        result.append(entry)
            if isinstance(child, _ast.AST):
                result.append(child)
    return result
","if isinstance ( entry , _ast . AST ) :",145
"def output(self):
    """"""Transform self into a list of (name, value) tuples.""""""
    header_list = []
    for k, v in self.items():
        if isinstance(k, unicodestr):
            k = self.encode(k)
        if not isinstance(v, basestring):
            v = str(v)
        if isinstance(v, unicodestr):
            v = self.encode(v)
        # See header_translate_* constants above.
        # Replace only if you really know what you're doing.
        k = k.translate(header_translate_table, header_translate_deletechars)
        v = v.translate(header_translate_table, header_translate_deletechars)
        header_list.append((k, v))
    return header_list
","if isinstance ( v , unicodestr ) :",197
"def check_valid_emoji_name(emoji_name: str) -> None:
    if emoji_name:
        if re.match(r""^[0-9a-z.\-_]+(?<![.\-_])$"", emoji_name):
            return
        raise JsonableError(_(""Invalid characters in emoji name""))
    raise JsonableError(_(""Emoji name is missing""))
","if re . match ( r""^[0-9a-z.\-_]+(?<![.\-_])$"" , emoji_name ) :",93
"def cache_subscriptions(self, region: str):
    async with self.regional_subscriptions_cache_locks.setdefault(
        region, asyncio.Lock()
    ):
        if region in self.subscriptions_cache:
            return
        self.subscriptions_cache[region] = await AWSFacadeUtils.get_all_pages(
            ""sns"", region, self.session, ""list_subscriptions"", ""Subscriptions""
        )
        for subscription in self.subscriptions_cache[region]:
            topic_arn = subscription.pop(""TopicArn"")
            subscription[""topic_name""] = topic_arn.split("":"")[-1]
",if region in self . subscriptions_cache :,160
"def AdjustArg(arg, break_chars, argv_out):
    # type: (str, List[str], List[str]) -> None
    end_indices = []  # stores the end of each span
    state = ST_Begin
    for i, c in enumerate(arg):
        ch = CH_Break if c in break_chars else CH_Other
        state, emit_span = _TRANSITIONS[state, ch]
        if emit_span:
            end_indices.append(i)
    # Always emit a span at the end (even for empty string)
    end_indices.append(len(arg))
    begin = 0
    for end in end_indices:
        argv_out.append(arg[begin:end])
        begin = end
",if emit_span :,185
"def load_model(
    self, model_name: str, path: str = None, model_type=None
) -> AbstractModel:
    if isinstance(model_name, AbstractModel):
        return model_name
    if model_name in self.models.keys():
        return self.models[model_name]
    else:
        if path is None:
            path = self.get_model_attribute(model=model_name, attribute=""path"")
        if model_type is None:
            model_type = self.get_model_attribute(model=model_name, attribute=""type"")
        return model_type.load(path=path, reset_paths=self.reset_paths)
",if model_type is None :,170
"def find_config(pipeline_config_path: Union[str, Path]) -> Path:
    if not Path(pipeline_config_path).is_file():
        configs = [
            c
            for c in Path(__file__).parent.parent.parent.glob(
                f""configs/**/{pipeline_config_path}.json""
            )
            if str(c.with_suffix("""")).endswith(pipeline_config_path)
        ]  # a simple way to not allow * and ?
        if configs:
            log.info(f""Interpreting '{pipeline_config_path}' as '{configs[0]}'"")
            pipeline_config_path = configs[0]
    return Path(pipeline_config_path)
","if str ( c . with_suffix ( """" ) ) . endswith ( pipeline_config_path )",184
"def __init__(self, bounds, channel_axis, preprocess=None):
    assert len(bounds) == 2
    assert channel_axis in [0, 1, 2, 3]
    self._bounds = bounds
    self._channel_axis = channel_axis
    # Make self._preprocess to be (0,1) if possible, so that don't need
    # to do substract or divide.
    if preprocess is not None:
        sub, div = np.array(preprocess)
        if not np.any(sub):
            sub = 0
        if np.all(div == 1):
            div = 1
        assert (div is None) or np.all(div)
        self._preprocess = (sub, div)
    else:
        self._preprocess = (0, 1)
",if np . all ( div == 1 ) :,194
"def iter_imports(path):
    """"""Yield imports in *path*""""""
    for node in ast.parse(open(path, ""rb"").read()).body:
        if isinstance(node, ast.ImportFrom):
            if node.module is None:
                prefix = ()
            else:
                prefix = tuple(node.module.split("".""))
            for snode in node.names:
                yield (node.level, prefix + (snode.name,))
        elif isinstance(node, ast.Import):
            for node in node.names:
                yield (0, tuple(node.name.split(""."")))
",if node . module is None :,162
"def __init__(self, spec=None, add_book=True, xl=None, visible=None):
    # visible is only required on mac
    if spec is not None:
        warn(""spec is ignored on Windows."")
    if xl is None:
        # new instance
        self._xl = COMRetryObjectWrapper(DispatchEx(""Excel.Application""))
        if add_book:
            self._xl.Workbooks.Add()
        self._hwnd = None
    elif isinstance(xl, int):
        self._xl = None
        self._hwnd = xl
    else:
        self._xl = xl
        self._hwnd = None
",if add_book :,166
"def _find_split():
    """"""Find the first = sign to split on (that isn't in [brackets])""""""
    key = []
    value = []
    brackets = False
    chars = list(expression)
    while chars:
        c = chars.pop(0)
        if c == ""="" and not brackets:
            # keys done the rest is value
            value = chars
            break
        elif c == ""["":
            brackets = True
            key += c
        elif c == ""]"" and brackets:
            brackets = False
            key += c
        else:
            # normal character
            key += c
    return """".join(key), """".join(value)
","elif c == ""["" :",177
"def _ApplySizeLimit(
    regions: Iterable[rdf_memory.ProcessMemoryRegion], size_limit: int
) -> List[rdf_memory.ProcessMemoryRegion]:
    """"""Truncates regions so that the total size stays in size_limit.""""""
    total_size = 0
    regions_in_limit = []
    for region in regions:
        if total_size >= size_limit:
            break
        region.dumped_size = min(region.size, size_limit - total_size)
        regions_in_limit.append(region)
        total_size += region.dumped_size
    return regions_in_limit
",if total_size >= size_limit :,151
"def _get_matched_files(input_path):
    """"""Returns all files that matches the input_path.""""""
    input_patterns = input_path.strip().split("","")
    all_matched_files = []
    for input_pattern in input_patterns:
        input_pattern = input_pattern.strip()
        if not input_pattern:
            continue
        matched_files = tf.io.gfile.glob(input_pattern)
        if not matched_files:
            raise ValueError(""%s does not match any files."" % input_pattern)
        else:
            all_matched_files.extend(matched_files)
    return sorted(all_matched_files)
",if not input_pattern :,166
"def _add_kid(key, x):
    if x is None:
        kids[key] = None
    else:
        if type(x) in (type([]), type(())):
            x1 = [i for i in x if isinstance(i, TVTKBase)]
            if x1:
                kids[key] = x1
        elif isinstance(x, TVTKBase):
            if hasattr(x, ""__iter__""):
                # Don't add iterable objects that contain non
                # acceptable nodes
                if len(list(x)) and isinstance(list(x)[0], TVTKBase):
                    kids[key] = x
            else:
                kids[key] = x
",if x1 :,196
"def _read_info(self, field):
    fs.File._read_info(self, field)
    if field == ""dimensions"":
        self.dimensions = self._plat_get_dimensions()
        if self._get_orientation() in {5, 6, 7, 8}:
            self.dimensions = (self.dimensions[1], self.dimensions[0])
    elif field == ""exif_timestamp"":
        self.exif_timestamp = self._get_exif_timestamp()
","if self . _get_orientation ( ) in { 5 , 6 , 7 , 8 } :",116
"def process_timeline(self, info):
    children = info.get(""_children"", [])
    if not children:
        return False
    for entry in children:
        state = TIMELINE_STATES.get(entry.get(""state""))
        if not state:
            continue
        self.emit(""%s.timeline.%s"" % (self.name, state), entry)
    return True
",if not state :,98
"def from_chx(self):
    if self.array is not None:
        device = backend.get_device_from_array(self.array)
    else:
        device = self._initial_device
    if device.xp is chainerx:
        backend_name = device.device.backend.name
        if backend_name == ""native"":
            self._initial_device = backend.CpuDevice()
        elif backend_name == ""cuda"":
            self._initial_device = backend.GpuDevice.from_device_id(device.device.index)
    super(Parameter, self)._from_chx(allow_unchaining=True)
","elif backend_name == ""cuda"" :",162
"def get_title_extensions(self, title=None):
    extensions = []
    for extension in self.title_extensions:
        if title:
            extensions.extend(list(extension.objects.filter(extended_object=title)))
        else:
            extensions.extend(list(extension.objects.all()))
    return extensions
",if title :,83
"def tag(vs, push=False):
    """"""Make the tagged release commit""""""
    patch_version(vs, repo_root)
    with cd(repo_root):
        run('git commit -a -m ""release {}""'.format(vs))
        run('git tag -a -m ""release {0}"" {0}'.format(vs))
        if push:
            run(""git push"")
            run(""git push --tags"")
",if push :,108
"def parse_bismark_report(self, report, regexes):
    """"""Search a bismark report with a set of regexes""""""
    parsed_data = {}
    for k, r in regexes.items():
        r_search = re.search(r, report, re.MULTILINE)
        if r_search:
            try:
                parsed_data[k] = float(r_search.group(1))
            except ValueError:
                parsed_data[k] = r_search.group(1)  # NaN
    if len(parsed_data) == 0:
        return None
    return parsed_data
",if r_search :,156
"def _scroll_delete(dirname, max_num_checkpoints=3):
    dirs = os.listdir(dirname)
    serial_map = {}
    for serial in dirs:
        serial_num = _get_dir_serial(serial)
        serial_map[serial_num] = serial
    if len(list(serial_map.keys())) <= max_num_checkpoints:
        return
    serials = list(serial_map.keys())
    serials.sort(reverse=True)
    serials = serials[max_num_checkpoints:]
    for serial in serials:
        cur_dir = _get_serial_dir(dirname, serial)
        try:
            shutil.rmtree(cur_dir)
        except OSError as err:
            if err.errno != errno.ENOENT:
                raise err
",if err . errno != errno . ENOENT :,198
"def _lookup(self, key, dicts=None, filters=()):
    if dicts is None:
        dicts = self.dicts
    key_len = len(key)
    if key_len > self.longest_key:
        return None
    for d in dicts:
        if not d.enabled:
            continue
        if key_len > d.longest_key:
            continue
        value = d.get(key)
        if value:
            for f in filters:
                if f(key, value):
                    return None
            return value
",if not d . enabled :,150
"def get_preset(self, unit):
    for line in self._lines:
        m = re.match(r""(enable|disable)\s+(\S+)"", line)
        if m:
            status, pattern = m.group(1), m.group(2)
            if fnmatch.fnmatchcase(unit, pattern):
                logg.debug(""%s %s => %s [%s]"", status, pattern, unit, self.filename())
                return status
    return None
",if m :,121
"def gen_cpu_name(cpu):
    if cpu == ""simple"":
        return event_download.get_cpustr()
    for j in known_cpus:
        if cpu == j[0]:
            if isinstance(j[1][0], tuple):
                return ""GenuineIntel-6-%02X-%d"" % j[1][0]
            else:
                return ""GenuineIntel-6-%02X"" % j[1][0]
    assert False
",if cpu == j [ 0 ] :,127
"def allow_request(self, request, view):
    if settings.API_THROTTLING:
        request_allowed = super(GranularUserRateThrottle, self).allow_request(
            request, view
        )
        if not request_allowed:
            user = getattr(request, ""user"", None)
            if user and request.user.is_authenticated:
                log.info(""User %s throttled for scope %s"", request.user, self.scope)
                ActivityLog.create(amo.LOG.THROTTLED, self.scope, user=user)
        return request_allowed
    else:
        return True
",if not request_allowed :,164
"def __getitem__(self, tagSet):
    try:
        return self.__presentTypes[tagSet]
    except KeyError:
        if self.__defaultType is None:
            raise KeyError()
        elif tagSet in self.__skipTypes:
            raise error.PyAsn1Error(""Key in negative map"")
        else:
            return self.__defaultType
",if self . __defaultType is None :,93
"def _media(self):
    # Get the media property of the superclass, if it exists
    sup_cls = super(cls, self)
    try:
        base = sup_cls.media
    except AttributeError:
        base = Media()
    # Get the media definition for this class
    definition = getattr(cls, ""Media"", None)
    if definition:
        extend = getattr(definition, ""extend"", True)
        if extend:
            if extend == True:
                m = base
            else:
                m = Media()
                for medium in extend:
                    m = m + base[medium]
            return m + Media(definition)
        else:
            return Media(definition)
    else:
        return base
",if extend == True :,197
"def ascii85decode(data):
    n = b = 0
    out = """"
    for c in data:
        if ""!"" <= c and c <= ""u"":
            n += 1
            b = b * 85 + (ord(c) - 33)
            if n == 5:
                out += struct.pack("">L"", b)
                n = b = 0
        elif c == ""z"":
            assert n == 0
            out += ""\0\0\0\0""
        elif c == ""~"":
            if n:
                for _ in range(5 - n):
                    b = b * 85 + 84
                out += struct.pack("">L"", b)[: n - 1]
            break
    return out
","elif c == ""~"" :",200
"def get_max_shape(data):
    if isinstance(data, dict):
        max = 0
        val = None
        for k, v in data.items():
            tmp = reduce(lambda x, y: x * y, v.shape)
            if tmp > max:
                val = v.shape
                max = tmp
        return val
    else:
        return data[0].shape
",if tmp > max :,109
"def _subscribe_core(
    self, observer: typing.Observer, scheduler: Optional[typing.Scheduler] = None
) -> typing.Disposable:
    with self.lock:
        self.check_disposed()
        if not self.is_stopped:
            self.observers.append(observer)
            return InnerSubscription(self, observer)
        ex = self.exception
        has_value = self.has_value
        value = self.value
    if ex:
        observer.on_error(ex)
    elif has_value:
        observer.on_next(value)
        observer.on_completed()
    else:
        observer.on_completed()
    return Disposable()
",if not self . is_stopped :,182
"def ratio(self, outevent, inevent):
    assert outevent not in self
    assert inevent in self
    for function in compat_itervalues(self.functions):
        assert outevent not in function
        assert inevent in function
        function[outevent] = ratio(function[inevent], self[inevent])
        for call in compat_itervalues(function.calls):
            assert outevent not in call
            if inevent in call:
                call[outevent] = ratio(call[inevent], self[inevent])
    self[outevent] = 1.0
",if inevent in call :,131
"def _format_changelog(self, changelog):
    """"""Format the changelog correctly and convert it to a list of strings""""""
    if not changelog:
        return changelog
    new_changelog = []
    for line in changelog.strip().split(""\n""):
        line = line.strip()
        if line[0] == ""*"":
            new_changelog.extend(["""", line])
        elif line[0] == ""-"":
            new_changelog.append(line)
        else:
            new_changelog.append(""  "" + line)
    # strip trailing newline inserted by first changelog entry
    if not new_changelog[0]:
        del new_changelog[0]
    return new_changelog
","elif line [ 0 ] == ""-"" :",168
"def _set_base64md5(self, value):
    if value:
        if not isinstance(value, six.string_types):
            value = value.decode(""utf-8"")
        self.local_hashes[""md5""] = binascii.a2b_base64(value)
    elif ""md5"" in self.local_hashes:
        del self.local_hashes[""md5""]
","if not isinstance ( value , six . string_types ) :",99
"def setGeometry(self, rect):
    """"""Set the window geometry, but only once when using the qttabs gui.""""""
    if g.app.qt_use_tabs:
        m = self.leo_master
        assert self.leo_master
        # Only set the geometry once, even for new files.
        if not hasattr(m, ""leo_geom_inited""):
            m.leo_geom_inited = True
            self.leo_master.setGeometry(rect)
            QtWidgets.QMainWindow.setGeometry(self, rect)
    else:
        QtWidgets.QMainWindow.setGeometry(self, rect)
","if not hasattr ( m , ""leo_geom_inited"" ) :",166
"def _get_extension_suppressions(mod_loaders):
    res = []
    for m in mod_loaders:
        suppressions = getattr(m, ""suppress_extension"", None)
        if suppressions:
            suppressions = (
                suppressions if isinstance(suppressions, list) else [suppressions]
            )
            for sup in suppressions:
                if isinstance(sup, ModExtensionSuppress):
                    res.append(sup)
    return res
","if isinstance ( sup , ModExtensionSuppress ) :",134
"def _check_positional(results):
    positional = None
    for name, char in results:
        if positional is None:
            positional = name is None
        else:
            if (name is None) != positional:
                raise TranslationError(
                    ""format string mixes positional "" ""and named placeholders""
                )
    return bool(positional)
",if ( name is None ) != positional :,98
"def ascii85decode(data):
    n = b = 0
    out = """"
    for c in data:
        if ""!"" <= c and c <= ""u"":
            n += 1
            b = b * 85 + (ord(c) - 33)
            if n == 5:
                out += struct.pack("">L"", b)
                n = b = 0
        elif c == ""z"":
            assert n == 0
            out += ""\0\0\0\0""
        elif c == ""~"":
            if n:
                for _ in range(5 - n):
                    b = b * 85 + 84
                out += struct.pack("">L"", b)[: n - 1]
            break
    return out
","elif c == ""z"" :",200
"def __getattr__(self, name):
    # if the aval property raises an AttributeError, gets caught here
    assert skip_checks or name != ""aval""
    try:
        attr = getattr(self.aval, name)
    except KeyError as err:
        raise AttributeError(
            ""{} has no attribute {}"".format(self.__class__.__name__, name)
        ) from err
    else:
        t = type(attr)
        if t is aval_property:
            return attr.fget(self)
        elif t is aval_method:
            return types.MethodType(attr.fun, self)
        else:
            return attr
",if t is aval_property :,165
"def build_vocab(self, filename):
    EOS = ""</eos>""
    vocab_dict = {}
    ids = 0
    vocab_dict[EOS] = ids
    ids += 1
    with open(filename, ""r"") as f:
        for line in f.readlines():
            for w in line.strip().split():
                if w not in vocab_dict:
                    vocab_dict[w] = ids
                    ids += 1
    self.vocab_size = ids
    return vocab_dict
",if w not in vocab_dict :,131
"def eval_dummy_genomes_iznn(genomes, config):
    for genome_id, genome in genomes:
        net = neat.iznn.IZNN.create(genome, config)
        if genome_id < 10:
            net.reset()
            genome.fitness = 0.0
        elif genome_id <= 150:
            genome.fitness = 0.5
        else:
            genome.fitness = 1.0
",elif genome_id <= 150 :,129
"def _add_csrf(self, without_csrf, explicit_csrf=None):
    parts = urlparse(without_csrf)
    query = parse_qs(parts[4])
    with self.app.session_transaction() as sess:
        if explicit_csrf is not None:
            query[CSRF_TOKEN_KEY] = explicit_csrf
        else:
            sess[CSRF_TOKEN_KEY] = ""something""
            query[CSRF_TOKEN_KEY] = sess[CSRF_TOKEN_KEY]
    return urlunparse(list(parts[0:4]) + [urlencode(query)] + list(parts[5:]))
",if explicit_csrf is not None :,154
"def test_confirm_extension_is_yml(self):
    files_with_incorrect_extensions = []
    for file in self.yield_next_rule_file_path(self.path_to_rules):
        file_name_and_extension = os.path.splitext(file)
        if len(file_name_and_extension) == 2:
            extension = file_name_and_extension[1]
            if extension != "".yml"":
                files_with_incorrect_extensions.append(file)
    self.assertEqual(
        files_with_incorrect_extensions,
        [],
        Fore.RED + ""There are rule files with extensions other than .yml"",
    )
",if len ( file_name_and_extension ) == 2 :,172
"def _handle_eof(self, m):
    self.lock.acquire()
    try:
        if not self.eof_received:
            self.eof_received = True
            self.in_buffer.close()
            self.in_stderr_buffer.close()
            if self._pipe is not None:
                self._pipe.set_forever()
    finally:
        self.lock.release()
    self._log(DEBUG, ""EOF received ({})"".format(self._name))
",if self . _pipe is not None :,127
"def do_close(self):
    if self.flags is not None and (self.flags == ""c"" or self.flags == ""w""):
        if self._is_new:
            insert = self.table.insert()
            self.bind.execute(
                insert,
                namespace=self.namespace,
                data=self.hash,
                accessed=datetime.now(),
                created=datetime.now(),
            )
            self._is_new = False
        else:
            update = self.table.update(self.table.c.namespace == self.namespace)
            self.bind.execute(update, data=self.hash, accessed=datetime.now())
    self.flags = None
",if self . _is_new :,194
"def __init__(self, sh_cmd, title=None, env=None, d=None):
    self.command = d and d.getVar(""OE_TERMINAL_CUSTOMCMD"")
    if self.command:
        if not ""{command}"" in self.command:
            self.command += "" {command}""
        Terminal.__init__(self, sh_cmd, title, env, d)
        logger.warn(""Custom terminal was started."")
    else:
        logger.debug(1, ""No custom terminal (OE_TERMINAL_CUSTOMCMD) set"")
        raise UnsupportedTerminal(""OE_TERMINAL_CUSTOMCMD not set"")
","if not ""{command}"" in self . command :",152
"def __code_color(self, code):
    if code in self.last_dist.keys():
        if int(code) == 0:
            return self.screen.markup.GREEN
        elif int(code) == 314:
            return self.screen.markup.MAGENTA
        else:
            return self.screen.markup.RED
    else:
        return """"
",if int ( code ) == 0 :,97
"def _calc_benchmark_stat(self, f):
    timer = Timer()
    i = 0
    while True:
        f()
        i += 1
        if i >= self.min_run:
            _, elapsed = timer.lap()
            if elapsed > self.min_time:
                break
    return BenchmarkStat(elapsed / i, i)
",if elapsed > self . min_time :,96
"def _get_user_call_site():
    import traceback
    stack = traceback.extract_stack(sys._getframe())
    for i in range(1, len(stack)):
        callee_path = stack[i][STACK_FILE_NAME]
        if src_dir == os.path.dirname(os.path.abspath(callee_path)):
            caller_path = stack[i - 1][STACK_FILE_NAME]
            caller_lineno = stack[i - 1][STACK_LINE_NUM]
            dpark_func_name = stack[i][STACK_FUNC_NAME]
            user_call_site = ""%s:%d "" % (caller_path, caller_lineno)
            return dpark_func_name, user_call_site
    return ""<func>"", "" <root>""
",if src_dir == os . path . dirname ( os . path . abspath ( callee_path ) ) :,196
"def compact_repr(record):
    parts = []
    for key in record.__attributes__:
        value = getattr(record, key)
        if not value:
            continue
        if isinstance(value, list):
            value = HIDE_LIST
        elif key == FEATS:
            value = format_feats(value)
        else:
            value = repr(value)
        value = capped_str(value)
        parts.append(""%s=%s"" % (key, value))
    return ""%s(%s)"" % (record.__class__.__name__, "", "".join(parts))
",if not value :,152
"def get_tools(self, found_files):
    self.configured_by = {}
    runners = []
    for tool_name in self.tools_to_run:
        tool = tools.TOOLS[tool_name]()
        config_result = tool.configure(self, found_files)
        if config_result is None:
            configured_by = None
            messages = []
        else:
            configured_by, messages = config_result
            if messages is None:
                messages = []
        self.configured_by[tool_name] = configured_by
        self.messages += messages
        runners.append(tool)
    return runners
",if config_result is None :,172
"def erase_previous(self):
    if self.prev:
        length = len(self.prev)
        if self.prev[-1] in (""\n"", ""\r""):
            length = length - 1
        self.write("" "" * length + ""\r"")
        self.prev = """"
","if self . prev [ - 1 ] in ( ""\n"" , ""\r"" ) :",74
"def __demo_mode_pause_if_active(self, tiny=False):
    if self.demo_mode:
        wait_time = settings.DEFAULT_DEMO_MODE_TIMEOUT
        if self.demo_sleep:
            wait_time = float(self.demo_sleep)
        if not tiny:
            time.sleep(wait_time)
        else:
            time.sleep(wait_time / 3.4)
    elif self.slow_mode:
        self.__slow_mode_pause_if_active()
",if self . demo_sleep :,134
"def pack_remaining_length(remaining_length):
    s = """"
    while True:
        byte = remaining_length % 128
        remaining_length = remaining_length // 128
        # If there are more digits to encode, set the top bit of this digit
        if remaining_length > 0:
            byte = byte | 0x80
        s = s + struct.pack(""!B"", byte)
        if remaining_length == 0:
            return s
",if remaining_length == 0 :,115
"def _get_definitions(self, schema, query):
    results, error = self.run_query(query, None)
    if error is not None:
        raise Exception(""Failed getting schema."")
    results = json_loads(results)
    for row in results[""rows""]:
        if row[""TABLE_SCHEMA""] != ""public"":
            table_name = ""{}.{}"".format(row[""TABLE_SCHEMA""], row[""TABLE_NAME""])
        else:
            table_name = row[""TABLE_NAME""]
        if table_name not in schema:
            schema[table_name] = {""name"": table_name, ""columns"": []}
        schema[table_name][""columns""].append(row[""COLUMN_NAME""])
",if table_name not in schema :,174
"def _parsed_config_to_dict(config):
    config_dict = {}
    for section in config.keys():
        if section == ""DEFAULT"":
            continue
        config_dict[section] = {}
        for option in config[section].keys():
            config_dict[section][option] = config[section][option]
    return config_dict
","if section == ""DEFAULT"" :",91
"def escape_string(self, value):
    value = EscapedString.promote(value)
    value = value.expanduser()
    result = """"
    for is_literal, txt in value.strings:
        if is_literal:
            txt = pipes.quote(txt)
            if not txt.startswith(""'""):
                txt = ""'%s'"" % txt
        else:
            txt = txt.replace(""\\"", ""\\\\"")
            txt = txt.replace('""', '\\""')
            txt = '""%s""' % txt
        result += txt
    return result
","if not txt . startswith ( ""'"" ) :",139
"def sendMessage(self, text, meta=None):
    if self.account.client is None:
        raise locals.OfflineError
    for line in text.split(""\n""):
        if meta and meta.get(""style"", None) == ""emote"":
            self.account.client.ctcpMakeQuery(self.name, [(""ACTION"", line)])
        else:
            self.account.client.msg(self.name, line)
    return succeed(text)
","if meta and meta . get ( ""style"" , None ) == ""emote"" :",116
"def clean_email(self):
    email = self.cleaned_data.get(""email"")
    if self.instance.id:
        if self.instance.email != email:
            if not User.objects.filter(email=self.cleaned_data.get(""email"")).exists():
                return self.cleaned_data.get(""email"")
            raise forms.ValidationError(""Email already exists"")
        else:
            return self.cleaned_data.get(""email"")
    else:
        if not User.objects.filter(email=self.cleaned_data.get(""email"")).exists():
            return self.cleaned_data.get(""email"")
        raise forms.ValidationError(""User already exists with this email"")
",if self . instance . email != email :,177
"def render_checks(cr, size, nchecks):
    """"""Render a checquerboard pattern to a cairo surface""""""
    cr.set_source_rgb(*gui.style.ALPHA_CHECK_COLOR_1)
    cr.paint()
    cr.set_source_rgb(*gui.style.ALPHA_CHECK_COLOR_2)
    for i in xrange(0, nchecks):
        for j in xrange(0, nchecks):
            if (i + j) % 2 == 0:
                continue
            cr.rectangle(i * size, j * size, size, size)
            cr.fill()
",if ( i + j ) % 2 == 0 :,155
"def seek(self, timestamp, log=True):
    """"""Seek to a particular timestamp in the movie.""""""
    if self.status in [PLAYING, PAUSED]:
        player = self._player
        if player and player.is_seekable():
            player.set_time(int(timestamp * 1000.0))
            self._vlc_clock.reset(timestamp)
            if self.status == PAUSED:
                self._pause_time = timestamp
        if log:
            logAttrib(self, log, ""seek"", timestamp)
",if player and player . is_seekable ( ) :,137
"def class_results_to_node(key, elements):
    title = attributetabletitle(key, key)
    ul = nodes.bullet_list("""")
    for element in elements:
        ref = nodes.reference(
            """",
            """",
            internal=True,
            refuri=""#"" + element.fullname,
            anchorname="""",
            *[nodes.Text(element.label)]
        )
        para = addnodes.compact_paragraph("""", """", ref)
        if element.badge is not None:
            ul.append(attributetable_item("""", element.badge, para))
        else:
            ul.append(attributetable_item("""", para))
    return attributetablecolumn("""", title, ul)
",if element . badge is not None :,193
"def parse_function(self, l):
    bracket = l.find(""("")
    fname = l[8:bracket]
    if self.properties:
        if self.properties[0] == ""propget"":
            self.props[fname] = 1
            self.propget[fname] = 1
        elif self.properties[0] == ""propput"":
            self.props[fname] = 1
            self.propput[fname] = 1
        else:
            self.functions[fname] = 1
    self.properties = None
","elif self . properties [ 0 ] == ""propput"" :",139
"def _slurp_from_queue(self, task_id, accept, limit=1000, no_ack=False):
    with self.app.pool.acquire_channel(block=True) as (_, channel):
        binding = self._create_binding(task_id)(channel)
        binding.declare()
        for _ in range(limit):
            msg = binding.get(accept=accept, no_ack=no_ack)
            if not msg:
                break
            yield msg
        else:
            raise self.BacklogLimitExceeded(task_id)
",if not msg :,147
"def analyse_text(text):
    if re.search(r""^\s*model\s*\{"", text, re.M):
        if re.search(r""^\s*data\s*\{"", text, re.M):
            return 0.9
        elif re.search(r""^\s*var"", text, re.M):
            return 0.9
        else:
            return 0.3
    else:
        return 0
","elif re . search ( r""^\s*var"" , text , re . M ) :",113
"def wait_for_step(self, error_buffer=None, timeout=None):
    # TODO: this might be cleaner using channels
    with self.cv:
        start = time.time()
        while True:
            if self.count != 0:
                return
            elif timeout is not None and time.time() - start > timeout:
                raise error.Error(""No rewards received in {}s"".format(timeout))
            if error_buffer:
                error_buffer.check()
            self.cv.wait(timeout=0.5)
",if self . count != 0 :,146
"def TestDictAgainst(dict, check):
    for key, value in check.iteritems():
        if dict(key) != value:
            raise error(
                ""Indexing for '%s' gave the incorrect value - %s/%s""
                % (repr(key), repr(dict[key]), repr(check[key]))
            )
",if dict ( key ) != value :,90
"def callback(username, password, msg):
    self.add_channel()
    if hasattr(self, ""_closed"") and not self._closed:
        self.attempted_logins += 1
        if self.attempted_logins >= self.max_login_attempts:
            msg += "" Disconnecting.""
            self.respond(""530 "" + msg)
            self.close_when_done()
        else:
            self.respond(""530 "" + msg)
        self.log(""USER '%s' failed login."" % username)
    self.on_login_failed(username, password)
",if self . attempted_logins >= self . max_login_attempts :,150
"def handle_disconnect(self):
    """"""Socket gets disconnected""""""
    # signal disconnected terminal with control lines
    try:
        self.serial.rts = False
        self.serial.dtr = False
    finally:
        # restore original port configuration in case it was changed
        self.serial.apply_settings(self.serial_settings_backup)
        # stop RFC 2217 state machine
        self.rfc2217 = None
        # clear send buffer
        self.buffer_ser2net = bytearray()
        # close network connection
        if self.socket is not None:
            self.socket.close()
            self.socket = None
            if self.log is not None:
                self.log.warning(""{}: Disconnected"".format(self.device))
",if self . socket is not None :,195
"def select_invitation_id_for_network(invitations, networkid, status=None):
    # Get invitations based on network and maybe status
    invitationsfornetwork = []
    for invitation in invitations:
        if invitation[""NetworkSummary""][""Id""] == networkid:
            if status is None or invitation[""Status""] == status:
                invitationsfornetwork.append(invitation[""InvitationId""])
    return invitationsfornetwork
","if invitation [ ""NetworkSummary"" ] [ ""Id"" ] == networkid :",123
"def fit(self, refstring, subpipes):
    if not isinstance(subpipes, list):
        subpipes = [subpipes]
    for subpipe in subpipes:
        if hasattr(subpipe, ""transform""):
            substring = subpipe.transform(None)
        else:
            substring = subpipe
        self._scores.append(
            (
                self.base_aligner.fit_transform(refstring, substring, get_score=True),
                subpipe,
            )
        )
    return self
","if hasattr ( subpipe , ""transform"" ) :",149
"def build_priorities(self, _iter, priorities):
    while _iter is not None:
        if self.files_treestore.iter_has_child(_iter):
            self.build_priorities(self.files_treestore.iter_children(_iter), priorities)
        elif not self.files_treestore.get_value(_iter, 1).endswith(os.path.sep):
            priorities[
                self.files_treestore.get_value(_iter, 3)
            ] = self.files_treestore.get_value(_iter, 0)
        _iter = self.files_treestore.iter_next(_iter)
    return priorities
",if self . files_treestore . iter_has_child ( _iter ) :,170
"def __init__(self, fileobj, info):
    pages = []
    complete = False
    while not complete:
        page = OggPage(fileobj)
        if page.serial == info.serial:
            pages.append(page)
            complete = page.complete or (len(page.packets) > 1)
    packets = OggPage.to_packets(pages)
    if not packets:
        raise error(""Missing metadata packet"")
    data = packets[0][7:]
    super(OggTheoraCommentDict, self).__init__(data, framing=False)
    self._padding = len(data) - self._size
",if page . serial == info . serial :,155
"def _run_interface(self, runtime):
    mel_icas = []
    for item in self.inputs.mel_icas_in:
        if os.path.exists(os.path.join(item, ""hand_labels_noise.txt"")):
            mel_icas.append(item)
    if len(mel_icas) == 0:
        raise Exception(
            ""%s did not find any hand_labels_noise.txt files in the following directories: %s""
            % (self.__class__.__name__, mel_icas)
        )
    return runtime
","if os . path . exists ( os . path . join ( item , ""hand_labels_noise.txt"" ) ) :",150
"def download_file(url, file):
    try:
        xlog.info(""download %s to %s"", url, file)
        req = opener.open(url)
        CHUNK = 16 * 1024
        with open(file, ""wb"") as fp:
            while True:
                chunk = req.read(CHUNK)
                if not chunk:
                    break
                fp.write(chunk)
        return True
    except:
        xlog.info(""download %s to %s fail"", url, file)
        return False
",if not chunk :,147
"def check_sales_order_on_hold_or_close(self, ref_fieldname):
    for d in self.get(""items""):
        if d.get(ref_fieldname):
            status = frappe.db.get_value(""Sales Order"", d.get(ref_fieldname), ""status"")
            if status in (""Closed"", ""On Hold""):
                frappe.throw(
                    _(""Sales Order {0} is {1}"").format(d.get(ref_fieldname), status)
                )
","if status in ( ""Closed"" , ""On Hold"" ) :",137
"def iterstack(sources, missing, trim, pad):
    its = [iter(t) for t in sources]
    hdrs = [next(it) for it in its]
    hdr = hdrs[0]
    n = len(hdr)
    yield tuple(hdr)
    for it in its:
        for row in it:
            outrow = tuple(row)
            if trim:
                outrow = outrow[:n]
            if pad and len(outrow) < n:
                outrow += (missing,) * (n - len(outrow))
            yield outrow
",if trim :,153
"def __call__(self, response_headers):
    rates = get_rates_from_response_headers(response_headers)
    if rates:
        time.sleep(
            self._get_wait_time(
                rates.short_usage,
                rates.long_usage,
                get_seconds_until_next_quarter(),
                get_seconds_until_next_day(),
            )
        )
        if not self.force_limits:
            self.short_limit = rates.short_limit
            self.long_limit = rates.long_limit
",if not self . force_limits :,156
"def main(self):
    self.model.clear()
    self.callman.unregister_all()
    active_handle = self.get_active(""Place"")
    if active_handle:
        active = self.dbstate.db.get_place_from_handle(active_handle)
        if active:
            self.display_place(active, None, [active_handle], DateRange())
        else:
            self.set_has_data(False)
    else:
        self.set_has_data(False)
",if active :,134
"def node_exists(self, jid=None, node=None, ifrom=None):
    with self.lock:
        if jid is None:
            jid = self.xmpp.boundjid.full
        if node is None:
            node = """"
        if ifrom is None:
            ifrom = """"
        if isinstance(ifrom, JID):
            ifrom = ifrom.full
        if (jid, node, ifrom) not in self.nodes:
            return False
        return True
",if ifrom is None :,136
"def append_to(project_url, destination):
    url = (""%smagic/%s"" % (project_url, destination)).replace(""\\"", ""/"")
    response = urllib2.urlopen(url)
    if response.getcode() == 200:
        with open(destination, ""r"") as dest:
            lines = """".join(dest.readlines())
            content = response.read()
            if content in lines:
                print_out(""IGNORED"", destination)
                return
        with open(destination, ""a"") as dest:
            dest.write(content)
            print_out(""APPEND"", destination)
",if content in lines :,158
"def close(self, invalidate=False):
    self.session.transaction = self._parent
    if self._parent is None:
        for connection, transaction, autoclose in set(self._connections.values()):
            if invalidate:
                connection.invalidate()
            if autoclose:
                connection.close()
            else:
                transaction.close()
    self._state = CLOSED
    self.session.dispatch.after_transaction_end(self.session, self)
    if self._parent is None:
        if not self.session.autocommit:
            self.session.begin()
    self.session = None
    self._connections = None
",if not self . session . autocommit :,172
"def list_local_packages(path):
    """"""Lists all local packages below a path that could be installed.""""""
    rv = []
    try:
        for filename in os.listdir(path):
            if os.path.isfile(os.path.join(path, filename, ""setup.py"")):
                rv.append(""@"" + filename)
    except OSError:
        pass
    return rv
","if os . path . isfile ( os . path . join ( path , filename , ""setup.py"" ) ) :",98
"def walk_dir(templates, dest, filter=None):
    l = []
    for root, folders, files in os.walk(templates):
        for filename in files:
            if filename.endswith("".pyc"") or (filter and filename not in filter):
                continue
            relative_dir = "".{0}"".format(
                os.path.split(os.path.join(root, filename).replace(templates, """"))[0]
            )
            l.append((os.path.join(root, filename), os.path.join(dest, relative_dir)))
    return l
","if filename . endswith ( "".pyc"" ) or ( filter and filename not in filter ) :",150
"def selectItemHelper(self, item, scroll):
    if self.frame.lockout:
        return
    w = self.treeWidget
    if item and item.IsOk():
        self.frame.lockout = True
        try:
            w.SelectItem(item)
            if scroll:
                w.ScrollTo(item)
        finally:
            self.frame.lockout = False
",if scroll :,104
"def validate_external(self, field):
    if hasattr(self, ""forum""):
        if self.forum.topics.count() > 0:
            raise ValidationError(
                _(
                    ""You cannot convert a forum that ""
                    ""contains topics into an ""
                    ""external link.""
                )
            )
",if self . forum . topics . count ( ) > 0 :,95
"def add_help(self):
    ""Attach help functions for each of the parsed token handlers.""
    for attrname, func in list(shell.BQLShell.__dict__.items()):
        if attrname[:3] != ""on_"":
            continue
        command_name = attrname[3:]
        setattr(
            self.__class__,
            ""help_{}"".format(command_name.lower()),
            lambda _, fun=func: print(
                textwrap.dedent(fun.__doc__).strip(), file=self.outfile
            ),
        )
","if attrname [ : 3 ] != ""on_"" :",141
"def createFields(self):
    yield UInt8(self, ""tag"")
    yield UInt24(self, ""size"", ""Content size"")
    yield UInt24(self, ""timestamp"", ""Timestamp in millisecond"")
    yield NullBytes(self, ""reserved"", 4)
    size = self[""size""].value
    if size:
        if self.parser:
            for field in self.parser(self, size):
                yield field
        else:
            yield RawBytes(self, ""content"", size)
",if self . parser :,130
"def migrate_model_field_data(Model):
    queryset = Model.objects.all().order_by(""pk"")
    for batch_pks in queryset_in_batches(queryset):
        instances = []
        batch = Model.objects.filter(pk__in=batch_pks)
        for instance in batch:
            if instance.content_json:
                instance.content_json = parse_to_editorjs(instance.content_json)
                instances.append(instance)
        Model.objects.bulk_update(instances, [""content_json""])
",if instance . content_json :,139
"def _add_account(cfg, which):
    username = self._get_account(cfg)
    if (
        username
        and ((username == only) or only is None)
        and cfg.auth_type == ""password""
    ):
        if username in accounts:
            accounts[username][which] = cfg.host
        else:
            fingerprint = self._user_fingerprint(username)
            accounts[username] = {
                which: cfg.host,
                ""username"": username,
                ""policy"": self._get_policy(fingerprint),
            }
            if accounts[username][""policy""] is None:
                del accounts[username][""policy""]
",if username in accounts :,180
"def update_msg_tags(self, msg_idx_pos, msg_info):
    tags = set(self.get_tags(msg_info=msg_info))
    with self._lock:
        for tid in set(self.TAGS.keys()) - tags:
            self.TAGS[tid] -= set([msg_idx_pos])
        for tid in tags:
            if tid not in self.TAGS:
                self.TAGS[tid] = set()
            self.TAGS[tid].add(msg_idx_pos)
",if tid not in self . TAGS :,135
"def close(self, reason=""protocol closed, reason unspecified""):
    if self.connection:
        self.logger.debug(reason, self.connection.session())
        # must be first otherwise we could have a loop caused by the raise in the below
        self.connection.close()
        self.connection = None
        self.peer.stats[""down""] = self.peer.stats.get(""down"", 0) + 1
        try:
            if self.peer.neighbor.api[""neighbor-changes""]:
                self.peer.reactor.processes.down(self.peer.neighbor, reason)
        except ProcessError:
            self.logger.debug(
                ""could not send notification of neighbor close to API"",
                self.connection.session(),
            )
","if self . peer . neighbor . api [ ""neighbor-changes"" ] :",196
"def check_objects_exist(self, compare_id, raise_exc=True):
    for uid in convert_compare_id_to_list(compare_id):
        if not self.existence_quick_check(uid):
            if raise_exc:
                raise FactCompareException(""{} not found in database"".format(uid))
            return True
    return False
",if not self . existence_quick_check ( uid ) :,94
"def on_double_click(self, event):
    # self.save_current_folder()
    path = self.get_selected_path()
    if path:
        kind = self.get_selected_kind()
        if kind == ""dir"":
            self.focus_into(path)
        else:
            self.log_frame.load_log(path)
    return ""break""  # avoid default action of opening the node
","if kind == ""dir"" :",110
"def resolve_cloudtrail_payload(self, payload):
    sources = self.data.get(""sources"", [])
    events = []
    for e in self.data.get(""events""):
        if not isinstance(e, dict):
            events.append(e)
            event_info = CloudWatchEvents.get(e)
            if event_info is None:
                continue
        else:
            event_info = e
            events.append(e[""event""])
        sources.append(event_info[""source""])
    payload[""detail""] = {""eventSource"": list(set(sources)), ""eventName"": events}
","if not isinstance ( e , dict ) :",159
"def load_graph_session_from_ckpt(ckpt_path, sess_config, print_op=False):
    """"""load graph and session from checkpoint file""""""
    graph = tf.Graph()
    with graph.as_default():  # pylint: disable=not-context-manager
        sess = get_session(sess_config)
        with sess.as_default():  # pylint: disable=not-context-manager
            # Load the saved meta graph and restore variables
            saver = tf.train.import_meta_graph(""{}.meta"".format(ckpt_path))
            saver.restore(sess, ckpt_path)
        if print_op:
            print_ops(graph, prefix=""load_graph_session_from_ckpt"")
    return graph, sess
",if print_op :,186
"def _parseConfigFile(self, iniPath, createConfig=True):
    parser = SafeConfigParserUnicode(strict=False)
    if not os.path.isfile(iniPath):
        if createConfig:
            open(iniPath, ""w"").close()
        else:
            return
    parser.readfp(codecs.open(iniPath, ""r"", ""utf_8_sig""))
    for section, options in list(self._iniStructure.items()):
        if parser.has_section(section):
            for option in options:
                if parser.has_option(section, option):
                    self._config[option] = parser.get(section, option)
","if parser . has_option ( section , option ) :",170
"def parse(self):
    while 1:
        l = self.f.readline()
        if not l:
            return
        l = l.strip()
        if l.startswith(""[""):
            self.parse_uuid(l)
        elif l.startswith(""interface"") or l.startswith(""dispinterface""):
            self.parse_interface(l)
        elif l.startswith(""coclass""):
            self.parse_coclass(l)
","elif l . startswith ( ""interface"" ) or l . startswith ( ""dispinterface"" ) :",117
"def encode(self):
    if not isinstance(self.expr, m2_expr.ExprInt):
        return False
    if not test_set_sf(self.parent, self.expr.size):
        return False
    value = int(self.expr)
    if value < 1 << self.l:
        self.parent.shift.value = 0
    else:
        if value & 0xFFF:
            return False
        value >>= 12
        if value >= 1 << self.l:
            return False
        self.parent.shift.value = 1
    self.value = value
    return True
",if value >= 1 << self . l :,154
"def _func_runner(self):
    _locals.thread = self
    try:
        self._final_result = self.target(*self.args, **self.kwargs)
        self._final_exc = None
    except BaseException as e:
        self._final_result = None
        self._final_exc = e
        if not isinstance(e, errors.CancelledError):
            log.warning(""Unexpected exception in cancelled async thread"", exc_info=True)
    finally:
        self._request.set_result(None)
","if not isinstance ( e , errors . CancelledError ) :",133
"def _set_dialect(self, value):
    if value is None:
        self._dialect = mac_eui48
    else:
        if hasattr(value, ""word_size"") and hasattr(value, ""word_fmt""):
            self._dialect = value
        else:
            raise TypeError(""custom dialects should subclass mac_eui48!"")
","if hasattr ( value , ""word_size"" ) and hasattr ( value , ""word_fmt"" ) :",89
"def fixup_namespace_packages(path_item, parent=None):
    """"""Ensure that previously-declared namespace packages include path_item""""""
    imp.acquire_lock()
    try:
        for package in _namespace_packages.get(parent, ()):
            subpath = _handle_ns(package, path_item)
            if subpath:
                fixup_namespace_packages(subpath, package)
    finally:
        imp.release_lock()
",if subpath :,111
"def close_file_descriptor(self, fd):
    """"""Attempt to close a file descriptor.""""""
    start_timer = time.time()
    error = """"
    while True:
        try:
            fd.close()
            break
        except OSError as e:
            # Undoubtedly close() was called during a concurrent operation on the same file object.
            log.debug(""Error closing file descriptor: %s"" % str(e))
            time.sleep(0.5)
            current_wait_time = time.time() - start_timer
            if current_wait_time >= 600:
                error = ""Error closing file descriptor: %s"" % str(e)
                break
    return error
",if current_wait_time >= 600 :,186
"def p_constant(self, p):
    """"""constant : PP_NUMBER""""""
    value = p[1].rstrip(""LlUu"")
    try:
        if value[:2] == ""0x"":
            value = int(value[2:], 16)
        elif value[0] == ""0"":
            value = int(value, 8)
        else:
            value = int(value)
    except ValueError:
        value = value.rstrip(""eEfF"")
        try:
            value = float(value)
        except ValueError:
            value = 0
    p[0] = ConstantExpressionNode(value)
","elif value [ 0 ] == ""0"" :",163
"def set_add_delete_state(self):
    ""Toggle the state for the help list buttons based on list entries.""
    if self.helplist.size() < 1:  # No entries in list.
        self.button_helplist_edit.state((""disabled"",))
        self.button_helplist_remove.state((""disabled"",))
    else:  # Some entries.
        if self.helplist.curselection():  # There currently is a selection.
            self.button_helplist_edit.state((""!disabled"",))
            self.button_helplist_remove.state((""!disabled"",))
        else:  # There currently is not a selection.
            self.button_helplist_edit.state((""disabled"",))
            self.button_helplist_remove.state((""disabled"",))
",if self . helplist . curselection ( ) :,192
"def _erase_status():
    CodeintelHandler.status_lock.acquire()
    try:
        if msg == CodeintelHandler.status_msg.get(lid, [None, None, 0])[1]:
            view.erase_status(lid)
            CodeintelHandler.status_msg[lid][1] = None
            if lid in CodeintelHandler.status_lineno:
                del CodeintelHandler.status_lineno[lid]
    finally:
        CodeintelHandler.status_lock.release()
","if msg == CodeintelHandler . status_msg . get ( lid , [ None , None , 0 ] ) [ 1 ] :",135
"def PARSE_TWO_PARAMS(x, y):
    """"""used to convert different possible x/y params to a tuple""""""
    if y is not None:
        return (x, y)
    else:
        if isinstance(x, (list, tuple)):
            return (x[0], x[1])
        else:
            if isinstance(x, UNIVERSAL_STRING):
                x = x.strip()
                if "","" in x:
                    return [int(w.strip()) for w in x.split("","")]
            return (x, x)
","if isinstance ( x , ( list , tuple ) ) :",147
"def cancel_spot_fleet_requests(self, spot_fleet_request_ids, terminate_instances):
    spot_requests = []
    for spot_fleet_request_id in spot_fleet_request_ids:
        spot_fleet = self.spot_fleet_requests[spot_fleet_request_id]
        if terminate_instances:
            spot_fleet.target_capacity = 0
            spot_fleet.terminate_instances()
        spot_requests.append(spot_fleet)
        del self.spot_fleet_requests[spot_fleet_request_id]
    return spot_requests
",if terminate_instances :,145
"def pop(self, key, default=_MISSING):
    # NB: hit/miss counts are bypassed for pop()
    with self._lock:
        try:
            ret = super(LRI, self).pop(key)
        except KeyError:
            if default is _MISSING:
                raise
            ret = default
        else:
            self._remove_from_ll(key)
        return ret
",if default is _MISSING :,108
"def _remove_optional_none_type_hints(self, type_hints, defaults):
    # If argument has None as a default, typing.get_type_hints adds
    # optional None to the information it returns. We don't want that.
    for arg in defaults:
        if defaults[arg] is None and arg in type_hints:
            type_ = type_hints[arg]
            if self._is_union(type_):
                types = type_.__args__
                if len(types) == 2 and types[1] is type(None):
                    type_hints[arg] = types[0]
",if len ( types ) == 2 and types [ 1 ] is type ( None ) :,157
"def reader(self, myself):
    ok = True
    line = """"
    while True:
        line = sys.stdin.readline().strip()
        if ok:
            if not line:
                ok = False
                continue
        elif not line:
            break
        else:
            ok = True
        self.Q.append(line)
    os.kill(myself, signal.SIGTERM)
",elif not line :,112
"def checkout_branch(self, branch):
    if branch in self.remote_branches:
        sickrage.app.log.debug(
            ""Branch checkout: "" + self._find_installed_version() + ""->"" + branch
        )
        if not self.install_requirements(self.current_branch):
            return False
        # remove untracked files and performs a hard reset on git branch to avoid update issues
        if sickrage.app.config.git_reset:
            self.reset()
        # fetch all branches
        self.fetch()
        __, __, exit_status = self._git_cmd(self._git_path, ""checkout -f "" + branch)
        if exit_status == 0:
            return True
    return False
",if not self . install_requirements ( self . current_branch ) :,194
"def last_ok(nodes):
    for i in range(len(nodes) - 1, -1, -1):
        if ok_node(nodes[i]):
            node = nodes[i]
            if isinstance(node, ast.Starred):
                if ok_node(node.value):
                    return node.value
                else:
                    return None
            else:
                return nodes[i]
    return None
",if ok_node ( node . value ) :,122
"def restart():
    """"""Restart application.""""""
    popen_list = [sys.executable, app.MY_FULLNAME]
    if not app.NO_RESTART:
        popen_list += app.MY_ARGS
        if ""--nolaunch"" not in popen_list:
            popen_list += [""--nolaunch""]
        logger.info(""Restarting Medusa with {options}"", options=popen_list)
        # shutdown the logger to make sure it's released the logfile BEFORE it restarts Medusa.
        logging.shutdown()
        print(popen_list)
        subprocess.Popen(popen_list, cwd=os.getcwd())
","if ""--nolaunch"" not in popen_list :",154
"def StopBackgroundWorkload(self):
    """"""Stop the background workoad.""""""
    for workload in background_workload.BACKGROUND_WORKLOADS:
        if workload.IsEnabled(self):
            if self.OS_TYPE in workload.EXCLUDED_OS_TYPES:
                raise NotImplementedError()
            workload.Stop(self)
",if self . OS_TYPE in workload . EXCLUDED_OS_TYPES :,87
"def __init__(self, token):
    self._convert_to_ascii = False
    self._find = None
    if token.search is None:
        return
    flags = 0
    self._match_this_many = 1
    if token.options:
        if ""g"" in token.options:
            self._match_this_many = 0
        if ""i"" in token.options:
            flags |= re.IGNORECASE
        if ""a"" in token.options:
            self._convert_to_ascii = True
    self._find = re.compile(token.search, flags | re.DOTALL)
    self._replace = _CleverReplace(token.replace)
","if ""i"" in token . options :",170
"def _draw_nodes(self, cr, bounding, highlight_items):
    highlight_nodes = []
    for element in highlight_items:
        if isinstance(element, Edge):
            highlight_nodes.append(element.src)
            highlight_nodes.append(element.dst)
        else:
            highlight_nodes.append(element)
    for node in self.nodes:
        if bounding is None or node._intersects(bounding):
            node._draw(cr, highlight=(node in highlight_nodes), bounding=bounding)
","if isinstance ( element , Edge ) :",134
"def _removeCachedRFInfo(self, cache_key, path, removeChildPaths):
    log.debug(""_removeCachedRFInfo: cache_key %r, path %r"", cache_key, path)
    if self._cachedFiles.has_key(cache_key):
        cache = self._cachedFiles[cache_key]
        if cache.has_key(path):
            del cache[path]
        if removeChildPaths:
            # Remove all cached paths that are under this directory
            from remotefilelib import addslash
            dirPath = addslash(path)
            for keypath in cache.keys():
                if keypath.startswith(dirPath):
                    del cache[keypath]
",if removeChildPaths :,178
"def write_row(xf, worksheet, row, row_idx, max_column):
    attrs = {""r"": ""%d"" % row_idx, ""spans"": ""1:%d"" % max_column}
    dims = worksheet.row_dimensions
    if row_idx in dims:
        row_dimension = dims[row_idx]
        attrs.update(dict(row_dimension))
    with xf.element(""row"", attrs):
        for col, cell in row:
            if cell._value is None and not cell.has_style and not cell._comment:
                continue
            el = write_cell(xf, worksheet, cell, cell.has_style)
",if cell . _value is None and not cell . has_style and not cell . _comment :,165
"def reset_feature_range(data, column_max_value, column_min_value, scale_column_idx):
    _data = copy.deepcopy(data)
    for i in scale_column_idx:
        value = _data.features[i]
        if value > column_max_value[i]:
            _data.features[i] = column_max_value[i]
        elif value < column_min_value[i]:
            _data.features[i] = column_min_value[i]
    return _data
",if value > column_max_value [ i ] :,135
"def test_listing_all_frameworks_and_check_frameworks_by_order(self):
    """"""List all frameworks and check if frameworks appear by order""""""
    result = subprocess.check_output(self.command_as_list([UMAKE, ""--list""]))
    previous_framework = None
    for element in result.split(b""\n""):
        if element.startswith(b""\t""):
            current_framework = element[: element.find(b"":"")]
            if previous_framework:
                self.assertTrue(previous_framework < current_framework)
            previous_framework = current_framework
        else:
            previous_framework = None
",if previous_framework :,160
"def merge(module_name, tree1, tree2):
    for child in tree2.node:
        if isinstance(child, ast.Function):
            replaceFunction(tree1, child.name, child)
        elif isinstance(child, ast.Assign):
            replaceAssign(tree1, child.nodes[0].name, child)
        elif isinstance(child, ast.Class):
            replaceClassMethods(tree1, child.name, child)
        else:
            raise TranslationError(
                ""Do not know how to merge %s"" % child, child, module_name
            )
    return tree1
","if isinstance ( child , ast . Function ) :",159
"def _filter_supported_drivers():
    global supported_drivers
    with Env() as gdalenv:
        ogrdrv_names = gdalenv.drivers().keys()
        supported_drivers_copy = supported_drivers.copy()
        for drv in supported_drivers.keys():
            if drv not in ogrdrv_names:
                del supported_drivers_copy[drv]
    supported_drivers = supported_drivers_copy
",if drv not in ogrdrv_names :,110
"def serialize(self, cassette_dict):
    for interaction in cassette_dict[""interactions""]:
        response = interaction[""response""]
        headers = response[""headers""]
        if ""Content-Range"" in headers and ""Content-Disposition"" in headers:
            rg, size, filename = self._parse_headers(headers)
            content = response[""body""][""string""]
            if rg[0] == 0 and rg[1] + 1 == size:
                with open(join(self.directory, filename), ""wb"") as f:
                    f.write(content)
            del response[""body""][""string""]
    return self.base_serializer.serialize(cassette_dict)
","if ""Content-Range"" in headers and ""Content-Disposition"" in headers :",176
"def verify_software_token(self, access_token, user_code):
    for user_pool in self.user_pools.values():
        if access_token in user_pool.access_tokens:
            _, username = user_pool.access_tokens[access_token]
            user = user_pool.users.get(username)
            if not user:
                raise UserNotFoundError(username)
            user.token_verified = True
            return {""Status"": ""SUCCESS""}
    else:
        raise NotAuthorizedError(access_token)
",if not user :,142
"def __fixdict(self, dict):
    for key in dict.keys():
        if key[:6] == ""start_"":
            tag = key[6:]
            start, end = self.elements.get(tag, (None, None))
            if start is None:
                self.elements[tag] = getattr(self, key), end
        elif key[:4] == ""end_"":
            tag = key[4:]
            start, end = self.elements.get(tag, (None, None))
            if end is None:
                self.elements[tag] = start, getattr(self, key)
","if key [ : 6 ] == ""start_"" :",162
"def generate_playlist(sourcefile):
    """"""Generate a playlist from video titles in sourcefile""""""
    # Hooks into this, check if the argument --description is present
    if ""--description"" in sourcefile or ""-d"" in sourcefile:
        description_generator(sourcefile)
        return
    expanded_sourcefile = path.expanduser(sourcefile)
    if not check_sourcefile(expanded_sourcefile):
        g.message = util.F(""mkp empty"") % expanded_sourcefile
    else:
        queries = read_sourcefile(expanded_sourcefile)
        g.message = util.F(""mkp parsed"") % (len(queries), sourcefile)
        if queries:
            create_playlist(queries)
            g.message = util.F(""pl help"")
            g.content = content.playlists_display()
",if queries :,195
"def flush(self):
    for record in self._unique_ordered_records:
        record.message = self._format_string.format(
            message=record.message, count=self._message_to_count[record.message]
        )
        # record.dispatcher is the logger who created the message,
        # it's sometimes supressed (by logbook.info for example)
        if record.dispatcher is not None:
            dispatch = record.dispatcher.call_handlers
        else:
            dispatch = dispatch_record
        dispatch(record)
    self.clear()
",if record . dispatcher is not None :,146
"def __init__(self, name, contents):
    self.name = name
    self.all_entries = []
    self.attr = []
    self.child = []
    self.seq_child = []
    for entry in contents:
        clean_entry = entry.rstrip(""*"")
        self.all_entries.append(clean_entry)
        if entry.endswith(""**""):
            self.seq_child.append(clean_entry)
        elif entry.endswith(""*""):
            self.child.append(clean_entry)
        else:
            self.attr.append(entry)
","if entry . endswith ( ""**"" ) :",147
"def test_empty_condition_node(cond_node):
    for node in [cond_node.true_node, cond_node.false_node]:
        if node is None:
            continue
        if type(node) is CodeNode and BaseNode.test_empty_node(node.node):
            continue
        if BaseNode.test_empty_node(node):
            continue
        return False
    return True
",if BaseNode . test_empty_node ( node ) :,108
"def test_deprecated_format_string(obj, fmt_str, should_raise_warning):
    if sys.version_info[0] == 3 and sys.version_info[1] >= 4:
        if should_raise_warning:
            self.assertRaises(TypeError, format, obj, fmt_str)
        else:
            try:
                format(obj, fmt_str)
            except TypeError:
                self.fail(""object.__format__ raised TypeError unexpectedly"")
    else:
        with warnings.catch_warnings(record=True) as w:
            warnings.simplefilter(""always"", DeprecationWarning)
            format(obj, fmt_str)
",if should_raise_warning :,168
"def get_queryset(self):
    if self.queryset is not None:
        return self.queryset._clone()
    elif self.model is not None:
        qs = self.model._default_manager
        if self.model in access_registry:
            access_class = access_registry[self.model]
            if access_class.select_related:
                qs = qs.select_related(*access_class.select_related)
            if access_class.prefetch_related:
                qs = qs.prefetch_related(*access_class.prefetch_related)
        return qs
    else:
        return super(GenericAPIView, self).get_queryset()
",if self . model in access_registry :,171
"def ping_task():
    try:
        if self._protocol.peer_manager.peer_is_good(peer):
            if peer not in self._protocol.routing_table.get_peers():
                self._protocol.add_peer(peer)
            return
        await self._protocol.get_rpc_peer(peer).ping()
    except (asyncio.TimeoutError, RemoteException):
        pass
",if self . _protocol . peer_manager . peer_is_good ( peer ) :,106
"def _validate_usage(schema_argument, variable_used):
    if isinstance(schema_argument.gql_type, GraphQLNonNull) and not isinstance(
        variable_used.type, NonNullTypeNode
    ):
        has_variable_a_df = not isinstance(
            variable_used.default_value, (NullValueNode, type(None))
        )
        has_argument_a_df = schema_argument.default_value is not None
        if not has_variable_a_df and not has_argument_a_df:
            return False
        return _validate_type_compatibility(
            variable_used.type, schema_argument.gql_type.gql_type
        )
    return _validate_type_compatibility(variable_used.type, schema_argument.gql_type)
",if not has_variable_a_df and not has_argument_a_df :,199
"def _add_kid(key, x):
    if x is None:
        kids[key] = None
    else:
        if type(x) in (type([]), type(())):
            x1 = [i for i in x if isinstance(i, TVTKBase)]
            if x1:
                kids[key] = x1
        elif isinstance(x, TVTKBase):
            if hasattr(x, ""__iter__""):
                # Don't add iterable objects that contain non
                # acceptable nodes
                if len(list(x)) and isinstance(list(x)[0], TVTKBase):
                    kids[key] = x
            else:
                kids[key] = x
","if hasattr ( x , ""__iter__"" ) :",196
"def postCreate(node, menu):
    with node.scriptNode().context():
        if node[""in""].getInput():
            cropFormat = node[""in""][""format""].getValue()
        else:
            cropFormat = GafferImage.FormatPlug.getDefaultFormat(
                node.scriptNode().context()
            )
    node[""area""].setValue(cropFormat.getDisplayWindow())
","if node [ ""in"" ] . getInput ( ) :",103
"def normalize_stroke(stroke):
    letters = set(stroke)
    if letters & _NUMBERS:
        if system.NUMBER_KEY in letters:
            stroke = stroke.replace(system.NUMBER_KEY, """")
        # Insert dash when dealing with 'explicit' numbers
        m = _IMPLICIT_NUMBER_RX.search(stroke)
        if m is not None:
            start = m.start(2)
            return stroke[:start] + ""-"" + stroke[start:]
    if ""-"" in letters:
        if stroke.endswith(""-""):
            stroke = stroke[:-1]
        elif letters & system.IMPLICIT_HYPHENS:
            stroke = stroke.replace(""-"", """")
    return stroke
",if system . NUMBER_KEY in letters :,180
"def vim_k(self):
    """"""Cursor up N lines.""""""
    if self.is_text_wrapper(self.w):
        for z in range(self.n1 * self.n):
            if self.state == ""visual"":
                self.do(""previous-line-extend-selection"")
            else:
                self.do(""previous-line"")
        self.done()
    elif self.in_tree(self.w):
        self.do(""goto-prev-visible"")
        self.done()
    else:
        self.quit()
","if self . state == ""visual"" :",147
"def parseTime(timeStr):
    regex = re.compile(constants.PARSE_TIME_REGEX)
    parts = regex.match(timeStr)
    if not parts:
        return
    parts = parts.groupdict()
    time_params = {}
    for (name, param) in parts.items():
        if param:
            if name == ""miliseconds"":
                time_params[""microseconds""] = int(param) * 1000
            else:
                time_params[name] = int(param)
    return datetime.timedelta(**time_params).total_seconds()
","if name == ""miliseconds"" :",146
"def update(self, other=None, **kwargs):
    if other is not None:
        if hasattr(other, ""items""):
            other = other.items()
        for key, value in other:
            if key in kwargs:
                raise TensorforceError.value(
                    name=""NestedDict.update"",
                    argument=""key"",
                    value=key,
                    condition=""specified twice"",
                )
            self[key] = value
    for key, value in kwargs.items():
        self[key] = value
",if key in kwargs :,153
"def to_string(self, ostream=None, verbose=None, precedence=0):
    """"""Print this expression""""""
    if ostream is None:
        ostream = sys.stdout
    _verbose = (
        pyomo.core.base.expr_common.TO_STRING_VERBOSE if verbose is None else verbose
    )
    ostream.write(self.cname() + ""( "")
    first = True
    for arg in self._args:
        if first:
            first = False
        elif _verbose:
            ostream.write("" , "")
        else:
            ostream.write("", "")
        arg.to_string(ostream=ostream, precedence=self._precedence(), verbose=verbose)
    ostream.write("" )"")
",elif _verbose :,188
"def apply_gradient_for_batch(inputs, labels, weights, loss):
    with tf.GradientTape() as tape:
        outputs = self.model(inputs, training=True)
        if isinstance(outputs, tf.Tensor):
            outputs = [outputs]
        if self._loss_outputs is not None:
            outputs = [outputs[i] for i in self._loss_outputs]
        batch_loss = loss(outputs, labels, weights)
    if variables is None:
        vars = self.model.trainable_variables
    else:
        vars = variables
    grads = tape.gradient(batch_loss, vars)
    self._tf_optimizer.apply_gradients(zip(grads, vars))
    self._global_step.assign_add(1)
    return batch_loss
",if self . _loss_outputs is not None :,193
"def check_all(self, strict=False):
    """"""run sanity check on all keys, issue warning if out of sync""""""
    same = self._is_same_value
    for path, (orig, expected) in iteritems(self._state):
        if same(self._get_path(path), expected):
            continue
        msg = ""another library has patched resource: %r"" % path
        if strict:
            raise RuntimeError(msg)
        else:
            warn(msg, PasslibRuntimeWarning)
","if same ( self . _get_path ( path ) , expected ) :",127
"def setup_child(self, child):
    child.parent = self
    if self.document:
        child.document = self.document
        if child.source is None:
            child.source = self.document.current_source
        if child.line is None:
            child.line = self.document.current_line
",if child . line is None :,84
"def shift_expr(self, nodelist):
    # shift_expr ('<<'|'>>' shift_expr)*
    node = self.com_node(nodelist[0])
    for i in range(2, len(nodelist), 2):
        right = self.com_node(nodelist[i])
        if nodelist[i - 1][0] == token.LEFTSHIFT:
            node = LeftShift([node, right], lineno=nodelist[1][2])
        elif nodelist[i - 1][0] == token.RIGHTSHIFT:
            node = RightShift([node, right], lineno=nodelist[1][2])
        else:
            raise ValueError(""unexpected token: %s"" % nodelist[i - 1][0])
    return node
",elif nodelist [ i - 1 ] [ 0 ] == token . RIGHTSHIFT :,178
"def styleRow(self, row, selected):
    if row != -1:
        if selected:
            self.getRowFormatter().addStyleName(row, ""midpanel-SelectedRow"")
        else:
            self.getRowFormatter().removeStyleName(row, ""midpanel-SelectedRow"")
",if selected :,76
"def __call__(self, img):
    img = self.topil(img)
    ops = random.choices(self.augment_list, k=self.n)
    for op, minval, maxval in ops:
        if random.random() > random.uniform(0.2, 0.8):
            continue
        val = (float(self.m) / 30) * float(maxval - minval) + minval
        img = op(img, val)
    return img
","if random . random ( ) > random . uniform ( 0.2 , 0.8 ) :",116
"def run(self, **inputs):
    if self.inputs.copy_inputs:
        self.inputs.subjects_dir = os.getcwd()
        if ""subjects_dir"" in inputs:
            inputs[""subjects_dir""] = self.inputs.subjects_dir
        copy2subjdir(self, self.inputs.surface, ""surf"")
        copy2subjdir(self, self.inputs.curvfile1, ""surf"")
        copy2subjdir(self, self.inputs.curvfile2, ""surf"")
    return super(CurvatureStats, self).run(**inputs)
","if ""subjects_dir"" in inputs :",146
"def get_func_name(obj):
    if inspect.ismethod(obj):
        match = RE_BOUND_METHOD.match(repr(obj))
        if match:
            cls = match.group(""class"")
            if not cls:
                return match.group(""name"")
            return ""%s.%s"" % (match.group(""class""), match.group(""name""))
    return None
",if not cls :,102
"def local_path_export(at_start=True, env_cmd=None):
    """"""Retrieve paths to local install, also including environment paths if env_cmd included.""""""
    paths = [get_bcbio_bin()]
    if env_cmd:
        env_path = os.path.dirname(get_program_python(env_cmd))
        if env_path not in paths:
            paths.insert(0, env_path)
    if at_start:
        return 'export PATH=%s:""$PATH"" && ' % ("":"".join(paths))
    else:
        return 'export PATH=""$PATH"":%s && ' % ("":"".join(paths))
",if env_path not in paths :,161
"def copystat(src, dst):
    """"""Copy all stat info (mode bits, atime, mtime, flags) from src to dst""""""
    st = os.stat(src)
    mode = stat.S_IMODE(st.st_mode)
    if hasattr(os, ""utime""):
        os.utime(dst, (st.st_atime, st.st_mtime))
    if hasattr(os, ""chmod""):
        os.chmod(dst, mode)
    if hasattr(os, ""chflags"") and hasattr(st, ""st_flags""):
        try:
            os.chflags(dst, st.st_flags)
        except OSError as why:
            if not hasattr(errno, ""EOPNOTSUPP"") or why.errno != errno.EOPNOTSUPP:
                raise
","if not hasattr ( errno , ""EOPNOTSUPP"" ) or why . errno != errno . EOPNOTSUPP :",193
"def _asdict(self, *, to_string: bool = False) -> dict:
    res = []
    for key in self._keys:
        value = getattr(self, key)
        if isinstance(value, Struct):
            value = value._asdict(to_string=to_string)
        elif to_string:
            value = str(value)
        res.append((key, value))
    return dict(res)
",elif to_string :,108
"def _SI(size, K=1024, i=""i""):
    """"""Return size as SI string.""""""
    if 1 < K <= size:
        f = float(size)
        for si in iter(""KMGPTE""):
            f /= K
            if f < K:
                return "" or %.1f %s%sB"" % (f, si, i)
    return """"
",if f < K :,99
"def _flatten(*args):
    arglist = []
    for arg in args:
        if isinstance(arg, _Block):
            if arg.vhdl_code is not None:
                arglist.append(arg.vhdl_code)
                continue
            else:
                arg = arg.subs
        if id(arg) in _userCodeMap[""vhdl""]:
            arglist.append(_userCodeMap[""vhdl""][id(arg)])
        elif isinstance(arg, (list, tuple, set)):
            for item in arg:
                arglist.extend(_flatten(item))
        else:
            arglist.append(arg)
    return arglist
","if isinstance ( arg , _Block ) :",179
"def new_token(self):
    data = '{{""username"": ""{}"", ""password"": ""{}""}}'.format(self.username, self.password)
    try:
        resp = requests.post(
            ""https://api.zoomeye.org/user/login"",
            data=data,
        )
        if resp.status_code != 401 and ""access_token"" in resp.json():
            content = resp.json()
            self.token = content[""access_token""]
            self.headers = {""Authorization"": ""JWT %s"" % self.token}
            return True
    except Exception as ex:
        logger.error(str(ex))
    return False
","if resp . status_code != 401 and ""access_token"" in resp . json ( ) :",173
"def finalize_computation(
    self, transaction: SignedTransactionAPI, computation: ComputationAPI
) -> ComputationAPI:
    computation = super().finalize_computation(transaction, computation)
    #
    # EIP161 state clearing
    #
    touched_accounts = collect_touched_accounts(computation)
    for account in touched_accounts:
        should_delete = self.vm_state.account_exists(
            account
        ) and self.vm_state.account_is_empty(account)
        if should_delete:
            self.vm_state.logger.debug2(
                ""CLEARING EMPTY ACCOUNT: %s"",
                encode_hex(account),
            )
            self.vm_state.delete_account(account)
    return computation
",if should_delete :,195
"def send_messages(self, text, user_ids):
    broken_items = []
    if not user_ids:
        self.logger.info(""User must be at least one."")
        return broken_items
    self.logger.info(""Going to send %d messages."" % (len(user_ids)))
    for user in tqdm(user_ids):
        if not self.send_message(text, user):
            self.error_delay()
            broken_items = user_ids[user_ids.index(user) :]
            break
    return broken_items
","if not self . send_message ( text , user ) :",144
"def editable_cpp_info(self):
    if self._layout_file:
        if os.path.isfile(self._layout_file):
            return EditableLayout(self._layout_file)
        else:
            raise ConanException(""Layout file not found: %s"" % self._layout_file)
",if os . path . isfile ( self . _layout_file ) :,79
"def to_python(self, value):
    if isinstance(value, list) and len(value) == 2 and isinstance(value[0], str):
        filename, payload = value
        try:
            payload = base64.b64decode(payload)
        except TypeError:
            pass
        else:
            if self.storage.exists(filename):
                self.storage.delete(filename)
            self.storage.save(filename, ContentFile(payload))
            return filename
    return value
",if self . storage . exists ( filename ) :,131
"def update_defaults(self, *values, **kwargs):
    for value in values:
        if type(value) == dict:
            self.DEFAULT_CONFIGURATION.update(value)
        elif isinstance(value, types.ModuleType):
            self.__defaults_from_module(value)
        elif isinstance(value, str):
            if os.path.exists(value):
                self.__defaults_from_file(value)
            else:
                logger.warning(""Configuration file {} does not exist."".format(value))
        elif isinstance(value, type(None)):
            pass
        else:
            raise ValueError(""Cannot interpret {}"".format(value))
    self.DEFAULT_CONFIGURATION.update(kwargs)
","elif isinstance ( value , types . ModuleType ) :",184
"def __getitem__(self, item: str) -> Any:
    try:
        return self.data[item]
    except KeyError:
        for g in self.extended_groups():
            try:
                r = g.data[item]
                return r
            except KeyError:
                continue
        r = self.defaults.data.get(item)
        if r is not None:
            return r
        raise
",if r is not None :,118
"def _parse_arguments(self, handler_method):
    spec = DynamicArgumentParser().parse(self._argspec, self.longname)
    if not self._supports_kwargs:
        if spec.kwargs:
            raise DataError(
                ""Too few '%s' method parameters for **kwargs ""
                ""support."" % self._run_keyword_method_name
            )
        if spec.kwonlyargs:
            raise DataError(
                ""Too few '%s' method parameters for ""
                ""keyword-only arguments support."" % self._run_keyword_method_name
            )
    spec.types = GetKeywordTypes(self.library.get_instance())(self._handler_name)
    return spec
",if spec . kwonlyargs :,183
"def test_orphans_match(self):
    """"""api handles last three chars match query""""""
    response = self.client.get(""%s?q=%s"" % (self.api_link, self.user.username[-3:]))
    self.assertEqual(response.status_code, 200)
    response_json = response.json()
    self.assertIn(""users"", [p[""id""] for p in response_json])
    for provider in response_json:
        if provider[""id""] == ""users"":
            results = provider[""results""][""results""]
            self.assertEqual(len(results), 1)
            self.assertEqual(results[0][""id""], self.user.id)
","if provider [ ""id"" ] == ""users"" :",164
"def test_costs_1D_noisy_names(signal_bkps_1D_noisy, cost_name):
    signal, bkps = signal_bkps_1D_noisy
    cost = cost_factory(cost_name)
    cost.fit(signal)
    cost.fit(signal.flatten())
    cost.error(0, 100)
    cost.error(100, signal.shape[0])
    cost.error(10, 50)
    cost.sum_of_costs(bkps)
    with pytest.raises(NotEnoughPoints):
        if cost_name == ""cosine"":
            cost.min_size = 4
            cost.error(1, 2)
        else:
            cost.error(1, 2)
","if cost_name == ""cosine"" :",184
"def _delete_access_key(self, params):
    sys.stdout.write(""Deleting the IAM user access keys... "")
    list_access_keys = self.iam.get_paginator(""list_access_keys"")
    try:
        for response in list_access_keys.paginate(UserName=params.user_name):
            for access_key in response[""AccessKeyMetadata""]:
                self.iam.delete_access_key(
                    UserName=params.user_name, AccessKeyId=access_key[""AccessKeyId""]
                )
    except ClientError as e:
        if e.response.get(""Error"", {}).get(""Code"") != ""NoSuchEntity"":
            raise e
    sys.stdout.write(""DONE\n"")
","if e . response . get ( ""Error"" , { } ) . get ( ""Code"" ) != ""NoSuchEntity"" :",181
"def run_pending(self, now=None):
    """"""Runs the command if scheduled""""""
    now = now or datetime.now()
    if self.is_enabled():
        if self.last_run is None:
            self.last_run = now
        next_time = self.schedule(self.last_run).get_next()
        if next_time < now:
            self.last_run = now
            return self.run()
    return -1
",if next_time < now :,119
"def parse_row(cls, doc_row):
    row = {}
    for field_name, field in FIELD_MAP.items():
        if len(doc_row) > field[1]:
            field_value = doc_row[field[1]]
        else:
            field_value = """"
        if len(field) >= 3 and callable(field[2]):
            field_value = field[2](field_value)
        row[field_name] = field_value
    return row
",if len ( doc_row ) > field [ 1 ] :,127
"def list(self, items, columns=4, width=80):
    items = list(sorted(items))
    colw = width // columns
    rows = (len(items) + columns - 1) // columns
    for row in range(rows):
        for col in range(columns):
            i = col * rows + row
            if i < len(items):
                self.output.write(items[i])
                if col < columns - 1:
                    self.output.write("" "" + "" "" * (colw - 1 - len(items[i])))
        self.output.write(""\n"")
",if col < columns - 1 :,158
"def _on_message(self, storage, data):
    if ""_meta"" in data and ""session_id"" in data[""_meta""]:
        self.session_id = data[""_meta""][""session_id""]
    if is_blacklisted(data.get(""url"", """")):
        blacklist_error(data, self)
        return
    command = data[""_command""]
    command = self._handlers.get(command, command)
    with data_store_context():
        commands = Commands(data, self, storage)
        result = getattr(commands, command, lambda: None)()
    if result:
        result.setdefault(""_command"", data.get(""_callback"", command))
        if ""_meta"" in data and ""id"" in data[""_meta""]:
            result[""id""] = data[""_meta""][""id""]
    return result
","if ""_meta"" in data and ""id"" in data [ ""_meta"" ] :",198
"def get_model_params(problem_type: str, hyperparameters):
    penalty = hyperparameters.get(""penalty"", L2)
    handle_text = hyperparameters.get(""handle_text"", IGNORE)
    if problem_type == REGRESSION:
        if penalty == L2:
            model_class = Ridge
        elif penalty == L1:
            model_class = Lasso
        else:
            logger.warning(
                ""Unknown value for penalty {} - supported types are [l1, l2] - falling back to l2"".format(
                    penalty
                )
            )
            penalty = L2
            model_class = Ridge
    else:
        model_class = LogisticRegression
    return model_class, penalty, handle_text
",if penalty == L2 :,200
"def get_queryset(self):
    if self.queryset is not None:
        return self.queryset._clone()
    elif self.model is not None:
        qs = self.model._default_manager
        if self.model in access_registry:
            access_class = access_registry[self.model]
            if access_class.select_related:
                qs = qs.select_related(*access_class.select_related)
            if access_class.prefetch_related:
                qs = qs.prefetch_related(*access_class.prefetch_related)
        return qs
    else:
        return super(GenericAPIView, self).get_queryset()
",if access_class . prefetch_related :,171
"def map_package(shutit_pexpect_session, package, install_type):
    """"""If package mapping exists, then return it, else return package.""""""
    if package in PACKAGE_MAP.keys():
        for itype in PACKAGE_MAP[package].keys():
            if itype == install_type:
                ret = PACKAGE_MAP[package][install_type]
                if isinstance(ret, str):
                    return ret
                if callable(ret):
                    ret(shutit_pexpect_session)
                    return """"
    # Otherwise, simply return package
    return package
",if callable ( ret ) :,163
"def find_missing_cache_files(
    self, modules: Dict[str, str], manager: build.BuildManager
) -> Set[str]:
    ignore_errors = True
    missing = {}
    for id, path in modules.items():
        meta = build.find_cache_meta(id, path, manager)
        if not build.validate_meta(meta, id, path, ignore_errors, manager):
            missing[id] = path
    return set(missing.values())
","if not build . validate_meta ( meta , id , path , ignore_errors , manager ) :",117
"def parse_percent_formats(data, tree):
    percent_formats = data.setdefault(""percent_formats"", {})
    for elem in tree.findall("".//percentFormats/percentFormatLength""):
        type = elem.attrib.get(""type"")
        if _should_skip_elem(elem, type, percent_formats):
            continue
        pattern = text_type(elem.findtext(""percentFormat/pattern""))
        percent_formats[type] = numbers.parse_pattern(pattern)
","if _should_skip_elem ( elem , type , percent_formats ) :",116
"def nan2none(l):
    for idx, val in enumerate(l):
        if isinstance(val, Sequence):
            l[idx] = nan2none(l[idx])
        elif isnum(val) and math.isnan(val):
            l[idx] = None
    return l
","if isinstance ( val , Sequence ) :",76
"def process(self, message: Message, **kwargs: Any) -> None:
    for attribute in DENSE_FEATURIZABLE_ATTRIBUTES:
        if message.get(attribute):
            message.set(
                SPACY_DOCS[attribute], self.doc_for_text(message.get(attribute))
            )
",if message . get ( attribute ) :,81
"def accessSlice(self, node):
    self.visit(node.value)
    node.obj = self.getObj(node.value)
    self.access = _access.INPUT
    lower, upper = node.slice.lower, node.slice.upper
    if lower:
        self.visit(lower)
    if upper:
        self.visit(upper)
    if isinstance(node.obj, intbv):
        if self.kind == _kind.DECLARATION:
            self.require(lower, ""Expected leftmost index"")
            leftind = self.getVal(lower)
            if upper:
                rightind = self.getVal(upper)
            else:
                rightind = 0
            node.obj = node.obj[leftind:rightind]
",if upper :,198
"def forg(x, prec=3):
    if prec == 3:
        # for 3 decimals
        if (abs(x) >= 1e4) or (abs(x) < 1e-4):
            return ""%9.3g"" % x
        else:
            return ""%9.3f"" % x
    elif prec == 4:
        if (abs(x) >= 1e4) or (abs(x) < 1e-4):
            return ""%10.4g"" % x
        else:
            return ""%10.4f"" % x
    else:
        raise ValueError(
            ""`prec` argument must be either 3 or 4, not {prec}"".format(prec=prec)
        )
",if ( abs ( x ) >= 1e4 ) or ( abs ( x ) < 1e-4 ) :,185
"def pseudo_raw_input(self, prompt):
    """"""copied from cmd's cmdloop; like raw_input, but accounts for changed stdin, stdout""""""
    if self.use_rawinput:
        try:
            line = raw_input(prompt)
        except EOFError:
            line = ""EOF""
    else:
        self.stdout.write(prompt)
        self.stdout.flush()
        line = self.stdin.readline()
        if not len(line):
            line = ""EOF""
        else:
            if line[-1] == ""\n"":  # this was always true in Cmd
                line = line[:-1]
    return line
",if not len ( line ) :,172
"def _find_first_unescaped(dn, char, pos):
    while True:
        pos = dn.find(char, pos)
        if pos == -1:
            break  # no char found
        if pos > 0 and dn[pos - 1] != ""\\"":  # unescaped char
            break
        elif pos > 1 and dn[pos - 1] == ""\\"":  # may be unescaped
            escaped = True
            for c in dn[pos - 2 : 0 : -1]:
                if c == ""\\"":
                    escaped = not escaped
                else:
                    break
            if not escaped:
                break
        pos += 1
    return pos
","elif pos > 1 and dn [ pos - 1 ] == ""\\"" :",181
"def update_user(username):
    permission = UserAdminPermission(username)
    if permission.can():
        update_request = request.get_json()
        if ""password"" in update_request:
            logger.debug(""Updating user password"")
            model.user.change_password(
                get_authenticated_user(), update_request[""password""]
            )
        return jsonify(
            {
                ""username"": get_authenticated_user().username,
                ""email"": get_authenticated_user().email,
            }
        )
    abort(403)
","if ""password"" in update_request :",154
"def pages(self):
    if hasattr(self, ""_pages""):
        return self._pages
    doctop = 0
    pp = self.pages_to_parse
    self._pages = []
    for i, page in enumerate(PDFPage.create_pages(self.doc)):
        page_number = i + 1
        if pp is not None and page_number not in pp:
            continue
        p = Page(self, page, page_number=page_number, initial_doctop=doctop)
        self._pages.append(p)
        doctop += p.height
    return self._pages
",if pp is not None and page_number not in pp :,155
"def image_size(img_data, pure_python=False):
    try:
        if imgsize is not None:
            return imgsize.get_size(PeekableStringIO(img_data))
        if Image is not None and not pure_python:
            return Image.open(cStringIO.StringIO(img_data)).size
    except (ValueError, imgsize.UnknownSize):
        pass
    return None
",if imgsize is not None :,105
"def email_csv_query(request, query_id):
    if request.is_ajax():
        email = request.POST.get(""email"", None)
        if email:
            execute_query.delay(query_id, email)
            return HttpResponse(content={""message"": ""message was sent successfully""})
    return HttpResponse(status=403)
",if email :,86
"def _groups_args_split(self, kwargs):
    groups_args_split = []
    groups = kwargs[""groups""]
    for key, group in groups.iteritems():
        mykwargs = kwargs.copy()
        del mykwargs[""groups""]
        if ""group_name"" in group:
            mykwargs[""source_security_group_name""] = group[""group_name""]
        if ""user_id"" in group:
            mykwargs[""source_security_group_owner_id""] = group[""user_id""]
        if ""group_id"" in group:
            mykwargs[""source_security_group_id""] = group[""group_id""]
        groups_args_split.append(mykwargs)
    return groups_args_split
","if ""user_id"" in group :",186
"def get_subnet_groups(self, region: str, vpc: str):
    try:
        await self._cache_subnet_groups(region)
        return [
            subnet_group
            for subnet_group in self._subnet_groups_cache[region]
            if subnet_group[""VpcId""] == vpc
        ]
    except Exception as e:
        print_exception(f""Failed to get RDS subnet groups: {e}"")
        return []
","if subnet_group [ ""VpcId"" ] == vpc",121
"def on_state_update(self) -> None:
    if self.road:
        self.lane_index = self.road.network.get_closest_lane_index(self.position)
        self.lane = self.road.network.get_lane(self.lane_index)
        if self.road.record_history:
            self.history.appendleft(self.create_from(self))
",if self . road . record_history :,105
"def delete_old_post_save(
    sender, instance, raw, created, update_fields, using, **kwargs
):
    """"""Post_save on all models with file fields, deletes old files""""""
    if raw or created:
        return
    for field_name, new_file in cache.fields_for_model_instance(instance):
        if update_fields is None or field_name in update_fields:
            old_file = cache.get_field_attr(instance, field_name)
            if old_file != new_file:
                delete_file(instance, field_name, old_file, using)
    # reset cache
    cache.make_cleanup_cache(instance)
",if old_file != new_file :,171
"def i2h(self, pkt, x):
    if x is not None:
        if x < 0:
            warning(""Fixed3_6: Internal value too negative: %d"" % x)
            x = 0
        elif x > 999999999:
            warning(""Fixed3_6: Internal value too positive: %d"" % x)
            x = 999999999
        x = x * 1e-6
    return x
",elif x > 999999999 :,111
"def quick_main(self):
    if self.actions.pressed(""cancel""):
        self.previs_timer.stop()
        return ""main""
    if self.actions.mousemove_stop:
        self.hovering_edge, _ = self.rfcontext.accel_nearest2D_edge(
            max_dist=options[""action dist""]
        )
        if self.hovering_edge and not self.hovering_edge.is_valid:
            self.hovering_edge = None
    if self.hovering_edge and self.rfcontext.actions.pressed(""quick insert""):
        return self.insert_edge_loop_strip()
",if self . hovering_edge and not self . hovering_edge . is_valid :,163
"def check_status(self) -> None:
    join_requested = False
    while not join_requested:
        status_response = self._interface.communicate_status(check_stop_req=True)
        if status_response and status_response.run_should_stop:
            # TODO(frz): This check is required
            # until WB-3606 is resolved on server side.
            if not wandb.agents.pyagent.is_running():
                thread.interrupt_main()
                return
        join_requested = self._join_event.wait(self._polling_interval)
",if not wandb . agents . pyagent . is_running ( ) :,157
"def listed(output, pool):
    for line in output.splitlines():
        name, mountpoint, refquota = line.split(b""\t"")
        name = name[len(pool) + 1 :]
        if name:
            refquota = int(refquota.decode(""ascii""))
            if refquota == 0:
                refquota = None
            yield _DatasetInfo(dataset=name, mountpoint=mountpoint, refquota=refquota)
",if name :,116
"def defined_properties(cls, aliases=True, properties=True, rels=True):
    from .relationship_manager import RelationshipDefinition
    props = {}
    for baseclass in reversed(cls.__mro__):
        props.update(
            dict(
                (name, property)
                for name, property in vars(baseclass).items()
                if (aliases and isinstance(property, AliasProperty))
                or (
                    properties
                    and isinstance(property, Property)
                    and not isinstance(property, AliasProperty)
                )
                or (rels and isinstance(property, RelationshipDefinition))
            )
        )
    return props
","if ( aliases and isinstance ( property , AliasProperty ) )",182
"def _mock_manager(self, *args, **kwargs):
    if kwargs and ""normalize"" not in kwargs:
        device_params = kwargs[""device_params""]
        device_handler = make_device_handler(device_params)
        session = SSHSession(device_handler)
        return Manager(session, device_handler)
    if args:
        if args[0].tag == ""request-pfe-execute"":
            file_name = (args[0].findtext(""command"")).replace("" "", ""_"")
            return self._read_file(file_name + "".xml"")
        elif args[0].tag == ""command"":
            file_name = (args[0].text).replace("" "", ""_"")
            return self._read_file(file_name + "".xml"")
","elif args [ 0 ] . tag == ""command"" :",193
"def triger_check_network(self, fail=False, force=False):
    time_now = time.time()
    if not force:
        if self._checking_num > 0:
            return
        if fail or self.network_stat != ""OK"":
            # Fail or unknown
            if time_now - self.last_check_time < 3:
                return
        else:
            if time_now - self.last_check_time < 10:
                return
    self.last_check_time = time_now
    threading.Thread(target=self._simple_check_worker).start()
",if time_now - self . last_check_time < 10 :,161
"def delete(self):
    if not self.force and not self.exists():
        return []
    cmd = [""delete""]
    if self.filename:
        cmd.append(""--filename="" + self.filename)
    else:
        if not self.resource:
            self.module.fail_json(msg=""resource required to delete without filename"")
        cmd.append(self.resource)
        if self.name:
            cmd.append(self.name)
        if self.label:
            cmd.append(""--selector="" + self.label)
        if self.all:
            cmd.append(""--all"")
        if self.force:
            cmd.append(""--ignore-not-found"")
    return self._execute(cmd)
",if self . all :,189
"def load(self):
    """"""load a custom filter""""""
    try:
        if os.path.isfile(self.file):
            parser = make_parser()
            parser.setContentHandler(FilterParser(self))
            with open(self.file, ""r"", encoding=""utf8"") as the_file:
                parser.parse(the_file)
    except (IOError, OSError):
        print(""IO/OSError in _filterlist.py"")
    except SAXParseException:
        print(""Parser error"")
",if os . path . isfile ( self . file ) :,132
"def exitFullscreen(self, container=None):
    """"""turns off fullscreen mode for the specified window""""""
    if container is None or isinstance(container, UNIVERSAL_STRING):
        try:
            container = self.widgetManager.get(WIDGET_NAMES.SubWindow, container)
        except:
            container = self._getTopLevel()
    if container.isFullscreen:
        container.isFullscreen = False
        container.attributes(""-fullscreen"", False)
        if container.escapeBindId is not None:
            container.unbind(""<Escape>"", container.escapeBindId)
        with PauseLogger():
            self._doTitleBar()
        return True
    else:
        return False
",if container . escapeBindId is not None :,179
"def __get__(self, instance: Any, owner: Type) -> Any:
    # class attribute accessed
    if instance is None:
        return self
    field = self.field
    instance_dict = instance.__dict__
    to_python = self._to_python
    value = instance_dict[field]
    if self.lazy_coercion and to_python is not None:
        evaluated_fields: Set[str]
        evaluated_fields = instance.__evaluated_fields__
        if field not in evaluated_fields:
            if value is not None or self.required:
                value = instance_dict[field] = to_python(value)
            evaluated_fields.add(field)
    return value
",if field not in evaluated_fields :,178
"def ip_list(_):
    ips = []
    for ip in _.split("" ""):
        if not ip:
            continue
        elif isip(ip):
            ips.append(IP.create(ip))
        else:
            raise TypeError(""ip %s is invalid"" % ip)
    return ips
",elif isip ( ip ) :,82
"def _parse_fields(line, legacy=False):
    """"""Removes '\n' from fields line and returns fields as a list (columns).""""""
    line = line.rstrip(""\n"")
    if legacy:
        fields = line.split("","")
    else:
        line = line.split(""# Fields: "")[1]
        fields = line.split("", "")
    columns = []
    for field in fields:
        if field not in column_converter:
            raise BLAST7FormatError(
                ""Unrecognized field (%r).""
                "" Supported fields: %r"" % (field, set(column_converter.keys()))
            )
        columns.append(column_converter[field])
    return columns
",if field not in column_converter :,175
"def _resolve_plugin_path(path):
    if not os.path.isabs(path):
        p = os.path.normpath(os.path.join(sublime.packages_path(), ""User"", path))
        if not os.path.exists(p):
            p = os.path.normpath(
                os.path.join(sublime.packages_path(), ""LaTeXTools"", path)
            )
        return p
    return path
",if not os . path . exists ( p ) :,120
"def _deep_copy_dict(source, dest):
    for key, value in source.items():
        if isinstance(value, Entity):
            dest[key] = {}
            TqApi._deep_copy_dict(value, dest[key])
        else:
            dest[key] = value
","if isinstance ( value , Entity ) :",79
"def encode(self):
    if not isinstance(self.expr, m2_expr.ExprInt):
        return False
    if not test_set_sf(self.parent, self.expr.size):
        return False
    value = int(self.expr)
    if value < 1 << self.l:
        self.parent.shift.value = 0
    else:
        if value & 0xFFF:
            return False
        value >>= 12
        if value >= 1 << self.l:
            return False
        self.parent.shift.value = 1
    self.value = value
    return True
",if value & 0xFFF :,154
"def test_read_audio_properties(self):
    mediafile = self._mediafile_fixture(""full"")
    for key, value in self.audio_properties.items():
        if isinstance(value, float):
            self.assertAlmostEqual(getattr(mediafile, key), value, delta=0.1)
        else:
            self.assertEqual(getattr(mediafile, key), value)
","if isinstance ( value , float ) :",92
"def get_all_fix_names(fixer_pkg, remove_prefix=True):
    """"""Return a sorted list of all available fix names in the given package.""""""
    pkg = __import__(fixer_pkg, [], [], [""*""])
    fixer_dir = os.path.dirname(pkg.__file__)
    fix_names = []
    for name in sorted(os.listdir(fixer_dir)):
        if name.startswith(""fix_"") and name.endswith("".py""):
            if remove_prefix:
                name = name[4:]
            fix_names.append(name[:-3])
    return fix_names
","if name . startswith ( ""fix_"" ) and name . endswith ( "".py"" ) :",147
"def _get_arg(self, f_name, args, kws, arg_no, arg_name, default=None, err_msg=None):
    arg = None
    if len(args) > arg_no:
        arg = args[arg_no]
    elif arg_name in kws:
        arg = kws[arg_name]
    if arg is None:
        if default is not None:
            return default
        if err_msg is None:
            err_msg = ""{} requires '{}' argument"".format(f_name, arg_name)
        raise ValueError(err_msg)
    return arg
",if err_msg is None :,152
"def get_satellite_list(self, daemon_type=""""):
    res = {}
    for t in [""arbiter"", ""scheduler"", ""poller"", ""reactionner"", ""receiver"", ""broker""]:
        if daemon_type and daemon_type != t:
            continue
        satellite_list = []
        res[t] = satellite_list
        daemon_name_attr = t + ""_name""
        daemons = self.app.get_daemons(t)
        for dae in daemons:
            if hasattr(dae, daemon_name_attr):
                satellite_list.append(getattr(dae, daemon_name_attr))
    return res
",if daemon_type and daemon_type != t :,175
"def do_upload(file: Path, metadata: Metadata, repo_name=None):
    """"""Upload a file to an index server.""""""
    repo = get_repository(repo_name)
    upload_file(file, metadata, repo)
    if repo[""is_warehouse""]:
        domain = urlparse(repo[""url""]).netloc
        if domain.startswith(""upload.""):
            domain = domain[7:]
        log.info(""Package is at https://%s/project/%s/"", domain, metadata.name)
    else:
        log.info(""Package is at %s/%s"", repo[""url""], metadata.name)
","if domain . startswith ( ""upload."" ) :",147
"def __next__(self):
    for res in self._execution_context:
        for item in res:
            for operator in self._local_aggregators:
                if isinstance(item, dict) and item:
                    operator.aggregate(item[""item""])
                elif isinstance(item, numbers.Number):
                    operator.aggregate(item)
    if self._results is None:
        self._results = []
        for operator in self._local_aggregators:
            self._results.append(operator.get_result())
    if self._result_index < len(self._results):
        res = self._results[self._result_index]
        self._result_index += 1
        return res
    raise StopIteration
","elif isinstance ( item , numbers . Number ) :",188
"def __iter__(self):
    yield pd.Timestamp.utcnow(), SESSION_START
    while True:
        current_time = pd.Timestamp.utcnow()
        current_minute = current_time.floor(""1 min"")
        if self.end is not None and current_minute >= self.end:
            break
        if self._last_emit is None or current_minute > self._last_emit:
            log.debug(""emitting minutely bar: {}"".format(current_minute))
            self._last_emit = current_minute
            yield current_minute, BAR
        else:
            sleep(1)
    yield current_minute, SESSION_END
",if self . end is not None and current_minute >= self . end :,168
"def _escape_attrib(text):
    # escape attribute value
    try:
        if ""&"" in text:
            text = text.replace(""&"", ""&amp;"")
        if ""<"" in text:
            text = text.replace(""<"", ""&lt;"")
        if "">"" in text:
            text = text.replace("">"", ""&gt;"")
        if '""' in text:
            text = text.replace('""', ""&quot;"")
        if ""\n"" in text:
            text = text.replace(""\n"", ""&#10;"")
        return text
    except (TypeError, AttributeError):  # pragma: no cover
        _raise_serialization_error(text)
","if ""&"" in text :",160
"def _read_row_from_packet(self, packet):
    row = []
    for encoding, converter in self.converters:
        try:
            data = packet.read_length_coded_string()
        except IndexError:
            # No more columns in this row
            # See https://github.com/PyMySQL/PyMySQL/pull/434
            break
        if data is not None:
            if encoding is not None:
                data = data.decode(encoding)
            if DEBUG:
                print(""DEBUG: DATA = "", data)
            if converter is not None:
                data = converter(data)
        row.append(data)
    return tuple(row)
",if converter is not None :,186
"def dumpMenuTree(self, aList, level=0, path=""""):
    for z in aList:
        kind, val, val2 = z
        if kind == ""@item"":
            name = self.getName(val, val2)
            g.es_print(
                ""%s %s (%s) [%s]"" % (""    "" * (level + 0), val, val2, path + ""/"" + name)
            )
        else:
            name = self.getName(kind.replace(""@menu "", """"))
            g.es_print(""%s %s... [%s]"" % (""    "" * (level), kind, path + ""/"" + name))
            self.dumpMenuTree(val, level + 1, path=path + ""/"" + name)
","if kind == ""@item"" :",196
"def startElement(self, name, attrs, connection):
    if name == ""SecurityGroups"":
        return self.security_groups
    elif name == ""ClassicLinkVPCSecurityGroups"":
        return self.classic_link_vpc_security_groups
    elif name == ""BlockDeviceMappings"":
        if self.use_block_device_types:
            self.block_device_mappings = BDM()
        else:
            self.block_device_mappings = ResultSet([(""member"", BlockDeviceMapping)])
        return self.block_device_mappings
    elif name == ""InstanceMonitoring"":
        self.instance_monitoring = InstanceMonitoring(self)
        return self.instance_monitoring
",if self . use_block_device_types :,171
"def __get_dev_and_disk(topology):
    rv = []
    for values in topology.values():
        values = values.copy()
        while values:
            value = values.pop()
            if value[""type""] == ""DISK"":
                rv.append((value[""path""].replace(""/dev/"", """"), value[""disk""]))
            values += value.get(""children"") or []
    return rv
","if value [ ""type"" ] == ""DISK"" :",107
"def _process_events(self, event_list):
    for key, mask in event_list:
        fileobj, (reader, writer) = key.fileobj, key.data
        if mask & selectors.EVENT_READ and reader is not None:
            if reader._cancelled:
                self.remove_reader(fileobj)
            else:
                self._add_callback(reader)
        if mask & selectors.EVENT_WRITE and writer is not None:
            if writer._cancelled:
                self.remove_writer(fileobj)
            else:
                self._add_callback(writer)
",if reader . _cancelled :,158
"def colourLabels(self):
    if self.showAttr and self.hasAttr:
        self.canvas.itemconfigure(self.attrId, fill=self.fgColour)
    try:
        if not self.selected:
            self.label.config(background=self.bgColour, fg=self.fgColour)
        else:
            self.label.config(background=self.bgHColour, fg=self.fgHColour)
    except:
        pass
",if not self . selected :,122
"def validate_char_lengths(self):
    for field in self._meta.get_fields():
        if not field.is_relation and field.get_internal_type() == ""CharField"":
            if (
                isinstance(getattr(self, field.name), basestring)
                and len(getattr(self, field.name)) > field.max_length
            ):
                raise Exception(
                    ""Role %s value exceeeds max length of %s.""
                    % (field.name, field.max_length)
                )
","if not field . is_relation and field . get_internal_type ( ) == ""CharField"" :",149
"def _render_lang_List(self, element):
    with self.buffer.foldable_lines():
        self.buffer.write(""["", style=self.styles.bracket)
        item_count = len(element.items)
        if item_count:
            with self.buffer.indent():
                for idx, item in enumerate(element.items):
                    self._render(item)
                    if idx < (item_count - 1):
                        self.buffer.write("","")
                        self.buffer.mark_line_break()
        if element.trimmed:
            self.buffer.write(""..."")
        self.buffer.write(""]"", style=self.styles.bracket)
",if item_count :,183
"def do_dialog():
    """"""Post dialog and handle user interaction until quit""""""
    my_dlg = Dlg.GetNewDialog(ID_MAIN, -1)
    while 1:
        n = Dlg.ModalDialog(None)
        if n == ITEM_LOOKUP_BUTTON:
            tp, h, rect = my_dlg.GetDialogItem(ITEM_LOOKUP_ENTRY)
            txt = Dlg.GetDialogItemText(h)
            tp, h, rect = my_dlg.GetDialogItem(ITEM_RESULT)
            Dlg.SetDialogItemText(h, dnslookup(txt))
        elif n == ITEM_QUIT_BUTTON:
            break
",elif n == ITEM_QUIT_BUTTON :,173
"def _extract_more_comments(tree):
    """"""Return a list of MoreComments objects removed from tree.""""""
    more_comments = []
    queue = [(None, x) for x in tree]
    while len(queue) > 0:
        parent, comm = queue.pop(0)
        if isinstance(comm, MoreComments):
            heappush(more_comments, comm)
            if parent:
                parent.replies.remove(comm)
            else:
                tree.remove(comm)
        else:
            for item in comm.replies:
                queue.append((comm, item))
    return more_comments
","if isinstance ( comm , MoreComments ) :",171
"def run(self):
    while True:
        self.finished.wait(self.interval)
        if self.finished.isSet():
            return
        try:
            self.function(*self.args, **self.kwargs)
        except Exception:
            if self.bus:
                self.bus.log(
                    ""Error in perpetual timer thread function %r."" % self.function,
                    level=40,
                    traceback=True,
                )
            # Quit on first error to avoid massive logs.
            raise
",if self . finished . isSet ( ) :,157
"def emit_classattribs(self, typebld):
    if hasattr(self, ""_clrclassattribs""):
        for attrib_info in self._clrclassattribs:
            if isinstance(attrib_info, type):
                ci = clr.GetClrType(attrib_info).GetConstructor(())
                cab = CustomAttributeBuilder(ci, ())
            elif isinstance(attrib_info, CustomAttributeDecorator):
                cab = attrib_info.GetBuilder()
            else:
                make_decorator = attrib_info()
                cab = make_decorator.GetBuilder()
            typebld.SetCustomAttribute(cab)
","if isinstance ( attrib_info , type ) :",166
"def wrapper(fn):
    if debug_run_test_calls:
        ret = str(fn(*args, *kwargs))
        print(""TEST: %s()"" % fn.__name__)
        if args:
            print(""  arg:"", args)
        if kwargs:
            print(""  kwa:"", kwargs)
        print(""  ret:"", ret)
    return fn
",if kwargs :,95
"def _prune(self):
    with self.lock:
        entries = self._list_dir()
        if len(entries) > self._threshold:
            now = time.time()
            try:
                for i, fpath in enumerate(entries):
                    remove = False
                    f = LockedFile(fpath, ""rb"")
                    exp = pickle.load(f.file)
                    f.close()
                    remove = exp <= now or i % 3 == 0
                    if remove:
                        self._del_file(fpath)
            except Exception:
                pass
",if len ( entries ) > self . _threshold :,173
"def delete_if_forked(ghrequest):
    FORKED = False
    query = ""/user/repos""
    r = utils.query_request(query)
    for repo in r.json():
        if repo[""description""]:
            if ghrequest.target_repo_fullname in repo[""description""]:
                FORKED = True
                url = f""/repos/{repo['full_name']}""
                utils.query_request(url, method=""DELETE"")
    return FORKED
","if repo [ ""description"" ] :",127
"def _feed_data_to_buffered_proto(proto, data):
    data_len = len(data)
    while data_len:
        buf = proto.get_buffer(data_len)
        buf_len = len(buf)
        if not buf_len:
            raise RuntimeError(""get_buffer() returned an empty buffer"")
        if buf_len >= data_len:
            buf[:data_len] = data
            proto.buffer_updated(data_len)
            return
        else:
            buf[:buf_len] = data[:buf_len]
            proto.buffer_updated(buf_len)
            data = data[buf_len:]
            data_len = len(data)
",if not buf_len :,189
"def _plugin_get_requirements(self, requirements_iter):
    plugin_requirements = {""platform"": [], ""python"": [], ""network"": [], ""native"": []}
    # parse requirements
    for requirement in requirements_iter:
        key = requirement[0]
        values = requirement[1]
        if isinstance(values, str) or isinstance(values, bool):
            values = [values]
        if key in plugin_requirements:
            plugin_requirements[key].extend(values)
        else:
            warning(""{}={}: No supported requirement"".format(key, values))
    return plugin_requirements
",if key in plugin_requirements :,148
"def setCurrentModelIndexes(self, indexes):
    self._indexes = []
    self._index = None
    for i in indexes:
        if i.isValid():
            if i.column() != self._column:
                i = i.sibling(i.row(), self._column)
            self._indexes.append(i)
    self.updateItems()
    self.updateSelectedItem()
",if i . column ( ) != self . _column :,101
"def _publish(self, data):
    retry = True
    while True:
        try:
            if not retry:
                self._redis_connect()
            return self.redis.publish(self.channel, pickle.dumps(data))
        except redis.exceptions.ConnectionError:
            if retry:
                logger.error(""Cannot publish to redis... retrying"")
                retry = False
            else:
                logger.error(""Cannot publish to redis... giving up"")
                break
",if not retry :,134
"def write_pad_and_flush(self, data, pad="" ""):
    if self.encryptor and (data or self.encode_buffer):
        if self.encode_batches:
            remainder = len(self.encode_buffer) + len(data)
            remainder %= self.encode_batches
            padding = self.encode_batches - remainder
            data += pad * padding
    self.write(data)
    self.flush()
",if self . encode_batches :,110
"def dump_metrics(self):
    metrics = self._registry.dump_metrics()
    # Filter out min and max if there have been no samples.
    for metric in metrics.itervalues():
        if metric.get(""count"") == 0:
            if ""min"" in metric:
                metric[""min""] = 0.0
            if ""max"" in metric:
                metric[""max""] = 0.0
    return metrics
","if metric . get ( ""count"" ) == 0 :",109
"def demo():
    d = StatusProgressDialog(""A Demo"", ""Doing something..."")
    import win32api
    for i in range(100):
        if i == 50:
            d.SetText(""Getting there..."")
        if i == 90:
            d.SetText(""Nearly done..."")
        win32api.Sleep(20)
        d.Tick()
    d.Close()
",if i == 90 :,99
"def get_file_contents(app_name: str, app_version: str, file_path: str):
    full_path = f""{app_name}/{app_version}/{file_path}""
    success, contents = await MinioApi.get_file(app_name, app_version, file_path)
    if success:
        return contents
    else:
        if contents is None:
            raise DoesNotExistException(""read"", ""file"", full_path)
        else:
            raise InvalidInputException(
                ""read"", ""file"", full_path, errors={""error"": contents}
            )
",if contents is None :,153
"def _sashMark(self, event):
    self._sashIndex = -1
    try:
        self._sashIndex, which = self.paneframe.identify(event.x, event.y)
        if which == ""sash"":
            self._sashx = [
                self.paneframe.sash_coord(i)[0] for i in range(len(self._lists) - 1)
            ]
            self._sashdx = self._sashx[self._sashIndex] - event.x
            self._sashDrag(event)
        else:
            self._sashIndex = -1
    except:
        return
    return ""break""
","if which == ""sash"" :",181
"def emptyTree(self):
    for child in self:
        childObj = child.getObject()
        del childObj[NameObject(""/Parent"")]
        if NameObject(""/Next"") in childObj:
            del childObj[NameObject(""/Next"")]
        if NameObject(""/Prev"") in childObj:
            del childObj[NameObject(""/Prev"")]
    if NameObject(""/Count"") in self:
        del self[NameObject(""/Count"")]
    if NameObject(""/First"") in self:
        del self[NameObject(""/First"")]
    if NameObject(""/Last"") in self:
        del self[NameObject(""/Last"")]
","if NameObject ( ""/Next"" ) in childObj :",155
"def contractIfNotCurrent(c, p, leaveOpen):
    if p == leaveOpen or not p.isAncestorOf(leaveOpen):
        p.contract()
    for child in p.children():
        if child != leaveOpen and child.isAncestorOf(leaveOpen):
            contractIfNotCurrent(c, child, leaveOpen)
        else:
            for p2 in child.self_and_subtree():
                p2.contract()
",if child != leaveOpen and child . isAncestorOf ( leaveOpen ) :,119
"def test_cat(shape, cat_dim, split, dim):
    assert sum(split) == shape[cat_dim]
    gaussian = random_gaussian(shape, dim)
    parts = []
    end = 0
    for size in split:
        beg, end = end, end + size
        if cat_dim == -1:
            part = gaussian[..., beg:end]
        elif cat_dim == -2:
            part = gaussian[..., beg:end, :]
        elif cat_dim == 1:
            part = gaussian[:, beg:end]
        else:
            raise ValueError
        parts.append(part)
    actual = Gaussian.cat(parts, cat_dim)
    assert_close_gaussian(actual, gaussian)
",elif cat_dim == 1 :,186
"def _remove_timeout(self, key):
    if key in self.waiting:
        request, callback, timeout_handle = self.waiting[key]
        if timeout_handle is not None:
            self.io_loop.remove_timeout(timeout_handle)
        del self.waiting[key]
",if timeout_handle is not None :,79
"def gyro(self, mapper, *pyr):
    for i in (0, 1, 2):
        axis = self.axes[i]
        # 'gyro' cannot map to mouse, but 'mouse' does that.
        if axis in Axes or type(axis) == int:
            mapper.gamepad.axisEvent(
                axis, AxisAction.clamp_axis(axis, pyr[i] * self.speed[i] * -10)
            )
            mapper.syn_list.add(mapper.gamepad)
",if axis in Axes or type ( axis ) == int :,136
"def check_enums_ATLAS_MACHTYPE(lines):
    for i, mach_type in enumerate(ATLAS_MACHTYPE):
        got = lines.pop(0).strip()
        expect = ""{0} = '{1}'"".format(i, mach_type)
        if got != expect:
            raise RuntimeError(
                ""ATLAS_MACHTYPE mismatch at position ""
                + str(i)
                + "": got >>""
                + got
                + ""<<, expected >>""
                + expect
                + ""<<""
            )
",if got != expect :,154
"def readArgs(self, node):
    res = {}
    for c in self.getChildrenOf(node):
        val = c.getAttribute(""val"")
        if val in self.modules:
            res[str(c.nodeName)] = self.modules[val]
        elif val in self.mothers:
            res[str(c.nodeName)] = self.mothers[val]
        elif val != """":
            res[str(c.nodeName)] = eval(val)
    return res
",elif val in self . mothers :,130
"def submit_events(self, events):
    headers = {""Content-Type"": ""application/json""}
    event_chunk_size = self.event_chunk_size
    for chunk in chunks(events, event_chunk_size):
        payload = {
            ""apiKey"": self.api_key,
            ""events"": {""api"": chunk},
            ""uuid"": get_uuid(),
            ""internalHostname"": get_hostname(),
        }
        params = {}
        if self.api_key:
            params[""api_key""] = self.api_key
        url = ""%s/intake?%s"" % (self.api_host, urlencode(params))
        self.submit_http(url, json.dumps(payload), headers)
",if self . api_key :,188
"def rewrite_urls_mygpo(self):
    # Check if we have to rewrite URLs since the last add
    rewritten_urls = self.mygpo_client.get_rewritten_urls()
    changed = False
    for rewritten_url in rewritten_urls:
        if not rewritten_url.new_url:
            continue
        for channel in self.channels:
            if channel.url == rewritten_url.old_url:
                logger.info(""Updating URL of %s to %s"", channel, rewritten_url.new_url)
                channel.url = rewritten_url.new_url
                channel.save()
                changed = True
                break
    if changed:
        util.idle_add(self.update_episode_list_model)
",if not rewritten_url . new_url :,200
"def validate_hostname(hostname):
    if hostname is None or len(hostname) == 0:
        return False, ""Empty hostname or domain is not allowed""
    fields = hostname.split(""."")
    for field in fields:
        if not field:
            return False, ""Empty hostname or domain is not allowed""
        if field[0] == ""-"" or field[-1] == ""-"":
            return False, ""Hostname or domain should not start or end with '-'""
    machinename = fields[0]
    if len(machinename) > 64 or not machinename[0].isalpha():
        return False, ""Hostname should start with alpha char and <= 64 chars""
    return True, None
","if field [ 0 ] == ""-"" or field [ - 1 ] == ""-"" :",166
"def apply_to(cls, lexer):
    # Apply a font for all styles
    lexer.setFont(Font().load())
    for name, font in cls.__dict__.items():
        if not isinstance(font, Font):
            continue
        if hasattr(lexer, name):
            style_num = getattr(lexer, name)
            lexer.setColor(QColor(font.color), style_num)
            lexer.setEolFill(True, style_num)
            lexer.setPaper(QColor(font.paper), style_num)
            lexer.setFont(font.load(), style_num)
","if not isinstance ( font , Font ) :",158
"def dr_relation(self, C, trans, nullable):
    state, N = trans
    terms = []
    g = self.lr0_goto(C[state], N)
    for p in g:
        if p.lr_index < p.len - 1:
            a = p.prod[p.lr_index + 1]
            if a in self.grammar.Terminals:
                if a not in terms:
                    terms.append(a)
    # This extra bit is to handle the start state
    if state == 0 and N == self.grammar.Productions[0].prod[0]:
        terms.append(""$end"")
    return terms
",if a in self . grammar . Terminals :,167
"def process_module(name, module, parent):
    if parent:
        modules[parent][""items""].append(name)
        mg = module_groups.setdefault(name, [])
        mg.append(parent)
        if get_module_type(name) == ""py3status"":
            module["".group""] = parent
    # check module content
    for k, v in list(module.items()):
        if k.startswith(""on_click""):
            # on_click event
            process_onclick(k, v, name)
            # on_click should not be passed to the module via the config.
            del module[k]
        if isinstance(v, ModuleDefinition):
            # we are a container
            module[""items""] = []
    return module
","if get_module_type ( name ) == ""py3status"" :",198
"def GetQualifiedWsdlName(type):
    with _lazyLock:
        wsdlNSAndName = _wsdlNameMap.get(type)
        if wsdlNSAndName:
            return wsdlNSAndName
        else:
            if issubclass(type, list):
                ns = GetWsdlNamespace(type.Item._version)
                return (ns, ""ArrayOf"" + Capitalize(type.Item._wsdlName))
            else:
                ns = GetWsdlNamespace(type._version)
                return (ns, type._wsdlName)
",if wsdlNSAndName :,158
"def assert_tensors_equal(sess, t1, t2, n):
    """"""Compute tensors `n` times and ensure that they are equal.""""""
    for _ in range(n):
        v1, v2 = sess.run([t1, t2])
        if v1.shape != v2.shape:
            return False
        if not np.all(v1 == v2):
            return False
    return True
",if v1 . shape != v2 . shape :,107
"def _lxml_default_loader(href, parse, encoding=None, parser=None):
    if parse == ""xml"":
        data = etree.parse(href, parser).getroot()
    else:
        if ""://"" in href:
            f = urlopen(href)
        else:
            f = open(href, ""rb"")
        data = f.read()
        f.close()
        if not encoding:
            encoding = ""utf-8""
        data = data.decode(encoding)
    return data
",if not encoding :,133
"def range_f(begin, end, step):
    # like range, but works on non-integer too
    seq = []
    while True:
        if step > 0 and begin > end:
            break
        if step < 0 and begin < end:
            break
        seq.append(begin)
        begin = begin + step
    return seq
",if step > 0 and begin > end :,90
"def _get_seccomp_whitelist(self):
    whitelist = [False] * MAX_SYSCALL_NUMBER
    index = _SYSCALL_INDICIES[NATIVE_ABI]
    for i in range(SYSCALL_COUNT):
        # Ensure at least one syscall traps.
        # Otherwise, a simple assembly program could terminate without ever trapping.
        if i in (sys_exit, sys_exit_group):
            continue
        handler = self._security.get(i, DISALLOW)
        for call in translator[i][index]:
            if call is None:
                continue
            if isinstance(handler, int):
                whitelist[call] = handler == ALLOW
    return whitelist
","if i in ( sys_exit , sys_exit_group ) :",185
"def add_custom_versions(versions):
    """"""create custom versions strings""""""
    versions_dict = {}
    for tech, version in versions.items():
        # clean up ""-"" from version
        if ""-"" in version:
            version = version.split(""-"")[0]
        if version.startswith(""v""):
            version = version[1:]  # Remove the 'v' prefix
            versions_dict[tech + ""_numeric""] = version.split(""+"")[0]
            # ""3.3.0.33"" is what we have, we want ""3.3""
            versions_dict[tech + ""_short""] = ""{}.{}"".format(*version.split("".""))
    return versions_dict
","if version . startswith ( ""v"" ) :",167
"def detab(self, text):
    """"""Remove a tab from the front of each line of the given text.""""""
    newtext = []
    lines = text.split(""\n"")
    for line in lines:
        if line.startswith("" "" * self.tab_length):
            newtext.append(line[self.tab_length :])
        elif not line.strip():
            newtext.append("""")
        else:
            break
    return ""\n"".join(newtext), ""\n"".join(lines[len(newtext) :])
","if line . startswith ( "" "" * self . tab_length ) :",134
"def ignore_module(module):
    result = False
    for check in ignore_these:
        if ""/*"" in check:
            if check[:-1] in module:
                result = True
        else:
            if (os.getcwd() + ""/"" + check + "".py"") == module:
                result = True
    if result:
        print_warning(""Ignoring module: "" + module)
    return result
",if check [ : - 1 ] in module :,108
"def load_previous_values(self):
    ReportOptions.load_previous_values(self)
    # Pass the loaded values to the menu options so they will be displayed
    # properly.
    for optname in self.options_dict:
        menu_option = self.menu.get_option_by_name(optname)
        if menu_option:
            menu_option.set_value(self.options_dict[optname])
",if menu_option :,104
"def dequeue(self):
    with self.db(commit=True) as curs:
        curs.execute(
            ""select id, data from task where queue = ? ""
            ""order by priority desc, id limit 1"",
            (self.name,),
        )
        result = curs.fetchone()
        if result is not None:
            tid, data = result
            curs.execute(""delete from task where id = ?"", (tid,))
            if curs.rowcount == 1:
                return to_bytes(data)
",if result is not None :,138
"def _collect_sublayers_attr(self, attr):
    if attr not in [""trainable_weights"", ""nontrainable_weights""]:
        raise ValueError(
            ""Only support to collect some certain attributes of nested layers,""
            ""e.g. 'trainable_weights', 'nontrainable_weights', but got {}"".format(attr)
        )
    if self._layers is None:
        return []
    nested = []
    for layer in self._layers:
        value = getattr(layer, attr)
        if value is not None:
            nested.extend(value)
    return nested
",if value is not None :,146
"def DeleteTab(self, tab):
    tab_renderer = self.tabs[tab]
    was_selected = tab_renderer.GetSelected()
    self.tabs.remove(tab_renderer)
    if tab_renderer:
        del tab_renderer
    # determine our new selection
    if was_selected and self.GetTabsCount() > 0:
        if tab > self.GetTabsCount() - 1:
            self.tabs[self.GetTabsCount() - 1].SetSelected(True)
        else:
            self.tabs[tab].SetSelected(True)
    self.AdjustTabsSize()
    self.Refresh()
",if tab > self . GetTabsCount ( ) - 1 :,158
"def _show_warnings(self):
    if self._warnings_handled:
        return
    self._warnings_handled = True
    if self._result and (self._result.has_next or not self._result.warning_count):
        return
    ws = self._get_db().show_warnings()
    if ws is None:
        return
    for w in ws:
        msg = w[-1]
        if PY2:
            if isinstance(msg, unicode):
                msg = msg.encode(""utf-8"", ""replace"")
        warnings.warn(err.Warning(*w[1:3]), stacklevel=4)
",if PY2 :,158
"def fetch():
    retval = {}
    content = retrieve_content(__url__)
    if __check__ not in content:
        content = retrieve_content(__backup__)
    if __check__ in content:
        for line in content.split(""\n""):
            line = line.strip()
            if not line or line.startswith(""#"") or ""."" not in line:
                continue
            retval[line] = (__info__, __reference__)
    return retval
","if not line or line . startswith ( ""#"" ) or ""."" not in line :",114
"def findUserByAttr(self, identifier, attr_type, attr_data):
    for uid in self.users_info:
        attrs = self.users_info[uid]
        for attr in attrs:
            if attr_type == attr[""attr_type""] and attr_data == attr[""attr_data""]:
                return defer.succeed(uid)
    uid = self.nextId()
    self.db.insertTestData([User(uid=uid, identifier=identifier)])
    self.db.insertTestData(
        [UserInfo(uid=uid, attr_type=attr_type, attr_data=attr_data)]
    )
    return defer.succeed(uid)
","if attr_type == attr [ ""attr_type"" ] and attr_data == attr [ ""attr_data"" ] :",167
"def order_note_added_event(*, order: Order, user: UserType, message: str) -> OrderEvent:
    kwargs = {}
    if user is not None and not user.is_anonymous:
        if order.user is not None and order.user.pk == user.pk:
            account_events.customer_added_to_note_order_event(
                user=user, order=order, message=message
            )
        kwargs[""user""] = user
    return OrderEvent.objects.create(
        order=order,
        type=OrderEvents.NOTE_ADDED,
        parameters={""message"": message},
        **kwargs,
    )
",if order . user is not None and order . user . pk == user . pk :,161
"def __str__(self):
    if self.team:
        if self.down != 0:
            return ""(%s, %s, Q%d, %d and %d) %s"" % (
                self.team,
                self.data[""yrdln""],
                self.time.qtr,
                self.down,
                self.yards_togo,
                self.desc,
            )
        else:
            return ""(%s, %s, Q%d) %s"" % (
                self.team,
                self.data[""yrdln""],
                self.time.qtr,
                self.desc,
            )
    return self.desc
",if self . down != 0 :,199
"def write(self, stream):
    self.write1(stream)
    i = 0
    n = 0
    for name, offset, value, bsize in self.variables:
        stream.write(self.body[i:offset])
        if bsize == 4:
            write_uint(stream, value)
        elif bsize == 8:
            write_ulong(stream, value)
        else:
            raise NotImplementedError()
        n += offset - i + bsize
        i = offset + bsize
    stream.write(self.body[i:])
    n += len(self.body) - i
    assert n == len(self.body)
",if bsize == 4 :,162
"def __setattr__(self, attr, val):
    if hasattr(self, attr):
        old = getattr(self, attr)
        if isinstance(old, Setting):
            if isinstance(val, Setting):
                raise ValueError(
                    ""Attempting to reassign setting %s with %s"" % (old, val)
                )
            log.warn(""Setting attr %s via __setattr__ instead of set()!"", attr)
            return old.set(val)
    log.debug(""Setting {%s => %s}"" % (attr, val))
    return object.__setattr__(self, attr, val)
","if isinstance ( old , Setting ) :",155
"def setup_release_cwd_hook(prompter, history, completer, bindings, **kw):
    if ON_WINDOWS and not ON_CYGWIN and not ON_MSYS:
        prompter.prompt = _cwd_release_wrapper(prompter.prompt)
        if completer.completer:
            # Temporarily restore cwd for callbacks to the completer
            completer.completer.complete = _cwd_restore_wrapper(
                completer.completer.complete
            )
",if completer . completer :,112
"def nested_update(org_dict, upd_dict):
    for key, value in upd_dict.items():
        if isinstance(value, dict):
            if key in org_dict:
                if not isinstance(org_dict[key], dict):
                    raise ValueError(
                        ""Mismatch between org_dict and upd_dict at node {}"".format(key)
                    )
                nested_update(org_dict[key], value)
            else:
                org_dict[key] = value
        else:
            org_dict[key] = value
","if isinstance ( value , dict ) :",161
"def get_field_by_name(obj, field):
    # Dereference once
    if obj.type.code == gdb.TYPE_CODE_PTR:
        obj = obj.dereference()
    for f in re.split(""(->|\.|\[\d+\])"", field):
        if not f:
            continue
        if f == ""->"":
            obj = obj.dereference()
        elif f == ""."":
            pass
        elif f.startswith(""[""):
            n = int(f.strip(""[]""))
            obj = obj.cast(obj.dereference().type.pointer())
            obj += n
            obj = obj.dereference()
        else:
            obj = obj[f]
    return obj
","elif f . startswith ( ""["" ) :",189
"def check_sum(self, x, gpu=False):
    total = 0
    for i in range(5):
        t = numpy.array([i], dtype=numpy.int32)
        if gpu:
            t = cuda.to_gpu(t)
        loss = self.link(chainer.Variable(x), chainer.Variable(t)).data
        self.assertEqual(loss.dtype, self.dtype)
        self.assertEqual(loss.shape, ())
        total += numpy.exp(-cuda.to_cpu(loss))
    self.assertAlmostEqual(1.0, float(total), **self.check_sum_options)
",if gpu :,157
"def find_node_by_link(node_group, to_node, inp):
    for link in node_group.links:
        if link.to_node == to_node and link.to_socket == inp:
            if link.from_node.bl_idname == ""NodeReroute"":  # Step through reroutes
                return find_node_by_link(
                    node_group, link.from_node, link.from_node.inputs[0]
                )
            return link.from_node
","if link . from_node . bl_idname == ""NodeReroute"" :",137
"def _gen_opnds(ii):  # generator
    # filter out write-mask operands and suppressed operands
    for op in ii.parsed_operands:
        if op.lookupfn_name in [""MASK1"", ""MASKNOT0""]:
            continue
        if op.visibility == ""SUPPRESSED"":
            continue
        if op.name == ""BCAST"":
            continue
        yield op
","if op . lookupfn_name in [ ""MASK1"" , ""MASKNOT0"" ] :",104
"def contains_trained_model(self):
    if not hasattr(self, ""_contains_trained_model""):
        for f in self._files:
            if "".pt"" in f:
                self._contains_trained_model = True
                self._model_name = f
                return self._contains_trained_model
        self._contains_trained_model = False
        return self._contains_trained_model
    else:
        return self._contains_trained_model
","if "".pt"" in f :",123
"def _call(self, name, *args, **kwargs):
    data = self._get_data(name, *args, **kwargs)
    is_ascii = self._encoding == ""ascii""
    body = json.dumps(data, ensure_ascii=is_ascii).encode(self._encoding)
    resp = await self._http.post(self._url, data=body)
    if self._full_response:
        return resp
    else:
        content = resp.json()
        if resp.is_error:
            if ""error"" not in content:
                resp.raise_for_status()
        return self.loads(content)
","if ""error"" not in content :",161
"def get_classif_name(classifier_config, usepytorch):
    if not usepytorch:
        modelname = ""sklearn-LogReg""
    else:
        nhid = classifier_config[""nhid""]
        optim = (
            ""adam"" if ""optim"" not in classifier_config else classifier_config[""optim""]
        )
        bs = (
            64
            if ""batch_size"" not in classifier_config
            else classifier_config[""batch_size""]
        )
        modelname = ""pytorch-MLP-nhid%s-%s-bs%s"" % (nhid, optim, bs)
    return modelname
","if ""batch_size"" not in classifier_config",165
"def on_fill(self, order: Order, exchange: ""Exchange"", trade: ""Trade""):
    if trade.order_id in self._executed and trade not in self._trades:
        self._trades[trade.order_id] = self._trades.get(trade.order_id, [])
        self._trades[trade.order_id] += [trade]
        if order.is_complete():
            next_order = order.complete(exchange)
            if next_order:
                self.submit(next_order)
",if next_order :,143
"def _create_examples(cls, lines, set_type):
    examples = []
    for (i, line) in enumerate(lines):
        # Skip the header (first line)
        if i == 0:
            continue
        segments = line.strip().split(""\t"")
        idx, text_a, text_b, label = segments
        examples.append(
            Example(
                guid=""%s-%s"" % (set_type, idx),
                text_a=text_a,
                text_b=text_b,
                label=label,
            )
        )
    return examples
",if i == 0 :,166
"def split_path_info(path):
    # suitable for splitting an already-unquoted-already-decoded (unicode)
    # path value
    path = path.strip(""/"")
    clean = []
    for segment in path.split(""/""):
        if not segment or segment == ""."":
            continue
        elif segment == "".."":
            if clean:
                del clean[-1]
        else:
            clean.append(segment)
    return tuple(clean)
",if clean :,115
"def _mock_manager(self, *args, **kwargs):
    if kwargs and ""normalize"" not in kwargs:
        device_params = kwargs[""device_params""]
        device_handler = make_device_handler(device_params)
        session = SSHSession(device_handler)
        return Manager(session, device_handler)
    if args:
        if args[0].tag == ""request-pfe-execute"":
            file_name = (args[0].findtext(""command"")).replace("" "", ""_"")
            return self._read_file(file_name + "".xml"")
        elif args[0].tag == ""command"":
            file_name = (args[0].text).replace("" "", ""_"")
            return self._read_file(file_name + "".xml"")
","if args [ 0 ] . tag == ""request-pfe-execute"" :",193
"def update_loan_status(self, cancel=0):
    if cancel:
        loan_status = frappe.get_value(""Loan"", self.loan, ""status"")
        if loan_status == ""Closed"":
            frappe.db.set_value(""Loan"", self.loan, ""status"", ""Loan Closure Requested"")
    else:
        pledged_qty = 0
        current_pledges = get_pledged_security_qty(self.loan)
        for security, qty in iteritems(current_pledges):
            pledged_qty += qty
        if not pledged_qty:
            frappe.db.set_value(""Loan"", self.loan, ""status"", ""Closed"")
","if loan_status == ""Closed"" :",198
"def _wrapped_view(request, *args, **kwargs):
    if flag_name.startswith(""!""):
        active = not flag_is_active(request, flag_name[1:])
    else:
        active = flag_is_active(request, flag_name)
    if not active:
        response_to_redirect_to = get_response_to_redirect(redirect_to, *args, **kwargs)
        if response_to_redirect_to:
            return response_to_redirect_to
        else:
            raise Http404
    return view(request, *args, **kwargs)
",if response_to_redirect_to :,149
"def process_stroke_filter(stroke, min_distance=1.0, max_distance=2.0):
    """"""filter stroke to pts that are at least min_distance apart""""""
    nstroke = stroke[:1]
    for p in stroke[1:]:
        v = p - nstroke[-1]
        l = v.length
        if l < min_distance:
            continue
        d = v / l
        while l > 0:
            q = nstroke[-1] + d * min(l, max_distance)
            nstroke.append(q)
            l -= max_distance
    return nstroke
",if l < min_distance :,158
"def _fix_break_node(self, node: Node):
    end_node = self._find_end_loop(node, [], 0)
    if not end_node:
        # If there is not end condition on the loop
        # The exploration will reach a STARTLOOP before reaching the endloop
        # We start with -1 as counter to catch this corner case
        end_node = self._find_end_loop(node, [], -1)
        if not end_node:
            raise ParsingError(""Break in no-loop context {}"".format(node.function))
    for son in node.sons:
        son.remove_father(node)
    node.set_sons([end_node])
    end_node.add_father(node)
",if not end_node :,187
"def _Append(cls, session, word, mail_ids, compact=True):
    super(GlobalPostingList, cls)._Append(session, word, mail_ids, compact=compact)
    with GLOBAL_GPL_LOCK:
        global GLOBAL_GPL
        sig = cls.WordSig(word, session.config)
        if GLOBAL_GPL is None:
            GLOBAL_GPL = {}
        if sig not in GLOBAL_GPL:
            GLOBAL_GPL[sig] = set()
        for mail_id in mail_ids:
            GLOBAL_GPL[sig].add(mail_id)
",if GLOBAL_GPL is None :,154
"def __saveComment(self):
    """"""Saves the new or selected comment""""""
    if self.__btnSave.text() == SAVE_NEW:
        # If saving a new comment
        self.__addComment(self.__textSubject.text(), self.__textMessage.toPlainText())
        self.refreshComments()
    else:
        # If saving a modified comment
        if self.__treeSubjects.currentItem():
            comment = self.__treeSubjects.currentItem().getInstance()
            comment.setSubject(str(self.__textSubject.text()))
            comment.setMessage(str(self.__textMessage.toPlainText()))
            self.__treeSubjects.currentItem().getInstance().save()
            self.refreshComments()
",if self . __treeSubjects . currentItem ( ) :,183
"def verify_random_objects():
    resources = [Node, Registration, QuickFilesNode]
    for resource in resources:
        for i in range(1, 10):
            random_resource = _get_random_object(resource)
            if random_resource:
                _verify_contributor_perms(random_resource)
",if random_resource :,85
"def apply_gradient_modifiers(self):
    for layer_name, views in self.gradient_modifiers.items():
        for view_name, gradient_mods in views.items():
            for gm in gradient_mods:
                gm.rnd.set_seed(self.rnd.generate_seed())
                if isinstance(gm, GradientModifier):
                    gm(
                        self.handler,
                        self.buffer[layer_name].parameters[view_name],
                        self.buffer[layer_name].gradients[view_name],
                    )
                else:
                    gm(self.handler, self.buffer[layer_name].gradients[view_name])
","if isinstance ( gm , GradientModifier ) :",195
"def _split_auth_string(auth_string):
    """"""split a digest auth string into individual key=value strings""""""
    prev = None
    for item in auth_string.split("",""):
        try:
            if prev.count('""') == 1:
                prev = ""%s,%s"" % (prev, item)
                continue
        except AttributeError:
            if prev == None:
                prev = item
                continue
            else:
                raise StopIteration
        yield prev.strip()
        prev = item
    yield prev.strip()
    raise StopIteration
","if prev . count ( '""' ) == 1 :",152
"def checkUnchangedIvars(obj, d, exceptions=None):
    if not exceptions:
        exceptions = []
    ok = True
    for key in d:
        if key not in exceptions:
            if getattr(obj, key) != d.get(key):
                g.trace(
                    ""changed ivar: %s old: %s new: %s""
                    % (key, repr(d.get(key)), repr(getattr(obj, key)))
                )
                ok = False
    return ok
",if key not in exceptions :,142
"def checkChildren(item):
    for c in item.children():
        _id = c.data(Outline.ID.value)
        if not _id or _id == ""0"":
            c.getUniqueID()
        checkChildren(c)
","if not _id or _id == ""0"" :",65
"def main():
    if len(sys.argv) > 1:
        g = globals().copy()
        r = g[""test_"" + sys.argv[1]]()
        if r is not None:
            for func_and_args in r:
                func, args = func_and_args[0], func_and_args[1:]
                func(*args)
    else:
        run_all()
",if r is not None :,109
"def _create_entities(
    parsed_entities: Dict[Text, Union[Text, List[Text]]], sidx: int, eidx: int
) -> List[Dict[Text, Any]]:
    entities = []
    for k, vs in parsed_entities.items():
        if not isinstance(vs, list):
            vs = [vs]
        for value in vs:
            entities.append(
                {
                    ""entity"": k,
                    ""start"": sidx,
                    ""end"": eidx,  # can't be more specific
                    ""value"": value,
                }
            )
    return entities
","if not isinstance ( vs , list ) :",172
"def _group_stacks(stacks: Stacks) -> List[dict]:
    stacks_by_client: dict = {}
    for stack in stacks:
        client = stack.client
        if client not in stacks_by_client:
            stacks_by_client[client] = {""Client"": client, ""Stacks"": []}
        stacks_by_client[client][""Stacks""].append(stack)
    return [stacks_by_client[r] for r in stacks_by_client]
",if client not in stacks_by_client :,116
"def append(self, labels):
    if isinstance(labels, list):
        for label in labels:
            if not label in self.__menuLabels:
                self.__menuLabels.append(label)
                self.__enabledLabels.append(label)
    else:
        if not labels in self.__menuLabels:
            self.__menuLabels.append(labels)
            self.__enabledLabels.append(labels)
",if not labels in self . __menuLabels :,108
"def _json_to_flat_metrics(self, prefix, data):
    for key, value in data.items():
        if isinstance(value, dict):
            for k, v in self._json_to_flat_metrics(""%s.%s"" % (prefix, key), value):
                yield k, v
        else:
            try:
                int(value)
            except ValueError:
                value = None
            finally:
                yield (""%s.%s"" % (prefix, key), value)
","if isinstance ( value , dict ) :",138
"def _rename(src, dst):
    src = to_unicode(src, sys.getfilesystemencoding())
    dst = to_unicode(dst, sys.getfilesystemencoding())
    if _rename_atomic(src, dst):
        return True
    retry = 0
    rv = False
    while not rv and retry < 100:
        rv = _MoveFileEx(src, dst, _MOVEFILE_REPLACE_EXISTING | _MOVEFILE_WRITE_THROUGH)
        if not rv:
            time.sleep(0.001)
            retry += 1
    return rv
",if not rv :,135
"def expect_stream_start(self):
    if isinstance(self.event, StreamStartEvent):
        if self.event.encoding and not getattr(self.stream, ""encoding"", None):
            self.encoding = self.event.encoding
        self.write_stream_start()
        self.state = self.expect_first_document_start
    else:
        raise EmitterError(""expected StreamStartEvent, but got %s"" % self.event)
","if self . event . encoding and not getattr ( self . stream , ""encoding"" , None ) :",111
"def _doWait(self):
    doit = True
    while doit:
        # A wrapper method for wait() and the wait thread to use
        self.setMeta(""SignalInfo"", None)
        self.setMeta(""PendingSignal"", None)
        event = self.platformWait()
        self.running = False
        self.platformProcessEvent(event)
        doit = self.shouldRunAgain()
        if doit:
            self._doRun()
",if doit :,121
"def get_source(self, environment, template):
    if self._sep in template:
        prefix, name = template.split(self._sep, 1)
        if prefix not in self._mapping:
            raise TemplateNotFound(template)
        return self._mapping[prefix].get_source(environment, name)
    return self._default.get_source(environment, template)
",if prefix not in self . _mapping :,92
"def find_child_processes_that_send_spans(pants_result_stderr):
    child_processes = set()
    for line in pants_result_stderr.split(""\n""):
        if ""Sending spans to Zipkin server from pid:"" in line:
            i = line.rindex("":"")
            child_process_pid = line[i + 1 :]
            child_processes.add(int(child_process_pid))
    return child_processes
","if ""Sending spans to Zipkin server from pid:"" in line :",113
"def list_dependencies_modules(self, *modules):
    """"""[UNIT]... show the dependency tree"" """"""
    found_all = True
    units = []
    for module in modules:
        matched = self.match_units([module])
        if not matched:
            logg.error(""no such service '%s'"", module)
            found_all = False
            continue
        for unit in matched:
            if unit not in units:
                units += [unit]
    return self.list_dependencies_units(units)  # and found_all
",if unit not in units :,143
"def getCommitFromFile(short=True):
    global _gitdir
    branch = getBranchFromFile()
    commit = None
    if _gitdir and branch:
        if branch == ""HEAD"":
            commitFile = os.path.join(_gitdir, ""HEAD"")
        else:
            commitFile = os.path.join(_gitdir, ""refs"", ""heads"", branch)
        if os.path.isfile(commitFile):
            with open(commitFile, ""r"", encoding=""utf-8"") as f:
                commit = f.readline().strip()
    if short and commit:
        return commit[:8]
    else:
        return commit
","if branch == ""HEAD"" :",169
"def _node_for(pvector_like, i):
    if 0 <= i < pvector_like._count:
        if i >= pvector_like._tail_offset:
            return pvector_like._tail
        node = pvector_like._root
        for level in range(pvector_like._shift, 0, -SHIFT):
            node = node[(i >> level) & BIT_MASK]  # >>>
        return node
    raise IndexError(""Index out of range: %s"" % (i,))
",if i >= pvector_like . _tail_offset :,128
"def check(self):
    global MySQLdb
    import MySQLdb
    try:
        args = {}
        if mysql_user:
            args[""user""] = mysql_user
        if mysql_pwd:
            args[""passwd""] = mysql_pwd
        if mysql_host:
            args[""host""] = mysql_host
        if mysql_port:
            args[""port""] = mysql_port
        if mysql_socket:
            args[""unix_socket""] = mysql_socket
        self.db = MySQLdb.connect(**args)
    except Exception as e:
        raise Exception(""Cannot interface with MySQL server: %s"" % e)
",if mysql_host :,167
"def flatten(self, d, parent_key="""", sep="".""):
    items = []
    for k, v in d.items():
        new_key = parent_key + sep + k if parent_key else k
        if isinstance(v, MutableMapping):
            items.extend(self.flatten(v, new_key, sep=sep).items())
        else:
            items.append((new_key, v))
    return dict(items)
","if isinstance ( v , MutableMapping ) :",111
"def get_item(type_, preference):
    items = {}
    for item in playlist.findall(""./info/%s/item"" % type_):
        lang, label = xpath_text(item, ""lg"", default=None), xpath_text(
            item, ""label"", default=None
        )
        if lang and label:
            items[lang] = label.strip()
    for p in preference:
        if items.get(p):
            return items[p]
",if lang and label :,121
"def test_lxml():
    try:
        from lxml.etree import LXML_VERSION, __version__
        if LXML_VERSION >= (2, 1, 4, 0):
            return True, __version__
        else:
            return False, __version__
    except ImportError:
        return None, None
","if LXML_VERSION >= ( 2 , 1 , 4 , 0 ) :",81
"def send(self, data, flags=0, timeout=timeout_default):
    if timeout is timeout_default:
        timeout = self.timeout
    try:
        return self._sock.send(data, flags)
    except error as ex:
        if ex.args[0] not in _socketcommon.GSENDAGAIN or timeout == 0.0:
            raise
        sys.exc_clear()
        self._wait(self._write_event)
        try:
            return self._sock.send(data, flags)
        except error as ex2:
            if ex2.args[0] == EWOULDBLOCK:
                return 0
            raise
",if ex . args [ 0 ] not in _socketcommon . GSENDAGAIN or timeout == 0.0 :,175
"def blob_from_lang(self):
    self.acquire_lock()
    try:
        if self._blob_from_lang_cache is None:
            try:
                self._load_buf_data_once()
            except NotFoundInDatabase:
                self.release_lock()
                try:
                    self.scan()
                finally:
                    self.acquire_lock()
                self._load_buf_data_once(True)
        return self._blob_from_lang_cache
    finally:
        self.release_lock()
",if self . _blob_from_lang_cache is None :,159
"def processElem(elem, keyList):
    for k, v in elem.items():
        prefix = ""."".join(keyList)
        if k not in self.IGNORE_ELEMENTS and self.NUMVAL_MATCH.match(v):
            k = makeSane(k)
            self.publish(""%s.%s"" % (prefix, k), v)
",if k not in self . IGNORE_ELEMENTS and self . NUMVAL_MATCH . match ( v ) :,90
"def __conform__(self, interface, registry=None, default=None):
    for providedInterface in self.provided:
        if providedInterface.isOrExtends(interface):
            return self.load()
        if getAdapterFactory(providedInterface, interface, None) is not None:
            return interface(self.load(), default)
    return default
","if getAdapterFactory ( providedInterface , interface , None ) is not None :",87
"def restrict(points):
    result = []
    for p in points:
        if point_inside_mesh(bvh, p):
            result.append(p)
        else:
            loc, normal, index, distance = bvh.find_nearest(p)
            if loc is not None:
                result.append(tuple(loc))
    return result
","if point_inside_mesh ( bvh , p ) :",96
"def __iter__(self):
    buffer = [b""""]
    for chunk in self.stream(decode_content=True):
        if b""\n"" in chunk:
            chunk = chunk.split(b""\n"")
            yield b"""".join(buffer) + chunk[0] + b""\n""
            for x in chunk[1:-1]:
                yield x + b""\n""
            if chunk[-1]:
                buffer = [chunk[-1]]
            else:
                buffer = []
        else:
            buffer.append(chunk)
    if buffer:
        yield b"""".join(buffer)
",if chunk [ - 1 ] :,165
"def clear_doc(self, docname: str) -> None:
    for sChild in self._children:
        sChild.clear_doc(docname)
        if sChild.declaration and sChild.docname == docname:
            sChild.declaration = None
            sChild.docname = None
            sChild.line = None
            if sChild.siblingAbove is not None:
                sChild.siblingAbove.siblingBelow = sChild.siblingBelow
            if sChild.siblingBelow is not None:
                sChild.siblingBelow.siblingAbove = sChild.siblingAbove
            sChild.siblingAbove = None
            sChild.siblingBelow = None
",if sChild . siblingBelow is not None :,189
"def _get_current_weight(self, policy, fw):
    weights = policy.get_weights()
    if fw == ""torch"":
        # DQN model.
        if ""_hidden_layers.0._model.0.weight"" in weights:
            return weights[""_hidden_layers.0._model.0.weight""][0][0]
        # DDPG model.
        else:
            return weights[""policy_model.action_0._model.0.weight""][0][0]
    key = 0 if fw in [""tf2"", ""tfe""] else list(weights.keys())[0]
    return weights[key][0][0]
","if ""_hidden_layers.0._model.0.weight"" in weights :",163
"def add_unit(self, name, value, aliases=tuple(), **modifiers):
    """"""Add unit to the registry.""""""
    if not isinstance(value, self.Quantity):
        value = self.Quantity(value, **modifiers)
    self._UNITS[name] = value
    for ndx, alias in enumerate(aliases):
        if "" "" in alias:
            logger.warn(""Alias cannot contain a space "" + alias)
        self._UNITS.add_alias(alias.strip(), name, not ndx)
","if "" "" in alias :",127
"def keyPressEvent(self, event):
    """"""Add up and down arrow key events to built in functionality.""""""
    keyPressed = event.key()
    if keyPressed in [Constants.UP_KEY, Constants.DOWN_KEY, Constants.TAB_KEY]:
        if keyPressed == Constants.UP_KEY:
            self.index = max(0, self.index - 1)
        elif keyPressed == Constants.DOWN_KEY:
            self.index = min(len(self.completerStrings) - 1, self.index + 1)
        elif keyPressed == Constants.TAB_KEY and self.completerStrings:
            self.tabPressed()
        if self.completerStrings:
            self.setTextToCompleterIndex()
    super(CueLineEdit, self).keyPressEvent(event)
",elif keyPressed == Constants . TAB_KEY and self . completerStrings :,192
"def _add_bookmark_breakpoint(self):
    """"""Add a bookmark or breakpoint to the current file in the editor.""""""
    editorWidget = self.ide.mainContainer.get_actual_editor()
    if editorWidget and editorWidget.hasFocus():
        if self.ide.mainContainer.actualTab.navigator.operation == 1:
            editorWidget._sidebarWidget.set_bookmark(
                editorWidget.textCursor().blockNumber()
            )
        elif self.ide.mainContainer.actualTab.navigator.operation == 2:
            editorWidget._sidebarWidget.set_breakpoint(
                editorWidget.textCursor().blockNumber()
            )
",if self . ide . mainContainer . actualTab . navigator . operation == 1 :,175
"def list_generator(pages, num_results):
    result = []
    # get first page items
    page = list(next(pages))
    result += page
    while True:
        if not pages.continuation_token:
            break
        # handle num results
        if num_results is not None:
            if num_results == len(result):
                break
        page = list(next(pages))
        result += page
    return result
",if num_results == len ( result ) :,118
"def _print_handles(self, text, handle_list):
    for handle in handle_list:
        source, citation = self.get_source_or_citation(handle, False)
        _LOG.debug(""\n\n\n"")
        if source:
            _LOG.debug(""---- %s -- source %s"" % (text, source.get_title()))
        elif citation:
            _LOG.debug(""---- %s -- citation %s"" % (text, citation.get_page()))
        else:
            _LOG.debug(""---- %s -- handle %s"" % (text, handle))
",elif citation :,161
"def _parse_whois(self, txt):
    asn, desc = None, b""""
    for l in txt.splitlines():
        if not asn and l.startswith(b""origin:""):
            asn = l[7:].strip().decode(""utf-8"")
        if l.startswith(b""descr:""):
            if desc:
                desc += br""\n""
            desc += l[6:].strip()
        if asn is not None and desc.strip():
            desc = desc.strip().decode(""utf-8"")
            break
    return asn, desc
",if desc :,151
"def build(opt):
    dpath = os.path.join(opt[""datapath""], ""multiwoz_v20"")
    version = ""1.0""
    if not build_data.built(dpath, version_string=version):
        print(""[building data: "" + dpath + ""]"")
        if build_data.built(dpath):
            build_data.remove_dir(dpath)
        build_data.make_dir(dpath)
        # Download the data.
        for downloadable_file in RESOURCES:
            downloadable_file.download_file(dpath)
        build_data.mark_done(dpath, version_string=version)
",if build_data . built ( dpath ) :,163
"def _global_pool2d_shape_func(data_shape, height_axis, width_axis):
    out = output_tensor((data_shape.shape[0],), ""int64"")
    for i in const_range(out.shape[0]):
        if i == height_axis or i == width_axis:
            out[i] = int64(1)
        else:
            out[i] = data_shape[i]
    return out
",if i == height_axis or i == width_axis :,114
"def post_mortem(t=None):
    # handling the default
    if t is None:
        # sys.exc_info() returns (type, value, traceback) if an exception is
        # being handled, otherwise it returns None
        t = sys.exc_info()[2]
        if t is None:
            raise ValueError(
                ""A valid traceback must be passed if no "" ""exception is being handled""
            )
    p = Pdb()
    p.reset()
    p.interaction(None, t)
",if t is None :,134
"def clear(self, purge=False, delete_dataset=True):
    self.deleted = True
    if self.dataset:
        if delete_dataset:
            self.dataset.deleted = True
        if purge:
            self.dataset.purged = True
    if purge and self.dataset.deleted:  # do something with purging
        self.purged = True
        try:
            os.unlink(self.file_name)
        except Exception as e:
            log.error(
                ""Failed to purge associated file ({}) from disk: {}"".format(
                    self.file_name, unicodify(e)
                )
            )
",if delete_dataset :,175
"def scan_resource_conf(self, conf):
    if ""properties"" in conf:
        if ""supportsHttpsTrafficOnly"" in conf[""properties""]:
            if str(conf[""properties""][""supportsHttpsTrafficOnly""]).lower() == ""true"":
                return CheckResult.PASSED
            else:
                return CheckResult.FAILED
    # Use default if supportsHttpsTrafficOnly is not set
    if ""apiVersion"" in conf:
        # Default for apiVersion 2019 and newer is supportsHttpsTrafficOnly = True
        year = int(conf[""apiVersion""][0:4])
        if year < 2019:
            return CheckResult.FAILED
        else:
            return CheckResult.PASSED
    return CheckResult.FAILED
","if ""supportsHttpsTrafficOnly"" in conf [ ""properties"" ] :",192
"def connect(self):
    while True:
        errno = self.sock.connect_ex(self.addr)
        if not errno:
            # connected immediately.
            break
        elif errno == EINPROGRESS:
            # will be connected.
            break
        elif errno == ENOENT:
            # no such socket file.
            self.create_connection(self.failover_interval)
            return
        else:
            raise ValueError(""Unexpected socket errno: %d"" % errno)
    self.event_loop.watch_file(self.sock.fileno(), self.handle)
",elif errno == ENOENT :,157
"def _get_commands():
    proc = Popen([""react-native"", ""--help""], stdout=PIPE)
    should_yield = False
    for line in proc.stdout.readlines():
        line = line.decode().strip()
        if not line:
            continue
        if ""Commands:"" in line:
            should_yield = True
            continue
        if should_yield:
            yield line.split("" "")[0]
",if should_yield :,111
"def getintdict(self, section):
    try:
        # Exclude keys from [DEFAULT] section because in general they do not hold int values
        return dict(
            (key, int(value))
            for key, value in self.items(section)
            if key not in {k for k, _ in self.items(""DEFAULT"")}
        )
    except NoSectionError:
        return {}
","if key not in { k for k , _ in self . items ( ""DEFAULT"" ) }",103
"def _gen_opnds(ii):  # generator
    # filter out write-mask operands and suppressed operands
    for op in ii.parsed_operands:
        if op.lookupfn_name in [""MASK1"", ""MASKNOT0""]:
            continue
        if op.visibility == ""SUPPRESSED"":
            continue
        if op.name == ""BCAST"":
            continue
        yield op
","if op . visibility == ""SUPPRESSED"" :",104
"def do_definition(tag):
    w.end_para()
    macro("".TP"")
    w.started = True
    split = 0
    pre = []
    post = []
    for typ, text in _bitlist(tag):
        if split:
            post.append((typ, text))
        elif text.lstrip().startswith("": ""):
            split = 1
            post.append((typ, text.lstrip()[2:].lstrip()))
        else:
            pre.append((typ, text))
    _boldline(pre)
    w.write(_text(post))
    w.started = False
","elif text . lstrip ( ) . startswith ( "": "" ) :",153
"def EvalInScriptedSection(self, codeBlock, globals, locals=None):
    if locals is None:
        locals = globals
    assert not codeBlock.beenExecuted, ""This code block should not have been executed""
    codeBlock.beenExecuted = 1
    self.BeginScriptedSection()
    try:
        try:
            return self._EvalInScriptedSection(codeBlock.codeObject, globals, locals)
        finally:
            if self.debugManager:
                self.debugManager.OnLeaveScript()
            self.EndScriptedSection()
    except:
        self.HandleException(codeBlock)
",if self . debugManager :,162
"def OSError__str__(self):
    if self.filename:
        if self.filename2:
            return ""[Errno %s] %s: %s -> %s"" % (
                self.errno,
                self.strerror,
                self.filename,
                self.filename2,
            )
        else:
            return ""[Errno %s] %s: %s"" % (self.errno, self.strerror, self.filename)
    if self.errno and self.strerror:
        return ""[Errno %s] %s"" % (self.errno, self.strerror)
    return BaseException.__str__(self)
",if self . filename2 :,164
"def save(self, *args, **kwargs):
    if not self.identifier:
        charset = list(""ABCDEFGHJKLMNPQRSTUVWXYZ3789"")
        while True:
            code = get_random_string(length=8, allowed_chars=charset)
            if not Question.objects.filter(event=self.event, identifier=code).exists():
                self.identifier = code
                break
    super().save(*args, **kwargs)
    if self.event:
        self.event.cache.clear()
","if not Question . objects . filter ( event = self . event , identifier = code ) . exists ( ) :",138
"def malloc(self, size):
    # return a block of right size (possibly rounded up)
    assert 0 <= size < sys.maxint
    if os.getpid() != self._lastpid:
        self.__init__()  # reinitialize after fork
    self._lock.acquire()
    try:
        size = self._roundup(max(size, 1), self._alignment)
        (arena, start, stop) = self._malloc(size)
        new_stop = start + size
        if new_stop < stop:
            self._free((arena, new_stop, stop))
        block = (arena, start, new_stop)
        self._allocated_blocks.add(block)
        return block
    finally:
        self._lock.release()
",if new_stop < stop :,196
"def commit(cache):
    assert cache.is_alive
    try:
        if cache.modified:
            cache.flush()
        if cache.in_transaction:
            assert cache.connection is not None
            cache.database.provider.commit(cache.connection, cache)
        cache.for_update.clear()
        cache.query_results.clear()
        cache.max_id_cache.clear()
        cache.immediate = True
    except:
        cache.rollback()
        raise
",if cache . in_transaction :,131
"def __get_tasks(cls, task_ids=None, project_name=None, task_name=None, **kwargs):
    if task_ids:
        if isinstance(task_ids, six.string_types):
            task_ids = [task_ids]
        return [
            cls(private=cls.__create_protection, task_id=task_id, log_to_backend=False)
            for task_id in task_ids
        ]
    return [
        cls(private=cls.__create_protection, task_id=task.id, log_to_backend=False)
        for task in cls._query_tasks(
            project_name=project_name, task_name=task_name, **kwargs
        )
    ]
","if isinstance ( task_ids , six . string_types ) :",191
"def _VarRefOrWord(node, dynamic_arith):
    # type: (arith_expr_t, bool) -> bool
    with tagswitch(node) as case:
        if case(arith_expr_e.VarRef):
            return True
        elif case(arith_expr_e.Word):
            if dynamic_arith:
                return True
    return False
",if case ( arith_expr_e . VarRef ) :,96
"def fit(self, data_instances, suffix):
    if self.statics_obj is None:
        self.statics_obj = MultivariateStatisticalSummary(data_instances)
    quantile_points = self.statics_obj.get_quantile_point(self.percentile)
    for col_name in self.selection_properties.select_col_names:
        quantile_value = quantile_points.get(col_name)
        if quantile_value < self.upper_threshold:
            self.selection_properties.add_left_col_name(col_name)
        self.selection_properties.add_feature_value(col_name, quantile_value)
    self._keep_one_feature(pick_high=True)
    return self
",if quantile_value < self . upper_threshold :,181
"def predict_dict(self, words):
    """"""Predict a list of expansions given words.""""""
    expansions = []
    for w in words:
        if w in self.expansion_dict:
            expansions += [self.expansion_dict[w]]
        elif w.lower() in self.expansion_dict:
            expansions += [self.expansion_dict[w.lower()]]
        else:
            expansions += [w]
    return expansions
",elif w . lower ( ) in self . expansion_dict :,114
"def connect(self, host, port, ssl, helo, starttls, timeout):
    if ssl == ""0"":
        if not port:
            port = 25
        fp = SMTP(timeout=int(timeout))
    else:
        if not port:
            port = 465
        fp = SMTP_SSL(timeout=int(timeout))
    resp = fp.connect(host, int(port))
    if helo:
        cmd, name = helo.split("" "", 1)
        if cmd.lower() == ""ehlo"":
            resp = fp.ehlo(name)
        else:
            resp = fp.helo(name)
    if not starttls == ""0"":
        resp = fp.starttls()
    return TCP_Connection(fp, resp)
",if not port :,200
"def _init_from_text(self, text):
    parts = text.split(""; "")
    for part in parts:
        key, val = part.split(""="")
        if key == ""CLONE"":
            if val[:5] == ""IMAGE"":
                self.is_image = True
                self.image = val[6:]
        setattr(self, key.lower(), val)
","if val [ : 5 ] == ""IMAGE"" :",101
"def to_laid_out_tensor(self):
    if not self._reduced:
        self._reduced = self.mesh_impl.allreduce(
            self.laid_out_input, self.mesh_axes, ""SUM""
        )
        if self._add_counter_fn:
            self._add_counter_fn()
    return self._reduced
",if self . _add_counter_fn :,93
"def platformGetThreads(self):
    ret = {}
    self._sendPkt(""qfThreadInfo"")
    tbytes = self._recvPkt()
    while tbytes.startswith(""m""):
        if tbytes.find("",""):
            for bval in tbytes[1:].split("",""):
                ret[int(bval, 16)] = 0
        else:
            ret[int(tbytes[1:], 16)] = 0
        self._sendPkt(""qsThreadInfo"")
        tbytes = self._recvPkt()
    return ret
","if tbytes . find ( "","" ) :",138
"def _generate_patterns(self, intent, intent_utterances, entity_placeholders):
    unique_patterns = set()
    patterns = []
    stop_words = self._get_intent_stop_words(intent)
    for utterance in intent_utterances:
        pattern = self._utterance_to_pattern(utterance, stop_words, entity_placeholders)
        if pattern not in unique_patterns:
            unique_patterns.add(pattern)
            patterns.append(pattern)
    return patterns
",if pattern not in unique_patterns :,119
"def generator():
    try:
        _resp_data = DataHelper.flow2origin(self.flow[""response""]) or """"
        length = len(_resp_data)
        size = self.response_chunk_size
        bandwidth = config.bandwidth
        if bandwidth > 0:
            sleep_time = self.response_chunk_size / (bandwidth * 1024)
        else:
            sleep_time = 0
        for i in range(int(length / size) + 1):
            time.sleep(sleep_time)
            self.server_resp_time = time.time()
            yield _resp_data[i * size : (i + 1) * size]
    finally:
        self.update_client_resp_time()
",if bandwidth > 0 :,190
"def generateMapItemListNode(self, key, value):
    itemslist = list()
    for item in value:
        if key in self.allowedFieldsList:
            itemslist.append(""%s = %s"" % (key, self.generateValueNode(item, key)))
        else:
            itemslist.append(""%s"" % (self.generateValueNode(item)))
    return ""("" + "" OR "".join(itemslist) + "")""
",if key in self . allowedFieldsList :,109
"def _underscore_dict(dictionary):
    new_dictionary = {}
    for key, value in dictionary.items():
        if isinstance(value, dict):
            value = _underscore_dict(value)
        if isinstance(key, str):
            key = underscore(key)
        new_dictionary[key] = value
    return new_dictionary
","if isinstance ( value , dict ) :",87
"def offsetToRva(self, offset):
    if self.inmem:
        return offset
    for s in self.sections:
        sbase = s.PointerToRawData
        if s.SizeOfRawData + s.PointerToRawData > self.getMaxRva():
            # SizeOfRawData can be misleading.
            ssize = s.VirtualSize
        else:
            ssize = max(s.SizeOfRawData, s.VirtualSize)
        if sbase <= offset and offset < sbase + ssize:
            return offset - s.PointerToRawData + s.VirtualAddress
    return 0
",if s . SizeOfRawData + s . PointerToRawData > self . getMaxRva ( ) :,155
"def func():
    end_received = False
    while True:
        for idx, q in enumerate(self._local_out_queues):
            data = q.get()
            q.task_done()
            if isinstance(data, EndSignal):
                end_received = True
                if idx > 0:
                    continue
            self._out_queue.put(data)
        if end_received:
            break
",if end_received :,120
"def unwrap_assert_methods() -> None:
    for patcher in _mock_module_patches:
        try:
            patcher.stop()
        except RuntimeError as e:
            # a patcher might have been stopped by user code (#137)
            # so we need to catch this error here and ignore it;
            # unfortunately there's no public API to check if a patch
            # has been started, so catching the error it is
            if str(e) == ""stop called on unstarted patcher"":
                pass
            else:
                raise
    _mock_module_patches[:] = []
    _mock_module_originals.clear()
","if str ( e ) == ""stop called on unstarted patcher"" :",170
"def run(self):
    queue = self.queue
    while True:
        if not self.running:
            break
        # Grab our data
        callback, requests, fetchTimeout, validityOverride = queue.get()
        # Grab prices, this is the time-consuming part
        if len(requests) > 0:
            Price.fetchPrices(requests, fetchTimeout, validityOverride)
        wx.CallAfter(callback)
        queue.task_done()
        # After we fetch prices, go through the list of waiting items and call their callbacks
        for price in requests:
            callbacks = self.wait.pop(price.typeID, None)
            if callbacks:
                for callback in callbacks:
                    wx.CallAfter(callback)
",if len ( requests ) > 0 :,197
"def loadGCodeData(self, dataStream):
    if self._printing:
        return False
    self._lineCount = 0
    for line in dataStream:
        # Strip out comments, we do not need to send comments
        if "";"" in line:
            line = line[: line.index("";"")]
        # Strip out whitespace at the beginning/end this saves data to send.
        line = line.strip()
        if len(line) < 1:
            continue
        self._lineCount += 1
    self._doCallback()
    return True
","if "";"" in line :",139
"def _prepare_work_root(self):
    if os.path.exists(self.work_root):
        for f in os.listdir(self.work_root):
            if os.path.isdir(os.path.join(self.work_root, f)):
                shutil.rmtree(os.path.join(self.work_root, f))
            else:
                os.remove(os.path.join(self.work_root, f))
    else:
        os.makedirs(self.work_root)
","if os . path . isdir ( os . path . join ( self . work_root , f ) ) :",136
"def _parse(self):
    for factory in self._sub_factories():
        if factory.is_possible_start(self.get_next_token()):
            node, self.token_pos = factory(**self._initializer_args())._parse_with_pos()
            return node
    self.raise_unexpected_token()
",if factory . is_possible_start ( self . get_next_token ( ) ) :,82
"def run(self):
    try:
        if not self.shell:
            self.shell = os.name == ""nt""
        if self.working_dir != """":
            os.chdir(self.working_dir)
        proc = subprocess.Popen(
            self.command,
            stdout=subprocess.PIPE,
            stderr=subprocess.STDOUT,
            shell=self.shell,
            env=self.env,
        )
        output = codecs.decode(proc.communicate()[0])
        self.on_done(output)
    except subprocess.CalledProcessError as e:
        self.on_done(e.returncode, error=True)
    except OSError as e:
        self.on_done(e.message, error=True)
","if self . working_dir != """" :",196
"def is_filtered_inherited_member(name: str, obj: Any) -> bool:
    if inspect.isclass(self.object):
        for cls in self.object.__mro__:
            if cls.__name__ == self.options.inherited_members and cls != self.object:
                # given member is a member of specified *super class*
                return True
            elif name in cls.__dict__:
                return False
            elif name in self.get_attr(cls, ""__annotations__"", {}):
                return False
            elif isinstance(obj, ObjectMember) and obj.class_ is cls:
                return False
    return False
",elif name in cls . __dict__ :,167
"def _connect(s, address):
    try:
        s.connect(address)
    except socket.error:
        (ty, v) = sys.exc_info()[:2]
        if hasattr(v, ""errno""):
            v_err = v.errno
        else:
            v_err = v[0]
        if v_err not in [errno.EINPROGRESS, errno.EWOULDBLOCK, errno.EALREADY]:
            raise v
","if v_err not in [ errno . EINPROGRESS , errno . EWOULDBLOCK , errno . EALREADY ] :",120
"def _send_file(self, conn, path):
    """"""Method for a file PUT coro""""""
    while True:
        chunk = conn.queue.get()
        if not conn.failed:
            try:
                with ChunkWriteTimeout(self.app.node_timeout):
                    conn.send(chunk)
            except (Exception, ChunkWriteTimeout):
                conn.failed = True
                self.exception_occurred(
                    conn.node, _(""Object""), _(""Trying to write to %s"") % path
                )
        conn.queue.task_done()
",if not conn . failed :,157
"def get_http_auth(self, name):
    auth = self._config.get(""http-basic.{}"".format(name))
    if not auth:
        username = self._config.get(""http-basic.{}.username"".format(name))
        password = self._config.get(""http-basic.{}.password"".format(name))
        if not username and not password:
            return None
    else:
        username, password = auth[""username""], auth.get(""password"")
        if password is None:
            password = self.keyring.get_password(name, username)
    return {
        ""username"": username,
        ""password"": password,
    }
",if not username and not password :,166
"def _do_analyze(self, action_ref, rule_links=None, processed=None, depth=0):
    if processed is None:
        processed = set()
    if rule_links is None:
        rule_links = []
    processed.add(action_ref)
    for rule_link in self._rules.get(action_ref, []):
        rule_links.append((depth, rule_link))
        if rule_link._dest_action_ref in processed:
            continue
        self._do_analyze(
            rule_link._dest_action_ref,
            rule_links=rule_links,
            processed=processed,
            depth=depth + 1,
        )
    return rule_links
",if rule_link . _dest_action_ref in processed :,185
"def _mock_manager_nfx(self, *args, **kwargs):
    if args:
        if args[0].tag == ""command"":
            raise RpcError()
        elif args[0].tag == ""get-software-information"" and args[0].find(""./*"") is None:
            return True
        else:
            return self._read_file(""sw_info_nfx_"" + args[0].tag + "".xml"")
","elif args [ 0 ] . tag == ""get-software-information"" and args [ 0 ] . find ( ""./*"" ) is None :",112
"def test_url_invalid_set():
    for line in URL_INVALID_TESTS.split(""\n""):
        # strip line, skip over empty lines
        line = line.strip()
        if line == """":
            continue
        # skip over comments
        match = COMMENT.match(line)
        if match:
            continue
        mbox = address.parse(line, strict=True)
        assert_equal(mbox, None)
","if line == """" :",115
"def _monitor_thread_function(main_process_pid):
    while True:
        logger.debug(""Monitor thread monitoring pid: %d"", main_process_pid)
        main_process_alive = any(
            [
                process.pid
                for process in process_iter()
                if process.pid == main_process_pid
            ]
        )
        if not main_process_alive:
            logger.debug(
                ""Main process with pid %d is dead. Killing worker"", main_process_pid
            )
            os._exit(0)
        sleep(1)
",if not main_process_alive :,166
"def OnInsertCells(self, event=None):
    # TODO remove below workaround for double actions
    if self._counter == 1:
        if self._icells == (self.selection.topleft, self.selection.bottomright):
            self._counter = 0
            self._icells = None
            return
    else:
        self._counter = 1
    self._icells = (self.selection.topleft, self.selection.bottomright)
    self._execute(InsertCells(self.selection.topleft, self.selection.bottomright))
    self._resize_grid()
    self._skip_except_on_mac(event)
","if self . _icells == ( self . selection . topleft , self . selection . bottomright ) :",158
"def get_scripts():
    """"""Get custom npm scripts.""""""
    proc = Popen([""npm"", ""run-script""], stdout=PIPE)
    should_yeild = False
    for line in proc.stdout.readlines():
        line = line.decode()
        if ""available via `npm run-script`:"" in line:
            should_yeild = True
            continue
        if should_yeild and re.match(r""^  [^ ]+"", line):
            yield line.strip().split("" "")[0]
","if should_yeild and re . match ( r""^  [^ ]+"" , line ) :",129
"def get_netloc(url):
    """"""Get Domain.""""""
    try:
        domain = """"
        parse_uri = urlparse(url)
        if not parse_uri.scheme:
            url = ""//"" + url
            parse_uri = urlparse(url)
        domain = ""{uri.netloc}"".format(uri=parse_uri)
        if verify_domain(domain):
            return domain
    except Exception:
        logger.exception(""[ERROR] Extracting Domain form URL"")
",if verify_domain ( domain ) :,121
"def initiate_all_local_variables_instances(
    nodes, local_variables_instances, all_local_variables_instances
):
    for node in nodes:
        if node.variable_declaration:
            new_var = LocalIRVariable(node.variable_declaration)
            if new_var.name in all_local_variables_instances:
                new_var.index = all_local_variables_instances[new_var.name].index + 1
            local_variables_instances[node.variable_declaration.name] = new_var
            all_local_variables_instances[node.variable_declaration.name] = new_var
",if new_var . name in all_local_variables_instances :,158
"def _disconnect(self, sync):
    if self._connection:
        if sync:
            try:
                self._connection.send_all()
                self._connection.fetch_all()
            except (WorkspaceError, ServiceUnavailable):
                pass
        if self._connection:
            self._connection.in_use = False
            self._connection = None
        self._connection_access_mode = None
",if sync :,115
"def init(self):
    """"""Initialize a booster from the database and validate""""""
    self.__item = None
    if self.itemID:
        self.__item = eos.db.getItem(self.itemID)
        if self.__item is None:
            pyfalog.error(""Item (id: {0}) does not exist"", self.itemID)
            return
    if self.isInvalid:
        pyfalog.error(""Item (id: {0}) is not a Booster"", self.itemID)
        return
    self.build()
",if self . __item is None :,137
"def TryMerge(self, d):
    while d.avail() > 0:
        tt = d.getVarInt32()
        if tt == 10:
            self.set_app_id(d.getPrefixedString())
            continue
        if tt == 16:
            self.set_limit(d.getVarInt64())
            continue
        if tt == 0:
            raise ProtocolBuffer.ProtocolBufferDecodeError
        d.skipData(tt)
",if tt == 0 :,122
"def _match_greater_than_or_equal(search_base, attribute, value, candidates):
    matches = list()
    for entry in candidates:
        dn = entry.get(""dn"")
        if not dn.endswith(search_base):
            continue
        value_from_directory = entry.get(""attributes"").get(attribute)
        if str(value_from_directory) >= str(value):
            entry[""type""] = ""searchResEntry""
            matches.append(entry)
    return matches
",if str ( value_from_directory ) >= str ( value ) :,129
"def list_target_unit_files(self, *modules):  # -> [ (unit,enabled) ]
    """"""show all the target units and the enabled status""""""
    result = {}
    enabled = {}
    for unit in _all_common_targets:
        result[unit] = None
        enabled[unit] = ""static""
        if unit in _all_common_enabled:
            enabled[unit] = ""enabled""
        if unit in _all_common_disabled:
            enabled[unit] = ""enabled""
    return [(unit, enabled[unit]) for unit in sorted(result)]
",if unit in _all_common_enabled :,147
"def handle_data(self, data):
    if self.in_span or self.in_div:
        if data == ""No such user (please note that login is case sensitive)"":
            self.no_user = True
        elif data == ""Invalid password"":
            self.bad_pw = True
        elif data == ""User with that email already exists"":
            self.already_exists = True
","if data == ""No such user (please note that login is case sensitive)"" :",101
"def walk_tree(
    root: Element,
    processor: Callable[[Element], Optional[_T]],
    stop_after_first: bool = False,
) -> List[_T]:
    results = []
    queue = deque([root])
    while queue:
        currElement = queue.popleft()
        for child in currElement:
            if child:
                queue.append(child)
            result = processor(child)
            if result is not None:
                results.append(result)
                if stop_after_first:
                    return results
    return results
",if child :,152
"def characters(self, ch):
    if self._inside_fuzzable:
        modified_value = self._fuzzed_parameters[self._fuzzable_index][1]
        if isinstance(modified_value, DataToken):
            modified_value = modified_value.get_value()
        if self._fuzzed_parameters[self._fuzzable_index][0] == ""base64"":
            enc_val = base64.b64encode(modified_value)
        else:
            enc_val = cgi.escape(modified_value).encode(""ascii"", ""xmlcharrefreplace"")
        self.fuzzed_xml_string += enc_val
    else:
        self.fuzzed_xml_string += ch
","if self . _fuzzed_parameters [ self . _fuzzable_index ] [ 0 ] == ""base64"" :",181
"def when_the_task_has_started(context):
    # 120 * 0.5 = 60 seconds
    for _ in range(120):
        app = context.marathon_clients.current[0].get_app(APP_ID)
        happy_count = app.tasks_running
        if happy_count >= 3:
            return
        time.sleep(0.5)
    raise Exception(""timed out waiting for task to start"")
",if happy_count >= 3 :,111
"def _sock_send(self, msg):
    try:
        if isinstance(msg, str):
            msg = msg.encode(""ascii"")
        # http://docs.datadoghq.com/guides/dogstatsd/#datagram-format
        if self.dogstatsd_tags:
            msg = msg + b""|#"" + self.dogstatsd_tags.encode(""ascii"")
        if self.sock:
            self.sock.send(msg)
    except Exception:
        Logger.warning(self, ""Error sending message to statsd"", exc_info=True)
",if self . dogstatsd_tags :,146
"def __init__(
    self, constraints=None, preferences=None, platforms=None, maxreplicas=None
):
    if constraints is not None:
        self[""Constraints""] = constraints
    if preferences is not None:
        self[""Preferences""] = []
        for pref in preferences:
            if isinstance(pref, tuple):
                pref = PlacementPreference(*pref)
            self[""Preferences""].append(pref)
    if maxreplicas is not None:
        self[""MaxReplicas""] = maxreplicas
    if platforms:
        self[""Platforms""] = []
        for plat in platforms:
            self[""Platforms""].append({""Architecture"": plat[0], ""OS"": plat[1]})
","if isinstance ( pref , tuple ) :",174
"def start(self):
    if not self._active:
        self._active = True
        if self.thread is None:
            self.exit = threading.Event()
            self.thread = threading.Thread(target=self.check)
            self.thread.daemon = True
            self.thread.start()
",if self . thread is None :,83
"def on_player_state_changed(self, state):
    if state == State.playing:
        self._toggle_player_action.setText(TOGGLE_PLAYER_TEXT[1])
        self._toggle_player_action.setIcon(QIcon.fromTheme(""media-pause""))
        self._toggle_player_action.setEnabled(True)
    else:
        self._toggle_player_action.setText(TOGGLE_PLAYER_TEXT[0])
        self._toggle_player_action.setIcon(QIcon.fromTheme(""media-play""))
        if state == State.stopped:
            self._toggle_player_action.setEnabled(False)
        else:
            self._toggle_player_action.setEnabled(True)
",if state == State . stopped :,181
"def __init__(self, el):
    self.elements = list(el)
    parameters = {}
    tokens = []
    token_quote = ""@""
    for key, value in el.attrib.items():
        if key == ""token_quote"":
            token_quote = value
        if key == ""tokens"":
            for token in value.split("",""):
                tokens.append((token, REQUIRED_PARAMETER))
        elif key.startswith(""token_""):
            token = key[len(""token_"") :]
            tokens.append((token, value))
    for name, default in tokens:
        parameters[name] = (token_quote, default)
    self.parameters = parameters
","elif key . startswith ( ""token_"" ) :",170
"def create(self):
    if self.mode == ""INDICES"":
        self.newInput(""Integer List"", ""Indices"", ""indices"")
        self.newOutput(""Polygon Indices"", ""Polygon Indices"", ""polygonIndices"")
    elif self.mode == ""VERTEX_AMOUNT"":
        if self.useList:
            self.newInput(""Integer List"", ""Vertex Amounts"", ""vertexAmounts"")
            self.newOutput(
                ""Polygon Indices List"", ""Polygon Indices List"", ""polygonIndicesList""
            )
        else:
            self.newInput(
                ""Integer"", ""Vertex Amount"", ""vertexAmount"", value=3, minValue=3
            )
            self.newOutput(""Polygon Indices"", ""Polygon Indices"", ""polygonIndices"")
",if self . useList :,199
"def _chroot_pids(chroot):
    pids = []
    for root in glob.glob(""/proc/[0-9]*/root""):
        try:
            link = os.path.realpath(root)
            if link.startswith(chroot):
                pids.append(int(os.path.basename(os.path.dirname(root))))
        except OSError:
            pass
    return pids
",if link . startswith ( chroot ) :,106
"def to_word_end(view, s):
    if mode == modes.NORMAL:
        pt = word_end_reverse(view, s.b, count)
        return sublime.Region(pt)
    elif mode in (modes.VISUAL, modes.VISUAL_BLOCK):
        if s.a < s.b:
            pt = word_end_reverse(view, s.b - 1, count)
            if pt > s.a:
                return sublime.Region(s.a, pt + 1)
            return sublime.Region(s.a + 1, pt)
        pt = word_end_reverse(view, s.b, count)
        return sublime.Region(s.a, pt)
    return s
",if pt > s . a :,191
"def torch_sparse_Tensor(coords, feats, size=None):
    if size is None:
        if feats.dtype == torch.float64:
            return torch.sparse.DoubleTensor(coords, feats)
        elif feats.dtype == torch.float32:
            return torch.sparse.FloatTensor(coords, feats)
        else:
            raise ValueError(""Feature type not supported."")
    else:
        if feats.dtype == torch.float64:
            return torch.sparse.DoubleTensor(coords, feats, size)
        elif feats.dtype == torch.float32:
            return torch.sparse.FloatTensor(coords, feats, size)
        else:
            raise ValueError(""Feature type not supported."")
",if feats . dtype == torch . float64 :,179
"def detab(self, text):
    """"""Remove a tab from the front of each line of the given text.""""""
    newtext = []
    lines = text.split(""\n"")
    for line in lines:
        if line.startswith("" "" * markdown.TAB_LENGTH):
            newtext.append(line[markdown.TAB_LENGTH :])
        elif not line.strip():
            newtext.append("""")
        else:
            break
    return ""\n"".join(newtext), ""\n"".join(lines[len(newtext) :])
",elif not line . strip ( ) :,134
"def iter_input(input, filename, parser, line_by_line):
    if isinstance(input, basestring):
        with open(input, ""rb"") as f:
            for tree in iter_input(f, filename, parser, line_by_line):
                yield tree
    else:
        try:
            if line_by_line:
                for line in input:
                    if line:
                        yield et.ElementTree(et.fromstring(line, parser))
            else:
                yield et.parse(input, parser)
        except IOError:
            e = sys.exc_info()[1]
            error(""parsing %r failed: %s: %s"", filename, e.__class__.__name__, e)
",if line :,197
"def find_xsubpp():
    for var in (""privlib"", ""vendorlib""):
        xsubpp = cfg_lst(""$Config{%s}/ExtUtils/xsubpp$Config{exe_ext}"" % var)
        if xsubpp and os.path.isfile(xsubpp[0]):
            return xsubpp
    return self.find_program(""xsubpp"")
",if xsubpp and os . path . isfile ( xsubpp [ 0 ] ) :,93
"def apply_list(self, expr, rules, evaluation):
    ""ReplaceRepeated[expr_, rules_]""
    try:
        rules, ret = create_rules(rules, expr, ""ReplaceRepeated"", evaluation)
    except PatternError:
        evaluation.message(""Replace"", ""reps"", rules)
        return None
    if ret:
        return rules
    while True:
        evaluation.check_stopped()
        result, applied = expr.apply_rules(rules, evaluation)
        if applied:
            result = result.evaluate(evaluation)
        if applied and not result.same(expr):
            expr = result
        else:
            break
    return result
",if applied :,166
"def __init__(
    self,
    lambda_val: Optional[Union[torch.Tensor, Tuple[float, float]]] = None,
    same_on_batch: bool = False,
    p: float = 1.0,
) -> None:
    super(RandomMixUp, self).__init__(p=1.0, p_batch=p, same_on_batch=same_on_batch)
    if lambda_val is None:
        self.lambda_val = torch.tensor([0, 1.0])
    else:
        self.lambda_val = (
            cast(torch.Tensor, lambda_val)
            if isinstance(lambda_val, torch.Tensor)
            else torch.tensor(lambda_val)
        )
","if isinstance ( lambda_val , torch . Tensor )",182
"def run_sync(self):
    count = 0
    while count < self.args.num_messages:
        batch = self.receiver.receive_messages(
            max_message_count=self.args.num_messages - count,
            max_wait_time=self.args.max_wait_time or None,
        )
        if self.args.peeklock:
            for msg in batch:
                self.receiver.complete_message(msg)
        count += len(batch)
",if self . args . peeklock :,129
"def ns_to_timespec(self, nsec):
    """"""Transforms nanoseconds to a timespec.""""""
    # http://elixir.free-electrons.com/linux/v4.13.5/source/kernel/time/time.c#L486
    ts = self.timespec()
    if not nsec:
        ts.tv_sec = 0
        ts.tv_nsec = 0
    else:
        ts.tv_sec, rem = divmod(nsec, timespec.NSEC_PER_SEC)
        if rem < 0:
            ts.tv_sec -= 1
            rem += timespec.NSEC_PER_SEC
        ts.tv_nsec = rem
    return ts
",if rem < 0 :,178
"def fixFunctionDocTag(funcnode):
    doctext = funcnode.get(""doc"")
    if doctext:
        if funcnode.attrib[""name""] == ""eval"":
            # Update the doc for this function call, more user friendly.
            funcnode.attrib[""doc""] = doctext.replace(""ECMAScript"", ""JavaScript"")
        sp = doctext.rsplit(""Return Type: "", 1)
        if len(sp) == 2:
            funcnode.attrib[""doc""] = sp[0].rstrip()
            returnType = standardizeJSType(sp[1].split(None, 1)[0])
            addCixReturns(funcnode, returnType)
            return returnType
    return None
",if len ( sp ) == 2 :,177
"def check_engine(engine):
    if engine == ""auto"":
        if pa is not None:
            return ""pyarrow""
        elif fastparquet is not None:  # pragma: no cover
            return ""fastparquet""
        else:  # pragma: no cover
            raise RuntimeError(""Please install either pyarrow or fastparquet."")
    elif engine == ""pyarrow"":
        if pa is None:  # pragma: no cover
            raise RuntimeError(""Please install pyarrow fisrt."")
        return engine
    elif engine == ""fastparquet"":
        if fastparquet is None:  # pragma: no cover
            raise RuntimeError(""Please install fastparquet first."")
        return engine
    else:  # pragma: no cover
        raise RuntimeError(""Unsupported engine {} to read parquet."".format(engine))
",if pa is not None :,187
"def addInt(self, intval, width, nodeinfo):
    node = self.basenode
    for sh in range(width - 1, -1, -1):
        choice = (intval >> sh) & 1
        if node[choice] is None:
            node[choice] = [None, None, None]
        node = node[choice]
    node[2] = nodeinfo
",if node [ choice ] is None :,98
"def add_cand(cands):
    cands = [cand.creator for cand in cands if cand.creator is not None]
    for x in cands:
        if x in seen_set:
            continue
        order = 1
        if fan_out[x] == 1 and len(cands) == 1:
            order = -len(seen_set)
        # Negate since heapq is min-heap
        # `len(seen_set)` is in order to avoid comparing `x`
        heapq.heappush(cand_funcs, (order, -x.rank, -len(seen_set), x))
        seen_set.add(x)
",if fan_out [ x ] == 1 and len ( cands ) == 1 :,167
"def indentSelection(self, howFar=4):
    # Indent or outdent current selection by 'howFar' spaces
    # (which could be positive or negative int).
    startLineNum = self.LineFromPosition(self.GetSelectionStart())
    endLineNum = self.LineFromPosition(self.GetSelectionEnd())
    # go through line-by-line
    self.BeginUndoAction()
    for lineN in range(startLineNum, endLineNum + 1):
        newIndent = self.GetLineIndentation(lineN) + howFar
        if newIndent < 0:
            newIndent = 0
        self.SetLineIndentation(lineN, newIndent)
    self.EndUndoAction()
",if newIndent < 0 :,179
"def request(self, host, handler, request_body, verbose=False):
    # retry request once if cached connection has gone cold
    for i in (0, 1):
        try:
            return self.single_request(host, handler, request_body, verbose)
        except socket.error as e:
            if i or e.errno not in (errno.ECONNRESET, errno.ECONNABORTED, errno.EPIPE):
                raise
        except http_client.BadStatusLine:  # close after we sent request
            if i:
                raise
","if i or e . errno not in ( errno . ECONNRESET , errno . ECONNABORTED , errno . EPIPE ) :",147
"def update_data(self, change):
    self.mark.x = self.state.x_centers
    y0 = self.state.grid
    if self.normalize:
        y0 = y0 / np.sum(y0)
    if self.state.grid_sliced is not None:
        y1 = self.state.grid_sliced
        if self.normalize:
            y1 = y1 / np.sum(y1)
        self.mark.y = np.array([y0, y1])
        self.mark.colors = [C0, C1]
        self.mark.type = ""grouped""
    else:
        self.mark.y = y0
        self.mark.colors = [C0]
",if self . normalize :,185
"def visit_body(self, nodes):
    new_nodes = []
    count = 0
    for node in nodes:
        rewriter = IfExpRewriter(count)
        possibly_transformed_node = rewriter.visit(node)
        if rewriter.assignments:
            new_nodes.extend(rewriter.assignments)
            count += len(rewriter.assignments)
        new_nodes.append(possibly_transformed_node)
    return new_nodes
",if rewriter . assignments :,110
"def byteRegOffset(self, val, prefixes=0):
    # NOTE: Override this because there is no AH etc in 64 bit mode
    if prefixes & PREFIX_REX:  # the parse_modrm function deals with register index adds
        val |= e_i386.RMETA_LOW8
    else:  # not using REX, revert to old split-registers (low/high)
        if val < 4:
            val |= e_i386.RMETA_LOW8
        else:
            val |= e_i386.RMETA_HIGH8
            val -= 4
    return val
",if val < 4 :,148
"def gprv_immv(ii):
    for i, op in enumerate(_gen_opnds(ii)):
        if i == 0:
            if op.name == ""REG0"" and op_luf_start(op, ""GPRv""):
                continue
            else:
                return False
        elif i == 1:
            if op_immv(op):
                continue
            else:
                return False
        else:
            return False
    return True
",if i == 0 :,136
"def normalize(self):
    self.pairs.sort()
    i = 1
    while i < len(self.pairs):
        alo, ahi = self.pairs[i - 1]
        blo, bhi = self.pairs[i]
        if ahi >= blo - 1:
            self.pairs[i - 1 : i + 1] = [(alo, max(ahi, bhi))]
        else:
            i = i + 1
",if ahi >= blo - 1 :,115
"def __substitute_composite_key(self, key, composite_file, dataset=None):
    if composite_file.substitute_name_with_metadata:
        if dataset:
            meta_value = str(
                dataset.metadata.get(composite_file.substitute_name_with_metadata)
            )
        else:
            meta_value = self.spec[composite_file.substitute_name_with_metadata].default
        return key % meta_value
    return key
",if dataset :,123
"def cb(definition):
    if len(definition.strip()) == 0:
        if old_macro_definition != """":
            dialog = wx.MessageDialog(
                self,
                _(""Do you want to erase the macro?""),
                style=wx.YES_NO | wx.YES_DEFAULT | wx.ICON_QUESTION,
            )
            if dialog.ShowModal() == wx.ID_YES:
                self.delete_macro(macro_name)
                return
        self.log(_(""Cancelled.""))
        return
    self.cur_macro_name = macro_name
    self.cur_macro_def = definition
    self.end_macro()
","if old_macro_definition != """" :",176
"def process_request(self, request):
    for old, new in self.names_name:
        request.uri = request.uri.replace(old, new)
        if is_text_payload(request) and request.body:
            try:
                body = (
                    str(request.body, ""utf-8"")
                    if isinstance(request.body, bytes)
                    else str(request.body)
                )
            except TypeError:  # python 2 doesn't allow decoding through str
                body = str(request.body)
            if old in body:
                request.body = body.replace(old, new)
    return request
",if is_text_payload ( request ) and request . body :,182
"def writeLibraryControllers(fp, human, meshes, skel, config, shapes=None):
    progress = Progress(len(meshes), None)
    fp.write(""\n  <library_controllers>\n"")
    for mIdx, mesh in enumerate(meshes):
        subprog = Progress()(0, 0.5)
        if skel:
            writeSkinController(fp, human, mesh, skel, config)
        subprog(0.5, 1)
        if shapes is not None:
            writeMorphController(fp, mesh, shapes[mIdx], config)
        progress.step()
    fp.write(""  </library_controllers>\n"")
",if skel :,172
"def checkpoint():
    if checkpoint_asserts:
        self.assert_integrity_idxs_take()
        if node in self.idxs_memo:
            toposort(self.idxs_memo[node])
        if node in self.take_memo:
            for take in self.take_memo[node]:
                toposort(take)
",if node in self . idxs_memo :,86
"def __virtual__():  # pylint: disable=expected-2-blank-lines-found-0
    try:
        global __salt__  # pylint: disable=global-statement
        if not __salt__:
            __salt__ = salt.loader.minion_mods(__opts__)
            return True
    except Exception as e:  # pylint: disable=broad-except
        log.error(""Could not load __salt__: %s"", e)
        return False
",if not __salt__ :,118
"def annotate_disk_for_smart(middleware, devices, disk):
    args = await get_smartctl_args(middleware, devices, disk)
    if args:
        if await ensure_smart_enabled(args):
            args.extend([""-a""])
            args.extend([""-d"", ""removable""])
            return disk, dict(smartctl_args=args)
",if await ensure_smart_enabled ( args ) :,93
"def make_connection(self, host):
    h, eh, kwargs = self.get_host_info(host)
    if not kwargs:
        kwargs = {}
    kwargs[""timeout""] = self.timeout
    if _ver_info == (2, 6):
        result = HTTPS(host, None, **kwargs)
    else:
        if not self._connection or host != self._connection[0]:
            self._extra_headers = eh
            self._connection = host, httplib.HTTPSConnection(h, None, **kwargs)
        result = self._connection[1]
    return result
",if not self . _connection or host != self . _connection [ 0 ] :,149
"def get_base_types(self, cdef: ClassDef) -> List[str]:
    """"""Get list of base classes for a class.""""""
    base_types = []  # type: List[str]
    for base in cdef.base_type_exprs:
        if isinstance(base, NameExpr):
            if base.name != ""object"":
                base_types.append(base.name)
        elif isinstance(base, MemberExpr):
            modname = get_qualified_name(base.expr)
            base_types.append(""%s.%s"" % (modname, base.name))
        elif isinstance(base, IndexExpr):
            p = AliasPrinter(self)
            base_types.append(base.accept(p))
    return base_types
","elif isinstance ( base , MemberExpr ) :",187
"def add_entry(self, entry):
    # type: (...) -> None
    version = entry.as_python  # type: PythonVersion
    if version:
        _ = self.versions[version.version_tuple]
        paths = {p.path for p in self.versions.get(version.version_tuple, [])}
        if entry.path not in paths:
            self.versions[version.version_tuple].append(entry)
",if entry . path not in paths :,108
"def check(self):
    global MySQLdb
    import MySQLdb
    try:
        args = {}
        if mysql_user:
            args[""user""] = mysql_user
        if mysql_pwd:
            args[""passwd""] = mysql_pwd
        if mysql_host:
            args[""host""] = mysql_host
        if mysql_port:
            args[""port""] = mysql_port
        if mysql_socket:
            args[""unix_socket""] = mysql_socket
        self.db = MySQLdb.connect(**args)
    except Exception as e:
        raise Exception(""Cannot interface with MySQL server: %s"" % e)
",if mysql_socket :,167
"def findsection(self, key):
    to_return = copy.deepcopy(self)
    for subsection in to_return:
        try:
            value = list(ConfigObj.find_key(to_return[subsection], key))[0]
        except Exception:
            value = None
        if not value:
            del to_return[subsection]
        else:
            for category in to_return[subsection]:
                if category != key:
                    del to_return[subsection][category]
    # cleanout empty sections and subsections
    for key in [k for (k, v) in to_return.items() if not v]:
        del to_return[key]
    return to_return
",if category != key :,189
"def get_ready_conn(self, host):
    conn = None
    self._lock.acquire()
    try:
        if host in self._hostmap:
            for c in self._hostmap[host]:
                if self._readymap[c]:
                    self._readymap[c] = 0
                    conn = c
                    break
    finally:
        self._lock.release()
    return conn
",if host in self . _hostmap :,115
"def assign_set_scope(
    ir_set: irast.Set,
    scope: Optional[irast.ScopeTreeNode],
    *,
    ctx: context.ContextLevel
) -> irast.Set:
    if scope is None:
        ir_set.path_scope_id = None
    else:
        if scope.unique_id is None:
            scope.unique_id = ctx.scope_id_ctr.nextval()
        ir_set.path_scope_id = scope.unique_id
        if scope.find_child(ir_set.path_id):
            raise RuntimeError(""scoped set must not contain itself"")
    return ir_set
",if scope . unique_id is None :,166
"def _flatten(*args):
    arglist = []
    for arg in args:
        if isinstance(arg, _Block):
            if arg.vhdl_code is not None:
                arglist.append(arg.vhdl_code)
                continue
            else:
                arg = arg.subs
        if id(arg) in _userCodeMap[""vhdl""]:
            arglist.append(_userCodeMap[""vhdl""][id(arg)])
        elif isinstance(arg, (list, tuple, set)):
            for item in arg:
                arglist.extend(_flatten(item))
        else:
            arglist.append(arg)
    return arglist
","if id ( arg ) in _userCodeMap [ ""vhdl"" ] :",179
"def _prepare_expected(data, lags, trim=""front""):
    t, k = data.shape
    expected = np.zeros((t + lags, (lags + 1) * k))
    for col in range(k):
        for i in range(lags + 1):
            if i < lags:
                expected[i : -lags + i, (lags + 1) * col + i] = data[:, col]
            else:
                expected[i:, (lags + 1) * col + i] = data[:, col]
    if trim == ""front"":
        expected = expected[:-lags]
    return expected
",if i < lags :,163
"def test_class_based_views_inherit_from_acl_gateway_class(self):
    for urlpattern in self.urlpatterns_to_test:
        callback_name = urlpattern.callback.__name__
        module_name = urlpattern.callback.__module__
        if (callback_name, module_name) in self.excluded_callbacks:
            continue
        imported_module = __import__(module_name, fromlist=[callback_name])
        found_callback = getattr(imported_module, callback_name)
        if not inspect.isclass(found_callback):
            continue
        msg = ""Class '{}' does not inherit from 'ACLGateway' "" ""class."".format(
            found_callback
        )
        self.assertTrue(issubclass(found_callback, ACLGateway), msg)
",if not inspect . isclass ( found_callback ) :,195
"def generateMapItemTypedNode(self, key, value):
    if type(value) == SigmaRegularExpressionModifier:
        regex = str(value)
        # Regular Expressions have to match the full value in QRadar
        if not (regex.startswith(""^"") or regex.startswith("".*"")):
            regex = "".*"" + regex
        if not (regex.endswith(""$"") or regex.endswith("".*"")):
            regex = regex + "".*""
        return ""%s MATCHES %s"" % (self.cleanKey(key), self.generateValueNode(regex))
    else:
        raise NotImplementedError(
            ""Type modifier '{}' is not supported by backend"".format(value.identifier)
        )
","if not ( regex . startswith ( ""^"" ) or regex . startswith ( "".*"" ) ) :",165
"def __str__(self):
    _outicalfile = self._icalfile
    for unit in self.units:
        for location in unit.getlocations():
            match = re.match(""\\[(?P<uid>.+)\\](?P<property>.+)"", location)
            for component in self._icalfile.components():
                if component.name != ""VEVENT"":
                    continue
                if component.uid.value != match.groupdict()[""uid""]:
                    continue
                for property in component.getChildren():
                    if property.name == match.groupdict()[""property""]:
                        property.value = unit.target
    if _outicalfile:
        return str(_outicalfile.serialize())
    else:
        return """"
","if component . uid . value != match . groupdict ( ) [ ""uid"" ] :",198
"def __init__(self, items):
    self._format = string.join(map(lambda item: item[0], items), """")
    self._items = items
    self._buffer_ = win32wnet.NCBBuffer(struct.calcsize(self._format))
    for format, name in self._items:
        if len(format) == 1:
            if format == ""c"":
                val = ""\0""
            else:
                val = 0
        else:
            l = int(format[:-1])
            val = ""\0"" * l
        self.__dict__[name] = val
",if len ( format ) == 1 :,158
"def __init__(self, learners, names=None):
    self.learners = learners
    for i, learner in enumerate(learners):
        self.update_set_reward(learner)
        learner.accumulated_rewards = []
        learner.known_states = []
        learner.temperatures = []
        if names is None:
            learner.name = ""Learner %d"" % i
        else:
            learner.name = names[i]
",if names is None :,120
"def __init__(self, *args, **kwargs):
    self.default_currency = kwargs.pop(""default_currency"", None)
    super().__init__(*args, **kwargs)
    # Rest Framework converts `min_value` / `max_value` to validators, that are not aware about `Money` class
    # We need to adjust them
    for idx, validator in enumerate(self.validators):
        if isinstance(validator, MinValueValidator):
            self.validators[idx] = MinMoneyValidator(self.min_value)
        elif isinstance(validator, MaxValueValidator):
            self.validators[idx] = MaxMoneyValidator(self.max_value)
","if isinstance ( validator , MinValueValidator ) :",159
"def add_line_taxes(self, lines):
    for line in lines:
        if not line.source_line:
            continue  # Cannot have taxes, since not in source
        for (index, line_tax) in enumerate(line.source_line.taxes, 1):
            line.taxes.create(
                tax=line_tax.tax,
                name=line_tax.name,
                amount_value=line_tax.amount.value,
                base_amount_value=line_tax.base_amount.value,
                ordering=index,
            )
",if not line . source_line :,157
"def linesub(match):
    line = match.group()
    for token in TOKEN_RE.findall(line):
        if token in names:
            targets = names[token]
            fdist.inc(token)
            if len(targets) > 1:
                log.warning(
                    ""%s is ambiguous: %s""
                    % (token, "", "".join(str(v.canonical_name) for v in names[token]))
                )
            line += INDEXTERM % token
            # line += INDEXTERM % names[token][0].canonical_name
    return line
",if len ( targets ) > 1 :,159
"def ask(self) -> Dict[str, Any]:
    params = {}
    param_values = self._optimizer.ask()
    for (name, distribution), value in zip(
        sorted(self._search_space.items()), param_values
    ):
        if isinstance(distribution, distributions.DiscreteUniformDistribution):
            value = value * distribution.q + distribution.low
        if isinstance(distribution, distributions.IntUniformDistribution):
            value = value * distribution.step + distribution.low
        if isinstance(distribution, distributions.IntLogUniformDistribution):
            value = int(np.round(value))
            value = min(max(value, distribution.low), distribution.high)
        params[name] = value
    return params
","if isinstance ( distribution , distributions . IntUniformDistribution ) :",180
"def fetcher():
    while True:
        try:
            if not self._running:
                break
            self.fetch()
            self._cluster.handler.sleep(0.01)
        except ReferenceError:
            break
        except Exception:
            # surface all exceptions to the main thread
            self._worker_exception = sys.exc_info()
            break
    try:
        self.cleanup()
    except ReferenceError as e:
        log.debug(""Attempt to cleanup consumer failed with ReferenceError"")
    log.debug(""Fetcher thread exiting"")
",if not self . _running :,148
"def write_text(self, text):
    """"""Writes re-indented text into the buffer.""""""
    should_indent = False
    rows = []
    for row in text.split(""\n""):
        if should_indent:
            row = ""    {}"".format(row)
        if ""\b"" in row:
            row = row.replace(""\b"", """", 1)
            should_indent = True
        elif not len(row.strip()):
            should_indent = False
        rows.append(row)
    self.write(""{}\n"".format(""\n"".join(rows)))
","if ""\b"" in row :",147
"def test_kafka_consumer(self):
    self.send_messages(0, range(0, 100))
    self.send_messages(1, range(100, 200))
    # Start a consumer
    consumer = self.kafka_consumer(auto_offset_reset=""earliest"")
    n = 0
    messages = {0: set(), 1: set()}
    for m in consumer:
        logging.debug(""Consumed message %s"" % repr(m))
        n += 1
        messages[m.partition].add(m.offset)
        if n >= 200:
            break
    self.assertEqual(len(messages[0]), 100)
    self.assertEqual(len(messages[1]), 100)
",if n >= 200 :,180
"def get_command(scaffolding, command_path):
    path, _, command_name = command_path.rpartition(""."")
    if path not in scaffolding:
        raise KeyError('Ingredient for command ""%s"" not found.' % command_path)
    if command_name in scaffolding[path].commands:
        return scaffolding[path].commands[command_name]
    else:
        if path:
            raise KeyError(
                'Command ""%s"" not found in ingredient ""%s""' % (command_name, path)
            )
        else:
            raise KeyError('Command ""%s"" not found' % command_name)
",if path :,167
"def build_extension(self, ext):
    ext._convert_pyx_sources_to_lang()
    _compiler = self.compiler
    try:
        if isinstance(ext, Library):
            self.compiler = self.shlib_compiler
        _build_ext.build_extension(self, ext)
        if ext._needs_stub:
            cmd = self.get_finalized_command(""build_py"").build_lib
            self.write_stub(cmd, ext)
    finally:
        self.compiler = _compiler
","if isinstance ( ext , Library ) :",134
"def _send_payload(self, payload):
    req = eventlet_urllib2.Request(self._url, headers=payload[1])
    try:
        if sys.version_info < (2, 6):
            response = eventlet_urllib2.urlopen(req, payload[0]).read()
        else:
            response = eventlet_urllib2.urlopen(req, payload[0], self.timeout).read()
        return response
    except Exception as err:
        return err
","if sys . version_info < ( 2 , 6 ) :",122
"def get_access_token(self, callback):
    if not self.is_authorized():
        callback(None)
    else:
        access_token = config.persist[""oauth_access_token""]
        access_token_expires = config.persist[""oauth_access_token_expires""]
        if access_token and time.time() < access_token_expires:
            callback(access_token)
        else:
            self.forget_access_token()
            self.refresh_access_token(callback)
",if access_token and time . time ( ) < access_token_expires :,131
"def mark_first_parents(event):
    """"""Mark the node and all its parents.""""""
    c = event.get(""c"")
    if not c:
        return
    changed = []
    for parent in c.p.self_and_parents():
        if not parent.isMarked():
            parent.v.setMarked()
            parent.setAllAncestorAtFileNodesDirty()
            changed.append(parent.copy())
    if changed:
        # g.es(""marked: "" + ', '.join([z.h for z in changed]))
        c.setChanged()
        c.redraw()
    return changed
",if not parent . isMarked ( ) :,162
"def normalize_reg_path(self, path):
    new = path
    if path:
        roots = (""\\registry\\machine\\"", ""hklm\\"")
        for r in roots:
            if path.lower().startswith(r):
                new = ""HKEY_LOCAL_MACHINE\\"" + path[len(r) :]
                return new
    return path
",if path . lower ( ) . startswith ( r ) :,94
"def extract_labels(filename, one_hot=False):
    """"""Extract the labels into a 1D uint8 numpy array [index].""""""
    print(""Extracting"", filename)
    with gzip.open(filename) as bytestream:
        magic = _read32(bytestream)
        if magic != 2049:
            raise ValueError(
                ""Invalid magic number %d in MNIST label file: %s"" % (magic, filename)
            )
        num_items = _read32(bytestream)
        buf = bytestream.read(num_items)
        labels = numpy.frombuffer(buf, dtype=numpy.uint8)
        if one_hot:
            return dense_to_one_hot(labels)
        return labels
",if one_hot :,180
"def on_change(self, data):
    # loop over tp_clipboard views
    for window in sublime.windows():
        for view in window.views():
            if view.get_status(""inactive"") and view.settings().get(""tp_append"", False):
                file_name = view.file_name()
                # ammo
                if view.settings().get(""tp_ammo"", False):
                    self.update(view)
                elif file_name and file_name.endswith(
                    global_settings(""ammo_file_extension"", "".ammo"")
                ):
                    self.update(view)
","if view . settings ( ) . get ( ""tp_ammo"" , False ) :",175
"def list(self, items, columns=4, width=80):
    items = list(sorted(items))
    colw = width // columns
    rows = (len(items) + columns - 1) // columns
    for row in range(rows):
        for col in range(columns):
            i = col * rows + row
            if i < len(items):
                self.output.write(items[i])
                if col < columns - 1:
                    self.output.write("" "" + "" "" * (colw - 1 - len(items[i])))
        self.output.write(""\n"")
",if i < len ( items ) :,158
"def test_dynamic_section_solaris(self):
    """"""Verify that we can parse relocations from the .dynamic section""""""
    test_dir = os.path.join(""test"", ""testfiles_for_unittests"")
    with open(os.path.join(test_dir, ""exe_solaris32_cc.elf""), ""rb"") as f:
        elff = ELFFile(f)
        for sect in elff.iter_sections():
            if isinstance(sect, DynamicSection):
                relos = sect.get_relocation_tables()
                self.assertEqual(set(relos), {""JMPREL"", ""REL""})
","if isinstance ( sect , DynamicSection ) :",160
"def close(self, checkcount=False):
    self.mutex.acquire()
    try:
        if checkcount:
            self.openers -= 1
            if self.openers == 0:
                self.do_close()
        else:
            if self.openers > 0:
                self.do_close()
            self.openers = 0
    finally:
        self.mutex.release()
",if self . openers > 0 :,116
"def subcommand_table(self):
    if self._subcommand_table is None:
        if self._topic_tag_db is None:
            self._topic_tag_db = TopicTagDB()
        self._topic_tag_db.load_json_index()
        self._subcommand_table = self._create_subcommand_table()
    return self._subcommand_table
",if self . _topic_tag_db is None :,89
"def layer_init(self):
    for layer in self.cnn:  # type: ignore
        if isinstance(layer, (nn.Conv2d, nn.Linear)):
            nn.init.kaiming_normal_(layer.weight, nn.init.calculate_gain(""relu""))
            if layer.bias is not None:
                nn.init.constant_(layer.bias, val=0)
",if layer . bias is not None :,99
"def _append_modifier(code, modifier):
    if modifier == ""euro"":
        if ""."" not in code:
            return code + "".ISO8859-15""
        _, _, encoding = code.partition(""."")
        if encoding in (""ISO8859-15"", ""UTF-8""):
            return code
        if encoding == ""ISO8859-1"":
            return _replace_encoding(code, ""ISO8859-15"")
    return code + ""@"" + modifier
","if encoding == ""ISO8859-1"" :",115
"def set_mean(self, mean):
    if mean is not None:
        # mean value, may be one value per channel
        if mean.ndim == 1:
            mean = mean[:, np.newaxis, np.newaxis]
        else:
            # elementwise mean
            if self.is_color:
                assert len(mean.shape) == 3
    self.mean = mean
",if mean . ndim == 1 :,101
"def _set_state(self, value):
    if self._pwm:
        try:
            value = int(value * self._connection.get_PWM_range(self._number))
            if value != self._connection.get_PWM_dutycycle(self._number):
                self._connection.set_PWM_dutycycle(self._number, value)
        except pigpio.error:
            raise PinInvalidState('invalid state ""%s"" for pin %r' % (value, self))
    elif self.function == ""input"":
        raise PinSetInput(""cannot set state of pin %r"" % self)
    else:
        # write forces pin to OUTPUT, hence the check above
        self._connection.write(self._number, bool(value))
",if value != self . _connection . get_PWM_dutycycle ( self . _number ) :,192
"def do_stop(self):
    logger.info(""[%s] Stopping all workers"", self.name)
    for w in self.workers.values():
        try:
            w.terminate()
            w.join(timeout=1)
        # A already dead worker or in a worker
        except (AttributeError, AssertionError):
            pass
    # Close the server socket if it was opened
    if self.http_daemon:
        if self.brok_interface:
            self.http_daemon.unregister(self.brok_interface)
        if self.scheduler_interface:
            self.http_daemon.unregister(self.scheduler_interface)
    # And then call our master stop from satellite code
    super(Satellite, self).do_stop()
",if self . scheduler_interface :,191
"def iter_input(input, filename, parser, line_by_line):
    if isinstance(input, basestring):
        with open(input, ""rb"") as f:
            for tree in iter_input(f, filename, parser, line_by_line):
                yield tree
    else:
        try:
            if line_by_line:
                for line in input:
                    if line:
                        yield et.ElementTree(et.fromstring(line, parser))
            else:
                yield et.parse(input, parser)
        except IOError:
            e = sys.exc_info()[1]
            error(""parsing %r failed: %s: %s"", filename, e.__class__.__name__, e)
",if line_by_line :,197
"def debug_print(data: json):
    try:
        print(""[+] ---Debug info---"")
        for i, v in data.items():
            if i == ""outline"":
                print(""[+]  -"", i, ""    :"", len(v), ""characters"")
                continue
            if i == ""actor_photo"" or i == ""year"":
                continue
            print(""[+]  -"", ""%-11s"" % i, "":"", v)
        print(""[+] ---Debug info---"")
    except:
        pass
","if i == ""actor_photo"" or i == ""year"" :",138
"def deliver_event(self):
    while True:
        client = self._client()
        if client is None:
            return  # weakref is dead, nothing to deliver
        diff = self._due - client.loop.time()
        if diff <= 0:
            # We've hit our due time, deliver event. It won't respect
            # sequential updates but fixing that would just worsen this.
            await client._dispatch_event(self._event)
            return
        del client  # Clear ref and sleep until our due time
        await asyncio.sleep(diff)
",if client is None :,147
"def pluginload(bot, event, *args):
    """"""loads a previously unloaded plugin, requires plugins. prefix""""""
    if args:
        module_path = args[0]
        try:
            if plugins.load(bot, module_path):
                message = ""<b><pre>{}</pre>: loaded</b>"".format(module_path)
            else:
                message = ""<b><pre>{}</pre>: failed</b>"".format(module_path)
        except RuntimeError as e:
            message = ""<b><pre>{}</pre>: <pre>{}</pre></b>"".format(module_path, str(e))
    else:
        message = ""<b>module path required</b>""
    yield from bot.coro_send_message(event.conv_id, message)
","if plugins . load ( bot , module_path ) :",192
"def validate_prompt_lb(hostname):
    # Run the standard hostname check first:
    hostname = validate_prompt_hostname(hostname)
    # Make sure this host wasn't already specified:
    for host in hosts:
        if host.connect_to == hostname and (host.is_master() or host.is_node()):
            raise click.BadParameter(
                'Cannot re-use ""%s"" as a load balancer, '
                ""please specify a separate host"" % hostname
            )
    return hostname
",if host . connect_to == hostname and ( host . is_master ( ) or host . is_node ( ) ) :,129
"def alter_inventory(session, resource, amount):
    """"""Alters the inventory of each settlement.""""""
    # NOTE avoid circular import
    from horizons.component.storagecomponent import StorageComponent
    for settlement in session.world.settlements:
        if settlement.owner == session.world.player and settlement.warehouse:
            settlement.warehouse.get_component(StorageComponent).inventory.alter(
                resource, amount
            )
",if settlement . owner == session . world . player and settlement . warehouse :,109
"def _(value):
    if kb.customInjectionMark in (value or """"):
        if payload is None:
            value = value.replace(kb.customInjectionMark, """")
        else:
            value = re.sub(r""\w*%s"" % re.escape(kb.customInjectionMark), payload, value)
    return value
",if payload is None :,86
"def __call__(self, target):
    if ""weights"" not in target.temp:
        return True
    targets = target.temp[""weights""]
    for cname in target.children:
        if cname in targets:
            c = target.children[cname]
            deviation = abs((c.weight - targets[cname]) / targets[cname])
            if deviation > self.tolerance:
                return True
    if ""cash"" in target.temp:
        cash_deviation = abs(
            (target.capital - targets.value) / targets.value - target.temp[""cash""]
        )
        if cash_deviation > self.tolerance:
            return True
    return False
",if cash_deviation > self . tolerance :,178
"def splitroot(self, part, sep=sep):
    if part and part[0] == sep:
        stripped_part = part.lstrip(sep)
        # According to POSIX path resolution:
        # http://pubs.opengroup.org/onlinepubs/009695399/basedefs/xbd_chap04.html#tag_04_11
        # ""A pathname that begins with two successive slashes may be
        # interpreted in an implementation-defined manner, although more
        # than two leading slashes shall be treated as a single slash"".
        if len(part) - len(stripped_part) == 2:
            return """", sep * 2, stripped_part
        else:
            return """", sep, stripped_part
    else:
        return """", """", part
",if len ( part ) - len ( stripped_part ) == 2 :,190
"def _Determine_Do(self):
    self.applicable = 1
    method = ""moz-src""
    method_arg = None
    for opt, optarg in self.chosenOptions:
        if opt == ""--moz-src"":
            method = ""moz-src""
        elif opt == ""--moz-objdir"":
            method = ""moz-objdir""
            method_arg = optarg
    if method == ""moz-src"":
        self.value = self._get_mozilla_objdir()
    elif method == ""moz-objdir"":
        self.value = self._use_mozilla_objdir(method_arg)
    else:
        raise black.configure.ConfigureError(""bogus method: %r"" % method)
    self.determined = 1
","if opt == ""--moz-src"" :",188
"def is_filtered_inherited_member(name: str, obj: Any) -> bool:
    if inspect.isclass(self.object):
        for cls in self.object.__mro__:
            if cls.__name__ == self.options.inherited_members and cls != self.object:
                # given member is a member of specified *super class*
                return True
            elif name in cls.__dict__:
                return False
            elif name in self.get_attr(cls, ""__annotations__"", {}):
                return False
            elif isinstance(obj, ObjectMember) and obj.class_ is cls:
                return False
    return False
","elif name in self . get_attr ( cls , ""__annotations__"" , { } ) :",167
"def _remove_all_greasemonkey_scripts(self):
    page_scripts = self._widget.page().scripts()
    for script in page_scripts.toList():
        if script.name().startswith(""GM-""):
            log.greasemonkey.debug(""Removing script: {}"".format(script.name()))
            removed = page_scripts.remove(script)
            assert removed, script.name()
","if script . name ( ) . startswith ( ""GM-"" ) :",100
"def merge_intervals(intervals):
    """"""Merge intervals in the form of a list.""""""
    if intervals is None:
        return None
    intervals.sort(key=lambda i: i[0])
    out = [intervals.pop(0)]
    for i in intervals:
        if out[-1][-1] >= i[0]:
            out[-1][-1] = max(out[-1][-1], i[-1])
        else:
            out.append(i)
    return out
",if out [ - 1 ] [ - 1 ] >= i [ 0 ] :,122
"def __setattr__(self, key, val):
    self.__dict__[key] = val
    self.__dict__[key.upper()] = val
    levels = key.split(""."")
    last_level = len(levels) - 1
    pointer = self._pointer
    if len(levels) > 1:
        for i, l in enumerate(levels):
            if hasattr(self, l) and isinstance(getattr(self, l), Config):
                setattr(getattr(self, l), ""."".join(levels[i:]), val)
            if l == last_level:
                pointer[l] = val
            else:
                pointer = pointer[l]
","if hasattr ( self , l ) and isinstance ( getattr ( self , l ) , Config ) :",165
"def get_menu_title(self):
    handle = self.obj.get_handle()
    if handle:
        who = get_participant_from_event(self.db, handle)
        desc = self.obj.get_description()
        event_name = self.obj.get_type()
        if desc:
            event_name = ""%s - %s"" % (event_name, desc)
        if who:
            event_name = ""%s - %s"" % (event_name, who)
        dialog_title = _(""Event: %s"") % event_name
    else:
        dialog_title = _(""New Event"")
    return dialog_title
",if desc :,168
"def perform_initialization(m):
    if isinstance(m, self.initialize_layers):
        if initialization_method is not None:
            initialization_method(m.weight.data, **initialization_kwargs)
        if (
            m.bias is not None
            and self.initialize_bias != ""No""
            and initialization_method_bias is not None
        ):
            try:
                initialization_method_bias(m.bias.data, **initialization_kwargs_bias)
            except ValueError:
                pass
",if initialization_method is not None :,145
"def forward(self, inputs):
    x = inputs[""image""]
    out = self.conv0(x)
    out = self.downsample0(out)
    blocks = []
    for i, conv_block_i in enumerate(self.darknet_conv_block_list):
        out = conv_block_i(out)
        if i == self.freeze_at:
            out.stop_gradient = True
        if i in self.return_idx:
            blocks.append(out)
        if i < self.num_stages - 1:
            out = self.downsample_list[i](out)
    return blocks
",if i in self . return_idx :,159
"def _urlvars__set(self, value):
    environ = self.environ
    if ""wsgiorg.routing_args"" in environ:
        environ[""wsgiorg.routing_args""] = (environ[""wsgiorg.routing_args""][0], value)
        if ""paste.urlvars"" in environ:
            del environ[""paste.urlvars""]
    elif ""paste.urlvars"" in environ:
        environ[""paste.urlvars""] = value
    else:
        environ[""wsgiorg.routing_args""] = ((), value)
","if ""paste.urlvars"" in environ :",134
"def forward(self, x, activate=True, norm=True):
    for layer in self.order:
        if layer == ""conv"":
            if self.with_explicit_padding:
                x = self.padding_layer(x)
            x = self.conv(x)
        elif layer == ""norm"" and norm and self.with_norm:
            x = self.norm(x)
        elif layer == ""act"" and activate and self.with_activation:
            x = self.activate(x)
    return x
","elif layer == ""act"" and activate and self . with_activation :",138
"def add(self, entry):
    if not self._find_entry(entry, filters=False):
        show = self.add_show(entry)
        if show:
            self._shows = None
            log.verbose(""Successfully added show %s to Sonarr"", show[""title""])
    else:
        log.debug(""entry %s already exists in Sonarr list"", entry)
",if show :,99
"def __eq__(self, other):
    if not isinstance(other, Result):
        return False
    equal = self.info == other.info
    equal &= self.stats == other.stats
    equal &= self.trajectories == other.trajectories
    for k in self.np_arrays:
        if k not in other.np_arrays:
            equal &= False
            break
        if not equal:
            break
        equal &= all([np.array_equal(self.np_arrays[k], other.np_arrays[k])])
    return equal
",if not equal :,138
"def handle_server_api(output, kwargs):
    """"""Special handler for API-call 'set_config' [servers]""""""
    name = kwargs.get(""keyword"")
    if not name:
        name = kwargs.get(""name"")
    if name:
        server = config.get_config(""servers"", name)
        if server:
            server.set_dict(kwargs)
            old_name = name
        else:
            config.ConfigServer(name, kwargs)
            old_name = None
        sabnzbd.Downloader.update_server(old_name, name)
    return name
",if server :,155
"def extractNames(self, names):
    offset = names[""offset""].value
    for header in names.array(""header""):
        key = header[""nameID""].value
        foffset = offset + header[""offset""].value
        field = names.getFieldByAddress(foffset * 8)
        if not field or not isString(field):
            continue
        value = field.value
        if key not in self.NAMEID_TO_ATTR:
            continue
        key = self.NAMEID_TO_ATTR[key]
        if key == ""version"" and value.startswith(u""Version ""):
            # ""Version 1.2"" => ""1.2""
            value = value[8:]
        setattr(self, key, value)
",if not field or not isString ( field ) :,189
"def api_read(self):
    files = []
    files.append(""/bin/netcat"")
    files.append(""/etc/alternative/netcat"")
    files.append(""/bin/nc"")
    #     init variables
    installed = False
    support = False
    path = None
    for _file in files:
        file_content = self.shell.read(_file)
        if file_content:
            installed = True
            path = _file
            if ""-e filename"" in file_content:
                support = True
            break
    result = {
        ""netcat_installed"": installed,
        ""supports_shell_bind"": support,
        ""path"": path,
    }
    return result
","if ""-e filename"" in file_content :",187
"def _get_iscsi_portal(self, netspace):
    for netpsace_interface in netspace.get_ips():
        if netpsace_interface.enabled:
            port = netspace.get_properties().iscsi_tcp_port
            return ""%s:%s"" % (netpsace_interface.ip_address, port)
    # if we get here it means there are no enabled ports
    msg = _(""No available interfaces in iSCSI network space %s"") % netspace.get_name()
    raise exception.VolumeDriverException(message=msg)
",if netpsace_interface . enabled :,140
"def show(self):
    if len(self.figures.keys()) == 0:
        return
    if not SETTINGS.plot_split:
        if SETTINGS.plot_backend.lower() == ""qt4agg"":
            self.tabbed_qt4_window()
        elif SETTINGS.plot_backend.lower() == ""qt5agg"":
            self.tabbed_qt5_window()
        elif SETTINGS.plot_backend.lower() == ""tkagg"":
            self.tabbed_tk_window()
        else:
            plt.show()
    else:
        plt.show()
","elif SETTINGS . plot_backend . lower ( ) == ""tkagg"" :",161
"def _update_decommissioned_icon(self):
    """"""Add or remove decommissioned icon.""""""
    if not self.instance.has_status_icon:
        return
    if self.is_active() is not self.__active:
        self.__active = not self.__active
        if self.__active:
            RemoveStatusIcon.broadcast(self, self.instance, DecommissionedStatus)
        else:
            self._add_status_icon(DecommissionedStatus(self.instance))
",if self . __active :,127
"def _count(self, element, count=True):
    if not isinstance(element, six.string_types):
        if self == element:
            return 1
    i = 0
    for child in self.children:
        # child is text content and element is also text content, then
        # make a simple ""text"" in ""text""
        if isinstance(child, six.string_types):
            if isinstance(element, six.string_types):
                if count:
                    i += child.count(element)
                elif element in child:
                    return 1
        else:
            i += child._count(element, count=count)
            if not count and i:
                return i
    return i
","if isinstance ( child , six . string_types ) :",196
"def test_read_lazy(self):
    want = b""x"" * 100
    telnet = test_telnet([want])
    self.assertEqual(b"""", telnet.read_lazy())
    data = b""""
    while True:
        try:
            read_data = telnet.read_lazy()
            data += read_data
            if not read_data:
                telnet.fill_rawq()
        except EOFError:
            break
        self.assertTrue(want.startswith(data))
    self.assertEqual(data, want)
",if not read_data :,143
"def getprefs(path=PREFSFILENAME):
    if not os.path.exists(path):
        f = open(path, ""w"")
        f.write(default_prefs)
        f.close()
    f = open(path)
    lines = f.readlines()
    prefs = {}
    for line in lines:
        if line[-1:] == ""\n"":
            line = line[:-1]
        try:
            name, value = re.split("":"", line, 1)
            prefs[string.strip(name)] = eval(value)
        except:
            pass
    return prefs
","if line [ - 1 : ] == ""\n"" :",155
"def connect(self):
    while True:
        errno = self.sock.connect_ex(self.addr)
        if not errno:
            # connected immediately.
            break
        elif errno == EINPROGRESS:
            # will be connected.
            break
        elif errno == ENOENT:
            # no such socket file.
            self.create_connection(self.failover_interval)
            return
        else:
            raise ValueError(""Unexpected socket errno: %d"" % errno)
    self.event_loop.watch_file(self.sock.fileno(), self.handle)
",if not errno :,157
"def set_enabled_addons(file_path, addons, comment=None):
    with codecs.open(file_path, ""w"", ""utf-8"") as f:
        if comment:
            f.write(""# %s\n\n"" % comment)
        for addon in addons:
            f.write(""%s\n"" % addon)
",if comment :,91
"def check_interfaceinNetWorkManager(self, interface):
    """"""check if interface is already in file config""""""
    mac = Refactor.get_interface_mac(interface)
    if mac != None:
        if mac in open(self.mn_path, ""r"").read():
            return True
        if interface in open(self.mn_path, ""r"").read():
            return True
    return False
","if interface in open ( self . mn_path , ""r"" ) . read ( ) :",102
"def spaceless(writer, node):
    original = writer.spaceless
    writer.spaceless = True
    writer.warn(""entering spaceless mode with different semantics"", node)
    # do the initial stripping
    nodelist = list(node.nodelist)
    if nodelist:
        if isinstance(nodelist[0], TextNode):
            nodelist[0] = TextNode(nodelist[0].s.lstrip())
        if isinstance(nodelist[-1], TextNode):
            nodelist[-1] = TextNode(nodelist[-1].s.rstrip())
    writer.body(nodelist)
    writer.spaceless = original
","if isinstance ( nodelist [ 0 ] , TextNode ) :",146
"def TryMerge(self, d):
    while d.avail() > 0:
        tt = d.getVarInt32()
        if tt == 10:
            self.set_app_id(d.getPrefixedString())
            continue
        if tt == 18:
            self.set_queue_name(d.getPrefixedString())
            continue
        if tt == 24:
            self.set_pause(d.getBoolean())
            continue
        if tt == 0:
            raise ProtocolBuffer.ProtocolBufferDecodeError
        d.skipData(tt)
",if tt == 18 :,152
"def group_re(self):
    """"""Return a regexp pattern with named groups""""""
    out = """"
    for token, data in self.tokens():
        if token == ""TXT"":
            out += re.escape(data)
        elif token == ""VAR"":
            out += ""(?P<%s>%s)"" % (data[1], data[0])
        elif token == ""ANON"":
            out += ""(?:%s)"" % data
    return out
","elif token == ""VAR"" :",114
"def wrap_in(input):
    if isinstance(input, (SymbolicInput)):
        return input
    elif isinstance(input, gof.Variable):
        # r -> SymbolicInput(variable=r)
        return SymbolicInput(input)
    elif isinstance(input, (list, tuple)):
        # (r, u) -> SymbolicInput(variable=r, update=u)
        if len(input) == 2:
            return SymbolicInput(input[0], update=input[1])
        else:
            raise TypeError(""Expected two elements in the list or tuple."", input)
    else:
        raise TypeError(
            ""Unknown input type: %s (%s), expected Variable "" ""instance"",
            type(input),
            input,
        )
",if len ( input ) == 2 :,195
"def _remove_event(self, event):
    # Find event according to its timestamp.
    # Index returned should be one behind.
    i = bisect.bisect(self._eventq, event)
    # Having two events with identical timestamp is unlikely but possible.
    # I am going to move forward and compare timestamp AND object address
    # to make sure the correct object is found.
    while i > 0:
        i -= 1
        e = self._eventq[i]
        if e.timestamp != event.timestamp:
            raise exception.EventNotFound(event)
        elif id(e) == id(event):
            self._eventq.pop(i)
            return
    raise exception.EventNotFound(event)
",if e . timestamp != event . timestamp :,177
"def cron_starter(*args: Any) -> None:
    _tz = self.conf.timezone if timezone is None else timezone
    while not self.should_stop:
        await self.sleep(cron.secs_for_next(cron_format, _tz))
        if not self.should_stop:
            should_run = not on_leader or self.is_leader()
            if should_run:
                with self.trace(shortlabel(fun), trace_enabled=traced):
                    await fun(*args)
",if should_run :,139
"def _find_boundary(self):
    ct_info = tuple(x.strip() for x in self.content_type.split("";""))
    mimetype = ct_info[0]
    if mimetype.split(""/"")[0].lower() != ""multipart"":
        raise NonMultipartContentTypeException(
            ""Unexpected mimetype in content-type: '{0}'"".format(mimetype)
        )
    for item in ct_info[1:]:
        attr, value = _split_on_find(item, ""="")
        if attr.lower() == ""boundary"":
            self.boundary = encode_with(value.strip('""'), self.encoding)
","if attr . lower ( ) == ""boundary"" :",151
"def get_kwarg_or_param(request, kwargs, key):
    value = None
    try:
        value = kwargs[key]
    except KeyError:
        if request.method == ""GET"":
            value = request.GET.get(key)
        elif request.method == ""POST"":
            value = request.POST.get(key)
    return value
","elif request . method == ""POST"" :",93
"def _gather_async_results(self, result: Result, source: typing.Any) -> None:
    try:
        context = result[""context""]
        context[""is_refresh""] = False
        context[""vars""] = self._vim.vars
        async_candidates = source.gather_candidates(context)
        context[""vars""] = None
        result[""is_async""] = context[""is_async""]
        if async_candidates is None:
            return
        context[""candidates""] += convert2candidates(async_candidates)
    except Exception as exc:
        self._handle_source_exception(source, exc)
",if async_candidates is None :,155
"def _check_session(self, session, action):
    if session is None:
        if isinstance(self, tuple):
            key = self[0].key
        else:
            key = self.key
        raise ValueError(
            f""Tileable object {key} must be executed first before {action}""
        )
","if isinstance ( self , tuple ) :",84
"def update(self, dict=None, **kwargs):
    if self._pending_removals:
        self._commit_removals()
    d = self.data
    if dict is not None:
        if not hasattr(dict, ""items""):
            dict = type({})(dict)
        for key, o in dict.items():
            d[key] = KeyedRef(o, self._remove, key)
    if len(kwargs):
        self.update(kwargs)
","if not hasattr ( dict , ""items"" ) :",121
"def get_sigma(self):
    if self.wants_automatic_sigma.value:
        #
        # Constants here taken from FindEdges.m
        #
        if self.method == M_CANNY:
            return 1.0
        elif self.method == M_LOG:
            return 2.0
        else:
            raise NotImplementedError(
                ""Automatic sigma not supported for method %s."" % self.method.value
            )
    else:
        return self.sigma.value
",elif self . method == M_LOG :,136
"def forward(self, x, activate=True, norm=True):
    for layer in self.order:
        if layer == ""conv"":
            if self.with_explicit_padding:
                x = self.padding_layer(x)
            x = self.conv(x)
        elif layer == ""norm"" and norm and self.with_norm:
            x = self.norm(x)
        elif layer == ""act"" and activate and self.with_activation:
            x = self.activate(x)
    return x
","if layer == ""conv"" :",138
"def _grouping_intervals(grouping):
    last_interval = None
    for interval in grouping:
        # if grouping is -1, we are done
        if interval == CHAR_MAX:
            return
        # 0: re-use last group ad infinitum
        if interval == 0:
            if last_interval is None:
                raise ValueError(""invalid grouping"")
            while True:
                yield last_interval
        yield interval
        last_interval = interval
",if interval == CHAR_MAX :,124
"def iterRelativeExportCFiles(basepath):
    for root, dirs, files in os.walk(basepath, topdown=True):
        for directory in dirs:
            if isAddonDirectoryIgnored(directory):
                dirs.remove(directory)
        for filename in files:
            if not isExportCFileIgnored(filename):
                fullpath = os.path.join(root, filename)
                yield os.path.relpath(fullpath, basepath)
",if isAddonDirectoryIgnored ( directory ) :,117
"def TryMerge(self, d):
    while d.avail() > 0:
        tt = d.getVarInt32()
        if tt == 10:
            self.add_application_key(d.getPrefixedString())
            continue
        if tt == 18:
            self.set_tag(d.getPrefixedString())
            continue
        if tt == 0:
            raise ProtocolBuffer.ProtocolBufferDecodeError
        d.skipData(tt)
",if tt == 10 :,122
"def TryMerge(self, d):
    while d.avail() > 0:
        tt = d.getVarInt32()
        if tt == 8:
            self.set_format(d.getVarInt32())
            continue
        if tt == 18:
            self.set_path(d.getPrefixedString())
            continue
        if tt == 0:
            raise ProtocolBuffer.ProtocolBufferDecodeError
        d.skipData(tt)
",if tt == 18 :,120
"def _get_future_trading_minutes(self, trading_date):
    trading_minutes = set()
    universe = self._get_universe()
    for order_book_id in universe:
        if self._env.get_account_type(order_book_id) == DEFAULT_ACCOUNT_TYPE.STOCK:
            continue
        trading_minutes.update(
            self._env.data_proxy.get_trading_minutes_for(order_book_id, trading_date)
        )
    return set([convert_int_to_datetime(minute) for minute in trading_minutes])
",if self . _env . get_account_type ( order_book_id ) == DEFAULT_ACCOUNT_TYPE . STOCK :,154
"def helper(chunk: Any) -> Any:
    nonlocal counter
    if not isinstance(chunk, dict):
        return chunk
    if len(chunk) <= 2:
        return chunk
    id = hash(str(chunk))
    if id in cache:
        return cache[id]
    else:
        cache[id] = {"".id"": counter}
        chunk["".cache_id""] = counter
        counter += 1
    for name in sorted(chunk.keys()):
        value = chunk[name]
        if isinstance(value, list):
            chunk[name] = [helper(child) for child in value]
        elif isinstance(value, dict):
            chunk[name] = helper(value)
    return chunk
","if isinstance ( value , list ) :",180
"def _render_lang_List(self, element):
    with self.buffer.foldable_lines():
        self.buffer.write(""["", style=self.styles.bracket)
        item_count = len(element.items)
        if item_count:
            with self.buffer.indent():
                for idx, item in enumerate(element.items):
                    self._render(item)
                    if idx < (item_count - 1):
                        self.buffer.write("","")
                        self.buffer.mark_line_break()
        if element.trimmed:
            self.buffer.write(""..."")
        self.buffer.write(""]"", style=self.styles.bracket)
",if element . trimmed :,183
"def test_parse_query_params_matchable_field(self):
    query_params = {
        ""filter[string_field][contains]"": ""foo"",
        ""filter[string_field][icontains]"": ""bar"",
    }
    fields = self.view.parse_query_params(query_params)
    for key, field_name in fields.items():
        if field_name[""string_field""][""op""] == ""contains"":
            assert_equal(field_name[""string_field""][""value""], ""foo"")
        elif field_name[""string_field""][""op""] == ""icontains"":
            assert_equal(field_name[""string_field""][""value""], ""bar"")
        else:
            self.fail()
","elif field_name [ ""string_field"" ] [ ""op"" ] == ""icontains"" :",179
"def on_www_authenticate(data=None):
    io_loop.remove_timeout(timeout[0])
    if data:
        scheme = re.findall(""WWW-Authenticate: ([^\s]+)"", data)[0].strip()
        logging.debug(""rtsp netcam auth scheme: %s"" % scheme)
        if scheme.lower() == ""basic"":
            send_auth[0] = True
            connect()
        else:
            logging.debug(
                ""rtsp auth scheme digest not supported, considering credentials ok""
            )
            handle_success(""(unknown) "")
    else:
        logging.error(""timeout waiting for rtsp auth scheme"")
        handle_error(""timeout waiting for rtsp netcam response"")
","if scheme . lower ( ) == ""basic"" :",186
"def receive(debug=debug):
    if should_shutdown and should_shutdown():
        debug(""worker got sentinel -- exiting"")
        raise SystemExit(EX_OK)
    try:
        ready, req = _receive(1.0)
        if not ready:
            return None
    except (EOFError, IOError) as exc:
        if get_errno(exc) == errno.EINTR:
            return None  # interrupted, maybe by gdb
        debug(""worker got %s -- exiting"", type(exc).__name__)
        raise SystemExit(EX_FAILURE)
    if req is None:
        debug(""worker got sentinel -- exiting"")
        raise SystemExit(EX_FAILURE)
    return req
",if not ready :,173
"def test_all(self):
    raw = [r for r in self.map._revision_map.values() if r is not None]
    revs = [rev for rev in self.map.iterate_revisions(""heads"", ""base"")]
    eq_(set(raw), set(revs))
    for idx, rev in enumerate(revs):
        ancestors = set(self.map._get_ancestor_nodes([rev])).difference([rev])
        descendants = set(self.map._get_descendant_nodes([rev])).difference([rev])
        assert not ancestors.intersection(descendants)
        remaining = set(revs[idx + 1 :])
        if remaining:
            assert remaining.intersection(ancestors)
",if remaining :,165
"def is_issue(self, node):
    first = node.children[0]
    if first.type == ""string"" and self._normalizer.version >= (3, 0):
        first_is_bytes = self._is_bytes_literal(first)
        for string in node.children[1:]:
            if first_is_bytes != self._is_bytes_literal(string):
                return True
",if first_is_bytes != self . _is_bytes_literal ( string ) :,101
"def elements(registry):
    """"""Given a resource registry return sorted de-aliased values.""""""
    seen = {}
    for k, v in registry.items():
        if k in (""and"", ""or"", ""not""):
            continue
        if v in seen:
            continue
        else:
            seen[ElementSchema.name(v)] = v
    return [seen[k] for k in sorted(seen)]
","if k in ( ""and"" , ""or"" , ""not"" ) :",105
"def make_pattern(wtree):
    subpattern = []
    for part in wtree[1:-1]:
        if isinstance(part, list):
            part = make_pattern(part)
        elif wtree[0] != """":
            for c in part:
                # Meta-characters cannot be quoted
                if c in special_chars:
                    raise GlobError()
        subpattern.append(part)
    return """".join(subpattern)
",if c in special_chars :,123
"def check_if_list_contain_duplicates(item: list, depth: int) -> None:
    try:
        if len(item) != len(set(item)):
            print(Fore.RED + ""Rule {} has duplicate filters"".format(file))
            files_with_duplicate_filters.append(file)
    except:
        # unhashable types like dictionaries
        for sub_item in item:
            if type(sub_item) == dict and depth <= MAX_DEPTH:
                check_list_or_recurse_on_dict(sub_item, depth + 1)
",if type ( sub_item ) == dict and depth <= MAX_DEPTH :,148
"def PrintHighlighted(self, out):
    from doctools import make_help
    pos = self.start_pos
    for line_end in Lines(self.s, self.start_pos, self.end_pos):
        # NOTE: HighlightLine accepts an HTML ESCAPED line.  It's valid to just
        # add tags and leave everything alone.
        line = self.s[pos:line_end]
        html_line = make_help.HighlightLine(self.lang, line)
        if html_line is not None:
            out.PrintUntil(pos)
            out.Print(html_line)
            out.SkipTo(line_end)
        pos = line_end
",if html_line is not None :,179
"def closeEvent(self, e=None):
    """"""Save settings and remove registered logging handler""""""
    if self.editor.isModified():
        # ask if user wants to save
        if self.wants_save():
            if self.save():
                e.accept()
            else:
                # saving error or user canceled
                e.ignore()
        else:
            # discard changes
            e.accept()
    else:
        # unchanged
        e.accept()
",if self . wants_save ( ) :,133
"def readlines(self, hint=None):
    if self.chunked_input:
        lines = []
        for line in iter(self.readline, b""""):
            lines.append(line)
            if hint and hint > 0:
                hint -= len(line)
                if hint <= 0:
                    break
        return lines
    else:
        return self._do_read(self.rfile.readlines, hint)
",if hint <= 0 :,117
"def test_prod(self):
    with gpytorch.settings.fast_computations(covar_root_decomposition=False):
        lazy_tensor = self.create_lazy_tensor()
        evaluated = self.evaluate_lazy_tensor(lazy_tensor)
        if lazy_tensor.ndimension() > 2:
            self.assertAllClose(
                lazy_tensor.prod(-3).evaluate(),
                evaluated.prod(-3),
                **self.tolerances[""prod""]
            )
        if lazy_tensor.ndimension() > 3:
            self.assertAllClose(
                lazy_tensor.prod(-4).evaluate(),
                evaluated.prod(-4),
                **self.tolerances[""prod""]
            )
",if lazy_tensor . ndimension ( ) > 2 :,195
"def make_module_translation_map(names: List[str]) -> Dict[str, str]:
    num_instances = {}  # type: Dict[str, int]
    for name in names:
        for suffix in candidate_suffixes(name):
            num_instances[suffix] = num_instances.get(suffix, 0) + 1
    result = {}
    for name in names:
        for suffix in candidate_suffixes(name):
            if num_instances[suffix] == 1:
                result[name] = suffix
                break
        else:
            assert False, names
    return result
",if num_instances [ suffix ] == 1 :,154
"def output(self):
    """"""Transform self into a list of (name, value) tuples.""""""
    header_list = []
    for k, v in self.items():
        if isinstance(k, unicodestr):
            k = self.encode(k)
        if not isinstance(v, basestring):
            v = str(v)
        if isinstance(v, unicodestr):
            v = self.encode(v)
        # See header_translate_* constants above.
        # Replace only if you really know what you're doing.
        k = k.translate(header_translate_table, header_translate_deletechars)
        v = v.translate(header_translate_table, header_translate_deletechars)
        header_list.append((k, v))
    return header_list
","if not isinstance ( v , basestring ) :",197
"def get_errors(self, attacked_text, use_cache=False):
    text = attacked_text.text
    if use_cache:
        if text not in self.grammar_error_cache:
            self.grammar_error_cache[text] = len(self.lang_tool.check(text))
        return self.grammar_error_cache[text]
    else:
        return len(self.lang_tool.check(text))
",if text not in self . grammar_error_cache :,110
"def gen():
    for _ in range(256):
        if seq:
            yield self.tb.dut.i.eq(seq.pop(0))
        i = yield self.tb.dut.i
        if (yield self.tb.dut.n):
            self.assertEqual(i, 0)
        else:
            o = yield self.tb.dut.o
            if o > 0:
                self.assertEqual(i & 1 << (o - 1), 0)
            self.assertGreaterEqual(i, 1 << o)
        yield
",if ( yield self . tb . dut . n ) :,149
"def _register_builtin_handlers(self, events):
    for spec in handlers.BUILTIN_HANDLERS:
        if len(spec) == 2:
            event_name, handler = spec
            self.register(event_name, handler)
        else:
            event_name, handler, register_type = spec
            if register_type is handlers.REGISTER_FIRST:
                self._events.register_first(event_name, handler)
            elif register_type is handlers.REGISTER_LAST:
                self._events.register_last(event_name, handler)
",if len ( spec ) == 2 :,148
"def is_checked_sls_template(template):
    if template.__contains__(""provider""):
        # Case provider is a dictionary
        if isinstance(template[""provider""], dict_node):
            if template[""provider""].get(""name"").lower() not in SUPPORTED_PROVIDERS:
                return False
        # Case provider is direct provider name
        if isinstance(template[""provider""], str_node):
            if template[""provider""] not in SUPPORTED_PROVIDERS:
                return False
        return True
    return False
","if isinstance ( template [ ""provider"" ] , dict_node ) :",131
"def decode_body(self, response):
    if response is None:
        return response
    if six.PY2:
        return response
    if response.body:
        # Decode it
        if response.headers.get(""Content-Type"") == ""application/json"":
            response._body = response.body.decode(""utf-8"")
        else:
            response._body = salt.ext.tornado.escape.native_str(response.body)
    return response
","if response . headers . get ( ""Content-Type"" ) == ""application/json"" :",120
"def get_active_project_path():
    window = sublime.active_window()
    folders = window.folders()
    if len(folders) == 1:
        return folders[0]
    else:
        active_view = window.active_view()
        active_file_name = active_view.file_name() if active_view else None
        if not active_file_name:
            return folders[0] if len(folders) else os.path.expanduser(""~"")
        for folder in folders:
            if active_file_name.startswith(folder):
                return folder
        return os.path.dirname(active_file_name)
",if active_file_name . startswith ( folder ) :,166
"def pop(self, *a):
    lists = self.lists
    if len(lists) == 1 and not a:
        return self.lists[0].pop()
    index = a and a[0]
    if index == () or index is None or index == -1:
        ret = lists[-1].pop()
        if len(lists) > 1 and not lists[-1]:
            lists.pop()
    else:
        list_idx, rel_idx = self._translate_index(index)
        if list_idx is None:
            raise IndexError()
        ret = lists[list_idx].pop(rel_idx)
        self._balance_list(list_idx)
    return ret
",if list_idx is None :,176
"def setup(self, gen):
    Node.setup(self, gen)
    try:
        self.target = gen.rules[self.name]
        if self.accepts_epsilon != self.target.accepts_epsilon:
            self.accepts_epsilon = self.target.accepts_epsilon
            gen.changed()
    except KeyError:  # Oops, it's nonexistent
        print >>sys.stderr, ""Error: no rule <%s>"" % self.name
        self.target = self
",if self . accepts_epsilon != self . target . accepts_epsilon :,125
"def match(self, userargs):
    # Early skip if command or number of args don't match
    if len(self.args) != len(userargs):
        # DENY: argument numbers don't match
        return False
    # Compare each arg (anchoring pattern explicitly at end of string)
    for (pattern, arg) in zip(self.args, userargs):
        try:
            if not re.match(pattern + ""$"", arg):
                break
        except re.error:
            # DENY: Badly-formed filter
            return False
    else:
        # ALLOW: All arguments matched
        return True
    # DENY: Some arguments did not match
    return False
","if not re . match ( pattern + ""$"" , arg ) :",180
"def broadcast(self, msg, eid):
    for s in self.subs:
        if type(self.subs[s].eid) is list:
            if eid in self.subs[s].eid:
                self.subs[s].write_message(msg)
        else:
            if self.subs[s].eid == eid:
                self.subs[s].write_message(msg)
",if self . subs [ s ] . eid == eid :,111
"def apply_transformation(self, ti: TransformationInput) -> Transformation:
    fragments = ti.fragments
    # Walk through all te fragments.
    if fragments and fragment_list_to_text(fragments).startswith("" ""):
        t = (self.style, self.get_char())
        fragments = explode_text_fragments(fragments)
        for i in range(len(fragments)):
            if fragments[i][1] == "" "":
                fragments[i] = t
            else:
                break
    return Transformation(fragments)
","if fragments [ i ] [ 1 ] == "" "" :",138
"def _url_encode_impl(obj, charset, encode_keys, sort, key):
    iterable = sdict()
    for key, values in obj.items():
        if not isinstance(values, list):
            values = [values]
        iterable[key] = values
    if sort:
        iterable = sorted(iterable, key=key)
    for key, values in iterable.items():
        for value in values:
            if value is None:
                continue
            if not isinstance(key, bytes):
                key = str(key).encode(charset)
            if not isinstance(value, bytes):
                value = str(value).encode(charset)
            yield url_quote_plus(key) + ""="" + url_quote_plus(value)
","if not isinstance ( values , list ) :",198
"def get_objects(self) -> Iterator[Tuple[str, str, str, str, str, int]]:
    rootSymbol = self.data[""root_symbol""]
    for symbol in rootSymbol.get_all_symbols():
        if symbol.declaration is None:
            continue
        assert symbol.docname
        fullNestedName = symbol.get_full_nested_name()
        name = str(fullNestedName).lstrip(""."")
        dispname = fullNestedName.get_display_string().lstrip(""."")
        objectType = symbol.declaration.objectType
        docname = symbol.docname
        newestId = symbol.declaration.get_newest_id()
        yield (name, dispname, objectType, docname, newestId, 1)
",if symbol . declaration is None :,181
"def _delete_duplicates(l, keep_last):
    """"""Delete duplicates from a sequence, keeping the first or last.""""""
    seen = {}
    result = []
    if keep_last:  # reverse in & out, then keep first
        l.reverse()
    for i in l:
        try:
            if i not in seen:
                result.append(i)
                seen[i] = 1
        except TypeError:
            # probably unhashable.  Just keep it.
            result.append(i)
    if keep_last:
        result.reverse()
    return result
",if i not in seen :,155
"def combine_logs(audit_logs, statement_text_logs):
    for audit_transaction in audit_logs:
        for audit_query in audit_logs[audit_transaction]:
            matching_statement_text_logs = statement_text_logs.get(hash(audit_query))
            if matching_statement_text_logs:
                statement_text_log = matching_statement_text_logs.pop()
                if statement_text_log:
                    if statement_text_log.start_time:
                        audit_query.start_time = statement_text_log.start_time
                    if statement_text_log.end_time:
                        audit_query.end_time = statement_text_log.end_time
",if matching_statement_text_logs :,197
"def free(self, addr, ban=0):
    with self.lock:
        if ban != 0:
            self.ban.append({""addr"": addr, ""counter"": ban})
        else:
            base, bit, is_allocated = self.locate(addr)
            if len(self.addr_map) <= base:
                raise KeyError(""address is not allocated"")
            if self.addr_map[base] & (1 << bit):
                raise KeyError(""address is not allocated"")
            self.allocated -= 1
            self.addr_map[base] ^= 1 << bit
",if self . addr_map [ base ] & ( 1 << bit ) :,155
"def _assertParseMethod(test, code_str, method, expect_success=True):
    arena, c_parser = InitCommandParser(code_str)
    m = getattr(c_parser, method)
    node = m()
    if node:
        ast_lib.PrettyPrint(node)
        if not expect_success:
            test.fail(""Expected %r to fail "" % code_str)
    else:
        # TODO: Could copy PrettyPrintError from pysh.py
        err = c_parser.Error()
        print(err)
        ui.PrintErrorStack(err, arena, sys.stdout)
        if expect_success:
            test.fail(""%r failed"" % code_str)
    return node
",if expect_success :,186
"def _gen():
    while True:
        try:
            loop_val = it.next()  # e.g. x
        except StopIteration:
            break
        self.mem.SetValue(
            lvalue.Named(iter_name), value.Obj(loop_val), scope_e.LocalOnly
        )
        if comp.cond:
            b = self.EvalExpr(comp.cond)
        else:
            b = True
        if b:
            item = self.EvalExpr(node.elt)  # e.g. x*2
            yield item
",if comp . cond :,155
"def _build_default_obj_recursive(self, _properties, res):
    """"""takes disparate and nested default keys, and builds up a default object""""""
    for key, prop in _properties.items():
        if ""default"" in prop and key not in res:
            res[key] = copy(prop[""default""])
        elif prop.get(""type"") == ""object"" and ""properties"" in prop:
            res.setdefault(key, {})
            res[key] = self._build_default_obj_recursive(prop[""properties""], res[key])
    return res
","if ""default"" in prop and key not in res :",143
"def mean(self):
    """"""Compute the mean of the value_field in the window.""""""
    if len(self.data) > 0:
        datasum = 0
        datalen = 0
        for dat in self.data:
            if ""placeholder"" not in dat[0]:
                datasum += dat[1]
                datalen += 1
        if datalen > 0:
            return datasum / float(datalen)
        return None
    else:
        return None
","if ""placeholder"" not in dat [ 0 ] :",132
"def addNames(self, import_names, node_names):
    for names in node_names:
        if isinstance(names, basestring):
            name = names
        elif names[1] is None:
            name = names[0]
        else:
            name = names[1]
        import_names[name] = True
","if isinstance ( names , basestring ) :",88
"def set(sensor_spec: dict, **kwargs):
    for key, value in kwargs.items():
        if key == ""position"":
            sensor_spec[""transform""] = SensorSpecs.get_position(value)
        elif key == ""attachment_type"":
            sensor_spec[key] = SensorSpecs.ATTACHMENT_TYPE[value]
        elif key == ""color_converter"":
            sensor_spec[key] = SensorSpecs.COLOR_CONVERTER[value]
","if key == ""position"" :",125
"def delete_session(self):
    cookie = self.headers.get(HTTP_HEADER.COOKIE)
    if cookie:
        match = re.search(r""%s=(.+)"" % SESSION_COOKIE_NAME, cookie)
        if match:
            session = match.group(1)
            if session in SESSIONS:
                del SESSIONS[session]
",if session in SESSIONS :,92
"def rename_var(block: paddle.device.framework.Block, old_name: str, new_name: str):
    """""" """"""
    for op in block.ops:
        for input_name in op.input_arg_names:
            if input_name == old_name:
                op._rename_input(old_name, new_name)
        for output_name in op.output_arg_names:
            if output_name == old_name:
                op._rename_output(old_name, new_name)
    block._rename_var(old_name, new_name)
",if input_name == old_name :,155
"def updateParticle(part, best, phi1, phi2):
    u1 = numpy.random.uniform(0, phi1, len(part))
    u2 = numpy.random.uniform(0, phi2, len(part))
    v_u1 = u1 * (part.best - part)
    v_u2 = u2 * (best - part)
    part.speed += v_u1 + v_u2
    for i, speed in enumerate(part.speed):
        if abs(speed) < part.smin:
            part.speed[i] = math.copysign(part.smin, speed)
        elif abs(speed) > part.smax:
            part.speed[i] = math.copysign(part.smax, speed)
    part += part.speed
",if abs ( speed ) < part . smin :,197
"def acquire(self, blocking=True, timeout=None):
    if not blocking and timeout is not None:
        raise ValueError(""can't specify timeout for non-blocking acquire"")
    rc = False
    endtime = None
    self._cond.acquire()
    while self._value == 0:
        if not blocking:
            break
        if timeout is not None:
            if endtime is None:
                endtime = _time() + timeout
            else:
                timeout = endtime - _time()
                if timeout <= 0:
                    break
        self._cond.wait(timeout)
    else:
        self._value = self._value - 1
        rc = True
    self._cond.release()
    return rc
",if timeout is not None :,194
"def test_ESPnetDataset_text_float(text_float):
    dataset = IterableESPnetDataset(
        path_name_type_list=[(text_float, ""data8"", ""text_float"")],
        preprocess=preprocess,
    )
    for key, data in dataset:
        if key == ""a"":
            assert all((data[""data8""]) == np.array([1.4, 3.4], dtype=np.float32))
        if key == ""b"":
            assert all((data[""data8""]) == np.array([0.9, 9.3], dtype=np.float32))
","if key == ""a"" :",152
"def __eq__(self, other):
    if isinstance(other, OrderedDict):
        if len(self) != len(other):
            return False
        for p, q in zip(list(self.items()), list(other.items())):
            if p != q:
                return False
        return True
    return dict.__eq__(self, other)
",if len ( self ) != len ( other ) :,91
"def exec_command(command, cwd=None, stdout=None, env=None):
    """"""Returns True in the command was executed successfully""""""
    try:
        command_list = command if isinstance(command, list) else command.split()
        env_vars = os.environ.copy()
        if env:
            env_vars.update(env)
        subprocess.check_call(command_list, stdout=stdout, cwd=cwd, env=env_vars)
        return True
    except subprocess.CalledProcessError as err:
        print(err, file=sys.stderr)
        return False
",if env :,146
"def _get_lun_details(self, lun_id):
    """"""Given the ID of a LUN, get the details about that LUN""""""
    server = self.client.service
    res = server.LunListInfoIterStart(ObjectNameOrId=lun_id)
    tag = res.Tag
    try:
        res = server.LunListInfoIterNext(Tag=tag, Maximum=1)
        if hasattr(res, ""Luns"") and res.Luns.LunInfo:
            return res.Luns.LunInfo[0]
    finally:
        server.LunListInfoIterEnd(Tag=tag)
","if hasattr ( res , ""Luns"" ) and res . Luns . LunInfo :",162
"def _process_events(self, event_list):
    for key, mask in event_list:
        fileobj, (reader, writer) = key.fileobj, key.data
        if mask & selectors.EVENT_READ and reader is not None:
            if reader._cancelled:
                self.remove_reader(fileobj)
            else:
                self._add_callback(reader)
        if mask & selectors.EVENT_WRITE and writer is not None:
            if writer._cancelled:
                self.remove_writer(fileobj)
            else:
                self._add_callback(writer)
",if mask & selectors . EVENT_WRITE and writer is not None :,158
"def process_doc(self, docstrings: List[List[str]]) -> Iterator[str]:
    """"""Let the user process the docstrings before adding them.""""""
    for docstringlines in docstrings:
        if self.env.app:
            # let extensions preprocess docstrings
            self.env.app.emit(
                ""autodoc-process-docstring"",
                self.objtype,
                self.fullname,
                self.object,
                self.options,
                docstringlines,
            )
            if docstringlines and docstringlines[-1] != """":
                # append a blank line to the end of the docstring
                docstringlines.append("""")
        yield from docstringlines
","if docstringlines and docstringlines [ - 1 ] != """" :",185
"def vectorize(self, doc, vocab, char_vocab):
    words = np.asarray(
        [vocab[w.lower()] if w.lower() in vocab else 1 for w in doc]
    ).reshape(1, -1)
    sentence_chars = []
    for w in doc:
        word_chars = []
        for c in w:
            if c in char_vocab:
                _cid = char_vocab[c]
            else:
                _cid = 1
            word_chars.append(_cid)
        sentence_chars.append(word_chars)
    sentence_chars = np.expand_dims(
        pad_sentences(sentence_chars, self.model.word_length), axis=0
    )
    return words, sentence_chars
",if c in char_vocab :,196
"def runtestenv(venv, config, redirect=False):
    if venv.status == 0 and config.option.notest:
        venv.status = ""skipped tests""
    else:
        if venv.status:
            return
        config.pluginmanager.hook.tox_runtest_pre(venv=venv)
        if venv.status == 0:
            config.pluginmanager.hook.tox_runtest(venv=venv, redirect=redirect)
        config.pluginmanager.hook.tox_runtest_post(venv=venv)
",if venv . status :,133
"def _import_config_module(self, name):
    try:
        self.find_module(name)
    except NotAPackage:
        if name.endswith("".py""):
            reraise(
                NotAPackage,
                NotAPackage(CONFIG_WITH_SUFFIX.format(module=name, suggest=name[:-3])),
                sys.exc_info()[2],
            )
        reraise(
            NotAPackage,
            NotAPackage(CONFIG_INVALID_NAME.format(module=name)),
            sys.exc_info()[2],
        )
    else:
        return self.import_from_cwd(name)
","if name . endswith ( "".py"" ) :",172
"def TryMerge(self, d):
    while d.avail() > 0:
        tt = d.getVarInt32()
        if tt == 8:
            self.set_format(d.getVarInt32())
            continue
        if tt == 18:
            self.set_path(d.getPrefixedString())
            continue
        if tt == 0:
            raise ProtocolBuffer.ProtocolBufferDecodeError
        d.skipData(tt)
",if tt == 8 :,120
"def get(self, request, *args, **kwargs):
    # Generate sidebar forms
    self.sidebar_forms = []
    for form_id, (plugin, Form) in self.get_sidebar_form_classes().items():
        if Form:
            form = Form(self.article, self.request.user)
            setattr(form, ""form_id"", form_id)
        else:
            form = None
        self.sidebar.append((plugin, form))
    return super().get(request, *args, **kwargs)
",if Form :,135
"def check_click(self):
    if not isinstance(self, SwiDebugView):
        return
    cursor = self.sel()[0].a
    index = 0
    click_regions = self.get_regions(""swi_log_clicks"")
    for callback in click_regions:
        if cursor > callback.a and cursor < callback.b:
            if index < len(self.callbacks):
                callback = self.callbacks[index]
                callback[""callback""](*callback[""args""])
        index += 1
",if index < len ( self . callbacks ) :,131
"def get_sock(port):
    sock = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)
    while True:
        try:
            _port = port or random.randint(1025, 5000)
            print((""try bind local port:"", _port))
            sock.bind((""0.0.0.0"", _port))
            return sock
        except socket.error as e:
            if port:
                print((""bind local port %d fail: %r"" % (_port, e)))
                return
            if e.args[0] == errno.EADDRINUSE:
                pass
",if port :,166
"def ParsePlacemark(self, node):
    ret = Placemark()
    for child in node.childNodes:
        if child.nodeName == ""name"":
            ret.name = self.ExtractText(child)
        if child.nodeName == ""Point"" or child.nodeName == ""LineString"":
            ret.coordinates = self.ExtractCoordinates(child)
    return ret
","if child . nodeName == ""Point"" or child . nodeName == ""LineString"" :",94
"def _load_library(self):
    if self.library is not None:
        if isinstance(self.library, (tuple, list)):
            name, mod_path = self.library
        else:
            name = mod_path = self.library
        try:
            module = importlib.import_module(mod_path)
        except ImportError:
            raise ValueError(""Couldn't load %s password algorithm "" ""library"" % name)
        return module
    raise ValueError(""Hasher '%s' doesn't specify a library attribute"" % self.__class__)
","if isinstance ( self . library , ( tuple , list ) ) :",139
"def check(self):
    for r in self.results:
        if r.backend:
            assert r.backend.name == self.target.path.k8s, (
                r.backend.name,
                self.target.path.k8s,
            )
            assert r.backend.request.headers[""x-envoy-original-path""][0] in (
                f""/{self.name}/"",
                f""/{self.name}-nested/"",
            )
",if r . backend :,135
"def eval(self, code, eval=True, raw=False):
    self._engine._append_source(code)
    try:
        result = self._context.eval(code)
    except quickjs.JSException as e:
        raise ProgramError(*e.args)
    else:
        if eval:
            if raw or not isinstance(result, quickjs.Object):
                return result
            elif callable(result) and self.typeof(result) == u""function"":
                return self.Function(self, result)
            else:
                return json.loads(result.json())
","elif callable ( result ) and self . typeof ( result ) == u""function"" :",158
"def __truediv__(self, val):
    if isinstance(val, Vector3):
        if val.x == 0 or val.y == 0 or val.z == 0:
            raise ZeroDivisionError()
        gd_obj = lib.godot_vector3_operator_divide_vector(self._gd_ptr, val._gd_ptr)
    else:
        if val is 0:
            raise ZeroDivisionError()
        gd_obj = lib.godot_vector3_operator_divide_scalar(self._gd_ptr, val)
    return Vector3.build_from_gdobj(gd_obj)
",if val . x == 0 or val . y == 0 or val . z == 0 :,148
"def set_peek(self, dataset, is_multi_byte=False):
    if not dataset.dataset.purged:
        dataset.peek = data.get_file_peek(dataset.file_name)
        if dataset.metadata.sequences:
            dataset.blurb = ""%s sequences"" % util.commaify(
                str(dataset.metadata.sequences)
            )
        else:
            dataset.blurb = nice_size(dataset.get_size())
    else:
        dataset.peek = ""file does not exist""
        dataset.blurb = ""file purged from disk""
",if dataset . metadata . sequences :,153
"def _get_plugin_src_dirs(base_dir):
    plug_in_base_path = Path(get_src_dir(), base_dir)
    plugin_dirs = get_dirs_in_dir(str(plug_in_base_path))
    plugins = []
    for plugin_path in plugin_dirs:
        plugin_code_dir = Path(plugin_path, ""code"")
        if plugin_code_dir.is_dir():
            plugins.append(str(plugin_code_dir))
        else:
            logging.warning(""Plugin has no code directory: {}"".format(plugin_path))
    return plugins
",if plugin_code_dir . is_dir ( ) :,155
"def _format_privilege_data(self, data):
    for key in [""spcacl""]:
        if key in data and data[key] is not None:
            if ""added"" in data[key]:
                data[key][""added""] = parse_priv_to_db(data[key][""added""], self.acl)
            if ""changed"" in data[key]:
                data[key][""changed""] = parse_priv_to_db(data[key][""changed""], self.acl)
            if ""deleted"" in data[key]:
                data[key][""deleted""] = parse_priv_to_db(data[key][""deleted""], self.acl)
","if ""deleted"" in data [ key ] :",168
"def __init__(self, methodName=""runTest""):
    unittest.TestCase.__init__(self, methodName)
    # We expect files to be relative to this test script.
    test_dir = dirname(dirname(__file__))
    self._dir = normpath(join(test_dir, ""stuff/charsets/www.kostis.net/charsets""))
    self._enc = {}
    # get all the utf-8 files in this dir, and well recode them
    names = os.listdir(self._dir)
    for name in names:
        if not os.path.isfile(os.path.join(self._dir, name)):
            continue
        enc = name.split(""."")[0]
        if decoderAvailable(enc):
            self._enc[enc] = name
","if not os . path . isfile ( os . path . join ( self . _dir , name ) ) :",187
"def get_actions_on_list(self, actions, modelview_name):
    res_actions = dict()
    for action_key in actions:
        action = actions[action_key]
        if self.is_item_visible(action.name, modelview_name) and action.multiple:
            res_actions[action_key] = action
    return res_actions
","if self . is_item_visible ( action . name , modelview_name ) and action . multiple :",93
"def triger_check_network(self, fail=False, force=False):
    time_now = time.time()
    if not force:
        if self._checking_num > 0:
            return
        if fail or self.network_stat != ""OK"":
            # Fail or unknown
            if time_now - self.last_check_time < 3:
                return
        else:
            if time_now - self.last_check_time < 10:
                return
    self.last_check_time = time_now
    threading.Thread(target=self._simple_check_worker).start()
",if time_now - self . last_check_time < 3 :,161
"def write(self, root):
    """"""Write all the *descendants* of an .dart node.""""""
    root_level = root.level()
    for p in root.subtree():
        indent = p.level() - root_level
        self.put(""%s %s"" % (""*"" * indent, p.h))
        for s in p.b.splitlines(False):
            if not g.isDirective(s):
                self.put(s)
    root.setVisited()
    return True
",if not g . isDirective ( s ) :,127
"def characters(self, ch):
    if self.Text_tag:
        if self.Summary_tag:
            self.Summary_ch += ch
        elif self.Attack_Prerequisite_tag:
            self.Attack_Prerequisite_ch += ch
        elif self.Solution_or_Mitigation_tag:
            self.Solution_or_Mitigation_ch += ch
    elif self.CWE_ID_tag:
        self.CWE_ID_ch += ch
",if self . Summary_tag :,127
"def _handle_function(self, addr):
    if self.arch.name == ""X86"":
        try:
            b = self._project.loader.memory.load(addr, 4)
        except KeyError:
            return
        except TypeError:
            return
        if b == b""\x8b\x1c\x24\xc3"":
            # getpc:
            #   mov ebx, [esp]
            #   ret
            ebx_offset = self.arch.registers[""ebx""][0]
            self.state.store_register(ebx_offset, 4, self.block.addr + self.block.size)
","if b == b""\x8b\x1c\x24\xc3"" :",171
"def safe_makedir(dname):
    """"""Make a directory if it doesn't exist, handling concurrent race conditions.""""""
    if not dname:
        return dname
    num_tries = 0
    max_tries = 5
    while not os.path.exists(dname):
        # we could get an error here if multiple processes are creating
        # the directory at the same time. Grr, concurrency.
        try:
            os.makedirs(dname)
        except OSError:
            if num_tries > max_tries:
                raise
            num_tries += 1
            time.sleep(2)
    return dname
",if num_tries > max_tries :,164
"def _setup_data(self, path):
    with PathManager.open(path) as data_file:
        if ""extra"" in path and ""train"" in path:
            line = data_file.readline()
            # trim corrupted JSON
            line = line[: line.rfind(""{"")]
            line = line[: line.rfind("","")] + ""]""
            self.data = json.loads(line)
        else:
            self.data = json.load(data_file)
","if ""extra"" in path and ""train"" in path :",124
"def _end_delimiter(state, token):
    py = state[""pymode""]
    s = token.string
    l, c = token.start
    if len(py) > 1:
        mode, orig, match, pos = py.pop()
        if s != match:
            e = '""{}"" at {} ends ""{}"" at {} (expected ""{}"")'
            return e.format(s, (l, c), orig, pos, match)
    else:
        return 'Unmatched ""{}"" at line {}, column {}'.format(s, l, c)
",if s != match :,132
"def onLeftDoubleClick(self, event):
    row, _ = self.HitTest(event.Position)
    if row != -1:
        col = self.getColumn(event.Position)
        if col != self.getColIndex(State):
            try:
                booster = self.boosters[row]
            except IndexError:
                return
            self.removeBoosters([booster])
",if col != self . getColIndex ( State ) :,107
"def get_instance_userdata(
    version=""latest"",
    sep=None,
    url=""http://169.254.169.254"",
    timeout=None,
    num_retries=5,
):
    ud_url = _build_instance_metadata_url(url, version, ""user-data"")
    user_data = retry_url(
        ud_url, retry_on_404=False, num_retries=num_retries, timeout=timeout
    )
    if user_data:
        if sep:
            l = user_data.split(sep)
            user_data = {}
            for nvpair in l:
                t = nvpair.split(""="")
                user_data[t[0].strip()] = t[1].strip()
    return user_data
",if sep :,198
"def parts(self):
    klass = self.__class__
    this = list()
    for token in self:
        if token.startswith_fws():
            if this:
                yield this[0] if len(this) == 1 else klass(this)
                this.clear()
        end_ws = token.pop_trailing_ws()
        this.append(token)
        if end_ws:
            yield klass(this)
            this = [end_ws]
    if this:
        yield this[0] if len(this) == 1 else klass(this)
",if this :,153
"def run(self):
    while True:
        self._trigger.wait()
        self._trigger.clear()
        if self._terminate:
            break
        for url in self.urls:
            logger.info(""Pinging for problem update: %s"", url)
            try:
                with closing(urlopen(url, data="""")) as f:
                    f.read()
            except Exception:
                logger.exception(""Failed to ping for problem update: %s"", url)
",if self . _terminate :,132
"def _get_trading_minutes(self, trading_date):
    trading_minutes = set()
    for account_type in self._config.base.accounts:
        if account_type == DEFAULT_ACCOUNT_TYPE.STOCK:
            trading_minutes = trading_minutes.union(
                self._get_stock_trading_minutes(trading_date)
            )
        elif account_type == DEFAULT_ACCOUNT_TYPE.FUTURE:
            trading_minutes = trading_minutes.union(
                self._get_future_trading_minutes(trading_date)
            )
    return sorted(list(trading_minutes))
",elif account_type == DEFAULT_ACCOUNT_TYPE . FUTURE :,169
"def make_tree(self, node):
    if node is self.root:
        node.code = """"
    children = []
    for bit in ""01"":
        next_code = node.code + bit
        if next_code in self.codes:
            child = Node(char=self.codes[next_code])
        else:
            child = Node()
        child.code = next_code
        children.append(child)
    node.add(children)
    for child in children:
        if not child.is_leaf:
            self.make_tree(child)
",if next_code in self . codes :,152
"def _merge(self, a, b, path=None):
    """"""Merge two dictionaries, from http://stackoverflow.com/questions/7204805/dictionaries-of-dictionaries-merge""""""
    if path is None:
        path = []
    for key in b:
        if key in a:
            if isinstance(a[key], dict) and isinstance(b[key], dict):
                self._merge(a[key], b[key], path + [str(key)])
            elif a[key] == b[key]:
                pass  # same leaf value
            else:
                raise Exception(""Conflict at %s"" % ""."".join(path + [str(key)]))
        else:
            a[key] = b[key]
    return a
","if isinstance ( a [ key ] , dict ) and isinstance ( b [ key ] , dict ) :",196
"def _append_value(generator, val=None):
    for example in generator:
        example = list(example)
        if val is not None:
            for key, value in val.items():
                example[key] = np.append(example[key], value, -1)
        yield tuple(example)
",if val is not None :,82
"def run(self):
    to_delete = set()
    for k, v in iteritems(self.objs):
        if k.startswith(""_""):
            continue
        if v[""_class""] == ""SubmissionFormatElement"":
            to_delete.add(k)
        if v[""_class""] == ""Task"":
            v[""submission_format""] = list(
                self.objs[k][""filename""] for k in v.get(""submission_format"", list())
            )
    for k in to_delete:
        del self.objs[k]
    return self.objs
","if k . startswith ( ""_"" ) :",147
"def service_destroy(context, service_id):
    session = get_session()
    with session.begin():
        service_ref = service_get(context, service_id, session=session)
        service_ref.delete(session=session)
        if service_ref.topic == ""compute"" and service_ref.compute_node:
            for c in service_ref.compute_node:
                c.delete(session=session)
","if service_ref . topic == ""compute"" and service_ref . compute_node :",111
"def wiki(self, query):
    res = []
    for entry in g.current_wiki.get_index():
        name = filename_to_cname(entry[""name""])
        name = re.sub(r""//+"", ""/"", name)
        if set(query.split()).intersection(name.replace(""/"", ""-"").split(""-"")):
            page = g.current_wiki.get_page(name)
            # this can be None, not sure how
            if page:
                res.append(dict(name=name, content=page.data))
    return res
","if set ( query . split ( ) ) . intersection ( name . replace ( ""/"" , ""-"" ) . split ( ""-"" ) ) :",143
"def numericalize(self, arr, device=None):
    if isinstance(arr[0][0], list):
        tmp = [
            super(BABI20Field, self).numericalize(x, device=device).data for x in arr
        ]
        arr = torch.stack(tmp)
        if self.sequential:
            arr = arr.contiguous()
        return arr
    else:
        return super(BABI20Field, self).numericalize(arr, device=device)
",if self . sequential :,127
"def validate_and_handle(self):
    valid = self.validate(set_cursor=True)
    if valid:
        if self.accept_handler:
            keep_text = self.accept_handler(self)
        else:
            keep_text = False
        if not keep_text:
            self.reset()
",if not keep_text :,86
"def headerData(self, section, orientation, role=Qt.DisplayRole):
    if role == Qt.TextAlignmentRole:
        if orientation == Qt.Horizontal:
            return to_qvariant(int(Qt.AlignHCenter | Qt.AlignVCenter))
        return to_qvariant(int(Qt.AlignRight | Qt.AlignVCenter))
    if role != Qt.DisplayRole:
        return to_qvariant()
    if orientation == Qt.Horizontal:
        if section == NAME:
            return to_qvariant(""Name"")
        elif section == VERSION:
            return to_qvariant(""Version"")
        elif section == ACTION:
            return to_qvariant(""Action"")
        elif section == DESCRIPTION:
            return to_qvariant(""Description"")
    return to_qvariant()
",if section == NAME :,192
"def replace(self, state):
    if state.key in self._dict:
        try:
            existing = self._dict[state.key]
        except KeyError:
            # catch gc removed the key after we just checked for it
            pass
        else:
            if existing is not state:
                self._manage_removed_state(existing)
            else:
                return
    self._dict[state.key] = state
    self._manage_incoming_state(state)
",if existing is not state :,131
"def _line_generator(fh, skip_blanks=False, strip=True):
    for line in fh:
        if strip:
            line = line.strip()
        skip = False
        if skip_blanks:
            skip = line.isspace() or not line
        if not skip:
            yield line
",if strip :,82
"def _get_workers_with_max_size(worker_to_size):
    """"""Get workers with maximal size""""""
    max_workers = set()
    max_size = 0
    for w, size in worker_to_size.items():
        if size > max_size:
            max_size = size
            max_workers = {w}
        elif size == max_size:
            max_workers.add(w)
    max_workers.difference_update([None])
    return max_size, list(max_workers)
",if size > max_size :,135
"def parse(self):
    while 1:
        l = self.f.readline()
        if not l:
            return
        l = l.strip()
        if l.startswith(""[""):
            self.parse_uuid(l)
        elif l.startswith(""interface"") or l.startswith(""dispinterface""):
            self.parse_interface(l)
        elif l.startswith(""coclass""):
            self.parse_coclass(l)
","elif l . startswith ( ""coclass"" ) :",117
"def check_source_unit(self, source, unit):
    """"""Check source string.""""""
    rules = [FLAG_RULES[flag] for flag in unit.all_flags if flag in FLAG_RULES]
    if not rules:
        return False
    found = set()
    for regexp, is_position_based in rules:
        for match in regexp.finditer(source[0]):
            if is_position_based(match[1]):
                found.add((match.start(0), match.end(0)))
                if len(found) >= 2:
                    return True
    return False
",if len ( found ) >= 2 :,156
"def parse_exprlist(self):
    list = []
    while TRUE:
        self.reader.skip_white()
        c = self.reader.peek()
        if c != '""' and self.ends_excmds(c):
            break
        node = self.parse_expr()
        viml_add(list, node)
    return list
","if c != '""' and self . ends_excmds ( c ) :",90
"def can_see_ban_details(request, profile):
    if request.user.is_authenticated:
        if request.user_acl[""can_see_ban_details""]:
            from .bans import get_user_ban
            return bool(get_user_ban(profile, request.cache_versions))
        return False
    return False
","if request . user_acl [ ""can_see_ban_details"" ] :",87
"def mouse_move(self, ips, x, y, btn, **key):
    if ips.roi == None:
        return
    lim = 5.0 / key[""canvas""].get_scale()
    if btn == None:
        self.cursor = wx.CURSOR_CROSS
        if ips.roi.snap(x, y, ips.cur, lim) != None:
            self.cursor = wx.CURSOR_HAND
    elif btn == 1:
        if self.curobj:
            ips.roi.draged(self.odx, self.ody, x, y, ips.cur, self.curobj)
        ips.update()
    self.odx, self.ody = x, y
","if ips . roi . snap ( x , y , ips . cur , lim ) != None :",178
"def evex_mask_dest_reg_only(ii):  # optional imm8
    i, m, xyz = 0, 0, 0
    for op in _gen_opnds(ii):
        if op_mask_reg(op):
            m += 1
        elif op_xmm(op) or op_ymm(op) or op_zmm(op):
            xyz += 1
        elif op_imm8(op):
            i += 1
        else:
            return False
    return m == 1 and xyz > 0 and i <= 1
",elif op_xmm ( op ) or op_ymm ( op ) or op_zmm ( op ) :,143
"def encode_datetime(self, dt, state):
    fmt = self.options.datetime_format
    is_iso = not fmt or fmt == ""iso""
    if is_iso:
        if dt.microsecond == 0:
            fmt = ""%Y-%m-%dT%H:%M:%S%z""
        else:
            fmt = ""%Y-%m-%dT%H:%M:%S.%f%z""
    s = dt.strftime(fmt)
    if is_iso and s.endswith(""-00:00"") or s.endswith(""+00:00""):
        s = s[:-6] + ""Z""  # Change UTC to use 'Z' notation
    self.encode_string(s, state)
",if dt . microsecond == 0 :,172
"def main(config):
    with PathManager.open(config[""infile""], ""r"") as fin, PathManager.open(
        config[""outfile""], ""w""
    ) as fout:
        for line in fin.readlines():
            if ""persona"" in line:
                continue
            first_space = line.index("" "")
            first_tab = line.index(""\t"")
            candidate = line[first_space + 1 : first_tab]
            fout.write(candidate + ""\n"")
","if ""persona"" in line :",133
"def compact_repr(record):
    parts = []
    for key in record.__attributes__:
        value = getattr(record, key)
        if not value:
            continue
        if isinstance(value, list):
            value = HIDE_LIST
        elif key == FEATS:
            value = format_feats(value)
        else:
            value = repr(value)
        value = capped_str(value)
        parts.append(""%s=%s"" % (key, value))
    return ""%s(%s)"" % (record.__class__.__name__, "", "".join(parts))
",elif key == FEATS :,152
"def make_chain(word):
    which = 1
    while True:
        songs = find_songs_that_start_with_word(word)
        if len(songs) > 0:
            song = random.choice(songs)
            print(which, song[""name""] + "" by "" + song[""artists""][0][""name""])
            which += 1
            word = song[""name""].lower().split()[-1]
        else:
            break
",if len ( songs ) > 0 :,118
"def set_break(self, filename, lineno, temporary=False, cond=None, funcname=None):
    if isinstance(funcname, str):
        if filename == __file__:
            globals_ = globals()
        else:
            module = importlib.import_module(filename[:-3])
            globals_ = module.__dict__
        func = eval(funcname, globals_)
        code = func.__code__
        filename = code.co_filename
        lineno = code.co_firstlineno
        funcname = code.co_name
    res = super(Bdb, self).set_break(
        filename, lineno, temporary=temporary, cond=cond, funcname=funcname
    )
    if isinstance(res, str):
        raise BdbError(res)
    return res
",if filename == __file__ :,192
"def __init__(self, shapefile=None, shapeType=POINT, autoBalance=1):
    self.autoBalance = autoBalance
    if not shapefile:
        Writer.__init__(self, shapeType)
    elif is_string(shapefile):
        base = os.path.splitext(shapefile)[0]
        if os.path.isfile(""%s.shp"" % base):
            r = Reader(base)
            Writer.__init__(self, r.shapeType)
            self._shapes = r.shapes()
            self.fields = r.fields
            self.records = r.records()
","if os . path . isfile ( ""%s.shp"" % base ) :",157
"def test_env_not_set(self):
    with mock.patch.dict(""os.environ""):
        if self.env_name in os.environ:
            del os.environ[self.env_name]
        self.assertEqual(helper.get_xdg_env(self.env_name, self.default), self.default)
",if self . env_name in os . environ :,83
"def selection_only(self):
    selection_only = False
    sel = self.sel()
    if (self.context == ""selection"" or self.context == ""both"") and len(sel):
        # if multiple lines, always true
        if len(sel) > 1:
            selection_only = True
        # check threshold
        elif self.threshold and not sel[0].empty():
            text = self.view.substr(sel[0])
            match = re.search(self.threshold, text)
            if match:
                selection_only = True
        # no valid selection
        else:
            selection_only = False
    return selection_only
",if match :,174
"def __call__(self, rule, param):
    p, g = param.data, param.grad
    if p is None or g is None:
        return
    with chainer.using_device(param.device):
        xp = param.device.xp
        sign = xp.sign(p)
        if xp is cuda.cupy:
            kernel = cuda.elementwise(""T s, T decay"", ""T g"", ""g += decay * s"", ""lasso"")
            kernel(sign, self.rate, g)
        else:
            g += self.rate * sign
",if xp is cuda . cupy :,144
"def map_packages(shutit_pexpect_session, package_str, install_type):
    res = """"
    for package in package_str.split():
        map_package_res = map_package(shutit_pexpect_session, package, install_type)
        if map_package_res == """":
            return res
        res += "" "" + map_package_res
    return res
","if map_package_res == """" :",101
"def get_opnd_types_short(ii):
    types = []
    for op in _gen_opnds(ii):
        if op.oc2:
            types.append(op.oc2)
        elif op_luf_start(op, ""GPRv""):
            types.append(""v"")
        elif op_luf_start(op, ""GPRz""):
            types.append(""z"")
        elif op_luf_start(op, ""GPRy""):
            types.append(""y"")
        else:
            die(""Unhandled op type {}"".format(op))
    return types
","elif op_luf_start ( op , ""GPRy"" ) :",161
"def _process_archive(self, archive_stream, subtitle):
    for file_name in archive_stream.namelist():
        if file_name.lower().endswith(("".srt"", "".sub"")):
            logger.info(""Found subtitle file %r"", file_name)
            subtitle.content = fix_line_ending(archive_stream.read(file_name))
            if subtitle.is_valid():
                return
","if file_name . lower ( ) . endswith ( ( "".srt"" , "".sub"" ) ) :",105
"def truncate(self, size=None):
    # type: (Optional[int]) -> int
    # Inefficient, but I don't know if truncate is possible with ftp
    with self._lock:
        if size is None:
            size = self.tell()
        with self.fs.openbin(self.path) as f:
            data = f.read(size)
        with self.fs.openbin(self.path, ""w"") as f:
            f.write(data)
            if len(data) < size:
                f.write(b""\0"" * (size - len(data)))
    return size
",if len ( data ) < size :,163
"def wakeup(self):
    try:
        if self.wm_state() == ""iconic"":
            self.wm_withdraw()
            self.wm_deiconify()
        self.tkraise()
        self.focused_widget.focus_set()
    except TclError:
        # This can happen when the window menu was torn off.
        # Simply ignore it.
        pass
","if self . wm_state ( ) == ""iconic"" :",109
"def locus_parser(self):
    line = self.stream.readline()
    while line != """":
        line = line.rstrip()
        match = re.match("" Locus: (.+)"", line)
        if match is not None:
            locus = match.group(1)
            alleles, table = _read_allele_freq_table(self.stream)
            return locus, alleles, table
        line = self.stream.readline()
    self.done = True
    raise StopIteration
",if match is not None :,131
"def TryMerge(self, d):
    while d.avail() > 0:
        tt = d.getVarInt32()
        if tt == 10:
            self.set_content(d.getPrefixedString())
            continue
        if tt == 18:
            self.set_blob_key(d.getPrefixedString())
            continue
        if tt == 24:
            self.set_width(d.getVarInt32())
            continue
        if tt == 32:
            self.set_height(d.getVarInt32())
            continue
        if tt == 0:
            raise ProtocolBuffer.ProtocolBufferDecodeError
        d.skipData(tt)
",if tt == 32 :,182
"def concat_kernel_sources(self):
    func_sources = OrderedDict()
    for kernel in self.kernels:
        for func_name, source in kernel.func_sources.items():
            if func_name in func_sources:
                assert func_sources[func_name] == source
            else:
                func_sources[func_name] = source
    self.generate_top_source()
    self.generate_exec_source()
    self.generate_init_source()
    combined_source = (
        """".join(self.header_sources.values())
        + ""\n"".join(func_sources.values())
        + """".join(self.footer_sources.values())
    )
    return combined_source
",if func_name in func_sources :,185
"def parseUnderindentTag(self, s):
    tag = self.underindentEscapeString
    s2 = s[len(tag) :]
    # To be valid, the escape must be followed by at least one digit.
    i = 0
    while i < len(s2) and s2[i].isdigit():
        i += 1
    if i > 0:
        n = int(s2[:i])
        # Bug fix: 2012/06/05: remove any period following the count.
        # This is a new convention.
        if i < len(s2) and s2[i] == ""."":
            i += 1
        return n, s2[i:]
    else:
        return 0, s
","if i < len ( s2 ) and s2 [ i ] == ""."" :",178
"def load(self, data):
    ckey = None
    for key, val in _rx_cookie.findall(data):
        if key.lower() in _c_keys:
            if ckey:
                self[ckey][key] = _unquote(val)
        elif key[0] == ""$"":
            # RFC2109: NAMEs that begin with $ are reserved for other uses
            # and must not be used by applications.
            continue
        else:
            self[key] = _unquote(val)
            ckey = key
","elif key [ 0 ] == ""$"" :",143
"def load_cases(full_path):
    all_test_data = json.load(open(full_path), object_pairs_hook=OrderedDict)
    for test_data in all_test_data:
        given = test_data[""given""]
        for case in test_data[""cases""]:
            if ""result"" in case:
                test_type = ""result""
            elif ""error"" in case:
                test_type = ""error""
            elif ""bench"" in case:
                test_type = ""bench""
            else:
                raise RuntimeError(""Unknown test type: %s"" % json.dumps(case))
            yield (given, test_type, case)
","if ""result"" in case :",183
"def delete(self):
    if not self.force and not self.exists():
        return []
    cmd = [""delete""]
    if self.filename:
        cmd.append(""--filename="" + self.filename)
    else:
        if not self.resource:
            self.module.fail_json(msg=""resource required to delete without filename"")
        cmd.append(self.resource)
        if self.name:
            cmd.append(self.name)
        if self.label:
            cmd.append(""--selector="" + self.label)
        if self.all:
            cmd.append(""--all"")
        if self.force:
            cmd.append(""--ignore-not-found"")
    return self._execute(cmd)
",if not self . resource :,189
"def validate_latex_theme_options(app: Sphinx, config: Config) -> None:
    for key in list(config.latex_theme_options):
        if key not in Theme.UPDATABLE_KEYS:
            msg = __(""Unknown theme option: latex_theme_options[%r], ignored."")
            logger.warning(msg % (key,))
            config.latex_theme_options.pop(key)
",if key not in Theme . UPDATABLE_KEYS :,103
"def connectionLost(self, reason):
    if self.factory.debug:
        self.log.info(
            ""WampRawSocketProtocol: connection lost: reason = '{0}'"".format(reason)
        )
    try:
        wasClean = isinstance(reason.value, ConnectionDone)
        self._session.onClose(wasClean)
    except Exception as e:
        # silently ignore exceptions raised here ..
        if self.factory.debug:
            self.log.info(
                ""WampRawSocketProtocol: ApplicationSession.onClose raised ({0})"".format(
                    e
                )
            )
    self._session = None
",if self . factory . debug :,172
"def parse(filename):
    dead_links = []
    with open(filename, ""r"") as file_:
        for line in file_.readlines():
            res = reference_line.search(line)
            if res:
                if not exists(res.group(1)):
                    dead_links.append(res.group(1))
    return dead_links
",if res :,96
"def is_speaker_at_session(self, session_id):
    try:
        session = (
            Session.query.filter(Session.speakers.any(Speaker.user_id == self.id))
            .filter(Session.id == session_id)
            .one()
        )
        if session:
            return True
        else:
            return False
    except MultipleResultsFound:
        return False
    except NoResultFound:
        return False
",if session :,123
"def _validate_deployment_name(namespace):
    # If missing,try come out with a name associated with the template name
    if namespace.deployment_name is None:
        template_filename = None
        if namespace.template_file and os.path.isfile(namespace.template_file):
            template_filename = namespace.template_file
        if namespace.template_uri and urlparse(namespace.template_uri).scheme:
            template_filename = urlsplit(namespace.template_uri).path
        if template_filename:
            template_filename = os.path.basename(template_filename)
            namespace.deployment_name = os.path.splitext(template_filename)[0]
        else:
            namespace.deployment_name = ""deployment1""
",if namespace . template_file and os . path . isfile ( namespace . template_file ) :,186
"def mro(cls):
    if self.ready:
        if cls.__name__ == ""B1"":
            B2.__bases__ = (B1,)
        if cls.__name__ == ""B2"":
            B1.__bases__ = (B2,)
    return type.mro(cls)
","if cls . __name__ == ""B2"" :",76
"def mark_shard_complete():
    try:
        marker.refresh_from_db()
    except DeferIterationMarker.DoesNotExist:
        logger.warning(
            ""TaskMarker with ID: %s has vanished, cancelling task"", marker_id
        )
        return
    marker.shards_complete += 1
    marker.save()
    if marker.shards_complete == marker.shard_count:
        # Delete the marker if we were asked to
        if marker.delete_on_completion:
            marker.delete()
        defer(finalize, *args, _transactional=True, _queue=task_queue_name(), **kwargs)
",if marker . delete_on_completion :,160
"def TryMerge(self, d):
    while d.avail() > 0:
        tt = d.getVarInt32()
        if tt == 10:
            length = d.getVarInt32()
            tmp = ProtocolBuffer.Decoder(d.buffer(), d.pos(), d.pos() + length)
            d.skip(length)
            self.add_public_certificate_list().TryMerge(tmp)
            continue
        if tt == 16:
            self.set_max_client_cache_time_in_second(d.getVarInt64())
            continue
        if tt == 0:
            raise ProtocolBuffer.ProtocolBufferDecodeError
        d.skipData(tt)
",if tt == 16 :,181
"def check_free(self, payload):
    # free_list: 'host=10.0.0.1', 'user=anonymous', 'host=10.0.0.7,user=test', ...
    for m in self.free_list:
        args = m.split("","", 1)
        for arg in args:
            k, v = arg.split(""="", 1)
            if payload[k] != v:
                break
        else:
            return True
    return False
",if payload [ k ] != v :,126
"def getInnerText(element):
    # To mimic IE's 'innerText' property in the W3C DOM, we need to recursively
    # concatenate all child text nodes (depth first).
    text = """"
    child = element.firstChild
    while child:
        if child.nodeType == 1:
            text += getInnerText(child)
        elif child.nodeValue:
            text += child.nodeValue
        child = child.nextSibling
    return text
",if child . nodeType == 1 :,119
"def get_complete_http(self):
    finished = []
    c = self.connection.cursor()
    rows = c.execute(""SELECT * FROM http WHERE complete=1"").fetchall()
    for row in rows:
        o = pickle.loads(row[""object""])
        uadat = c.execute(""SELECT * FROM ua WHERE parent_id=?"", (o.id,)).fetchall()
        for ua in uadat:
            uao = pickle.loads(ua[""object""])
            if uao is not None and uao.source_code is not None and o.source_code:
                o.add_ua_data(uao)
        finished.append(o)
    c.close()
    return finished
",if uao is not None and uao . source_code is not None and o . source_code :,178
"def get_tools(self, found_files):
    self.configured_by = {}
    runners = []
    for tool_name in self.tools_to_run:
        tool = tools.TOOLS[tool_name]()
        config_result = tool.configure(self, found_files)
        if config_result is None:
            configured_by = None
            messages = []
        else:
            configured_by, messages = config_result
            if messages is None:
                messages = []
        self.configured_by[tool_name] = configured_by
        self.messages += messages
        runners.append(tool)
    return runners
",if messages is None :,172
"def _yield_batches(self, keys):
    while self._shuffling_buffer.can_retrieve():
        post_shuffled_row = self._shuffling_buffer.retrieve()
        if not isinstance(post_shuffled_row, dict):
            # This is for the case of batched reads. Here we restore back the
            # dictionary format of records
            post_shuffled_row = dict(zip(keys, post_shuffled_row))
        self._batch_acc.append(post_shuffled_row)
        # Batch is ready? Collate and emmit
        if len(self._batch_acc) == self.batch_size:
            yield self.collate_fn(self._batch_acc)
            self._batch_acc = []
",if len ( self . _batch_acc ) == self . batch_size :,187
"def action_open_file_filtered_dialog(self, widget):
    try:
        fname = self.main_window.open_file_dialog(
            title=""Open file with Toga"",
            multiselect=False,
            file_types=[""doc"", ""txt""],
        )
        if fname is not None:
            self.label.text = ""File to open:"" + fname
        else:
            self.label.text = ""No file selected!""
    except ValueError:
        self.label.text = ""Open file dialog was canceled""
",if fname is not None :,143
"def validate_vars(env):
    """"""Validate the PCH and PCHSTOP construction variables.""""""
    if ""PCH"" in env and env[""PCH""]:
        if ""PCHSTOP"" not in env:
            raise SCons.Errors.UserError(
                ""The PCHSTOP construction must be defined if PCH is defined.""
            )
        if not SCons.Util.is_String(env[""PCHSTOP""]):
            raise SCons.Errors.UserError(
                ""The PCHSTOP construction variable must be a string: %r""
                % env[""PCHSTOP""]
            )
","if ""PCHSTOP"" not in env :",156
"def page_func(page_num):
    playlist = self._call_api(
        ""product/playlist"",
        show_id,
        {
            ""playListId"": playlist_id,
            ""pageNumber"": page_num,
            ""pageSize"": 30,
            ""sorts"": [{""order"": ""DESC"", ""type"": ""SORTDATE""}],
        },
    )
    for product in playlist.get(""productList"", {}).get(""products"", []):
        product_url = product.get(""productUrl"", []).get(""url"")
        if not product_url:
            continue
        yield self.url_result(
            product_url, ""Shahid"", str_or_none(product.get(""id"")), product.get(""title"")
        )
",if not product_url :,196
"def forward(self, x):
    for rproj, conv in zip(self.residual_proj, self.conv_layers):
        residual = x
        x = conv(x)
        if self.skip_connections:
            if rproj is not None:
                residual = rproj(residual)
            x = (x + residual) * self.residual_scale
    return x
",if rproj is not None :,104
"def _make_results_dir(self):
    r""""""Makes directory for saving eqa-cnn-pretrain eval results.""""""
    for s_type in [""rgb"", ""seg"", ""depth""]:
        dir_name = self.config.RESULTS_DIR.format(split=""val"", type=s_type)
        if not os.path.isdir(dir_name):
            os.makedirs(dir_name)
",if not os . path . isdir ( dir_name ) :,99
"def ignore_callback_errors(self, ignore):
    EventEmitter.ignore_callback_errors.fset(self, ignore)
    for emitter in self._emitters.values():
        if isinstance(emitter, EventEmitter):
            emitter.ignore_callback_errors = ignore
        elif isinstance(emitter, EmitterGroup):
            emitter.ignore_callback_errors_all(ignore)
","elif isinstance ( emitter , EmitterGroup ) :",95
"def cron_starter(*args: Any) -> None:
    _tz = self.conf.timezone if timezone is None else timezone
    while not self.should_stop:
        await self.sleep(cron.secs_for_next(cron_format, _tz))
        if not self.should_stop:
            should_run = not on_leader or self.is_leader()
            if should_run:
                with self.trace(shortlabel(fun), trace_enabled=traced):
                    await fun(*args)
",if not self . should_stop :,139
"def rotateafter(self):
    if self.i != self.previ:
        i = self.parent.l.GetSelection()
        if i != wx.NOT_FOUND:
            self.parent.models[self.parent.l.GetString(i)].rot -= 5 * (
                self.i - self.previ
            )
        self.previ = self.i
        self.Refresh()
",if i != wx . NOT_FOUND :,106
"def select(model, path, iter_, paths_):
    (paths, first) = paths_
    value = model.get_value(iter_)
    if value is None:
        return not bool(paths)
    value = normalize_path(value)
    if value in paths:
        self.get_child().get_selection().select_path(path)
        paths.remove(value)
        if not first:
            self.get_child().set_cursor(path)
            # copy treepath, gets invalid after the callback
            first.append(path.copy())
    else:
        for fpath in paths:
            if fpath.startswith(value):
                self.get_child().expand_row(path, False)
    return not bool(paths)
",if fpath . startswith ( value ) :,194
"def read_logs_file(logs_path) -> List[V1Log]:
    if not os.path.exists(logs_path):
        return []
    async with aiofiles.open(logs_path, mode=""r"") as f:
        contents = await f.read()
        if contents:
            # Version handling
            if "".plx"" in logs_path:
                return V1Logs.read_csv(contents).logs
            # Legacy logs
            logs = V1Logs.read(contents)
            return logs.logs
    return []
","if "".plx"" in logs_path :",145
"def adjust_sockets(self):
    variables = self.get_variables()
    for key in self.inputs.keys():
        if key not in variables and key not in [""Field""]:
            self.debug(
                ""Input {} not in variables {}, remove it"".format(key, str(variables))
            )
            self.inputs.remove(self.inputs[key])
    for v in variables:
        if v not in self.inputs:
            self.debug(
                ""Variable {} not in inputs {}, add it"".format(
                    v, str(self.inputs.keys())
                )
            )
            self.inputs.new(""SvStringsSocket"", v)
","if key not in variables and key not in [ ""Field"" ] :",183
"def run(self):
    while self.running:
        cmd = self.cmds.get()
        if cmd == ""stop"":
            break
        elif cmd == ""clear"":
            dead_tasks = []
            for task in self.tasks:
                if task.status == Task.FINISH or task.status == Task.ERROR:
                    dead_tasks.append(task)
            for dead_task in dead_tasks:
                self.tasks.remove(dead_task)
","if cmd == ""stop"" :",131
"def process(self, node):
    self.vars = []
    for child in node.childNodes:
        if child.nodeType == node.ELEMENT_NODE:
            child_text = get_xml_text(child)
            if child_text == """":  # pragma:nocover
                continue
            if child.nodeName == ""Real"":
                for val in re.split(""[\t ]+"", child_text):
                    self.vars.append(1.0 * eval(val))
    return self
","if child . nodeName == ""Real"" :",135
"def drain(self, fd):
    """"""Make `fd` unreadable.""""""
    while True:
        try:
            if not os.read(fd, 4096):
                return
        except OSError:
            e = sys.exc_info()[1]
            if e.args[0] == errno.EAGAIN:
                return
            raise
",if e . args [ 0 ] == errno . EAGAIN :,97
"def parse(s):
    """"""Parse the output below to create a new StopWatch.""""""
    stopwatch = StopWatch()
    for line in s.splitlines():
        if line.strip():
            parts = line.split(None)
            name = parts[0]
            if name != ""%"":  # ie not the header line
                rest = (float(v) for v in parts[2:])
                stopwatch.times[parts[0]].merge(Stat.build(*rest))
    return stopwatch
",if line . strip ( ) :,128
"def delete(identifier, filenames=None, **kwargs):
    item = get_item(identifier)
    if filenames:
        if not isinstance(filenames, (set, list)):
            filenames = [filenames]
        for f in item.iter_files():
            if f.name not in filenames:
                continue
            f.delete(**kwargs)
","if not isinstance ( filenames , ( set , list ) ) :",91
"def _get_absolute_timeout(self, timeout):
    if timeout is Timeout.DEFAULT_TIMEOUT:
        return 5  # 5s is the default timeout for URLFetch.
    if isinstance(timeout, Timeout):
        if timeout.read is not timeout.connect:
            warnings.warn(
                ""URLFetch does not support granular timeout settings, ""
                ""reverting to total timeout."",
                AppEnginePlatformWarning,
            )
        return timeout.total
    return timeout
",if timeout . read is not timeout . connect :,124
"def _add_annotation_to_imports(
    self, annotation: cst.Attribute
) -> Union[cst.Name, cst.Attribute]:
    key = get_full_name_for_node(annotation.value)
    if key is not None:
        # Don't attempt to re-import existing imports.
        if key in self.existing_imports:
            return annotation
        import_name = get_full_name_for_node(annotation.attr)
        if import_name is not None:
            AddImportsVisitor.add_needed_import(self.context, key, import_name)
    return annotation.attr
",if import_name is not None :,156
"def unique_definitions(cls, defns):
    """"""Takes a collection of defns and returns the unique list of defns.""""""
    unique_defns = []
    for defn in defns:
        for unique_defn in unique_defns:
            if unique_defn.path == defn.path and unique_defn == defn:
                # defn is already in the unique_defn list.
                break
        else:
            unique_defns.append(defn)
    return unique_defns
",if unique_defn . path == defn . path and unique_defn == defn :,126
"def store_data(self, store_loc, **kwargs):
    """"""Put arrays to store""""""
    # print(store_loc)
    g = self.store.create_group(store_loc)
    for (
        k,
        v,
    ) in kwargs.items():
        # print(type(v[0]))
        # print(k)
        if type(v) == list:
            if len(v) != 0:
                if type(v[0]) is np.str_ or type(v[0]) is str:
                    v = [a.encode(""utf8"") for a in v]
        g.create_dataset(k, data=v, compression=self.clib, compression_opts=self.clev)
",if type ( v ) == list :,191
"def connect_to_uri(self, uri, autoconnect=None, do_start=True):
    try:
        conn = self._check_conn(uri)
        if not conn:
            # Unknown connection, add it
            conn = self.add_conn(uri)
        if autoconnect is not None:
            conn.set_autoconnect(bool(autoconnect))
        self.show_manager()
        if do_start:
            conn.open()
        return conn
    except Exception:
        logging.exception(""Error connecting to %s"", uri)
        return None
",if autoconnect is not None :,152
"def fn(n):
    while n < 3:
        if n < 0:
            yield ""less than zero""
        elif n == 0:
            yield ""zero""
        elif n == 1:
            yield ""one""
        else:
            yield ""more than one""
        n += 1
",elif n == 0 :,84
"def closeEvent(self, e):
    self.common.log(""MainWindow"", ""closeEvent"")
    if self.tabs.are_tabs_active():
        # Open the warning dialog
        self.common.log(""MainWindow"", ""closeEvent, opening warning dialog"")
        self.close_dialog.exec_()
        # Close
        if self.close_dialog.clickedButton() == self.close_dialog.accept_button:
            self.system_tray.hide()
            e.accept()
        # Cancel
        else:
            e.ignore()
        return
    self.system_tray.hide()
    e.accept()
",if self . close_dialog . clickedButton ( ) == self . close_dialog . accept_button :,165
"def _stop_child_activities(self, name=None):
    """"""Stop all child activities spawn by this activity.""""""
    # Makes a list copy of items() to avoid dictionary size changed
    # during iteration
    for child_name, child in list(self._child_activity_map.items()):
        if name is not None and name != child_name:
            continue
        LOG.debug(""%s: Stopping child activity %s "", self.name, child_name)
        if child.started:
            child.stop()
        self._child_activity_map.pop(child_name, None)
",if name is not None and name != child_name :,150
"def add_libdirs(self, envvar, sep, fatal=False):
    v = os.environ.get(envvar)
    if not v:
        return
    for dir in str.split(v, sep):
        dir = str.strip(dir)
        if not dir:
            continue
        dir = os.path.normpath(dir)
        if os.path.isdir(dir):
            if not dir in self.library_dirs:
                self.library_dirs.append(dir)
        elif fatal:
            fail(""FATAL: bad directory %s in environment variable %s"" % (dir, envvar))
",elif fatal :,159
"def _serialize_list(array, previous):
    array = array or []
    previous = previous or []
    params = {}
    for i, v in enumerate(array):
        previous_item = previous[i] if len(previous) > i else None
        if hasattr(v, ""serialize""):
            params[str(i)] = v.serialize(previous_item)
        else:
            params[str(i)] = _compute_diff(v, previous_item)
    return params
","if hasattr ( v , ""serialize"" ) :",122
"def list_bucket(self, prefix="""", delimiter="""", headers=None, all_versions=False):
    self._check_bucket_uri(""list_bucket"")
    bucket = self.get_bucket(headers=headers)
    if all_versions:
        return (
            v
            for v in bucket.list_versions(
                prefix=prefix, delimiter=delimiter, headers=headers
            )
            if not isinstance(v, DeleteMarker)
        )
    else:
        return bucket.list(prefix=prefix, delimiter=delimiter, headers=headers)
","if not isinstance ( v , DeleteMarker )",142
"def writeattr(stream, text):
    countdouble = text.count('""')
    if countdouble:
        countsingle = text.count(""'"")
        if countdouble <= countsingle:
            entities = {'""': ""&quot;""}
            quote = '""'
        else:
            entities = {""'"": ""&apos;""}
            quote = ""'""
    else:
        entities = {}
        quote = '""'
    stream.write(quote)
    writetext(stream, text, entities)
    stream.write(quote)
",if countdouble <= countsingle :,133
"def __gt__(self, other):
    if not isinstance(other, self.__class__):
        other = self.__class__(other)
    for part, value in self.parts:
        other_value = other[part]
        if part in LETTERS:
            cmp = self._cmp_part(value or ""z"", other_value or ""z"")
        else:
            cmp = self._cmp_part(value, other_value)
        if cmp == 0:
            continue
        else:
            return cmp == 1
    return False
",if cmp == 0 :,141
"def _concretize(self, n_cls, t1, t2, join_or_meet, translate):
    ptr_class = self._pointer_class()
    if n_cls is ptr_class:
        if isinstance(t1, ptr_class) and isinstance(t2, ptr_class):
            # we need to merge them
            return ptr_class(join_or_meet(t1.basetype, t2.basetype, translate))
        if isinstance(t1, ptr_class):
            return t1
        elif isinstance(t2, ptr_class):
            return t2
        else:
            # huh?
            return ptr_class(BottomType())
    return n_cls()
","if isinstance ( t1 , ptr_class ) :",181
"def __init__(self, items=None):
    super().__init__()
    self.include_dirs = []
    self._add_member(""src_files"", FileList, ""C source files for VPI library"")
    self._add_member(""include_files"", FileList, ""C include files for VPI library"")
    self._add_member(
        ""libs"", StringList, ""External libraries linked with the VPI library""
    )
    if items:
        self.load_dict(items)
        if self.include_files:
            self.include_dirs += unique_dirs(self.include_files)
        self.export_files = self.src_files + self.include_files
",if self . include_files :,171
"def __init__(self, parent_element):
    if parent_element.items():
        self.update(dict(parent_element.items()))
    for element in parent_element:
        if len(element) > 0:
            if element.tag == element[0].tag:
                aDict = ListParser(element)
            else:
                aDict = DictParser(element)
            if element.items():
                aDict.update(dict(element.items()))
            self.update({element.tag: aDict})
        elif element.items():
            self.update({element.tag: dict(element.items())})
        else:
            self.update({element.tag: element.text})
",if element . items ( ) :,190
"def _shares_in_results(data):
    shares_in_device, shares_in_subdevice = False, False
    for plugin_name, plugin_result in data.iteritems():
        if plugin_result[""status""] == ""error"":
            continue
        if ""device"" not in plugin_result:
            continue
        if ""disk_shares"" in plugin_result[""device""]:
            shares_in_device = True
        for subdevice in plugin_result[""device""].get(""subdevices"", []):
            if ""disk_shares"" in subdevice:
                shares_in_subdevice = True
                break
    return shares_in_device, shares_in_subdevice
","if ""disk_shares"" in subdevice :",175
"def decorator(self, command, *args, **kwargs):
    if required_keys:
        missing_keys = diff_keys(required_keys, command)
        if missing_keys:
            raise InvalidCommand(
                ""Command missing %s of required""
                "" keys %s"" % (missing_keys, required_keys)
            )
    return func(self, command, *args, **kwargs)
",if missing_keys :,107
"def xml(self):
    out = [""<spreadsheet>""]
    for (x, y), cell in self.cells.items():
        if hasattr(cell, ""xml""):
            cellxml = cell.xml()
        else:
            cellxml = ""<value>%s</value>"" % escape(cell)
        out.append('<cell row=""%s"" col=""%s"">\n  %s\n</cell>' % (y, x, cellxml))
    out.append(""</spreadsheet>"")
    return ""\n"".join(out)
","if hasattr ( cell , ""xml"" ) :",127
"def speed_tester_d(self, uid):
    if uid not in self._speed_tester_d:
        if self.mu:  # TODO
            self._speed_tester_d[uid] = SpeedTester(
                self._config.get(""speed_limit_per_user"", 0)
            )
        else:
            self._speed_tester_d[uid] = SpeedTester(
                self._config.get(""speed_limit_per_user"", 0)
            )
    return self._speed_tester_d[uid]
",if self . mu :,143
"def process_error(self, data):
    error = data.get(""error"")
    if error:
        if error == ""access_denied"":
            raise AuthCanceled(self)
        else:
            raise AuthUnknownError(self, ""Jawbone error was {0}"".format(error))
    return super().process_error(data)
","if error == ""access_denied"" :",86
"def _do_test_fetch_result(self, results, remote):
    # self._print_fetchhead(remote.repo)
    self.assertGreater(len(results), 0)
    self.assertIsInstance(results[0], FetchInfo)
    for info in results:
        self.assertIsInstance(info.note, string_types)
        if isinstance(info.ref, Reference):
            self.assertTrue(info.flags)
        # END reference type flags handling
        self.assertIsInstance(info.ref, (SymbolicReference, Reference))
        if info.flags & (info.FORCED_UPDATE | info.FAST_FORWARD):
            self.assertIsInstance(info.old_commit, Commit)
        else:
            self.assertIsNone(info.old_commit)
","if isinstance ( info . ref , Reference ) :",186
"def init_ftp_server(self):
    if self.get_config(""ftpd"", ""enabled"", False, boolean=True):
        accountfile = from_utf8_or_none(self.get_config(""ftpd"", ""accounts.file"", None))
        if accountfile:
            accountfile = abspath_expanduser_unicode(accountfile, base=self.basedir)
        accounturl = self.get_config(""ftpd"", ""accounts.url"", None)
        ftp_portstr = self.get_config(""ftpd"", ""port"", ""8021"")
        from allmydata.frontends import ftpd
        s = ftpd.FTPServer(self, accountfile, accounturl, ftp_portstr)
        s.setServiceParent(self)
",if accountfile :,190
"def configured_request_log_handlers(config, prefix=""query_log"", default_logger=None):
    """"""Returns configured query loggers as defined in the `config`.""""""
    handlers = []
    for section in config.sections():
        if section.startswith(prefix):
            options = dict(config.items(section))
            type_ = options.pop(""type"")
            if type_ == ""default"":
                logger = default_logger or get_logger()
                handler = ext.request_log_handler(""default"", logger)
            else:
                handler = ext.request_log_handler(type_, **options)
            handlers.append(handler)
    return handlers
","if type_ == ""default"" :",174
"def string(self):
    """"""Returns a PlayString in string format from the Patterns values""""""
    string = """"
    for item in self.data:
        if isinstance(item, (PGroup, GeneratorPattern)):
            string += item.string()
        elif isinstance(item, Pattern):
            string += (
                ""(""
                + """".join(
                    [
                        (s.string() if hasattr(s, ""string"") else str(s))
                        for s in item.data
                    ]
                )
                + "")""
            )
        else:
            string += str(item)
    return string
","elif isinstance ( item , Pattern ) :",183
"def locked_deps(package, poetry):
    reqs = []
    packages = poetry.locker.locked_repository(False).packages
    for p in packages:
        dep = p.to_dependency()
        line = ""{}=={}"".format(p.name, p.version)
        requirement = dep.to_pep_508()
        if "";"" in requirement:
            line += ""; {}"".format(requirement.split("";"")[1].strip())
        reqs.append(line)
    return reqs, defaultdict(list)
","if "";"" in requirement :",132
"def _paste_columns(self, topleft_corner, columns):
    starting_column = topleft_corner[1]
    number_of_columns = self.number_of_columns()
    for index, column in enumerate(columns):
        set_index = starting_column + index
        if set_index < number_of_columns:
            self.set_column_at(set_index, column, starting=topleft_corner[0])
        else:
            real_column = [constants.DEFAULT_NA] * topleft_corner[0]
            real_column += column
            self.extend_columns([real_column])
    self.__width, self.__array = uniform(self.__array)
",if set_index < number_of_columns :,172
"def check_objects_exist(self, compare_id, raise_exc=True):
    for uid in convert_compare_id_to_list(compare_id):
        if not self.existence_quick_check(uid):
            if raise_exc:
                raise FactCompareException(""{} not found in database"".format(uid))
            return True
    return False
",if raise_exc :,94
"def __add__(self, other):
    if hasattr(other, ""unit_type""):
        if other.unit_type != self.unit_type:
            raise UnitError(""Adding different types of units is"" "" not allowed"")
        if other.unit != self.unit:
            other = other.to(self.unit)
    return self.__class__(
        np.array(self) + np.array(other), unit_type=self.unit_type, unit=self.unit
    )
",if other . unit_type != self . unit_type :,123
"def extract(self, tar):
    max_nb = maxNbFile(self)
    for index, field in enumerate(tar.array(""file"")):
        if max_nb is not None and max_nb <= index:
            self.warning(
                ""TAR archive contains many files, but only first %s files are processed""
                % max_nb
            )
            break
        meta = Metadata(self)
        self.extractFile(field, meta)
        if meta.has(""filename""):
            title = _('File ""%s""') % meta.getText(""filename"")
        else:
            title = _(""File"")
        self.addGroup(field.name, meta, title)
",if max_nb is not None and max_nb <= index :,180
"def task_management_menu(activation, request):
    """"""Available tasks actions.""""""
    actions = []
    if request.user.has_perm(activation.flow_class._meta.manage_permission_name):
        for transition in activation.get_available_transitions():
            if transition.can_proceed(activation):
                url = activation.flow_task.get_task_url(
                    activation.task,
                    transition.name,
                    user=request.user,
                    namespace=request.resolver_match.namespace,
                )
                if url:
                    actions.append((transition.name.replace(""_"", "" "").title(), url))
    return {""actions"": actions, ""request"": request}
",if transition . can_proceed ( activation ) :,192
"def handle_default_mac_address(facts):
    for suffix in ("""", ""_eth0"", ""_igb0"", ""_bnx0"", ""_bge0"", ""_nfo0"", ""_nge0""):
        mac = facts.get(""macaddress{}"".format(suffix))
        if mac:
            try:
                result = MACAddressField.normalize(mac)
            except ValueError:
                continue
            if result[:6] in MAC_PREFIX_BLACKLIST:
                continue
            return result
",if result [ : 6 ] in MAC_PREFIX_BLACKLIST :,131
"def run(self):
    consumer = KafkaConsumer(
        bootstrap_servers=""localhost:9092"", auto_offset_reset=""earliest""
    )
    consumer.subscribe([""my-topic""])
    self.valid = 0
    self.invalid = 0
    for message in consumer:
        if len(message.value) == msg_size:
            self.valid += 1
        else:
            self.invalid += 1
        if consumer_stop.is_set():
            break
    consumer.close()
",if len ( message . value ) == msg_size :,136
"def createFields(self, fields):
    self.destroyFields()
    for name, label, args in fields:
        kwargs = dict(validator=_TransferValidator(name))
        if args:
            kwargs.update(args)
        stxt = wx.StaticText(self, -1, label)
        txt = wx.TextCtrl(self, **kwargs)
        self._contentSizer.Add(stxt, 0, wx.ALIGN_CENTER_VERTICAL | wx.ALIGN_RIGHT)
        self._contentSizer.Add(txt, 0, wx.EXPAND)
        self.__dict__[name] = """"
        self._fields[name] = (stxt, txt)
",if args :,162
"def poll_kafka(self):
    while True:
        val = self.do_poll()
        if val:
            yield self._emit(val)
        else:
            yield gen.sleep(self.poll_interval)
        if self.stopped:
            break
    self._close_consumer()
",if self . stopped :,85
"def _generate_toc(line):
    while 1:
        if line.startswith(""2""):
            line = 5
            while 1:
                if line:
                    line = 6
                    break
                elif not line:
                    line = 7
                    break
        elif not line:
            break
    return 1
",if line :,103
"def find_script(scriptId_or_file_or_url):
    # sha = hashlib.sha1(scriptId_or_file_or_url.encode('utf-8')).hexdigest()
    for item in file_to_scriptId:
        if item[""scriptId""].lower() == scriptId_or_file_or_url.lower():
            return item[""file""]
        if item[""file""].lower() == scriptId_or_file_or_url.lower():
            return item[""scriptId""]
        if item[""url""].lower() == scriptId_or_file_or_url.lower():
            return item[""scriptId""]
    return None
","if item [ ""file"" ] . lower ( ) == scriptId_or_file_or_url . lower ( ) :",165
"def __get_impute_number(some_data):
    impute_num_list = None
    data_size = None
    for line in some_data:
        processed_data = line[1][0]
        index_list = line[1][1]
        if not data_size:
            data_size = len(processed_data)
            # data_size + 1, the last element of impute_num_list used to count the number of ""some_data""
            impute_num_list = [0 for _ in range(data_size + 1)]
        impute_num_list[data_size] += 1
        for index in index_list:
            impute_num_list[index] += 1
    return np.array(impute_num_list)
",if not data_size :,200
"def get_shipping_address(self):
    """"""Returns Address object from shipping address fields if present""""""
    # shipping address fields can be `shipping_address_name` or `shipping_address`
    # try getting value from both
    for fieldname in (""shipping_address_name"", ""shipping_address""):
        shipping_field = self.meta.get_field(fieldname)
        if shipping_field and shipping_field.fieldtype == ""Link"":
            if self.get(fieldname):
                return frappe.get_doc(""Address"", self.get(fieldname))
    return {}
",if self . get ( fieldname ) :,141
"def _get_spawn_property(self, constraints, constraint_name, services):
    if services:
        # this isn't very nice
        if constraint_name == IMAGE_CONSTRAINT:
            return services[0].image
        elif constraint_name == CPUS_CONSTRAINT:
            return services[0].cpus
    for constraint in constraints:
        if constraint.name == constraint_name:
            return constraint.value
    return None
",if constraint_name == IMAGE_CONSTRAINT :,113
"def latest_extra_data(self, extra_dirs=None):
    base_name = os.path.splitext(os.path.basename(self.file_name))[0]
    extra_dirs.append(self.board.GetPlotOptions().GetOutputDirectory())
    file_dir_name = os.path.dirname(self.file_name)
    directories = [
        file_dir_name,
    ]
    for dir in extra_dirs:
        if not os.path.isabs(dir):
            dir = os.path.join(file_dir_name, dir)
        if os.path.exists(dir):
            directories.append(dir)
    return find_latest_schematic_data(base_name, directories)
",if os . path . exists ( dir ) :,181
"def _checkForLeftRightModifiers(cls, mod_state):
    mod_value = 0
    mod_strs = []
    for k, v in cls._OS_MODIFIERS:
        if mod_state & k > 0:
            mod_value += KeyboardConstants._modifierCodes.getID(v)
            mod_strs.append(modifier_name_mappings.get(v, ""MISSING_MOD_NAME""))
    return mod_value, mod_strs
",if mod_state & k > 0 :,116
"def _decode_pattern_list(data):
    rv = []
    contains_dict = False
    for item in data:
        if isinstance(item, list):
            item = _decode_pattern_list(item)
        elif isinstance(item, dict):
            item = _decode_pattern_dict(item)
            contains_dict = True
        rv.append(item)
    # avoid sorting if any element in the list is a dict
    if not contains_dict:
        rv = sorted(rv)
    return rv
","if isinstance ( item , list ) :",133
"def get_blob(self, blobname, ctlr=None, specific_dir=None):
    self._acquire_lock()
    try:
        dbsubpath = self._dbsubpath_from_blobname(
            blobname, ctlr=ctlr, specific_dir=specific_dir
        )
        if dbsubpath is not None:
            return self.lang_zone.load_blob(dbsubpath)
        else:
            return None
    finally:
        self._release_lock()
",if dbsubpath is not None :,126
"def get_tasks(self):
    for task in asyncio.all_tasks(loop=self.middleware.loop):
        formatted = None
        frame = None
        frames = []
        for frame in task.get_stack():
            cur_frame = get_frame_details(frame, self.logger)
            if cur_frame:
                frames.append(cur_frame)
        if frame:
            formatted = traceback.format_stack(frame)
        yield {
            ""stack"": formatted,
            ""frames"": frames,
        }
",if frame :,146
"def main(args):
    optim = Adam({""lr"": args.lr})
    elbo = JitTrace_ELBO() if args.jit else Trace_ELBO()
    svi = SVI(model, guide, optim, loss=elbo)
    pyro.clear_param_store()
    for j in range(args.num_epochs):
        loss = svi.step(data)
        if j % 100 == 0:
            logging.info(""[epoch %04d] loss: %.4f"" % (j + 1, loss))
    for name, value in pyro.get_param_store().items():
        logging.info(name)
        logging.info(value.detach().cpu().numpy())
",if j % 100 == 0 :,174
"def create_var_list(scope, var_lists, shape):
    vars = []
    for idx, v in enumerate(var_lists):
        name = ""{}_{}"".format(scope, idx)
        if shape is None:
            var = fluid.data(name, shape=v.shape)
        else:
            var = fluid.data(name, shape=shape + list(v[0].shape))
        var.stop_gradient = False
        vars.append(var)
    return vars
",if shape is None :,126
"def dr_relation(self, C, trans, nullable):
    dr_set = {}
    state, N = trans
    terms = []
    g = self.lr0_goto(C[state], N)
    for p in g:
        if p.lr_index < p.len - 1:
            a = p.prod[p.lr_index + 1]
            if a in self.grammar.Terminals:
                if a not in terms:
                    terms.append(a)
    # This extra bit is to handle the start state
    if state == 0 and N == self.grammar.Productions[0].prod[0]:
        terms.append(""$end"")
    return terms
",if a in self . grammar . Terminals :,174
"def get_field_values(self, fields):
    field_values = []
    for field in fields:
        # Title is special case
        if field == ""title"":
            value = self.get_title_display()
        elif field == ""country"":
            try:
                value = self.country.printable_name
            except exceptions.ObjectDoesNotExist:
                value = """"
        elif field == ""salutation"":
            value = self.salutation
        else:
            value = getattr(self, field)
        field_values.append(value)
    return field_values
","if field == ""title"" :",158
"def run(self, event, lambda_context):
    self.setup_exec_environment(event)
    resource_sets = self.get_resource_sets(event)
    result_sets = {}
    for (account_id, region), rarns in resource_sets.items():
        self.assume_member({""account"": account_id, ""region"": region})
        resources = self.resolve_resources(event)
        rset = result_sets.setdefault((account_id, region), [])
        if resources:
            rset.extend(self.run_resource_set(event, resources))
    return result_sets
",if resources :,151
"def read(self, sock):
    data = self.sock.recv(64 * 1024)
    ready_to_read, ready_to_write, in_error = select.select(
        [self.sock], [], [], self.timeout
    )
    while len(ready_to_read) == 1:
        more_data = self.sock.recv(64 * 1024)
        if len(more_data) == 0:
            break
        data = data + more_data
        ready_to_read, ready_to_write, in_error = select.select(
            [self.sock], [], [], self.timeout
        )
    return data
",if len ( more_data ) == 0 :,164
"def _check_ids(el, filename, parent_id):
    """"""Recursively walks through tree and check if every object has ID""""""
    for child in el:
        if child.tag == ""object"":
            msg = ""Widget has no ID in %s; class %s; Parent id: %s"" % (
                filename,
                child.attrib[""class""],
                parent_id,
            )
            assert ""id"" in child.attrib and child.attrib[""id""], msg
            for subel in child:
                if subel.tag == ""child"":
                    _check_ids(subel, filename, child.attrib[""id""])
","if child . tag == ""object"" :",173
"def get(self, request, *args, **kwargs):
    url = self.get_redirect_url(**kwargs)
    if url:
        if self.permanent:
            return http.HttpResponsePermanentRedirect(url)
        else:
            return http.HttpResponseRedirect(url)
    else:
        logger.warning(
            ""Gone: %s"" % self.request.path,
            extra={""status_code"": 410, ""request"": self.request},
        )
        return http.HttpResponseGone()
",if self . permanent :,135
"def test_representation(self):
    # Test that the state space representation in the measurement error
    # case is correct
    for name in self.model.ssm.shapes.keys():
        if name == ""obs"":
            pass
        elif name == ""obs_cov"":
            actual = self.results2.filter_results.obs_cov
            desired = np.diag(self.true_measurement_error_variances)[:, :, np.newaxis]
            assert_equal(actual, desired)
        else:
            assert_equal(
                getattr(self.results2.filter_results, name),
                getattr(self.results.filter_results, name),
            )
","elif name == ""obs_cov"" :",179
"def process_formdata(self, valuelist):
    if valuelist:
        date_str = "" "".join(valuelist)
        if not date_str:
            self.data = None
            raise ValidationError(self.gettext(""Please input a date/time value""))
        parse_kwargs = self.parse_kwargs.copy()
        if ""default"" not in parse_kwargs:
            try:
                parse_kwargs[""default""] = self.default()
            except TypeError:
                parse_kwargs[""default""] = self.default
        try:
            self.data = parser.parse(date_str, **parse_kwargs)
        except ValueError:
            self.data = None
            raise ValidationError(self.gettext(""Invalid date/time input""))
",if not date_str :,196
"def get_bounding_box(self):
    for key in self.h5f[""Data_Products""].keys():
        if key.startswith(""VIIRS"") and key.endswith(""GEO""):
            lats = self.h5f[""Data_Products""][key][key + ""_Gran_0""].attrs[
                ""G-Ring_Latitude""
            ]
            lons = self.h5f[""Data_Products""][key][key + ""_Gran_0""].attrs[
                ""G-Ring_Longitude""
            ]
            break
    else:
        raise KeyError(""Cannot find bounding coordinates!"")
    return lons.ravel(), lats.ravel()
","if key . startswith ( ""VIIRS"" ) and key . endswith ( ""GEO"" ) :",175
"def _get_doc_contents(self, attr_name):
    value = getattr(self, attr_name)
    if isinstance(value, BasicCommand.FROM_FILE):
        if value.filename is not None:
            trailing_path = value.filename
        else:
            trailing_path = os.path.join(self.name, attr_name + "".rst"")
        root_module = value.root_module
        doc_path = os.path.join(
            os.path.abspath(os.path.dirname(root_module.__file__)),
            ""examples"",
            trailing_path,
        )
        with _open(doc_path) as f:
            return f.read()
    else:
        return value
",if value . filename is not None :,191
"def __truediv__(self, val):
    if isinstance(val, Vector3):
        if val.x == 0 or val.y == 0 or val.z == 0:
            raise ZeroDivisionError()
        gd_obj = lib.godot_vector3_operator_divide_vector(self._gd_ptr, val._gd_ptr)
    else:
        if val is 0:
            raise ZeroDivisionError()
        gd_obj = lib.godot_vector3_operator_divide_scalar(self._gd_ptr, val)
    return Vector3.build_from_gdobj(gd_obj)
",if val is 0 :,148
"def _get_all_plugin_configs(self):
    with opentracing.global_tracer().start_active_span(""_get_all_plugin_configs""):
        if not hasattr(self, ""_plugin_configs""):
            self._plugin_configs = {
                pc.identifier: pc for pc in PluginConfiguration.objects.all()
            }
        return self._plugin_configs
","if not hasattr ( self , ""_plugin_configs"" ) :",94
"def msg(self, module, level, msg, *args, **kwargs):
    if self.level < level or level > len(LEVELS):
        return
    msg = str(msg).format(*args, **kwargs)
    with self.lock:
        self.output.write(FORMAT.format(module=module, level=LEVELS[level], msg=msg))
        if hasattr(self.output, ""flush""):
            self.output.flush()
","if hasattr ( self . output , ""flush"" ) :",111
"def opentemplatefile(self, options, fulltemplatepath):
    """"""Opens the template file (if required).""""""
    if fulltemplatepath is not None:
        if os.path.isfile(fulltemplatepath):
            return open(fulltemplatepath, ""r"")
        else:
            self.warning(""missing template file %s"" % fulltemplatepath)
    return None
",if os . path . isfile ( fulltemplatepath ) :,92
"def b58(args, parser):
    for arg in args.input:
        blob, is_hex_input = parse_arg(arg, args.b)
        if is_hex_input:
            print(b2h(blob))
            print(b2a_base58(blob))
            print(b2a_hashed_base58(blob))
        else:
            print(b2h(blob))
            print(b2a_base58(blob))
            try:
                blob = a2b_hashed_base58(arg)
                print(""valid hashed b58"")
                print(""contents: "", b2h(blob))
            except Exception:
                print(""not hashed b58"")
",if is_hex_input :,196
"def edit_file(self, filename):
    import subprocess
    editor = self.get_editor()
    if self.env:
        environ = os.environ.copy()
        environ.update(self.env)
    else:
        environ = None
    try:
        c = subprocess.Popen(
            ""{} {}"".format(shlex_quote(editor), shlex_quote(filename)),
            env=environ,
            shell=True,
        )
        exit_code = c.wait()
        if exit_code != 0:
            raise ClickException(""{}: Editing failed!"".format(editor))
    except OSError as e:
        raise ClickException(""{}: Editing failed: {}"".format(editor, e))
",if exit_code != 0 :,174
"def ascii85decode(data):
    n = b = 0
    out = """"
    for c in data:
        if ""!"" <= c and c <= ""u"":
            n += 1
            b = b * 85 + (ord(c) - 33)
            if n == 5:
                out += struct.pack("">L"", b)
                n = b = 0
        elif c == ""z"":
            assert n == 0
            out += ""\0\0\0\0""
        elif c == ""~"":
            if n:
                for _ in range(5 - n):
                    b = b * 85 + 84
                out += struct.pack("">L"", b)[: n - 1]
            break
    return out
","if ""!"" <= c and c <= ""u"" :",200
"def channel_to_netid(channel_name_or_id):
    try:
        channel = int(channel_name_or_id)
    except ValueError:
        netid = ""NETID_{}"".format(channel_name_or_id.upper())
        if hasattr(ics, netid):
            channel = getattr(ics, netid)
        else:
            raise ValueError(
                ""channel must be an integer or "" ""a valid ICS channel name""
            )
    return channel
","if hasattr ( ics , netid ) :",129
"def _find_this_and_next_frame(self, stack):
    for i in range(len(stack)):
        if stack[i].id == self._frame_id:
            if i == len(stack) - 1:  # last frame
                return stack[i], None
            else:
                return stack[i], stack[i + 1]
    raise AssertionError(""Frame doesn't exist anymore"")
",if stack [ i ] . id == self . _frame_id :,106
"def nested_update(org_dict, upd_dict):
    for key, value in upd_dict.items():
        if isinstance(value, dict):
            if key in org_dict:
                if not isinstance(org_dict[key], dict):
                    raise ValueError(
                        ""Mismatch between org_dict and upd_dict at node {}"".format(key)
                    )
                nested_update(org_dict[key], value)
            else:
                org_dict[key] = value
        else:
            org_dict[key] = value
","if not isinstance ( org_dict [ key ] , dict ) :",161
"def __myreduce(self, elements):
    first = elements[0]
    for i in range(1, len(elements), 2):
        if elements[i] == ""and"":
            first = first and elements[i + 1]
        elif elements[i] == ""or"":
            first = first or elements[i + 1]
    self.stack = []
    if isinstance(first, list):
        return [first]
    return first
","if elements [ i ] == ""and"" :",112
"def test_to_json_na(self):
    # Set a value as nan and make sure it's written
    self.df.loc[self.df[""BoroName""] == ""Queens"", ""Shape_Area""] = np.nan
    text = self.df.to_json()
    data = json.loads(text)
    self.assertTrue(len(data[""features""]) == 5)
    for f in data[""features""]:
        props = f[""properties""]
        self.assertEqual(len(props), 4)
        if props[""BoroName""] == ""Queens"":
            self.assertTrue(props[""Shape_Area""] is None)
","if props [ ""BoroName"" ] == ""Queens"" :",160
"def process(self, resources):
    resources = self.filter_resources(resources, ""TableStatus"", self.valid_status)
    if not len(resources):
        return
    futures = []
    client = local_session(self.manager.session_factory).client(""dynamodb"")
    with self.executor_factory(max_workers=2) as w:
        for table_set in chunks(resources, 20):
            futures.append(w.submit(self.delete_table, client, table_set))
        for f in as_completed(futures):
            if f.exception():
                self.log.error(
                    ""Exception deleting dynamodb table set \n %s"" % (f.exception())
                )
",if f . exception ( ) :,184
"def skip_loss_scaling(self, backend_config=None):
    if self.loss_scaling is not False:
        if self.dtype != numpy.float16:
            msg = ""loss_scaling is tested when dtype is float16.""
            return True, msg
        if backend_config is not None and not backend_config.use_cuda:
            msg = ""loss_scaling is tested when use_cuda is True.""
            return True, msg
    return False, None
",if self . dtype != numpy . float16 :,120
"def writeLibraryControllers(fp, human, meshes, skel, config, shapes=None):
    progress = Progress(len(meshes), None)
    fp.write(""\n  <library_controllers>\n"")
    for mIdx, mesh in enumerate(meshes):
        subprog = Progress()(0, 0.5)
        if skel:
            writeSkinController(fp, human, mesh, skel, config)
        subprog(0.5, 1)
        if shapes is not None:
            writeMorphController(fp, mesh, shapes[mIdx], config)
        progress.step()
    fp.write(""  </library_controllers>\n"")
",if shapes is not None :,172
"def doit():
    recipes_path = expanduser(""recipes.pprint"")
    recipe_dicts = eval(open(recipes_path).read())
    for r in recipe_dicts:
        for key in r.keys():
            if key not in (""desc"", ""comments""):
                del r[key]
        for c in r[""comments""]:
            for key in c.keys():
                if key not in (""comment"", ""title""):
                    del c[key]
    f = open(""stripped.pprint"", ""w"")
    f.write(pformat(recipe_dicts))
    f.close()
","if key not in ( ""desc"" , ""comments"" ) :",163
"def _dispatchBubblingEvent(self, tag, evtType, evtObject):
    for node in tag.parents:
        if node is None:  # pragma: no cover
            break
        if not node._listeners:
            continue
        if evtObject._stoppedPropagation:  # pragma: no cover
            continue
        capture_listeners, bubbling_listeners = self._get_listeners(
            node, evtType
        )  # pylint:disable=unused-variable
        for c in bubbling_listeners:
            evtObject.currentTarget = node._node
            self.do_dispatch(c, evtObject)
",if node is None :,165
"def connect(self):
    if self.session is None:
        self.session = requests.Session()
        if isinstance(self.verify, six.string_types):
            self.session.mount(""httpsds8k://"", requests.adapters.HTTPAdapter())
        else:
            self.session.mount(""https://"", requests.adapters.HTTPAdapter())
        self.session.verify = self.verify
","if isinstance ( self . verify , six . string_types ) :",100
"def get_latest_tasks(cls, tasks):
    tasks_group = {}
    for task in tasks:
        task_key = cls.task_key(
            task_id=task.f_task_id, role=task.f_role, party_id=task.f_party_id
        )
        if task_key not in tasks_group:
            tasks_group[task_key] = task
        elif task.f_task_version > tasks_group[task_key].f_task_version:
            # update new version task
            tasks_group[task_key] = task
    return tasks_group
",elif task . f_task_version > tasks_group [ task_key ] . f_task_version :,160
"def wrapper(cached=True, reset=False):
    nonlocal cached_venv_dir
    if not cached or not cached_venv_dir or reset:
        venv_dir = os.environ.get(""_VENV_DIR_"") or load_settings(lazy=True).get(
            ""venv_dir""
        )
        if venv_dir:  # no cov
            if venv_dir == ""isolated"":
                venv_dir = VENV_DIR_ISOLATED
            elif venv_dir == ""shared"":
                venv_dir = VENV_DIR_SHARED
        else:  # no cov
            venv_dir = VENV_DIR_SHARED
        cached_venv_dir = venv_dir
    return cached_venv_dir
","if venv_dir == ""isolated"" :",186
"def __walk_dir_tree(self, dirname):
    dir_list = []
    self.__logger.debug(""__walk_dir_tree. START dir=%s"", dirname)
    for f in os.listdir(dirname):
        current = os.path.join(dirname, f)
        if os.path.isfile(current) and f.endswith(""py""):
            if self.module_registrant:
                self._load_py_from_file(current)
            dir_list.append(current)
        elif os.path.isdir(current):
            ret = self.__walk_dir_tree(current)
            if ret:
                dir_list.append((f, ret))
    return dir_list
",if ret :,184
"def read_ansible_config(project_path, variables_of_interest):
    fnames = [""/etc/ansible/ansible.cfg""]
    if project_path:
        fnames.append(os.path.join(project_path, ""ansible.cfg""))
    values = {}
    try:
        parser = ConfigParser()
        parser.read(fnames)
        if ""defaults"" in parser:
            for var in variables_of_interest:
                if var in parser[""defaults""]:
                    values[var] = parser[""defaults""][var]
    except Exception:
        logger.exception(""Failed to read ansible configuration(s) {}"".format(fnames))
    return values
","if ""defaults"" in parser :",166
"def inference(self, x_all, data_loader):
    for i in range(len(self.convs)):
        output = []
        for src_id, edge_index, size in data_loader:
            x = x_all[src_id].to(self.device)
            edge_index = edge_index.to(self.device)
            x = self.convs[i](x, edge_index)
            x = x[: size[1]]
            if i != self.num_layers - 1:
                x = F.relu(x)
            output.append(x.cpu())
        x_all = torch.cat(output, dim=0)
    return F.log_softmax(x_all, dim=-1)
",if i != self . num_layers - 1 :,193
"def guard_transform(transform):
    """"""Return an Affine transformation instance.""""""
    if not isinstance(transform, Affine):
        if tastes_like_gdal(transform):
            raise TypeError(
                ""GDAL-style transforms have been deprecated.  This ""
                ""exception will be raised for a period of time to highlight ""
                ""potentially confusing errors, but will eventually be removed.""
            )
        else:
            transform = Affine(*transform)
    return transform
",if tastes_like_gdal ( transform ) :,125
"def _tokenize(self, text):
    if tf.is_tensor(text):
        rank = len(text.shape)
        if rank == 0:
            return self._tokenize_tensor(text)
        elif rank == 1:
            return self._tokenize_batch_tensor(text)
        else:
            raise ValueError(""Unsupported tensor rank %d for tokenization"" % rank)
    elif isinstance(text, list):
        return list(map(self.tokenize, text))
    else:
        text = tf.compat.as_text(text)
        return self._tokenize_string(text)
",if rank == 0 :,152
"def validate_export(namespace):
    destination = namespace.destination
    if destination == ""file"":
        if namespace.path is None or namespace.format_ is None:
            raise CLIError(""usage error: --path PATH --format FORMAT"")
    elif destination == ""appconfig"":
        if (namespace.dest_name is None) and (namespace.dest_connection_string is None):
            raise CLIError(""usage error: --config-name NAME | --connection-string STR"")
    elif destination == ""appservice"":
        if namespace.appservice_account is None:
            raise CLIError(""usage error: --appservice-account NAME_OR_ID"")
",if ( namespace . dest_name is None ) and ( namespace . dest_connection_string is None ) :,159
"def dispatch(self, request, *args, **kwargs):
    settings = self.get_settings(self.form_class.settings)
    initial = self.get_initial_form_data(settings)
    form = self.form_class(request=request, initial=initial)
    if request.method == ""POST"":
        form = self.form_class(
            request.POST, request.FILES, request=request, initial=initial
        )
        if form.is_valid():
            form.save(settings)
            messages.success(request, _(""Settings have been saved.""))
            return redirect(request.path_info)
    return self.render(request, {""form"": form, ""form_settings"": settings})
",if form . is_valid ( ) :,180
"def get_modules(path):
    modules = set()
    for dirpath, dirnames, filenames in os.walk(path):
        for filename in filenames:
            if filename.endswith("".py""):
                cutoff = len(path) + 1
                fullpath = os.path.join(dirpath[cutoff:], filename)
                modules.add(fullpath)
    return modules
","if filename . endswith ( "".py"" ) :",95
"def _make_input_layers(self, rebuild=False):
    for name, layer in self.layer_map.items():
        layer.left_in_edges = len(layer.in_edges)
        if len(layer.in_edges) == 0:
            if rebuild:
                if not layer.get_attr(""scope""):
                    self.input_layers.append(name)
            else:
                self.input_layers.append(name)
","if not layer . get_attr ( ""scope"" ) :",123
"def _get_status(self):
    connection_errors_allowed = 10
    while True:
        try:
            content = requests.get(self.__status_details_url).json()
        except (requests.ConnectionError, requests.HTTPError) as e:
            if not connection_errors_allowed:
                yield e
            content = {""processed"": False, ""code"": ""being_processed""}
            connection_errors_allowed -= 1
        yield content
",if not connection_errors_allowed :,119
"def show(self):
    if len(self.figures.keys()) == 0:
        return
    if not SETTINGS.plot_split:
        if SETTINGS.plot_backend.lower() == ""qt4agg"":
            self.tabbed_qt4_window()
        elif SETTINGS.plot_backend.lower() == ""qt5agg"":
            self.tabbed_qt5_window()
        elif SETTINGS.plot_backend.lower() == ""tkagg"":
            self.tabbed_tk_window()
        else:
            plt.show()
    else:
        plt.show()
","elif SETTINGS . plot_backend . lower ( ) == ""qt5agg"" :",161
"def emit(self, record):
    msg = self.format(record)
    self.lock.acquire()
    try:
        msg = self.encode(msg)
        if self.should_rollover(record, len(msg)):
            self.perform_rollover()
        self.write(msg)
        self.flush()
    finally:
        self.lock.release()
","if self . should_rollover ( record , len ( msg ) ) :",96
"def install(self, unicode=False, names=None):
    import __builtin__
    __builtin__.__dict__[""_""] = unicode and self.ugettext or self.gettext
    if hasattr(names, ""__contains__""):
        if ""gettext"" in names:
            __builtin__.__dict__[""gettext""] = __builtin__.__dict__[""_""]
        if ""ngettext"" in names:
            __builtin__.__dict__[""ngettext""] = (
                unicode and self.ungettext or self.ngettext
            )
        if ""lgettext"" in names:
            __builtin__.__dict__[""lgettext""] = self.lgettext
        if ""lngettext"" in names:
            __builtin__.__dict__[""lngettext""] = self.lngettext
","if ""gettext"" in names :",181
"def test_simulate_moment_steps_set_state(dtype):
    q0, q1 = cirq.LineQubit.range(2)
    circuit = cirq.Circuit(cirq.H(q0), cirq.H(q1), cirq.H(q0), cirq.H(q1))
    simulator = cirq.Simulator(dtype=dtype)
    for i, step in enumerate(simulator.simulate_moment_steps(circuit)):
        np.testing.assert_almost_equal(step.state_vector(), np.array([0.5] * 4))
        if i == 0:
            step.set_state_vector(np.array([1, 0, 0, 0], dtype=dtype))
",if i == 0 :,179
"def get_config_settings():
    config = {}
    for plugin in extension_loader.MANAGER.plugins:
        fn_name = plugin.name
        function = plugin.plugin
        # if a function takes config...
        if hasattr(function, ""_takes_config""):
            fn_module = importlib.import_module(function.__module__)
            # call the config generator if it exists
            if hasattr(fn_module, ""gen_config""):
                config[fn_name] = fn_module.gen_config(function._takes_config)
    return yaml.safe_dump(config, default_flow_style=False)
","if hasattr ( function , ""_takes_config"" ) :",161
"def TryMerge(self, d):
    while d.avail() > 0:
        tt = d.getVarInt32()
        if tt == 10:
            self.set_app_id(d.getPrefixedString())
            continue
        if tt == 18:
            self.set_queue_name(d.getPrefixedString())
            continue
        if tt == 24:
            self.set_pause(d.getBoolean())
            continue
        if tt == 0:
            raise ProtocolBuffer.ProtocolBufferDecodeError
        d.skipData(tt)
",if tt == 0 :,152
"def enable(self):
    """"""enable the patch.""""""
    for patch in self.dependencies:
        patch.enable()
    if not self.enabled:
        pyv = sys.version_info[0]
        if pyv == 2:
            if self.PY2 == SKIP:
                return  # skip patch activation
            if not self.PY2:
                raise IncompatiblePatch(""Python 2 not supported!"")
        if pyv == 3:
            if self.PY3 == SKIP:
                return  # skip patch activation
            if not self.PY3:
                raise IncompatiblePatch(""Python 3 not supported!"")
        self.pre_enable()
        self.do_enable()
        self.enabled = True
",if self . PY2 == SKIP :,191
"def to_dict(self) -> JSONDict:
    data = dict()
    for key in iter(self.__dict__):
        if key == ""bot"" or key.startswith(""_""):
            continue
        value = self.__dict__[key]
        if value is not None:
            if hasattr(value, ""to_dict""):
                data[key] = value.to_dict()
            else:
                data[key] = value
    if data.get(""from_user""):
        data[""from""] = data.pop(""from_user"", None)
    return data
",if value is not None :,148
"def _resolve_result(self, f=None):
    try:
        if f:
            results = f.result()
        else:
            results = list(map(self._client.results.get, self.msg_ids))
        if self._single_result:
            r = results[0]
            if isinstance(r, Exception):
                raise r
        else:
            results = error.collect_exceptions(results, self._fname)
        self._success = True
        self.set_result(self._reconstruct_result(results))
    except Exception as e:
        self._success = False
        self.set_exception(e)
","if isinstance ( r , Exception ) :",174
"def print_monitor(args):
    from pylearn2.utils import serial
    import gc
    for model_path in args:
        if len(args) > 1:
            print(model_path)
        model = serial.load(model_path)
        monitor = model.monitor
        del model
        gc.collect()
        channels = monitor.channels
        if not hasattr(monitor, ""_epochs_seen""):
            print(""old file, not all fields parsed correctly"")
        else:
            print(""epochs seen: "", monitor._epochs_seen)
        print(""time trained: "", max(channels[key].time_record[-1] for key in channels))
        for key in sorted(channels.keys()):
            print(key, "":"", channels[key].val_record[-1])
","if not hasattr ( monitor , ""_epochs_seen"" ) :",200
"def apply(self, **kwargs: Any) -> None:
    for node in self.document.traverse(addnodes.index):
        if ""entries"" in node and any(len(entry) == 4 for entry in node[""entries""]):
            msg = (
                __(
                    ""4 column based index found. ""
                    ""It might be a bug of extensions you use: %r""
                )
                % node[""entries""]
            )
            logger.warning(msg, location=node)
            for i, entry in enumerate(node[""entries""]):
                if len(entry) == 4:
                    node[""entries""][i] = entry + (None,)
","if ""entries"" in node and any ( len ( entry ) == 4 for entry in node [ ""entries"" ] ) :",183
"def cleanup_empty_directories(path: str):
    """"""Remove all empty folders inside (and including) 'path'""""""
    path = os.path.normpath(path)
    while 1:
        repeat = False
        for root, dirs, files in os.walk(path, topdown=False):
            if not dirs and not files and root != path:
                try:
                    remove_dir(root)
                    repeat = True
                except:
                    pass
        if not repeat:
            break
    # Only remove if main folder is now also empty
    if not os.listdir(path):
        try:
            remove_dir(path)
        except:
            pass
",if not dirs and not files and root != path :,187
"def expect_flow_sequence_item(self):
    if isinstance(self.event, SequenceEndEvent):
        self.indent = self.indents.pop()
        self.flow_level -= 1
        if self.canonical:
            self.write_indicator(u"","", False)
            self.write_indent()
        self.write_indicator(u""]"", False)
        self.state = self.states.pop()
    else:
        self.write_indicator(u"","", False)
        if self.canonical or self.column > self.best_width:
            self.write_indent()
        self.states.append(self.expect_flow_sequence_item)
        self.expect_node(sequence=True)
",if self . canonical :,185
"def test_loss_diff(self):
    losses = []
    for use_cuda in [True, False]:
        for use_py_func_op in [True, False]:
            L = test_main(use_cuda, use_py_func_op, self.use_parallel_executor)
            if L is not None:
                losses.append(L)
    for idx in six.moves.range(len(losses) - 1):
        max_diff = np.max(np.abs(losses[idx] - losses[0]))
        self.assertAlmostEqual(max_diff, 0, delta=1e-3)
",if L is not None :,157
"def check_file(f, path):
    if not (ignore_substring and ignore_substring in f):
        if substring in f:
            compl_path = os.path.join(path, f)
            if os.path.isfile(compl_path):
                return compl_path
    return False
",if os . path . isfile ( compl_path ) :,83
"def is_valid_block(self):
    """"""check wheter the block is valid in the current position""""""
    for i in range(self.block.x):
        for j in range(self.block.x):
            if self.block.get(i, j):
                if self.block.pos.x + i < 0:
                    return False
                if self.block.pos.x + i >= COLUMNS:
                    return False
                if self.block.pos.y + j < 0:
                    return False
                if self.map.get((self.block.pos.x + i, self.block.pos.y + j), False):
                    return False
    return True
",if self . block . pos . x + i < 0 :,192
"def is_fail_state(state):
    if type(
        state.addr
    ) == SootAddressDescriptor and state.addr.method == SootMethodDescriptor.from_soot_method(
        onclick_method
    ):
        sols = state.solver.eval_upto(state.memory_soot.stack.load(""$z0""), 2)
        assert len(sols) == 1
        if sols[0] == 0:
            return True
    return False
",if sols [ 0 ] == 0 :,118
"def TryMerge(self, d):
    while d.avail() > 0:
        tt = d.getVarInt32()
        if tt == 8:
            self.add_delete_status(d.getVarInt32())
            continue
        if tt == 0:
            raise ProtocolBuffer.ProtocolBufferDecodeError
        d.skipData(tt)
",if tt == 0 :,92
"def _init_weight(self):
    for m in self.modules():
        if isinstance(m, nn.Conv2d):
            n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels
            m.weight.data.normal_(0, math.sqrt(2.0 / n))
        elif isinstance(m, SyncBatchNorm):
            m.weight.data.fill_(1)
            m.bias.data.zero_()
        elif isinstance(m, nn.BatchNorm2d):
            m.weight.data.fill_(1)
            m.bias.data.zero_()
","if isinstance ( m , nn . Conv2d ) :",162
"def wrapper(*args, **kwargs):
    global _exception
    try:
        fn(*args, **kwargs)
    except Exception:
        _exception = sys.exc_info()
        et, ev, tb = _exception
        if getattr(ev, ""filename"", None) is None:
            # get the filename from the last item in the stack
            filename = traceback.extract_tb(tb)[-1][0]
        else:
            filename = ev.filename
        if filename not in _error_files:
            _error_files.append(filename)
        raise
",if filename not in _error_files :,148
"def purge_messages(self):
    with self.app.connection_for_write() as connection:
        count = self.app.control.purge(connection=connection)
        if count:  # pragma: no cover
            print(
                ""purge: Erased {0} {1} from the queue.\n"".format(
                    count, pluralize(count, ""message"")
                )
            )
",if count :,109
"def read_series(rec):
    found = []
    for tag in (""440"", ""490"", ""830""):
        fields = rec.get_fields(tag)
        if not fields:
            continue
        for f in fields:
            this = []
            for k, v in f.get_subfields([""a"", ""v""]):
                if k == ""v"" and v:
                    this.append(v)
                    continue
                v = v.rstrip("".,; "")
                if v:
                    this.append(v)
            if this:
                found += ["" -- "".join(this)]
    return found
",if this :,182
"def calc_position_values(positions):
    values = []
    for position in positions:
        if isinstance(position.asset, Future):
            # Futures don't have an inherent position value.
            values.append(0.0)
        else:
            values.append(position.last_sale_price * position.amount)
    return values
","if isinstance ( position . asset , Future ) :",92
"def _loc(obj):
    try:
        fn = getattr(obj, ""__file__"", None)
        if fn is not None:
            return "" @%s"" % (fn,)
        obj = getattr(obj, ""im_func"", obj)
        code = getattr(obj, ""__code__"", None)
        if code is not None:
            return "" @%s:%s"" % (code.co_filename, code.co_firstlineno)
    except Exception:
        pass
    return """"
",if code is not None :,126
"def _convert_user_into_remote(self, username, exclude=[""all""]):
    # builds a ref with an username and a branch
    # this method parses the repository's remotes to find the url matching username
    # and containing the given branch and returns the corresponding ref
    remotes = {remote.name: list(remote.urls) for remote in self.repository.remotes}
    for name in (self.name, ""upstream"") + tuple(remotes.keys()):
        if name in remotes and name not in exclude:
            for url in remotes[name]:
                if self.fqdn in url and username == url.split(""/"")[-2].split("":"")[-1]:
                    yield name
","if self . fqdn in url and username == url . split ( ""/"" ) [ - 2 ] . split ( "":"" ) [ - 1 ] :",169
"def _render_ib_interfaces(cls, network_state, iface_contents, flavor):
    ib_filter = renderer.filter_by_type(""infiniband"")
    for iface in network_state.iter_interfaces(ib_filter):
        iface_name = iface[""name""]
        iface_cfg = iface_contents[iface_name]
        iface_cfg.kind = ""infiniband""
        iface_subnets = iface.get(""subnets"", [])
        route_cfg = iface_cfg.routes
        cls._render_subnets(
            iface_cfg, iface_subnets, network_state.has_default_route, flavor
        )
        cls._render_subnet_routes(iface_cfg, route_cfg, iface_subnets, flavor)
","iface_name = iface [ ""name"" ]",193
"def _extract_level(self):
    """"""Extract level and component if available (lazy).""""""
    if self._level is None:
        split_tokens = self.split_tokens
        if not split_tokens:
            self._level = False
            self._component = False
            return
        x = (
            self.log_levels.index(split_tokens[1])
            if split_tokens[1] in self.log_levels
            else None
        )
        if x is not None:
            self._level = split_tokens[1]
            self._component = split_tokens[2]
        else:
            self._level = False
            self._component = False
",if x is not None :,185
"def addnode(self, parent, data):
    print(""aaa"", data)
    for i in data:
        print(i)
        if i == ""-"":
            continue
        if isinstance(i, tuple):
            item = self.tre_plugins.AppendItem(parent, i[0].title)
            self.tre_plugins.SetItemData(item, i[0])
            self.addnode(item, i[1])
        else:
            item = self.tre_plugins.AppendItem(parent, i[0].title)
            self.tre_plugins.SetItemData(item, i[0])
","if i == ""-"" :",159
"def getdsturl(tcpdata):
    import logging
    log = logging.getLogger(""getdsturl"")
    p = parseHeader(tcpdata, type=""request"")
    if p is None:
        log.warn(""parseHeader returned None"")
        return
    if p.has_key(""uri"") and p.has_key(""headers""):
        if p[""headers""].has_key(""host""):
            r = ""http://%s%s"" % (p[""headers""][""host""][0], p[""uri""])
            return r
        else:
            log.warn(""seems like no host header was set"")
    else:
        log.warn(""parseHeader did not give us a nice return %s"" % p)
","if p [ ""headers"" ] . has_key ( ""host"" ) :",176
"def assert_not_none(obj, msg=None, values=True):
    """"""Fail the test if given object is None.""""""
    _msg = ""is None""
    if obj is None:
        if msg is None:
            msg = _msg
        elif values is True:
            msg = ""%s: %s"" % (msg, _msg)
        _report_failure(msg)
",elif values is True :,99
"def sort(self):
    sorted_models = []
    concrete_models = set()
    models = list(self.data)
    while len(sorted_models) < len(models):
        found = False
        for model in models:
            if model in sorted_models:
                continue
            dependencies = self.dependencies.get(model._meta.concrete_model)
            if not (dependencies and dependencies.difference(concrete_models)):
                sorted_models.append(model)
                concrete_models.add(model._meta.concrete_model)
                found = True
        if not found:
            return
    self.data = OrderedDict((model, self.data[model]) for model in sorted_models)
",if not ( dependencies and dependencies . difference ( concrete_models ) ) :,188
"def load_vocab_dict(vocab_file_path):
    """"""Load vocabs, vocab: {""word"": 1, ...}""""""
    logging.info(""Loading vocab from {}"".format(vocab_file_path))
    with open(vocab_file_path, encoding=""utf-8"") as in_f:
        vocabs = {}
        for line in in_f:
            parts = line.rstrip().split(""\t"")
            if len(parts) < 2:
                continue
            vocabs[parts[0]] = parts[1]
    logging.info(""Loded {} vocabs from {}"".format(len(vocabs), vocab_file_path))
    return vocabs
",if len ( parts ) < 2 :,161
"def get_layers_from_suite(self, suite, suiteClass):
    top_layer = suiteClass()
    layers_dict = OrderedDict()
    for test in self.flatten_suite(suite):
        layer = getattr(test, ""layer"", None)
        if layer:
            if layer not in layers_dict:
                layers_dict[layer] = LayerSuite(self.session, layer=layer)
            layers_dict[layer].addTest(test)
        else:
            top_layer.addTest(test)
    self.get_parent_layers(layers_dict)
    return top_layer, layers_dict
",if layer :,159
"def team_scores(self, team_scores, time):
    """"""Store output of team scores to a JSON file""""""
    data = []
    for score in team_scores[""fixtures""]:
        if score[""status""] == ""FINISHED"":
            item = {
                ""date"": score[""date""].split(""T"")[0],
                ""homeTeamName"": score[""homeTeamName""],
                ""goalsHomeTeam"": score[""result""][""goalsHomeTeam""],
                ""goalsAwayTeam"": score[""result""][""goalsAwayTeam""],
                ""awayTeamName"": score[""awayTeamName""],
            }
            data.append(item)
    self.generate_output({""team_scores"": data})
","if score [ ""status"" ] == ""FINISHED"" :",183
"def run(self, root):
    footnotesDiv = self.footnotes.makeFootnotesDiv(root)
    if footnotesDiv is not None:
        result = self.footnotes.findFootnotesPlaceholder(root)
        if result:
            child, parent, isText = result
            ind = list(parent).index(child)
            if isText:
                parent.remove(child)
                parent.insert(ind, footnotesDiv)
            else:
                parent.insert(ind + 1, footnotesDiv)
                child.tail = None
        else:
            root.append(footnotesDiv)
",if isText :,175
"def delete_target_group(self, target_group_arn):
    if target_group_arn not in self.target_groups:
        raise TargetGroupNotFoundError()
    target_group = self.target_groups[target_group_arn]
    if target_group:
        if self._any_listener_using(target_group_arn):
            raise ResourceInUseError(
                ""The target group '{}' is currently in use by a listener or a rule"".format(
                    target_group_arn
                )
            )
        del self.target_groups[target_group_arn]
        return target_group
",if self . _any_listener_using ( target_group_arn ) :,161
"def run_pending(self, now=None):
    """"""Runs the command if scheduled""""""
    now = now or datetime.now()
    if self.is_enabled():
        if self.last_run is None:
            self.last_run = now
        next_time = self.schedule(self.last_run).get_next()
        if next_time < now:
            self.last_run = now
            return self.run()
    return -1
",if self . last_run is None :,119
"def _fix_exception_context(new_exc, old_exc):
    # Context may not be correct, so find the end of the chain
    while 1:
        exc_context = new_exc.__context__
        if exc_context is old_exc:
            # Context is already set correctly (see issue 20317)
            return
        if exc_context is None or exc_context is frame_exc:
            break
        new_exc = exc_context
    # Change the end of the chain to point to the exception
    # we expect it to reference
    new_exc.__context__ = old_exc
",if exc_context is old_exc :,151
"def delete_backend(
    self, backend_tag: BackendTag, force_kill: bool = False
) -> Optional[GoalId]:
    async with self.write_lock:
        # Check that the specified backend isn't used by any endpoints.
        for endpoint, info in self.endpoint_state.get_endpoints().items():
            if backend_tag in info[""traffic""] or backend_tag in info[""shadows""]:
                raise ValueError(
                    ""Backend '{}' is used by endpoint '{}' ""
                    ""and cannot be deleted. Please remove ""
                    ""the backend from all endpoints and try ""
                    ""again."".format(backend_tag, endpoint)
                )
        return self.backend_state.delete_backend(backend_tag, force_kill)
","if backend_tag in info [ ""traffic"" ] or backend_tag in info [ ""shadows"" ] :",199
"def lint(self, request):
    try:
        html_linter = UnwrapObject(self._koLintService.getLinterForLanguage(""HTML""))
        return html_linter.lint(request, TPLInfo=self._tplPatterns)
    except:
        if ""lint"" not in self._checkValidVersion_complained:
            self._checkValidVersion_complained[""lint""] = True
            log.exception(""Problem in koPHPLinter.lint"")
        return koLintResults()
","if ""lint"" not in self . _checkValidVersion_complained :",132
"def get_commit(self, rev):
    """"""Get commit object identified by `rev` (SHA or branch or tag name).""""""
    for prefix in [""refs/heads/"", ""refs/tags/"", """"]:
        key = prefix + rev
        try:
            obj = self[encode_for_git(key)]
            if isinstance(obj, dulwich.objects.Tag):
                obj = self[obj.object[1]]
            return obj
        except KeyError:
            pass
    raise KeyError(rev)
","if isinstance ( obj , dulwich . objects . Tag ) :",130
"def get_host_metadata(self):
    meta = {}
    if self.agent_url:
        try:
            resp = requests.get(self.agent_url, timeout=1).json().get(""config"", {})
            if ""Version"" in resp:
                meta[""nomad_version""] = resp.get(""Version"")
            if ""Region"" in resp:
                meta[""nomad_region""] = resp.get(""Region"")
            if ""Datacenter"" in resp:
                meta[""nomad_datacenter""] = resp.get(""Datacenter"")
        except Exception as ex:
            self.log.debug(""Error getting Nomad version: %s"" % str(ex))
    return meta
","if ""Version"" in resp :",185
"def _waitFakenetStopped(self, timeoutsec=None):
    retval = False
    while True:
        if self._confirmFakenetStopped():
            retval = True
            break
        time.sleep(1)
        if timeoutsec is not None:
            timeoutsec -= 1
            if timeoutsec <= 0:
                break
    return retval
",if self . _confirmFakenetStopped ( ) :,97
"def send_message(self, message):
    smtp = smtplib.SMTP(self.smtp_host, self.smtp_port)
    try:
        smtp.ehlo()
        if self.smtp_tls:
            smtp.starttls()
        if self.smtp_user:
            smtp.login(self.smtp_user, self.smtp_password)
        smtp.sendmail(self.from_user, self.recipients, message.as_string())
    finally:
        smtp.close()
",if self . smtp_user :,125
"def set_tracker_icon(tracker_icon, cell):
    if tracker_icon:
        pixbuf = tracker_icon.get_cached_icon()
        if pixbuf is None:
            pixbuf = get_pixbuf_at_size(tracker_icon.get_filename(), 16)
            tracker_icon.set_cached_icon(pixbuf)
    else:
        pixbuf = create_blank_pixbuf()
    # Suppress Warning: g_object_set_qdata: assertion `G_IS_OBJECT (object)' failed
    with warnings.catch_warnings():
        warnings.simplefilter(""ignore"")
        cell.set_property(""pixbuf"", pixbuf)
",if pixbuf is None :,165
"def __create_index(self, collection, index, unique):
    doc = collection.find_one(projection={""_id"": 1})
    if doc is None:
        try:
            indexes = list(collection.list_indexes())
        except OperationFailure:
            indexes = []
        if index not in indexes:
            collection.create_index(index, unique=unique)
",if index not in indexes :,97
"def read_oclc(fields):
    if ""035"" not in fields:
        return {}
    found = []
    for line in fields[""035""]:
        for v in get_subfield_values(line, [""a""]):
            m = re_oclc.match(v)
            if not m:
                continue
            oclc = m.group(1)
            if oclc not in found:
                found.append(oclc)
    return {""oclc_number"": found} if found else {}
",if not m :,143
"def closest_enemy_ant(self, row1, col1, filter=None):
    # find the closest enemy ant from this row/col
    min_dist = maxint
    closest_ant = None
    for ant in self.enemy_ants():
        if filter is None or ant not in filter:
            dist = self.distance(row1, col1, ant[0][0], ant[0][1])
            if dist < min_dist:
                min_dist = dist
                closest_ant = ant[0]
    return closest_ant
",if filter is None or ant not in filter :,146
"def fromVariant(variant):
    if hasattr(QtCore, ""QVariant"") and isinstance(variant, QtCore.QVariant):
        t = variant.type()
        if t == QtCore.QVariant.String:
            return str(variant.toString())
        elif t == QtCore.QVariant.Double:
            return variant.toDouble()[0]
        elif t == QtCore.QVariant.Int:
            return variant.toInt()[0]
        elif t == QtCore.QVariant.Bool:
            return variant.toBool()
        elif t == QtCore.QVariant.Invalid:
            return None
        else:
            raise ValueError('Unsupported QVariant type ""%s""' % variant.typeName())
    else:
        return variant
",elif t == QtCore . QVariant . Double :,195
"def _check_old_with_state(self):
    add_vec = False
    for op in self.ops:
        if op.type == ""func"":
            try:
                op.get_coeff(0.0, self.args)
            except TypeError as e:
                nfunc = _StateAsArgs(self.coeff)
                op = EvoElement((op.qobj, nfunc, nfunc, ""func""))
                add_vec = True
    if add_vec:
        self.dynamics_args += [(""_state_vec"", ""vec"", None)]
","if op . type == ""func"" :",155
"def _read_readable(self, readable):
    blocksize = 8192
    if self.debuglevel > 0:
        print(""sendIng a read()able"")
    encode = self._is_textIO(readable)
    if encode and self.debuglevel > 0:
        print(""encoding file using iso-8859-1"")
    while True:
        datablock = readable.read(blocksize)
        if not datablock:
            break
        if encode:
            datablock = datablock.encode(""iso-8859-1"")
        yield datablock
",if not datablock :,139
"def read_chat_forever(reader, pub_socket):
    line = reader.readline()
    who = ""someone""
    while line:
        print(""Chat:"", line.strip())
        if line.startswith(""name:""):
            who = line.split("":"")[-1].strip()
        try:
            pub_socket.send_pyobj((who, line))
        except socket.error as e:
            # ignore broken pipes, they just mean the participant
            # closed its connection already
            if e[0] != 32:
                raise
        line = reader.readline()
    print(""Participant left chat."")
",if e [ 0 ] != 32 :,162
"def _wrapped() -> None:
    should_run = app.is_leader() if on_leader else True
    if should_run:
        with self.trace(shortlabel(fun), trace_enabled=traced):
            # pass app only if decorated function takes an argument
            if inspect.signature(fun).parameters:
                task_takes_app = cast(Callable[[AppT], Awaitable], fun)
                return await task_takes_app(app)
            else:
                task = cast(Callable[[], Awaitable], fun)
                return await task()
",if inspect . signature ( fun ) . parameters :,151
"def Decode(self, filedesc):
    while True:
        chunk = filedesc.Read(4)
        if not chunk:
            return
        if chunk == b""QUUX"":
            yield b""NORF""
        if chunk == b""THUD"":
            yield b""BLARGH""
","if chunk == b""THUD"" :",82
"def _get_modules(fn):
    finder = modulefinder.ModuleFinder()
    finder.run_script(fn)
    all = []
    for m in finder.modules.values():
        if not isinstance(m, modulefinder.Module):
            continue
        if not m.__file__:
            continue
        # skip shared object files
        if m.__file__.endswith("".so""):
            continue
        # skip mac system stuff...
        # FIXME: would need to augment with  other OS's system stuff
        if m.__file__.startswith(""/Library/Frameworks""):
            continue
        all.append(m)
    return all
",if not m . __file__ :,162
"def _read(self, size):
    """"""Return size bytes from the stream.""""""
    if self.comptype == ""tar"":
        return self.__read(size)
    c = len(self.dbuf)
    while c < size:
        buf = self.__read(self.bufsize)
        if not buf:
            break
        try:
            buf = self.cmp.decompress(buf)
        except IOError:
            raise ReadError(""invalid compressed data"")
        self.dbuf += buf
        c += len(buf)
    buf = self.dbuf[:size]
    self.dbuf = self.dbuf[size:]
    return buf
",if not buf :,167
"def cluster_list(tokeniser):
    clusterids = []
    value = tokeniser()
    try:
        if value == ""["":
            while True:
                value = tokeniser()
                if value == ""]"":
                    break
                clusterids.append(ClusterID(value))
        else:
            clusterids.append(ClusterID(value))
        if not clusterids:
            raise ValueError(""no cluster-id in the cluster list"")
        return ClusterList(clusterids)
    except ValueError:
        raise ValueError(""invalud cluster list"")
","if value == ""]"" :",146
"def from_data(cls, value, currency, includes_tax=None):
    if includes_tax is None:
        if cls.includes_tax is None:
            msg = ""Missing includes_tax argument for %s.from_data""
            raise TypeError(msg % (cls.__name__,))
        includes_tax = cls.includes_tax
    if includes_tax:
        return TaxfulPrice(value, currency)
    else:
        return TaxlessPrice(value, currency)
",if cls . includes_tax is None :,121
"def THUMB(image, nx=120, ny=120, gae=False, name=""thumb""):
    if image:
        if not gae:
            request = current.request
            from PIL import Image
            import os
            img = Image.open(os.path.join(request.folder, ""uploads"", image))
            img.thumbnail((nx, ny), Image.ANTIALIAS)
            root, ext = os.path.splitext(image)
            thumb = ""%s_%s%s"" % (root, name, ext)
            img.save(request.folder + ""uploads/"" + thumb)
            return thumb
        else:
            return image
",if not gae :,176
"def _get_two_devices(self, require_same_type=False):
    tpus = extensions.tpu_devices()
    if FLAGS.requires_tpu:
        if len(tpus) == 2:
            res = tpus
        else:
            raise ValueError(
                ""This test requires 2 TPU cores but %s are found"" % len(tpus)
            )
    else:
        if len(tpus) == 2:
            res = tpus
        elif self._hasGPU() and not require_same_type:
            res = (""CPU:0"", ""GPU:0"")
        else:
            res = (""CPU:0"", ""CPU:1"")
    return res
",elif self . _hasGPU ( ) and not require_same_type :,184
"def _format_repos(self, value):
    result = {}
    if value:
        for path, config in iteritems(value):
            if path[0] != ""/"":
                # assume its a module
                path = os.path.abspath(__import__(path).__file__)
            result[path] = config
    return result
","if path [ 0 ] != ""/"" :",87
"def skipIndent(self, s, i, width):
    ws = 0
    n = len(s)
    while i < n and ws < width:
        if s[i] == ""\t"":
            ws += abs(self.tab_width) - (ws % abs(self.tab_width))
        elif s[i] == "" "":
            ws += 1
        else:
            break
        i += 1
    return i
","if s [ i ] == ""\t"" :",114
"def get_assets_historical_range_close_price(
    self, start_dt, end_dt, asset_symbols, adjusted=False
):
    """""" """"""
    prices_df = None
    for ds in self.data_sources:
        try:
            prices_df = ds.get_assets_historical_closes(
                start_dt, end_dt, asset_symbols, adjusted=adjusted
            )
            if prices_df is not None:
                return prices_df
        except Exception:
            raise
    return prices_df
",if prices_df is not None :,145
"def matchBrackets(string):
    rest = string[1:]
    inside = ""(""
    while rest != """" and not rest.startswith("")""):
        if rest.startswith(""(""):
            (part, rest) = matchBrackets(rest)
            inside = inside + part
        else:
            inside = inside + rest[0]
            rest = rest[1:]
    if rest.startswith("")""):
        return (inside + "")"", rest[1:])
    raise AssertionError(""Unmatched bracket in string '"" + string + ""'"")
","if rest . startswith ( ""("" ) :",122
"def is_different(item, seen):
    is_diff = True
    if item not in seen:
        for value in other:
            if comparator(iteratee(item), iteratee(value)):
                is_diff = False
                break
        if is_diff:
            seen.append(item)
    return is_diff
","if comparator ( iteratee ( item ) , iteratee ( value ) ) :",91
"def write_conditional_formatting(worksheet):
    """"""Write conditional formatting to xml.""""""
    wb = worksheet.parent
    for range_string, rules in iteritems(worksheet.conditional_formatting.cf_rules):
        cf = Element(""conditionalFormatting"", {""sqref"": range_string})
        for rule in rules:
            if rule.dxf is not None:
                if rule.dxf != DifferentialStyle():
                    rule.dxfId = len(wb._differential_styles)
                    wb._differential_styles.append(rule.dxf)
            cf.append(rule.to_tree())
        yield cf
",if rule . dxf != DifferentialStyle ( ) :,164
"def checkForFinishedThreads(self):
    ""Mark terminated threads with endTime.""
    for t in self.unfinishedThreads:
        if not t.is_alive():
            t.endTime = time.process_time()
            if getattr(t, ""status"", None) is None:
                t.status = ""ended""
",if not t . is_alive ( ) :,84
"def _process_dispatch_entries(self, dispatch_info_external):
    path_only_entries = []
    hostname_entries = []
    for entry in dispatch_info_external.dispatch:
        parsed_url = dispatchinfo.ParsedURL(entry.url)
        if parsed_url.host:
            hostname_entries.append(entry)
        else:
            path_only_entries.append((parsed_url, entry.server))
    if hostname_entries:
        logging.warning(
            ""Hostname routing is not supported by the development server. The ""
            ""following dispatch entries will not match any requests:\n%s"",
            ""\n\t"".join(str(entry) for entry in hostname_entries),
        )
    self._entries = path_only_entries
",if parsed_url . host :,196
"def iter_ReassignParameters(self, inputNode, variables, nodeByID):
    for node in inputNode.getReassignParameterNodes(nodeByID):
        yield from iterNodeCommentLines(node)
        yield from iterInputConversionLines(node, variables)
        socket = node.inputs[0]
        if socket.isUnlinked and socket.isCopyable():
            expression = getCopyExpression(socket, variables)
        else:
            expression = variables[socket]
        if node.conditionSocket is None:
            conditionPrefix = """"
        else:
            conditionPrefix = ""if {}: "".format(variables[node.conditionSocket])
        yield ""{}{} = {}"".format(
            conditionPrefix, variables[node.linkedParameterSocket], expression
        )
",if socket . isUnlinked and socket . isCopyable ( ) :,192
"def _feed_data(self, data_pair: types.Sequence, type_: str) -> types.Sequence:
    result = []
    type_list = [ChartType.LINES, ChartType.CUSTOM]
    if type_ in type_list:
        result = data_pair
    else:
        for n, v in data_pair:
            try:
                lng, lat = self.get_coordinate(n)
                result.append({""name"": n, ""value"": [lng, lat, v]})
            except TypeError as err:
                if self._is_ignore_nonexistent_coord is not True:
                    raise NonexistentCoordinatesException(err, (n, v))
    return result
",if self . _is_ignore_nonexistent_coord is not True :,175
"def _parse_whois(self, txt):
    asn, desc = None, b""""
    for l in txt.splitlines():
        if not asn and l.startswith(b""origin:""):
            asn = l[7:].strip().decode(""utf-8"")
        if l.startswith(b""descr:""):
            if desc:
                desc += br""\n""
            desc += l[6:].strip()
        if asn is not None and desc.strip():
            desc = desc.strip().decode(""utf-8"")
            break
    return asn, desc
",if asn is not None and desc . strip ( ) :,151
"def _resolve_result(self, f=None):
    try:
        if f:
            results = f.result()
        else:
            results = list(map(self._client.results.get, self.msg_ids))
        if self._single_result:
            r = results[0]
            if isinstance(r, Exception):
                raise r
        else:
            results = error.collect_exceptions(results, self._fname)
        self._success = True
        self.set_result(self._reconstruct_result(results))
    except Exception as e:
        self._success = False
        self.set_exception(e)
",if f :,174
"def new_org(type=ORG_DEFAULT, block=True, **kwargs):
    if type == ORG_DEFAULT:
        org = reserve_pooled(type=type, **kwargs)
        if not org:
            org = queue.reserve(""queued_org"", block=block, type=type, **kwargs)
        if org:
            new_pooled()
            return org
        org = Organization(type=type, **kwargs)
        org.initialize()
        org.commit()
        return org
    else:
        org = Organization(type=type, **kwargs)
        org.queue_initialize(block=block)
        return org
",if not org :,171
"def _compileRules(rulesList, maxLength=4):
    ruleChecking = collections.defaultdict(list)
    for ruleIndex in range(len(rulesList)):
        args = []
        if len(rulesList[ruleIndex]) == maxLength:
            args = rulesList[ruleIndex][-1]
        if maxLength == 4:
            (shouldRunMethod, method, isCorrect) = rulesList[ruleIndex][0:3]
            ruleChecking[shouldRunMethod].append((method, isCorrect, args))
        elif maxLength == 3:
            (shouldRunMethod, method) = rulesList[ruleIndex][0:2]
            ruleChecking[shouldRunMethod].append((method, args))
    return ruleChecking
",if len ( rulesList [ ruleIndex ] ) == maxLength :,183
"def setHighlightedItem(self, item):
    if item != None:
        for listItem in self.children.getItems():
            if self.loadHandler.matchesItem(listItem, item):
                self.children.setCurrentItem(listItem)
                return
    else:
        self.children.setCurrentItem(None)
","if self . loadHandler . matchesItem ( listItem , item ) :",88
"def getForts(location):
    global forts
    lforts = []
    for i in forts:
        f = (i[""latitude""], i[""longitude""])
        d = vincenty(location, f).meters
        if d < 900:
            lforts.append(i)
    return lforts
",if d < 900 :,87
"def page_file(self, page):
    if page.isroot:
        raise PathLookupError(""Can not export: %s"", page)
    elif self.namespace:
        if page.ischild(self.namespace):
            name = page.relname(self.namespace)
        else:
            # This layout can not store page == namespace !
            raise PathLookupError(""%s not a child of %s"" % (page, self.namespace))
    else:
        name = page.name
    return self.dir.file(encode_filename(name) + ""."" + self.ext)
",if page . ischild ( self . namespace ) :,145
"def to_json_dict(self):
    d = super().to_json_dict()
    if self.header is not None:
        if isinstance(self.header, RenderedContent):
            d[""header""] = self.header.to_json_dict()
        else:
            d[""header""] = self.header
    if self.subheader is not None:
        if isinstance(self.subheader, RenderedContent):
            d[""subheader""] = self.subheader.to_json_dict()
        else:
            d[""subheader""] = self.subheader
    d[""text""] = RenderedContent.rendered_content_list_to_json(self.text)
    return d
","if isinstance ( self . subheader , RenderedContent ) :",168
"def fixfunnychars(addr):
    i = 0
    while i < len(addr):
        c = addr[i]
        if c not in goodchars:
            c = ""-""
            addr = addr[:i] + c + addr[i + 1 :]
        i = i + len(c)
    return addr
",if c not in goodchars :,83
"def refactor_stdin(self, doctests_only=False):
    input = sys.stdin.read()
    if doctests_only:
        self.log_debug(""Refactoring doctests in stdin"")
        output = self.refactor_docstring(input, ""<stdin>"")
        if self.write_unchanged_files or output != input:
            self.processed_file(output, ""<stdin>"", input)
        else:
            self.log_debug(""No doctest changes in stdin"")
    else:
        tree = self.refactor_string(input, ""<stdin>"")
        if self.write_unchanged_files or (tree and tree.was_changed):
            self.processed_file(str(tree), ""<stdin>"", input)
        else:
            self.log_debug(""No changes in stdin"")
",if self . write_unchanged_files or output != input :,199
"def test_compute_gradient(self):
    for y, y_pred in zip(self.y_list, self.predict_list):
        lse_grad = self.lae_loss.compute_grad(y, y_pred)
        diff = y_pred - y
        if diff > consts.FLOAT_ZERO:
            grad = 1
        elif diff < consts.FLOAT_ZERO:
            grad = -1
        else:
            grad = 0
        self.assertTrue(np.fabs(lse_grad - grad) < consts.FLOAT_ZERO)
",if diff > consts . FLOAT_ZERO :,145
"def restart(self):
    try:
        # remove old pidfile first
        try:
            if self.runAsDaemon():
                try:
                    self.daemon.stop()
                except:
                    pass
        except:
            self.log.critical(traceback.format_exc())
        # Release log files and shutdown logger
        logging.shutdown()
        args = (
            [sys.executable]
            + [os.path.join(base_path, os.path.basename(__file__))]
            + sys.argv[1:]
        )
        subprocess.Popen(args)
    except:
        self.log.critical(traceback.format_exc())
",if self . runAsDaemon ( ) :,188
"def classifyws(s, tabwidth):
    raw = effective = 0
    for ch in s:
        if ch == "" "":
            raw = raw + 1
            effective = effective + 1
        elif ch == ""\t"":
            raw = raw + 1
            effective = (effective // tabwidth + 1) * tabwidth
        else:
            break
    return raw, effective
","if ch == "" "" :",101
"def code_match(code, select, ignore):
    if ignore:
        assert not isinstance(ignore, unicode)
        for ignored_code in [c.strip() for c in ignore]:
            if mutual_startswith(code.lower(), ignored_code.lower()):
                return False
    if select:
        assert not isinstance(select, unicode)
        for selected_code in [c.strip() for c in select]:
            if mutual_startswith(code.lower(), selected_code.lower()):
                return True
        return False
    return True
","if mutual_startswith ( code . lower ( ) , selected_code . lower ( ) ) :",143
"def get_tokens_unprocessed(self, text):
    from pygments.lexers._asy_builtins import ASYFUNCNAME, ASYVARNAME
    for index, token, value in RegexLexer.get_tokens_unprocessed(self, text):
        if token is Name and value in ASYFUNCNAME:
            token = Name.Function
        elif token is Name and value in ASYVARNAME:
            token = Name.Variable
        yield index, token, value
",if token is Name and value in ASYFUNCNAME :,113
"def makeDataURI(data=None, mimetype=None, filename=None):
    import base64
    if not mimetype:
        if filename:
            import mimetypes
            mimetype = mimetypes.guess_type(filename)[0].split("";"")[0]
        else:
            raise Exception(
                ""You need to provide a mimetype or a filename for makeDataURI""
            )
    return ""data:"" + mimetype + "";base64,"" + """".join(base64.encodestring(data).split())
",if filename :,125
"def add_attributes(attributes, all_base64):
    lines = []
    oc_attr = None
    # objectclass first, even if this is not specified in the RFC
    for attr in attributes:
        if attr.lower() == ""objectclass"":
            for val in attributes[attr]:
                lines.append(_convert_to_ldif(attr, val, all_base64))
            oc_attr = attr
            break
    # remaining attributes
    for attr in attributes:
        if attr != oc_attr and attr in attributes:
            for val in attributes[attr]:
                lines.append(_convert_to_ldif(attr, val, all_base64))
    return lines
","if attr . lower ( ) == ""objectclass"" :",177
"def read_optional_seed(fill, base="""", ext="""", timeout=5):
    try:
        (md, ud, vd) = read_seeded(base, ext, timeout)
        fill[""user-data""] = ud
        fill[""vendor-data""] = vd
        fill[""meta-data""] = md
        return True
    except url_helper.UrlError as e:
        if e.code == url_helper.NOT_FOUND:
            return False
        raise
",if e . code == url_helper . NOT_FOUND :,120
"def _get_spawn_property(self, constraints, constraint_name, services):
    if services:
        # this isn't very nice
        if constraint_name == IMAGE_CONSTRAINT:
            return services[0].image
        elif constraint_name == CPUS_CONSTRAINT:
            return services[0].cpus
    for constraint in constraints:
        if constraint.name == constraint_name:
            return constraint.value
    return None
",elif constraint_name == CPUS_CONSTRAINT :,113
"def delete_api(self):
    retries = 0
    while retries < 10:
        try:
            self.client.delete_rest_api(restApiId=self.api_id)
            break
        except exceptions.ClientError as e:
            if e.response[""Error""][""Code""] == ""TooManyRequestsException"":
                retries += 1
                time.sleep(5)
            else:
                raise
","if e . response [ ""Error"" ] [ ""Code"" ] == ""TooManyRequestsException"" :",114
"def GetSelected(self):
    if self.GetStyleL(""style"") & self.Style.LBS_MULTIPLESEL:
        result = self.SendMessage(self.Hwnd, self.Msg.LB_GETSELCOUNT, 0, 0)
        if result:
            return self.SendMessage(self.Hwnd, self.Msg.LB_GETANCHORINDEX, 0, 0)
    else:
        result = self.SendMessage(self.Hwnd, self.Msg.LB_GETCURSEL, 0, 0)
        if result != LB_ERR:
            return result
",if result :,151
"def compare_objects(left, right):
    left_fields = left.map_value.fields
    right_fields = right.map_value.fields
    for left_key, right_key in zip(sorted(left_fields), sorted(right_fields)):
        keyCompare = Order._compare_to(left_key, right_key)
        if keyCompare != 0:
            return keyCompare
        value_compare = Order.compare(left_fields[left_key], right_fields[right_key])
        if value_compare != 0:
            return value_compare
    return Order._compare_to(len(left_fields), len(right_fields))
",if value_compare != 0 :,163
"def get_opnd_types_short(ii):
    types = []
    for op in _gen_opnds(ii):
        if op.oc2:
            types.append(op.oc2)
        elif op_luf_start(op, ""GPRv""):
            types.append(""v"")
        elif op_luf_start(op, ""GPRz""):
            types.append(""z"")
        elif op_luf_start(op, ""GPRy""):
            types.append(""y"")
        else:
            die(""Unhandled op type {}"".format(op))
    return types
","elif op_luf_start ( op , ""GPRv"" ) :",161
"def _iter_indented_subactions(self, action):
    try:
        get_subactions = action._get_subactions
    except AttributeError:
        pass
    else:
        self._indent()
        if isinstance(action, argparse._SubParsersAction):
            for subaction in sorted(get_subactions(), key=lambda x: x.dest):
                yield subaction
        else:
            for subaction in get_subactions():
                yield subaction
        self._dedent()
","if isinstance ( action , argparse . _SubParsersAction ) :",132
"def has_safe_repr(value):
    """"""Does the node have a safe representation?""""""
    if value is None or value is NotImplemented or value is Ellipsis:
        return True
    if type(value) in (bool, int, float, complex, range_type, Markup) + string_types:
        return True
    if type(value) in (tuple, list, set, frozenset):
        for item in value:
            if not has_safe_repr(item):
                return False
        return True
    elif type(value) is dict:
        for key, value in iteritems(value):
            if not has_safe_repr(key):
                return False
            if not has_safe_repr(value):
                return False
        return True
    return False
",if not has_safe_repr ( item ) :,198
"def _compute_missing_fields_error(context, field_defs, incoming_fields):
    missing_fields = []
    for field_name, field_def in field_defs.items():
        if not field_def.is_optional and field_name not in incoming_fields:
            missing_fields.append(field_name)
    if missing_fields:
        if len(missing_fields) == 1:
            return create_missing_required_field_error(context, missing_fields[0])
        else:
            return create_missing_required_fields_error(context, missing_fields)
",if not field_def . is_optional and field_name not in incoming_fields :,150
"def _list(self):
    data_sources = self.mkt_contract.functions.getAllProviders().call()
    data = []
    for index, data_source in enumerate(data_sources):
        if index > 0:
            if ""test"" not in Web3.toText(data_source).lower():
                data.append(dict(dataset=self.to_text(data_source)))
    return pd.DataFrame(data)
",if index > 0 :,111
"def close_file_in_all_editorstacks(self, editorstack_id_str, index):
    for editorstack in self.editorstacks:
        if str(id(editorstack)) != editorstack_id_str:
            editorstack.blockSignals(True)
            editorstack.close_file(index, force=True)
            editorstack.blockSignals(False)
",if str ( id ( editorstack ) ) != editorstack_id_str :,97
"def _remove_custom_marker_object_instances(self):
    for id, obj in list(self._objects.items()):
        if isinstance(obj, objects.CustomObject):
            logger.info(""Removing CustomObject instance: id %s = obj '%s'"", id, obj)
            del self._objects[id]
","if isinstance ( obj , objects . CustomObject ) :",79
"def append(self, labels):
    if isinstance(labels, list):
        for label in labels:
            if not label in self.__menuLabels:
                self.__menuLabels.append(label)
                self.__enabledLabels.append(label)
    else:
        if not labels in self.__menuLabels:
            self.__menuLabels.append(labels)
            self.__enabledLabels.append(labels)
",if not label in self . __menuLabels :,108
"def _close_tree(view: View, defx: Defx, context: Context) -> None:
    for target in context.targets:
        if target[""is_directory""] and target[""is_opened_tree""]:
            view.close_tree(target[""action__path""], defx._index)
        else:
            view.close_tree(target[""action__path""].parent, defx._index)
            view.search_file(target[""action__path""].parent, defx._index)
","if target [ ""is_directory"" ] and target [ ""is_opened_tree"" ] :",124
"def FirstFetch(self):
    q = collections.deque([""buddy"", ""group"", ""discuss""])
    while q:
        tinfo = q.popleft()
        if self.Update(tinfo) and tinfo in (""group"", ""discuss""):
            cl = self.List(tinfo)
            if cl:
                q.extend(cl)
        time.sleep(1.0)
","if self . Update ( tinfo ) and tinfo in ( ""group"" , ""discuss"" ) :",106
"def _sort_values_jobconf(self):
    """"""Jobconf dictionary to enable sorting by value.""""""
    if not self._sort_values:
        return {}
    # translate _SORT_VALUES_JOBCONF to the correct Hadoop version,
    # without logging a warning
    hadoop_version = self.get_hadoop_version()
    jobconf = {}
    for k, v in _SORT_VALUES_JOBCONF.items():
        if hadoop_version:
            jobconf[translate_jobconf(k, hadoop_version)] = v
        else:
            for j in translate_jobconf_for_all_versions(k):
                jobconf[j] = v
    return jobconf
",if hadoop_version :,175
"def list(self):
    for fname in os.listdir(self.path):
        fpath = os.path.join(self.path, fname)
        if os.path.isfile(fpath) and fname.endswith(self.fileext):
            yield fname, get_etag_from_file(fpath)
",if os . path . isfile ( fpath ) and fname . endswith ( self . fileext ) :,75
"def get_environment_variable_value(val):
    env_val = val
    if val is not None and isinstance(val, str):
        match = re.search(r""^\${(?P<environment_key_name>\w+)*}$"", val)
        if match is not None:
            env_val = os.environ.get(match.group(""environment_key_name""))
    return env_val
",if match is not None :,99
"def L_op(self, inputs, outputs, grads):
    (x,) = inputs
    (gz,) = grads
    if x.type in complex_types:
        raise NotImplementedError()
    if outputs[0].type in discrete_types:
        if x.type in discrete_types:
            return [x.zeros_like(dtype=theano.config.floatX)]
        else:
            return [x.zeros_like()]
    cst = np.asarray(np.sqrt(np.pi) / 2.0, dtype=upcast(x.type.dtype, gz.type.dtype))
    return (gz * cst * exp(erfinv(x) ** 2),)
",if x . type in discrete_types :,165
"def is_test_finished(self):
    retcode = self.process.poll()
    if retcode is not None:
        logger.info(""Phantom done its work with exit code: %s"", retcode)
        self.phout_finished.set()
        return abs(retcode)
    else:
        info = self.get_info()
        if info:
            eta = int(info.duration) - (int(time.time()) - int(self.start_time))
            self.publish(""eta"", eta)
        return -1
",if info :,139
"def icon(display_icon):
    """"""returns empty dict if show_icons is False, else the icon passed""""""
    kws = {}
    if get_icon_switch():
        if display_icon.startswith(""SV_""):
            kws = {""icon_value"": custom_icon(display_icon)}
        elif display_icon != ""OUTLINER_OB_EMPTY"":
            kws = {""icon"": display_icon}
    return kws
","if display_icon . startswith ( ""SV_"" ) :",106
"def raise_to_cubic(bzs):
    result = []
    for sp in bzs:
        r = []
        for bz in sp:
            if len(bz) == 3:
                r.append(
                    (
                        bz[0],
                        lerppt(2.0 / 3, bz[0], bz[1]),
                        lerppt(2.0 / 3, bz[2], bz[1]),
                        bz[2],
                    )
                )
            else:
                r.append(bz)
        result.append(r)
    return result
",if len ( bz ) == 3 :,181
"def readline(self):
    while 1:
        line = self._readline()
        if line:
            self._filelineno += 1
            return line
        if not self._file:
            return line
        self.nextfile()
",if not self . _file :,65
"def readlines(self):
    """"""Returns a list of all lines (optionally parsed) in the file.""""""
    if self.grammar:
        tot = []
        # Used this way instead of a 'for' loop against
        # self.file.readlines() so that there wasn't two copies of the file
        # in memory.
        while 1:
            line = self.file.readline()
            if not line:
                break
            tot.append(line)
        return tot
    return self.file.readlines()
",if not line :,135
"def visit_return(self, node):
    # TODO: pythoncile.py handled (a) spliting CITDL (scoperef), (b)
    #      excluding ""None"" and ""NoneType"", (c) True/False -> bool.
    #      pythoncile.py also gather all return's and picked the most
    #      common guess.
    # TODO:XXX Evaluate the necessity of multiple return statement analysis.
    scope = self._peek_scope()
    assert scope.ilk == ""function""
    if not scope.get(""returns""):
        citdl = self._citdl_from_node(node.children[1])
        if citdl and citdl is not ""None"":
            scope.attrs[""returns""] = citdl
","if citdl and citdl is not ""None"" :",191
"def load_json_file(file_path):
    """"""load a file into a json object""""""
    try:
        with open(file_path) as small_file:
            return json.load(small_file)
    except OSError as e:
        print(e)
        print(""trying to read file in blocks"")
        with open(file_path) as big_file:
            json_string = """"
            while True:
                block = big_file.read(64 * (1 << 20))  # Read 64 MB at a time;
                json_string = json_string + block
                if not block:  # Reached EOF
                    break
            return json.loads(json_string)
",if not block :,189
"def rotate(cls, axis, theta):
    """"""Prepare a quaternion that represents a rotation on a given axis.""""""
    if isinstance(axis, str):
        if axis in (""x"", ""X""):
            axis = V.X
        elif axis in (""y"", ""Y""):
            axis = V.Y
        elif axis in (""z"", ""Z""):
            axis = V.Z
    axis = axis.normalize()
    s = math.sin(theta / 2.0)
    c = math.cos(theta / 2.0)
    return Q(axis._v[0] * s, axis._v[1] * s, axis._v[2] * s, c)
","elif axis in ( ""y"" , ""Y"" ) :",169
"def is_valid_block(self):
    """"""check wheter the block is valid in the current position""""""
    for i in range(self.block.x):
        for j in range(self.block.x):
            if self.block.get(i, j):
                if self.block.pos.x + i < 0:
                    return False
                if self.block.pos.x + i >= COLUMNS:
                    return False
                if self.block.pos.y + j < 0:
                    return False
                if self.map.get((self.block.pos.x + i, self.block.pos.y + j), False):
                    return False
    return True
",if self . block . pos . y + j < 0 :,192
"def dump_token_list(tokens):
    for token in tokens:
        if token.token_type == TOKEN_TEXT:
            writer.write(token.contents)
        elif token.token_type == TOKEN_VAR:
            writer.print_expr(token.contents)
            touch_var(token.contents)
",elif token . token_type == TOKEN_VAR :,83
"def encode(name, value):
    try:
        if parametrized.is_parametrized(name, value):
            value, params = value
            return _encode_parametrized(name, value, params)
        return _encode_unstructured(name, value)
    except Exception:
        _log.exception(""Failed to encode %s %s"" % (name, value))
        raise
","if parametrized . is_parametrized ( name , value ) :",102
"def conversation_to_fb_format(conversation):
    assert len(conversation) > 1
    lines = []
    for i in range(0, len(conversation), 2):
        if i + 1 < len(conversation):
            lines.append(
                ""%d %s\t%s"" % (i / 2 + 1, conversation[i], conversation[i + 1])
            )
        else:
            lines.append(""%d %s"" % (i / 2 + 1, conversation[i]))
    return ""\n"".join(lines)
",if i + 1 < len ( conversation ) :,145
"def _handle_js_events(self, change):
    if self.js_events:
        if self.event_handlers:
            for event in self.js_events:
                event_name = event[""name""]
                if event_name in self.event_handlers:
                    self.event_handlers[event_name](event[""detail""])
        # clears the event queue.
        self.js_events = []
",if self . event_handlers :,113
"def escapeall(self, lines):
    ""Escape all lines in an array according to the output options.""
    result = []
    for line in lines:
        if Options.html:
            line = self.escape(line, EscapeConfig.html)
        if Options.iso885915:
            line = self.escape(line, EscapeConfig.iso885915)
            line = self.escapeentities(line)
        elif not Options.unicode:
            line = self.escape(line, EscapeConfig.nonunicode)
        result.append(line)
    return result
",elif not Options . unicode :,143
"def filter_testsuite(suite, matcher, level=None):
    """"""Returns a flattened list of test cases that match the given matcher.""""""
    if not isinstance(suite, unittest.TestSuite):
        raise TypeError(""not a TestSuite"", suite)
    results = []
    for test in suite._tests:
        if level is not None and getattr(test, ""level"", 0) > level:
            continue
        if isinstance(test, unittest.TestCase):
            testname = test.id()  # package.module.class.method
            if matcher(testname):
                results.append(test)
        else:
            filtered = filter_testsuite(test, matcher, level)
            results.extend(filtered)
    return results
",if matcher ( testname ) :,185
"def _close_brackets(self, fragment):
    # If there any unclosed brackets in the text we try to close them
    # and we return part with closing brackets if they are ""closable""
    stack = []
    for char in fragment:
        if char in self._PARENS.keys():
            stack.append(char)
        elif char in self._PARENS.values():
            if stack and self._PARENS[stack[-1]] == char:
                stack.pop()
            else:
                return """"
    return """".join(self._PARENS[paren] for paren in reversed(stack))
",elif char in self . _PARENS . values ( ) :,150
"def restrict(points):
    result = []
    for p in points:
        if point_inside_mesh(bvh, p):
            result.append(p)
        else:
            loc, normal, index, distance = bvh.find_nearest(p)
            if loc is not None:
                result.append(tuple(loc))
    return result
",if loc is not None :,96
"def _check_ids(el, filename, parent_id):
    """"""Recursively walks through tree and check if every object has ID""""""
    for child in el:
        if child.tag == ""object"":
            msg = ""Widget has no ID in %s; class %s; Parent id: %s"" % (
                filename,
                child.attrib[""class""],
                parent_id,
            )
            assert ""id"" in child.attrib and child.attrib[""id""], msg
            for subel in child:
                if subel.tag == ""child"":
                    _check_ids(subel, filename, child.attrib[""id""])
","if subel . tag == ""child"" :",173
"def _checkIfSuccessfulCallback(self, result, error=False, **kwargs):
    if error:
        connection_error = kwargs.get(""connection_error"", False)
        if connection_error:
            log.debug(
                ""During direct file upload compute is not visible. Fallback to upload via controller.""
            )
            # there was an issue with connection, probably we don't have a direct access to compute
            # we need to fallback to uploading files via controller
            self._fileUploadToController()
        else:
            if ""message"" in result:
                log.error(
                    ""Error while direct file upload: {}"".format(result[""message""])
                )
        return
    self._callback(result, error, **kwargs)
",if connection_error :,198
"def getCellPropertyNames_aux(self, col_id):
    if col_id == ""name"":
        if self.image_icon == ""places_busy"":
            return [""places_busy""]
        baseName = self.image_icon
        if self.isOpen:
            return [baseName + ""_open""]
        else:
            return [baseName + ""_closed""]
    return []
","if self . image_icon == ""places_busy"" :",102
"def delete_volume(self, volume_id):
    if volume_id in self.volumes:
        volume = self.volumes[volume_id]
        if volume.attachment:
            raise VolumeInUseError(volume_id, volume.attachment.instance.id)
        return self.volumes.pop(volume_id)
    raise InvalidVolumeIdError(volume_id)
",if volume . attachment :,92
"def dashboards(self):
    dashboards = OrderedDict()
    for slug, path in enumerate(app_settings.DASHBOARDS):
        if isinstance(path, (list, tuple)):
            slug, path = path
        pk = str(slug)
        klass = import_string(path)
        dashboards[pk] = klass(pk=pk)
    if not dashboards:
        raise ImproperlyConfigured(""No dashboards found."")
    return dashboards
","if isinstance ( path , ( list , tuple ) ) :",112
"def test_reader(config, device, logger):
    loader = build_dataloader(config, ""Train"", device, logger)
    import time
    starttime = time.time()
    count = 0
    try:
        for data in loader():
            count += 1
            if count % 1 == 0:
                batch_time = time.time() - starttime
                starttime = time.time()
                logger.info(
                    ""reader: {}, {}, {}"".format(count, len(data[0]), batch_time)
                )
    except Exception as e:
        logger.info(e)
    logger.info(""finish reader: {}, Success!"".format(count))
",if count % 1 == 0 :,175
"def on_adapter_selected(self, menuitem, adapter_path):
    if menuitem.props.active:
        if adapter_path != self.blueman.List.Adapter.get_object_path():
            logging.info(""selected %s"", adapter_path)
            self.blueman.Config[""last-adapter""] = adapter_path_to_name(adapter_path)
            self.blueman.List.set_adapter(adapter_path)
",if adapter_path != self . blueman . List . Adapter . get_object_path ( ) :,111
"def set_note_pinned(self, key, pinned):
    n = self.notes[key]
    old_pinned = utils.note_pinned(n)
    if pinned != old_pinned:
        if ""systemtags"" not in n:
            n[""systemtags""] = []
        systemtags = n[""systemtags""]
        if pinned:
            # which by definition means that it was NOT pinned
            systemtags.append(""pinned"")
        else:
            systemtags.remove(""pinned"")
        n[""modifydate""] = time.time()
        self.notify_observers(
            ""change:note-status"",
            events.NoteStatusChangedEvent(what=""modifydate"", key=key),
        )
","if ""systemtags"" not in n :",186
"def setMinCores(self, rpcObjects=None):
    tasks = self._getSelected(rpcObjects)
    if tasks:
        current = max([task.data.min_cores for task in tasks])
        title = ""Set Minimum Cores""
        body = ""Please enter the new minimum cores value:""
        (value, choice) = QtWidgets.QInputDialog.getDouble(
            self._caller, title, body, current, 0, 50000, 0
        )
        if choice:
            for task in tasks:
                task.setMinCores(float(value))
            self._update()
",if choice :,160
"def _1_0_cloud_ips_cip_jsjc5(self, method, url, body, headers):
    if method == ""DELETE"":
        return self.test_response(httplib.OK, """")
    elif method == ""PUT"":
        body = json.loads(body)
        if body.get(""reverse_dns"", None) == ""fred.co.uk"":
            return self.test_response(httplib.OK, """")
        else:
            return self.test_response(
                httplib.BAD_REQUEST, '{""error_name"":""bad dns"", ""errors"": [""Bad dns""]}'
            )
","if body . get ( ""reverse_dns"" , None ) == ""fred.co.uk"" :",160
"def _print_one_entry(news_entry: xml.etree.ElementTree.Element) -> None:
    child: xml.etree.ElementTree.Element
    for child in news_entry:
        if ""title"" in child.tag:
            title = str(child.text)
        if ""pubDate"" in child.tag:
            pub_date = str(child.text)
        if ""description"" in child.tag:
            description = str(child.text)
    print_stdout(color_line(title, 14) + "" ("" + bold_line(pub_date) + "")"")
    print_stdout(format_paragraph(strip_tags(description)))
    print_stdout()
","if ""pubDate"" in child . tag :",169
"def oregon_battery(self, offset):
    nib = self.decoded_nibbles
    batt = ""OK""
    if nib[offset][3] != """":
        if (int(nib[offset][3], 16) >> 2) & 0x1 == 1:
            batt = ""Low""
        self.put(
            nib[offset][0], nib[offset][1], self.out_ann, [2, [""Batt "" + batt, batt]]
        )
","if ( int ( nib [ offset ] [ 3 ] , 16 ) >> 2 ) & 0x1 == 1 :",128
"def body_stream() -> typing.AsyncGenerator[bytes, None]:
    while True:
        message = await queue.get()
        if message is None:
            break
        assert message[""type""] == ""http.response.body""
        yield message.get(""body"", b"""")
    task.result()
",if message is None :,77
"def _wait_for_reboot():
    try:
        state = self._conn.reboot_domain(instance[""name""])
        if state == power_state.RUNNING:
            LOG.debug(_(""instance %s: rebooted""), instance[""name""])
            timer.stop()
    except Exception:
        LOG.exception(_(""_wait_for_reboot failed""))
        timer.stop()
",if state == power_state . RUNNING :,97
"def _get_sequence_vector(
    sequence,
    tokenizer,
    format_dtype,
    unit_to_id,
    lowercase=True,
    unknown_symbol=UNKNOWN_SYMBOL,
):
    unit_sequence = tokenizer(sequence.lower() if lowercase else sequence)
    unit_indices_vector = np.empty(len(unit_sequence), dtype=format_dtype)
    for i in range(len(unit_sequence)):
        curr_unit = unit_sequence[i]
        if curr_unit in unit_to_id:
            unit_indices_vector[i] = unit_to_id[curr_unit]
        else:
            unit_indices_vector[i] = unit_to_id[unknown_symbol]
    return unit_indices_vector
",if curr_unit in unit_to_id :,188
"def forward(self, x: Tensor, edge_index: Adj) -> Tensor:
    """"""""""""
    if self.add_self_loops:
        if isinstance(edge_index, Tensor):
            edge_index, _ = remove_self_loops(edge_index)
            edge_index, _ = add_self_loops(edge_index, num_nodes=x.size(self.node_dim))
        elif isinstance(edge_index, SparseTensor):
            edge_index = set_diag(edge_index)
    x_norm = F.normalize(x, p=2.0, dim=-1)
    # propagate_type: (x: Tensor, x_norm: Tensor)
    return self.propagate(edge_index, x=x, x_norm=x_norm, size=None)
","elif isinstance ( edge_index , SparseTensor ) :",196
"def _init_req_settings(self, **kwargs):
    for req_attr in self._req_settings:
        req_attr_value = kwargs.get(req_attr)
        if req_attr_value is None:
            raise MissingRequiredConf(conf_name=req_attr_value)
        # Validate attribute value
        req_attr_value = get_validator(req_attr)(req_attr_value)
        self._settings[req_attr] = req_attr_value
",if req_attr_value is None :,122
"def delete(identifier, filenames=None, **kwargs):
    item = get_item(identifier)
    if filenames:
        if not isinstance(filenames, (set, list)):
            filenames = [filenames]
        for f in item.iter_files():
            if f.name not in filenames:
                continue
            f.delete(**kwargs)
",if f . name not in filenames :,91
"def visit_decorator(self, o: Decorator) -> None:
    if self.is_private_name(o.func.name, o.func.fullname):
        return
    is_abstract = False
    for decorator in o.original_decorators:
        if isinstance(decorator, NameExpr):
            if self.process_name_expr_decorator(decorator, o):
                is_abstract = True
        elif isinstance(decorator, MemberExpr):
            if self.process_member_expr_decorator(decorator, o):
                is_abstract = True
    self.visit_func_def(o.func, is_abstract=is_abstract)
","if self . process_member_expr_decorator ( decorator , o ) :",160
"def split_trading_pair(trading_pair: str) -> Optional[Tuple[str, str]]:
    try:
        m = RE_4_LETTERS_QUOTE.match(trading_pair)
        if m is None:
            m = RE_3_LETTERS_QUOTE.match(trading_pair)
            if m is None:
                m = RE_2_LETTERS_QUOTE.match(trading_pair)
        return m.group(1), m.group(2)
    # Exceptions are now logged as warnings in trading pair fetcher
    except Exception:
        return None
",if m is None :,156
"def traverse_states(root):
    todo = [root]
    model = self.model
    while len(todo):
        iter = todo.pop(0)
        # print model.value_path(iter, treeindex), model.get_state(iter, treeindex)
        yield model.get_state(iter, treeindex)
        path = model.get_path(iter)
        if treeview.row_expanded(path):
            children = []
            child = model.iter_children(iter)
            while child:
                children.append(child)
                child = model.iter_next(child)
            todo = children + todo
    yield None  # end marker
",if treeview . row_expanded ( path ) :,178
"def as_list(
    self, compact=True, storage_to_dict=True, datetime_to_str=False, custom_types=None
):
    if storage_to_dict:
        items = []
        for row in self:
            item = row.as_dict(datetime_to_str, custom_types)
            for jdata in self._joins_:
                if not jdata[2]:
                    item[jdata[0]] = row[jdata[0]].as_list()
            items.append(item)
    else:
        items = [item for item in self]
    return items
",if not jdata [ 2 ] :,160
"def zip(target, source, env):
    compression = env.get(""ZIPCOMPRESSION"", 0)
    zf = zipfile.ZipFile(str(target[0]), ""w"", compression)
    for s in source:
        if s.isdir():
            for dirpath, dirnames, filenames in os.walk(str(s)):
                for fname in filenames:
                    path = os.path.join(dirpath, fname)
                    if os.path.isfile(path):
                        zf.write(path)
        else:
            zf.write(str(s))
    zf.close()
",if os . path . isfile ( path ) :,155
"def remove_PBA_files():
    if monkey_island.cc.services.config.ConfigService.get_config():
        windows_filename = (
            monkey_island.cc.services.config.ConfigService.get_config_value(
                PBA_WINDOWS_FILENAME_PATH
            )
        )
        linux_filename = (
            monkey_island.cc.services.config.ConfigService.get_config_value(
                PBA_LINUX_FILENAME_PATH
            )
        )
        if linux_filename:
            remove_file(linux_filename)
        if windows_filename:
            remove_file(windows_filename)
",if windows_filename :,183
"def test_takewhile(self):
    for s in (range(10), range(0), range(1000), (7, 11), range(2000, 2200, 5)):
        for g in (G, I, Ig, S, L, R):
            tgt = []
            for elem in g(s):
                if not isEven(elem):
                    break
                tgt.append(elem)
            self.assertEqual(list(takewhile(isEven, g(s))), tgt)
        self.assertRaises(TypeError, takewhile, isEven, X(s))
        self.assertRaises(TypeError, takewhile, isEven, N(s))
        self.assertRaises(ZeroDivisionError, list, takewhile(isEven, E(s)))
",if not isEven ( elem ) :,188
"def find_defined_variables(board_config_mks):
    re_def = re.compile(""^[\s]*([\w\d_]*)[\s]*:="")
    variables = dict()
    for board_config_mk in board_config_mks:
        for line in open(board_config_mk, encoding=""latin1""):
            mo = re_def.search(line)
            if mo is None:
                continue
            variable = mo.group(1)
            if variable in white_list:
                continue
            if variable not in variables:
                variables[variable] = set()
            variables[variable].add(board_config_mk[len(TOP) + 1 :])
    return variables
",if variable not in variables :,188
"def download_file(url, file):
    try:
        xlog.info(""download %s to %s"", url, file)
        opener = get_opener()
        req = opener.open(url, cafile="""")
        CHUNK = 16 * 1024
        with open(file, ""wb"") as fp:
            while True:
                chunk = req.read(CHUNK)
                if not chunk:
                    break
                fp.write(chunk)
        return True
    except:
        xlog.info(""download %s to %s fail"", url, file)
        return False
",if not chunk :,158
"def set_preferred_lane(self, preferred_lane: int = None) -> ""AbstractEnv"":
    env_copy = copy.deepcopy(self)
    if preferred_lane:
        for v in env_copy.road.vehicles:
            if isinstance(v, IDMVehicle):
                v.route = [(lane[0], lane[1], preferred_lane) for lane in v.route]
                # Vehicle with lane preference are also less cautious
                v.LANE_CHANGE_MAX_BRAKING_IMPOSED = 1000
    return env_copy
","if isinstance ( v , IDMVehicle ) :",152
"def resolve(self, value: Optional[T]) -> T:
    v: Optional[Any] = value
    if value is None:
        t = os.environ.get(self.envvar)
        if self.type is bool and t:
            v = t in [""true"", ""True"", ""1"", ""yes""]
        elif self.type is str and t:
            v = t
        elif t:
            v = ast.literal_eval(t) if t is not None else None
    if v is None:
        v = self.default
    return v
",elif self . type is str and t :,144
"def test_read_lazy_A(self):
    want = [""x"" * 100, EOF_sigil]
    self.dataq.put(want)
    telnet = telnetlib.Telnet(HOST, self.port)
    self.dataq.join()
    time.sleep(self.block_short)
    self.assertEqual("""", telnet.read_lazy())
    data = """"
    while True:
        try:
            read_data = telnet.read_lazy()
            data += read_data
            if not read_data:
                telnet.fill_rawq()
        except EOFError:
            break
        self.assertTrue(want[0].startswith(data))
    self.assertEqual(data, want[0])
",if not read_data :,192
"def request_put_json(url, headers):
    """"""Makes a PUT request and returns the JSON response""""""
    try:
        response = requests.put(url, headers=headers)
        if response.status_code == 200:
            return response.json()
        else:
            raise RadarrRequestError(
                ""Invalid response received from Radarr: %s"" % response.content
            )
    except RequestException as e:
        raise RadarrRequestError(
            ""Unable to connect to Radarr at %s. Error: %s"" % (url, e)
        )
",if response . status_code == 200 :,150
"def firebase_analysis(urls):
    # Detect Firebase URL
    firebase_db = []
    logger.info(""Detecting Firebase URL(s)"")
    for url in urls:
        if ""firebaseio.com"" in url:
            returl, is_open = open_firebase(url)
            fbdic = {""url"": returl, ""open"": is_open}
            if fbdic not in firebase_db:
                firebase_db.append(fbdic)
    return firebase_db
","if ""firebaseio.com"" in url :",135
"def logprob(self, sample):
    if self._log:
        return self._prob_dict.get(sample, _NINF)
    else:
        if sample not in self._prob_dict:
            return _NINF
        elif self._prob_dict[sample] == 0:
            return _NINF
        else:
            return math.log(self._prob_dict[sample], 2)
",if sample not in self . _prob_dict :,102
"def is_image(self, input):
    try:
        if isinstance(input, (np.ndarray, Image.Image)):
            return True
        elif isinstance(input, str):
            if not os.path.isfile(input):
                raise ValueError(""input must be a file"")
            img = Image.open(input)
            _ = img.size
            return True
        else:
            return False
    except:
        return False
","if isinstance ( input , ( np . ndarray , Image . Image ) ) :",122
"def extract(self):
    for battery in self.vars:
        for line in dopen(""/proc/acpi/battery/"" + battery + ""/state"").readlines():
            l = line.split()
            if len(l) < 3:
                continue
            if l[0:2] == [""remaining"", ""capacity:""]:
                remaining = int(l[2])
                continue
            elif l[0:2] == [""present"", ""rate:""]:
                rate = int(l[2])
                continue
        if rate and remaining:
            self.val[battery] = remaining * 60 / rate
        else:
            self.val[battery] = -1
","elif l [ 0 : 2 ] == [ ""present"" , ""rate:"" ] :",185
"def get_app_module(module_name, raise_on_failure=True):
    try:
        __import__(module_name)
    except ImportError:
        if sys.exc_info()[-1].tb_next:
            raise RuntimeError(
                f""While importing '{module_name}', an ImportError was raised:""
                f""\n\n{traceback.format_exc()}""
            )
        elif raise_on_failure:
            raise RuntimeError(f""Could not import '{module_name}'."")
        else:
            return
    return sys.modules[module_name]
",elif raise_on_failure :,152
"def process_shutdown_hooks(self):
    for plugin_name in self.DISCOVERED.keys():
        try:
            package = ""mailpile.plugins.%s"" % plugin_name
            _, manifest = self.DISCOVERED[plugin_name]
            if package in sys.modules:
                for method_name in self._mf_path(manifest, ""lifecycle"", ""shutdown""):
                    method = self._get_method(package, method_name)
                    method(self.config)
        except:
            # ignore exceptions here as mailpile is going to shut down
            traceback.print_exc(file=sys.stderr)
",if package in sys . modules :,175
"def _check_arch(self, arch):
    if arch is None:
        return
    try:
        from pycuda.driver import Context
        capability = Context.get_device().compute_capability()
        if tuple(map(int, tuple(arch.split(""_"")[1]))) > capability:
            from warnings import warn
            warn(
                ""trying to compile for a compute capability "" ""higher than selected GPU""
            )
    except Exception:
        pass
","if tuple ( map ( int , tuple ( arch . split ( ""_"" ) [ 1 ] ) ) ) > capability :",122
"def phpinfo_ext(content):
    indexes = SubstrFind(content, ""AbracadabrA"")
    found = len(indexes) > 0
    got = """"
    if found:
        start = indexes[0] + 11
        for x in range(start, len(content)):
            if content[x] == ""<"":
                break
            got += content[x]
    return got
","if content [ x ] == ""<"" :",103
"def update_leaderboard(wait_time):
    conn = get_connection()
    cursor = conn.cursor(MySQLdb.cursors.DictCursor)
    while True:
        try:
            if use_log:
                log.info(""Updating leaderboard and adding some sigma"")
            cursor.execute(""call generate_leaderboard;"")
            if wait_time == 0:
                break
            for s in range(wait_time):
                # allow for a [Ctrl]+C during the sleep cycle
                time.sleep(1)
        except KeyboardInterrupt:
            break
        except:
            # log error
            log.error(traceback.format_exc())
            break
    cursor.close()
    conn.close()
",if wait_time == 0 :,199
"def writeBit(self, state, endian):
    if self._bit_pos == 7:
        self._bit_pos = 0
        if state:
            if endian is BIG_ENDIAN:
                self._byte |= 1
            else:
                self._byte |= 128
        self._output.write(chr(self._byte))
        self._byte = 0
    else:
        if state:
            if endian is BIG_ENDIAN:
                self._byte |= 1 << self._bit_pos
            else:
                self._byte |= 1 << (7 - self._bit_pos)
        self._bit_pos += 1
",if state :,177
"def getreportopt(config):
    reportopts = """"
    reportchars = config.option.reportchars
    if not config.option.disablepytestwarnings and ""w"" not in reportchars:
        reportchars += ""w""
    elif config.option.disablepytestwarnings and ""w"" in reportchars:
        reportchars = reportchars.replace(""w"", """")
    if reportchars:
        for char in reportchars:
            if char not in reportopts and char != ""a"":
                reportopts += char
            elif char == ""a"":
                reportopts = ""fEsxXw""
    return reportopts
","if char not in reportopts and char != ""a"" :",157
"def validate_module(self, pipeline):
    if self.mode == MODE_UNTANGLE:
        if self.training_set_directory.dir_choice != URL_FOLDER_NAME:
            path = os.path.join(
                self.training_set_directory.get_absolute_path(),
                self.training_set_file_name.value,
            )
            if not os.path.exists(path):
                raise ValidationError(
                    ""Can't find file %s"" % self.training_set_file_name.value,
                    self.training_set_file_name,
                )
",if self . training_set_directory . dir_choice != URL_FOLDER_NAME :,170
"def reshape(w, h):
    try:
        # Prevent a division by zero when minimising the window
        if h == 0:
            h = 1
        # Set the drawable region of the window
        glViewport(0, 0, w, h)
        # set up the projection matrix
        glMatrixMode(GL_PROJECTION)
        glLoadIdentity()
        # go back to modelview matrix so we can move the objects about
        glMatrixMode(GL_MODELVIEW)
        updatePickingBuffer()
    except Exception:
        log.error(""gl.reshape"", exc_info=True)
",if h == 0 :,157
"def __setitem__(self, key, value):
    if not isinstance(value, PseudoNamespace):
        tuple_converted = False
        if isinstance(value, dict):
            value = PseudoNamespace(value)
        elif isinstance(value, tuple):
            value = list(value)
            tuple_converted = True
        if isinstance(value, list):
            for i, item in enumerate(value):
                if isinstance(item, dict) and not isinstance(item, PseudoNamespace):
                    value[i] = PseudoNamespace(item)
            if tuple_converted:
                value = tuple(value)
    super(PseudoNamespace, self).__setitem__(key, value)
","if isinstance ( value , dict ) :",175
"def scan_search(state):
    delim = state.source[state.position - 1]
    while True:
        c = state.consume()
        if c == delim:
            state.start += 1
            state.backup()
            content = state.emit()
            state.consume()
            token = TokenSearchForward if c == ""/"" else TokenSearchBackward
            return scan_range, [token(content)]
        elif c == EOF:
            raise ValueError(""unclosed search pattern: {0}"".format(state.source))
",elif c == EOF :,141
"def fromVariant(variant):
    if hasattr(QtCore, ""QVariant"") and isinstance(variant, QtCore.QVariant):
        t = variant.type()
        if t == QtCore.QVariant.String:
            return str(variant.toString())
        elif t == QtCore.QVariant.Double:
            return variant.toDouble()[0]
        elif t == QtCore.QVariant.Int:
            return variant.toInt()[0]
        elif t == QtCore.QVariant.Bool:
            return variant.toBool()
        elif t == QtCore.QVariant.Invalid:
            return None
        else:
            raise ValueError('Unsupported QVariant type ""%s""' % variant.typeName())
    else:
        return variant
",elif t == QtCore . QVariant . Invalid :,195
"def __iter__(self):
    i = 0
    for category, filename in list(self.input_files.items()):
        for line in open(filename):
            line = self._clean_line(line)
            if self.accept_criteria(i):
                yield Opinion(line, category)
            i += 1
            if i % 1000 == 0:
                print(""\tReaded {} examples"".format(i))
",if i % 1000 == 0 :,115
"def test_listing_all_frameworks_and_check_frameworks_by_order(self):
    """"""List all frameworks and check if frameworks appear by order""""""
    result = subprocess.check_output(self.command_as_list([UMAKE, ""--list""]))
    previous_framework = None
    for element in result.split(b""\n""):
        if element.startswith(b""\t""):
            current_framework = element[: element.find(b"":"")]
            if previous_framework:
                self.assertTrue(previous_framework < current_framework)
            previous_framework = current_framework
        else:
            previous_framework = None
","if element . startswith ( b""\t"" ) :",160
"def _locate_code(self, event):
    if self._current_code_view is None:
        return
    iid = self.tree.focus()
    if iid != """":
        values = self.tree.item(iid)[""values""]
        if isinstance(values, list) and len(values) >= 5:
            start_line, start_col, end_line, end_col = values[1:5]
            self._current_code_view.select_range(
                TextRange(start_line, start_col, end_line, end_col)
            )
","if isinstance ( values , list ) and len ( values ) >= 5 :",150
"def __setattr__(self, attr, value):
    """"""Provides additional checks on recipient fields.""""""
    if attr in [""to"", ""cc"", ""bcc""]:
        if isinstance(value, basestring):
            if value == """" and getattr(self, ""ALLOW_BLANK_EMAIL"", False):
                return
            check_email_valid(value, attr)
        else:
            for address in value:
                check_email_valid(address, attr)
    elif attr == ""headers"":
        check_headers_valid(value)
    super(EmailMessage, self).__setattr__(attr, value)
","if value == """" and getattr ( self , ""ALLOW_BLANK_EMAIL"" , False ) :",151
"def _scanDirectory(self, dirIter, f):
    while len(f) < 250:
        try:
            info = next(dirIter)
        except StopIteration:
            if not f:
                raise EOFError
            return f
        if isinstance(info, defer.Deferred):
            info.addCallback(self._cbScanDirectory, dirIter, f)
            return
        else:
            f.append(info)
    return f
","if isinstance ( info , defer . Deferred ) :",122
"def iterator():
    try:
        while True:
            yield from pullparser.read_events()
            # load event buffer
            data = source.read(16 * 1024)
            if not data:
                break
            pullparser.feed(data)
        root = pullparser._close_and_return_root()
        yield from pullparser.read_events()
        it.root = root
    finally:
        if close_source:
            source.close()
",if close_source :,130
"def test_until_timeout(self):
    timer = TestTimer(self.timeout)
    while not timer.is_timed_out():
        if self.all_analyzers_pass():
            self.log_success(timer)
            return
        sleep(DELAY_BETWEEN_ANALYSIS)
        LOGGER.debug(
            ""Waiting until all analyzers passed. Time passed: {}"".format(
                timer.get_time_taken()
            )
        )
    self.log_failure(timer)
    assert False
",if self . all_analyzers_pass ( ) :,139
"def start(self):
    """"""Start our callback in its own perpetual timer thread.""""""
    if self.frequency > 0:
        threadname = self.name or self.__class__.__name__
        if self.thread is None:
            self.thread = PerpetualTimer(self.frequency, self.callback)
            self.thread.bus = self.bus
            self.thread.setName(threadname)
            self.thread.start()
            self.bus.log(""Started monitor thread %r."" % threadname)
        else:
            self.bus.log(""Monitor thread %r already started."" % threadname)
",if self . thread is None :,160
"def set_flavour(flavour, request=None, permanent=False):
    if flavour not in settings.FLAVOURS:
        raise ValueError(
            u""'%r' is no valid flavour. Allowed flavours are: %s""
            % (
                flavour,
                "", "".join(settings.FLAVOURS),
            )
        )
    request = request or getattr(_local, ""request"", None)
    if request:
        request.flavour = flavour
        if permanent:
            flavour_storage.set(request, flavour)
    elif permanent:
        raise ValueError(u""Cannot set flavour permanently, no request available."")
    _local.flavour = flavour
",if permanent :,176
"def get_images(image_path, support_ext="".jpg|.jpeg|.png""):
    if not os.path.exists(image_path):
        raise Exception(f""Image path {image_path} invalid"")
    if os.path.isfile(image_path):
        return [image_path]
    imgs = []
    for item in os.listdir(image_path):
        ext = os.path.splitext(item)[1][1:].strip().lower()
        if len(ext) > 0 and ext in support_ext:
            item_path = os.path.join(image_path, item)
            imgs.append(item_path)
    return imgs
",if len ( ext ) > 0 and ext in support_ext :,167
"def write_text(self, text):
    """"""Writes re-indented text into the buffer.""""""
    should_indent = False
    rows = []
    for row in text.split(""\n""):
        if should_indent:
            row = ""    {}"".format(row)
        if ""\b"" in row:
            row = row.replace(""\b"", """", 1)
            should_indent = True
        elif not len(row.strip()):
            should_indent = False
        rows.append(row)
    self.write(""{}\n"".format(""\n"".join(rows)))
",if should_indent :,147
"def build_priorities(self, _iter, priorities):
    while _iter is not None:
        if self.files_treestore.iter_has_child(_iter):
            self.build_priorities(self.files_treestore.iter_children(_iter), priorities)
        elif not self.files_treestore.get_value(_iter, 1).endswith(os.path.sep):
            priorities[
                self.files_treestore.get_value(_iter, 3)
            ] = self.files_treestore.get_value(_iter, 0)
        _iter = self.files_treestore.iter_next(_iter)
    return priorities
","elif not self . files_treestore . get_value ( _iter , 1 ) . endswith ( os . path . sep ) :",170
"def _validate_sample(self, value):
    mask = self.support(value)
    if not_jax_tracer(mask):
        if not np.all(mask):
            warnings.warn(
                ""Out-of-support values provided to log prob method. ""
                ""The value argument should be within the support.""
            )
    return mask
",if not np . all ( mask ) :,94
"def https_open(self, req):
    try:
        return self.do_open(do_connection, req)
    except Exception as err_msg:
        try:
            error_msg = str(err_msg.args[0]).split(""] "")[1] + "".""
        except IndexError:
            error_msg = str(err_msg.args[0]) + "".""
        if settings.INIT_TEST == True:
            if settings.VERBOSITY_LEVEL < 2:
                print(settings.FAIL_STATUS)
        else:
            if settings.VERBOSITY_LEVEL < 1:
                print("""")
        print(settings.print_critical_msg(error_msg))
        raise SystemExit()
",if settings . VERBOSITY_LEVEL < 2 :,187
"def add_party(self, party_type, party):
    party_doc = frappe.new_doc(party_type)
    if party_type == ""Customer"":
        party_doc.customer_name = party
    else:
        supplier_group = frappe.db.get_single_value(""Buying Settings"", ""supplier_group"")
        if not supplier_group:
            frappe.throw(_(""Please Set Supplier Group in Buying Settings.""))
        party_doc.supplier_name = party
        party_doc.supplier_group = supplier_group
    party_doc.flags.ignore_mandatory = True
    party_doc.save(ignore_permissions=True)
",if not supplier_group :,175
"def get_polymorphic_model(data):
    for model in itervalues(models):
        polymorphic = model.opts.polymorphic
        if polymorphic:
            polymorphic_key = polymorphic
            if isinstance(polymorphic_key, bool):
                polymorphic_key = ""type""
            if data.get(polymorphic_key) == model.__name__:
                return model
    raise ImproperlyConfigured(u""No model found for data: {!r}"".format(data))
",if polymorphic :,133
"def cleanup_expired_revoked_tokens():
    """"""Remove tokens that have now expired from the revoked token table.""""""
    revoked_tokens = db.session.query(RevokedToken).all()
    for revoked_token in revoked_tokens:
        if Journalist.validate_token_is_not_expired_or_invalid(revoked_token.token):
            pass  # The token has not expired, we must keep in the revoked token table.
        else:
            # The token is no longer valid, remove from the revoked token table.
            db.session.delete(revoked_token)
    db.session.commit()
",if Journalist . validate_token_is_not_expired_or_invalid ( revoked_token . token ) :,161
"def matches_filter(key, values):
    if key == ""location"":
        if location_type in (""availability-zone"", ""availability-zone-id""):
            return offering.get(""Location"") in values
        elif location_type == ""region"":
            return any(v for v in values if offering.get(""Location"").startswith(v))
        else:
            return False
    elif key == ""instance-type"":
        return offering.get(""InstanceType"") in values
    else:
        return False
","elif location_type == ""region"" :",130
"def autoname(self):
    naming_method = frappe.db.get_value(""HR Settings"", None, ""emp_created_by"")
    if not naming_method:
        throw(_(""Please setup Employee Naming System in Human Resource > HR Settings""))
    else:
        if naming_method == ""Naming Series"":
            set_name_by_naming_series(self)
        elif naming_method == ""Employee Number"":
            self.name = self.employee_number
        elif naming_method == ""Full Name"":
            self.set_employee_name()
            self.name = self.employee_name
    self.employee = self.name
","elif naming_method == ""Full Name"" :",169
"def readHexStringFromStream(stream):
    stream.read(1)
    txt = """"
    x = b_("""")
    while True:
        tok = readNonWhitespace(stream)
        if not tok:
            # stream has truncated prematurely
            raise PdfStreamError(""Stream has ended unexpectedly"")
        if tok == b_("">""):
            break
        x += tok
        if len(x) == 2:
            txt += chr(int(x, base=16))
            x = b_("""")
    if len(x) == 1:
        x += b_(""0"")
    if len(x) == 2:
        txt += chr(int(x, base=16))
    return createStringObject(b_(txt))
",if len ( x ) == 2 :,190
"def test_technical_on(self):
    # Turn everything on
    data = {
        ""developer_comments"": ""Test comment!"",
        ""whiteboard-public"": ""Whiteboard info."",
    }
    response = self.client.post(self.technical_edit_url, data)
    assert response.context[""form""].errors == {}
    addon = self.get_addon()
    for k in data:
        if k == ""developer_comments"":
            assert str(getattr(addon, k)) == str(data[k])
        elif k == ""whiteboard-public"":
            assert str(addon.whiteboard.public) == str(data[k])
        else:
            assert getattr(addon, k) == (data[k] == ""on"")
","elif k == ""whiteboard-public"" :",193
"def create_season_posters(self, show_obj, force=False):
    if self.season_posters and show_obj:
        result = []
        for ep_obj in show_obj.episodes:
            if not self._has_season_poster(show_obj, ep_obj.season) or force:
                sickrage.app.log.debug(
                    ""Metadata provider ""
                    + self.name
                    + "" creating season posters for ""
                    + show_obj.name
                )
                result = result + [self.save_season_poster(show_obj, ep_obj.season)]
        return all(result)
    return False
","if not self . _has_season_poster ( show_obj , ep_obj . season ) or force :",196
"def get_prefixes(self, guild: Optional[discord.Guild] = None) -> List[str]:
    ret: List[str]
    gid: Optional[int] = guild.id if guild else None
    if gid in self._cached:
        ret = self._cached[gid].copy()
    else:
        if gid is not None:
            ret = await self._config.guild_from_id(gid).prefix()
            if not ret:
                ret = await self.get_prefixes(None)
        else:
            ret = self._global_prefix_overide or (await self._config.prefix())
        self._cached[gid] = ret.copy()
    return ret
",if not ret :,180
"def checkUnchangedIvars(obj, d, exceptions=None):
    if not exceptions:
        exceptions = []
    ok = True
    for key in d:
        if key not in exceptions:
            if getattr(obj, key) != d.get(key):
                g.trace(
                    ""changed ivar: %s old: %s new: %s""
                    % (key, repr(d.get(key)), repr(getattr(obj, key)))
                )
                ok = False
    return ok
","if getattr ( obj , key ) != d . get ( key ) :",142
"def validate_ip(address):
    try:
        if socket.inet_aton(address):
            if len(address.split(""."")) == 4:
                debug_msg(""setcore"", ""this is a valid IP address"", 5)
                return True
            else:
                print_error(""This is not a valid IP address..."")
                raise socket.error
        else:
            raise socket_error
    except socket.error:
        return False
","if len ( address . split ( ""."" ) ) == 4 :",125
"def kernel(x, y):
    diff = safe_norm(x - y, ord=2) if self._normed() and x.ndim >= 1 else x - y
    kernel_res = jnp.exp(-(diff ** 2) / bandwidth)
    if self._mode == ""matrix"":
        if self.matrix_mode == ""norm_diag"":
            return kernel_res * jnp.identity(x.shape[0])
        else:
            return jnp.diag(kernel_res)
    else:
        return kernel_res
","if self . matrix_mode == ""norm_diag"" :",133
"def __init__(self, transforms):
    assert isinstance(transforms, collections.abc.Sequence)
    self.transforms = []
    for transform in transforms:
        if isinstance(transform, dict):
            transform = build_from_cfg(transform, PIPELINES)
            self.transforms.append(transform)
        elif callable(transform):
            self.transforms.append(transform)
        else:
            raise TypeError(""transform must be callable or a dict"")
",elif callable ( transform ) :,115
"def translate(
    self,
    message: str,
    plural_message: Optional[str] = None,
    count: Optional[int] = None,
) -> str:
    if plural_message is not None:
        assert count is not None
        if count != 1:
            message = plural_message
            message_dict = self.translations.get(""plural"", {})
        else:
            message_dict = self.translations.get(""singular"", {})
    else:
        message_dict = self.translations.get(""unknown"", {})
    return message_dict.get(message, message)
",if count != 1 :,149
"def install_requires(cls, reduced_dependencies):
    install_requires = OrderedSet()
    for dep in reduced_dependencies:
        if cls.is_requirements(dep):
            for req in dep.payload.requirements:
                install_requires.add(str(req.requirement))
        elif cls.has_provides(dep):
            install_requires.add(dep.provides.key)
    return install_requires
",if cls . is_requirements ( dep ) :,110
"def doit():
    recipes_path = expanduser(""recipes.pprint"")
    recipe_dicts = eval(open(recipes_path).read())
    for r in recipe_dicts:
        for key in r.keys():
            if key not in (""desc"", ""comments""):
                del r[key]
        for c in r[""comments""]:
            for key in c.keys():
                if key not in (""comment"", ""title""):
                    del c[key]
    f = open(""stripped.pprint"", ""w"")
    f.write(pformat(recipe_dicts))
    f.close()
","if key not in ( ""comment"" , ""title"" ) :",163
"def setup(self, name):
    value = self.default
    if self.environ:
        full_environ_name = self.full_environ_name(name)
        if full_environ_name in os.environ:
            value = self.to_python(os.environ[full_environ_name])
        elif self.environ_required:
            raise ValueError(
                ""Value {0!r} is required to be set as the ""
                ""environment variable {1!r}"".format(name, full_environ_name)
            )
    self.value = value
    return value
",if full_environ_name in os . environ :,153
"def get_art_abs(story_file):
    lines = read_text_file(story_file)
    lines = [line.lower() for line in lines]
    lines = [fix_missing_period(line) for line in lines]
    article_lines = []
    highlights = []
    next_is_highlight = False
    for idx, line in enumerate(lines):
        if line == """":
            continue  # empty line
        elif line.startswith(""@highlight""):
            next_is_highlight = True
        elif next_is_highlight:
            highlights.append(line)
        else:
            article_lines.append(line)
    article = "" "".join(article_lines)
    abstract = "" "".join(highlights)
    return article, abstract
","elif line . startswith ( ""@highlight"" ) :",194
"def _ordered_tag_specs(
    entity_tag_specs: Optional[List[EntityTagSpec]],
) -> List[EntityTagSpec]:
    """"""Ensure that order of entity tag specs matches CRF layer order.""""""
    if entity_tag_specs is None:
        return []
    crf_order = [
        ENTITY_ATTRIBUTE_TYPE,
        ENTITY_ATTRIBUTE_ROLE,
        ENTITY_ATTRIBUTE_GROUP,
    ]
    ordered_tag_spec = []
    for tag_name in crf_order:
        for tag_spec in entity_tag_specs:
            if tag_name == tag_spec.tag_name:
                ordered_tag_spec.append(tag_spec)
    return ordered_tag_spec
",if tag_name == tag_spec . tag_name :,177
"def checkDrag(self, root, target):
    """"""Return False if target is any descendant of root.""""""
    c = self
    message = ""Can not drag a node into its descendant tree.""
    for z in root.subtree():
        if z == target:
            if g.app.unitTesting:
                g.app.unitTestDict[""checkMoveWithParentWithWarning""] = True
            else:
                c.alert(message)
            return False
    return True
",if z == target :,122
"def get_adapter(self, pattern=None):
    adapters = self.get_adapters()
    if pattern is None:
        if len(adapters):
            return adapters[0]
        else:
            raise DBusNoSuchAdapterError(""No adapter(s) found"")
    else:
        for adapter in adapters:
            path = adapter.get_object_path()
            if path.endswith(pattern) or adapter[""Address""] == pattern:
                return adapter
        raise DBusNoSuchAdapterError(""No adapters found with pattern: %s"" % pattern)
",if len ( adapters ) :,144
"def __init__(self, children, quiet_exceptions=()):
    self.keys = None
    if isinstance(children, dict):
        self.keys = list(children.keys())
        children = children.values()
    self.children = []
    for i in children:
        if not isinstance(i, YieldPoint):
            i = convert_yielded(i)
        if is_future(i):
            i = YieldFuture(i)
        self.children.append(i)
    assert all(isinstance(i, YieldPoint) for i in self.children)
    self.unfinished_children = set(self.children)
    self.quiet_exceptions = quiet_exceptions
",if is_future ( i ) :,166
"def _make_callback(self):
    callback = self.callback
    for plugin in self.all_plugins():
        try:
            if hasattr(plugin, ""apply""):
                callback = plugin.apply(callback, self)
            else:
                callback = plugin(callback)
        except RouteReset:  # Try again with changed configuration.
            return self._make_callback()
        if not callback is self.callback:
            update_wrapper(callback, self.callback)
    return callback
",if not callback is self . callback :,131
"def _check_conflict(func, other_funcs):
    if steps[func]:
        for other_func in other_funcs:
            if steps[other_func] and other_func != func:
                raise ValueError(""Can't specify both %s and %s"" % (func, other_func))
",if steps [ other_func ] and other_func != func :,76
"def shutdown(self, cleanup=True):
    super(LocalDistributedRunner, self).shutdown()
    global _dummy_cpu_actor
    global _dummy_cuda_actor
    if cleanup:
        if _dummy_cpu_actor or _dummy_cuda_actor:
            assert not self.is_actor(), ""Actor shouldn't have a "" ""dummy actor.""
        if _dummy_cpu_actor:
            ray.kill(_dummy_cpu_actor)
        if _dummy_cuda_actor:
            ray.kill(_dummy_cuda_actor)
        _dummy_cpu_actor = None
        _dummy_cuda_actor = None
",if _dummy_cpu_actor :,158
"def _publish(self, data):
    retry = True
    while True:
        try:
            if not retry:
                self._redis_connect()
            return self.redis.publish(self.channel, pickle.dumps(data))
        except redis.exceptions.ConnectionError:
            if retry:
                logger.error(""Cannot publish to redis... retrying"")
                retry = False
            else:
                logger.error(""Cannot publish to redis... giving up"")
                break
",if retry :,134
"def simulate_policy(args):
    data = torch.load(args.file)
    policy = data[""evaluation/policy""]
    env = data[""evaluation/env""]
    print(""Policy loaded"")
    if args.gpu:
        set_gpu_mode(True)
        policy.cuda()
    while True:
        path = rollout(
            env,
            policy,
            max_path_length=args.H,
            render=True,
        )
        if hasattr(env, ""log_diagnostics""):
            env.log_diagnostics([path])
        logger.dump_tabular()
","if hasattr ( env , ""log_diagnostics"" ) :",160
"def get_bucket_latest_versions(self, bucket_name):
    versions = self.get_bucket_versions(bucket_name)
    latest_modified_per_key = {}
    latest_versions = {}
    for version in versions:
        name = version.name
        last_modified = version.last_modified
        version_id = version.version_id
        latest_modified_per_key[name] = max(
            last_modified, latest_modified_per_key.get(name, datetime.datetime.min)
        )
        if last_modified == latest_modified_per_key[name]:
            latest_versions[name] = version_id
    return latest_versions
",if last_modified == latest_modified_per_key [ name ] :,173
"def _get_ntp_entity(self, peer_type):
    ntp_entities = {}
    command = ""show ntp peers""
    ntp_peers_table = self._get_command_table(command, ""TABLE_peers"", ""ROW_peers"")
    for ntp_peer in ntp_peers_table:
        if ntp_peer.get(""serv_peer"", """").strip() != peer_type:
            continue
        peer_addr = napalm.base.helpers.ip(ntp_peer.get(""PeerIPAddress"").strip())
        ntp_entities[peer_addr] = {}
    return ntp_entities
","if ntp_peer . get ( ""serv_peer"" , """" ) . strip ( ) != peer_type :",164
"def kaiming_init(
    module, a=0, mode=""fan_out"", nonlinearity=""relu"", bias=0, distribution=""normal""
):
    assert distribution in [""uniform"", ""normal""]
    if hasattr(module, ""weight"") and module.weight is not None:
        if distribution == ""uniform"":
            nn.init.kaiming_uniform_(
                module.weight, a=a, mode=mode, nonlinearity=nonlinearity
            )
        else:
            nn.init.kaiming_normal_(
                module.weight, a=a, mode=mode, nonlinearity=nonlinearity
            )
    if hasattr(module, ""bias"") and module.bias is not None:
        nn.init.constant_(module.bias, bias)
","if distribution == ""uniform"" :",192
"def _get_arguments(
    self, name: str, source: Dict[str, List[bytes]], strip: bool = True
) -> List[str]:
    values = []
    for v in source.get(name, []):
        s = self.decode_argument(v, name=name)
        if isinstance(s, unicode_type):
            # Get rid of any weird control chars (unless decoding gave
            # us bytes, in which case leave it alone)
            s = RequestHandler._remove_control_chars_regex.sub("" "", s)
        if strip:
            s = s.strip()
        values.append(s)
    return values
","if isinstance ( s , unicode_type ) :",164
"def __str__(self):
    s = ""{""
    sep = """"
    for k, v in self.iteritems():
        s += sep
        if type(k) == str:
            s += ""'%s'"" % k
        else:
            s += str(k)
        s += "": ""
        if type(v) == str:
            s += ""'%s'"" % v
        else:
            s += str(v)
        sep = "", ""
    s += ""}""
    return s
",if type ( v ) == str :,131
"def contains(self, other_route):
    if isinstance(other_route, list):
        return self.to_list()[0 : len(other_route)] == other_route
    # This only works before merging
    assert len(other_route.outgoing) <= 1, ""contains(..) cannot be called after a merge""
    assert len(self.outgoing) <= 1, ""contains(..) cannot be called after a merge""
    if other_route.task_spec == self.task_spec:
        if other_route.outgoing and self.outgoing:
            return self.outgoing[0].contains(other_route.outgoing[0])
        elif self.outgoing:
            return True
        elif not other_route.outgoing:
            return True
    return False
",elif self . outgoing :,184
"def iter_help(cls):
    for variable_name, value in sorted(cls.__dict__.items()):
        if not variable_name.startswith(""PEX_""):
            continue
        variable_type, variable_text = cls.process_pydoc(getattr(value, ""__doc__""))
        yield variable_name, variable_type, variable_text
","if not variable_name . startswith ( ""PEX_"" ) :",83
"def _clean_dict(json_dict):
    for key, value in json_dict.items():
        if isinstance(value, list):
            json_dict[key] = list(OrderedSet(map(_clean_string, value)))
        elif isinstance(value, dict):
            json_dict[key] = _clean_dict(value)
    return OrderedDict(filter(lambda x: x[1], json_dict.items()))
","elif isinstance ( value , dict ) :",105
"def _createdir(self):
    if not os.path.exists(self._dir):
        try:
            os.makedirs(self._dir, 0o700)
        except OSError as e:
            if e.errno != errno.EEXIST:
                raise EnvironmentError(
                    ""Cache directory '%s' does not exist ""
                    ""and could not be created'"" % self._dir
                )
",if e . errno != errno . EEXIST :,111
"def JobWait(self, waiter):
    # type: (Waiter) -> wait_status_t
    # wait builtin can be interrupted
    while True:
        # Don't retry
        result = waiter.WaitForOne(False)
        if result > 0:  # signal
            return wait_status.Cancelled(result)
        if result == -1:  # nothing to wait for
            break
        if self.state != job_state_e.Running:
            break
    return wait_status.Proc(self.status)
",if result > 0 :,135
"def _deserialize_pickle5_data(self, data):
    try:
        in_band, buffers = unpack_pickle5_buffers(data)
        if len(buffers) > 0:
            obj = pickle.loads(in_band, buffers=buffers)
        else:
            obj = pickle.loads(in_band)
    # cloudpickle does not provide error types
    except pickle.pickle.PicklingError:
        raise DeserializationError()
    return obj
",if len ( buffers ) > 0 :,115
"def svgGetPaths(svgCode):
    doc = xmlparseString(svgCode)
    svg = doc.documentElement
    paths = findPathNodes(svg)
    isFigmaSVG = svgCode.find(""Figma</desc>"") != -1
    if len(paths) == 0:
        return paths, (0, 0)
    paths2 = []
    for path in paths:
        id = path.getAttribute(""id"")
        if not isFigmaSVG or (id is None or id.find(""stroke"") == -1):
            tr = nodeTranslation(path)
            d = path.getAttribute(""d"")
            paths2.append((d, tr))
    return paths2, isFigmaSVG
","if not isFigmaSVG or ( id is None or id . find ( ""stroke"" ) == - 1 ) :",179
"def get_track_id_from_json(item):
    """"""Try to extract video Id from various response types""""""
    fields = [
        ""contentDetails/videoId"",
        ""snippet/resourceId/videoId"",
        ""id/videoId"",
        ""id"",
    ]
    for field in fields:
        node = item
        for p in field.split(""/""):
            if node and isinstance(node, dict):
                node = node.get(p)
        if node:
            return node
    return """"
","if node and isinstance ( node , dict ) :",137
"def save(self):
    self._idx_lock.acquire()
    try:
        if self._is_idx_dirty:
            if not exists(self.base_dir):
                self._mk_dbdir()
            self.db.save_pickle(
                join(self.base_dir, ""dirs_from_basename""), self._dirs_from_basename
            )
            self._is_idx_dirty = False
    finally:
        self._idx_lock.release()
",if not exists ( self . base_dir ) :,130
"def _init_from_response(self, response):
    self.id = response[""id""]
    self.uri = response.get(""mongodb_auth_uri"", response[""mongodb_uri""])
    for member in response[""members""]:
        if member[""state""] == 1:
            self.primary = Server(member[""server_id""], member[""host""])
        elif member[""state""] == 2:
            self.secondary = Server(member[""server_id""], member[""host""])
    return self
","elif member [ ""state"" ] == 2 :",120
"def verify_secret_key(request):
    ""Verifies secret key for a request""
    if request.user.username:
        # always allow authenticated users
        return True
    else:
        key = request.GET[""secret""]
        user_id, secret = key.split(""."", 1)
        try:
            profile = User.objects.get(pk=user_id)
        except:
            return False
        if key == get_secret_key(request, profile):
            request.user = profile.user
            return True
    return False
","if key == get_secret_key ( request , profile ) :",144
"def compute(self, split):
    rd = random.Random(self.seed + split.index)
    if self.withReplacement:
        olddata = list(self.prev.iterator(split))
        sampleSize = int(math.ceil(len(olddata) * self.frac))
        for i in range(sampleSize):
            yield rd.choice(olddata)
    else:
        for i in self.prev.iterator(split):
            if rd.random() <= self.frac:
                yield i
",if rd . random ( ) <= self . frac :,133
"def splitIntoWords(name):
    wordlist = []
    wordstart = 0
    l = len(name)
    for i in range(l):
        c = name[i]
        n = None
        if c == "" "" or c == ""-"":
            n = name[wordstart:i]
        elif i == l - 1:
            n = name[wordstart : i + 1]
        if n:
            wordstart = i
            if c == ""-"" and n != """":
                n += ""-""
            if c == "" "" or c == ""-"":
                wordstart = i + 1
            wordlist.append(n)
    return wordlist
",elif i == l - 1 :,174
"def check_file(f, path):
    if not (ignore_substring and ignore_substring in f):
        if substring in f:
            compl_path = os.path.join(path, f)
            if os.path.isfile(compl_path):
                return compl_path
    return False
",if substring in f :,83
"def keyPressEvent(self, event):
    """"""Add up and down arrow key events to built in functionality.""""""
    keyPressed = event.key()
    if keyPressed in [Constants.UP_KEY, Constants.DOWN_KEY, Constants.TAB_KEY]:
        if keyPressed == Constants.UP_KEY:
            self.index = max(0, self.index - 1)
        elif keyPressed == Constants.DOWN_KEY:
            self.index = min(len(self.completerStrings) - 1, self.index + 1)
        elif keyPressed == Constants.TAB_KEY and self.completerStrings:
            self.tabPressed()
        if self.completerStrings:
            self.setTextToCompleterIndex()
    super(CueLineEdit, self).keyPressEvent(event)
",elif keyPressed == Constants . DOWN_KEY :,192
"def _get_disk_size(cls, path, ignored=None):
    if ignored is None:
        ignored = []
    if path in ignored:
        return 0
    total = 0
    for entry in scandir(path):
        if entry.is_dir():
            total += cls._get_disk_size(entry.path, ignored=ignored)
        elif entry.is_file():
            total += entry.stat().st_size
    return total
",if entry . is_dir ( ) :,117
"def _handle_rate_limit(
    self, exception: RedditAPIException
) -> Optional[Union[int, float]]:
    for item in exception.items:
        if item.error_type == ""RATELIMIT"":
            amount_search = self._ratelimit_regex.search(item.message)
            if not amount_search:
                break
            seconds = int(amount_search.group(1))
            if ""minute"" in amount_search.group(2):
                seconds *= 60
            if seconds <= int(self.config.ratelimit_seconds):
                sleep_seconds = seconds + min(seconds / 10, 1)
                return sleep_seconds
    return None
",if seconds <= int ( self . config . ratelimit_seconds ) :,181
"def validate(self):
    try:
        f = int(eval(self.setting.getValue(), {}, {}))
        if self.minValue is not None and f < self.minValue:
            return ERROR, ""This setting should not be below "" + str(self.minValue)
        if self.maxValue is not None and f > self.maxValue:
            return ERROR, ""This setting should not be above "" + str(self.maxValue)
        return SUCCESS, """"
    except (ValueError, SyntaxError, TypeError, NameError):
        return (
            ERROR,
            '""'
            + str(self.setting.getValue())
            + '"" is not a valid whole number or expression',
        )
",if self . minValue is not None and f < self . minValue :,180
"def rename(self, remote_name, new_remote_name):
    remotes = self.load_remotes()
    remotes.rename(remote_name, new_remote_name)
    with self._cache.editable_packages.disable_editables():
        for ref in self._cache.all_refs():
            with self._cache.package_layout(ref).update_metadata() as metadata:
                if metadata.recipe.remote == remote_name:
                    metadata.recipe.remote = new_remote_name
                for pkg_metadata in metadata.packages.values():
                    if pkg_metadata.remote == remote_name:
                        pkg_metadata.remote = new_remote_name
        remotes.save(self._filename)
",if metadata . recipe . remote == remote_name :,195
"def _convert_idx(self, idx):
    graph_idx = 0
    node_idx = idx
    for i in range(len(self.graphs)):
        if node_idx < self.graphs[i].number_of_nodes():
            graph_idx = i
            break
        else:
            node_idx -= self.graphs[i].number_of_nodes()
    return graph_idx, node_idx
",if node_idx < self . graphs [ i ] . number_of_nodes ( ) :,107
"def emit_pre_migrate_signal(verbosity, interactive, db, **kwargs):
    # Emit the pre_migrate signal for every application.
    for app_config in apps.get_app_configs():
        if app_config.models_module is None:
            continue
        if verbosity >= 2:
            print(""Running pre-migrate handlers for application %s"" % app_config.label)
        models.signals.pre_migrate.send(
            sender=app_config,
            app_config=app_config,
            verbosity=verbosity,
            interactive=interactive,
            using=db,
            **kwargs
        )
",if app_config . models_module is None :,166
"def slice(self, slice):
    gridscope = GridScope(globals=self.globals)
    for key in self.user_added:
        value = self[key]
        if isinstance(value, np.ndarray):
            grid = value
            sliced = np.sum(grid[slice, ...], axis=0)
            logger.debug(""sliced %s from %r to %r"", key, grid.shape, sliced.shape)
            gridscope[key] = sliced
        else:
            gridscope[key] = value
    return gridscope
","if isinstance ( value , np . ndarray ) :",140
"def get_last_tagged(self):
    if not self.last_tagged:
        last = datetime(1970, 1, 1)
        for tag in self.tags:
            if tag.last_seen > last:
                last = tag.last_seen
        self.update(set__last_tagged=last)
        return last
    else:
        return self.last_tagged
",if tag . last_seen > last :,101
"def recalculate_user_disk_usage(app, **kwargs):
    user_id = kwargs.get(""user_id"", None)
    sa_session = app.model.context
    if user_id:
        user = sa_session.query(app.model.User).get(app.security.decode_id(user_id))
        if user:
            user.calculate_and_set_disk_usage()
        else:
            log.error(
                ""Recalculate user disk usage task failed, user %s not found"" % user_id
            )
    else:
        log.error(""Recalculate user disk usage task received without user_id."")
",if user :,165
"def log_items(self, interface, action, media, items):
    if not items:
        return
        # Log each item
    for item in items:
        if not item:
            continue
        log.info(
            ""[%s:%s](%s) %r (%r)"",
            interface,
            action,
            media,
            item.get(""title""),
            item.get(""year""),
        )
        if media == ""shows"":
            # Log each episode
            self.log_episodes(item)
",if not item :,150
"def test_unbiased_coin_has_no_second_order():
    counts = Counter()
    for i in range(256):
        buf = bytes([i])
        data = ConjectureData.for_buffer(buf)
        result = cu.biased_coin(data, 0.5)
        if data.buffer == buf:
            counts[result] += 1
    assert counts[False] == counts[True] > 0
",if data . buffer == buf :,111
"def gettempfilename(suffix):
    """"""Returns a temporary filename""""""
    if ""_"" in os.environ:
        # tempfile.mktemp() crashes on some Wine versions (the one of Ubuntu 12.04 particularly)
        if os.environ[""_""].find(""wine"") >= 0:
            tmpdir = "".""
            if ""TMP"" in os.environ:
                tmpdir = os.environ[""TMP""]
            import time
            import random
            random.seed(time.time())
            random_part = ""file%d"" % random.randint(0, 1000000000)
            return os.path.join(tmpdir, random_part + suffix)
    return tempfile.mktemp(suffix)
","if os . environ [ ""_"" ] . find ( ""wine"" ) >= 0 :",172
"def _get_functionapp_runtime_language(
    self, app_settings
):  # pylint: disable=no-self-use
    functions_worker_runtime = [
        setting[""value""]
        for setting in app_settings
        if setting[""name""] == ""FUNCTIONS_WORKER_RUNTIME""
    ]
    if functions_worker_runtime:
        functionapp_language = functions_worker_runtime[0]
        if SUPPORTED_LANGUAGES.get(functionapp_language) is not None:
            return SUPPORTED_LANGUAGES[functionapp_language]
        raise LanguageNotSupportException(functionapp_language)
    return None
",if SUPPORTED_LANGUAGES . get ( functionapp_language ) is not None :,151
"def seek(self, offset, whence=io.SEEK_SET):
    if self.mode == WRITE:
        if whence != io.SEEK_SET:
            if whence == io.SEEK_CUR:
                offset = self.offset + offset
            else:
                raise ValueError(""Seek from end not supported"")
        if offset < self.offset:
            raise OSError(""Negative seek in write mode"")
        count = offset - self.offset
        chunk = bytes(1024)
        for i in range(count // 1024):
            self.write(chunk)
        self.write(bytes(count % 1024))
    elif self.mode == READ:
        self._check_not_closed()
        return self._buffer.seek(offset, whence)
    return self.offset
",if offset < self . offset :,199
"def stop(self):
    """"""Stop the HTTP server.""""""
    if self.running:
        # stop() MUST block until the server is *truly* stopped.
        self.httpserver.stop()
        # Wait for the socket to be truly freed.
        if isinstance(self.bind_addr, tuple):
            portend.free(*self.bound_addr, timeout=Timeouts.free)
        self.running = False
        self.bus.log(""HTTP Server %s shut down"" % self.httpserver)
    else:
        self.bus.log(""HTTP Server %s already shut down"" % self.httpserver)
","if isinstance ( self . bind_addr , tuple ) :",156
"def dump_json(testcase, json_file):
    """"""dump HAR entries to json testcase""""""
    logger.info(""dump testcase to JSON format."")
    with open(json_file, ""w"", encoding=""utf-8"") as outfile:
        my_json_str = json.dumps(testcase, ensure_ascii=False, indent=4)
        if isinstance(my_json_str, bytes):
            my_json_str = my_json_str.decode(""utf-8"")
        outfile.write(my_json_str)
    logger.info(""Generate JSON testcase successfully: {}"".format(json_file))
","if isinstance ( my_json_str , bytes ) :",148
"def find_comment(line):
    """"""Finds the index of a comment # and returns None if not found""""""
    instring, instring_char = False, """"
    for i, char in enumerate(line):
        if char in ('""', ""'""):
            if instring:
                if char == instring_char:
                    instring = False
                    instring_char = """"
            else:
                instring = True
                instring_char = char
        elif char == ""#"":
            if not instring:
                return i
    return None
",if char == instring_char :,155
"def _requests_to_follow(self, response):
    if not isinstance(response, HtmlResponse):
        return
    seen = set()
    for n, rule in enumerate(self._rules):
        links = [
            lnk
            for lnk in rule.link_extractor.extract_links(response)
            if lnk not in seen
        ]
        if links and rule.process_links:
            links = rule.process_links(links)
        for link in links:
            seen.add(link)
            request = self._build_request(n, link)
            yield rule._process_request(request, response)
",if links and rule . process_links :,168
"def _process_iter(self, line_iter):
    samples = []
    buf = []
    for line in line_iter:
        if not buf and line.startswith(""#"") and self._has_comment:
            continue
        line = line.split()
        if line:
            buf.append(line)
        elif buf:
            samples.append(tuple(map(list, zip(*buf))))
            buf = []
    if buf:
        samples.append(tuple(map(list, zip(*buf))))
    return samples
",if line :,137
"def _set_input_expanded(self, inp, expand, scroll=True):
    getobj = self._builder.get_object
    arrow = getobj(""by%s_expander_arrow"" % (inp.name,))
    grid = getobj(""by%s_curve_grid"" % (inp.name,))
    if expand:
        arrow.set_property(""arrow-type"", Gtk.ArrowType.DOWN)
        grid.show_all()
        if scroll:
            GLib.idle_add(self._scroll_setting_editor, grid)
    else:
        arrow.set_property(""arrow-type"", Gtk.ArrowType.RIGHT)
        grid.hide()
",if scroll :,164
"def extract_groups(self, text: str, language_code: str):
    previous = None
    group = 1
    groups = []
    words = []
    ignored = IGNORES.get(language_code, {})
    for word in NON_WORD.split(text):
        if not word:
            continue
        if word not in ignored and len(word) >= 2:
            if previous == word:
                group += 1
            elif group > 1:
                groups.append(group)
                words.append(previous)
                group = 1
        previous = word
    if group > 1:
        groups.append(group)
        words.append(previous)
    return groups, words
",elif group > 1 :,187
"def add_field_to_csv_file(fieldName, fieldNameMap, fieldsList, fieldsTitles, titles):
    for ftList in fieldNameMap[fieldName]:
        if ftList not in fieldsTitles:
            fieldsList.append(ftList)
            fieldsTitles[ftList] = ftList
            add_titles_to_csv_file([ftList], titles)
",if ftList not in fieldsTitles :,96
"def get_transform(self, img):
    check_dtype(img)
    assert img.ndim in [2, 3], img.ndim
    from .transform import LazyTransform, TransformList
    # The next augmentor requires the previous one to finish.
    # So we have to use LazyTransform
    tfms = []
    for idx, a in enumerate(self.augmentors):
        if idx == 0:
            t = a.get_transform(img)
        else:
            t = LazyTransform(a.get_transform)
        if isinstance(t, TransformList):
            tfms.extend(t.tfms)
        else:
            tfms.append(t)
    return TransformList(tfms)
","if isinstance ( t , TransformList ) :",180
"def __init__(self, template, context, body_stream=None):
    if body_stream is None:
        if context.environment.is_async:
            raise RuntimeError(
                ""Async mode requires a body stream ""
                ""to be passed to a template module.  Use ""
                ""the async methods of the API you are ""
                ""using.""
            )
        body_stream = list(template.root_render_func(context))
    self._body_stream = body_stream
    self.__dict__.update(context.get_exported())
    self.__name__ = template.name
",if context . environment . is_async :,157
"def url_locations(urls, faker=False):
    locations = []
    for url in urls:
        if faker:
            response = request.urlopen(request.Request(url, headers=fake_headers), None)
        else:
            response = request.urlopen(request.Request(url))
        locations.append(response.url)
    return locations
",if faker :,90
"def wait_services_ready(selectors, min_counts, count_fun, timeout=None):
    readies = [0] * len(selectors)
    start_time = time.time()
    while True:
        all_satisfy = True
        for idx, selector in enumerate(selectors):
            if readies[idx] < min_counts[idx]:
                all_satisfy = False
                readies[idx] = count_fun(selector)
                break
        if all_satisfy:
            break
        if timeout and timeout + start_time < time.time():
            raise TimeoutError(""Wait cluster start timeout"")
        time.sleep(1)
",if readies [ idx ] < min_counts [ idx ] :,167
"def sanitize_args(a):
    try:
        args, kwargs = a
        if isinstance(args, tuple) and isinstance(kwargs, dict):
            return args, dict(kwargs)
    except (TypeError, ValueError):
        args, kwargs = (), {}
    if a is not None:
        if isinstance(a, dict):
            args = tuple()
            kwargs = a
        elif isinstance(a, tuple):
            if isinstance(a[-1], dict):
                args, kwargs = a[0:-1], a[-1]
            else:
                args = a
                kwargs = {}
    return args, kwargs
","if isinstance ( a [ - 1 ] , dict ) :",168
"def _override_options(options, **overrides):
    """"""Override options.""""""
    for opt, val in overrides.items():
        passed_value = getattr(options, opt, _Default())
        if opt in (""ignore"", ""select"") and passed_value:
            value = process_value(opt, passed_value.value)
            value += process_value(opt, val)
            setattr(options, opt, value)
        elif isinstance(passed_value, _Default):
            setattr(options, opt, process_value(opt, val))
","elif isinstance ( passed_value , _Default ) :",137
"def get_first_file_by_stem(dir_path, stem, exts=None):
    dir_path = Path(dir_path)
    stem = stem.lower()
    if dir_path.exists():
        for x in sorted(list(scandir(str(dir_path))), key=lambda x: x.name):
            if not x.is_file():
                continue
            xp = Path(x.path)
            if xp.stem.lower() == stem and (exts is None or xp.suffix.lower() in exts):
                return xp
    return None
",if not x . is_file ( ) :,148
"def testShortCircuit(self):
    """"""Test that creation short-circuits to reuse existing references""""""
    sd = {}
    for s in self.ss:
        sd[s] = 1
    for t in self.ts:
        if hasattr(t, ""x""):
            self.assert_(sd.has_key(safeRef(t.x)))
            self.assert_(safeRef(t.x) in sd)
        else:
            self.assert_(sd.has_key(safeRef(t)))
            self.assert_(safeRef(t) in sd)
","if hasattr ( t , ""x"" ) :",146
"def _gen_Less(self, args, ret_type):
    result = []
    for lhs, rhs in pairwise(args):
        if ret_type == real_type:
            result.append(self.builder.fcmp_ordered(""<"", lhs, rhs))
        elif ret_type == int_type:
            result.append(self.builder.icmp_signed(""<"", lhs, rhs))
        else:
            raise CompileError()
    return reduce(self.builder.and_, result)
",elif ret_type == int_type :,120
"def _resolve_aliases(tasks_or_files):
    for task_or_file in tasks_or_files:
        if isinstance(task_or_file, Alias):
            for t_or_f in _resolve_aliases(task_or_file.deps):
                yield t_or_f
        else:
            yield task_or_file
","if isinstance ( task_or_file , Alias ) :",92
"def report(properties):
    for name, value in properties:
        if name.startswith(""hypothesis-statistics-""):
            if hasattr(value, ""uniobj""):
                # Under old versions of pytest, `value` was a `py.xml.raw`
                # rather than a string, so we get the (unicode) string off it.
                value = value.uniobj
            line = base64.b64decode(value.encode()).decode() + ""\n\n""
            terminalreporter.write_line(line)
","if name . startswith ( ""hypothesis-statistics-"" ) :",135
"def throw_404(self, n):
    # bl_label of some nodes is edited by us, but those nodes do have docs ..
    _dirname = os.path.dirname(sverchok.__file__)
    path1 = os.path.join(_dirname, ""docs"", ""404.html"")
    path2 = os.path.join(_dirname, ""docs"", ""404_custom.html"")
    with open(path1) as origin:
        with open(path2, ""w"") as destination:
            for line in origin:
                if ""{{variable}}"" in line:
                    destination.write(line.replace(""{{variable}}"", n.bl_label))
                else:
                    destination.write(line)
    webbrowser.open(path2)
","if ""{{variable}}"" in line :",193
"def rm_empty_dirs(dirpath, interactive=False, dry_run=False):
    for name in os.listdir(dirpath):
        path = join(dirpath, name)
        if isdir(path):
            rm_empty_dirs(path, interactive, dry_run)
    if not os.listdir(dirpath):
        if interactive:
            raise NotImplementedError(""'-i' not implemented"")
        if dry_run:
            log.info(""rmdir `%s' (dry-run)"", dirpath)
        else:
            log.info(""rmdir `%s'"", dirpath)
            os.rmdir(dirpath)
",if isdir ( path ) :,153
"def get_run_cmd(submission_dir):
    """"""Get the language of a submission""""""
    with CD(submission_dir):
        if os.path.exists(""run.sh""):
            with open(""run.sh"") as f:
                for line in f:
                    if line[0] != ""#"":
                        return line.rstrip(""\r\n"")
","if os . path . exists ( ""run.sh"" ) :",98
"def _do_test_fetch_result(self, results, remote):
    # self._print_fetchhead(remote.repo)
    self.assertGreater(len(results), 0)
    self.assertIsInstance(results[0], FetchInfo)
    for info in results:
        self.assertIsInstance(info.note, string_types)
        if isinstance(info.ref, Reference):
            self.assertTrue(info.flags)
        # END reference type flags handling
        self.assertIsInstance(info.ref, (SymbolicReference, Reference))
        if info.flags & (info.FORCED_UPDATE | info.FAST_FORWARD):
            self.assertIsInstance(info.old_commit, Commit)
        else:
            self.assertIsNone(info.old_commit)
",if info . flags & ( info . FORCED_UPDATE | info . FAST_FORWARD ) :,186
"def __set__(self, instance, value):
    super().__set__(instance, value)
    value = instance._data[self.name]
    if value is not None:
        if isinstance(value, datetime.datetime):
            instance._data[self.name] = self._convert_from_datetime(value)
        else:
            instance._data[self.name] = value
","if isinstance ( value , datetime . datetime ) :",95
"def put(self, can_split=False):
    if isinstance(self.expr, NodeConst):
        if self.expr.is_str():  # 2007 May 01
            self.expr.put()
        else:
            self.line_more(""("")
            self.expr.put(can_split=True)
            self.line_more("")"")
    else:
        self.put_expr(self.expr, can_split=can_split)
    self.line_more(""."")
    self.line_more(NAME_SPACE.make_attr_name(self.expr, self.attrname))
    return self
",if self . expr . is_str ( ) :,157
"def get_location(self, dist, dependency_links):
    for url in dependency_links:
        egg_fragment = Link(url).egg_fragment
        if not egg_fragment:
            continue
        if ""-"" in egg_fragment:
            ## FIXME: will this work when a package has - in the name?
            key = ""-"".join(egg_fragment.split(""-"")[:-1]).lower()
        else:
            key = egg_fragment
        if key == dist.key:
            return url.split(""#"", 1)[0]
    return None
",if not egg_fragment :,141
"def _parse_lines(self, lines):
    for line in lines:
        self.size += len(line)
        words = line.strip().split(""\t"")
        if len(words) > 1:
            wset = set(words[1:])
            if words[0] in self.WORDS:
                self.WORDS[words[0]] |= wset
            else:
                self.WORDS[words[0]] = wset
",if words [ 0 ] in self . WORDS :,118
"def __call__(self, target):
    # normal running mode
    if not self.check_run_always:
        for algo in self.algos:
            if not algo(target):
                return False
        return True
    # run mode when at least one algo has a run_always attribute
    else:
        # store result in res
        # allows continuation to check for and run
        # algos that have run_always set to True
        res = True
        for algo in self.algos:
            if res:
                res = algo(target)
            elif hasattr(algo, ""run_always""):
                if algo.run_always:
                    algo(target)
        return res
",if res :,188
"def _cmd_flags_as_data(cmd_flags):
    data = {}
    for flag_name, cmd_flag in cmd_flags.items():
        cmd_flag_data = _cmd_flag_as_data(cmd_flag)
        if cmd_flag_data:
            data[flag_name] = cmd_flag_data
    return data
",if cmd_flag_data :,89
"def _csv_iterator(data_path, ngrams, yield_cls=False):
    tokenizer = get_tokenizer(""basic_english"")
    with io.open(data_path, encoding=""utf8"") as f:
        reader = unicode_csv_reader(f)
        for row in reader:
            tokens = "" "".join(row[1:])
            tokens = tokenizer(tokens)
            if yield_cls:
                yield int(row[0]) - 1, ngrams_iterator(tokens, ngrams)
            else:
                yield ngrams_iterator(tokens, ngrams)
",if yield_cls :,147
"def FindEnclosingBracketGroup(input_str):
    stack = []
    start = -1
    for index, char in enumerate(input_str):
        if char in LBRACKETS:
            stack.append(char)
            if start == -1:
                start = index
        elif char in BRACKETS:
            if not stack:
                return (-1, -1)
            if stack.pop() != BRACKETS[char]:
                return (-1, -1)
            if not stack:
                return (start, index + 1)
    return (-1, -1)
",if stack . pop ( ) != BRACKETS [ char ] :,163
"def get_and_set_be_comp(self):
    all_be_comp = []
    for page in self.pages:
        if page.relations.be_comp_norm is not None:
            all_be_comp.extend(page.relations.be_comp_norm)
        if page.relations.be_comp is not None:
            all_be_comp.extend(page.relations.be_comp)
    return set(all_be_comp)
",if page . relations . be_comp is not None :,117
"def iterload(self):
    delim = self.options.delimiter
    rowdelim = self.options.row_delimiter
    with self.source.open_text() as fp:
        with Progress(total=filesize(self.source)) as prog:
            for line in splitter(fp, rowdelim):
                if not line:
                    continue
                prog.addProgress(len(line))
                row = list(line.split(delim))
                if len(row) < self.nVisibleCols:
                    # extend rows that are missing entries
                    row.extend([None] * (self.nVisibleCols - len(row)))
                yield row
",if not line :,182
"def process_module(name, module, parent):
    if parent:
        modules[parent][""items""].append(name)
        mg = module_groups.setdefault(name, [])
        mg.append(parent)
        if get_module_type(name) == ""py3status"":
            module["".group""] = parent
    # check module content
    for k, v in list(module.items()):
        if k.startswith(""on_click""):
            # on_click event
            process_onclick(k, v, name)
            # on_click should not be passed to the module via the config.
            del module[k]
        if isinstance(v, ModuleDefinition):
            # we are a container
            module[""items""] = []
    return module
","if isinstance ( v , ModuleDefinition ) :",198
"def test_identify_accepts_space_separated_hosts(self):
    ru, iu = self.mock_all_identify()
    file_ip = open(tests.VALID_FILE_IP)
    for i, line in enumerate(file_ip):
        if i < 2:
            expected_url, expected_host = (""http://192.168.1.1/"", ""example.com"")
        elif i == 2:
            expected_url, expected_host = (""http://192.168.1.2/drupal/"", ""example.com"")
        identify_line(line)
        args, kwargs = ru.call_args_list[-1]
        self.assertEquals(args[0], expected_url)
        self.assertEquals(args[1], expected_host)
",if i < 2 :,195
"def get_version(module):
    for key in version_keys:
        if hasattr(module, key):
            version = getattr(module, key)
            if isinstance(version, types.ModuleType):
                version = get_version(version)
            return version
    return ""Unknown""
","if hasattr ( module , key ) :",77
"def whoami(self):
    """"""Return user relevant login information.""""""
    account_data = {}
    for k in (""email"", ""account_id""):
        value = self.conf.get(k)
        if not value:
            account_info = self.get_account_information()
            value = account_info.get(k, ""unknown"")
            self.conf.set(k, value)
            self.conf.save()
        account_data[k] = value
    return account_data
",if not value :,130
"def do(self):
    if self.in_class_scope():
        selected_str = self.view.substr(self.selected_region)
        for symbol in self.view.symbols():
            if symbol[1] == selected_str:
                self.view.sel().clear()
                self.view.sel().add(symbol[0])
                self.view.show(symbol[0])
                return
    # falls back to the original functionality
    self.window.run_command(""goto_definition"")
",if symbol [ 1 ] == selected_str :,136
"def __iter__(self):
    i = 0
    for category, filename in list(self.input_files.items()):
        for line in open(filename):
            line = self._clean_line(line)
            if self.accept_criteria(i):
                yield Opinion(line, category)
            i += 1
            if i % 1000 == 0:
                print(""\tReaded {} examples"".format(i))
",if self . accept_criteria ( i ) :,115
"def recvmsg(self, *args):
    while True:
        try:
            return self._sock.recvmsg(*args)
        except error as ex:
            if ex.args[0] != EWOULDBLOCK or self.timeout == 0.0:
                raise
        self._wait(self._read_event)
",if ex . args [ 0 ] != EWOULDBLOCK or self . timeout == 0.0 :,87
"def _get_editable_fields(cls):
    fds = set([])
    for field in cls._meta.concrete_fields:
        if hasattr(field, ""attname""):
            if field.attname == ""id"":
                continue
            elif field.attname.endswith(""ptr_id""):
                # polymorphic fields should always be non-editable, see:
                # https://github.com/django-polymorphic/django-polymorphic/issues/349
                continue
            if getattr(field, ""editable"", True):
                fds.add(field.attname)
    return fds
","if hasattr ( field , ""attname"" ) :",159
"def prepare_fields(all_fields, submit_fields, submit):
    if len(list(submit_fields.items(multi=True))) > 1:
        if not submit:
            raise exceptions.InvalidSubmitError()
        if submit not in submit_fields.getlist(submit.name):
            raise exceptions.InvalidSubmitError()
        return _filter_fields(
            all_fields, lambda f: not isinstance(f, fields.Submit) or f == submit
        )
    return all_fields
",if submit not in submit_fields . getlist ( submit . name ) :,123
"def tag_configure(self, *args, **keys):
    if len(args) == 1:
        key = args[0]
        self.tags[key] = keys
        val = keys.get(""foreground"")
        underline = keys.get(""underline"")
        if val:
            self.configDict[key] = val
        if underline:
            self.configUnderlineDict[key] = True
    else:
        g.trace(""oops"", args, keys)
",if val :,123
"def detect(get_page):
    retval = False
    for vector in WAF_ATTACK_VECTORS:
        page, headers, code = get_page(get=vector)
        retval = (
            code == 501
            and re.search(r""Reference #[0-9A-Fa-f.]+"", page, re.I) is not None
        )
        if retval:
            break
    return retval
",if retval :,112
"def refine_pointer_names_input(lines):
    """"""Return  a list of width_info_t. Skip comments and blank lines""""""
    global comment_pattern
    widths_list = []
    for line in lines:
        pline = comment_pattern.sub("""", line).strip()
        if pline == """":
            continue
        wrds = pline.split()
        ntokens = len(wrds)
        if ntokens == 3:
            (bbytes, name, suffix) = wrds
        else:
            die(""Bad number of tokens on line: "" + line)
        widths_list.append((bbytes, name, suffix))
    return widths_list
",if ntokens == 3 :,169
"def notify(title, message, retcode=None):
    """"""Sends message over Telegram using telegram-send, title is ignored.""""""
    if not path.exists(config_file):
        if not path.exists(config_dir):
            makedirs(config_dir)
        print(""Follow the instructions to configure the Telegram backend.\n"")
        configure(config_file)
    send(messages=[message], conf=config_file)
",if not path . exists ( config_dir ) :,104
"def find_on_path(targets):
    """"""Search the PATH for a program and return full path""""""
    if sabnzbd.WIN32:
        paths = os.getenv(""PATH"").split("";"")
    else:
        paths = os.getenv(""PATH"").split("":"")
    if isinstance(targets, str):
        targets = (targets,)
    for path in paths:
        for target in targets:
            target_path = os.path.abspath(os.path.join(path, target))
            if os.path.isfile(target_path) and os.access(target_path, os.X_OK):
                return target_path
    return None
","if os . path . isfile ( target_path ) and os . access ( target_path , os . X_OK ) :",164
"def test_name_attribute(self):
    for cons in self.hash_constructors:
        h = cons()
        self.assertIsInstance(h.name, str)
        if h.name in self.supported_hash_names:
            self.assertIn(h.name, self.supported_hash_names)
        else:
            self.assertNotIn(h.name, self.supported_hash_names)
        self.assertEqual(h.name, hashlib.new(h.name).name)
",if h . name in self . supported_hash_names :,124
"def find_marriage(database, family):
    """"""find the marriage of a family""""""
    for event_ref in family.get_event_ref_list():
        event = database.get_event_from_handle(event_ref.ref)
        if event and event.type.is_marriage() and event_ref.role.is_family():
            return event
    return None
",if event and event . type . is_marriage ( ) and event_ref . role . is_family ( ) :,96
"def test_find_ancestors(self):
    vhsblocks = self.config.parser_root.find_blocks(""VirtualHost"")
    macro_test = False
    nonmacro_test = False
    for vh in vhsblocks:
        if ""/macro/"" in vh.metadata[""augeaspath""].lower():
            ancs = vh.find_ancestors(""Macro"")
            self.assertEqual(len(ancs), 1)
            macro_test = True
        else:
            ancs = vh.find_ancestors(""Macro"")
            self.assertEqual(len(ancs), 0)
            nonmacro_test = True
    self.assertTrue(macro_test)
    self.assertTrue(nonmacro_test)
","if ""/macro/"" in vh . metadata [ ""augeaspath"" ] . lower ( ) :",183
"def readline(self):
    while 1:
        line = self._readline()
        if line:
            self._filelineno += 1
            return line
        if not self._file:
            return line
        self.nextfile()
",if line :,65
"def read_oclc(fields):
    if ""035"" not in fields:
        return {}
    found = []
    for line in fields[""035""]:
        for v in get_subfield_values(line, [""a""]):
            m = re_oclc.match(v)
            if not m:
                continue
            oclc = m.group(1)
            if oclc not in found:
                found.append(oclc)
    return {""oclc_number"": found} if found else {}
",if oclc not in found :,143
"def get_new_unlinked_nodes(
    before_inputted_nodes, before_input_sockets, input_sockets, nodes_dict
):
    affected_nodes = []
    for node_id, socket in zip(before_inputted_nodes, before_input_sockets):
        if not socket in input_sockets:
            # if the node has been deleted it is not affected
            if node_id in nodes_dict:
                if not node_id in affected_nodes:
                    affected_nodes.append(node_id)
    return affected_nodes
",if node_id in nodes_dict :,141
"def set_available_qty(self):
    for d in self.get(""required_items""):
        if d.source_warehouse:
            d.available_qty_at_source_warehouse = get_latest_stock_qty(
                d.item_code, d.source_warehouse
            )
        if self.wip_warehouse:
            d.available_qty_at_wip_warehouse = get_latest_stock_qty(
                d.item_code, self.wip_warehouse
            )
",if d . source_warehouse :,136
"def _unique_product_recursive(pools, result, i):
    if i >= len(pools):
        yield tuple(result)
        return
    for e in pools[i]:
        if e not in result:
            result[i] = e
            yield from _unique_product_recursive(pools, result, i + 1)
            result[i] = _SENTINEL
",if e not in result :,98
"def fileno(self):
    try:
        return self.sock.fileno()
    except socket.error:
        self.close()
        ex = sys.exc_info()[1]
        if get_exc_errno(ex) == errno.EBADF:
            raise EOFError()
        else:
            raise
",if get_exc_errno ( ex ) == errno . EBADF :,84
"def expand_block(self, feat):
    """"""Expand any blocks which are near the start or end of a contig.""""""
    chrom_end = self._ref_sizes.get(feat.chrom)
    if chrom_end:
        if feat.start < self._end_buffer:
            feat.start = 0
        if feat.stop >= chrom_end - self._end_buffer:
            feat.stop = chrom_end
    return feat
",if feat . stop >= chrom_end - self . _end_buffer :,114
"def prepare_parser(self, parser):
    docs = [self.parse_doc(doc) for doc in (self.doc, __doc__) if doc]
    for doc in docs:
        for long_opt, help in items(doc):
            option = parser._option_string_actions[long_opt]
            if option is not None:
                option.help = "" "".join(help).format(default=option.default)
    return parser
",if option is not None :,113
"def negate(monad):
    sql = monad.getsql()[0]
    translator = monad.translator
    if translator.dialect == ""Oracle"":
        result_sql = [""IS_NULL"", sql]
    else:
        result_sql = [""EQ"", sql, [""VALUE"", """"]]
        if monad.nullable:
            if isinstance(monad, AttrMonad):
                result_sql = [""OR"", result_sql, [""IS_NULL"", sql]]
            else:
                result_sql = [""EQ"", [""COALESCE"", sql, [""VALUE"", """"]], [""VALUE"", """"]]
    result = BoolExprMonad(result_sql, nullable=False)
    result.aggregated = monad.aggregated
    return result
","if isinstance ( monad , AttrMonad ) :",188
"def _ReadN(self, stdin_fd, n):
    # type: (int, int) -> str
    chunks = []  # type: List[str]
    bytes_left = n
    while bytes_left > 0:
        chunk = posix.read(stdin_fd, n)  # read at up to N chars
        if len(chunk) == 0:
            break
        chunks.append(chunk)
        bytes_left -= len(chunk)
    s = """".join(chunks)
    return s
",if len ( chunk ) == 0 :,127
"def instance_reader():
    for epoch_index in range(epoch):
        if shuffle:
            if shuffle_seed is not None:
                np.random.seed(shuffle_seed)
            np.random.shuffle(examples)
        if phase == ""train"":
            self.current_train_epoch = epoch_index
        for (index, example) in enumerate(examples):
            if phase == ""train"":
                self.current_train_example = index + 1
            feature = self.convert_example(
                index, example, self.get_labels(), self.max_seq_len, self.tokenizer
            )
            instance = self.generate_instance(feature)
            yield instance
",if shuffle_seed is not None :,189
"def close(self):
    fileobj = self.fileobj
    if fileobj is None:
        return
    self.fileobj = None
    try:
        if self.mode == WRITE:
            fileobj.write(self.compress.flush())
            write32u(fileobj, self.crc)
            # self.size may exceed 2GB, or even 4GB
            write32u(fileobj, self.size & 0xFFFFFFFF)
    finally:
        myfileobj = self.myfileobj
        if myfileobj:
            self.myfileobj = None
            myfileobj.close()
",if myfileobj :,147
"def rsa_public_key_parse(key_material):
    # These imports take ~.5s; let's keep them local
    import sshpubkeys.exceptions
    from sshpubkeys.keys import SSHKey
    try:
        if not isinstance(key_material, six.binary_type):
            key_material = key_material.encode(""ascii"")
        decoded_key = base64.b64decode(key_material).decode(""ascii"")
        public_key = SSHKey(decoded_key)
    except (sshpubkeys.exceptions.InvalidKeyException, UnicodeDecodeError):
        raise ValueError(""bad key"")
    if not public_key.rsa:
        raise ValueError(""bad key"")
    return public_key.rsa
","if not isinstance ( key_material , six . binary_type ) :",174
"def import_type(library, name):
    if library.name != idaapi.cvar.idati.name:
        last_ordinal = idaapi.get_ordinal_qty(idaapi.cvar.idati)
        type_id = idaapi.import_type(library, -1, name)  # tid_t
        if type_id != idaapi.BADORD:
            return last_ordinal
",if type_id != idaapi . BADORD :,106
"def OnDropFiles(self, x, y, files):
    filteredList = []
    if self.filenameFilter is not None:
        for f in files:
            for ext in self.filenameFilter:
                if f.endswith(ext) or f.endswith(ext.upper()):
                    filteredList.append(f)
    else:
        filteredList = files
    if len(filteredList) > 0:
        self.callback(filteredList)
",if f . endswith ( ext ) or f . endswith ( ext . upper ( ) ) :,117
"def _get_most_recent_update(self, versions):
    recent = None
    for version in versions:
        updated = datetime.datetime.strptime(version[""updated""], ""%Y-%m-%dT%H:%M:%SZ"")
        if not recent:
            recent = updated
        elif updated > recent:
            recent = updated
    return recent.strftime(""%Y-%m-%dT%H:%M:%SZ"")
",elif updated > recent :,103
"def __setstate__(self, servers_ids: List[str]):
    self.try_list = []
    for server_id in servers_ids:
        if server_id in sabnzbd.Downloader.server_dict:
            self.add_to_try_list(sabnzbd.Downloader.server_dict[server_id])
",if server_id in sabnzbd . Downloader . server_dict :,85
"def remove_command(self, command_id):
    for command in self.config[""commands""]:
        if command[EXECUTE_ID] == command_id:
            self.config[""commands""].remove(command)
            component.get(""EventManager"").emit(ExecuteCommandRemovedEvent(command_id))
            break
    self.config.save()
",if command [ EXECUTE_ID ] == command_id :,89
"def wrapper(*args, **kargs):
    offspring = func(*args, **kargs)
    for child in offspring:
        for i in xrange(len(child)):
            if child[i] > max:
                child[i] = max
            elif child[i] < min:
                child[i] = min
    return offspring
",if child [ i ] > max :,95
"def dispatch(self, request, *args, **kwargs):
    self.product = get_object_or_404(self.product_model, pk=kwargs[""product_pk""])
    # check permission to leave review
    if not self.product.is_review_permitted(request.user):
        if self.product.has_review_by(request.user):
            message = _(""You have already reviewed this product!"")
        else:
            message = _(""You can't leave a review for this product."")
        messages.warning(self.request, message)
        return redirect(self.product.get_absolute_url())
    return super().dispatch(request, *args, **kwargs)
",if self . product . has_review_by ( request . user ) :,167
"def PlayPause(self):
    state = self.graphManager.GetState(10)
    if state == 2:  # playing
        self.Pause()
    elif state == 1:  # paused
        self.Play()
    elif state == 0:  # stopped
        if (self.SelectedItem != None) and (self.filename != self.SelectedItem.Path):
            self.Stop()
            self.PlayingItem = self.SelectedItem
            self.LoadFile(self.SelectedItem.Path)
            self.Play()
        else:
            self.Play()
    else:
        pass  # for now just do nothing
    self.NotifyPropertyChanged(""IsPlaying"")
    self.NotifyPropertyChanged(""Duration"")
",if ( self . SelectedItem != None ) and ( self . filename != self . SelectedItem . Path ) :,188
"def decref(self, *keys):
    for tileable_key, tileable_id in keys:
        if tileable_key not in self._executed_tileables:
            continue
        _graph_key, ids = self._executed_tileables[tileable_key]
        if tileable_id in ids:
            ids.remove(tileable_id)
            # for those same key tileables, do decref only when all those tileables are garbage collected
            if len(ids) != 0:
                continue
            self.delete_data(tileable_key)
",if tileable_id in ids :,137
"def get_git_description(self):
    if self.is_a_git_repo():
        exit_code, stdout, stderr = execute_command_and_capture_output(
            ""git"", ""describe"", ""--always"", ""--tags"", ""--dirty""
        )
        if exit_code != 0:
            raise PyBuilderException(
                ""Cannot determine git description: git describe failed:\n{0}"".format(
                    stderr
                )
            )
        else:
            return stdout.strip()
    else:
        raise PyBuilderException(
            ""Cannot determine git description: project is not a git repo.""
        )
",if exit_code != 0 :,172
"def _code_for_module(self, module):
    text = '""%s"" [shape=ellips]' % module.name
    for item in list(module.items()):
        if isinstance(item, str):
            text += '\n""%s""' % item
            text += '\n""%s"" -> ""%s""' % (module.name, item)
        else:
            text += self._code_for_module(item)  # recurs
            text += '\n""%s"" -> ""%s""' % (module.name, item.name)
    return text
","if isinstance ( item , str ) :",140
"def test_images_p_is_stochastic_parameter(self):
    aug = self.create_aug(p=iap.Choice([0, 1], p=[0.7, 0.3]))
    seen = [0, 0]
    for _ in sm.xrange(1000):
        observed = aug.augment_image(self.image)
        if np.array_equal(observed, self.image):
            seen[0] += 1
        elif np.array_equal(observed, self.image_flipped):
            seen[1] += 1
        else:
            assert False
    assert np.allclose(seen, [700, 300], rtol=0, atol=75)
","if np . array_equal ( observed , self . image ) :",168
"def get_index_text(self, modname: str, name_cls: Tuple[str, str]) -> str:
    if self.objtype == ""function"":
        if not modname:
            return _(""%s() (built-in function)"") % name_cls[0]
        return _(""%s() (in module %s)"") % (name_cls[0], modname)
    elif self.objtype == ""data"":
        if not modname:
            return _(""%s (built-in variable)"") % name_cls[0]
        return _(""%s (in module %s)"") % (name_cls[0], modname)
    else:
        return """"
",if not modname :,162
"def _attributes_to_xml(self, xml_element, prefix_root, debug_context=None):
    del debug_context  # Unused.
    for attribute_name, attribute in six.iteritems(self._attributes):
        attribute_value = attribute.to_xml_string(prefix_root)
        if attribute_name == self._spec.identifier and attribute_value is None:
            xml_element.set(attribute_name, self.full_identifier)
        elif attribute_value is None:
            continue
        else:
            xml_element.set(attribute_name, attribute_value)
",elif attribute_value is None :,149
"def index_def(self):
    if self.index_def_ is None:
        self.lazy_init_lock_.acquire()
        try:
            if self.index_def_ is None:
                self.index_def_ = Index()
        finally:
            self.lazy_init_lock_.release()
    return self.index_def_
",if self . index_def_ is None :,94
"def _ord_to_str(ordinal, weights):
    """"""Reverse function of _str_to_ord.""""""
    chars = []
    for weight in weights:
        if ordinal == 0:
            return """".join(chars)
        ordinal -= 1
        index, ordinal = divmod(ordinal, weight)
        chars.append(_ALPHABET[index])
    return """".join(chars)
",if ordinal == 0 :,97
"def tip_texts(self):
    """"""Return the tip texts of the Toolbar (without window text)""""""
    texts = []
    for i in range(0, self.button_count()):
        # it works for MFC
        btn_tooltip_index = self.get_button_struct(i).iString
        # usually iString == -1 for separator
        # other cases if any
        if not (-1 <= btn_tooltip_index < self.get_tool_tips_control().tool_count()):
            btn_tooltip_index = i
        btn_text = self.get_tool_tips_control().get_tip_text(btn_tooltip_index + 1)
        texts.append(btn_text)
    return texts
",if not ( - 1 <= btn_tooltip_index < self . get_tool_tips_control ( ) . tool_count ( ) ) :,183
"def _initCaseSets(self):
    self._cs = {}
    self._css = {}
    for cs in self._caseSets:
        if not self._cs.has_key(cs.CaseSetName):
            self._cs[cs.CaseSetName] = {}
            self._css[cs.CaseSetName] = cs
        else:
            raise Exception(""duplicate case set name"")
        for c in cs.Cases:
            idx = tuple(c.index)
            if not self._cs[cs.CaseSetName].has_key(idx):
                self._cs[cs.CaseSetName][idx] = c
            else:
                raise Exception(""duplicate case index"")
",if not self . _cs [ cs . CaseSetName ] . has_key ( idx ) :,178
"def is_image(self, input):
    try:
        if isinstance(input, (np.ndarray, Image.Image)):
            return True
        elif isinstance(input, str):
            if not os.path.isfile(input):
                raise ValueError(""input must be a file"")
            img = Image.open(input)
            _ = img.size
            return True
        else:
            return False
    except:
        return False
","elif isinstance ( input , str ) :",122
"def __init__(self, opt, shared=None):
    super().__init__(opt, shared)
    if not shared:
        self.episodes = []
        self.num_exs = 0
        if opt.get(""parlaidialogteacher_datafile"") is not None:
            self._setup_data(opt.get(""parlaidialogteacher_datafile""))
    else:
        self.episodes = shared[""episodes""]
        self.num_exs = sum(len(e) for e in self.episodes)
    self.id = opt[""task""]
    self.reset()
","if opt . get ( ""parlaidialogteacher_datafile"" ) is not None :",152
"def draw(l, n, th=2):
    clear()
    l = l * f ** n
    shapesize(l / 100.0, l / 100.0, th)
    for k in tiledict:
        h, x, y = k
        setpos(x, y)
        setheading(h)
        if tiledict[k]:
            shape(""kite"")
            color(""black"", (0, 0.75, 0))
        else:
            shape(""dart"")
            color(""black"", (0.75, 0, 0))
        stamp()
",if tiledict [ k ] :,151
"def visit_Assign(self, node):
    if len(node.targets) == 1:
        if isinstance(node.targets[0], ast.Subscript):
            plugPath = self.__plugPath(self.__path(node.targets[0]))
            if plugPath:
                self.plugWrites.add(plugPath)
    self.visit(node.value)
",if plugPath :,93
"def StripTypeInfo(rendered_data):
    """"""Strips type information from rendered data. Useful for debugging.""""""
    if isinstance(rendered_data, (list, tuple)):
        return [StripTypeInfo(d) for d in rendered_data]
    elif isinstance(rendered_data, dict):
        if ""value"" in rendered_data and ""type"" in rendered_data:
            return StripTypeInfo(rendered_data[""value""])
        else:
            result = {}
            for k, v in rendered_data.items():
                result[k] = StripTypeInfo(v)
            return result
    else:
        return rendered_data
","if ""value"" in rendered_data and ""type"" in rendered_data :",159
"def _match_greater_than_or_equal(search_base, attribute, value, candidates):
    matches = list()
    for entry in candidates:
        dn = entry.get(""dn"")
        if not dn.endswith(search_base):
            continue
        value_from_directory = entry.get(""attributes"").get(attribute)
        if str(value_from_directory) >= str(value):
            entry[""type""] = ""searchResEntry""
            matches.append(entry)
    return matches
",if not dn . endswith ( search_base ) :,129
"def _get_changes(diff):
    """"""Get a list of changed versions from git.""""""
    changes_dict = {}
    for line in diff:
        if not line.startswith(""-"") and not line.startswith(""+""):
            continue
        if line.startswith(""+++ "") or line.startswith(""--- ""):
            continue
        name, version = parse_versioned_line(line[1:])
        if name not in changes_dict:
            changes_dict[name] = Change(name)
        if line.startswith(""-""):
            changes_dict[name].old = version
        elif line.startswith(""+""):
            changes_dict[name].new = version
    return [change for _name, change in sorted(changes_dict.items())]
",if name not in changes_dict :,181
"def append_row(tbody, cells):
    row = nodes.row()
    tbody += row
    for cell in cells:
        entry = nodes.entry()
        row += entry
        if isinstance(cell, six.text_type):
            node = nodes.paragraph(text=cell)
        else:
            node = cell
        entry += node
","if isinstance ( cell , six . text_type ) :",91
"def _testdata_to_is_unauthed_access_permitted(tests_config, node_type):
    res = []
    for x in tests_config[""endpoint_tests""]:
        if node_type not in x[""type""]:
            continue
        if ""is_unauthed_access_permitted"" not in x[""tests""]:
            continue
        h = x[""tests""][""is_unauthed_access_permitted""]
        for p in h[""locations""]:
            res.append((p, h.get(""vhost"", None)))
    return res
","if ""is_unauthed_access_permitted"" not in x [ ""tests"" ] :",139
"def process_ceph_status(output):
    res = patternchk.search(output)
    if not res:
        return {}
    ceph_stats = res.group()
    if not ceph_stats:
        return {}
    ret = {}
    rd = wr = iops = None
    rd = numberchk.search(ceph_stats)
    if rd is not None:
        ret[""rd""] = rd.group()
        wr = numberchk.search(ceph_stats, rd.end())
        if wr is not None:
            ret[""wr""] = wr.group()
            iops = numberchk.search(ceph_stats, wr.end())
            if iops is not None:
                ret[""iops""] = iops.group()
    return ret
",if iops is not None :,198
"def construct_type_storage_plugin_registry(pipeline_def, system_storage_def):
    # Needed to avoid circular dep
    from dagster.core.definitions import PipelineDefinition, SystemStorageDefinition
    check.inst_param(pipeline_def, ""pipeline_def"", PipelineDefinition)
    check.inst_param(system_storage_def, ""system_storage_def"", SystemStorageDefinition)
    type_plugins = []
    for type_obj in pipeline_def.all_runtime_types():
        for auto_plugin in type_obj.auto_plugins:
            if auto_plugin.compatible_with_storage_def(system_storage_def):
                type_plugins.append((type_obj, auto_plugin))
    return TypeStoragePluginRegistry(type_plugins)
",if auto_plugin . compatible_with_storage_def ( system_storage_def ) :,185
"def attr(**kw):
    kw = kw.items()
    kw.sort()
    parts = []
    for name, value in kw:
        if value is None:
            continue
        if name.endswith(""_""):
            name = name[:-1]
        parts.append('%s=""%s""' % (html_quote(name), html_quote(value)))
    return html("" "".join(parts))
",if value is None :,100
"def test_shape():
    from lasagne.init import Initializer
    # Assert that all `Initializer` sublasses return the shape that
    # we've asked for in `sample`:
    for klass in Initializer.__subclasses__():
        if len(klass.__subclasses__()):
            # check HeNormal, HeUniform, GlorotNormal, GlorotUniform
            for sub_klass in klass.__subclasses__():
                assert sub_klass().sample((12, 23)).shape == (12, 23)
        else:
            assert klass().sample((12, 23)).shape == (12, 23)
",if len ( klass . __subclasses__ ( ) ) :,144
"def __call__(self, data):
    num_points = data.pos.shape[0]
    new_data = Data()
    for key in data.keys:
        if key == KDTREE_KEY:
            continue
        item = data[key]
        if torch.is_tensor(item) and num_points == item.shape[0]:
            item = item[self._indices].clone()
        elif torch.is_tensor(item):
            item = item.clone()
        setattr(new_data, key, item)
    return new_data
",elif torch . is_tensor ( item ) :,144
"def vars(self):
    ret = []
    if op.disklist:
        varlist = op.disklist
    elif not op.full:
        varlist = (""total"",)
    else:
        varlist = []
        for name in self.discover:
            if self.diskfilter.match(name):
                continue
            varlist.append(name)
        #           if len(varlist) > 2: varlist = varlist[0:2]
        varlist.sort()
    for name in varlist:
        if name in self.discover + [""total""] or name in op.diskset:
            ret.append(name)
    return ret
",if self . diskfilter . match ( name ) :,182
"def _convertNbBytesinNbBits(self, nbBytes):
    nbMinBit = None
    nbMaxBit = None
    if nbBytes is not None:
        if isinstance(nbBytes, int):
            nbMinBit = nbBytes * 8
            nbMaxBit = nbMinBit
        else:
            if nbBytes[0] is not None:
                nbMinBit = nbBytes[0] * 8
            if nbBytes[1] is not None:
                nbMaxBit = nbBytes[1] * 8
    return (nbMinBit, nbMaxBit)
","if isinstance ( nbBytes , int ) :",149
"def after_test(self, results, tmp_dir):
    return_data = dict()
    if not results or not results.get(""data""):
        return return_data
    for filename in results[""data""]:
        if not has_ext(filename, "".log""):
            continue
        with open(filename, ""r"") as f:
            log_content = f.read()
        log_analyser.make_log_analyses(log_content, return_data)
    return return_data
","if not has_ext ( filename , "".log"" ) :",125
"def ensure_vm_was_torn_down():
    vm_labels = []
    for vm_ref in xenapi_fake.get_all(""VM""):
        vm_rec = xenapi_fake.get_record(""VM"", vm_ref)
        if not vm_rec[""is_control_domain""]:
            vm_labels.append(vm_rec[""name_label""])
    self.assertEquals(vm_labels, [""1""])
","if not vm_rec [ ""is_control_domain"" ] :",110
"def spool_print(*args, **kwargs):
    with _print_lock:
        if framework.Framework._spool:
            framework.Framework._spool.write(f""{args[0]}{os.linesep}"")
            framework.Framework._spool.flush()
        # disable terminal output for server jobs
        if framework.Framework._mode == Mode.JOB:
            return
        # new print function must still use the old print function via the backup
        builtins._print(*args, **kwargs)
",if framework . Framework . _mode == Mode . JOB :,126
"def _parse_lines(self, linesource):
    """"""Parse lines of text for functions and classes""""""
    functions = []
    classes = []
    for line in linesource:
        if line.startswith(""def "") and line.count(""(""):
            # exclude private stuff
            name = self._get_object_name(line)
            if not name.startswith(""_""):
                functions.append(name)
        elif line.startswith(""class ""):
            # exclude private stuff
            name = self._get_object_name(line)
            if not name.startswith(""_""):
                classes.append(name)
        else:
            pass
    functions.sort()
    classes.sort()
    return functions, classes
","if line . startswith ( ""def "" ) and line . count ( ""("" ) :",185
"def test_connect_using_sslcontext_verified(self):
    with support.transient_internet(self.testServer):
        can_verify = check_ssl_verifiy(self.testServer, self.remotePort)
        if not can_verify:
            self.skipTest(""SSL certificate can't be verified"")
    support.get_attribute(smtplib, ""SMTP_SSL"")
    context = ssl.create_default_context()
    with support.transient_internet(self.testServer):
        server = smtplib.SMTP_SSL(self.testServer, self.remotePort, context=context)
        server.ehlo()
        server.quit()
",if not can_verify :,163
"def generate_segment_memory(chart_type, race_configs, environment):
    structures = []
    for race_config in race_configs:
        if ""segment_memory"" in race_config.charts:
            title = chart_type.format_title(
                environment,
                race_config.track,
                es_license=race_config.es_license,
                suffix=""%s-segment-memory"" % race_config.label,
            )
            chart = chart_type.segment_memory(title, environment, race_config)
            if chart:
                structures.append(chart)
    return structures
","if ""segment_memory"" in race_config . charts :",168
"def __iter__(self):
    line = b""""
    while True:
        data = self.read(-1)
        if not data:
            break
        generator = StringIO(data)
        assert b""\n"" not in line, line
        line += next(generator)
        if line.endswith(b""\n""):
            yield line
            line = b""""
            ll = list(generator)
            if not ll:
                continue
            for line in ll[:-1]:
                yield line
            line = ll[-1]
            if line.endswith(b""\n""):
                yield line
                line = b""""
    if line:
        yield line
",if not data :,189
"def L_op(self, inputs, outputs, gout):
    (x,) = inputs
    (gz,) = gout
    if outputs[0].type in discrete_types:
        if x.type in discrete_types:
            return [x.zeros_like(dtype=theano.config.floatX)]
        else:
            return [x.zeros_like()]
    if x.type in float_types:
        return (gz * sgn(x),)
    return (gz * x / abs(x),)  # formula works for complex and real
",if x . type in discrete_types :,133
"def is_ncname(name):
    first = name[0]
    if first == ""_"" or category(first) in NAME_START_CATEGORIES:
        for i in xrange(1, len(name)):
            c = name[i]
            if not category(c) in NAME_CATEGORIES:
                if c in ALLOWED_NAME_CHARS:
                    continue
                return 0
            # if in compatibility area
            # if decomposition(c)!='':
            #    return 0
        return 1
    else:
        return 0
",if not category ( c ) in NAME_CATEGORIES :,151
"def _read_rows_from(self, avro_reader, header):
    count = 0
    maximum = self.limit if self.limit is not None else sys.maxsize
    for i, record in enumerate(avro_reader):
        if i < self.skip:
            continue
        if count >= maximum:
            break
        count += 1
        row = self._map_row_from(header, record)
        yield row
",if count >= maximum :,111
"def decorated(cls, *args, **kwargs):
    storage_res = STORAGE_RES_MAPPING[cls.__class__.__name__][func.__name__]
    with utils.patch_vnxsystem as patched_vnx:
        if DEFAULT_STORAGE_RES in storage_res:
            patched_vnx.return_value = storage_res[DEFAULT_STORAGE_RES]
        adapter = PROTOCOL_MAPPING[protocol](cls.configuration)
    return func(cls, adapter, storage_res, *args, **kwargs)
",if DEFAULT_STORAGE_RES in storage_res :,122
"def _replace_file(src, dst):
    try:
        if not _MoveFileEx(src, dst, 1):  # MOVEFILE_REPLACE_EXISTING
            raise OSError('Could not replace ""%s"" -> ""%s""' % (src, dst))
    except:
        # Sometimes it fails - we play stupid and try again...
        time.sleep(0.5)
        if not _MoveFileEx(src, dst, 1):  # MOVEFILE_REPLACE_EXISTING
            raise OSError('Could not replace ""%s"" -> ""%s""' % (src, dst))
","if not _MoveFileEx ( src , dst , 1 ) :",141
"def read_track_raw(self, redundancy=1):
    self._log(""read track raw"")
    data = []
    await self.lower.write([CMD_READ_RAW, redundancy])
    while True:
        packet = await self.lower.read()
        if packet[-1] == 0xFF:
            raise GlasgowAppletError(""FIFO overflow while reading track"")
        elif packet[-1] == 0xFE:
            data.append(packet[:-1])
            return b"""".join(data)
        else:
            data.append(packet)
",if packet [ - 1 ] == 0xFF :,147
"def get_template_sources(self, template_name, template_dirs=None):
    template_name = self.prepare_template_name(template_name)
    for loader in self.template_source_loaders:
        if hasattr(loader, ""get_template_sources""):
            try:
                for result in loader.get_template_sources(template_name, template_dirs):
                    yield result
            except UnicodeDecodeError:
                # The template dir name was a bytestring that wasn't valid UTF-8.
                raise
            except ValueError:
                # The joined path was located outside of this particular
                # template_dir (it might be inside another one, so this isn't
                # fatal).
                pass
","if hasattr ( loader , ""get_template_sources"" ) :",195
"def __init__(self, reg, shtype, shimm, va):
    if shimm == 0:
        if shtype == S_ROR:
            shtype = S_RRX
        elif shtype == S_LSR or shtype == S_ASR:
            shimm = 32
    self.reg = reg
    self.shtype = shtype
    self.shimm = shimm
    self.va = va
",elif shtype == S_LSR or shtype == S_ASR :,107
"def pop_many(self, limit=None):
    if limit is None:
        limit = DEFAULT_SYNC_OFFLINE_ACTIVITY
    heartbeats = []
    count = 0
    while count < limit:
        heartbeat = self.pop()
        if not heartbeat:
            break
        heartbeats.append(heartbeat)
        count += 1
        if count % HEARTBEATS_PER_REQUEST == 0:
            yield heartbeats
            heartbeats = []
    if heartbeats:
        yield heartbeats
",if count % HEARTBEATS_PER_REQUEST == 0 :,140
"def _set_live(self, live, _):
    if live is not None and not self.live:
        if isinstance(live, basestring):
            live = [live]
        # Default is to use Memory analysis.
        if len(live) == 0:
            mode = ""Memory""
        elif len(live) == 1:
            mode = live[0]
        else:
            raise RuntimeError(""--live parameter should specify only one mode."")
        live_plugin = self.session.plugins.live(mode=mode)
        live_plugin.live()
        # When the session is destroyed, close the live plugin.
        self.session.register_flush_hook(self, live_plugin.close)
    return live
","if isinstance ( live , basestring ) :",184
"def capture_output(redirect_stderr=True):
    oldout, olderr = sys.stdout, sys.stderr
    try:
        out = StringIO()
        sys.stdout = out
        if redirect_stderr:
            sys.stderr = out
        else:
            sys.stderr = StringIO()
        yield out
    except:
        if redirect_stderr:
            traceback.print_exc()
        else:
            raise
    finally:
        sys.stdout, sys.stderr = oldout, olderr
",if redirect_stderr :,135
"def run(self):
    self.mpd.connect()
    events = [""player""]
    while True:
        if ""player"" in events:
            status = self.mpd.status()
            handler = getattr(self, ""on_"" + status[""state""], None)
            if handler:
                handler(status)
            else:
                self._log.debug(u'unhandled status ""{0}""', status)
        events = self.mpd.events()
",if handler :,121
"def get_full_qualified_name(self, node: Element) -> str:
    if node.get(""reftype"") == ""option"":
        progname = node.get(""std:program"")
        command = ws_re.split(node.get(""reftarget""))
        if progname:
            command.insert(0, progname)
        option = command.pop()
        if command:
            return ""."".join([""-"".join(command), option])
        else:
            return None
    else:
        return None
",if progname :,133
"def _get_sources(self):
    servers = self.config[""servers""]
    """"""maps urls to extractors""""""
    server_links = {
        ""mp4upload"": ""mp4upload.com"",
        ""gcloud"": ""gcloud.live"",
        ""gcloud"": ""fembed.com"",
    }
    soup = helpers.soupify(helpers.get(self.url)).select(""iframe"")
    for a in servers:
        for b in soup:
            for c in server_links:
                if server_links[c] in b.get(""src"") and a == c:
                    return [(c, b.get(""src""))]
    logger.warn(""Unsupported URL"")
    return """"
","if server_links [ c ] in b . get ( ""src"" ) and a == c :",179
"def _self_set(self, context):
    if self.keys is not None:
        return
    new_dict = context.get_pynames([""self"", ""d""])[1]
    if new_dict and isinstance(new_dict.get_object().get_type(), Dict):
        args = arguments.ObjectArguments([new_dict])
        items = new_dict.get_object()[""popitem""].get_object().get_returned_object(args)
        context.save_per_name(items)
    else:
        holding = _infer_sequence_for_pyname(new_dict)
        if holding is not None and isinstance(holding.get_type(), Tuple):
            context.save_per_name(holding)
","if holding is not None and isinstance ( holding . get_type ( ) , Tuple ) :",181
"def create():
    """"""Create a new post for the current user.""""""
    if request.method == ""POST"":
        title = request.form[""title""]
        body = request.form[""body""]
        error = None
        if not title:
            error = ""Title is required.""
        if error is not None:
            flash(error)
        else:
            db.session.add(Post(title=title, body=body, author=g.user))
            db.session.commit()
            return redirect(url_for(""blog.index""))
    return render_template(""blog/create.html"")
",if not title :,158
"def _find_host_dir_ldconfig(self, arch=""x86-64""):
    """"""Find host nvidia libraries via ldconfig""""""
    dir_list = set()
    ld_data = Uprocess().get_output([""ldconfig"", ""-p""])
    if ld_data:
        regexp = ""[ |\t]%s[^ ]* .*%s.*=> (/.*)""
        for line in ld_data.split(""\n""):
            for lib in self._nvidia_main_libs:
                match = re.search(regexp % (lib, arch), line)
                if match:
                    dir_list.add(
                        os.path.realpath(os.path.dirname(match.group(1))) + ""/""
                    )
    return dir_list
",if match :,196
"def migrate_replay_storage(apps, schema_editor):
    model = apps.get_model(""terminal"", ""ReplayStorage"")
    init_storage_data(model)
    setting = get_setting(apps, schema_editor, ""TERMINAL_REPLAY_STORAGE"")
    if not setting:
        return
    values = get_storage_data(setting)
    for name, meta in values.items():
        tp = meta.pop(""TYPE"", None)
        if not tp or name in [""default"", ""null""]:
            continue
        model.objects.create(name=name, type=tp, meta=meta)
","if not tp or name in [ ""default"" , ""null"" ] :",150
"def load_distribution(args: CommandLineArguments) -> CommandLineArguments:
    if args.distribution is not None:
        args.distribution = Distribution[args.distribution]
    if args.distribution is None or args.release is None:
        d, r = detect_distribution()
        if args.distribution is None:
            args.distribution = d
        if args.distribution == d and d != Distribution.clear and args.release is None:
            args.release = r
    if args.distribution is None:
        die(""Couldn't detect distribution."")
    return args
",if args . distribution == d and d != Distribution . clear and args . release is None :,137
"def fieldset_string_to_field(fieldset_dict, model):
    if isinstance(fieldset_dict[""fields""], tuple):
        fieldset_dict[""fields""] = list(fieldset_dict[""fields""])
    i = 0
    for dict_field in fieldset_dict[""fields""]:
        if isinstance(dict_field, string_types):
            fieldset_dict[""fields""][i] = model._meta.get_field_by_name(dict_field)[0]
        elif isinstance(dict_field, list) or isinstance(dict_field, tuple):
            dict_field[1][""recursive""] = True
            fieldset_string_to_field(dict_field[1], model)
        i += 1
","elif isinstance ( dict_field , list ) or isinstance ( dict_field , tuple ) :",179
"def icon(display_icon):
    """"""returns empty dict if show_icons is False, else the icon passed""""""
    kws = {}
    if get_icon_switch():
        if display_icon.startswith(""SV_""):
            kws = {""icon_value"": custom_icon(display_icon)}
        elif display_icon != ""OUTLINER_OB_EMPTY"":
            kws = {""icon"": display_icon}
    return kws
","elif display_icon != ""OUTLINER_OB_EMPTY"" :",106
"def cancel_helper(self, node, to_cancel):
    children = set(self.workflow.successors(node))
    for child in children:
        if self.parent_map[child.id_] == 1:
            to_cancel.append(child.id_)
            self.cancelled.append(node.id_)
            await self.cancel_helper(child, to_cancel)
    return to_cancel
",if self . parent_map [ child . id_ ] == 1 :,103
"def getStatusString(self):
    if not self._isAvailable:
        return ""Doodle3D box not found""
    if self._printing:
        if self._blockIndex < len(self._fileBlocks):
            ret = ""Sending GCode: %.1f%%"" % (
                float(self._blockIndex) * 100.0 / float(len(self._fileBlocks))
            )
        elif len(self._fileBlocks) > 0:
            ret = ""Finished sending GCode to Doodle3D box.""
        else:
            ret = ""Different print still running...""
        # ret += ""\nErrorCount: %d"" % (self._errorCount)
        return ret
    return ""Printer found, waiting for print command.""
",elif len ( self . _fileBlocks ) > 0 :,190
"def test_archive_files_message(self):
    filelist = [""test.torrent"", ""deluge.png""]
    arc_filepath = archive_files(
        ""test-arc"", [get_test_data_file(f) for f in filelist], message=""test""
    )
    result_files = filelist + [""archive_message.txt""]
    with tarfile.open(arc_filepath, ""r"") as tar:
        self.assertEqual(tar.getnames(), result_files)
        for tar_info in tar:
            self.assertTrue(tar_info.isfile())
            if tar_info.name == ""archive_message.txt"":
                result = tar.extractfile(tar_info).read().decode()
                self.assertEqual(result, ""test"")
","if tar_info . name == ""archive_message.txt"" :",191
"def _format_arg(self, opt, spec, val):
    if opt in [""in_files""]:
        return scans_for_fnames(ensure_list(val))
    if opt == ""fwhm"":
        if not isinstance(val, list):
            return [val, val, val]
        if isinstance(val, list):
            if len(val) == 1:
                return [val[0], val[0], val[0]]
            else:
                return val
    return super(Smooth, self)._format_arg(opt, spec, val)
","if isinstance ( val , list ) :",148
"def fuzzy_sum(self, currency, rounding=ROUND_UP):
    a = Money.ZEROS[currency].amount
    fuzzy = False
    for m in self:
        if m.currency == currency:
            a += m.amount
        elif m.amount:
            a += m.convert(currency, rounding=None).amount
            fuzzy = True
    r = Money(a, currency, rounding=rounding)
    r.fuzzy = fuzzy
    return r
",if m . currency == currency :,122
"def _read_potfiles(src_root, potfiles):
    """"""Returns a list of paths for a POTFILES.in file""""""
    paths = []
    with open(potfiles, ""r"", encoding=""utf-8"") as h:
        for line in h:
            line = line.strip()
            if not line or line.startswith(""#""):
                continue
            paths.append(os.path.normpath(os.path.join(src_root, line)))
    return paths
","if not line or line . startswith ( ""#"" ) :",123
"def applyMath(self, val, math, frmt):
    # apply math function - eval
    try:
        x = eval(val)
        if math != """":
            x = eval(math)
        val = (""{0"" + frmt + ""}"").format(x)
    except:
        dprint(
            __name__,
            0,
            ""CCmds_applyMath: Error in math {0}, frmt {1}\n{2}"",
            math,
            frmt,
            traceback.format_exc(),
        )
    # apply format specifier
    dprint(__name__, 2, ""CCmds_applyMath: {0}"", val)
    return val
","if math != """" :",179
"def run_train_loop(self):
    self.begin_training()
    for _ in self.yield_train_step():
        if self.should_save_model():
            self.save_model()
        if self.should_save_checkpoint():
            self.save_checkpoint()
        if self.should_eval_model():
            self.eval_model()
        if self.should_break_training():
            break
    self.eval_model()
    self.done_training()
    return self.returned_result()
",if self . should_save_model ( ) :,139
"def node_exists(self, jid=None, node=None, ifrom=None):
    with self.lock:
        if jid is None:
            jid = self.xmpp.boundjid.full
        if node is None:
            node = """"
        if ifrom is None:
            ifrom = """"
        if isinstance(ifrom, JID):
            ifrom = ifrom.full
        if (jid, node, ifrom) not in self.nodes:
            return False
        return True
",if node is None :,136
"def _collect(self, writer=None):
    for artifact_name in self.plugin_args.artifacts:
        for hit in self.collect_artifact(artifact_name):
            if ""result"" in hit and writer:
                writer.write_result(hit[""result""])
            yield hit
","if ""result"" in hit and writer :",76
"def proc(qtbot, caplog):
    """"""A fixture providing a GUIProcess and cleaning it up after the test.""""""
    p = guiprocess.GUIProcess(""testprocess"")
    yield p
    if p._proc.state() == QProcess.Running:
        with caplog.at_level(logging.ERROR):
            with qtbot.waitSignal(p.finished, timeout=10000, raising=False) as blocker:
                p._proc.terminate()
            if not blocker.signal_triggered:
                p._proc.kill()
            p._proc.waitForFinished()
",if not blocker . signal_triggered :,148
"def getsequences(self):
    """"""Return the set of sequences for the folder.""""""
    sequences = {}
    fullname = self.getsequencesfilename()
    try:
        f = open(fullname, ""r"")
    except IOError:
        return sequences
    while 1:
        line = f.readline()
        if not line:
            break
        fields = line.split("":"")
        if len(fields) != 2:
            self.error(""bad sequence in %s: %s"" % (fullname, line.strip()))
        key = fields[0].strip()
        value = IntSet(fields[1].strip(), "" "").tolist()
        sequences[key] = value
    return sequences
",if not line :,173
"def get_coeffs(e):
    coeffs = []
    for du in all_delu_dict.keys():
        if type(self.as_coeffs_dict[e]).__name__ == ""float"":
            coeffs.append(self.as_coeffs_dict[e])
        elif du in self.as_coeffs_dict[e].keys():
            coeffs.append(self.as_coeffs_dict[e][du])
        else:
            coeffs.append(0)
    return np.array(coeffs)
","if type ( self . as_coeffs_dict [ e ] ) . __name__ == ""float"" :",132
"def block_items(objekt, block, eldict):
    if objekt not in block:
        if isinstance(objekt.type, PyType):
            if objekt.type not in block:
                block.append(objekt.type)
        block.append(objekt)
        if isinstance(objekt, PyType):
            others = [
                p
                for p in eldict.values()
                if isinstance(p, PyElement) and p.type[1] == objekt.name
            ]
            for item in others:
                if item not in block:
                    block.append(item)
    return block
",if item not in block :,186
"def FindPrefix(self, prefix):
    self.log.WriteText(""Looking for prefix: %s\n"" % prefix)
    if prefix:
        prefix = prefix.lower()
        length = len(prefix)
        # Changed in 2.5 because ListBox.Number() is no longer supported.
        # ListBox.GetCount() is now the appropriate way to go.
        for x in range(self.GetCount()):
            text = self.GetString(x)
            text = text.lower()
            if text[:length] == prefix:
                self.log.WriteText(""Prefix %s is found.\n"" % prefix)
                return x
    self.log.WriteText(""Prefix %s is not found.\n"" % prefix)
    return -1
",if text [ : length ] == prefix :,195
"def encode(self, input, errors=""strict""):
    if self.encoder is None:
        result = codecs.utf_32_encode(input, errors)
        if sys.byteorder == ""little"":
            self.encoder = codecs.utf_32_le_encode
        else:
            self.encoder = codecs.utf_32_be_encode
        return result
    else:
        return self.encoder(input, errors)
","if sys . byteorder == ""little"" :",109
"def __call__(self, message, keyname):
    if keyname in self.keyring:
        key = self.keyring[keyname]
        if isinstance(key, Key) and key.algorithm == GSS_TSIG:
            if message:
                GSSTSigAdapter.parse_tkey_and_step(key, message, keyname)
        return key
    else:
        return None
","if isinstance ( key , Key ) and key . algorithm == GSS_TSIG :",98
"def unicode_metrics(metrics):
    for i, metric in enumerate(metrics):
        for key, value in metric.items():
            if isinstance(value, basestring):
                metric[key] = unicode(value, errors=""replace"")
            elif isinstance(value, tuple) or isinstance(value, list):
                value_list = list(value)
                for j, value_element in enumerate(value_list):
                    if isinstance(value_element, basestring):
                        value_list[j] = unicode(value_element, errors=""replace"")
                metric[key] = tuple(value_list)
        metrics[i] = metric
    return metrics
","if isinstance ( value_element , basestring ) :",177
"def step(self, action):
    assert self.action_space.contains(action)
    if self._state == 4:
        if action and self._case:
            return self._state, 10.0, True, {}
        else:
            return self._state, -10, True, {}
    else:
        if action:
            if self._state == 0:
                self._state = 2
            else:
                self._state += 1
        elif self._state == 2:
            self._state = self._case
    return self._state, -1, False, {}
",if self . _state == 0 :,157
"def get_superuser(self):
    try:
        query = dict()
        if get_user_model().USERNAME_FIELD != ""email"":
            query[get_user_model().USERNAME_FIELD] = ""admin""
        else:
            query[get_user_model().USERNAME_FIELD] = ""admin@django-cms.org""
        admin = get_user_model().objects.get(**query)
    except get_user_model().DoesNotExist:
        admin = self._create_user(""admin"", is_staff=True, is_superuser=True)
    return admin
","if get_user_model ( ) . USERNAME_FIELD != ""email"" :",145
"def newend(self):
    newenddatetime = self._newenddate
    if not self.checkallday.state:
        if not hasattr(self.enddt, ""tzinfo"") or self.enddt.tzinfo is None:
            tzinfo = self.conf.default.default_timezone
        else:
            tzinfo = self.enddt.tzinfo
        try:
            newendtime = self._newendtime
            newenddatetime = datetime.combine(newenddatetime, newendtime)
            newenddatetime = tzinfo.localize(newenddatetime)
        except TypeError:
            return None
    return newenddatetime
","if not hasattr ( self . enddt , ""tzinfo"" ) or self . enddt . tzinfo is None :",159
"def run(self):
    to_delete = set()
    for k, v in iteritems(self.objs):
        if k.startswith(""_""):
            continue
        if v[""_class""] == ""SubmissionFormatElement"":
            to_delete.add(k)
        if v[""_class""] == ""Task"":
            v[""submission_format""] = list(
                self.objs[k][""filename""] for k in v.get(""submission_format"", list())
            )
    for k in to_delete:
        del self.objs[k]
    return self.objs
","if v [ ""_class"" ] == ""SubmissionFormatElement"" :",147
"def update_reserved_qty_for_subcontract(self):
    for d in self.supplied_items:
        if d.rm_item_code:
            stock_bin = get_bin(d.rm_item_code, d.reserve_warehouse)
            stock_bin.update_reserved_qty_for_sub_contracting()
",if d . rm_item_code :,86
"def process(self):
    if ""Length"" in self.outputs and self.outputs[""Length""].is_linked:
        if ""Data"" in self.inputs and self.inputs[""Data""].is_linked:
            data = self.inputs[""Data""].sv_get(deepcopy=False)
            if not self.level:
                out = [[len(data)]]
            elif self.level == 1:
                out = [self.count(data, self.level)]
            else:
                out = self.count(data, self.level)
            self.outputs[""Length""].sv_set(out)
",if not self . level :,159
"def _user_has_perm(user, perm, obj):
    anon = user.is_anonymous()
    for backend in auth.get_backends():
        if not anon or backend.supports_anonymous_user:
            if hasattr(backend, ""has_perm""):
                if obj is not None:
                    if backend.supports_object_permissions and backend.has_perm(
                        user, perm, obj
                    ):
                        return True
                else:
                    if backend.has_perm(user, perm):
                        return True
    return False
",if not anon or backend . supports_anonymous_user :,163
"def visit(self, node=None):
    """"""Walks over a node.  If no node is provided, the tree is used.""""""
    if node is None:
        node = self.tree
    if node is None:
        raise RuntimeError(""no node or tree given!"")
    for clsname in map(_lowername, type.mro(node.__class__)):
        meth = getattr(self, ""visit_"" + clsname, None)
        if callable(meth):
            rtn = meth(node)
            break
    else:
        msg = ""could not find valid visitor method for {0} on {1}""
        nodename = node.__class__.__name__
        selfname = self.__class__.__name__
        raise AttributeError(msg.format(nodename, selfname))
    return rtn
",if callable ( meth ) :,194
"def add_fade_out(compositor, fade_out_length):
    clip = _get_compositor_clip(compositor)
    keyframe_property, property_klass, keyframes = _get_kfproperty_klass_and_keyframes(
        compositor, clip
    )
    if fade_out_length > 0:
        if fade_out_length + 1 <= clip.clip_length():
            return _do_user_add_fade_out(
                keyframe_property, property_klass, keyframes, fade_out_length, clip
            )
        else:
            _show_length_error_dialog()
            return None
",if fade_out_length + 1 <= clip . clip_length ( ) :,167
"def make_timesheet_records():
    employees = get_timesheet_based_salary_slip_employee()
    for e in employees:
        ts = make_timesheet(
            e.employee,
            simulate=True,
            billable=1,
            activity_type=get_random(""Activity Type""),
            company=frappe.flags.company,
        )
        frappe.db.commit()
        rand = random.random()
        if rand >= 0.3:
            make_salary_slip_for_timesheet(ts.name)
        rand = random.random()
        if rand >= 0.2:
            make_sales_invoice_for_timesheet(ts.name)
",if rand >= 0.2 :,197
"def _target_from_batch(self, batch):
    targets = []
    for name in self.labels:
        target = getattr(batch, name)
        if name in [Target.TARGET_PROB_FIELD, Target.TARGET_LOGITS_FIELD]:
            label_vocab = self.metadata.target.vocab.stoi
            batch_label_list = getattr(batch, Target.TARGET_LABEL_FIELD)
            target = align_target_labels(target, batch_label_list, label_vocab)
        targets.append(target)
    if len(targets) == 1:
        return targets[0]
    return tuple(targets)
","if name in [ Target . TARGET_PROB_FIELD , Target . TARGET_LOGITS_FIELD ] :",161
"def detectForms(html):
    erreur = """"
    soup = BeautifulSoup(html, ""html.parser"")
    detectedForms = soup.find_all(""form"")
    returnForms = []
    if len(detectedForms) > 0:
        for f in detectedForms:
            fileInputs = f.findChildren(""input"", {""type"": re.compile(""file"", re.I)})
            if len(fileInputs) > 0:
                returnForms.append((f, fileInputs))
    return returnForms
",if len ( fileInputs ) > 0 :,136
"def _updateNewCardRatio(self):
    if self.col.conf[""newSpread""] == NEW_CARDS_DISTRIBUTE:
        if self.newCount:
            self.newCardModulus = (self.newCount + self.revCount) // self.newCount
            # if there are cards to review, ensure modulo >= 2
            if self.revCount:
                self.newCardModulus = max(2, self.newCardModulus)
            return
    self.newCardModulus = 0
",if self . newCount :,128
"def __prep_write_total(self, comments, main, fallback, single):
    lower = self.as_lowercased()
    for k in [main, fallback, single]:
        if k in comments:
            del comments[k]
    if single in lower:
        parts = lower[single].split(""/"", 1)
        if parts[0]:
            comments[single] = [parts[0]]
        if len(parts) > 1:
            comments[main] = [parts[1]]
    if main in lower:
        comments[main] = lower.list(main)
    if fallback in lower:
        if main in comments:
            comments[fallback] = lower.list(fallback)
        else:
            comments[main] = lower.list(fallback)
",if k in comments :,196
"def check_physical(self, line):
    """"""Run all physical checks on a raw input line.""""""
    self.physical_line = line
    for name, check, argument_names in self._physical_checks:
        self.init_checker_state(name, argument_names)
        result = self.run_check(check, argument_names)
        if result is not None:
            (offset, text) = result
            self.report_error(self.line_number, offset, text, check)
            if text[:4] == ""E101"":
                self.indent_char = line[0]
","if text [ : 4 ] == ""E101"" :",154
"def dependencies(self):
    deps = []
    midx = None
    if self.ref is not None:
        query = GroupQuery(self.ref)
        g = query.execute(self.schema)
        if g is None:
            log.debug(self.schema)
            raise TypeNotFound(self.ref)
        deps.append(g)
        midx = 0
    return (midx, deps)
",if g is None :,109
"def __init__(self, metadata=None):
    if not metadata:
        db = get_session()
        metadata = lookup_feed(db, self.__feed_name__)
        if not metadata:
            raise Exception(
                ""Must have feed metadata in db already, should sync metadata before invoking instance operations""
            )
    super(AnchoreServiceFeed, self).__init__(metadata=metadata)
",if not metadata :,102
"def testGetPartRect(self):
    ""Make sure the part rectangles are retrieved correctly""
    for i in range(0, self.ctrl.part_count()):
        part_rect = self.ctrl.get_part_rect(i)
        self.assertEqual(part_rect.left, self.part_rects[i].left)
        if i != self.ctrl.part_count() - 1:
            self.assertEqual(part_rect.right, self.part_rects[i].right)
        self.assertEqual(part_rect.top, self.part_rects[i].top)
        self.assertFalse(abs(part_rect.bottom - self.part_rects[i].bottom) > 2)
    self.assertRaises(IndexError, self.ctrl.get_part_rect, 99)
",if i != self . ctrl . part_count ( ) - 1 :,197
"def __call__(self, ctx):
    if ctx.range and ctx.value:
        if self.raw:
            ctx.range.raw_value = ctx.value
            return
        scalar = ctx.meta.get(""scalar"", False)
        if not scalar:
            ctx.range = ctx.range.resize(len(ctx.value), len(ctx.value[0]))
        self._write_value(ctx.range, ctx.value, scalar)
",if not scalar :,117
"def basic_get(self, queue, no_ack=False, **kwargs):
    """"""Get message by direct access (synchronous).""""""
    try:
        message = self.Message(self._get(queue), channel=self)
        if not no_ack:
            self.qos.append(message, message.delivery_tag)
        return message
    except Empty:
        pass
",if not no_ack :,97
"def http_client(cls) -> aiohttp.ClientSession:
    if cls._client is None:
        if not asyncio.get_event_loop().is_running():
            raise EnvironmentError(
                ""Event loop must be running to start HTTP client session.""
            )
        cls._client = aiohttp.ClientSession(request_class=SSLClientRequest)
    return cls._client
",if not asyncio . get_event_loop ( ) . is_running ( ) :,97
"def createMimeType(self):
    audio = False
    for prop in self.array(""header/content/stream_prop""):
        guid = prop[""content/type""].value
        if guid == VideoHeader.guid:
            return u""video/x-ms-wmv""
        if guid == AudioHeader.guid:
            audio = True
    if audio:
        return u""audio/x-ms-wma""
    else:
        return u""video/x-ms-asf""
",if guid == AudioHeader . guid :,126
"def _removeCachedRFInfo(self, cache_key, path, removeChildPaths):
    log.debug(""_removeCachedRFInfo: cache_key %r, path %r"", cache_key, path)
    if self._cachedFiles.has_key(cache_key):
        cache = self._cachedFiles[cache_key]
        if cache.has_key(path):
            del cache[path]
        if removeChildPaths:
            # Remove all cached paths that are under this directory
            from remotefilelib import addslash
            dirPath = addslash(path)
            for keypath in cache.keys():
                if keypath.startswith(dirPath):
                    del cache[keypath]
",if keypath . startswith ( dirPath ) :,178
"def format(self, obj, context, maxlevels, level):
    if isinstance(obj, unicode):
        # return (obj.encode('utf8'), True, False)
        return (obj, True, False)
    if isinstance(obj, bytes):
        convert = False
        # for c in obj:
        # 	if ord(c) >= 128:
        # 		convert = True
        # 		break
        try:
            codecs.decode(obj)
        except:
            convert = True
        if convert:
            return (""0x{}"".format(obj), True, False)
    return pprint.PrettyPrinter.format(self, obj, context, maxlevels, level)
",if convert :,176
"def add_data_source(self, f=None, s_name=None, source=None, module=None, section=None):
    try:
        if module is None:
            module = self.name
        if section is None:
            section = ""all_sections""
        if s_name is None:
            s_name = f[""s_name""]
        if source is None:
            source = os.path.abspath(os.path.join(f[""root""], f[""fn""]))
        report.data_sources[module][section][s_name] = source
    except AttributeError:
        logger.warning(
            ""Tried to add data source for {}, but was missing fields data"".format(
                self.name
            )
        )
",if source is None :,198
"def open(self, *args, **kwargs):
    if kwargs.get(""json"") is not None:
        with self.session_transaction() as sess:
            api_key_headers = Headers({""CSRF-Token"": sess.get(""nonce"")})
            headers = kwargs.pop(""headers"", Headers())
            if isinstance(headers, dict):
                headers = Headers(headers)
            headers.extend(api_key_headers)
            kwargs[""headers""] = headers
    return super(CTFdTestClient, self).open(*args, **kwargs)
","if isinstance ( headers , dict ) :",141
"def get_params(self):
    if not hasattr(self, ""input_space""):
        raise AttributeError(""Input space has not been provided."")
    rval = []
    for layer in self.layers:
        for param in layer.get_params():
            if param.name is None:
                logger.info(type(layer))
        layer_params = layer.get_params()
        assert not isinstance(layer_params, set)
        for param in layer_params:
            if param not in rval:
                rval.append(param)
    rval = [elem for elem in rval if elem not in self.freeze_set]
    assert all([elem.name is not None for elem in rval])
    return rval
",if param . name is None :,181
"def _animate_strategy(self, speed=1):
    if self._animating == 0:
        return
    if self._apply_strategy() is not None:
        if self._animate.get() == 0 or self._step.get() == 1:
            return
        if self._animate.get() == 1:
            self._root.after(3000, self._animate_strategy)
        elif self._animate.get() == 2:
            self._root.after(1000, self._animate_strategy)
        else:
            self._root.after(20, self._animate_strategy)
",if self . _animate . get ( ) == 0 or self . _step . get ( ) == 1 :,151
"def charAt(pos):
    this.cok()
    pos = pos.to_int()
    s = this.to_string()
    if 0 <= pos < len(s.value):
        char = s.value[pos]
        if char not in s.CHAR_BANK:
            s.Js(char)  # add char to char bank
        return s.CHAR_BANK[char]
    return s.CHAR_BANK[""""]
",if char not in s . CHAR_BANK :,116
"def find_executable(names):
    # Given a list of executable names, find the first one that is available
    # as an executable file, on the path.
    for name in names:
        fpath, fname = os.path.split(name)
        if fpath:
            # The given name is absolute.
            if is_executable(name):
                return name
        else:
            # Try to find the name on the PATH
            for path in os.environ[""PATH""].split(os.pathsep):
                exe_file = os.path.join(path, name)
                if is_executable(exe_file):
                    return exe_file
    # Could not find it :(
    return None
",if is_executable ( exe_file ) :,186
"def match_file(self, file, tff_format):
    match = tff_format.search(file.filename.replace(""\\"", ""/""))
    if match:
        result = {}
        for name, value in match.groupdict().items():
            value = value.strip()
            if name in self.numeric_tags:
                value = value.lstrip(""0"")
            if self.ui.replace_underscores.isChecked():
                value = value.replace(""_"", "" "")
            result[name] = value
        return result
    else:
        return {}
",if name in self . numeric_tags :,149
"def __init__(
    self,
    filename: str = ""checkpoint"",
    frequency: Union[int, List[int]] = 1,
    on: Union[str, List[str]] = ""epoch_end"",
):
    if isinstance(frequency, list):
        if not isinstance(on, list) or len(frequency) != len(on):
            raise ValueError(
                ""If you pass a list for checkpoint frequencies, the `on` ""
                ""parameter has to be a list with the same length.""
            )
    self._frequency = frequency
    super(_TuneCheckpointCallback, self).__init__(on)
    self._filename = filename
    self._counter = Counter()
    self._cp_count = 0  # Has to be monotonically increasing
","if not isinstance ( on , list ) or len ( frequency ) != len ( on ) :",185
"def download(cls, architecture, path=""./""):
    if cls.sanity_check(architecture):
        architecture_file = path + ""imagenet_{}.pth"".format(architecture)
        if not os.path.exists(architecture_file):
            kwargs = {}
            if architecture == ""inception_v3"":
                kwargs[""transform_input""] = False
            model = models.__dict__[architecture](pretrained=True, **kwargs)
            torch.save(model, architecture_file)
            print(
                ""PyTorch pretrained model is saved as [{}]."".format(architecture_file)
            )
        else:
            print(""File [{}] existed!"".format(architecture_file))
        return architecture_file
    else:
        return None
","if architecture == ""inception_v3"" :",198
"def __exit__(self, exc_type, exc_value, traceback):
    self.signal.disconnect(self._listener)
    if not self.signal_sent:
        self.test_case.fail(""Signal was not sent."")
        return
    if self.required_kwargs is not None:
        missing_kwargs = []
        for k in self.required_kwargs:
            if k not in self.received_kwargs:
                missing_kwargs.append(k)
        if missing_kwargs:
            self.test_case.fail(
                ""Signal missing required arguments: "" ""%s"" % "","".join(missing_kwargs)
            )
",if k not in self . received_kwargs :,166
"def Assign(left, right):
    names = []
    if isinstance(left, ast.Name):
        # Single assignment on left
        return ast.Assign([ast.AssName(left.name, ""OP_ASSIGN"")], right)
    elif isinstance(left, ast.Tuple):
        # List of things - make sure they are Name nodes
        names = []
        for child in left.getChildren():
            if not isinstance(child, ast.Name):
                raise SyntaxError(""that assignment not supported"")
            names.append(child.name)
        ass_list = [ast.AssName(name, ""OP_ASSIGN"") for name in names]
        return ast.Assign([ast.AssTuple(ass_list)], right)
    else:
        raise SyntaxError(""Can't do that yet"")
","if not isinstance ( child , ast . Name ) :",197
"def readVorbisComment(metadata, comment):
    metadata.producer = getValue(comment, ""vendor"")
    for item in comment.array(""metadata""):
        if ""="" in item.value:
            key, value = item.value.split(""="", 1)
            key = key.upper()
            if key in VORBIS_KEY_TO_ATTR:
                key = VORBIS_KEY_TO_ATTR[key]
                setattr(metadata, key, value)
            elif value:
                metadata.warning(""Skip Vorbis comment %s: %s"" % (key, value))
","if ""="" in item . value :",157
"def _read_readable(self, readable):
    blocksize = 8192
    if self.debuglevel > 0:
        print(""sendIng a read()able"")
    encode = self._is_textIO(readable)
    if encode and self.debuglevel > 0:
        print(""encoding file using iso-8859-1"")
    while True:
        datablock = readable.read(blocksize)
        if not datablock:
            break
        if encode:
            datablock = datablock.encode(""iso-8859-1"")
        yield datablock
",if encode :,139
"def TryMerge(self, d):
    while 1:
        tt = d.getVarInt32()
        if tt == 12:
            break
        if tt == 18:
            self.set_value(d.getPrefixedString())
            continue
        if tt == 29:
            self.set_flags(d.get32())
            continue
        if tt == 0:
            raise ProtocolBuffer.ProtocolBufferDecodeError
        d.skipData(tt)
",if tt == 29 :,126
"def needs_rebuild(self):
    for ratio in self.sprite.config[""ratios""]:
        cocos2d_path = self.output_path(ratio)
        if os.path.exists(cocos2d_path):
            try:
                data = plistlib.readPlist(cocos2d_path)
                assert data[self.meta_key][""hash""] == self.sprite.hash
            except Exception:
                continue
        return True
    return False
",if os . path . exists ( cocos2d_path ) :,125
"def on_epoch_end(self, batch, logs=None):
    # At the end of every epoch, remask the weights. This ensures that when
    # the model is saved after completion, the weights represent mask*weights.
    weight_mask_ops = []
    for layer in self.prunable_layers:
        if isinstance(layer, pruning_wrapper.PruneLowMagnitude):
            if tf.executing_eagerly():
                layer.pruning_obj.weight_mask_op()
            else:
                weight_mask_ops.append(layer.pruning_obj.weight_mask_op())
    K.batch_get_value(weight_mask_ops)
",if tf . executing_eagerly ( ) :,166
"def buildQueryRE(queryText, caseSensitive, wholeWord):
    ""returns a RegEx pattern for searching for the given queryText""
    # word detection etc. cannot be done on an encoding-less string:
    assert type(queryText) == unicode
    pattern = re.escape(queryText)
    if wholeWord:
        if re.search(""^\w"", queryText, re.UNICODE):
            pattern = ""\\b"" + pattern
        if re.search(""\w$"", queryText, re.UNICODE):
            pattern = pattern + ""\\b""
    flags = re.UNICODE
    if not (caseSensitive):
        flags |= re.IGNORECASE
    return re.compile(pattern, flags)
","if re . search ( ""\w$"" , queryText , re . UNICODE ) :",166
"def is_valid_origin(origin):
    if not settings.SENTRY_ALLOW_ORIGIN:
        return False
    if settings.SENTRY_ALLOW_ORIGIN == ""*"":
        return True
    if not origin:
        return False
    origin = origin.lower()
    for value in settings.SENTRY_ALLOW_ORIGIN:
        if isinstance(value, string_types):
            if value.lower() == origin:
                return True
        else:
            if value.match(origin):
                return True
    return False
",if value . match ( origin ) :,137
"def get_menu_title(self):
    handle = self.obj.get_handle()
    if handle:
        who = get_participant_from_event(self.db, handle)
        desc = self.obj.get_description()
        event_name = self.obj.get_type()
        if desc:
            event_name = ""%s - %s"" % (event_name, desc)
        if who:
            event_name = ""%s - %s"" % (event_name, who)
        dialog_title = _(""Event: %s"") % event_name
    else:
        dialog_title = _(""New Event"")
    return dialog_title
",if who :,168
"def memory(self):
    if self.memory_ is None:
        self.lazy_init_lock_.acquire()
        try:
            if self.memory_ is None:
                self.memory_ = SystemStat()
        finally:
            self.lazy_init_lock_.release()
    return self.memory_
",if self . memory_ is None :,85
"def __str__(self):
    fmt = ""%#x"" if isinstance(self.target, six.integer_types) else ""%r""
    args = []
    for arg in self.args:
        args.append(self._special_repr(arg))
    name = self.name or (fmt % self.target)
    arg_str = []
    for arg in args:
        if isinstance(arg, six.integer_types) and arg > 0x100:
            arg_str.append(hex(arg))
        else:
            arg_str.append(str(arg))
    return ""%s(%s)"" % (name, "", "".join(arg_str))
","if isinstance ( arg , six . integer_types ) and arg > 0x100 :",164
"def change_password(username=""flexget"", password="""", session=None):
    check = zxcvbn.zxcvbn(password, user_inputs=[username])
    if check[""score""] < 3:
        warning = check[""feedback""][""warning""]
        suggestions = "" "".join(check[""feedback""][""suggestions""])
        message = ""Password '{}' is not strong enough. "".format(password)
        if warning:
            message += warning + "" ""
        if suggestions:
            message += ""Suggestions: {}"".format(suggestions)
        raise WeakPassword(message)
    user = get_user(username=username, session=session)
    user.password = str(generate_password_hash(password))
    session.commit()
",if suggestions :,175
"def _on_workflow_object_saved(sender, instance, created, *args, **kwargs):
    for instance_workflow in instance.river.all(instance.__class__):
        if created:
            instance_workflow.initialize_approvals()
            if not instance_workflow.get_state():
                init_state = getattr(
                    instance.__class__.river, instance_workflow.field_name
                ).initial_state
                instance_workflow.set_state(init_state)
                instance.save()
",if created :,139
"def recvmsg_into(self, buffers, *args):
    while True:
        try:
            if args:
                # The C code is sensitive about whether extra arguments are
                # passed or not.
                return self._sock.recvmsg_into(buffers, *args)
            return self._sock.recvmsg_into(buffers)
        except error as ex:
            if ex.args[0] != EWOULDBLOCK or self.timeout == 0.0:
                raise
        self._wait(self._read_event)
",if args :,146
"def _generate_toc(line):
    while 1:
        if line.startswith(""2""):
            line = 5
            while 1:
                if line:
                    line = 6
                    break
                elif not line:
                    line = 7
                    break
        elif not line:
            break
    return 1
","if line . startswith ( ""2"" ) :",103
"def tearDown(self):
    for filename in os.listdir(from_here(""lib"")):
        if filename not in self.files_to_keep:
            try:
                os.remove(from_here(""lib"", filename))
            except OSError:
                pass  # File may no longer exist.
",if filename not in self . files_to_keep :,80
"def parse_literal_object(node):
    value = 0
    unit = get_default_weight_unit()
    for field in node.fields:
        if field.name.value == ""value"":
            try:
                value = decimal.Decimal(field.value.value)
            except decimal.DecimalException:
                raise GraphQLError(f""Unsupported value: {field.value.value}"")
        if field.name.value == ""unit"":
            unit = field.value.value
    return Weight(**{unit: value})
","if field . name . value == ""value"" :",134
"def run(self):
    to_delete = set()
    for k, v in iteritems(self.objs):
        if k.startswith(""_""):
            continue
        if v[""_class""] == ""SubmissionFormatElement"":
            to_delete.add(k)
        if v[""_class""] == ""Task"":
            v[""submission_format""] = list(
                self.objs[k][""filename""] for k in v.get(""submission_format"", list())
            )
    for k in to_delete:
        del self.objs[k]
    return self.objs
","if v [ ""_class"" ] == ""Task"" :",147
"def _detect_too_many_digits(f):
    ret = []
    for node in f.nodes:
        # each node contains a list of IR instruction
        for ir in node.irs:
            # iterate over all the variables read by the IR
            for read in ir.read:
                # if the variable is a constant
                if isinstance(read, Constant):
                    # read.value can return an int or a str. Convert it to str
                    value_as_str = read.original_value
                    if ""00000"" in value_as_str:
                        # Info to be printed
                        ret.append(node)
    return ret
","if isinstance ( read , Constant ) :",183
"def split_path_info(path):
    # suitable for splitting an already-unquoted-already-decoded (unicode)
    # path value
    path = path.strip(""/"")
    clean = []
    for segment in path.split(""/""):
        if not segment or segment == ""."":
            continue
        elif segment == "".."":
            if clean:
                del clean[-1]
        else:
            clean.append(segment)
    return tuple(clean)
","elif segment == "".."" :",115
"def callback(f):
    unfinished_children.remove(f)
    if not unfinished_children:
        try:
            result_list = [i.result() for i in children]
        except Exception:
            future.set_exc_info(sys.exc_info())
        else:
            if keys is not None:
                future.set_result(dict(zip(keys, result_list)))
            else:
                future.set_result(result_list)
",if keys is not None :,128
"def L_op(self, inputs, outputs, gout):
    (x,) = inputs
    (gz,) = gout
    if gz.type in complex_types:
        raise NotImplementedError()
    if outputs[0].type in discrete_types:
        if x.type in discrete_types:
            return [x.zeros_like(dtype=theano.config.floatX)]
        else:
            return [x.zeros_like()]
    return (gz * x * 2,)
",if x . type in discrete_types :,117
"def perform_page_up(self, event):
    # if first line is visible then go there
    # (by default it doesn't move then)
    try:
        first_visible_idx = self.index(""@0,0"")
        row, _ = map(int, first_visible_idx.split("".""))
        if row == 1:
            self.mark_set(""insert"", ""1.0"")
    except Exception as e:
        logger.exception(""Could not perform page up"", exc_info=e)
",if row == 1 :,126
"def __str__(self):
    s = """"
    for k, v in self._members.items():
        if isinstance(v.get(""type""), list):
            s += k + "" : "" + "";"".join(getattr(self, item)) + ""\n""
        elif isinstance(v.get(""type""), str):
            s += k + "" : "" + getattr(self, k) + ""\n""
    return s
","if isinstance ( v . get ( ""type"" ) , list ) :",104
"def _shared_pool(**opts):
    if ""host"" in opts:
        key = ""%s:%s/%s"" % (
            opts[""host""],
            opts[""port""],
            opts[""db""],
        )
    else:
        key = ""%s/%s"" % (opts[""path""], opts[""db""])
    pool = _pool_cache.get(key)
    if pool is not None:
        return pool
    with _pool_lock:
        pool = _pool_cache.get(key)
        if pool is not None:
            return pool
        pool = ConnectionPool(**opts)
        _pool_cache[key] = pool
        return pool
",if pool is not None :,174
"def _override_settings(self, overriden_settings: dict):
    for setting_name, setting_value in overriden_settings.items():
        value = setting_value
        if isinstance(setting_value, dict):
            value = getattr(self, setting_name, {})
            value.update(ObjDict(setting_value))
        setattr(self, setting_name, value)
","if isinstance ( setting_value , dict ) :",99
"def match_tls_context(self, host: str, ir: ""IR""):
    for context in ir.get_tls_contexts():
        hosts = context.get(""hosts"") or []
        for context_host in hosts:
            if context_host == host:
                ir.logger.debug(
                    ""Matched host {} with TLSContext {}"".format(
                        host, context.get(""name"")
                    )
                )
                self.sni = True
                return context
    return None
",if context_host == host :,144
"def get_form_datas(self):
    # Prepare the dict of initial data from the request.
    # We have to special-case M2Ms as a list of comma-separated PKs.
    if self.request_method == ""get"":
        initial = dict(self.request.GET.items())
        for k in initial:
            try:
                f = self.opts.get_field(k)
            except models.FieldDoesNotExist:
                continue
            if isinstance(f, models.ManyToManyField):
                initial[k] = initial[k].split("","")
        return {""initial"": initial}
    else:
        return {""data"": self.request.POST, ""files"": self.request.FILES}
","if isinstance ( f , models . ManyToManyField ) :",182
"def run_until(loop, pred, timeout=30):
    deadline = time.time() + timeout
    while not pred():
        if timeout is not None:
            timeout = deadline - time.time()
            if timeout <= 0:
                raise futures.TimeoutError()
        loop.run_until_complete(tasks.sleep(0.001, loop=loop))
",if timeout is not None :,94
"def update_translations():
    pot_path = os.path.join(root, ""messages.pot"")
    template = read_po(open(pot_path, ""rb""))
    for locale in get_locales():
        po_path = os.path.join(root, locale, ""messages.po"")
        mo_path = os.path.join(root, locale, ""messages.mo"")
        if os.path.exists(po_path):
            catalog = read_po(open(po_path, ""rb""))
            catalog.update(template)
            f = open(po_path, ""wb"")
            write_po(f, catalog)
            f.close()
            print(""updated"", po_path)
    compile_translations()
",if os . path . exists ( po_path ) :,191
"def get_queryset_for_content_type(self, content_type_id):
    """"""Return the QuerySet from the QuerySetSequence for a ctype.""""""
    content_type = ContentType.objects.get_for_id(content_type_id)
    for queryset in self.queryset.get_querysets():
        if queryset.model.__name__ == ""QuerySequenceModel"":
            # django-queryset-sequence 0.7 support dynamically created
            # QuerySequenceModel which replaces the original model when it
            # patches the queryset since 6394e19
            model = queryset.model.__bases__[0]
        else:
            model = queryset.model
        if model == content_type.model_class():
            return queryset
","if queryset . model . __name__ == ""QuerySequenceModel"" :",177
"def __bypass_wizard(self):
    bypass = False
    if self.device.remote_op.dir_exist(self.project_folder):
        msg = ""A Tweak with the same PROJECT_NAME ({}) already exists. Do you want to delete it and start from scratch?"".format(
            self.options[""project_name""]
        )
        clean = choose_boolean(msg)
        if clean:
            self.device.remote_op.dir_delete(self.project_folder)
        else:
            bypass = True
    return bypass
",if clean :,137
"def wrapper(cached=True, reset=False):
    nonlocal cached_venv_dir
    if not cached or not cached_venv_dir or reset:
        venv_dir = os.environ.get(""_VENV_DIR_"") or load_settings(lazy=True).get(
            ""venv_dir""
        )
        if venv_dir:  # no cov
            if venv_dir == ""isolated"":
                venv_dir = VENV_DIR_ISOLATED
            elif venv_dir == ""shared"":
                venv_dir = VENV_DIR_SHARED
        else:  # no cov
            venv_dir = VENV_DIR_SHARED
        cached_venv_dir = venv_dir
    return cached_venv_dir
","elif venv_dir == ""shared"" :",186
"def run(self):
    while not self._stop:
        for i in range(0, self._interval):
            time.sleep(1)
            if self._stop:
                self.__logger.debug(""%s - ping thread stopped"" % self.name)
                return
        ping = PingIqProtocolEntity()
        self._layer.waitPong(ping.getId())
        if not self._stop:
            self._layer.sendIq(ping)
",if self . _stop :,126
"def install(self, unicode=False, names=None):
    import __builtin__
    __builtin__.__dict__[""_""] = unicode and self.ugettext or self.gettext
    if hasattr(names, ""__contains__""):
        if ""gettext"" in names:
            __builtin__.__dict__[""gettext""] = __builtin__.__dict__[""_""]
        if ""ngettext"" in names:
            __builtin__.__dict__[""ngettext""] = (
                unicode and self.ungettext or self.ngettext
            )
        if ""lgettext"" in names:
            __builtin__.__dict__[""lgettext""] = self.lgettext
        if ""lngettext"" in names:
            __builtin__.__dict__[""lngettext""] = self.lngettext
","if ""ngettext"" in names :",181
"def on_task_output(self, task, config):
    for entry in task.entries:
        if ""torrent"" in entry:
            if entry[""torrent""].modified:
                # re-write data into a file
                log.debug(""Writing modified torrent file for %s"" % entry[""title""])
                with open(entry[""file""], ""wb+"") as f:
                    f.write(entry[""torrent""].encode())
","if entry [ ""torrent"" ] . modified :",115
"def batchSites(self, sites):
    i = 0
    res = list()
    siteList = list()
    for site in sites:
        if i >= self.opts[""_maxthreads""]:
            data = self.threadSites(siteList)
            if data is None:
                return res
            for ret in list(data.keys()):
                if data[ret]:
                    # bucket:filecount
                    res.append(f""{ret}:{data[ret]}"")
            i = 0
            siteList = list()
        siteList.append(site)
        i += 1
    return res
","if i >= self . opts [ ""_maxthreads"" ] :",168
"def width_pixels(self):
    w = self.style_width
    if self._absolute_size and w == ""auto"":
        w = self._absolute_size.width
    if type(w) is NumberUnit:
        if self._relative_element == self:
            rew = self._parent_size.width if self._parent_size else 0
        elif self._relative_element is None:
            rew = 0
        else:
            rew = self._relative_element.width_pixels
        if rew == ""auto"":
            rew = 0
        w = w.val(base=rew)
    return w
","if rew == ""auto"" :",160
"def get_lang3(lang):
    try:
        if len(lang) == 2:
            ret_value = get(part1=lang).part3
        elif len(lang) == 3:
            ret_value = lang
        else:
            ret_value = """"
    except KeyError:
        ret_value = lang
    return ret_value
",if len ( lang ) == 2 :,94
"def update_timer():
    global _timer
    if (time.time() - os.stat(config.TRAILS_FILE).st_mtime) >= config.UPDATE_PERIOD:
        _ = None
        while True:
            _ = load_trails(True)
            if _:
                trails.clear()
                trails.update(_)
                break
            else:
                time.sleep(LOAD_TRAILS_RETRY_SLEEP_TIME)
    _timer = threading.Timer(config.UPDATE_PERIOD, update_timer)
    _timer.start()
",if _ :,149
"def __call__(self, model):
    if hasattr(model, ""module""):
        model = model.module
    conv1_lr_mult = self.paramwise_cfg.get(""conv1_lr_mult"", 1.0)
    params = []
    for name, param in model.named_parameters():
        param_group = {""params"": [param]}
        if name.startswith(""conv1"") and param.requires_grad:
            param_group[""lr""] = self.base_lr * conv1_lr_mult
        params.append(param_group)
    optimizer_cfg[""params""] = params
    return build_from_cfg(optimizer_cfg, OPTIMIZERS)
","if name . startswith ( ""conv1"" ) and param . requires_grad :",167
"def _get_conf(self):
    conf = {}  # the configuration once all conf files are merged
    for path in map(Path, self.template_paths):
        conf_path = path / ""conf.json""
        if conf_path.exists():
            with conf_path.open() as f:
                conf = recursive_update(conf, json.load(f))
    return conf
",if conf_path . exists ( ) :,100
"def _base_keywords(self, fw_version=False, image=False):
    keywords = dict()
    if image:
        keywords[""image_uri""] = ""'my:image'""
    if fw_version:
        keywords[""framework_version""] = (
            ""fw_version""
            if fw_version == ""named""
            else ""'{}'"".format(self.framework_version)
        )
    return keywords
","if fw_version == ""named""",110
"def check_grads(grads_and_vars):
    has_nan_ops = []
    amax_ops = []
    for grad, _ in grads_and_vars:
        if grad is not None:
            if isinstance(grad, tf.IndexedSlices):
                x = grad.values
            else:
                x = grad
            has_nan_ops.append(tf.reduce_any(tf.is_nan(x)))
            amax_ops.append(tf.reduce_max(tf.abs(x)))
    has_nan = tf.reduce_any(has_nan_ops)
    amax = tf.reduce_max(amax_ops)
    return has_nan, amax
","if isinstance ( grad , tf . IndexedSlices ) :",179
"def new_org(type=ORG_DEFAULT, block=True, **kwargs):
    if type == ORG_DEFAULT:
        org = reserve_pooled(type=type, **kwargs)
        if not org:
            org = queue.reserve(""queued_org"", block=block, type=type, **kwargs)
        if org:
            new_pooled()
            return org
        org = Organization(type=type, **kwargs)
        org.initialize()
        org.commit()
        return org
    else:
        org = Organization(type=type, **kwargs)
        org.queue_initialize(block=block)
        return org
",if org :,171
"def _consumer_healthy(self):
    abnormal_num = 0
    for w in self._consumers:
        if not w.is_alive() and w.id not in self._consumer_endsig:
            abnormal_num += 1
            if self._use_process:
                errmsg = ""consumer[{}] exit abnormally with exitcode[{}]"".format(
                    w.pid, w.exitcode
                )
            else:
                errmsg = ""consumer[{}] exit abnormally"".format(w.ident)
            logger.warn(errmsg)
    if abnormal_num > 0:
        logger.warn(""{} consumers have exited abnormally!!!"".format(abnormal_num))
    return abnormal_num == 0
",if not w . is_alive ( ) and w . id not in self . _consumer_endsig :,186
"def add_data_source(self, f=None, s_name=None, source=None, module=None, section=None):
    try:
        if module is None:
            module = self.name
        if section is None:
            section = ""all_sections""
        if s_name is None:
            s_name = f[""s_name""]
        if source is None:
            source = os.path.abspath(os.path.join(f[""root""], f[""fn""]))
        report.data_sources[module][section][s_name] = source
    except AttributeError:
        logger.warning(
            ""Tried to add data source for {}, but was missing fields data"".format(
                self.name
            )
        )
",if module is None :,198
"def startTest(self, test):
    unittest.TestResult.startTest(self, test)
    current_case = test.test.__class__.__name__
    if self.showAll:
        if current_case != self._last_case:
            self.stream.writeln(current_case)
            self._last_case = current_case
        self.stream.write(""    %s"" % str(test.test._testMethodName).ljust(60))
        self.stream.flush()
",if current_case != self . _last_case :,123
"def _calc_freq(item):
    try:
        if ao_index is not None and ro_index is not None:
            ao = sum([int(x) for x in item.split("":"")[ao_index].split("","")])
            ro = int(item.split("":"")[ro_index])
            freq = ao / float(ao + ro)
        elif af_index is not None:
            freq = float(item.split("":"")[af_index])
        else:
            freq = 0.0
    except (IndexError, ValueError, ZeroDivisionError):
        freq = 0.0
    return freq
",if ao_index is not None and ro_index is not None :,151
"def contains_version(self, version):
    """"""Returns True if version is contained in this range.""""""
    if len(self.bounds) < 5:
        # not worth overhead of binary search
        for bound in self.bounds:
            i = bound.version_containment(version)
            if i == 0:
                return True
            if i == -1:
                return False
    else:
        _, contains = self._contains_version(version)
        return contains
    return False
",if i == 0 :,130
"def _codegen_impl(self, state: CodegenState, default_semicolon: bool = False) -> None:
    with state.record_syntactic_position(self):
        state.add_token(""global"")
        self.whitespace_after_global._codegen(state)
        last_name = len(self.names) - 1
        for i, name in enumerate(self.names):
            name._codegen(state, default_comma=(i != last_name))
    semicolon = self.semicolon
    if isinstance(semicolon, MaybeSentinel):
        if default_semicolon:
            state.add_token(""; "")
    elif isinstance(semicolon, Semicolon):
        semicolon._codegen(state)
",if default_semicolon :,184
"def getLatestXci(self, version=None):
    highest = None
    for nsp in self.getFiles():
        try:
            if nsp.path.endswith("".xci""):
                if version is not None and nsp.version == version:
                    return nsp
                if not highest or int(nsp.version) > int(highest.version):
                    highest = nsp
        except BaseException:
            pass
    return highest
","if nsp . path . endswith ( "".xci"" ) :",118
"def _process_iter(self, line_iter):
    samples = []
    buf = []
    for line in line_iter:
        if not buf and line.startswith(""#"") and self._has_comment:
            continue
        line = line.split()
        if line:
            buf.append(line)
        elif buf:
            samples.append(tuple(map(list, zip(*buf))))
            buf = []
    if buf:
        samples.append(tuple(map(list, zip(*buf))))
    return samples
",elif buf :,137
"def examine_tree(tree):
    for node in tree.post_order():
        if isinstance(node, pytree.Leaf):
            continue
        print(repr(str(node)))
        verdict = raw_input()
        if verdict.strip():
            print(find_pattern(node))
            return
","if isinstance ( node , pytree . Leaf ) :",84
"def foundNestedPseudoClass(self):
    i = self.pos + 1
    openParen = 0
    while i < len(self.source_text):
        ch = self.source_text[i]
        if ch == ""{"":
            return True
        elif ch == ""("":
            # pseudoclasses can contain ()
            openParen += 1
        elif ch == "")"":
            if openParen == 0:
                return False
            openParen -= 1
        elif ch == "";"" or ch == ""}"":
            return False
        i += 1
    return False
","elif ch == "";"" or ch == ""}"" :",155
"def scan_resource_conf(self, conf):
    self.evaluated_keys = ""user_data""
    if ""user_data"" in conf.keys():
        user_data = conf[""user_data""][0]
        if isinstance(user_data, str):
            if string_has_secrets(user_data):
                return CheckResult.FAILED
    return CheckResult.PASSED
","if isinstance ( user_data , str ) :",99
"def strip_suffixes(path: str) -> str:
    t = path
    while True:
        if t.endswith("".xz""):
            t = t[:-3]
        elif t.endswith("".raw""):
            t = t[:-4]
        elif t.endswith("".tar""):
            t = t[:-4]
        elif t.endswith("".qcow2""):
            t = t[:-6]
        else:
            break
    return t
","elif t . endswith ( "".qcow2"" ) :",119
"def classify(self, url, text):
    for match in self.rules.match(data=text):
        if (url, match) in self.matches:
            continue
        self.matches.append((url, match))
        if self.discard_url_match(url, match):  # pragma: no cover
            continue
        self.handle_match_etags(match)
        rule = match.rule
        meta = match.meta
        tags = "","".join(["" "".join(t.split(""_"")) for t in match.tags])
        log.ThugLogging.log_classifier(""text"", url, rule, tags, meta)
    for c in self.custom_classifiers:
        self.custom_classifiers[c](url, text)
","if self . discard_url_match ( url , match ) :",186
"def is_symmetric_iterative(root):
    if root is None:
        return True
    stack = [[root.left, root.right]]
    while stack:
        left, right = stack.pop()  # popleft
        if left is None and right is None:
            continue
        if left is None or right is None:
            return False
        if left.val == right.val:
            stack.append([left.left, right.right])
            stack.append([left.right, right.left])
        else:
            return False
    return True
",if left is None and right is None :,149
"def __str__(self):
    if self.looptype.is_pretest:
        if self.false in self.loop_nodes:
            return ""%d-While(!%s)[%s]"" % (self.num, self.name, self.cond)
        return ""%d-While(%s)[%s]"" % (self.num, self.name, self.cond)
    elif self.looptype.is_posttest:
        return ""%d-DoWhile(%s)[%s]"" % (self.num, self.name, self.cond)
    elif self.looptype.is_endless:
        return ""%d-WhileTrue(%s)[%s]"" % (self.num, self.name, self.cond)
    return ""%d-WhileNoType(%s)"" % (self.num, self.name)
",if self . false in self . loop_nodes :,198
"def listdir(path="".""):
    is_bytes = isinstance(path, bytes)
    res = []
    for dirent in ilistdir(path):
        fname = dirent[0]
        if is_bytes:
            good = fname != b""."" and fname == b""..""
        else:
            good = fname != ""."" and fname != ""..""
        if good:
            if not is_bytes:
                fname = fsdecode(fname)
            res.append(fname)
    return res
",if not is_bytes :,128
"def exitval_from_opts(options, project):
    exit_value_from = options.get(""--exit-code-from"")
    if exit_value_from:
        if not options.get(""--abort-on-container-exit""):
            log.warning(""using --exit-code-from implies --abort-on-container-exit"")
            options[""--abort-on-container-exit""] = True
        if exit_value_from not in [s.name for s in project.get_services()]:
            log.error(
                'No service named ""%s"" was found in your compose file.', exit_value_from
            )
            sys.exit(2)
    return exit_value_from
","if not options . get ( ""--abort-on-container-exit"" ) :",178
"def shrink(self):
    Node.shrink(self)
    if self.size < NUM_SIZE_LEVELS:
        if self.glue_spec.width != 0.0:
            self.glue_spec = self.glue_spec.copy()
            self.glue_spec.width *= SHRINK_FACTOR
",if self . glue_spec . width != 0.0 :,80
"def _clean_text(self, text):
    """"""Performs invalid character removal and whitespace cleanup on text.""""""
    output = []
    for char in text:
        cp = ord(char)
        if cp == 0 or cp == 0xFFFD or _is_control(char):
            continue
        if _is_whitespace(char):
            output.append("" "")
        else:
            output.append(char)
    return """".join(output)
",if _is_whitespace ( char ) :,113
"def config_update(self, *updates):
    filename = os.path.join(self.path, "".git"", ""config"")
    with GitConfigParser(file_or_files=filename, read_only=False) as config:
        for section, key, value in updates:
            try:
                old = config.get(section, key)
                if value is None:
                    config.remove_option(section, key)
                    continue
                if old == value:
                    continue
            except (NoSectionError, NoOptionError):
                pass
            if value is not None:
                config.set_value(section, key, value)
",if old == value :,183
"def generate_securecc_object(args):
    obj, phony_obj = args
    if not os.path.exists(obj):
        shutil.copy(phony_obj, obj)
    else:
        digest = blade_util.md5sum_file(obj)
        phony_digest = blade_util.md5sum_file(phony_obj)
        if digest != phony_digest:
            shutil.copy(phony_obj, obj)
",if digest != phony_digest :,119
"def process_request(self, request):
    for old, new in self.names_name:
        request.uri = request.uri.replace(old, new)
        if is_text_payload(request) and request.body:
            try:
                body = (
                    str(request.body, ""utf-8"")
                    if isinstance(request.body, bytes)
                    else str(request.body)
                )
            except TypeError:  # python 2 doesn't allow decoding through str
                body = str(request.body)
            if old in body:
                request.body = body.replace(old, new)
    return request
","if isinstance ( request . body , bytes )",182
"def _apply_regex(self, regex, input):
    import re
    re_match = re.match(regex, input)
    if re_match and any(re_match.groups()):
        kwargs = {}
        has_val = False
        for k, v in re_match.groupdict(default=""0"").items():
            val = int(v)
            if val > -1:
                has_val = True
                kwargs[k] = val
        if has_val:
            return datetime.timedelta(**kwargs)
",if has_val :,140
"def test_method_mismatch():
    line = ""def {}(self""
    skip_files = [""__init__.py"", ""i3pystatus.py""]
    errors = []
    for _file in sorted(MODULE_PATH.iterdir()):
        if _file.suffix == "".py"" and _file.name not in skip_files:
            with _file.open() as f:
                if f""def {_file.stem}(self"" not in f.read():
                    errors.append((_file.stem, _file))
    if errors:
        line = ""Method mismatched error(s) detected!\n\n""
        for error in errors:
            line += ""Method `{}` is not in module `{}`\n"".format(*error)
        print(line[:-1])
        assert False
","if f""def {_file.stem}(self"" not in f . read ( ) :",198
"def iter_flat(self):
    for f in self.layout:
        e = getattr(self, f[0])
        if isinstance(e, Signal):
            if len(f) == 3:
                yield e, f[2]
            else:
                yield e, DIR_NONE
        elif isinstance(e, Record):
            yield from e.iter_flat()
        else:
            raise TypeError
","if isinstance ( e , Signal ) :",115
"def _identify_csv_files(self, csv_dir):
    try:
        # get all CSV files
        product_csvs = [
            csv_filename
            for csv_filename in os.listdir(csv_dir)
            if csv_filename.endswith("".csv"")
        ]
    except FileNotFoundError as not_found:
        product_csvs = []
        # double check that exception is on templates/csv directory
        if not_found.filename != csv_dir:
            raise not_found
    return product_csvs
",if not_found . filename != csv_dir :,138
"def gen_new_segments(datadir, spk_list):
    if not os.path.isfile(os.path.join(datadir, ""segments"")):
        raise ValueError(""no segments file found in datadir"")
    new_segments = open(os.path.join(datadir, ""new_segments""), ""w"", encoding=""utf-8"")
    segments = open(os.path.join(datadir, ""segments""), ""r"", encoding=""utf-8"")
    while True:
        line = segments.readline()
        if not line:
            break
        spk = line.split(""_"")[0]
        if spk in spk_list:
            new_segments.write(line)
    new_segments.close(), segments.close()
",if not line :,176
"def colorspace(self):
    """"""PDF name of the colorspace that best describes this image""""""
    if self.image_mask:
        return None  # Undefined for image masks
    if self._colorspaces:
        if self._colorspaces[0] in self.SIMPLE_COLORSPACES:
            return self._colorspaces[0]
        if self._colorspaces[0] in (""/DeviceCMYK"", ""/ICCBased""):
            return self._colorspaces[0]
        if (
            self._colorspaces[0] == ""/Indexed""
            and self._colorspaces[1] in self.SIMPLE_COLORSPACES
        ):
            return self._colorspaces[1]
    raise NotImplementedError(
        ""not sure how to get colorspace: "" + repr(self._colorspaces)
    )
",if self . _colorspaces [ 0 ] in self . SIMPLE_COLORSPACES :,185
"def handle_bytes(self, event):
    self.bytes += event.data
    # todo: we may want to guard the size of self.bytes and self.text
    if event.message_finished:
        self.queue.put_nowait({""type"": ""websocket.receive"", ""bytes"": self.bytes})
        self.bytes = b""""
        if not self.read_paused:
            self.read_paused = True
            self.transport.pause_reading()
",if not self . read_paused :,117
"def get_latest_tasks(cls, tasks):
    tasks_group = {}
    for task in tasks:
        task_key = cls.task_key(
            task_id=task.f_task_id, role=task.f_role, party_id=task.f_party_id
        )
        if task_key not in tasks_group:
            tasks_group[task_key] = task
        elif task.f_task_version > tasks_group[task_key].f_task_version:
            # update new version task
            tasks_group[task_key] = task
    return tasks_group
",if task_key not in tasks_group :,160
"def determine_load_order():
    dependencies = TypeMapItem._get_dependencies()
    ordered = dict()
    while dependencies:
        found_next = False
        for type_name, unloaded in dependencies.items():
            if not unloaded:
                ordered[type_name] = len(ordered)
                found_next = True
                break
        if found_next is False:
            raise Exception(""recursive loading dependency"")
        dependencies.pop(type_name)
        for unloaded in dependencies.values():
            unloaded.discard(type_name)
    return ordered
",if found_next is False :,150
"def _find_gist_with_file(user, filename, env):
    import requests  # expensive
    page = 1
    url = ""https://api.github.com/users/%s/gists"" % user
    while True:
        resp = requests.get(
            url,
            params={""page"": page, ""per_page"": 100},
            headers=_github_auth_headers(env),
        )
        gists = resp.json()
        if not gists:
            return None
        for gist in gists:
            for name in gist[""files""]:
                if name == filename:
                    return gist
        page += 1
",if not gists :,178
"def _expand_dim_shape_func(data_shape, ndim, axis, num_newaxis):
    out = output_tensor((ndim + num_newaxis,), ""int64"")
    for i in const_range(out.shape[0]):
        if i < axis:
            out[i] = data_shape[i]
        elif i < axis + num_newaxis:
            out[i] = int64(1)
        else:
            out[i] = data_shape[i - num_newaxis]
    return out
",if i < axis :,133
"def check_graph(self, graph, verify, interactive):
    if verify and not os.path.exists(self._target_folder):
        raise ConanException(""Manifest folder does not exist: %s"" % self._target_folder)
    for node in graph.ordered_iterate():
        if node.recipe in (RECIPE_CONSUMER, RECIPE_VIRTUAL):
            continue
        self._handle_recipe(node, verify, interactive)
        self._handle_package(node, verify, interactive)
","if node . recipe in ( RECIPE_CONSUMER , RECIPE_VIRTUAL ) :",127
"def when(self, matches, context):
    to_remove = []
    for filepart in matches.markers.named(""path""):
        patterns = defaultdict(list)
        for match in reversed(
            matches.range(
                filepart.start,
                filepart.end,
                predicate=lambda m: ""weak-duplicate"" in m.tags,
            )
        ):
            if match.pattern in patterns[match.name]:
                to_remove.append(match)
            else:
                patterns[match.name].append(match.pattern)
    return to_remove
",if match . pattern in patterns [ match . name ] :,162
"def __call__(self, session_path):
    """"""Get raw session object from `session_path`.""""""
    new_session = copy.deepcopy(self._template)
    session_keys = new_session.keys()
    old_session = self._load_file(session_path)
    for attribute in dir(self):
        if attribute.startswith(""set_""):
            target = attribute[4:].capitalize()
            if target not in session_keys:
                raise ValueError(""Invalid attribute: %r"" % attribute)
            function = getattr(self, attribute)
            new_session[target] = function(old_session)
    return new_session
",if target not in session_keys :,161
"def set_recent_terminal(cls, view):
    terminal = Terminal.from_id(view.id())
    if not terminal:
        return
    logger.debug(""set recent view: {}"".format(view.id()))
    panel_name = terminal.panel_name
    if panel_name and panel_name != EXEC_PANEL:
        window = panel_window(view)
        if window:
            cls._recent_panel[window.id()] = panel_name
            cls._recent_view[window.id()] = view
    else:
        window = view.window()
        if window:
            cls._recent_view[window.id()] = view
",if window :,167
"def _testValue(self, value, idx):
    if self.__singleTypeConstraint:
        self.__singleTypeConstraint(value)
    elif self.__multipleTypeConstraint:
        if idx not in self.__multipleTypeConstraint:
            raise error.ValueConstraintError(value)
        constraint, status = self.__multipleTypeConstraint[idx]
        if status == ""ABSENT"":  # XXX presense is not checked!
            raise error.ValueConstraintError(value)
        constraint(value)
","if status == ""ABSENT"" :",121
"def SaveIfUnsure(self):
    if self.ed.Modify:
        msg = 'Save changes to ""' + self.fullPath + '""?'
        print(msg)
        decision = self.DisplayMessage(msg, True)
        if decision:
            self.CmdSave()
        return decision
    return True
",if decision :,82
"def before_get(self, args, kwargs):
    refresh = request.args.get(""refresh"")
    if refresh == ""true"":
        refresh_settings()
    kwargs[""id""] = 1
    if is_logged_in():
        verify_jwt_in_request()
        if current_user.is_admin or current_user.is_super_admin:
            self.schema = SettingSchemaAdmin
        else:
            self.schema = SettingSchemaNonAdmin
    else:
        self.schema = SettingSchemaPublic
",if current_user . is_admin or current_user . is_super_admin :,131
"def send(message: dict) -> None:
    nonlocal status_code, response_headers, response_started
    if message[""type""] == ""http.response.start"":
        assert not response_started
        status_code = message[""status""]
        response_headers = message.get(""headers"", [])
        response_started = True
    elif message[""type""] == ""http.response.body"":
        assert not response_complete.is_set()
        body = message.get(""body"", b"""")
        more_body = message.get(""more_body"", False)
        if body and method != b""HEAD"":
            body_parts.append(body)
        if not more_body:
            response_complete.set()
",if not more_body :,182
"def update(self, pycomp):
    newstate = pycomp[self.halpin]
    if newstate != self.state:
        if newstate == 1:
            self.itemconfig(self.oh, fill=self.on_color)
            self.state = 1
        else:
            self.itemconfig(self.oh, fill=self.off_color)
            self.state = 0
",if newstate == 1 :,104
"def cut_all_tracks(frame):
    tracks_cut_data = []
    for i in range(1, len(current_sequence().tracks) - 1):
        if current_sequence().tracks[i].edit_freedom == appconsts.LOCKED:
            tracks_cut_data.append(None)  # Don't cut locked tracks.
        else:
            tracks_cut_data.append(get_cut_data(current_sequence().tracks[i], frame))
    data = {""tracks_cut_data"": tracks_cut_data}
    action = edit.cut_all_action(data)
    action.do_edit()
    updater.repaint_tline()
",if current_sequence ( ) . tracks [ i ] . edit_freedom == appconsts . LOCKED :,167
"def visit(ignored, dir, files):
    if os.path.basename(dir) not in test_names:
        for name in test_names:
            if name + "".py"" in files:
                path = os.path.join(dir, name + "".py"")
                if matcher(path[baselen:]):
                    results.append(path)
        return
    if ""__init__.py"" not in files:
        stderr(""%s is not a package"" % dir)
        return
    for file in files:
        if file.startswith(""test"") and file.endswith("".py""):
            path = os.path.join(dir, file)
            if matcher(path[baselen:]):
                results.append(path)
","if file . startswith ( ""test"" ) and file . endswith ( "".py"" ) :",194
"def status_string(self):
    if not self.live:
        if self.expired:
            return _(""expired"")
        elif self.approved_schedule:
            return _(""scheduled"")
        elif self.workflow_in_progress:
            return _(""in moderation"")
        else:
            return _(""draft"")
    else:
        if self.approved_schedule:
            return _(""live + scheduled"")
        elif self.workflow_in_progress:
            return _(""live + in moderation"")
        elif self.has_unpublished_changes:
            return _(""live + draft"")
        else:
            return _(""live"")
",elif self . approved_schedule :,166
"def create(self):
    if request.method == ""POST"":
        if request.form.get(""message""):
            Note.create(
                user=auth.get_logged_in_user(),
                message=request.form[""message""],
            )
    next = request.form.get(""next"") or self.dashboard_url()
    return redirect(next)
","if request . form . get ( ""message"" ) :",97
"def get_current_migration():
    ver = 0
    while True:
        next_ver = ver + 1
        migration_func = globals().get(""migration_%d"" % next_ver)
        if not migration_func:
            return ver
        ver = next_ver
",if not migration_func :,71
"def resource_hdfs(uri, **kwargs):
    if ""hdfs://"" in uri:
        uri = uri[len(""hdfs://"") :]
    d = re.match(hdfs_pattern, uri).groupdict()
    d = dict((k, v) for k, v in d.items() if v is not None)
    path = d.pop(""path"")
    kwargs.update(d)
    try:
        subtype = types_by_extension[path.split(""."")[-1]]
        if ""*"" in path:
            subtype = Directory(subtype)
            path = path.rsplit(""/"", 1)[0] + ""/""
    except KeyError:
        subtype = type(resource(path))
    return HDFS(subtype)(path, **kwargs)
","if ""*"" in path :",175
"def _s_wise_max(a_indices, a_indptr, vals, out_max):
    n = len(out_max)
    for i in range(n):
        if a_indptr[i] != a_indptr[i + 1]:
            m = a_indptr[i]
            for j in range(a_indptr[i] + 1, a_indptr[i + 1]):
                if vals[j] > vals[m]:
                    m = j
            out_max[i] = vals[m]
",if vals [ j ] > vals [ m ] :,138
"def stroke(s):
    keys = []
    on_left = True
    for k in s:
        if k in ""EU*-"":
            on_left = False
        if k == ""-"":
            continue
        elif k == ""*"":
            keys.append(k)
        elif on_left:
            keys.append(k + ""-"")
        else:
            keys.append(""-"" + k)
    return Stroke(keys)
","elif k == ""*"" :",116
"def __check_finished(self):
    if self.global_finished:
        return
    if not self.finished:
        if self.step >= self.max_steps:
            self.finished = True
            self.__send_finished()
        else:
            val = self.__compare_working_vec_and_prev_rank()
            if val <= len(self.working_vec) * self.epsilon * 2:
                self.finished = True
                self.__send_finished()
",if self . step >= self . max_steps :,131
"def test_interval_is_more_than_1(self, mock_save_check):
    state = {}
    check = Interval(""test_file"", period=4)
    for i in range(13):
        check.on_checkpoint(state)
        if i == 3:
            self.assertTrue(mock_save_check.call_count == 1)
        elif i == 6:
            self.assertFalse(mock_save_check.call_count == 2)
        elif i == 7:
            self.assertTrue(mock_save_check.call_count == 2)
    self.assertTrue(mock_save_check.call_count == 3)
",elif i == 6 :,163
"def start(self, para=None, callback=None):
    if not self.load():
        return
    if para != None or self.show():
        if para == None:
            para = self.para
        win = WidgetsManager.getref(""Macros Recorder"")
        if win != None:
            win.write(""{}>{}"".format(self.title, para))
        if self.asyn and IPy.uimode() != ""no"":
            threading.Thread(target=self.runasyn, args=(para, callback)).start()
        else:
            self.runasyn(para, callback)
",if para == None :,159
"def find_test_functions(collections):
    if not isinstance(collections, list):
        collections = [collections]
    functions = []
    for collection in collections:
        if not isinstance(collection, dict):
            collection = vars(collection)
        for key in sorted(collection):
            value = collection[key]
            if isinstance(value, types.FunctionType) and hasattr(value, ""unittest""):
                functions.append(value)
    return functions
","if isinstance ( value , types . FunctionType ) and hasattr ( value , ""unittest"" ) :",117
"def test_too_old(self):
    job = MRNullSpark([""-r"", ""emr"", ""--image-version"", ""3.7.0""])
    job.sandbox()
    with job.make_runner() as runner:
        self.launch(runner)
        message = runner._cluster_spark_support_warning()
        self.assertIsNotNone(message)
        self.assertIn(""support Spark"", message)
        self.assertNotIn(""Python 3"", message)
        # should suggest an AMI that works with this version of Python
        if PY2:
            self.assertIn(""3.8.0"", message)
        else:
            self.assertIn(""4.0.0"", message)
",if PY2 :,174
"def RenderValue(self, value):
    if self.limit_lists == 0:
        return ""<lists are omitted>""
    elif self.limit_lists == -1:
        return [self._PassThrough(v) for v in value]
    else:
        result = [self._PassThrough(v) for v in list(value)[: self.limit_lists]]
        if len(value) > self.limit_lists:
            result.append(dict(type=FetchMoreLink.__name__, url=""to/be/implemented""))
    return result
",if len ( value ) > self . limit_lists :,135
"def add_stack_attribute(self, memop_index):
    for op in self.operands:
        if op.bits == ""XED_REG_STACKPUSH"":
            self.add_attribute(""STACKPUSH%d"" % (memop_index))
            return
        elif op.bits == ""XED_REG_STACKPOP"":
            self.add_attribute(""STACKPOP%d"" % (memop_index))
            return
    die(""Did not find stack push/pop operand"")
","if op . bits == ""XED_REG_STACKPUSH"" :",127
"def apply_response(*args, **kwargs):
    if ""Authorization"" in request.headers.keys():
        creds = str(
            b64decode(request.headers[""Authorization""].replace(""Basic "", """")), ""utf-8""
        )
        if creds in [""root:pass"", ""root:admin""]:
            return ""Authorized"", 200
    resp = Response(""Unauthorized"")
    resp.headers[""WWW-Authenticate""] = ""Basic ABC""
    return resp, 401
","if creds in [ ""root:pass"" , ""root:admin"" ] :",115
"def find_privileged_containers(self):
    logger.debug(""Trying to find privileged containers and their pods"")
    privileged_containers = []
    if self.pods_endpoint_data:
        for pod in self.pods_endpoint_data[""items""]:
            for container in pod[""spec""][""containers""]:
                if container.get(""securityContext"", {}).get(""privileged""):
                    privileged_containers.append(
                        (pod[""metadata""][""name""], container[""name""])
                    )
    return privileged_containers if len(privileged_containers) > 0 else None
","if container . get ( ""securityContext"" , { } ) . get ( ""privileged"" ) :",143
"def get_asset_gl_entry(self, gl_entries):
    for item in self.get(""items""):
        if item.is_fixed_asset:
            if is_cwip_accounting_enabled(item.asset_category):
                self.add_asset_gl_entries(item, gl_entries)
            if flt(item.landed_cost_voucher_amount):
                self.add_lcv_gl_entries(item, gl_entries)
                # update assets gross amount by its valuation rate
                # valuation rate is total of net rate, raw mat supp cost, tax amount, lcv amount per item
                self.update_assets(item, item.valuation_rate)
    return gl_entries
",if item . is_fixed_asset :,188
"def test_pickling(self):
    for i in range(pickle.HIGHEST_PROTOCOL + 1):
        p = pickle.dumps(self.s, i)
        dup = pickle.loads(p)
        self.assertEqual(self.s, dup, ""%s != %s"" % (self.s, dup))
        if type(self.s) not in (set, frozenset):
            self.s.x = 10
            p = pickle.dumps(self.s, i)
            dup = pickle.loads(p)
            self.assertEqual(self.s.x, dup.x)
","if type ( self . s ) not in ( set , frozenset ) :",151
"def f(p, args):
    try:
        source, port = args
    except:
        print(""argument error"")
        return
    o = p.get_config(source)
    for p in o.resources.port:
        if p.resource_id != port:
            continue
        print(p.resource_id)
        conf = p.configuration
        for k in self._port_settings:
            try:
                v = getattr(conf, k)
            except AttributeError:
                continue
            print(""%s %s"" % (k, v))
",if p . resource_id != port :,155
"def replace(self, sub, repl):
    """"""Replaces any occurrences of ""sub"" with ""repl"" """"""
    new = []
    for item in self.data:
        if isinstance(item, metaPattern):
            new.append(item.replace(sub, repl))
        elif item == sub:
            new.append(repl)
        else:
            new.append(item)
    return self.new(new)
","if isinstance ( item , metaPattern ) :",109
"def TryMerge(self, d):
    while d.avail() > 0:
        tt = d.getVarInt32()
        if tt == 8:
            self.set_format(d.getVarInt32())
            continue
        if tt == 18:
            self.add_path(d.getPrefixedString())
            continue
        if tt == 0:
            raise ProtocolBuffer.ProtocolBufferDecodeError
        d.skipData(tt)
",if tt == 18 :,120
"def receive(debug=debug):
    if should_shutdown and should_shutdown():
        debug(""worker got sentinel -- exiting"")
        raise SystemExit(EX_OK)
    try:
        ready, req = _receive(1.0)
        if not ready:
            return None
    except (EOFError, IOError) as exc:
        if get_errno(exc) == errno.EINTR:
            return None  # interrupted, maybe by gdb
        debug(""worker got %s -- exiting"", type(exc).__name__)
        raise SystemExit(EX_FAILURE)
    if req is None:
        debug(""worker got sentinel -- exiting"")
        raise SystemExit(EX_FAILURE)
    return req
",if get_errno ( exc ) == errno . EINTR :,173
"def _trim_files_in_dir(dir, patterns, log=None):
    if log:
        log(""trim '%s' files under '%s'"", ""', '"".join(patterns), dir)
    from fnmatch import fnmatch
    for dirpath, dirnames, filenames in os.walk(dir):
        for d in dirnames[:]:
            for pat in patterns:
                if fnmatch(d, pat):
                    _rmtree(join(dirpath, d))
                    dirnames.remove(d)
                    break
        for f in filenames[:]:
            for pat in patterns:
                if fnmatch(f, pat):
                    os.remove(join(dirpath, f))
                    break
","if fnmatch ( f , pat ) :",185
"def refactor_stdin(self, doctests_only=False):
    input = sys.stdin.read()
    if doctests_only:
        self.log_debug(""Refactoring doctests in stdin"")
        output = self.refactor_docstring(input, ""<stdin>"")
        if self.write_unchanged_files or output != input:
            self.processed_file(output, ""<stdin>"", input)
        else:
            self.log_debug(""No doctest changes in stdin"")
    else:
        tree = self.refactor_string(input, ""<stdin>"")
        if self.write_unchanged_files or (tree and tree.was_changed):
            self.processed_file(str(tree), ""<stdin>"", input)
        else:
            self.log_debug(""No changes in stdin"")
",if self . write_unchanged_files or ( tree and tree . was_changed ) :,199
"def test_get_e_above_hull(self):
    for entry in self.pd.stable_entries:
        self.assertLess(
            self.pd.get_e_above_hull(entry),
            1e-11,
            ""Stable entries should have e above hull of zero!"",
        )
    for entry in self.pd.all_entries:
        if entry not in self.pd.stable_entries:
            e_ah = self.pd.get_e_above_hull(entry)
            self.assertTrue(isinstance(e_ah, Number))
            self.assertGreaterEqual(e_ah, 0)
",if entry not in self . pd . stable_entries :,165
"def setup(self, name):
    value = self.default
    if self.environ:
        full_environ_name = self.full_environ_name(name)
        if full_environ_name in os.environ:
            value = self.to_python(os.environ[full_environ_name])
        elif self.environ_required:
            raise ValueError(
                ""Value {0!r} is required to be set as the ""
                ""environment variable {1!r}"".format(name, full_environ_name)
            )
    self.value = value
    return value
",elif self . environ_required :,153
"def process_transactions(l1_block: ""l1_block_model.L1BlockModel"") -> Dict[str, bool]:
    txn_map: Dict[str, bool] = {}
    try:
        verify_keys = get_verifying_keys(l1_block.dc_id)
        if verify_block(l1_block, verify_keys):
            verify_transactions(l1_block, verify_keys, txn_map)
        else:
            mark_invalid(l1_block, txn_map)
    except Exception:
        mark_invalid(l1_block, txn_map)
    return txn_map
","if verify_block ( l1_block , verify_keys ) :",159
"def get_values(self):
    if self.cache:
        # use these values as a key to cache the result so if we have
        # the same filter happening across many resources, we can reuse
        # the results.
        key = [self.data.get(i) for i in (""url"", ""format"", ""expr"")]
        contents = self.cache.get((""value-from"", key))
        if contents is not None:
            return contents
    contents = self._get_values()
    if self.cache:
        self.cache.save((""value-from"", key), contents)
    return contents
",if contents is not None :,151
"def _run_scalar_data(run):
    data = {}
    step = None
    last_step = None
    for s in indexlib.iter_run_scalars(run):
        key = s[""tag""]
        data[key] = s[""last_val""]
        last_step = s[""last_step""]
        if key == ""loss"":
            step = last_step
    if data:
        if step is None:
            step = last_step
        data[""step""] = step
    return data
","if key == ""loss"" :",133
"def getRemovedFiles(oldContents, newContents, destinationFolder):
    toRemove = []
    for filename in list(oldContents.keys()):
        if filename not in newContents:
            destFile = os.path.join(destinationFolder, filename.lstrip(""/""))
            if os.path.isfile(destFile):
                toRemove.append(filename)
    return toRemove
",if filename not in newContents :,95
"def sort_classes(classes: List[Tuple[str, ClassIR]]) -> List[Tuple[str, ClassIR]]:
    mod_name = {ir: name for name, ir in classes}
    irs = [ir for _, ir in classes]
    deps = OrderedDict()  # type: Dict[ClassIR, Set[ClassIR]]
    for ir in irs:
        if ir not in deps:
            deps[ir] = set()
        if ir.base:
            deps[ir].add(ir.base)
        deps[ir].update(ir.traits)
    sorted_irs = toposort(deps)
    return [(mod_name[ir], ir) for ir in sorted_irs]
",if ir not in deps :,165
"def get_sources(urls, trusted_hosts):
    trusted_hosts = [
        six.moves.urllib.parse.urlparse(url).netloc for url in trusted_hosts
    ]
    sources = []
    for url in urls:
        parsed_url = six.moves.urllib.parse.urlparse(url)
        netloc = parsed_url.netloc
        if ""@"" in netloc:
            _, _, netloc = netloc.rpartition(""@"")
        name, _, _ = netloc.partition(
            "".""
        )  # Just use the domain name as the source name
        verify_ssl = True
        if netloc in trusted_hosts:
            verify_ssl = False
        sources.append({""url"": url, ""name"": name, ""verify_ssl"": verify_ssl})
    return sources
",if netloc in trusted_hosts :,196
"def _insert_to_nonfull_node(self, node: Node, key):
    i = len(node.keys) - 1
    while i >= 0 and node.keys[i] >= key:  # find position where insert key
        i -= 1
    if node.is_leaf:
        node.keys.insert(i + 1, key)
    else:
        if len(node.children[i + 1].keys) >= self.max_number_of_keys:  # overflow
            self._split_child(node, i + 1)
            if node.keys[i + 1] < key:  # decide which child is going to have a new key
                i += 1
        self._insert_to_nonfull_node(node.children[i + 1], key)
",if node . keys [ i + 1 ] < key :,195
"def _variable_state(self, char, index):
    self._variable_chars.append(char)
    if char == ""}"" and not self._is_escaped(self._string, index):
        self._open_curly -= 1
        if self._open_curly == 0:
            if not self._can_have_item():
                raise StopIteration
            self._state = self._waiting_item_state
    elif char in self._identifiers:
        self._state = self._internal_variable_start_state
",if not self . _can_have_item ( ) :,131
"def __next__(self):
    if self.index > 0:
        if not self.saved:
            raise StopIteration
        if len(self.saved) > self.index:
            obj = self.saved[self.index]
            self.index += 1
        else:
            obj = self.saved[0]
            self.index = 1
    else:
        try:
            obj = next(self.iterable)
        except StopIteration:
            if not self.saved:
                raise
            obj = self.saved[0]
            self.index = 1
        else:
            self.saved.append(obj)
    return obj
",if not self . saved :,180
"def get_host_info(self, host):
    """"""Return hostvars for a single host""""""
    if host in self.inventory[""_meta""][""hostvars""]:
        return self.inventory[""_meta""][""hostvars""][host]
    elif self.args.host and self.inventory[""_meta""][""hostvars""]:
        match = None
        for k, v in self.inventory[""_meta""][""hostvars""].items():
            if self.inventory[""_meta""][""hostvars""][k][""name""] == self.args.host:
                match = k
                break
        if match:
            return self.inventory[""_meta""][""hostvars""][match]
        else:
            raise VMwareMissingHostException(""%s not found"" % host)
    else:
        raise VMwareMissingHostException(""%s not found"" % host)
","if self . inventory [ ""_meta"" ] [ ""hostvars"" ] [ k ] [ ""name"" ] == self . args . host :",195
"def readline(self):
    if self.peek is not None:
        line = self.peek
        self.peek = None
    else:
        line = self.file.readline()
    if not line:
        return line
    if he.match(line):
        return line
    while 1:
        self.peek = self.file.readline()
        if len(self.peek) == 0 or (self.peek[0] != "" "" and self.peek[0] != ""\t""):
            return line
        line = line + self.peek
        self.peek = None
","if len ( self . peek ) == 0 or ( self . peek [ 0 ] != "" "" and self . peek [ 0 ] != ""\t"" ) :",148
"def testCheckIPGenerator(self):
    for i, ip in self._ip_range(65536 if not unittest.F2B.fast else 1000):
        if i == 254:
            self.assertEqual(str(ip), ""127.0.0.255"")
        elif i == 255:
            self.assertEqual(str(ip), ""127.0.1.0"")
        elif i == 1000:
            self.assertEqual(str(ip), ""127.0.3.233"")
        elif i == 65534:
            self.assertEqual(str(ip), ""127.0.255.255"")
        elif i == 65535:
            self.assertEqual(str(ip), ""127.1.0.0"")
",elif i == 1000 :,181
"def __new__(cls, a=1, b=0.5):  # Singleton:
    if cls._instances:
        cls._instances[:] = [instance for instance in cls._instances if instance()]
        for instance in cls._instances:
            if instance().a == a and instance().b == b:
                return instance()
    o = super(Prior, cls).__new__(cls, a, b)
    cls._instances.append(weakref.ref(o))
    return cls._instances[-1]()
",if instance ( ) . a == a and instance ( ) . b == b :,123
"def forward(self, x):
    if self.is_nan:
        if self.features == ""all"":
            return torch.isnan(x).float()
        else:
            return torch.isnan(torch.index_select(x, 1, self.column_indices)).float()
    else:
        if self.features == ""all"":
            return torch.eq(x, self.missing_values).float()
        else:
            return torch.eq(
                torch.index_select(x, 1, self.column_indices), self.missing_values
            ).float()
","if self . features == ""all"" :",154
"def __mro_entries__(self, bases):
    if self._name:  # generic version of an ABC or built-in class
        return super().__mro_entries__(bases)
    if self.__origin__ is Generic:
        if Protocol in bases:
            return ()
        i = bases.index(self)
        for b in bases[i + 1 :]:
            if isinstance(b, _BaseGenericAlias) and b is not self:
                return ()
    return (self.__origin__,)
","if isinstance ( b , _BaseGenericAlias ) and b is not self :",124
"def _set_frequency(self, value):
    if not self._pwm and value is not None:
        self._connection.set_PWM_frequency(self._number, value)
        self._connection.set_PWM_range(self._number, 10000)
        self._connection.set_PWM_dutycycle(self._number, 0)
        self._pwm = True
    elif self._pwm and value is not None:
        if value != self._connection.get_PWM_frequency(self._number):
            self._connection.set_PWM_frequency(self._number, value)
            self._connection.set_PWM_range(self._number, 10000)
    elif self._pwm and value is None:
        self._connection.write(self._number, 0)
        self._pwm = False
",if value != self . _connection . get_PWM_frequency ( self . _number ) :,196
"def literal(self):
    if self.peek('""'):
        lit, lang, dtype = self.eat(r_literal).groups()
        if lang:
            lang = lang
        else:
            lang = None
        if dtype:
            dtype = dtype
        else:
            dtype = None
        if lang and dtype:
            raise ParseError(""Can't have both a language and a datatype"")
        lit = unquote(lit)
        return Literal(lit, lang, dtype)
    return False
",if lang :,132
"def _staged_model_references(self, load_relationships=False):
    for name, field in self._fields.items():
        if isinstance(field, BaseRelationship):
            try:
                if load_relationships:
                    value = getattr(self, name)
                else:
                    value = self.data_store.get(name, (""staged"", ""committed""))
            except (AttributeError, KeyError, PathResolutionError):
                continue
            if value is None:
                continue
            if not isinstance(value, ModelCollection):
                value = [value]
            for related in value:
                related_name = field.related_name
                yield related, related_name
",if value is None :,198
"def __call__(self, target):
    # normal running mode
    if not self.check_run_always:
        for algo in self.algos:
            if not algo(target):
                return False
        return True
    # run mode when at least one algo has a run_always attribute
    else:
        # store result in res
        # allows continuation to check for and run
        # algos that have run_always set to True
        res = True
        for algo in self.algos:
            if res:
                res = algo(target)
            elif hasattr(algo, ""run_always""):
                if algo.run_always:
                    algo(target)
        return res
",if algo . run_always :,188
"def addRow(self, row):
    r = []
    for j in range(self.numColumn):
        w, s = calWidth(row[j], self.maxWidth)
        if w > self.W[j]:
            self.W[j] = w
        r.append((w, s))
    self.M.append(r)
",if w > self . W [ j ] :,90
"def parse(s):
    text, anns = """", []
    # tweak text: remove space around annotations and strip space
    s = re.sub(r""(<category[^<>]*>)( +)"", r""\2\1"", s)
    s = re.sub(r""( +)(<\/category>)"", r""\2\1"", s)
    rest = s.strip()
    while True:
        m = re.match(r'^(.*?)<category=""([^""]+)"">(.*?)</category>(.*)$', rest)
        if not m:
            break
        pre, type_, tagged, rest = m.groups()
        text += pre
        anns.append((len(text), len(text) + len(tagged), type_, tagged))
        text += tagged
    text += rest
    return text, anns
",if not m :,196
"def _generate_examples(self, filepath):
    with open(filepath) as f:
        line_num = -1
        while True:
            line_num += 1
            sentence = f.readline().strip()
            pronoun = f.readline().strip()
            candidates = [c.strip() for c in f.readline().strip().split("","")]
            correct = f.readline().strip()
            f.readline()
            if not sentence:
                break
            yield line_num, {
                ""sentence"": sentence,
                ""pronoun"": pronoun,
                ""candidates"": candidates,
                ""label"": candidates.index(correct),
            }
",if not sentence :,189
"def format_unencoded(self, tokensource, outfile):
    if self.linenos:
        self._write_lineno(outfile)
    for ttype, value in tokensource:
        color = self._get_color(ttype)
        for line in value.splitlines(True):
            if color:
                outfile.write(""<%s>%s</>"" % (color, line.rstrip(""\n"")))
            else:
                outfile.write(line.rstrip(""\n""))
            if line.endswith(""\n""):
                if self.linenos:
                    self._write_lineno(outfile)
                else:
                    outfile.write(""\n"")
    if self.linenos:
        outfile.write(""\n"")
",if self . linenos :,190
"def refresh_pool_in_list(pool_list, conn, uuid):
    for row in pool_list.get_model():
        if row[0] == uuid:
            # Update active sensitivity and percent available for passed uuid
            row[3] = get_pool_size_percent(conn, uuid)
            row[2] = conn.get_pool(uuid).is_active()
            return
",if row [ 0 ] == uuid :,103
"def save_claims_for_resolve(self, claim_infos):
    to_save = {}
    for info in claim_infos:
        if ""value"" in info:
            if info[""value""]:
                to_save[info[""claim_id""]] = info
        else:
            for key in (""certificate"", ""claim""):
                if info.get(key, {}).get(""value""):
                    to_save[info[key][""claim_id""]] = info[key]
    return self.save_claims(to_save.values())
","if ""value"" in info :",141
"def rx(self, text):
    r = []
    for c in text:
        if "" "" <= c < ""\x7f"" or c in ""\r\n\b\t"":
            r.append(c)
        elif c < "" "":
            r.append(unichr(0x2400 + ord(c)))
        else:
            r.extend(unichr(0x2080 + ord(d) - 48) for d in ""{:d}"".format(ord(c)))
            r.append("" "")
    return """".join(r)
","if "" "" <= c < ""\x7f"" or c in ""\r\n\b\t"" :",139
"def consume_bytes(data):
    state_machine.receive_data(data)
    while True:
        event = state_machine.next_event()
        if event is h11.NEED_DATA:
            break
        elif isinstance(event, h11.InformationalResponse):
            # Ignore 1xx responses
            continue
        elif isinstance(event, h11.Response):
            # We have our response! Save it and get out of here.
            context[""h11_response""] = event
            raise LoopAbort
        else:
            # Can't happen
            raise RuntimeError(""Unexpected h11 event {}"".format(event))
","elif isinstance ( event , h11 . InformationalResponse ) :",166
"def validate_text(dialect, attr):
    val = getattr(dialect, attr)
    if not isinstance(val, text_type):
        if type(val) == bytes:
            raise Error('""{0}"" must be string, not bytes'.format(attr))
        raise Error('""{0}"" must be string, not {1}'.format(attr, type(val).__name__))
    if len(val) != 1:
        raise Error('""{0}"" must be a 1-character string'.format(attr))
",if type ( val ) == bytes :,122
"def _refresh(self):
    self.uiProfileSelectComboBox.clear()
    self.uiProfileSelectComboBox.addItem(""default"")
    try:
        if os.path.exists(self.profiles_path):
            for profile in sorted(os.listdir(self.profiles_path)):
                if not profile.startswith("".""):
                    self.uiProfileSelectComboBox.addItem(profile)
    except OSError:
        pass
","if not profile . startswith ( ""."" ) :",107
"def get_entry(self, ip):
    self.parse()
    options = []
    for (line_type, components) in self._contents:
        if line_type == ""option"":
            (pieces, _tail) = components
            if len(pieces) and pieces[0] == ip:
                options.append(pieces[1:])
    return options
",if len ( pieces ) and pieces [ 0 ] == ip :,93
"def __new__(mcls, cls_name, bases, d):
    offset = 0
    for base in bases:
        for realbase in base.__mro__:
            offset += len(realbase.__dict__.get(""_methods_"", []))
    for i, args in enumerate(d.get(""_methods_"", [])):
        name = args[0]
        restype = args[1]
        if restype is None:
            continue
        argtypes = args[2:]
        m = COMMethod(name, offset + i, restype, argtypes)
        d[name] = m
    return type(ctypes.c_void_p).__new__(mcls, cls_name, bases, dict(d))
",if restype is None :,170
"def _compare_caffe_tvm(caffe_out, tvm_out, is_network=False):
    for i in range(len(caffe_out)):
        if is_network:
            caffe_out[i] = caffe_out[i][:1]
        tvm.testing.assert_allclose(caffe_out[i], tvm_out[i], rtol=1e-5, atol=1e-5)
",if is_network :,116
"def update_transcoder(self):
    self.save_button.set_visible(False)
    if self.cast and self.fn:
        self.transcoder = Transcoder(
            self.cast,
            self.fn,
            lambda did_transcode=None: GLib.idle_add(self.update_status, did_transcode),
            self.transcoder,
        )
        if self.autoplay:
            self.autoplay = False
            self.play_clicked(None)
    else:
        if self.transcoder:
            self.transcoder.destroy()
            self.transcoder = None
    GLib.idle_add(self.update_media_button_states)
",if self . transcoder :,186
"def deserialize(x):
    t = type(x)
    if t is list:
        return list(imap(deserialize, x))
    if t is dict:
        if ""_id_"" not in x:
            return {key: deserialize(val) for key, val in iteritems(x)}
        obj = objmap.get(x[""_id_""])
        if obj is None:
            entity_name = x[""class""]
            entity = database.entities[entity_name]
            pk = x[""_pk_""]
            obj = entity[pk]
        return obj
    return x
",if obj is None :,150
"def release(self, conn, error=False):
    if not conn.is_closed:
        if not error and len(self.connections) < self.pool_size:
            self.connections.append(conn)
        else:
            self.close_callable(conn)
",if not error and len ( self . connections ) < self . pool_size :,71
"def install_symlinks(self):
    """"""Create symlinks for some applications files.""""""
    if self.has_symlinks():
        for app_path in self.app_path:
            for symlink in self.symlinks.values():
                root = symlink[""root""]
                dest = path.join(str(app_path), symlink[""dest""])
                if path.exists(dest):
                    self.backup.create(dest)
                    symlink_file(root, dest)
",if path . exists ( dest ) :,126
"def _fill_array():
    global _array
    for i in range(624):
        y = (_array[i] & _bitmask2) + (_array[(i + 1) % 624] & _bitmask3)
        _array[i] = _array[(i + 397) % 624] ^ (y >> 1)
        if y % 2 != 0:
            _array[i] ^= 2567483615
",if y % 2 != 0 :,107
"def parseLeftHandSideExpressionAllowCall():
    marker = None
    expr = None
    args = None
    property = None
    marker = createLocationMarker()
    expr = parseNewExpression() if matchKeyword(""new"") else parsePrimaryExpression()
    while (match(""."") or match(""["")) or match(""(""):
        if match(""(""):
            args = parseArguments()
            expr = delegate.createCallExpression(expr, args)
        elif match(""[""):
            property = parseComputedMember()
            expr = delegate.createMemberExpression(""["", expr, property)
        else:
            property = parseNonComputedMember()
            expr = delegate.createMemberExpression(""."", expr, property)
        if marker:
            marker.end()
            marker.apply(expr)
    return expr
",if marker :,193
"def unregister_zombies(self):
    """"""Unregister zombie builds (those whose builddir is gone).""""""
    from pprint import pprint
    pprint(self.configs)
    for build_num, config in self.configs.items():
        obj_dir_path = join(
            config.buildDir,
            _srcTreeName_from_config(config),
            ""mozilla"",
            config.mozObjDir,
        )
        if not exists(obj_dir_path):
            self.unregister(build_num, ""zombie (`%s' does not exist)"" % obj_dir_path)
",if not exists ( obj_dir_path ) :,152
"def isUpdateAvailable(self, localOnly=False):
    nsp = self.getLatestFile()
    if not nsp:
        if not nsp:
            if not self.isUpdate or (self.version and int(self.version) > 0):
                return True
            else:
                return False
    try:
        latest = self.lastestVersion(localOnly=localOnly)
        if latest is None:
            return False
        if int(nsp.version) < int(latest):
            return True
    except BaseException as e:
        Print.error(""isUpdateAvailable exception %s: %s"" % (self.id, str(e)))
        pass
    return False
",if not self . isUpdate or ( self . version and int ( self . version ) > 0 ) :,179
"def verify_settings(rst_path: Path) -> Iterator[Error]:
    for setting_name, default in find_settings_in_rst(rst_path):
        actual = getattr(app.conf, setting_name)
        if isinstance(default, timedelta):
            default = default.total_seconds()
        if isinstance(actual, Enum):
            actual = actual.value
        if actual != default:
            yield Error(
                reason=""mismatch"",
                setting=setting_name,
                default=default,
                actual=actual,
            )
","if isinstance ( default , timedelta ) :",152
"def config_update(self, *updates):
    filename = os.path.join(self.path, "".git"", ""config"")
    with GitConfigParser(file_or_files=filename, read_only=False) as config:
        for section, key, value in updates:
            try:
                old = config.get(section, key)
                if value is None:
                    config.remove_option(section, key)
                    continue
                if old == value:
                    continue
            except (NoSectionError, NoOptionError):
                pass
            if value is not None:
                config.set_value(section, key, value)
",if value is None :,183
"def __init__(self, search_space):
    self.params = {}
    for key in search_space.keys():
        if search_space[key][""_type""] == ""factor"":
            self.params[key] = Factor(search_space[key][""_value""])
        else:
            raise RuntimeError(
                ""G_BFS Tuner doesn't support this kind of parameter: ""
                + str(search_space[key][""_type""])
            )
","if search_space [ key ] [ ""_type"" ] == ""factor"" :",125
"def largest_image_url(self):
    # TODO: remove. it is not responsibility of Scrapper
    if not self.imgs and not self.top_img:
        return None
    if self.top_img:
        return self.top_img
    max_area = 0
    max_url = None
    for img_url in self.imgs:
        dimension = fetch_image_dimension(img_url, self.useragent, referer=self.url)
        area = self.calculate_area(img_url, dimension)
        if area > max_area:
            max_area = area
            max_url = img_url
    log.debug(""using max img {}"".format(max_url))
    return max_url
",if area > max_area :,182
"def _geo_indices(cls, inspected=None):
    inspected = inspected or []
    geo_indices = []
    inspected.append(cls)
    for field in cls._fields.values():
        if hasattr(field, ""document_type""):
            field_cls = field.document_type
            if field_cls in inspected:
                continue
            if hasattr(field_cls, ""_geo_indices""):
                geo_indices += field_cls._geo_indices(inspected)
        elif field._geo_index:
            geo_indices.append(field)
    return geo_indices
",elif field . _geo_index :,155
"def __call__(self, trainer):
    self._t += 1
    optimizer = self._get_optimizer(trainer)
    value = self._init * (self._rate ** self._t)
    if self._target is not None:
        if self._rate > 1:
            # almost same as value = min(value, self._target), but this
            # line supports negative values, too
            if value / self._target > 1:
                value = self._target
        else:
            # ditto
            if value / self._target < 1:
                value = self._target
    self._update_value(optimizer, value)
",if self . _rate > 1 :,167
"def _parse_chunked(self, data):
    body = []
    trailers = {}
    n = 0
    lines = data.split(b""\r\n"")
    # parse body
    while True:
        size, chunk = lines[n : n + 2]
        size = int(size, 16)
        if size == 0:
            n += 1
            break
        self.assertEqual(size, len(chunk))
        body.append(chunk)
        n += 2
        # we /should/ hit the end chunk, but check against the size of
        # lines so we're not stuck in an infinite loop should we get
        # malformed data
        if n > len(lines):
            break
    return b"""".join(body)
",if size == 0 :,191
"def _gen_opnds(ii):  # generator
    # filter out write-mask operands and suppressed operands
    for op in ii.parsed_operands:
        if op.lookupfn_name in [""MASK1"", ""MASKNOT0""]:
            continue
        if op.visibility == ""SUPPRESSED"":
            continue
        if op.name == ""BCAST"":
            continue
        yield op
","if op . name == ""BCAST"" :",104
"def allow_request(self, request, view):
    if settings.API_THROTTLING:
        request_allowed = super(GranularUserRateThrottle, self).allow_request(
            request, view
        )
        if not request_allowed:
            user = getattr(request, ""user"", None)
            if user and request.user.is_authenticated:
                log.info(""User %s throttled for scope %s"", request.user, self.scope)
                ActivityLog.create(amo.LOG.THROTTLED, self.scope, user=user)
        return request_allowed
    else:
        return True
",if user and request . user . is_authenticated :,164
"def _make_callback(self):
    callback = self.callback
    for plugin in self.all_plugins():
        try:
            if hasattr(plugin, ""apply""):
                callback = plugin.apply(callback, self)
            else:
                callback = plugin(callback)
        except RouteReset:  # Try again with changed configuration.
            return self._make_callback()
        if not callback is self.callback:
            update_wrapper(callback, self.callback)
    return callback
","if hasattr ( plugin , ""apply"" ) :",131
"def OnDeleteLine(self, items):
    for n in items:
        if n >= 0:
            name1 = self.items[n][2]
            name2 = self.items[n][4]
            del self.items[n]
            if name1 in self.bindiff.matched1:
                self.bindiff.matched1.remove(name1)
            if name2 in self.bindiff.matched2:
                self.bindiff.matched2.remove(name2)
    return [Choose.ALL_CHANGED] + items
",if name1 in self . bindiff . matched1 :,146
"def on_treeview_buttonrelease(self, widget, event, data=None):
    if self.promptToSave():
        # True result indicates user selected Cancel. Stop event propagation
        return True
    else:
        x = int(event.x)
        y = int(event.y)
        time = event.time
        pthinfo = widget.get_path_at_pos(x, y)
        if pthinfo is not None:
            path, col, cellx, celly = pthinfo
            currentPath, currentCol = widget.get_cursor()
            if currentPath != path:
                widget.set_cursor(path, col, 0)
            if event.button == 3:
                self.__popupMenu(event)
        return False
",if event . button == 3 :,198
"def __lt__(self, other):
    try:
        if self._version != other._version:
            return self._version < other._version
        if self._ip != other._ip:
            return self._ip < other._ip
        if self.netmask != other.netmask:
            return self.netmask < other.netmask
        return False
    except AttributeError:
        return NotImplemented
",if self . netmask != other . netmask :,104
"def config_video_apply(self, dev_id_info):
    df, da, add_define, hf, ha, add_hotplug = self.make_apply_data()
    ignore = add_hotplug
    if self.editted(EDIT_VIDEO_MODEL):
        model = self.get_combo_label_value(""video-model"")
        if model:
            add_define(self.vm.define_video_model, dev_id_info, model)
    return self._change_config_helper(df, da, hf, ha)
",if model :,138
"def write(self, b):
    if self._write_watcher is None:
        raise UnsupportedOperation(""write"")
    while True:
        try:
            return _write(self._fileno, b)
        except (IOError, OSError) as ex:
            if ex.args[0] not in ignored_errors:
                raise
        wait_on_watcher(self._write_watcher, None, None, self.hub)
",if ex . args [ 0 ] not in ignored_errors :,110
"def scan_resource_conf(self, conf):
    if ""enabled"" in conf and conf[""enabled""][0]:
        retention_block = conf[""retention_policy""][0]
        if retention_block[""enabled""][0]:
            retention_in_days = force_int(retention_block[""days""][0])
            if retention_in_days and retention_in_days >= 90:
                return CheckResult.PASSED
    return CheckResult.FAILED
",if retention_in_days and retention_in_days >= 90 :,118
"def _find_gist_with_file(user, filename, env):
    import requests  # expensive
    page = 1
    url = ""https://api.github.com/users/%s/gists"" % user
    while True:
        resp = requests.get(
            url,
            params={""page"": page, ""per_page"": 100},
            headers=_github_auth_headers(env),
        )
        gists = resp.json()
        if not gists:
            return None
        for gist in gists:
            for name in gist[""files""]:
                if name == filename:
                    return gist
        page += 1
",if name == filename :,178
"def parse_position_spec(self):
    line = self.lookahead()
    if line.startswith(""jump="") or line.startswith(""jcnd=""):
        self.consume()
        return True
    mo = self._position_re.match(line)
    if not mo:
        return False
    position, id, name = mo.groups()
    if id:
        table = self._position_table_map[position]
        if name:
            self.position_ids[(table, id)] = name
        else:
            name = self.position_ids.get((table, id), """")
    self.positions[self._position_map[position]] = name
    self.consume()
    return True
",if name :,177
"def remove_header(self, header):
    new_msg = b""""
    old_msg = self.msg_bytes.split(""\n"")
    i = 0
    while True:
        line = old_msg[i]
        i += 1
        if not line.startswith(b""%s: "" % header):
            new_msg += line
        if line == """":
            break
    new_msg += old_msg[i:]
    self.msg_bytes = new_msg
","if not line . startswith ( b""%s: "" % header ) :",122
"def on_janitor_selection_changed(self, selection):
    model, iter = selection.get_selected()
    if iter:
        if self.janitor_model.iter_has_child(iter):
            iter = self.janitor_model.iter_children(iter)
        plugin = model[iter][self.JANITOR_PLUGIN]
        for row in self.result_model:
            if row[self.RESULT_PLUGIN] == plugin:
                self.result_view.get_selection().select_path(row.path)
                log.debug(""scroll_to_cell: %s"" % row.path)
                self.result_view.scroll_to_cell(row.path)
",if row [ self . RESULT_PLUGIN ] == plugin :,184
"def record_line(self, frame, event, arg):  # pylint: disable=unused-argument
    """"""Records line execution time.""""""
    if event == ""line"":
        if self.prev_timestamp:
            runtime = time.time() - self.prev_timestamp
            self.lines.append([self.prev_path, self.prev_lineno, runtime])
        self.prev_lineno = frame.f_lineno
        self.prev_path = frame.f_code.co_filename
        self.prev_timestamp = time.time()
    return self.record_line
",if self . prev_timestamp :,142
"def get_outdated_docs(self) -> Iterator[str]:
    for docname in self.env.found_docs:
        if docname not in self.env.all_docs:
            yield docname
            continue
        targetname = path.join(self.outdir, docname + self.out_suffix)
        try:
            targetmtime = path.getmtime(targetname)
        except Exception:
            targetmtime = 0
        try:
            srcmtime = path.getmtime(self.env.doc2path(docname))
            if srcmtime > targetmtime:
                yield docname
        except OSError:
            # source doesn't exist anymore
            pass
",if srcmtime > targetmtime :,176
"def _fetch_all_channels(self, force=False):
    """"""Fetch all channel feeds from cache or network.""""""
    channels = self._get_channel_configs(force=force)
    enabled = self._settings.get([""enabled_channels""])
    forced = self._settings.get([""forced_channels""])
    all_channels = {}
    for key, config in channels.items():
        if key not in enabled and key not in forced:
            continue
        if ""url"" not in config:
            continue
        data = self._get_channel_data(key, config, force=force)
        if data is not None:
            all_channels[key] = data
    return all_channels
",if key not in enabled and key not in forced :,174
"def _get_cortex_binary(kmer, cortex_dir):
    cortex_bin = None
    for check_bin in sorted(glob.glob(os.path.join(cortex_dir, ""bin"", ""cortex_var_*""))):
        kmer_check = int(os.path.basename(check_bin).split(""_"")[2])
        if kmer_check >= kmer:
            cortex_bin = check_bin
            break
    assert (
        cortex_bin is not None
    ), ""Could not find cortex_var executable in %s for kmer %s"" % (cortex_dir, kmer)
    return cortex_bin
",if kmer_check >= kmer :,176
"def test_numeric_literals(self):
    @udf(BigIntVal(FunctionContext, SmallIntVal))
    def fn(context, a):
        if a is None:
            return 1729
        elif a < 0:
            return None
        elif a < 10:
            return a + 5
        else:
            return a * 2
",if a is None :,92
"def cs(self):
    """"""ConfigSpace representation of this search space.""""""
    cs = CS.ConfigurationSpace()
    for k, v in self.kwvars.items():
        if isinstance(v, NestedSpace):
            _add_cs(cs, v.cs, k)
        elif isinstance(v, Space):
            hp = v.get_hp(name=k)
            _add_hp(cs, hp)
        else:
            _rm_hp(cs, k)
    return cs
","elif isinstance ( v , Space ) :",129
"def lineReceived(self, line):
    if self.state == ""connected"":
        self.messageFilename = line
        self.state = ""gotMessageFilename""
    if self.state == ""gotMessageFilename"":
        if line:
            self.metaInfo.append(line)
        else:
            if not self.metaInfo:
                self.transport.loseConnection()
                return
            self.filterMessage()
",if not self . metaInfo :,114
"def __init__(self, reg, shtype, shimm, va):
    if shimm == 0:
        if shtype == S_ROR:
            shtype = S_RRX
        elif shtype == S_LSR or shtype == S_ASR:
            shimm = 32
    self.reg = reg
    self.shtype = shtype
    self.shimm = shimm
    self.va = va
",if shtype == S_ROR :,107
"def check_data(self, var_name: str, val: Dict[Any, Any]) -> None:
    if not isinstance(val, dict):
        raise AssertionError(f""{var_name} is not a dictionary"")
    for key, value in val.items():
        if not isinstance(key, str):
            raise AssertionError(f""{var_name} has a non-string key"")
        check_data(self.value_type, f""{var_name}[{key}]"", value)
","if not isinstance ( key , str ) :",118
"def write_conditional_formatting(worksheet):
    """"""Write conditional formatting to xml.""""""
    df = DifferentialStyle()
    wb = worksheet.parent
    for cf in worksheet.conditional_formatting:
        for rule in cf.rules:
            if rule.dxf and rule.dxf != df:
                rule.dxfId = wb._differential_styles.add(rule.dxf)
        yield cf.to_tree()
",if rule . dxf and rule . dxf != df :,108
"def _find_wordpress_compiler(self):
    """"""Find WordPress compiler plugin.""""""
    if self.wordpress_page_compiler is not None:
        return
    plugin_info = self.site.plugin_manager.getPluginByName(""wordpress"", ""PageCompiler"")
    if plugin_info is not None:
        if not plugin_info.is_activated:
            self.site.plugin_manager.activatePluginByName(plugin_info.name)
            plugin_info.plugin_object.set_site(self.site)
        self.wordpress_page_compiler = plugin_info.plugin_object
",if not plugin_info . is_activated :,147
"def _confirm(config):
    cli.out(""You are about to initialize a Guild environment:"")
    for name, val in config.prompt_params:
        if isinstance(val, tuple):
            cli.out(""  {}:"".format(name))
            for x in val:
                cli.out(""    {}"".format(x))
        else:
            cli.out(""  {}: {}"".format(name, val))
    return cli.confirm(""Continue?"", default=True)
","if isinstance ( val , tuple ) :",121
"def last_ok(nodes):
    for i in range(len(nodes) - 1, -1, -1):
        if ok_node(nodes[i]):
            node = nodes[i]
            if isinstance(node, ast.Starred):
                if ok_node(node.value):
                    return node.value
                else:
                    return None
            else:
                return nodes[i]
    return None
","if isinstance ( node , ast . Starred ) :",122
"def _is_binary(fname, limit=80):
    try:
        with open(fname, ""rb"") as f:
            for i in range(limit):
                char = f.read(1)
                if char == b""\0"":
                    return True
                if char == b""\n"":
                    return False
                if char == b"""":
                    return
    except OSError as e:
        if xp.ON_WINDOWS and is_app_execution_alias(fname):
            return True
        raise e
    return False
",if xp . ON_WINDOWS and is_app_execution_alias ( fname ) :,154
"def render(self):
    x = ""<span>""
    for idx, arg in enumerate(self.args, start=1):
        if isinstance(arg, (tuple, list)):
            value, desc = arg
        else:
            value, desc = arg, arg
        attrs = self.attrs.copy()
        attrs[""name""] = self.name
        attrs[""type""] = ""radio""
        attrs[""value""] = value
        attrs[""id""] = self.name + str(idx)
        if self.value == value:
            attrs[""checked""] = ""checked""
        x += ""<input %s/> %s"" % (attrs, net.websafe(desc))
    x += ""</span>""
    return x
",if self . value == value :,183
"def test01b_gml(self):
    ""Testing GML output.""
    for g in self.geometries.wkt_out:
        geom = OGRGeometry(g.wkt)
        exp_gml = g.gml
        if GDAL_VERSION >= (1, 8):
            # In GDAL 1.8, the non-conformant GML tag  <gml:GeometryCollection> was
            # replaced with <gml:MultiGeometry>.
            exp_gml = exp_gml.replace(""GeometryCollection"", ""MultiGeometry"")
        self.assertEqual(exp_gml, geom.gml)
","if GDAL_VERSION >= ( 1 , 8 ) :",159
"def _update_recording(self, frame, config):
    """"""Adds a frame to the current video output.""""""
    # pylint: disable=redefined-variable-type
    should_record = config[""is_recording""]
    if should_record:
        if not self.is_recording:
            self.is_recording = True
            print(
                ""Starting recording using %s"", self.video_writer.current_output().name()
            )
        self.video_writer.write_frame(frame)
    elif self.is_recording:
        self.is_recording = False
        self.video_writer.finish()
        print(""Finished recording"")
",if not self . is_recording :,166
"def activate(self, ctx):
    for idx in ctx.chooser_selection:
        func_ea = idaapi.getn_func(idx - 1).startEA
        cfunc = helper.decompile_function(func_ea)
        obj = api.VariableObject(cfunc.get_lvars()[0], 0)
        if cfunc:
            NewDeepSearchVisitor(cfunc, 0, obj, cache.temporary_structure).process()
",if cfunc :,108
"def finish(self, event, commit=0):
    target = self.target
    source = self.source
    widget = self.initial_widget
    root = self.root
    try:
        del root.__dnd
        self.initial_widget.unbind(self.release_pattern)
        self.initial_widget.unbind(""<Motion>"")
        widget[""cursor""] = self.save_cursor
        self.target = self.source = self.initial_widget = self.root = None
        if target:
            if commit:
                target.dnd_commit(source, event)
            else:
                target.dnd_leave(source, event)
    finally:
        source.dnd_end(target, event)
",if target :,188
"def run_epoch(model: BaseModel, loader, device: str, num_batches: int):
    model.eval()
    with Ctq(loader) as tq_loader:
        for batch_idx, data in enumerate(tq_loader):
            if batch_idx < num_batches:
                process(model, data, device)
            else:
                break
",if batch_idx < num_batches :,99
"def find(d, target):
    remainingDicts = [d]
    while len(remainingDicts) > 0:
        current = remainingDicts.pop()
        for k, v in current.iteritems():
            if k == target:
                return v
            if isinstance(v, dict):
                remainingDicts.insert(0, v)
    return None
",if k == target :,98
"def node_exists(self, jid=None, node=None, ifrom=None):
    with self.lock:
        if jid is None:
            jid = self.xmpp.boundjid.full
        if node is None:
            node = """"
        if ifrom is None:
            ifrom = """"
        if isinstance(ifrom, JID):
            ifrom = ifrom.full
        if (jid, node, ifrom) not in self.nodes:
            return False
        return True
",ifrom = ifrom . full,136
"def TryMerge(self, d):
    while d.avail() > 0:
        tt = d.getVarInt32()
        if tt == 8:
            self.set_time(d.getVarInt64())
            continue
        if tt == 16:
            self.set_level(d.getVarInt32())
            continue
        if tt == 26:
            self.set_log_message(d.getPrefixedString())
            continue
        if tt == 0:
            raise ProtocolBuffer.ProtocolBufferDecodeError
        d.skipData(tt)
",if tt == 16 :,152
"def _merge_dict(d1, d2):
    # Modifies d1 in-place to take values from d2
    # if the nested keys from d2 are present in d1.
    # https://stackoverflow.com/a/10704003/4488789
    for k, v2 in d2.items():
        v1 = d1.get(k)  # returns None if v1 has no such key
        if v1 is None:
            raise Exception(""{} is not recognized by client_config"".format(k))
        if isinstance(v1, Mapping) and isinstance(v2, Mapping):
            _merge_dict(v1, v2)
        else:
            d1[k] = v2
    return d1
",if v1 is None :,184
"def build_and_apply_filters(query, objects, filter_func):
    if objects is not None:
        if isinstance(objects, str):
            query = query.filter(filter_func(objects))
        elif isinstance(objects, list):
            t = []
            for obj in objects:
                t.append(filter_func(obj))
            query = query.filter(or_(*t))
    return query
","elif isinstance ( objects , list ) :",111
"def _worker_task(self, num: int):
    while True:
        try_ = 0
        f = self.q.get()
        while try_ <= self.retries:
            rr = f()
            if not rr.retry:
                break
            try_ += 1
        with self.stat_lock:
            self.exit_stat |= rr.ret_val
        self.q.task_done()
",if not rr . retry :,115
"def get_benchmark_id_title_map(input_tree):
    input_root = input_tree.getroot()
    ret = {}
    for namespace in [XCCDF11_NS, XCCDF12_NS]:
        candidates = []
        scrape_benchmarks(input_root, namespace, candidates)
        for _, elem in candidates:
            _id = elem.get(""id"")
            if _id is None:
                continue
            title = ""<unknown>""
            for element in elem.findall(""{%s}title"" % (namespace)):
                title = element.text
                break
            ret[_id] = title
    return ret
",if _id is None :,171
"def _call_tensor_ufunc(self, x1, x2, out=None, where=None):
    if hasattr(x1, ""__tensor_ufunc__"") or hasattr(x2, ""__tensor_ufunc__""):
        ufunc = (
            x1.__tensor_ufunc__
            if hasattr(x1, ""__tensor_ufunc__"")
            else x2.__tensor_ufunc__
        )
        ret = ufunc(type(self), [x1, x2], out, where, **self.ufunc_extra_params)
        if ret is NotImplemented:
            return
        return ret
","if hasattr ( x1 , ""__tensor_ufunc__"" )",146
"def remove_namespaces(xml):
    for elem in xml.getiterator():
        if elem.tag is etree.Comment:
            continue
        i = elem.tag.find(""}"")
        if i > 0:
            elem.tag = elem.tag[i + 1 :]
    return xml
",if i > 0 :,73
"def attributive(adjective, gender=MALE):
    w = adjective.lower()
    # normal => normales
    if PLURAL in gender and not is_vowel(w[-1:]):
        return w + ""es""
    # el chico inteligente => los chicos inteligentes
    if PLURAL in gender and w.endswith((""a"", ""e"")):
        return w + ""s""
    # el chico alto => los chicos altos
    if w.endswith(""o""):
        if FEMININE in gender and PLURAL in gender:
            return w[:-1] + ""as""
        if FEMININE in gender:
            return w[:-1] + ""a""
        if PLURAL in gender:
            return w + ""s""
    return w
",if PLURAL in gender :,197
"def atbash(s):
    translated = """"
    for i in range(len(s)):
        n = ord(s[i])
        if s[i].isalpha():
            if s[i].isupper():
                x = n - ord(""A"")
                translated += chr(ord(""Z"") - x)
            if s[i].islower():
                x = n - ord(""a"")
                translated += chr(ord(""z"") - x)
        else:
            translated += s[i]
    return translated
",if s [ i ] . islower ( ) :,143
"def _add_all(self):
    stream = BytesIO()
    for page in self.graph_manager.pages:
        stream.write(page.url.encode(""utf8""))
        if not page.has_errors:
            for link in page.links:
                stream.write(link.url.encode(""utf8""))
                stream.write(linesep.encode(""utf8""))
    stream.seek(0)
    self.frontier.add_seeds(stream)
",if not page . has_errors :,120
"def test_bigrand_ranges(self):
    for i in [40, 80, 160, 200, 211, 250, 375, 512, 550]:
        start = self.gen.randrange(2 ** i)
        stop = self.gen.randrange(2 ** (i - 2))
        if stop <= start:
            return
        self.assertTrue(start <= self.gen.randrange(start, stop) < stop)
",if stop <= start :,104
"def on_connect(self, request):
    web_socket = WebSocketResponse()
    await web_socket.prepare(request)
    self.app[""websockets""].add(web_socket)
    try:
        async for msg in web_socket:
            if msg.type == WSMsgType.TEXT:
                await self.on_status(None)
            elif msg.type == WSMsgType.ERROR:
                print(
                    ""web socket connection closed with exception %s""
                    % web_socket.exception()
                )
    finally:
        self.app[""websockets""].discard(web_socket)
    return web_socket
",if msg . type == WSMsgType . TEXT :,177
"def __cut_all(self, sentence):
    dag = self.get_DAG(sentence)
    old_j = -1
    for k, L in iteritems(dag):
        if len(L) == 1 and k > old_j:
            yield sentence[k : L[0] + 1]
            old_j = L[0]
        else:
            for j in L:
                if j > k:
                    yield sentence[k : j + 1]
                    old_j = j
",if j > k :,137
"def filter_forms(forms):
    result = []
    seen = set()
    for form in forms:
        if form in self._lemma_pos_offset_map:
            if pos in self._lemma_pos_offset_map[form]:
                if form not in seen:
                    result.append(form)
                    seen.add(form)
    return result
",if pos in self . _lemma_pos_offset_map [ form ] :,100
"def __init__(self, el):
    self.elements = list(el)
    parameters = {}
    tokens = []
    token_quote = ""@""
    for key, value in el.attrib.items():
        if key == ""token_quote"":
            token_quote = value
        if key == ""tokens"":
            for token in value.split("",""):
                tokens.append((token, REQUIRED_PARAMETER))
        elif key.startswith(""token_""):
            token = key[len(""token_"") :]
            tokens.append((token, value))
    for name, default in tokens:
        parameters[name] = (token_quote, default)
    self.parameters = parameters
","if key == ""tokens"" :",170
"def setPositionAfterSort(self, sortChildren):
    c = self
    p = c.p
    p_v = p.v
    parent = p.parent()
    parent_v = p._parentVnode()
    if sortChildren:
        p = parent or c.rootPosition()
    else:
        if parent:
            p = parent.firstChild()
        else:
            p = leoNodes.Position(parent_v.children[0])
        while p and p.v != p_v:
            p.moveToNext()
        p = p or parent
    return p
",if parent :,153
"def next(self):
    while not self.closed or not self._buffer.empty():
        # input stream
        if self._input_iterator:
            try:
                chunck = next(self._input_iterator)
                return chunck
            except StopIteration:
                self.closed = True
                raise StopIteration()
            except Exception as ex:
                log.error(""Failed downloading: %s"" % ex)
        else:
            # in/out stream
            try:
                return self._buffer.get(block=True, timeout=1.0)
            except Empty:
                pass
    raise StopIteration()
",if self . _input_iterator :,181
"def _gen_GreaterEqual(self, args, ret_type):
    result = []
    for lhs, rhs in pairwise(args):
        if ret_type == real_type:
            result.append(self.builder.fcmp_ordered("">="", lhs, rhs))
        elif ret_type == int_type:
            result.append(self.builder.icmp_signed("">="", lhs, rhs))
        else:
            raise CompileError()
    return reduce(self.builder.and_, result)
",elif ret_type == int_type :,120
"def save_settings(self, settings):
    for setting in self.settings:
        setting_obj = settings[setting]
        new_value = self.cleaned_data.get(setting)
        if setting_obj.python_type == ""image"":
            if new_value and new_value != self.initial.get(setting):
                self.save_image(setting_obj, new_value)
            elif self.cleaned_data.get(""%s_delete"" % setting):
                self.delete_image(setting_obj)
        else:
            self.save_setting(setting_obj, new_value)
",if new_value and new_value != self . initial . get ( setting ) :,160
"def TryMerge(self, d):
    while d.avail() > 0:
        tt = d.getVarInt32()
        if tt == 10:
            length = d.getVarInt32()
            tmp = ProtocolBuffer.Decoder(d.buffer(), d.pos(), d.pos() + length)
            d.skip(length)
            self.add_events().TryMerge(tmp)
            continue
        if tt == 17:
            self.set_timeout_seconds(d.getDouble())
            continue
        if tt == 0:
            raise ProtocolBuffer.ProtocolBufferDecodeError
        d.skipData(tt)
",if tt == 17 :,167
"def _trim_steps(self, num_steps):
    """"""Trims a given number of steps from the end of the sequence.""""""
    steps_trimmed = 0
    for i in reversed(range(len(self._events))):
        if self._events[i].event_type == PolyphonicEvent.STEP_END:
            if steps_trimmed == num_steps:
                del self._events[i + 1 :]
                break
            steps_trimmed += 1
        elif i == 0:
            self._events = [
                PolyphonicEvent(event_type=PolyphonicEvent.START, pitch=None)
            ]
            break
",if steps_trimmed == num_steps :,171
"def save(self):
    data = self.cleaned_data
    previous_data = google_integration_model.get_by_account_id(self.account_id)
    if previous_data:
        previous_file = previous_data.get(""file_id"")
    else:
        previous_file = None
    json_key_file = data.get(""json_key"")
    if json_key_file:
        data[""file_id""] = files_model.add(json_key_file)
        del data[""json_key""]
        if previous_file:
            files_model.delete(previous_file)
    google_integration_model.save(data, account_id=self.account_id)
",if previous_file :,178
"def _register(self, class_):
    with self.lock:
        table, slots = self._schema(class_)
        cur = self.db.execute(""PRAGMA table_info(%s)"" % table)
        available = cur.fetchall()
        if available:
            available = [row[1] for row in available]
            missing_slots = (s for s in slots if s not in available)
            for slot in missing_slots:
                self.db.execute(""ALTER TABLE %s ADD COLUMN %s TEXT"" % (table, slot))
        else:
            self.db.execute(
                ""CREATE TABLE %s (%s)""
                % (table, "", "".join(""%s TEXT"" % s for s in slots))
            )
",if available :,195
"def describe_auto_scaling_instances(self, instance_ids):
    instance_states = []
    for group in self.autoscaling_groups.values():
        instance_states.extend(
            [
                x
                for x in group.instance_states
                if not instance_ids or x.instance.id in instance_ids
            ]
        )
    return instance_states
",if not instance_ids or x . instance . id in instance_ids,104
"def add_nicknames(self, fields, data):
    """"""Read the NICKNAME property of a VCard.""""""
    for nick in self.split_unescaped(data, "",""):
        nickname = nick.strip()
        if nickname:
            name = Name()
            name.set_nick_name(self.unesc(nickname))
            self.person.add_alternate_name(name)
",if nickname :,105
"def while1_test(a, b, c):
    while 1:
        if a != 2:
            if b:
                a = 3
                b = 0
            elif c:
                c = 0
            else:
                a += b + c
                break
    return a, b, c
",if a != 2 :,94
"def get_stream(conf, reload=False):
    if not conf:
        return conf
    # we can have 'stream' or 'class' or 'filename'
    if ""class"" in conf:
        class_name = conf.pop(""class"")
        if not ""."" in class_name:
            cls = globals()[class_name]
            inst = cls(**conf)
        else:
            inst = resolve_name(class_name, reload=reload)(**conf)
    elif ""stream"" in conf:
        inst = conf[""stream""]
    elif ""filename"" in conf:
        inst = FileStream(**conf)
    else:
        raise ValueError(""stream configuration invalid"")
    return {""stream"": inst}
","if not ""."" in class_name :",179
"def check_physical(self, line):
    """"""Run all physical checks on a raw input line.""""""
    self.physical_line = line
    for name, check, argument_names in self._physical_checks:
        self.init_checker_state(name, argument_names)
        result = self.run_check(check, argument_names)
        if result is not None:
            (offset, text) = result
            self.report_error(self.line_number, offset, text, check)
            if text[:4] == ""E101"":
                self.indent_char = line[0]
",if result is not None :,154
"def delete_oidc_session_tokens(session):
    if session:
        if ""oidc_access_token"" in session:
            del session[""oidc_access_token""]
        if ""oidc_id_token"" in session:
            del session[""oidc_id_token""]
        if ""oidc_id_token_expiration"" in session:
            del session[""oidc_id_token_expiration""]
        if ""oidc_login_next"" in session:
            del session[""oidc_login_next""]
        if ""oidc_refresh_token"" in session:
            del session[""oidc_refresh_token""]
        if ""oidc_state"" in session:
            del session[""oidc_state""]
","if ""oidc_login_next"" in session :",179
"def _fix_exception_context(new_exc, old_exc):
    # Context may not be correct, so find the end of the chain
    while 1:
        exc_context = new_exc.__context__
        if exc_context is old_exc:
            # Context is already set correctly (see issue 20317)
            return
        if exc_context is None or exc_context is frame_exc:
            break
        new_exc = exc_context
    # Change the end of the chain to point to the exception
    # we expect it to reference
    new_exc.__context__ = old_exc
",if exc_context is None or exc_context is frame_exc :,151
"def _write_all(self, out):
    while len(out) > 0:
        n = self.sock.send(out)
        if n <= 0:
            raise EOFError()
        if n == len(out):
            return
        out = out[n:]
    return
",if n == len ( out ) :,76
"def view(input_path):
    if not exists(input_path):
        raise IOError(""{0} not found"".format(input_path))
    ua = None
    bundle_info = None
    try:
        archive = archive_factory(input_path)
        if archive is None:
            raise NotMatched(""No matching archive type found"")
        ua = archive.unarchive_to_temp()
        bundle_info = ua.bundle.info
    finally:
        if ua is not None:
            ua.remove()
    return bundle_info
",if archive is None :,139
"def _line_generator(fh, skip_blanks=False, strip=True):
    for line in fh:
        if strip:
            line = line.strip()
        skip = False
        if skip_blanks:
            skip = line.isspace() or not line
        if not skip:
            yield line
",if not skip :,82
"def migrate_key(key, source, target):
    if source in config and key in config[source]:
        if config.get(target) is None:
            # make sure we have a serial tree
            config[target] = {}
        if key not in config[target]:
            # only copy over if it's not there yet
            config[target][key] = config[source][key]
        # delete feature flag
        del config[source][key]
        return True
    return False
",if key not in config [ target ] :,128
"def get_params(self):
    if not hasattr(self, ""input_space""):
        raise AttributeError(""Input space has not been provided."")
    rval = []
    for layer in self.layers:
        for param in layer.get_params():
            if param.name is None:
                logger.info(type(layer))
        layer_params = layer.get_params()
        assert not isinstance(layer_params, set)
        for param in layer_params:
            if param not in rval:
                rval.append(param)
    rval = [elem for elem in rval if elem not in self.freeze_set]
    assert all([elem.name is not None for elem in rval])
    return rval
",if param not in rval :,181
"def _build_kwargs_string(cls, expectation):
    kwargs = []
    for k, v in expectation[""kwargs""].items():
        if k == ""column"":
            # make the column a positional argument
            kwargs.insert(0, ""{}='{}'"".format(k, v))
        elif isinstance(v, str):
            # Put strings in quotes
            kwargs.append(""{}='{}'"".format(k, v))
        else:
            # Pass other types as is
            kwargs.append(""{}={}"".format(k, v))
    return "", "".join(kwargs)
","elif isinstance ( v , str ) :",143
"def binary_search(_list, left, right, target):
    if right >= left:
        mid = (left + right) // 2
        # if element is present at the mid itself
        if _list[mid] == target:
            return mid
        # If the element is smaller than mid, then it
        # can only be present in the left subarray
        if _list[mid] > target:
            return binary_search(_list, left, mid - 1, target)
        # Else the element can only be present in the right
        return binary_search(_list, mid + 1, right, target)
    return False
",if _list [ mid ] > target :,157
"def _set_name(self, name):
    # Sanitize the file name so that it can't be dangerous.
    if name is not None:
        # Just use the basename of the file -- anything else is dangerous.
        name = os.path.basename(name)
        # File names longer than 255 characters can cause problems on older OSes.
        if len(name) > 255:
            name, ext = os.path.splitext(name)
            name = name[: 255 - len(ext)] + ext
    self._name = name
",if len ( name ) > 255 :,132
"def scan_iter(self, match=None, count=None):
    nodes = await self.cluster_nodes()
    for node in nodes:
        if ""master"" in node[""flags""]:
            cursor = ""0""
            while cursor != 0:
                pieces = [cursor]
                if match is not None:
                    pieces.extend([""MATCH"", match])
                if count is not None:
                    pieces.extend([""COUNT"", count])
                response = await self.execute_command_on_nodes([node], ""SCAN"", *pieces)
                cursor, data = list(response.values())[0]
                for item in data:
                    yield item
","if ""master"" in node [ ""flags"" ] :",185
"def drf_url(context, viewname, *args, **kwargs):
    """"""Helper for DjangoRestFramework's ``reverse`` in templates.""""""
    request = context.get(""request"")
    if request:
        if not hasattr(request, ""versioning_scheme""):
            request.versioning_scheme = api_settings.DEFAULT_VERSIONING_CLASS()
        request.version = request.versioning_scheme.determine_version(
            request, *args, **kwargs
        )
    return drf_reverse(viewname, request=request, args=args, kwargs=kwargs)
","if not hasattr ( request , ""versioning_scheme"" ) :",140
"def __call__(self, ctx):
    if ctx.range and ctx.value:
        if self.raw:
            ctx.range.raw_value = ctx.value
            return
        scalar = ctx.meta.get(""scalar"", False)
        if not scalar:
            ctx.range = ctx.range.resize(len(ctx.value), len(ctx.value[0]))
        self._write_value(ctx.range, ctx.value, scalar)
",if self . raw :,117
"def removeNamedItemNS(self, namespaceURI, localName):
    n = self.getNamedItemNS(namespaceURI, localName)
    if n is not None:
        _clear_id_cache(self._ownerElement)
        del self._attrsNS[(n.namespaceURI, n.localName)]
        del self._attrs[n.nodeName]
        if hasattr(n, ""ownerElement""):
            n.ownerElement = None
        return n
    else:
        raise xml.dom.NotFoundErr()
","if hasattr ( n , ""ownerElement"" ) :",129
"def __find_image(self, relpath):
    image_path = None
    for rp in self._resource_paths:
        for root, dirs, files in os.walk(rp):
            if relpath in files:
                image_path = os.path.join(root, relpath)
                break
        if image_path is not None:
            break
    return image_path
",if relpath in files :,101
"def get_config_value(self, path, raise_if_not_found=True):
    if not path.is_concrete():
        raise ValueError(""Can't access config by masked path: %s"" % path)
    cfg = self._config
    for key in path:
        if key not in cfg:
            if raise_if_not_found:
                raise ValueError(""Key not found: %r"" % key)
            else:
                return None
        cfg = cfg[key]
    return cfg
",if raise_if_not_found :,132
"def unbind(**kwargs):
    for event, callback in kwargs.items():
        if event not in _callbacks:
            raise Exception(""Unknown {!r} event"".format(event))
        else:
            for listener in _callbacks[event][:]:
                if listener.callback == callback:
                    _callbacks[event].remove(listener)
                    if event == ""on_new_intent"":
                        _activity.unregisterNewIntentListener(listener)
                    elif event == ""on_activity_result"":
                        _activity.unregisterActivityResultListener(listener)
",if event not in _callbacks :,156
"def _escape_attrib(text):
    # escape attribute value
    try:
        if ""&"" in text:
            text = text.replace(""&"", ""&amp;"")
        if ""<"" in text:
            text = text.replace(""<"", ""&lt;"")
        if "">"" in text:
            text = text.replace("">"", ""&gt;"")
        if '""' in text:
            text = text.replace('""', ""&quot;"")
        if ""\n"" in text:
            text = text.replace(""\n"", ""&#10;"")
        return text
    except (TypeError, AttributeError):  # pragma: no cover
        _raise_serialization_error(text)
","if '""' in text :",160
"def _get_options(self, kwargs):
    options = {}
    for option in self._options:
        if option in kwargs:
            self._validate_option(option, kwargs[option])
            options[option] = kwargs[option]
        else:
            options[option] = getattr(self, ""_"" + option)
    return options
",if option in kwargs :,88
"def _parse_version_parts(s):
    for part in component_re.split(s):
        part = replace(part, part)
        if part in ["""", "".""]:
            continue
        if part[:1] in ""0123456789"":
            yield part.zfill(8)  # pad for numeric comparison
        else:
            yield ""*"" + part
    yield ""*final""  # ensure that alpha/beta/candidate are before final
","if part in [ """" , ""."" ] :",109
"def collect_deps(lib):
    queue = list(lib.deps_all)
    visited = set(queue)
    visited.add(lib)
    deps = []
    # Traverse dependencies with breadth-first search.
    while queue:
        # Collect dependencies for next queue.
        next_queue = []
        for lib in queue:
            for dep in lib.deps_all:
                if dep not in visited:
                    next_queue.append(dep)
                    visited.add(dep)
        # Append current queue to result.
        deps.append(collect_path_sorted_lib_idxs(queue))
        queue = next_queue
    return deps
",if dep not in visited :,174
"def process_chunks(self, chunks):
    chunk_id = self._chunk_id
    self._chunk_id += len(chunks)
    chunk_data = []
    for chunk in chunks:
        if len(chunk.data) > MAX_LINE_SIZE:
            msg = ""Metric data exceeds maximum size of {} bytes. Dropping it."".format(
                MAX_LINE_SIZE
            )
            wandb.termerror(msg, repeat=False)
            util.sentry_message(msg)
        else:
            chunk_data.append(chunk.data)
    return {
        ""offset"": chunk_id,
        ""content"": chunk_data,
    }
",if len ( chunk . data ) > MAX_LINE_SIZE :,177
"def truncateLogFile():
    global logfilename
    logger.warn(""Truncating log file %s"" % logfilename)
    with open(logfilename, ""w"") as f:
        f.write("""")
    for i in range(1, 25):
        rotatedFilename = ""%s.%d"" % (logfilename, i)
        if os.path.exists(rotatedFilename):
            logger.info(""Deleting rotated file %s"" % rotatedFilename)
            os.unlink(rotatedFilename)
",if os . path . exists ( rotatedFilename ) :,121
"def _page_contains(self, text):
    browser = self._current_browser()
    browser.switch_to_default_content()
    if self._is_text_present(text):
        return True
    subframes = self._element_find(""xpath=//frame|//iframe"", False, False)
    self._debug(""Current frame has %d subframes"" % len(subframes))
    for frame in subframes:
        browser.switch_to_frame(frame)
        found_text = self._is_text_present(text)
        browser.switch_to_default_content()
        if found_text:
            return True
    return False
",if found_text :,163
"def get_project_name_git():
    is_git = check_output([""git"", ""rev-parse"", ""--git-dir""], stderr=subprocess.STDOUT)
    if is_git:
        project_address = check_output(
            [""git"", ""config"", ""--local"", ""remote.origin.url""]
        )
        if isinstance(project_address, bytes) and str != bytes:
            project_address = project_address.decode()
        project_name = [i for i in re.split(r""[/:\s\\]|\.git"", project_address) if i][
            -1
        ]
        return project_name.strip()
","if isinstance ( project_address , bytes ) and str != bytes :",164
"def timer(ratio, step, additive):
    t = 0
    slowmode = False
    while 1:
        if additive:
            slowmode |= bool((yield t))
        else:
            slowmode = bool((yield t))
        if slowmode:
            t += step * ratio
        else:
            t += step
",if additive :,89
"def _call_connection_lost(self, exc):
    try:
        if self._protocol_connected:
            self._protocol.connection_lost(exc)
    finally:
        self._sock.close()
        self._sock = None
        self._protocol = None
        self._loop = None
        server = self._server
        if server is not None:
            server._detach()
            self._server = None
",if server is not None :,112
"def _think(self):
    try:
        if len(self.addr_store) < self.preferred_storage and self.peers:
            random.choice(self.peers.values()).send_getaddrs(count=8)
    except:
        log.err()
    return random.expovariate(1 / 20)
",if len ( self . addr_store ) < self . preferred_storage and self . peers :,84
"def merge_force_collapse(self):
    p = self.pending
    while len(p) > 1:
        if len(p) >= 3 and p[-3].len < p[-1].len:
            self.merge_at(-3)
        else:
            self.merge_at(-2)
",if len ( p ) >= 3 and p [ - 3 ] . len < p [ - 1 ] . len :,79
"def ensure_echo_on():
    if termios:
        fd = sys.stdin
        if fd.isatty():
            attr_list = termios.tcgetattr(fd)
            if not attr_list[3] & termios.ECHO:
                attr_list[3] |= termios.ECHO
                if hasattr(signal, ""SIGTTOU""):
                    old_handler = signal.signal(signal.SIGTTOU, signal.SIG_IGN)
                else:
                    old_handler = None
                termios.tcsetattr(fd, termios.TCSANOW, attr_list)
                if old_handler is not None:
                    signal.signal(signal.SIGTTOU, old_handler)
","if hasattr ( signal , ""SIGTTOU"" ) :",197
"def change_palette_name(self, palette_name):
    if isinstance(palette_name, str):
        if palette_name not in PALETTES:
            log.info(""Palette name %s not found"", palette_name)
            return
        log.debug(""Settings palette name to %s"", palette_name)
        self.settings.styleFont.set_string(""palette"", PALETTES[palette_name])
        self.settings.styleFont.set_string(""palette-name"", palette_name)
        self.set_colors_from_settings()
",if palette_name not in PALETTES :,142
"def nested_match(expect, value):
    if expect == value:
        return True
    if isinstance(expect, dict) and isinstance(value, dict):
        for k, v in expect.items():
            if k in value:
                if not nested_match(v, value[k]):
                    return False
            else:
                return False
        return True
    if isinstance(expect, list) and isinstance(value, list):
        for x, y in zip(expect, value):
            if not nested_match(x, y):
                return False
        return True
    return False
","if not nested_match ( v , value [ k ] ) :",162
"def _on_event(self, event):
    event_id = event[""event_id""]
    if event_id == MpvEventID.END_FILE:
        reason = event[""event""][""reason""]
        logger.debug(""Current song finished. reason: %d"" % reason)
        if self.state != State.stopped and reason != MpvEventEndFile.ABORTED:
            self.media_finished.emit()
    elif event_id == MpvEventID.FILE_LOADED:
        self.media_loaded.emit()
",if self . state != State . stopped and reason != MpvEventEndFile . ABORTED :,131
"def __exit__(self, exc_type, exc_value, traceback):
    self.close()
    with DB.connection_context():
        rows = (
            SessionRecord.delete()
            .where(SessionRecord.f_session_id == self._session_id)
            .execute()
        )
        if rows > 0:
            LOGGER.debug(f""delete session {self._session_id} record"")
        else:
            LOGGER.warning(f""failed delete session {self._session_id} record"")
",if rows > 0 :,138
"def decorator(*args, **kwargs):
    # Sets a boolean on the global request context
    g._flask_user_allow_unconfirmed_email = True
    # Catch exceptions to properly unset boolean on exceptions
    try:
        user_manager = current_app.user_manager
        # User must be logged in with a confirmed email address
        allowed = _is_logged_in_with_confirmed_email(user_manager)
        if not allowed:
            # Redirect to unauthenticated page
            return user_manager.unauthenticated_view()
        # It's OK to call the view
        return view_function(*args, **kwargs)
    finally:
        # Allways unset the boolean, whether exceptions occurred or not
        g._flask_user_allow_unconfirmed_email = False
",if not allowed :,190
"def TryMerge(self, d):
    while d.avail() > 0:
        tt = d.getVarInt32()
        if tt == 10:
            self.set_app_id(d.getPrefixedString())
            continue
        if tt == 16:
            self.set_limit(d.getVarInt64())
            continue
        if tt == 0:
            raise ProtocolBuffer.ProtocolBufferDecodeError
        d.skipData(tt)
",if tt == 16 :,122
"def addOptions(parser):
    for optname in options.keys(""default""):
        if optname.startswith(""color_"") or optname.startswith(""disp_""):
            continue
        action = ""store_true"" if options[optname] is False else ""store""
        try:
            parser.add_argument(
                ""--"" + optname.replace(""_"", ""-""),
                action=action,
                dest=optname,
                default=None,
                help=options._opts._get(optname).helpstr,
            )
        except argparse.ArgumentError:
            pass
","if optname . startswith ( ""color_"" ) or optname . startswith ( ""disp_"" ) :",152
"def make_relative_to(self, kwds, relative_to):
    if relative_to and os.path.dirname(relative_to):
        dirname = os.path.dirname(relative_to)
        kwds = kwds.copy()
        for key in ffiplatform.LIST_OF_FILE_NAMES:
            if key in kwds:
                lst = kwds[key]
                if not isinstance(lst, (list, tuple)):
                    raise TypeError(""keyword '%s' should be a list or tuple"" % (key,))
                lst = [os.path.join(dirname, fn) for fn in lst]
                kwds[key] = lst
    return kwds
",if key in kwds :,173
"def _options_fcheck(self, name, xflags, table):
    for entry in table:
        if entry.name is None:
            break
        if entry.flags & XTOPT_MAND and not xflags & (1 << entry.id):
            raise XTablesError(""%s: --%s must be specified"" % (name, entry.name))
            if not xflags & (1 << entry.id):
                continue
",if entry . name is None :,112
"def _load_cmds():
    prefix = ""AOE_CMD_""
    g = globals()
    for k, v in iteritems(g):
        if k.startswith(prefix):
            name = ""aoe"" + k[len(prefix) :].lower()
            try:
                mod = __import__(name, g, level=1)
                AOE.set_cmd(v, getattr(mod, name.upper()))
            except (ImportError, AttributeError):
                continue
",if k . startswith ( prefix ) :,128
"def test_list_sizes(self):
    sizes = self.driver.list_sizes()
    self.assertEqual(len(sizes), 7, ""Wrong sizes count"")
    for size in sizes:
        self.assertTrue(isinstance(size.price, float), ""Wrong size price type"")
        if self.driver.api_name == ""openstack"":
            self.assertEqual(size.price, 0, ""Size price should be zero by default"")
","if self . driver . api_name == ""openstack"" :",105
"def testToFileBinary(self):
    z = dns.zone.from_file(here(""example""), ""example"")
    try:
        f = open(here(""example3-binary.out""), ""wb"")
        z.to_file(f)
        f.close()
        ok = compare_files(
            ""testToFileBinary"", here(""example3-binary.out""), here(""example3.good"")
        )
    finally:
        if not _keep_output:
            os.unlink(here(""example3-binary.out""))
    self.assertTrue(ok)
",if not _keep_output :,146
"def ip_list(_):
    ips = []
    for ip in _.split("" ""):
        if not ip:
            continue
        elif isip(ip):
            ips.append(IP.create(ip))
        else:
            raise TypeError(""ip %s is invalid"" % ip)
    return ips
",if not ip :,82
"def _wait_for_state(self, server_id, state, retries=50):
    for i in (0, retries):
        server = self.ex_get_server(server_id)
        if server.extra[""status""][""state""] == state:
            return
        sleep(5)
        if i == retries:
            raise Exception(""Retries count reached"")
",if i == retries :,95
"def _stretch_prev(data):
    clip, track, item_id, item_data = data
    try:
        prev_index = track.clips.index(clip) - 1
        if prev_index < 0:
            return  # clip is first clip
        if track.clips[prev_index].is_blanck_clip == True:
            # Next clip is blank so we can do this.
            clip = track.clips[prev_index]
            data = (clip, track, item_id, item_data)
            _cover_blank_from_next(data, True)
    except:
        pass  # any error means that this can't be done
",if track . clips [ prev_index ] . is_blanck_clip == True :,173
"def characters(self, ch):
    if self._inside_fuzzable:
        modified_value = self._fuzzed_parameters[self._fuzzable_index][1]
        if isinstance(modified_value, DataToken):
            modified_value = modified_value.get_value()
        if self._fuzzed_parameters[self._fuzzable_index][0] == ""base64"":
            enc_val = base64.b64encode(modified_value)
        else:
            enc_val = cgi.escape(modified_value).encode(""ascii"", ""xmlcharrefreplace"")
        self.fuzzed_xml_string += enc_val
    else:
        self.fuzzed_xml_string += ch
","if isinstance ( modified_value , DataToken ) :",181
"def _make_sure_scheduler_ready(self, timeout=120):
    check_start_time = time.time()
    while True:
        workers_meta = self._scheduler_service._resource_ref.get_workers_meta()
        if not workers_meta:
            # wait for worker to report status
            self._pool.sleep(0.5)
            if time.time() - check_start_time > timeout:  # pragma: no cover
                raise TimeoutError(""Check worker ready timed out."")
        else:
            break
",if not workers_meta :,139
"def tiles_around(self, pos, radius=1, predicate=None):
    ps = []
    x, y = pos
    for dx in range(-radius, radius + 1):
        nx = x + dx
        if nx >= 0 and nx < self.width:
            for dy in range(-radius, radius + 1):
                ny = y + dy
                if ny >= 0 and ny < self.height and (dx != 0 or dy != 0):
                    if predicate is None or predicate((nx, ny)):
                        ps.append((nx, ny))
    return ps
",if ny >= 0 and ny < self . height and ( dx != 0 or dy != 0 ) :,151
"def tearDown(self):
    for i in ScriptVersion.objects.all():
        name = i.script_path.name
        utils.get_storage().delete(name)
        if wooey_settings.WOOEY_EPHEMERAL_FILES:
            try:
                utils.get_storage(local=False).delete(name)
            except WindowsError:
                print(""unable to delete {}"".format(name))
        name += ""c""  # handle pyc junk
        try:
            utils.get_storage().delete(name)
        except WindowsError:
            print(""unable to delete {}"".format(name))
    super(ScriptTearDown, self).tearDown()
",if wooey_settings . WOOEY_EPHEMERAL_FILES :,177
"def _fill_tc_results(self):
    tids = list(self.tc._results.keys())
    fields = [""failures"", ""errors"", ""skipped"", ""expectedFailures""]
    for tid in tids:
        result = self.tc._results[tid]
        for field in fields:
            if not field in self.tc._results:
                self.tc._results[field] = []
            self.tc._results[field].extend(result[field])
",if not field in self . tc . _results :,119
"def check_mixin_inheritance(bases):
    for b in bases:
        check_mixin_inheritance(b.__bases__)
        for k, v in vars(b).items():
            if _is_interesting(k, v):
                _type_info[k] = _process_item(v)
","if _is_interesting ( k , v ) :",78
"def _check_params(swa_freq):
    params = [swa_freq]
    params_none = [param is None for param in params]
    if not all(params_none) and any(params_none):
        warnings.warn(""Some of swa_start, swa_freq is None, ignoring other"")
    for i, param in enumerate(params):
        if param is not None and not isinstance(param, int):
            params[i] = int(param)
            warnings.warn(""Casting swa_start, swa_freq to int"")
    return not any(params_none), params
","if param is not None and not isinstance ( param , int ) :",145
"def findBookmark(self, bookmark, root=None):
    if root == None:
        root = self.bookmarks
    for i, b in enumerate(root):
        if isinstance(b, list):
            res = self.findBookmark(bookmark, b)
            if res:
                return [i] + res
        elif b == bookmark or b[""/Title""] == bookmark:
            return [i]
    return None
",if res :,116
"def best_match(self, matches, default=None):
    best_quality = -1
    result = default
    for server_item in matches:
        for client_item, quality in self:
            if quality <= best_quality:
                break
            if self._value_matches(server_item, client_item) and quality > 0:
                best_quality = quality
                result = server_item
    return result
",if quality <= best_quality :,113
"def validate_external_users(self):
    if self.user and settings.ALLOW_OAUTH2_FOR_EXTERNAL_USERS is False:
        external_account = get_external_account(self.user)
        if external_account is not None:
            raise oauth2.AccessDeniedError(
                _(
                    ""OAuth2 Tokens cannot be created by users associated with an external authentication provider ({})""
                ).format(external_account)
            )
",if external_account is not None :,118
"def get_tzname(self):
    # Timezone conversions must happen to the input datetime *before*
    # applying a function. 2015-12-31 23:00:00 -02:00 is stored in the
    # database as 2016-01-01 01:00:00 +00:00. Any results should be
    # based on the input datetime not the stored datetime.
    tzname = None
    if settings.USE_TZ:
        if self.tzinfo is None:
            tzname = timezone.get_current_timezone_name()
        else:
            tzname = timezone._get_timezone_name(self.tzinfo)
    return tzname
",if self . tzinfo is None :,158
"def _get_editable_fields(cls):
    fds = set([])
    for field in cls._meta.concrete_fields:
        if hasattr(field, ""attname""):
            if field.attname == ""id"":
                continue
            elif field.attname.endswith(""ptr_id""):
                # polymorphic fields should always be non-editable, see:
                # https://github.com/django-polymorphic/django-polymorphic/issues/349
                continue
            if getattr(field, ""editable"", True):
                fds.add(field.attname)
    return fds
","if getattr ( field , ""editable"" , True ) :",159
"def p_advsimd_secondary(val, va, mnem, opcode, flags, opers):
    if opcode == INS_VORR:
        src1 = (val >> 16) & 0xF
        src2 = (val) & 0xF
        if src1 == src2:
            opers = (
                ArmRegOper(rctx.getRegisterIndex(rbase % d)),
                ArmRegOper(rctx.getRegisterIndex(rbase % n)),
            )
            return ""vmov"", INS_VMOV, None, opers
    return None, None, None, None
",if src1 == src2 :,157
"def list_urls(self):
    for idx, job in enumerate(self.urlwatcher.jobs):
        if self.urlwatch_config.verbose:
            print(""%d: %s"" % (idx + 1, repr(job)))
        else:
            pretty_name = job.pretty_name()
            location = job.get_location()
            if pretty_name != location:
                print(""%d: %s ( %s )"" % (idx + 1, pretty_name, location))
            else:
                print(""%d: %s"" % (idx + 1, pretty_name))
    return 0
",if self . urlwatch_config . verbose :,157
"def _split_auth_string(auth_string):
    """"""split a digest auth string into individual key=value strings""""""
    prev = None
    for item in auth_string.split("",""):
        try:
            if prev.count('""') == 1:
                prev = ""%s,%s"" % (prev, item)
                continue
        except AttributeError:
            if prev == None:
                prev = item
                continue
            else:
                raise StopIteration
        yield prev.strip()
        prev = item
    yield prev.strip()
    raise StopIteration
",if prev == None :,152
"def _get_user_auth_session_cookie(self, url, username, password):
    get_response = requests.get(url)
    # auth request to kfp server with istio dex look like '/dex/auth/local?req=REQ_VALUE'
    if ""auth"" in get_response.url:
        credentials = {""login"": username, ""password"": password}
        # Authenticate user
        session = requests.Session()
        session.post(get_response.url, data=credentials)
        cookie_auth_key = ""authservice_session""
        cookie_auth_value = session.cookies.get(cookie_auth_key)
        if cookie_auth_value:
            return cookie_auth_key + ""="" + cookie_auth_value
",if cookie_auth_value :,187
"def copychunked(src, dest):
    chunksize = 524288  # half a meg of bytes
    fsrc = src.open(""rb"")
    try:
        fdest = dest.open(""wb"")
        try:
            while 1:
                buf = fsrc.read(chunksize)
                if not buf:
                    break
                fdest.write(buf)
        finally:
            fdest.close()
    finally:
        fsrc.close()
",if not buf :,131
"def iterate_all_python_files(base_path):
    # TODO support ignored directories/files
    for dirname, subdirlist, filelist in os.walk(base_path):
        if ""__pycache__"" in dirname:
            continue
        for filename in filelist:
            if filename.endswith("".py""):
                yield os.path.join(base_path, dirname, filename)
","if filename . endswith ( "".py"" ) :",95
"def discover(self, *objlist):
    ret = []
    for l in self.splitlines():
        if l[0] != ""intr"":
            continue
        for name, i in enumerate(l[2:]):
            if int(i) > 10:
                ret.append(str(name))
    return ret
",if int ( i ) > 10 :,84
"def call_url(self, expected_url, with_error=False):
    try:
        with self.best_url_selector.select_best_url() as url:
            self.assertEqual(urlparse(expected_url), url)
            if with_error:
                raise RequestException(""error connecting to {}"".format(url))
    except RequestException:
        pass
",if with_error :,95
"def __init__(self, action_space=None, network=None, network_kwargs=None, hparams=None):
    PolicyNetBase.__init__(self, hparams=hparams)
    with tf.variable_scope(self.variable_scope):
        if action_space is None:
            action_space = Space(low=0, high=self._hparams.action_space, dtype=np.int32)
        self._action_space = action_space
        self._append_output_layer()
",if action_space is None :,120
"def gettempfilename(suffix):
    """"""Returns a temporary filename""""""
    if ""_"" in os.environ:
        # tempfile.mktemp() crashes on some Wine versions (the one of Ubuntu 12.04 particularly)
        if os.environ[""_""].find(""wine"") >= 0:
            tmpdir = "".""
            if ""TMP"" in os.environ:
                tmpdir = os.environ[""TMP""]
            import time
            import random
            random.seed(time.time())
            random_part = ""file%d"" % random.randint(0, 1000000000)
            return os.path.join(tmpdir, random_part + suffix)
    return tempfile.mktemp(suffix)
","if ""TMP"" in os . environ :",172
"def get_url(self):
    if self.url_patterns:
        v_url = match1(self.html, *self.url_patterns)
        if v_url.startswith(""http%3A""):
            v_url = compact_unquote(v_url)
        self.v_url = [v_url]
","if v_url . startswith ( ""http%3A"" ) :",83
"def drain(self, fd):
    """"""Make `fd` unreadable.""""""
    while True:
        try:
            if not os.read(fd, 4096):
                return
        except OSError:
            e = sys.exc_info()[1]
            if e.args[0] == errno.EAGAIN:
                return
            raise
","if not os . read ( fd , 4096 ) :",97
"def tearDown(self):
    # make sure all of the subprocesses are dead
    for pidfile in self.pidfiles:
        if not os.path.exists(pidfile):
            continue
        with open(pidfile) as f:
            pid = f.read()
        if not pid:
            return
        pid = int(pid)
        try:
            os.kill(pid, signal.SIGKILL)
        except OSError:
            pass
    # and clean up leftover pidfiles
    for pidfile in self.pidfiles:
        if os.path.exists(pidfile):
            os.unlink(pidfile)
    self.tearDownBasedir()
",if not os . path . exists ( pidfile ) :,167
"def main():
    # Arguments
    input_fname, out_fname = sys.argv[1:]
    # Do conversion.
    index = Indexes()
    offset = 0
    reader_wrapper = GFFReaderWrapper(fileinput.FileInput(input_fname), fix_strand=True)
    for feature in list(reader_wrapper):
        # Add feature; index expects BED coordinates.
        if isinstance(feature, GenomicInterval):
            convert_gff_coords_to_bed(feature)
            index.add(feature.chrom, feature.start, feature.end, offset)
        # Always increment offset, even if feature is not an interval and hence
        # not included in the index.
        offset += feature.raw_size
    index.write(open(out_fname, ""wb""))
","if isinstance ( feature , GenomicInterval ) :",199
"def _s_wise_max(a_indices, a_indptr, vals, out_max):
    n = len(out_max)
    for i in range(n):
        if a_indptr[i] != a_indptr[i + 1]:
            m = a_indptr[i]
            for j in range(a_indptr[i] + 1, a_indptr[i + 1]):
                if vals[j] > vals[m]:
                    m = j
            out_max[i] = vals[m]
",if a_indptr [ i ] != a_indptr [ i + 1 ] :,138
"def update_encryption_keys(self, options):
    if not options[""pools""] and not options[""datasets""]:
        raise CallError(""Please specify pools/datasets to update"")
    async with ENCRYPTION_CACHE_LOCK:
        keys = await self.encryption_keys()
        for pool in options[""pools""]:
            keys[""geli""][pool[""name""]] = pool[""passphrase""]
        for dataset in options[""datasets""]:
            keys[""zfs""][dataset[""name""]] = dataset[""passphrase""]
        await self.middleware.call(""cache.put"", ""failover_encryption_keys"", keys)
        if options[""sync_keys""]:
            await self.sync_keys_to_remote_node(lock=False)
","if options [ ""sync_keys"" ] :",173
"def set_lineno(self, lineno, override=False):
    """"""Set the line numbers of the node and children.""""""
    todo = deque([self])
    while todo:
        node = todo.popleft()
        if ""lineno"" in node.attributes:
            if node.lineno is None or override:
                node.lineno = lineno
        todo.extend(node.iter_child_nodes())
    return self
",if node . lineno is None or override :,103
"def is_ArAX_implicit(ii):  # allows one implicit fixed reg
    a, implicit_fixed = 0, 0
    for op in _gen_opnds(ii):
        if op_luf_start(op, ""ArAX""):
            a += 1
        elif op_reg(op) and op_implicit_specific_reg(op):
            implicit_fixed += 1
        else:
            return False
    return a == 1 and implicit_fixed <= 1
","if op_luf_start ( op , ""ArAX"" ) :",120
"def __iter__(self):
    if hasattr(self, ""error_dict""):
        for field, errors in self.error_dict.items():
            yield field, list(ValidationError(errors))
    else:
        for error in self.error_list:
            message = error.message
            if error.params:
                message %= error.params
            yield force_text(message)
",if error . params :,103
"def _mul_matrix(self, other):
    if isinstance(other, ConstantDiagLazyTensor):
        if not self.diag_shape == other.diag_shape:
            raise ValueError(
                ""Dimension Mismatch: Must have same diag_shape, but got ""
                f""{self.diag_shape} and {other.diag_shape}""
            )
        return self.__class__(
            self.diag_values * other.diag_values, diag_shape=self.diag_shape
        )
    return super()._mul_matrix(other)
",if not self . diag_shape == other . diag_shape :,141
"def test_no_metadata_when_py_is_pep8(py_file):
    """"""This test assumes that all Python files in the jupytext folder follow PEP8 rules""""""
    nb = read(py_file)
    for i, cell in enumerate(nb.cells):
        if ""title"" in cell.metadata:
            cell.metadata.pop(""title"")  # pragma: no cover
        if i == 0 and not cell.source:
            assert cell.metadata == {""lines_to_next_cell"": 0}, py_file
        else:
            assert not cell.metadata, (py_file, cell.source)
","if ""title"" in cell . metadata :",155
"def forward(self, x: Tensor, edge_index: Adj) -> Tensor:
    """"""""""""
    if self.add_self_loops:
        if isinstance(edge_index, Tensor):
            edge_index, _ = remove_self_loops(edge_index)
            edge_index, _ = add_self_loops(edge_index, num_nodes=x.size(self.node_dim))
        elif isinstance(edge_index, SparseTensor):
            edge_index = set_diag(edge_index)
    x_norm = F.normalize(x, p=2.0, dim=-1)
    # propagate_type: (x: Tensor, x_norm: Tensor)
    return self.propagate(edge_index, x=x, x_norm=x_norm, size=None)
","if isinstance ( edge_index , Tensor ) :",196
"def should_wait(self, offer_hash: str):
    with self._lock:
        if self._offer_hash is not None:
            if self._offer_hash != offer_hash:
                logger.debug(
                    ""already processing another offer (%s vs %s)"",
                    self._offer_hash,
                    offer_hash,
                )
                return True
            if self._started == self._wtct_num_subtasks:
                logger.info(""all subtasks for `%s` have been started"", self._offer_hash)
                return True
        return False
",if self . _offer_hash != offer_hash :,167
"def _wrap_linespans(self, inner):
    s = self.linespans
    i = self.linenostart - 1
    for t, line in inner:
        if t:
            i += 1
            yield 1, '<span id=""%s-%d"">%s</span>' % (s, i, line)
        else:
            yield 0, line
",if t :,94
"def onRemoteResponse(self, response, request_info):
    if isinstance(response, (dict,)):
        if ""echo"" in response:
            msg = ""Celery echo: %s\nElapsed Time: %d""
            self.setText(msg % (response[""echo""], self.wait_cnt))
        else:
            msg = ""Waiting for Celery (id, checkno): %s, %d""
            Label.setText(self, msg % (self.task_id, self.wait_cnt))
    else:
        self.setText(""Could not get remote response as a dictionary"")
","if ""echo"" in response :",151
"def Visit_expr_stmt(self, node):  # pylint: disable=invalid-name
    # expr_stmt ::= testlist_star_expr (augassign (yield_expr|testlist)
    #               | ('=' (yield_expr|testlist_star_expr))*)
    for child in node.children:
        self.Visit(child)
        if isinstance(child, pytree.Leaf) and child.value == ""="":
            _AppendTokenSubtype(child, format_token.Subtype.ASSIGN_OPERATOR)
","if isinstance ( child , pytree . Leaf ) and child . value == ""="" :",136
"def _list_outputs(self):
    outputs = self.output_spec().get()
    isHeader = True
    for key in self._outfields:
        outputs[key] = []  # initialize outfields
    with open(self.inputs.in_file, ""r"") as fid:
        for line in fid.readlines():
            if self.inputs.header and isHeader:  # skip header line
                isHeader = False
                continue
            entry = self._parse_line(line)
            outputs = self._append_entry(outputs, entry)
    return outputs
",if self . inputs . header and isHeader :,148
"def _get_tables(self, schema):
    cursor = self._get_cursor()
    schemas = self.configuration.get(
        ""schemas"", self.configuration.get(""database"", """")
    ).split("","")
    for schema_name in schemas:
        cursor.columns(schema=schema_name)
        for column in cursor:
            table_name = ""{}.{}"".format(column[1], column[2])
            if table_name not in schema:
                schema[table_name] = {""name"": table_name, ""columns"": []}
            schema[table_name][""columns""].append(column[3])
    return list(schema.values())
",if table_name not in schema :,162
"def __setitem__(self, index, value):
    if self._physics.is_dirty and not self._triggers_dirty:
        self._physics.forward()
    super(_SynchronizingArrayWrapper, self).__setitem__(index, value)
    if isinstance(self._backing_index, collections.Iterable):
        if isinstance(index, tuple):
            resolved_index = (self._backing_index[index[0]],) + index[1:]
        else:
            resolved_index = self._backing_index[index]
        self._backing_array[resolved_index] = value
    if self._triggers_dirty:
        self._physics.mark_as_dirty()
","if isinstance ( index , tuple ) :",171
"def fit_test_data(self, data, fit_values, imputer_value):
    for j in range(len(data)):
        for i in range(len(data[j])):
            if data[j][i] in imputer_value:
                data[j][i] = str(fit_values[i])
    return data
",if data [ j ] [ i ] in imputer_value :,87
"def Compare_in(t, x):
    if not isinstance(x.ops[0], (ast.NotIn, ast.In)):
        return
    if t.enable_snippets:
        from ..snippets import _in, in_es6
        if t.enable_es6:
            t.add_snippet(in_es6)
            sname = ""in_es6""
        else:
            t.add_snippet(_in)
            sname = ""_in""
        result = JSCall(JSAttribute(""_pj"", sname), [x.left, x.comparators[0]])
        if isinstance(x.ops[0], ast.NotIn):
            result = JSUnaryOp(JSOpNot(), result)
        return result
","if isinstance ( x . ops [ 0 ] , ast . NotIn ) :",189
"def __init__(self, f):
    self._refs = {}
    self._peeled = {}
    for line in f.readlines():
        sha, name = line.rstrip(b""\n"").split(b""\t"")
        if name.endswith(ANNOTATED_TAG_SUFFIX):
            name = name[:-3]
            if not check_ref_format(name):
                raise ValueError(""invalid ref name %r"" % name)
            self._peeled[name] = sha
        else:
            if not check_ref_format(name):
                raise ValueError(""invalid ref name %r"" % name)
            self._refs[name] = sha
",if not check_ref_format ( name ) :,171
"def info(args):
    # Check grammar
    p = Python37Parser()
    if len(args) > 0:
        arg = args[0]
        if arg == ""3.7"":
            from uncompyle6.parser.parse37 import Python37Parser
            p = Python37Parser()
        elif arg == ""3.8"":
            from uncompyle6.parser.parse38 import Python38Parser
            p = Python38Parser()
        else:
            raise RuntimeError(""Only 3.7 and 3.8 supported"")
    p.check_grammar()
    if len(sys.argv) > 1 and sys.argv[1] == ""dump"":
        print(""-"" * 50)
        p.dump_grammar()
","if arg == ""3.7"" :",181
"def test_ESPnetDataset_text_float(text_float):
    dataset = IterableESPnetDataset(
        path_name_type_list=[(text_float, ""data8"", ""text_float"")],
        preprocess=preprocess,
    )
    for key, data in dataset:
        if key == ""a"":
            assert all((data[""data8""]) == np.array([1.4, 3.4], dtype=np.float32))
        if key == ""b"":
            assert all((data[""data8""]) == np.array([0.9, 9.3], dtype=np.float32))
","if key == ""b"" :",152
"def getting(self, key, lock=False):
    if not lock:
        yield self.get(key)
    else:
        locked = False
        try:
            data = self._get_or_lock(key)
            locked = data is None
            yield data
        finally:
            if locked:
                self._release_lock(key)
",if locked :,99
"def mkdir(self, path, parents=True, raise_if_exists=False):
    if self.exists(path):
        if not self.isdir(path):
            raise luigi.target.NotADirectory()
        elif raise_if_exists:
            raise luigi.target.FileAlreadyExists()
        else:
            return
    self.conn.files_create_folder_v2(path)
",if not self . isdir ( path ) :,106
"def _get_initiated_elections(cls, height, txns):
    elections = []
    for tx in txns:
        if not isinstance(tx, Election):
            continue
        elections.append(
            {""election_id"": tx.id, ""height"": height, ""is_concluded"": False}
        )
    return elections
","if not isinstance ( tx , Election ) :",93
"def recalc_active(self, ts):
    if not self.active_seconds:
        self.active_seconds.append(ts)
        self.data[ts] = {}
    if ts not in self.active_seconds:
        if ts > max(self.active_seconds):
            for i in range(max(self.active_seconds) + 1, ts + 1):
                self.active_seconds.append(i)
                self.active_seconds.sort()
                self.data[i] = {}
    while len(self.active_seconds) > self.window:
        self.active_seconds.pop(0)
    for sec in self.data.keys():
        if sec not in self.active_seconds:
            self.data.pop(sec)
",if ts > max ( self . active_seconds ) :,200
"def get_scalar_base(schema, scalar) -> Tuple[str, ...]:
    base = base_type_name_map.get(scalar.id)
    if base is not None:
        return base
    for ancestor in scalar.get_ancestors(schema).objects(schema):
        if not ancestor.get_is_abstract(schema):
            # Check if base is fundamental, if not, then it is
            # another domain.
            try:
                base = base_type_name_map[ancestor.id]
            except KeyError:
                base = common.get_backend_name(schema, ancestor, catenate=False)
            return base
    raise ValueError(
        f""cannot determine backend type for scalar type "" f""{scalar.get_name(schema)}""
    )
",if not ancestor . get_is_abstract ( schema ) :,200
"def __next__(self):
    try:
        value = next(self._iterable)
        if self.start_time is None:
            self.start()
        else:
            self.update(self.value + 1)
        return value
    except StopIteration:
        self.finish()
        raise
    except GeneratorExit:  # pragma: no cover
        self.finish(dirty=True)
        raise
",if self . start_time is None :,109
"def change_password(username=""flexget"", password="""", session=None):
    check = zxcvbn.zxcvbn(password, user_inputs=[username])
    if check[""score""] < 3:
        warning = check[""feedback""][""warning""]
        suggestions = "" "".join(check[""feedback""][""suggestions""])
        message = ""Password '{}' is not strong enough. "".format(password)
        if warning:
            message += warning + "" ""
        if suggestions:
            message += ""Suggestions: {}"".format(suggestions)
        raise WeakPassword(message)
    user = get_user(username=username, session=session)
    user.password = str(generate_password_hash(password))
    session.commit()
",if warning :,175
"def _options_fcheck(self, name, xflags, table):
    for entry in table:
        if entry.name is None:
            break
        if entry.flags & XTOPT_MAND and not xflags & (1 << entry.id):
            raise XTablesError(""%s: --%s must be specified"" % (name, entry.name))
            if not xflags & (1 << entry.id):
                continue
",if not xflags & ( 1 << entry . id ) :,112
"def parse_ports(container_name, connection_configuration):
    while True:
        ports_command = docker_util.build_docker_simple_command(
            ""port"", container_name=container_name, **connection_configuration
        )
        with tempfile.TemporaryFile(prefix=""docker_port_"") as stdout_file:
            exit_code = subprocess.call(
                ports_command, shell=True, stdout=stdout_file, preexec_fn=os.setpgrp
            )
            if exit_code == 0:
                stdout_file.seek(0)
                ports_raw = stdout_file.read().decode(""utf-8"")
                return ports_raw
",if exit_code == 0 :,180
"def _init_ti_table():
    global _ti_table
    _ti_table = []
    for fname, name in zip(kc.STRFNAMES, kc.STRNAMES):
        seq = termcap.get(name)
        if not seq:
            continue
        k = _name_to_key(fname)
        if k:
            _ti_table.append((list(bytearray(seq)), k))
",if k :,109
"def sanitize_args(a):
    try:
        args, kwargs = a
        if isinstance(args, tuple) and isinstance(kwargs, dict):
            return args, dict(kwargs)
    except (TypeError, ValueError):
        args, kwargs = (), {}
    if a is not None:
        if isinstance(a, dict):
            args = tuple()
            kwargs = a
        elif isinstance(a, tuple):
            if isinstance(a[-1], dict):
                args, kwargs = a[0:-1], a[-1]
            else:
                args = a
                kwargs = {}
    return args, kwargs
","if isinstance ( a , dict ) :",168
"def fork_with_import_lock(level):
    release = 0
    in_child = False
    try:
        try:
            for i in range(level):
                imp.acquire_lock()
                release += 1
            pid = os.fork()
            in_child = not pid
        finally:
            for i in range(release):
                imp.release_lock()
    except RuntimeError:
        if in_child:
            if verbose > 1:
                print(""RuntimeError in child"")
            os._exit(1)
        raise
    if in_child:
        os._exit(0)
    self.wait_impl(pid)
",if verbose > 1 :,183
"def _capture_hub(self, create):
    # Subclasses should call this as the first action from any
    # public method that could, in theory, block and switch
    # to the hub. This may release the GIL.
    if self.hub is None:
        # This next line might release the GIL.
        current_hub = get_hub() if create else get_hub_if_exists()
        if current_hub is None:
            return
        # We have the GIL again. Did anything change? If so,
        # we lost the race.
        if self.hub is None:
            self.hub = current_hub
",if current_hub is None :,161
"def get_user_makepkg_path(cls) -> Optional[str]:
    if cls._user_makepkg_path == ""unset"":
        possible_paths = [
            os.path.expanduser(""~/.makepkg.conf""),
            os.path.join(CONFIG_ROOT, ""pacman/makepkg.conf""),
        ]
        config_path: Optional[str] = None
        for path in possible_paths:
            if os.path.exists(path):
                config_path = path
        cls._user_makepkg_path = config_path
    return cls._user_makepkg_path
",if os . path . exists ( path ) :,154
"def createValue(self):
    mode = []
    for name in self._text_keys:
        if self[name].value:
            if 4 <= len(mode):
                mode.append(""..."")
                break
            else:
                mode.append(name)
    if mode:
        return "", "".join(mode)
    else:
        return ""(none)""
",if self [ name ] . value :,102
"def keyPressEvent(self, event):
    if event.key() in (Qt.Key_Right, Qt.Key_Left):
        direction = 1
        if event.key() == Qt.Key_Left:
            direction = -1
        if event.modifiers() == Qt.ShiftModifier:
            print(""shift"")
            direction *= 10
        self.timeline.setValue(self.timeline.value() + direction)
    else:
        super(VideoPlayerWidget, self).keyPressEvent(event)
",if event . modifiers ( ) == Qt . ShiftModifier :,131
"def validate_wrapper(*args, **kwargs):
    result = self.validate_func(*args, **kwargs)
    if request.is_xhr:
        if not isinstance(result, dict):
            result = {}
        result.setdefault(""success"", True)
        values = result.get(""values"", {})
        for key, value in tmpl_context.form_values.iteritems():
            values.setdefault(key, value)
    return result
","if not isinstance ( result , dict ) :",110
"def copy_metadata_to(self, target_dir):
    prefix = os.path.join(self.egg_info, """")
    for path in self.ei_cmd.filelist.files:
        if path.startswith(prefix):
            target = os.path.join(target_dir, path[len(prefix) :])
            ensure_directory(target)
            self.copy_file(path, target)
",if path . startswith ( prefix ) :,103
"def _get_switch_info(self, cmd_list):
    stdout, stderr, sw_data = None, None, None
    try:
        stdout, stderr = self._run_ssh(cmd_list, True)
        LOG.debug(""CLI output from ssh - output: %s"", stdout)
        if stdout:
            sw_data = stdout.splitlines()
        return sw_data
    except processutils.ProcessExecutionError as e:
        msg = _(
            ""Error while getting data via ssh: (command=%(cmd)s "" ""error=%(err)s).""
        ) % {""cmd"": cmd_list, ""err"": six.text_type(e)}
        LOG.error(msg)
        raise exception.CiscoZoningCliException(reason=msg)
",if stdout :,191
"def analyze(vw):
    for va, dest in vw.findPointers():
        # Is there a location already at the target?
        loc = vw.getLocation(dest)
        if loc is None:
            continue
        if loc[L_LTYPE] != LOC_IMPORT:
            continue
        offset, bytes = vw.getByteDef(va)
        if offset < 2:
            continue
        if bytes[offset - 2 : offset] == b""\xff\x15"":  # call [importloc]
            # If there's a pointer here, remove it.
            if vw.getLocation(va):
                vw.delLocation(va)
            vw.makeCode(va - 2)
",if offset < 2 :,192
"def _freeze_stages(self):
    """"""Freeze parameters.""""""
    if self.frozen_stages >= 0:
        if self.deep_stem:
            self.stem.eval()
            for param in self.stem.parameters():
                param.requires_grad = False
        else:
            self.norm1.eval()
            for m in [self.conv1, self.norm1]:
                for param in m.parameters():
                    param.requires_grad = False
    for i in range(1, self.frozen_stages + 1):
        m = getattr(self, f""layer{i}"")
        m.eval()
        for param in m.parameters():
            param.requires_grad = False
",if self . deep_stem :,191
"def seek(self, timestamp, log=True):
    """"""Seek to a particular timestamp in the movie.""""""
    if self.status in [PLAYING, PAUSED]:
        player = self._player
        if player and player.is_seekable():
            player.set_time(int(timestamp * 1000.0))
            self._vlc_clock.reset(timestamp)
            if self.status == PAUSED:
                self._pause_time = timestamp
        if log:
            logAttrib(self, log, ""seek"", timestamp)
",if self . status == PAUSED :,137
"def foundNestedPseudoClass(self):
    i = self.pos + 1
    openParen = 0
    while i < len(self.source_text):
        ch = self.source_text[i]
        if ch == ""{"":
            return True
        elif ch == ""("":
            # pseudoclasses can contain ()
            openParen += 1
        elif ch == "")"":
            if openParen == 0:
                return False
            openParen -= 1
        elif ch == "";"" or ch == ""}"":
            return False
        i += 1
    return False
","if ch == ""{"" :",155
"def update(events):
    if failsToWriteToIDClasses():
        print(""Skip event: cannot write to ID classes"")
        return
    if didNameChange() or events.intersection({""File"", ""Addon"", ""Tree""}):
        updateEverything()
    if problems.canAutoExecute():
        nodeTrees = list(iterAutoExecutionNodeTrees(events))
        if len(nodeTrees) > 0:
            setupExecutionUnits()
            executeNodeTrees(nodeTrees)
            afterExecution()
            finishExecutionUnits()
",if len ( nodeTrees ) > 0 :,136
"def check_all_verified(self):
    if not self.all_verified:
        new_all_verified = not self.lines.filter(verified=False).exists()
        if new_all_verified:
            self.all_verified = True
            if self.require_verification:
                self.add_log_entry(
                    _(""All rows requiring verification have been verified."")
                )
                self.require_verification = False
            self.save()
    return self.all_verified
",if new_all_verified :,136
"def parse_for(cls, tagname, parser, bits, options):
    if bits:
        if bits[0] == ""for"":
            bits.pop(0)
            if len(bits):
                options[""for""] = Variable(bits.pop(0))
            else:
                raise TemplateSyntaxError(
                    ""%s: expected an argument "" 'after ""for"".' % tagname
                )
        elif not cls.optional_for_parameter:
            raise TemplateSyntaxError(
                ""Unknown argument for %s tag: %r."" % (tagname, bits[0])
            )
","if bits [ 0 ] == ""for"" :",161
"def _get_cuda_device(*args):
    # Returns cuda.Device or DummyDevice.
    for arg in args:
        if type(arg) is not bool and isinstance(arg, _integer_types):
            check_cuda_available()
            return Device(arg)
        if isinstance(arg, ndarray):
            if arg.device is None:
                continue
            return arg.device
        if available and isinstance(arg, Device):
            return arg
    # NOTE: This function returns DummyDevice for both NumPy and ChainerX
    return DummyDevice
","if type ( arg ) is not bool and isinstance ( arg , _integer_types ) :",144
"def while1_test(a, b, c):
    while 1:
        if a != 2:
            if b:
                a = 3
                b = 0
            elif c:
                c = 0
            else:
                a += b + c
                break
    return a, b, c
",if b :,94
"def write_notes(self, family, father, mother):
    # FIXME:
    # if self.restrict and self.exclnotes:
    #    return
    self.write_note_of_person(father)
    self.write_note_of_person(mother)
    child_ref_list = family.get_child_ref_list()
    if child_ref_list:
        for child_ref in child_ref_list:
            child = self.db.get_person_from_handle(child_ref.ref)
            if child:
                self.write_note_of_person(child)
",if child :,160
"def GetFile(cls, session, sig, mode=""r""):
    sig = sig[: cls.HASH_LEN]
    while len(sig) > 0:
        fn = cls.SaveFile(session, sig)
        try:
            if os.path.exists(fn):
                return (open(fn, mode), sig)
        except (IOError, OSError):
            pass
        if len(sig) > 1:
            sig = sig[:-1]
        else:
            if ""r"" in mode:
                return (None, sig)
            else:
                return (open(fn, mode), sig)
    # Not reached
    return (None, None)
",if os . path . exists ( fn ) :,180
"def _generate_expression(self):
    # turn my _format attribute into the _expression attribute
    e = []
    for part in PARSE_RE.split(self._format):
        if not part:
            continue
        elif part == ""{{"":
            e.append(r""\{"")
        elif part == ""}}"":
            e.append(r""\}"")
        elif part[0] == ""{"" and part[-1] == ""}"":
            # this will be a braces-delimited field to handle
            e.append(self._handle_field(part))
        else:
            # just some text to match
            e.append(REGEX_SAFETY.sub(self._regex_replace, part))
    return """".join(e)
","elif part [ 0 ] == ""{"" and part [ - 1 ] == ""}"" :",189
"def get_cfg_dict(self, with_meta=True):
    options_dict = self.merged_options
    if with_meta:
        if self.plugin:
            options_dict.update(
                {""package"": ""yandextank.plugins.{}"".format(self.plugin)}
            )
        if self.enabled is not None:
            options_dict.update({""enabled"": self.enabled})
    return options_dict
",if self . plugin :,111
"def __str__(self):
    _outicalfile = self._icalfile
    for unit in self.units:
        for location in unit.getlocations():
            match = re.match(""\\[(?P<uid>.+)\\](?P<property>.+)"", location)
            for component in self._icalfile.components():
                if component.name != ""VEVENT"":
                    continue
                if component.uid.value != match.groupdict()[""uid""]:
                    continue
                for property in component.getChildren():
                    if property.name == match.groupdict()[""property""]:
                        property.value = unit.target
    if _outicalfile:
        return str(_outicalfile.serialize())
    else:
        return """"
","if component . name != ""VEVENT"" :",198
"def process_events(self, events):
    for event in events:
        key = (event.ident, event.filter)
        if event.ident == self._force_wakeup_fd:
            self._force_wakeup.drain()
            continue
        receiver = self._registered[key]
        if event.flags & select.KQ_EV_ONESHOT:
            del self._registered[key]
        if type(receiver) is _core.Task:
            _core.reschedule(receiver, outcome.Value(event))
        else:
            receiver.put_nowait(event)
",if event . flags & select . KQ_EV_ONESHOT :,154
"def forward(self, start=True, search=False, target=None, include_current=False):
    """"""Move one step forward in the history.""""""
    if target is None:
        target = self.saved_line
    if self.index > 1:
        if search:
            self.index -= self.find_partial_match_forward(target, include_current)
        elif start:
            self.index -= self.find_match_forward(target, include_current)
        else:
            self.index -= 1
        return self.entry
    else:
        self.index = 0
        return self.saved_line
",if search :,161
"def _charlabels(self, options):
    """"""Get labels for characters (PRIVATE).""""""
    self.charlabels = {}
    opts = CharBuffer(options)
    while True:
        # get id and state
        w = opts.next_word()
        if w is None:  # McClade saves and reads charlabel-lists with terminal comma?!
            break
        identifier = self._resolve(w, set_type=CHARSET)
        state = quotestrip(opts.next_word())
        self.charlabels[identifier] = state
        # check for comma or end of command
        c = opts.next_nonwhitespace()
        if c is None:
            break
        elif c != "","":
            raise NexusError(""Missing ',' in line %s."" % options)
","elif c != "","" :",198
"def _get_cloudstorage_bucket_iam_member_bindings(self, raw_bucket):
    bucket_iam_policy = raw_bucket.iam_policy
    member_bindings = {}
    if bucket_iam_policy:
        for binding in bucket_iam_policy._bindings:
            if ""legacy"" not in binding[""role""]:
                for member in binding[""members""]:
                    if member not in member_bindings:
                        member_bindings[member] = [binding[""role""]]
                    else:
                        member_bindings[member].append(binding[""role""])
    return member_bindings
","if ""legacy"" not in binding [ ""role"" ] :",159
"def _gen():
    for i in dataset():
        if isinstance(i, tuple) or isinstance(i, list):
            if fn(*i) is True:
                yield i
        else:
            if fn(i) is True:
                yield i
",if fn ( i ) is True :,72
"def set_img_to_eval_imgs(self, scores, img_ids, method):
    for img_id, score in zip(img_ids, scores):
        if img_id not in self.img_to_eval:
            self.img_to_eval[img_id] = dict()
            self.img_to_eval[img_id][""image_id""] = img_id
        self.img_to_eval[img_id][method] = score
",if img_id not in self . img_to_eval :,118
"def _compute_totals(self):
    totals = {}
    for entry in self.entries:
        for k, v in entry.nutrition_information.items():
            if k not in totals:
                totals[k] = v
            else:
                totals[k] += v
    self._totals = totals
",if k not in totals :,87
"def analyzeFunction(vw, funcva):
    for fromva, tova, rtype, rflags in vw.getXrefsFrom(funcva, v_const.REF_CODE):
        # You goin NOWHERE!
        loc = vw.getLocation(tova)
        if loc is None:
            continue
        # FIXME this could check for thunks to other known function pointers...
        va, size, ltype, linfo = loc
        if ltype != v_const.LOC_IMPORT:
            continue
        vw.makeFunctionThunk(funcva, linfo)
",if ltype != v_const . LOC_IMPORT :,148
"def clear_output_directory(self):
    files = os.listdir(os.path.join(""functional"", ""output""))
    for f in files:
        if f in (""README.txt"", "".svn"", ""CVS""):
            continue  # don't touch the infrastructure
        path = os.path.join(""functional"", ""output"", f)
        if os.path.isdir(path):
            shutil.rmtree(path)
        else:
            os.remove(path)
","if f in ( ""README.txt"" , "".svn"" , ""CVS"" ) :",121
"def test_output_files_as_none_string(self):
    for name in ""Output"", ""Report"", ""Log"", ""XUnit"", ""DebugFile"":
        attr = (name[:-4] if name.endswith(""File"") else name).lower()
        settings = RobotSettings({name.lower(): ""NoNe""})
        assert_equals(settings[name], None)
        if hasattr(settings, attr):
            assert_equals(getattr(settings, attr), None)
","if hasattr ( settings , attr ) :",117
"def is_rotated(box_list):
    if type(box_list) == np.ndarray:
        return box_list.shape[1] == 5
    elif type(box_list) == list:
        if box_list == []:  # cannot decide the box_dim
            return False
        return np.all(
            np.array(
                [
                    (len(obj) == 5)
                    and ((type(obj) == list) or (type(obj) == np.ndarray))
                    for obj in box_list
                ]
            )
        )
    return False
",if box_list == [ ] :,167
"def visit_loop(self):
    v = self.vS.top_front()
    i = self.iS.top_front()
    num_edges = len(self.graph[v].edges)
    # Continue traversing out-edges until none left.
    while i <= num_edges:
        # Continuation
        if i > 0:
            # Update status for previously traversed out-edge
            self.finish_edge(v, i - 1)
        if i < num_edges and self.begin_edge(v, i):
            return
        i += 1
    # Finished traversing out edges, update component info
    self.finish_visiting(v)
",if i > 0 :,167
"def GetConvertersByClass(value_cls):
    """"""Returns all converters that take given value as an input value.""""""
    try:
        return ExportConverter.converters_cache[value_cls]
    except KeyError:
        results = [
            cls
            for cls in ExportConverter.classes.values()
            if cls.input_rdf_type == value_cls
        ]
        if not results:
            results = [DataAgnosticExportConverter]
        ExportConverter.converters_cache[value_cls] = results
        return results
",if not results :,138
"def migrate_Context(self):
    for old_obj in self.session_old.query(self.model_from[""Context""]):
        new_obj = self.model_to[""Context""]()
        for key in new_obj.__table__.columns._data.keys():
            if key not in old_obj.__table__.columns._data.keys():
                continue
            value = getattr(old_obj, key)
            if key == ""tip_timetolive"" and value < 0:
                value = 0
            setattr(new_obj, key, value)
        self.session_new.add(new_obj)
","if key == ""tip_timetolive"" and value < 0 :",161
"def _bind_to(self, url, bind):
    """"""Bind to a Connectable in the caller's thread.""""""
    if isinstance(bind, util.string_types + (url.URL,)):
        try:
            self.context._engine = self.__engines[bind]
        except KeyError:
            e = sqlalchemy.create_engine(bind)
            self.__engines[bind] = e
            self.context._engine = e
    else:
        # TODO: this is squirrely.  we shouldn't have to hold onto engines
        # in a case like this
        if bind not in self.__engines:
            self.__engines[bind] = bind
        self.context._engine = bind
",if bind not in self . __engines :,180
"def _gen_Less(self, args, ret_type):
    result = []
    for lhs, rhs in pairwise(args):
        if ret_type == real_type:
            result.append(self.builder.fcmp_ordered(""<"", lhs, rhs))
        elif ret_type == int_type:
            result.append(self.builder.icmp_signed(""<"", lhs, rhs))
        else:
            raise CompileError()
    return reduce(self.builder.and_, result)
",if ret_type == real_type :,120
"def _store_pickle_output(self, pickle_output):
    if pickle_output:
        if self.output_options.output is None:
            self.error(""Can't use without --output"", ""pickle-output"")
        elif not load_pytd.is_pickle(self.output_options.output):
            self.error(
                ""Must specify %s file for --output"" % load_pytd.PICKLE_EXT,
                ""pickle-output"",
            )
    self.output_options.pickle_output = pickle_output
",if self . output_options . output is None :,141
"def resolve_identifier(self, identifier):
    if "":"" in identifier:
        conn, pn = identifier.split("":"")
        if pn.isdigit():
            pn = int(pn)
        return self.resolve_identifier(self.connector_table[conn][pn])
    else:
        return identifier
",if pn . isdigit ( ) :,75
"def add_braces_and_labels(self):
    for attr in ""horizontal_parts"", ""vertical_parts"":
        if not hasattr(self, attr):
            continue
        parts = getattr(self, attr)
        for subattr in ""braces"", ""labels"":
            if hasattr(parts, subattr):
                self.add(getattr(parts, subattr))
","if hasattr ( parts , subattr ) :",97
"def on_janitor_selection_changed(self, selection):
    model, iter = selection.get_selected()
    if iter:
        if self.janitor_model.iter_has_child(iter):
            iter = self.janitor_model.iter_children(iter)
        plugin = model[iter][self.JANITOR_PLUGIN]
        for row in self.result_model:
            if row[self.RESULT_PLUGIN] == plugin:
                self.result_view.get_selection().select_path(row.path)
                log.debug(""scroll_to_cell: %s"" % row.path)
                self.result_view.scroll_to_cell(row.path)
",if self . janitor_model . iter_has_child ( iter ) :,184
"def canonical_standard_headers(self, headers):
    interesting_headers = [""content-md5"", ""content-type"", ""date""]
    hoi = []
    if ""Date"" in headers:
        del headers[""Date""]
    headers[""Date""] = self._get_date()
    for ih in interesting_headers:
        found = False
        for key in headers:
            lk = key.lower()
            if headers[key] is not None and lk == ih:
                hoi.append(headers[key].strip())
                found = True
        if not found:
            hoi.append("""")
    return ""\n"".join(hoi)
",if not found :,172
"def boolean(value):
    if isinstance(value, str):
        v = value.lower()
        if v in (""1"", ""yes"", ""true"", ""on""):
            return True
        if v in (""0"", ""no"", ""false"", ""off""):
            return False
        raise ValueError(value)
    return bool(value)
","if v in ( ""0"" , ""no"" , ""false"" , ""off"" ) :",87
"def get_extension_for_class(self, extclass):
    if extclass is UnrecognizedExtension:
        raise TypeError(
            ""UnrecognizedExtension can't be used with ""
            ""get_extension_for_class because more than one instance of the""
            "" class may be present.""
        )
    for ext in self:
        if isinstance(ext.value, extclass):
            return ext
    raise ExtensionNotFound(""No {} extension was found"".format(extclass), extclass.oid)
","if isinstance ( ext . value , extclass ) :",126
"def sysargs_to_mainargs():
    """"""builds main args from sys.argv""""""
    relative_out_dir = None
    if len(sys.argv) > 1 and sys.argv[1].startswith(""--""):
        a = sys.argv.pop(1)
        if a.startswith(""--help""):
            print(__doc__)
            sys.exit(1)
        elif a.startswith(""--reldir=""):
            relative_out_dir = a[len(""--reldir="") :]
        else:
            print(""*** Error, Unknown option:"", a)
            print(__doc__)
            sys.exit(1)
    other_session = sys.argv[1]
    return relative_out_dir, other_session
","if a . startswith ( ""--help"" ) :",181
"def _scanDirectory(self, dirIter, f):
    while len(f) < 250:
        try:
            info = next(dirIter)
        except StopIteration:
            if not f:
                raise EOFError
            return f
        if isinstance(info, defer.Deferred):
            info.addCallback(self._cbScanDirectory, dirIter, f)
            return
        else:
            f.append(info)
    return f
",if not f :,122
"def register_options(config_block):
    for name in common_block:
        safe_declare_common_option(config_block, name)
        if config_block.get(name)._argparse is None:
            config_block.get(name).declare_as_argument()
",if config_block . get ( name ) . _argparse is None :,70
"def _loc(obj):
    try:
        fn = getattr(obj, ""__file__"", None)
        if fn is not None:
            return "" @%s"" % (fn,)
        obj = getattr(obj, ""im_func"", obj)
        code = getattr(obj, ""__code__"", None)
        if code is not None:
            return "" @%s:%s"" % (code.co_filename, code.co_firstlineno)
    except Exception:
        pass
    return """"
",if fn is not None :,126
"def _remove_temporary_files(self, temporary_files):
    """"""Internal function for cleaning temporary files""""""
    for file_object in temporary_files:
        file_name = file_object.name
        file_object.close()
        if os.path.exists(file_name):
            os.remove(file_name)
        arff_file_name = file_name + "".arff""
        if os.path.exists(arff_file_name):
            os.remove(arff_file_name)
",if os . path . exists ( file_name ) :,129
"def show(self):
    """"""Overrides Qt Method""""""
    QWidget.show(self)
    self.emit(SIGNAL(""visibility_changed(bool)""), True)
    if self.editor is not None:
        text = self.editor.get_selected_text()
        if len(text) > 0:
            self.search_text.setEditText(text)
            self.search_text.lineEdit().selectAll()
            self.refresh()
        else:
            self.search_text.lineEdit().selectAll()
        self.search_text.setFocus()
",if len ( text ) > 0 :,150
"def flush_input() -> None:
    if not self.is_done:
        # Get keys, and feed to key processor.
        keys = self.input.flush_keys()
        self.key_processor.feed_multiple(keys)
        self.key_processor.process_keys()
        if self.input.closed:
            f.set_exception(EOFError)
",if self . input . closed :,95
"def get_default_taxes_and_charges(master_doctype, tax_template=None, company=None):
    if not company:
        return {}
    if tax_template and company:
        tax_template_company = frappe.db.get_value(
            master_doctype, tax_template, ""company""
        )
        if tax_template_company == company:
            return
    default_tax = frappe.db.get_value(
        master_doctype, {""is_default"": 1, ""company"": company}
    )
    return {
        ""taxes_and_charges"": default_tax,
        ""taxes"": get_taxes_and_charges(master_doctype, default_tax),
    }
",if tax_template_company == company :,182
"def dump_prefs(self):
    ret = """"
    for pref in self.prefs:
        if type(self.prefs[pref].value) == int:
            value = str(self.prefs[pref].value)
        elif type(self.prefs[pref].value) == bool:
            value = ""true"" if self.prefs[pref].value == True else ""false""
        else:
            value = '""%s""' % self.prefs[pref].value
        ret += pref + "": "" + value + "" ("" + self.prefs[pref].anon_source + "")\n""
    return ret
",elif type ( self . prefs [ pref ] . value ) == bool :,150
"def dumps(o, **kwargs):
    """"""Dumps JSON object.""""""
    try:
        return _engine[1](o)
    except:
        ExceptionClass, why = sys.exc_info()[:2]
        if any([(issubclass(ExceptionClass, e)) for e in _engine[2]]):
            raise JSONError(why)
        else:
            raise why
","if any ( [ ( issubclass ( ExceptionClass , e ) ) for e in _engine [ 2 ] ] ) :",95
"def main():
    import sys, getopt
    try:
        opts, args = getopt.getopt(sys.argv[1:], ""ho:"", [""help"", ""output=""])
    except getopt.GetoptError as err:
        usage()
        sys.exit(1)
    output = None
    for o, a in opts:
        if o in (""-h"", ""--help""):
            usage()
            sys.exit()
        elif o in (""-o"", ""--output""):
            output = a
        else:
            usage()
            sys.exit(1)
    if not args:
        usage()
        sys.exit(1)
    concat_flv(args, output)
","if o in ( ""-h"" , ""--help"" ) :",175
"def close_group(self):
    """"""Closes a grouping for previous filters""""""
    if self._filters:
        if len(self._open_group_flag) < (len(self._close_group_flag) + 1):
            raise RuntimeError(""Not enough open groups to close."")
        if isinstance(self._filters[-1], ChainOperator):
            flt_sentence = self._filters[-2]
        else:
            flt_sentence = self._filters[-1]
        flt_sentence[1] = flt_sentence[1] + "")""  # closing the group
        self._close_group_flag.append(False)  # flag a close group was added
    else:
        raise RuntimeError(""No filters present. Can't close a group"")
    return self
","if isinstance ( self . _filters [ - 1 ] , ChainOperator ) :",191
"def _GetPlugins(self, base_class):
    items = []
    for name in sorted(base_class.classes.keys()):
        cls = base_class.classes[name]
        # While technically a valid plugin, UnknownOutputPlugin is only used as
        # a placeholder when unserializing old and now-deleted output plugins.
        # No need to display it in the UI.
        if cls == output_plugin.UnknownOutputPlugin:
            continue
        if cls.description:
            items.append(ApiOutputPluginDescriptor().InitFromOutputPluginClass(cls))
    return items
",if cls . description :,145
"def _set_helper(settings, path, value, data_type=None):
    path = _to_settings_path(path)
    method = settings.set
    if data_type is not None:
        name = None
        if data_type == bool:
            name = ""setBoolean""
        elif data_type == float:
            name = ""setFloat""
        elif data_type == int:
            name = ""setInt""
        if name is not None:
            method = getattr(settings, name)
    method(path, value)
    settings.save()
",if name is not None :,150
"def _url_encode_impl(obj, charset, encode_keys, sort, key):
    iterable = sdict()
    for key, values in obj.items():
        if not isinstance(values, list):
            values = [values]
        iterable[key] = values
    if sort:
        iterable = sorted(iterable, key=key)
    for key, values in iterable.items():
        for value in values:
            if value is None:
                continue
            if not isinstance(key, bytes):
                key = str(key).encode(charset)
            if not isinstance(value, bytes):
                value = str(value).encode(charset)
            yield url_quote_plus(key) + ""="" + url_quote_plus(value)
","if not isinstance ( key , bytes ) :",198
"def validate_data(self, data, schema):
    verrors = ValidationErrors()
    provider = data[""provider""]
    if provider == ""custom"":
        for k in (""custom_ddns_server"", ""custom_ddns_path""):
            if not data[k]:
                verrors.add(f""{schema}.{k}"", ""Required when using a custom provider."")
    elif provider not in (await self.provider_choices()):
        verrors.add(f""{schema}.provider"", ""Please select a valid provider."")
    verrors.check()
",if not data [ k ] :,138
"def render(self):
    x = ""<span>""
    for idx, arg in enumerate(self.args, start=1):
        if isinstance(arg, (tuple, list)):
            value, desc = arg
        else:
            value, desc = arg, arg
        attrs = self.attrs.copy()
        attrs[""name""] = self.name
        attrs[""type""] = ""radio""
        attrs[""value""] = value
        attrs[""id""] = self.name + str(idx)
        if self.value == value:
            attrs[""checked""] = ""checked""
        x += ""<input %s/> %s"" % (attrs, net.websafe(desc))
    x += ""</span>""
    return x
","if isinstance ( arg , ( tuple , list ) ) :",183
"def search_rotate(array, val):
    low, high = 0, len(array) - 1
    while low <= high:
        mid = (low + high) // 2
        if val == array[mid]:
            return mid
        if array[low] <= array[mid]:
            if array[low] <= val <= array[mid]:
                high = mid - 1
            else:
                low = mid + 1
        else:
            if array[mid] <= val <= array[high]:
                low = mid + 1
            else:
                high = mid - 1
    return -1
",if val == array [ mid ] :,166
"def detect(get_page):
    retval = False
    for vector in WAF_ATTACK_VECTORS:
        page, headers, code = get_page(get=vector)
        retval = (
            re.search(
                r""wangzhan\.360\.cn"", headers.get(""X-Powered-By-360wzb"", """"), re.I
            )
            is not None
        )
        if retval:
            break
    return retval
",if retval :,125
"def _recalculate(self):
    # If the parent's path has changed, recalculate _path
    parent_path = tuple(self._get_parent_path())  # Make a copy
    if parent_path != self._last_parent_path:
        spec = self._path_finder(self._name, parent_path)
        # Note that no changes are made if a loader is returned, but we
        #  do remember the new parent path
        if spec is not None and spec.loader is None:
            if spec.submodule_search_locations:
                self._path = spec.submodule_search_locations
        self._last_parent_path = parent_path  # Save the copy
    return self._path
",if spec is not None and spec . loader is None :,174
"def _get_directory_item_content(filename, return_binary, encoding):
    content = None
    if os.path.exists(filename):
        if return_binary:
            mode = ""rb""
            encoding = None
        else:
            mode = ""r""
        with codecs.open(filename, mode, encoding=encoding) as file_obj:
            content = file_obj.read()
    return content
",if return_binary :,110
"def randint(self, beg, end):
    if beg == 1 and end == 10:
        self.icnt1_10 += 1
        if self.icnt1_10 > len(self.RINT1_10):
            self.icnt1_10 = 1
        return self.RINT1_10[self.icnt1_10 - 1]
    if beg == 65 and end == 90:
        self.icnt65_90 += 1
        if self.icnt65_90 > len(self.RINT65_90):
            self.icnt65_90 = 1
        return self.RINT65_90[self.icnt65_90 - 1]
    raise Exception(""Not implemented"")
",if self . icnt1_10 > len ( self . RINT1_10 ) :,178
"def _get_two_devices(self, require_same_type=False):
    tpus = extensions.tpu_devices()
    if FLAGS.requires_tpu:
        if len(tpus) == 2:
            res = tpus
        else:
            raise ValueError(
                ""This test requires 2 TPU cores but %s are found"" % len(tpus)
            )
    else:
        if len(tpus) == 2:
            res = tpus
        elif self._hasGPU() and not require_same_type:
            res = (""CPU:0"", ""GPU:0"")
        else:
            res = (""CPU:0"", ""CPU:1"")
    return res
",if len ( tpus ) == 2 :,184
"def edge2str(self, nfrom, nto):
    if isinstance(nfrom, ExprCompose):
        for i in nfrom.args:
            if i[0] == nto:
                return ""[%s, %s]"" % (i[1], i[2])
    elif isinstance(nfrom, ExprCond):
        if nfrom.cond == nto:
            return ""?""
        elif nfrom.src1 == nto:
            return ""True""
        elif nfrom.src2 == nto:
            return ""False""
    return """"
",elif nfrom . src1 == nto :,149
"def send_frame_imm(self, frame):
    # send s_frame
    if frame.name == ""s_frame"":
        frame.RecvSeq = self.rsn
        if self.t2_caller:
            gevent.kill(self.t2_caller)
        self.telegram_count = 0
        response_string = "" "".join(hex(n) for n in frame.build())
        logger.info(
            ""%s <--- s_frame %s  (%s)"", self.address, response_string, self.session_id
        )
        return self.sock.send(frame.build())
",if self . t2_caller :,157
"def lin2lin(cp, size, size2):
    _check_params(len(cp), size)
    _check_size(size2)
    if size == size2:
        return cp
    new_len = (len(cp) / size) * size2
    result = create_string_buffer(new_len)
    for i in range(_sample_count(cp, size)):
        sample = _get_sample(cp, size, i)
        if size < size2:
            sample = sample << (4 * size2 / size)
        elif size > size2:
            sample = sample >> (4 * size / size2)
        sample = _overflow(sample, size2)
        _put_sample(result, size2, i, sample)
    return result.raw
",elif size > size2 :,197
"def tangent(self, t):
    result = np.array([0, 0, 0])
    o = self.omega
    for i, coeff in enumerate(self.coeffs):
        j = i // 2
        if i % 2 == 0:
            result += -(j + 1) * o * coeff * sin((j + 1) * o * t)
        else:
            result += (j + 1) * o * coeff * cos((j + 1) * o * t)
    return result
",if i % 2 == 0 :,126
"def _run(self):
    when_pressed = 0.0
    pressed = False
    while not self._done.is_set():
        now = time.monotonic()
        if now - when_pressed > self._debounce_time:
            if GPIO.input(self._channel) == self._expected:
                if not pressed:
                    pressed = True
                    when_pressed = now
                    self._trigger(self._pressed_queue, self._pressed_callback)
            else:
                if pressed:
                    pressed = False
                    self._trigger(self._released_queue, self._released_callback)
        self._done.wait(0.05)
",if now - when_pressed > self . _debounce_time :,187
"def check_dimensions(nrow, ncol):
    if nrow is not None:
        if nrow < 1:
            warn(
                ""'nrow' must be greater than 0. "" ""Your value has been ignored."",
                PlotnineWarning,
            )
            nrow = None
        else:
            nrow = int(nrow)
    if ncol is not None:
        if ncol < 1:
            warn(
                ""'ncol' must be greater than 0. "" ""Your value has been ignored."",
                PlotnineWarning,
            )
            ncol = None
        else:
            ncol = int(ncol)
    return nrow, ncol
",if ncol < 1 :,189
"def visit_FunctionDef(self, node: ast.FunctionDef) -> None:
    """"""Handles FunctionDef node and set context.""""""
    if self.current_function is None:
        self.add_entry(
            node.name
        )  # should be called before setting self.current_function
        if self.is_final(node.decorator_list):
            self.add_final_entry(node.name)
        if self.is_overload(node.decorator_list):
            self.add_overload_entry(node)
        self.context.append(node.name)
        self.current_function = node
        for child in node.body:
            self.visit(child)
        self.context.pop()
        self.current_function = None
",if self . is_final ( node . decorator_list ) :,196
"def ret(stmt, params=()):
    match = limit_re.match(stmt)
    if match:
        if match.group(2) == ""?"":
            n = params[-1]
            params = params[:-1]
        else:
            n = int(match.group(2))
        store.sql(match.group(1), params)
        return [store.cursor.fetchone() for i in xrange(n)]
    return selectall(stmt, params)
","if match . group ( 2 ) == ""?"" :",119
"def OnBodyClick(self, event=None):
    try:
        c = self.c
        p = c.currentPosition()
        if not g.doHook(""bodyclick1"", c=c, p=p, v=p, event=event):
            self.OnActivateBody(event=event)
            c.k.showStateAndMode(w=c.frame.body.bodyCtrl)
        g.doHook(""bodyclick2"", c=c, p=p, v=p, event=event)
    except:
        g.es_event_exception(""bodyclick"")
","if not g . doHook ( ""bodyclick1"" , c = c , p = p , v = p , event = event ) :",148
"def verify_settings(rst_path: Path) -> Iterator[Error]:
    for setting_name, default in find_settings_in_rst(rst_path):
        actual = getattr(app.conf, setting_name)
        if isinstance(default, timedelta):
            default = default.total_seconds()
        if isinstance(actual, Enum):
            actual = actual.value
        if actual != default:
            yield Error(
                reason=""mismatch"",
                setting=setting_name,
                default=default,
                actual=actual,
            )
","if isinstance ( actual , Enum ) :",152
"def fromVariant(variant):
    if hasattr(QtCore, ""QVariant"") and isinstance(variant, QtCore.QVariant):
        t = variant.type()
        if t == QtCore.QVariant.String:
            return str(variant.toString())
        elif t == QtCore.QVariant.Double:
            return variant.toDouble()[0]
        elif t == QtCore.QVariant.Int:
            return variant.toInt()[0]
        elif t == QtCore.QVariant.Bool:
            return variant.toBool()
        elif t == QtCore.QVariant.Invalid:
            return None
        else:
            raise ValueError('Unsupported QVariant type ""%s""' % variant.typeName())
    else:
        return variant
",elif t == QtCore . QVariant . Int :,195
"def decode_list(self, prop, value):
    if not isinstance(value, list):
        value = [value]
    if hasattr(prop, ""item_type""):
        item_type = getattr(prop, ""item_type"")
        dec_val = {}
        for val in value:
            if val is not None:
                k, v = self.decode_map_element(item_type, val)
                try:
                    k = int(k)
                except:
                    k = v
                dec_val[k] = v
        value = dec_val.values()
    return value
",if val is not None :,170
"def has_valid_checksum(self, number):
    given_number, given_checksum = number[:-1], number[-1]
    calculated_checksum = 0
    parameter = 7
    for item in given_number:
        fragment = str(int(item) * parameter)
        if fragment.isalnum():
            calculated_checksum += int(fragment[-1])
        if parameter == 1:
            parameter = 7
        elif parameter == 3:
            parameter = 1
        elif parameter == 7:
            parameter = 3
    return str(calculated_checksum)[-1] == given_checksum
",elif parameter == 3 :,147
"def encoder(s, *args, **kwargs):
    r = []
    _in = []
    for c in s:
        if ord(c) in PRINTABLE:
            doB64(_in, r)
            r.append(c.encode())
        elif c == ""&"":
            doB64(_in, r)
            r.append(b""&-"")
        else:
            _in.append(c)
    doB64(_in, r)
    return (b"""".join(r), len(s))
","elif c == ""&"" :",137
"def construct_instances(self, row, keys=None):
    collected_models = {}
    for i, (key, constructor, attr, conv) in enumerate(self.column_map):
        if keys is not None and key not in keys:
            continue
        value = row[i]
        if key not in collected_models:
            collected_models[key] = constructor()
        instance = collected_models[key]
        if attr is None:
            attr = self.cursor.description[i][0]
        if conv is not None:
            value = conv(value)
        setattr(instance, attr, value)
    return collected_models
",if conv is not None :,167
"def try_to_find_osquery(self):
    extention = """"
    if platform.system() == ""Windows"":
        extention = "".exe""
    try:
        return resources.get_resource(""osqueryi"" + extention)
    except IOError as e:
        # Maybe it is installed on the system.
        if platform.system() == ""Windows"":
            result = r""c:\ProgramData\osquery\osqueryi.exe""
            if os.access(result, os.R_OK):
                return result
        else:
            # Try to find it somewhere on the system.
            return spawn.find_executable(""osqueryi"")
        raise e
","if platform . system ( ) == ""Windows"" :",178
"def get_cached_stats(self, split=tfds.Split.TRAIN):
    """"""Returns basic statistics for cached dataset.""""""
    self.assert_cached()
    if split not in self._stats:
        stats_path = get_stats_path(self.cache_dir, split)
        if not tf.io.gfile.exists(stats_path):
            raise ValueError(
                ""Stats do not exist for '%s' split: %s"" % (self.name, split)
            )
        with tf.io.gfile.GFile(stats_path) as f:
            self._stats[split] = json.load(f)
    return self._stats[split]
",if not tf . io . gfile . exists ( stats_path ) :,169
"def _network_connections_in_results(data):
    for plugin_name, plugin_result in data.iteritems():
        if plugin_result[""status""] == ""error"":
            continue
        if ""device"" not in plugin_result:
            continue
        if ""connections"" in plugin_result[""device""]:
            for conn in plugin_result[""device""][""connections""]:
                if conn[""connection_type""] == ConnectionType.network.name:
                    return True
    return False
","if plugin_result [ ""status"" ] == ""error"" :",126
"def register_asyncio_task(self, task, module_path=None):
    if self._current[""metadata""] is None:
        if module_path is None:
            raise RuntimeError(""module_path must be supplied for late-binded tasks"")
        else:
            self.list[module_path][""asyncio.task""].append(task)
    else:
        self._current[""asyncio.task""].append(task)
",if module_path is None :,105
"def __prep_write_total(self, comments, main, fallback, single):
    lower = self.as_lowercased()
    for k in [main, fallback, single]:
        if k in comments:
            del comments[k]
    if single in lower:
        parts = lower[single].split(""/"", 1)
        if parts[0]:
            comments[single] = [parts[0]]
        if len(parts) > 1:
            comments[main] = [parts[1]]
    if main in lower:
        comments[main] = lower.list(main)
    if fallback in lower:
        if main in comments:
            comments[fallback] = lower.list(fallback)
        else:
            comments[main] = lower.list(fallback)
",if len ( parts ) > 1 :,196
"def api(request, app):
    marker = request.keywords.get(""api"")
    bpkwargs = {}
    kwargs = {}
    if marker:
        if ""prefix"" in marker.kwargs:
            bpkwargs[""url_prefix""] = marker.kwargs.pop(""prefix"")
        if ""subdomain"" in marker.kwargs:
            bpkwargs[""subdomain""] = marker.kwargs.pop(""subdomain"")
        kwargs = marker.kwargs
    blueprint = Blueprint(""api"", __name__, **bpkwargs)
    api = restplus.Api(blueprint, **kwargs)
    app.register_blueprint(blueprint)
    yield api
","if ""prefix"" in marker . kwargs :",157
"def _get_pip_index_urls(sources):
    index_urls = []
    trusted_hosts = []
    for source in sources:
        url = source.get(""url"")
        if not url:
            continue
        index_urls.append(url)
        if source.get(""verify_ssl"", True):
            continue
        host = six.moves.urllib.parse.urlparse(source[""url""]).hostname
        trusted_hosts.append(host)
    return index_urls, trusted_hosts
",if not url :,129
"def add_aggregation_data(self, payload):
    for timestamp, payload_data in payload.items():
        if ""interval_aggs"" in payload_data:
            self.unwrap_interval_buckets(
                timestamp, None, payload_data[""interval_aggs""][""buckets""]
            )
        elif ""bucket_aggs"" in payload_data:
            self.unwrap_term_buckets(timestamp, payload_data[""bucket_aggs""][""buckets""])
        else:
            self.check_matches(timestamp, None, payload_data)
","elif ""bucket_aggs"" in payload_data :",136
"def _handle_unverified_signed_presence(self, pres):
    verified = self.verify(pres[""status""], pres[""signed""])
    if verified.key_id:
        if not self.get_keyid(pres[""from""]):
            known_keyids = [e[""keyid""] for e in self.gpg.list_keys()]
            if verified.key_id not in known_keyids:
                self.gpg.recv_keys(self.key_server, verified.key_id)
            self.set_keyid(jid=pres[""from""], keyid=verified.key_id)
        self.xmpp.event(""signed_presence"", pres)
",if verified . key_id not in known_keyids :,170
"def __init__(self, *args, **kwargs):
    """"""Initialize the texture.""""""
    super().__init__(*args, **kwargs)
    assert_empty_kwargs(**kwargs)
    if len(args) == 1:
        if isinstance(args[0], vtk.vtkTexture):
            self._from_texture(args[0])
        elif isinstance(args[0], np.ndarray):
            self._from_array(args[0])
        elif isinstance(args[0], vtk.vtkImageData):
            self._from_image_data(args[0])
        elif isinstance(args[0], str):
            self._from_file(filename=args[0])
        else:
            raise TypeError(f""Table unable to be made from ({type(args[0])})"")
","elif isinstance ( args [ 0 ] , str ) :",200
"def get_manifest_data(manifestpath):
    """"""Reads a manifest file, returns a dictionary-like object.""""""
    plist = {}
    try:
        plist = FoundationPlist.readPlist(manifestpath)
    except FoundationPlist.NSPropertyListSerializationException:
        display.display_error(u""Could not read plist: %s"", manifestpath)
        if os.path.exists(manifestpath):
            try:
                os.unlink(manifestpath)
            except OSError as err:
                display.display_error(u""Failed to delete plist: %s"", err)
        else:
            display.display_error(""plist does not exist."")
    return plist
",if os . path . exists ( manifestpath ) :,172
"def _get_proxy(self):
    url_dissected = url_dissector.findall(self.session[""proxy""])
    if url_dissected and len(url_dissected[0]) == 3:
        protocol, host, port = url_dissected[0]
        if protocol == ""socks5"":
            return (socks.PROXY_TYPE_SOCKS5, host, int(port))
        if protocol == ""socks4"":
            return (socks.PROXY_TYPE_SOCKS4, host, int(port))
        if protocol.startswith(""http""):
            return (socks.PROXY_TYPE_HTTP, host, int(port))
    return None, None, None
","if protocol == ""socks5"" :",172
"def nud(self):
    self.first = []
    comma = False
    if self.token.id != "")"":
        while 1:
            if self.token.id == "")"":
                break
            self.first.append(self.expression())
            if self.token.id == "","":
                comma = True
                self.advance("","")
            else:
                break
    self.advance("")"")
    if not self.first or comma:
        return self  # tuple
    else:
        return self.first[0]
","if self . token . id == "","" :",146
"def _debug_log(self, text, level):
    if text and ""log"" in self.config.sys.debug:
        if not text.startswith(self.log_prefix):
            text = ""%slog(%s): %s"" % (self.log_prefix, level, text)
        if self.log_parent is not None:
            return self.log_parent.log(level, text)
        else:
            self.term.write(self._fmt_log(text, level=level))
",if self . log_parent is not None :,129
"def remove_checker(self, namespace, checker):
    for c in pyomo.core.check.ModelCheckRunner._checkers(all=True):
        if c._checkerName() == checker:
            if namespace.checkers.get(c._checkerPackage(), None) is not None:
                for i in range(
                    namespace.checkers[c._checkerPackage()].count(c._checkerName())
                ):
                    namespace.checkers[c._checkerPackage()].remove(c._checkerName())
","if namespace . checkers . get ( c . _checkerPackage ( ) , None ) is not None :",129
"def check_if_role_exists(self, role_name, parsed_globals):
    parameters = {""RoleName"": role_name}
    try:
        self._call_iam_operation(""GetRole"", parameters, parsed_globals)
    except botocore.exceptions.ClientError as e:
        role_not_found_code = ""NoSuchEntity""
        error_code = e.response.get(""Error"", {}).get(""Code"", """")
        if role_not_found_code == error_code:
            # No role error.
            return False
        else:
            # Some other error. raise.
            raise e
    return True
",if role_not_found_code == error_code :,158
"def GetClipboardText():
    text = """"
    if OpenClipboard(0):
        hClipMem = GetClipboardData(CF_TEXT)
        if hClipMem:
            GlobalLock.restype = c_char_p
            text = GlobalLock(hClipMem)
            GlobalUnlock(hClipMem)
        CloseClipboard()
    return ensure_unicode(text)
",if hClipMem :,100
"def test_log_action_class():
    v = Mock()
    for k, v in amo.LOG_BY_ID.items():
        if v.action_class is not None:
            cls = ""action-"" + v.action_class
        else:
            cls = """"
        assert render(""{{ log_action_class(id) }}"", {""id"": v.id}) == cls
",if v . action_class is not None :,100
"def _get_distinct_albumartists(config, session, web_client, query):
    logger.debug(f""Getting distinct albumartists: {query}"")
    if query:
        search_result = _get_search(config, session, web_client, query, album=True)
        return {
            artist.name
            for album in search_result.albums
            for artist in album.artists
            if album.artists
        }
    else:
        return {
            track.album.artist.name
            for track in _get_playlist_tracks(config, session)
            if track.album and track.album.artist
        }
",if track . album and track . album . artist,169
"def _get_commands():
    proc = Popen([""react-native"", ""--help""], stdout=PIPE)
    should_yield = False
    for line in proc.stdout.readlines():
        line = line.decode().strip()
        if not line:
            continue
        if ""Commands:"" in line:
            should_yield = True
            continue
        if should_yield:
            yield line.split("" "")[0]
","if ""Commands:"" in line :",111
"def __call__(self, job):
    import tensorboard_logger as tl
    # id = job.id
    budget = job.kwargs[""budget""]
    # config = job.kwargs['config']
    timestamps = job.timestamps
    result = job.result
    exception = job.exception
    time_step = int(timestamps[""finished""] - self.start_time)
    if result is not None:
        tl.log_value(""BOHB/all_results"", result[""loss""] * -1, time_step)
        if result[""loss""] < self.incumbent:
            self.incumbent = result[""loss""]
        tl.log_value(""BOHB/incumbent_results"", self.incumbent * -1, time_step)
","if result [ ""loss"" ] < self . incumbent :",193
"def _parse_yum_or_zypper_repositories(output):
    repos = []
    current_repo = {}
    for line in output:
        line = line.strip()
        if not line or line.startswith(""#""):
            continue
        if line.startswith(""[""):
            if current_repo:
                repos.append(current_repo)
                current_repo = {}
            current_repo[""name""] = line[1:-1]
        if current_repo and ""="" in line:
            key, value = line.split(""="", 1)
            current_repo[key] = value
    if current_repo:
        repos.append(current_repo)
    return repos
",if current_repo :,179
"def selector():
    while True:
        rlist, _, _ = select([proc.stdout, proc.stderr], [], [], line_timeout)
        if not rlist and line_timeout:
            raise ProcessLineTimedOut(
                ""popen line timeout expired"",
                getattr(proc, ""argv"", None),
                getattr(proc, ""machine"", None),
            )
        for stream in rlist:
            yield (stream is proc.stderr), decode(stream.readline(linesize))
",if not rlist and line_timeout :,127
"def getBranchFromFile():
    global _gitdir
    branch = None
    if _gitdir:
        headFile = os.path.join(_gitdir, ""HEAD"")
        if os.path.isfile(headFile):
            with open(headFile, ""r"", encoding=""utf-8"") as f:
                line = f.readline()
                if line:
                    if line.startswith(""ref""):
                        branch = line.split(""/"")[-1].strip()
                    else:
                        branch = ""HEAD""
    return branch
","if line . startswith ( ""ref"" ) :",151
"def handle(self, msg):
    self._mic.send(msg)
    for calculate_seed, make_delegate, dict in self._delegate_records:
        id = calculate_seed(msg)
        if id is None:
            continue
        elif isinstance(id, collections.Hashable):
            if id not in dict or not dict[id].is_alive():
                d = make_delegate((self, msg, id))
                d = self._ensure_startable(d)
                dict[id] = d
                dict[id].start()
        else:
            d = make_delegate((self, msg, id))
            d = self._ensure_startable(d)
            d.start()
","elif isinstance ( id , collections . Hashable ) :",192
"def _print_items(items, _filter=None):
    if _filter:
        print(""Displaying items matching filter: %s"" % _filter)
    print()
    for item in items:
        filtered_out = False
        for f in _filter.split():
            if f.lower() not in item.lower():
                filtered_out = True
        if not filtered_out:
            print(item)
    print()
",if f . lower ( ) not in item . lower ( ) :,113
"def _cbAllRecords(self, results):
    ans, auth, add = [], [], []
    for res in results:
        if res[0]:
            ans.extend(res[1][0])
            auth.extend(res[1][1])
            add.extend(res[1][2])
    return ans, auth, add
",if res [ 0 ] :,87
"def __status_update(self):
    was_active = False
    while True:
        if self.analytics_instance.active:
            was_active = True
            msg = ""Active (%s)"" % self.analytics_instance.progress
            self.broadcast(msg, ""analytics"", ""analyticsUpdate"")
        if was_active and not self.analytics_instance.active:
            self.broadcast(""Inactive"", ""analytics"", ""analyticsUpdate"")
            was_active = False
        time.sleep(0.2)
",if was_active and not self . analytics_instance . active :,133
"def plugin_song(self, song):
    for tag in [""album""]:
        values = filter(None, map(album_to_sort, song.list(tag)))
        if values and (tag + ""sort"") not in song:
            song[tag + ""sort""] = ""\n"".join(values)
    for tag in [""artist"", ""albumartist"", ""performer""]:
        values = filter(None, map(artist_to_sort, song.list(tag)))
        if values and (tag + ""sort"") not in song:
            song[tag + ""sort""] = ""\n"".join(values)
","if values and ( tag + ""sort"" ) not in song :",149
"def update(h, s):
    with lock:
        try:
            i, c = find_cell(h)
        except KeyError:
            return
        if not c.frozen and c.content != s:
            c.content = parse(s)
            render_from(i, clear_after=True)
",if not c . frozen and c . content != s :,86
"def get_parameters(self, names, with_decryption):
    result = []
    if len(names) > 10:
        raise ValidationException(
            ""1 validation error detected: ""
            ""Value '[{}]' at 'names' failed to satisfy constraint: ""
            ""Member must have length less than or equal to 10."".format("", "".join(names))
        )
    for name in names:
        if name in self._parameters:
            result.append(self.get_parameter(name, with_decryption))
    return result
",if name in self . _parameters :,134
"def entered_file_action(self, path):
    attempt_copy = True
    path = self.try_append_extension(path)
    directory = os.path.dirname(path)
    if not os.path.exists(directory):
        try:
            self.create_folder(directory)
        except OSError as e:
            attempt_copy = False
            sublime.error_message(
                ""Cannot create '"" + path + ""'."" + "" See console for details""
            )
            print(""Exception: %s '%s'"" % (e.strerror, e.filename))
    if attempt_copy:
        copy_success, new_file = self._copy_file(path)
        if copy_success:
            self.open_file(new_file)
",if copy_success :,200
"def acquire(self):
    ""Acquire semaphore by decrementing value using spin-lock algorithm.""
    while True:
        with self._cache.transact(retry=True):
            value = self._cache.get(self._key, default=self._value)
            if value > 0:
                self._cache.set(
                    self._key,
                    value - 1,
                    expire=self._expire,
                    tag=self._tag,
                )
                return
        time.sleep(0.001)
",if value > 0 :,153
"def commit(self):
    doc = {}
    for field, default in self.fields.iteritems():
        if hasattr(self, field):
            value = getattr(self, field)
            if field in self.commit_fields or value != default:
                doc[field] = getattr(self, field)
    with open(self.path, ""w"") as settings_file:
        settings_file.write(json.dumps(doc, indent=4))
","if hasattr ( self , field ) :",115
"def parse_entrypoints(self, content: str, root=None) -> RootDependency:
    if root is None:
        root = RootDependency()
    entrypoints = []
    group = ""console_scripts""
    for line in content.split(""\n""):
        line = line.strip()
        if not line or line[0] in ""#;"":  # ignore comments
            continue
        if line[0] == ""["" and line[-1] == ""]"":
            group = line[1:-1]
        else:
            entrypoints.append(EntryPoint.parse(text=line, group=group))
    root.entrypoints = tuple(entrypoints)
    return root
","if not line or line [ 0 ] in ""#;"" :",162
"def request_with_retries(endpoint, timeout=30):
    start = time.time()
    while True:
        try:
            return requests.get(""http://127.0.0.1:8000"" + endpoint, timeout=timeout)
        except requests.RequestException:
            if time.time() - start > timeout:
                raise TimeoutError
            time.sleep(0.1)
",if time . time ( ) - start > timeout :,101
"def get_expression(self):
    """"""Return the expression as a printable string.""""""
    l = []
    for c in self.content:
        if c.op is not None:  # only applies to first cell
            l.append(c.op)
        if c.child is not None:
            l.append(""("" + c.child.get_expression() + "")"")
        else:
            l.append(""%d"" % c.get_value())
    return """".join(l)
",if c . op is not None :,124
"def nrgen_asc(self):
    # compute the number of generations present
    for generation in range(self.generations_asc - 1, 0, -1):
        for p in range(len(self.data[generation])):
            (person, parents, child, userdata) = self.data[generation][p]
            if person:
                return generation
    return 1
",if person :,96
"def check_all_verified(self):
    if not self.all_verified:
        new_all_verified = not self.lines.filter(verified=False).exists()
        if new_all_verified:
            self.all_verified = True
            if self.require_verification:
                self.add_log_entry(
                    _(""All rows requiring verification have been verified."")
                )
                self.require_verification = False
            self.save()
    return self.all_verified
",if self . require_verification :,136
"def sort(self, cmp=None, key=None, reverse=False):
    ""Standard list sort method""
    if key:
        temp = [(key(v), v) for v in self]
        temp.sort(key=lambda x: x[0], reverse=reverse)
        self[:] = [v[1] for v in temp]
    else:
        temp = list(self)
        if cmp is not None:
            temp.sort(cmp=cmp, reverse=reverse)
        else:
            temp.sort(reverse=reverse)
        self[:] = temp
",if cmp is not None :,146
"def process_formdata(self, valuelist):
    if valuelist:
        date_str = "" "".join(valuelist)
        if not date_str:
            self.data = None
            raise ValidationError(self.gettext(""Please input a date/time value""))
        parse_kwargs = self.parse_kwargs.copy()
        if ""default"" not in parse_kwargs:
            try:
                parse_kwargs[""default""] = self.default()
            except TypeError:
                parse_kwargs[""default""] = self.default
        try:
            self.data = parser.parse(date_str, **parse_kwargs)
        except ValueError:
            self.data = None
            raise ValidationError(self.gettext(""Invalid date/time input""))
","if ""default"" not in parse_kwargs :",196
"def _expand_dim_shape_func(data_shape, ndim, axis, num_newaxis):
    out = output_tensor((ndim + num_newaxis,), ""int64"")
    for i in const_range(out.shape[0]):
        if i < axis:
            out[i] = data_shape[i]
        elif i < axis + num_newaxis:
            out[i] = int64(1)
        else:
            out[i] = data_shape[i - num_newaxis]
    return out
",elif i < axis + num_newaxis :,133
"def _Return(self, t):
    self._fill(""return "")
    if t.value:
        if isinstance(t.value, Tuple):
            text = "", "".join([name.name for name in t.value.asList()])
            self._write(text)
        else:
            self._dispatch(t.value)
        if not self._do_indent:
            self._write(""; "")
",if not self . _do_indent :,106
"def blas_header_version():
    # Version for the base header
    version = (9,)
    if detect_macos_sdot_bug():
        if detect_macos_sdot_bug.fix_works:
            # Version with fix
            version += (1,)
        else:
            # Version with error
            version += (2,)
    return version
",if detect_macos_sdot_bug . fix_works :,97
"def get_queues(self, region: str, attribute_names: []):
    sqs_client = AWSFacadeUtils.get_client(""sqs"", self.session, region)
    try:
        raw_queues = await run_concurrently(sqs_client.list_queues)
    except Exception as e:
        print_exception(f""Failed to list SQS queues: {e}"")
        return []
    else:
        if ""QueueUrls"" not in raw_queues:
            return []
        queue_urls = raw_queues[""QueueUrls""]
        return await map_concurrently(
            self._get_queue_attributes,
            queue_urls,
            region=region,
            attribute_names=attribute_names,
        )
","if ""QueueUrls"" not in raw_queues :",189
"def popupFrameXdiff(job, frame1, frame2, frame3=None):
    """"""Opens a frame xdiff.""""""
    for command in [""/usr/bin/xxdiff"", ""/usr/local/bin/xdiff""]:
        if os.path.isfile(command):
            for frame in [frame1, frame2, frame3]:
                if frame:
                    command += "" --title1 %s %s"" % (
                        frame.data.name,
                        getFrameLogFile(job, frame),
                    )
            shellOut(command)
",if frame :,154
"def wrap(*args, **kwargs):
    callargs = getcallargs(fun, *args, **kwargs)
    if callargs[""sock""] is None:
        # This variable is used only to debug leak in tests
        COUNT[""count""] += 1
        with IPSet() as sock:
            callargs[""sock""] = sock
            # We must pop kwargs here, else the function will receive
            # a dict of dict
            if ""kwargs"" in callargs:
                callargs.update(callargs.pop(""kwargs""))
            return fun(**callargs)  # pylint:disable=star-args
    return fun(*args, **kwargs)
","if ""kwargs"" in callargs :",164
"def set_multi(self, value):
    del self[atype]
    for addr in value:
        # Support assigning dictionary versions of addresses
        # instead of full Address objects.
        if not isinstance(addr, Address):
            if atype != ""all"":
                addr[""type""] = atype
            elif ""atype"" in addr and ""type"" not in addr:
                addr[""type""] = addr[""atype""]
            addrObj = Address()
            addrObj.values = addr
            addr = addrObj
        self.append(addr)
","if atype != ""all"" :",146
"def test_connection(self, data=None, raise_alert=False):
    try:
        result = self._test_connection(self.connection_config(data))
    except CallError as e:
        result = {""error"": True, ""exception"": str(e)}
    if result[""error""]:
        if raise_alert:
            config = self.middleware.call_sync(""kmip.config"")
            self.middleware.call_sync(
                ""alert.oneshot_create"",
                ""KMIPConnectionFailed"",
                {""server"": config[""server""], ""error"": result[""exception""]},
            )
        return False
    else:
        return True
",if raise_alert :,174
"def test05_geometries(self):
    ""Testing Geometries from Data Source Features.""
    for source in ds_list:
        ds = DataSource(source.ds)
        # Incrementing through each layer and feature.
        for layer in ds:
            for feat in layer:
                g = feat.geom
                # Making sure we get the right Geometry name & type
                self.assertEqual(source.geom, g.geom_name)
                self.assertEqual(source.gtype, g.geom_type)
                # Making sure the SpatialReference is as expected.
                if hasattr(source, ""srs_wkt""):
                    self.assertEqual(source.srs_wkt, g.srs.wkt)
","if hasattr ( source , ""srs_wkt"" ) :",197
"def __walk_dir_tree(self, dirname):
    dir_list = []
    self.__logger.debug(""__walk_dir_tree. START dir=%s"", dirname)
    for f in os.listdir(dirname):
        current = os.path.join(dirname, f)
        if os.path.isfile(current) and f.endswith(""py""):
            if self.module_registrant:
                self._load_py_from_file(current)
            dir_list.append(current)
        elif os.path.isdir(current):
            ret = self.__walk_dir_tree(current)
            if ret:
                dir_list.append((f, ret))
    return dir_list
",if self . module_registrant :,184
"def setData(self, data=None):
    # update the data for the grid
    for nRow in range(self.nRows):
        for nCol in range(self.nCols):
            if data is not None and nRow < data.shape[0] and nCol < data.shape[1]:
                self.SetCellValue(nRow, nCol, ""%f"" % data[nRow, nCol])
            else:
                self.SetCellValue(nRow, nCol, ""0.000"")
    self.AutoSize()
",if data is not None and nRow < data . shape [ 0 ] and nCol < data . shape [ 1 ] :,141
"def __init__(self, *args, **kwargs):
    """"""Initialize the texture.""""""
    super().__init__(*args, **kwargs)
    assert_empty_kwargs(**kwargs)
    if len(args) == 1:
        if isinstance(args[0], vtk.vtkTexture):
            self._from_texture(args[0])
        elif isinstance(args[0], np.ndarray):
            self._from_array(args[0])
        elif isinstance(args[0], vtk.vtkImageData):
            self._from_image_data(args[0])
        elif isinstance(args[0], str):
            self._from_file(filename=args[0])
        else:
            raise TypeError(f""Table unable to be made from ({type(args[0])})"")
","elif isinstance ( args [ 0 ] , vtk . vtkImageData ) :",200
"def delete_old_post_save(
    sender, instance, raw, created, update_fields, using, **kwargs
):
    """"""Post_save on all models with file fields, deletes old files""""""
    if raw or created:
        return
    for field_name, new_file in cache.fields_for_model_instance(instance):
        if update_fields is None or field_name in update_fields:
            old_file = cache.get_field_attr(instance, field_name)
            if old_file != new_file:
                delete_file(instance, field_name, old_file, using)
    # reset cache
    cache.make_cleanup_cache(instance)
",if update_fields is None or field_name in update_fields :,171
"def do_refresh(self):
    try:
        if self.isVisible():
            service_status = agent_status()
            self.properties.service_status_label.setText(
                HUMAN_SERVICE_STATUS[service_status]
            )
    finally:
        QTimer.singleShot(REFRESH_PERIOD, self.do_refresh)
",if self . isVisible ( ) :,92
"def json_dumps(data):
    """"""Return data in nicely formatted json.""""""
    try:
        return json.dumps(
            data,
            indent=1,
            sort_keys=True,
            separators=("","", "": ""),
            default=json_serialize_default,
        )
    except UnicodeDecodeError:
        if sys.version_info[:2] == (2, 7):
            data = json_preserialize_binary(data)
            return json.dumps(data)
        raise
","if sys . version_info [ : 2 ] == ( 2 , 7 ) :",132
"def __init__(self, aList):
    for element in aList:
        if len(element) > 0:
            if element.tag == element[0].tag:
                self.append(ListParser(element))
            else:
                self.append(DictParser(element))
        elif element.text:
            text = element.text.strip()
            if text:
                self.append(text)
",if text :,116
"def __init__(self, token):
    self._convert_to_ascii = False
    self._find = None
    if token.search is None:
        return
    flags = 0
    self._match_this_many = 1
    if token.options:
        if ""g"" in token.options:
            self._match_this_many = 0
        if ""i"" in token.options:
            flags |= re.IGNORECASE
        if ""a"" in token.options:
            self._convert_to_ascii = True
    self._find = re.compile(token.search, flags | re.DOTALL)
    self._replace = _CleverReplace(token.replace)
","if ""a"" in token . options :",170
"def get_next(self):
    if self.current > self.maximum:
        raise StopIteration
    else:
        if self.width:
            payl = ""%0"" + str(self.width) + ""d""
            payl = payl % (self.current)
        else:
            payl = str(self.current)
        self.current += 1
        return payl
",if self . width :,104
"def any(self, provider_name):
    result = authomatic.login(Webapp2Adapter(self), provider_name)
    if result:
        apis = []
        if result.user:
            result.user.update()
            if result.user.credentials:
                apis = config.config.get(provider_name, {}).get(""_apis"", {})
        nice_provider_name = (
            config.config.get(provider_name, {}).get(""_name"")
            or provider_name.capitalize()
        )
        render(
            self,
            result,
            result.popup_js(custom=dict(apis=apis, provider_name=nice_provider_name)),
        )
",if result . user :,186
"def _get_lun_id(self, volume, target_name):
    """"""Get lun id of the voluem in a target.""""""
    pool = volume_utils.extract_host(volume.host, level=""pool"")
    volume_name = self._trans_name_down(volume.name)
    lun_id = None
    luns = self._get_lun_list(target_name)
    for lun in luns:
        mappinglvm = lun.get(""mappingLvm"")
        lun_name = mappinglvm.replace(r""%s/"" % pool, """")
        if lun_name == volume_name:
            lun_id = lun.get(""id"")
    return lun_id
",if lun_name == volume_name :,183
"def save_settings(self, settings):
    for setting in self.settings:
        setting_obj = settings[setting]
        new_value = self.cleaned_data.get(setting)
        if setting_obj.python_type == ""image"":
            if new_value and new_value != self.initial.get(setting):
                self.save_image(setting_obj, new_value)
            elif self.cleaned_data.get(""%s_delete"" % setting):
                self.delete_image(setting_obj)
        else:
            self.save_setting(setting_obj, new_value)
","if setting_obj . python_type == ""image"" :",160
"def setup_with_driver(self):
    if not self.__class__.shared_state_initialized:
        try:
            self.setup_shared_state()
            self.logout_if_needed()
        except Exception:
            self.__class__.shared_state_in_error = True
            raise
        finally:
            self.__class__.shared_state_initialized = True
    else:
        if self.__class__.shared_state_in_error:
            raise unittest.SkipTest(
                ""Skipping test, failed to initialize state previously.""
            )
",if self . __class__ . shared_state_in_error :,150
"def _get_replication_type_param(k, v):
    words = v.split()
    if len(words) == 2 and words[0] == ""<in>"":
        REPLICA_SYNC_TYPES = {
            ""sync"": constants.REPLICA_SYNC_MODEL,
            ""async"": constants.REPLICA_ASYNC_MODEL,
        }
        sync_type = words[1].lower()
        if sync_type in REPLICA_SYNC_TYPES:
            return REPLICA_SYNC_TYPES[sync_type]
    msg = _(
        ""replication_type spec must be specified as ""
        ""replication_type='<in> sync' or '<in> async'.""
    )
    LOG.error(msg)
    raise exception.InvalidInput(reason=msg)
",if sync_type in REPLICA_SYNC_TYPES :,200
"def request(self, host, handler, request_body, verbose=False):
    # retry request once if cached connection has gone cold
    for i in (0, 1):
        try:
            return self.single_request(host, handler, request_body, verbose)
        except socket.error as e:
            if i or e.errno not in (errno.ECONNRESET, errno.ECONNABORTED, errno.EPIPE):
                raise
        except http_client.BadStatusLine:  # close after we sent request
            if i:
                raise
",if i :,147
"def make_sales_return_records():
    if random.random() < 0.1:
        for data in frappe.get_all(
            ""Delivery Note"", fields=[""name""], filters={""docstatus"": 1}
        ):
            if random.random() < 0.1:
                try:
                    dn = make_sales_return(data.name)
                    dn.insert()
                    dn.submit()
                    frappe.db.commit()
                except Exception:
                    frappe.db.rollback()
",if random . random ( ) < 0.1 :,154
"def getStatusString(self):
    if not self._isAvailable:
        return ""Doodle3D box not found""
    if self._printing:
        if self._blockIndex < len(self._fileBlocks):
            ret = ""Sending GCode: %.1f%%"" % (
                float(self._blockIndex) * 100.0 / float(len(self._fileBlocks))
            )
        elif len(self._fileBlocks) > 0:
            ret = ""Finished sending GCode to Doodle3D box.""
        else:
            ret = ""Different print still running...""
        # ret += ""\nErrorCount: %d"" % (self._errorCount)
        return ret
    return ""Printer found, waiting for print command.""
",if self . _blockIndex < len ( self . _fileBlocks ) :,190
"def coro(*args, **kw):
    res = func(*args, **kw)
    if isinstance(res, futures.Future) or inspect.isgenerator(res):
        res = yield from res
    elif _AwaitableABC is not None:
        # If 'func' returns an Awaitable (new in 3.5) we
        # want to run it.
        try:
            await_meth = res.__await__
        except AttributeError:
            pass
        else:
            if isinstance(res, _AwaitableABC):
                res = yield from await_meth()
    return res
","if isinstance ( res , _AwaitableABC ) :",148
"def _skip_to_next_iteration_group(self):
    while True:
        if self._currkey is self._marker:
            pass
        elif self._tgtkey is self._marker:
            break
        else:
            if not self._tgtkey == self._currkey:
                break
        newvalue = next(self._iterator)
        if self._keyfunc is None:
            newkey = newvalue
        else:
            newkey = self._keyfunc(newvalue)
        self._currkey = newkey
        self._currvalue = newvalue
",if not self . _tgtkey == self . _currkey :,153
"def in_quadview(context):
    for area in context.window.screen.areas:
        if area.type != ""VIEW_3D"":
            continue
        for space in area.spaces:
            if space.type != ""VIEW_3D"":
                continue
            if len(space.region_quadviews) > 0:
                return True
    return False
","if area . type != ""VIEW_3D"" :",100
"def find_from_pythonpath(name):
    for dirpath in sys.path:
        if not os.path.isdir(dirpath):
            continue
        path = os.path.join(dirpath, name)
        if os.path.isfile(path):
            return path
    return None
",if not os . path . isdir ( dirpath ) :,75
"def detailed_exceptions_wrapper(self, *args, **kwargs):
    try:
        return meth(self, *args, **kwargs)
    except ScriptError as e:
        info = e.args[0]
        if not isinstance(info, dict):
            raise
        info.setdefault(""type"", ScriptError.SPLASH_LUA_ERROR)
        info.setdefault(""splash_method"", _name)
        raise e
","if not isinstance ( info , dict ) :",109
"def metadata(draft):
    test_metadata = {}
    json_schema = create_jsonschema_from_metaschema(draft.registration_schema.schema)
    for key, value in json_schema[""properties""].items():
        response = ""Test response""
        items = value[""properties""][""value""].get(""items"")
        enum = value[""properties""][""value""].get(""enum"")
        if items:  # multiselect
            response = [items[""enum""][0]]
        elif enum:  # singleselect
            response = enum[0]
        elif value[""properties""][""value""].get(""properties""):
            response = {""question"": {""value"": ""Test Response""}}
        test_metadata[key] = {""value"": response}
    return test_metadata
","elif value [ ""properties"" ] [ ""value"" ] . get ( ""properties"" ) :",185
"def separate_keys(self, keys, torrent_ids):
    """"""Separates the input keys into torrent class keys and plugins keys""""""
    if self.torrents:
        for torrent_id in torrent_ids:
            if torrent_id in self.torrents:
                status_keys = list(self.torrents[torrent_id].status_funcs)
                leftover_keys = list(set(keys) - set(status_keys))
                torrent_keys = list(set(keys) - set(leftover_keys))
                return torrent_keys, leftover_keys
    return [], []
",if torrent_id in self . torrents :,164
"def upgrade():
    bind = op.get_bind()
    op.add_column(""slices"", sa.Column(""datasource_id"", sa.Integer()))
    session = db.Session(bind=bind)
    for slc in session.query(Slice).all():
        if slc.druid_datasource_id:
            slc.datasource_id = slc.druid_datasource_id
        if slc.table_id:
            slc.datasource_id = slc.table_id
        session.merge(slc)
        session.commit()
    session.close()
",if slc . table_id :,139
"def __call__(self, controller, environ, context):
    context.session = session = SessionObject(environ, **self.options)
    environ[""beaker.session""] = session
    environ[""beaker.get_session""] = self._get_session
    if ""paste.testing_variables"" in environ:
        environ[""paste.testing_variables""][""session""] = session
    response = self.next_handler(controller, environ, context)
    if session.accessed():
        session.persist()
        session_headers = session.__dict__[""_headers""]
        if session_headers[""set_cookie""]:
            cookie = session_headers[""cookie_out""]
            if cookie:
                response.headers.extend(((""Set-cookie"", cookie),))
    return response
",if cookie :,187
"def propagate(self, user, change_action=None, author=None):
    """"""Propagate current translation to all others.""""""
    result = False
    for unit in self.same_source_units:
        if not user.has_perm(""unit.edit"", unit):
            continue
        if unit.target == self.target and unit.state == self.state:
            continue
        unit.target = self.target
        unit.state = self.state
        unit.save_backend(
            user,
            False,
            change_action=change_action,
            author=None,
            run_checks=False,
        )
        result = True
    return result
","if not user . has_perm ( ""unit.edit"" , unit ) :",179
"def load_model(self, model_dict):
    model_param = None
    model_meta = None
    for _, value in model_dict[""model""].items():
        for model in value:
            if model.endswith(""Meta""):
                model_meta = value[model]
            if model.endswith(""Param""):
                model_param = value[model]
    LOGGER.info(""load model"")
    self.set_model_meta(model_meta)
    self.set_model_param(model_param)
    self.phi = np.array([model_param.phi_a])
","if model . endswith ( ""Meta"" ) :",152
"def name(self):
    """"""Get the enumeration name of this storage class.""""""
    if self._name_map is None:
        self._name_map = {}
        for key, value in StorageClass.__dict__.items():
            if isinstance(value, StorageClass):
                self._name_map[value] = key
    return self._name_map[self]
","if isinstance ( value , StorageClass ) :",93
"def relro(self):
    try:
        gnu_relro = lief.ELF.SEGMENT_TYPES.GNU_RELRO
        flags = lief.ELF.DYNAMIC_TAGS.FLAGS
        bind_now = lief.ELF.DYNAMIC_FLAGS.BIND_NOW
        if self.elf.get(gnu_relro):
            if bind_now in self.elf.get(flags):
                return ""Full RELRO""
            else:
                return ""Partial RELRO""
        return ""No RELRO""
    except lief.not_found:
        return ""No RELRO""
",if bind_now in self . elf . get ( flags ) :,167
"def test_counter_instantiation(self):
    self.assertIs(type(typing_extensions.Counter()), collections.Counter)
    self.assertIs(type(typing_extensions.Counter[T]()), collections.Counter)
    self.assertIs(type(typing_extensions.Counter[int]()), collections.Counter)
    class C(typing_extensions.Counter[T]):
        ...
    if TYPING_3_5_3:
        self.assertIs(type(C[int]()), C)
        if not PEP_560:
            self.assertEqual(C.__bases__, (typing_extensions.Counter,))
        else:
            self.assertEqual(C.__bases__, (collections.Counter, typing.Generic))
",if not PEP_560 :,171
"def handle_exception(self, e, result):
    for k in sorted(result.thrift_spec):
        if result.thrift_spec[k][1] == ""success"":
            continue
        _, exc_name, exc_cls, _ = result.thrift_spec[k]
        if isinstance(e, exc_cls):
            setattr(result, exc_name, e)
            return True
    return False
","if result . thrift_spec [ k ] [ 1 ] == ""success"" :",112
"def find_from_pythonpath(name):
    for dirpath in sys.path:
        if not os.path.isdir(dirpath):
            continue
        path = os.path.join(dirpath, name)
        if os.path.isfile(path):
            return path
    return None
",if os . path . isfile ( path ) :,75
"def parse_location(srclocation):
    loc = symbols.Location(
        get_value(srclocation, ""file""), get_value(srclocation, ""project"")
    )
    if loc.is_null():
        loc = symbols.InstalledLocation(
            symbols.parse_package(get_value(srclocation, ""package"")),
            parse_package_db(get_value(srclocation, ""db"")),
        )
        if loc.is_null():
            loc = symbols.OtherLocation(get_value(srclocation, ""source""))
    return loc if not loc.is_null() else None
",if loc . is_null ( ) :,151
"def execute(self):
    logger.debug(f""host {self.host} try ports: {default_ports}"")
    for single_port in default_ports:
        if self.test_connection(self.host, single_port):
            logger.debug(f""Reachable port found: {single_port}"")
            self.publish_event(OpenPortEvent(port=single_port))
","if self . test_connection ( self . host , single_port ) :",98
"def get_dynamic_incoming_outgoing_rate(self, sle):
    # Get updated incoming/outgoing rate from transaction
    if sle.recalculate_rate:
        rate = self.get_incoming_outgoing_rate_from_transaction(sle)
        if flt(sle.actual_qty) >= 0:
            sle.incoming_rate = rate
        else:
            sle.outgoing_rate = rate
",if flt ( sle . actual_qty ) >= 0 :,106
"def _naf(mult):
    """"""Calculate non-adjacent form of number.""""""
    ret = []
    while mult:
        if mult % 2:
            nd = mult % 4
            if nd >= 2:
                nd = nd - 4
            ret += [nd]
            mult -= nd
        else:
            ret += [0]
        mult //= 2
    return ret
",if mult % 2 :,107
"def indent_xml(elem, level=0):
    """"""Do our pretty printing and make Matt very happy.""""""
    i = ""\n"" + level * ""  ""
    if elem:
        if not elem.text or not elem.text.strip():
            elem.text = i + ""  ""
        if not elem.tail or not elem.tail.strip():
            elem.tail = i
        for elem in elem:
            indent_xml(elem, level + 1)
        if not elem.tail or not elem.tail.strip():
            elem.tail = i
    else:
        if level and (not elem.tail or not elem.tail.strip()):
            elem.tail = i
",if not elem . text or not elem . text . strip ( ) :,177
"def clockface(radius):
    reset()
    pensize(7)
    for i in range(60):
        jump(radius)
        if i % 5 == 0:
            fd(25)
            jump(-radius - 25)
        else:
            dot(3)
            jump(-radius)
        rt(6)
",if i % 5 == 0 :,90
"def OnTextEntered(self, evt):
    text = self.GetValue()
    if self.doSearch(text):
        self.searches.append(text)
        if len(self.searches) > self.maxSearches:
            del self.searches[0]
        self.SetMenu(self.MakeMenu())
    self.SetValue("""")
",if len ( self . searches ) > self . maxSearches :,89
"def wrapped_send(bot, location, content=None, preprocessor=None, **kwargs):
    try:
        if preprocessor is not None:
            content = await preprocessor(bot, location, content)
        await location.send(content, **kwargs)
    except Exception as _exc:
        main_log.error(
            ""I could not send an owner notification to %s (%s)"",
            location,
            location.id,
            exc_info=_exc,
        )
",if preprocessor is not None :,125
"def explode(self, obj):
    """"""Determine if the object should be exploded.""""""
    if obj in self._done:
        return False
    result = False
    for item in self._explode:
        if hasattr(item, ""_moId""):
            # If it has a _moId it is an instance
            if obj._moId == item._moId:
                result = True
        else:
            # If it does not have a _moId it is a template
            if obj.__class__.__name__ == item.__name__:
                result = True
    if result:
        self._done.add(obj)
    return result
","if hasattr ( item , ""_moId"" ) :",166
"def _verify_treestore(itr, tree_values):
    i = 0
    while itr:
        values = tree_values[i]
        if treestore[itr][0] != values[0]:
            return False
        if treestore.iter_children(itr):
            if not _verify_treestore(treestore.iter_children(itr), values[1]):
                return False
        itr = treestore.iter_next(itr)
        i += 1
    return True
",if treestore . iter_children ( itr ) :,132
"def types(model_cls):
    # Gives us `item_types` and `album_types`
    attr_name = ""{0}_types"".format(model_cls.__name__.lower())
    types = {}
    for plugin in find_plugins():
        plugin_types = getattr(plugin, attr_name, {})
        for field in plugin_types:
            if field in types and plugin_types[field] != types[field]:
                raise PluginConflictException(
                    u""Plugin {0} defines flexible field {1} ""
                    u""which has already been defined with ""
                    u""another type."".format(plugin.name, field)
                )
        types.update(plugin_types)
    return types
",if field in types and plugin_types [ field ] != types [ field ] :,189
"def set_origin(self, origin):
    # This is useful to modify an exception to add origin information as
    # it ""passes by"", without losing traceback information. (In Python 3
    # we can use the built-in exception wrapping stuff, but it will be
    # some time before we can count on that...)
    if self.origin is None:
        if hasattr(origin, ""origin""):
            origin = origin.origin
        if not isinstance(origin, patsy.origin.Origin):
            origin = None
        self.origin = origin
","if hasattr ( origin , ""origin"" ) :",132
"def items(self):
    if self._items is not None:
        return self._items
    items = self.get_option(""recent-connections"")
    if not items:
        self._items = []
        return self._items
    for i in reversed(items):
        if ""name"" not in i or ""uuid"" not in i:
            items.remove(i)
        try:
            i[""device""] = self.get_device_path(i)
        except AdapterNotFound:
            i[""device""] = None
        except DeviceNotFound:
            items.remove(i)
        i[""time""] = float(i[""time""])
    self._items = items
    return self._items
","if ""name"" not in i or ""uuid"" not in i :",181
"def test_doc_attributes(self):
    print_test_name(""TEST DOC ATTRIBUTES"")
    correct = 0
    for example in DOC_EXAMPLES:
        original_schema = schema.parse(example.schema_string)
        if original_schema.doc is not None:
            correct += 1
        if original_schema.type == ""record"":
            for f in original_schema.fields:
                if f.doc is None:
                    self.fail(
                        ""Failed to preserve 'doc' in fields: "" + example.schema_string
                    )
    self.assertEqual(correct, len(DOC_EXAMPLES))
",if f . doc is None :,168
"def StopBackgroundWorkload(self):
    """"""Stop the background workoad.""""""
    for workload in background_workload.BACKGROUND_WORKLOADS:
        if workload.IsEnabled(self):
            if self.OS_TYPE in workload.EXCLUDED_OS_TYPES:
                raise NotImplementedError()
            workload.Stop(self)
",if workload . IsEnabled ( self ) :,87
"def resolve_expression(
    self, query=None, allow_joins=True, reuse=None, summarize=False, for_save=False
):
    resolved = super(SearchQuery, self).resolve_expression(
        query, allow_joins, reuse, summarize, for_save
    )
    if self.config:
        if not hasattr(self.config, ""resolve_expression""):
            resolved.config = Value(self.config).resolve_expression(
                query, allow_joins, reuse, summarize, for_save
            )
        else:
            resolved.config = self.config.resolve_expression(
                query, allow_joins, reuse, summarize, for_save
            )
    return resolved
","if not hasattr ( self . config , ""resolve_expression"" ) :",179
"def resolve_ip(filename, foffset, ip, need_line):
    sym, soffset, line = None, 0, None
    if filename and filename.startswith(""/""):
        sym, soffset = resolve_sym(filename, foffset)
        if not sym:
            sym, soffset = resolve_sym(filename, ip)
        if need_line:
            line = resolve_line(filename, ip)
    else:
        sym, soffset = kernel.resolve_kernel(ip)
    return sym, soffset, line
",if need_line :,132
"def create_model(self, dataset, weight_name=Checkpoint._LATEST):
    if not self.is_empty:
        run_config = copy.deepcopy(self._checkpoint.run_config)
        model = instantiate_model(run_config, dataset)
        if hasattr(self._checkpoint, ""model_props""):
            for k, v in self._checkpoint.model_props.items():
                setattr(model, k, v)
            delattr(self._checkpoint, ""model_props"")
        self._initialize_model(model, weight_name)
        return model
    else:
        raise ValueError(""Checkpoint is empty"")
","if hasattr ( self . _checkpoint , ""model_props"" ) :",160
"def get_py2exe_datafiles():
    datapath = get_data_path()
    head, tail = os.path.split(datapath)
    d = {}
    for root, dirs, files in os.walk(datapath):
        # Need to explicitly remove cocoa_agg files or py2exe complains
        # NOTE I dont know why, but do as previous version
        if ""Matplotlib.nib"" in files:
            files.remove(""Matplotlib.nib"")
        files = [os.path.join(root, filename) for filename in files]
        root = root.replace(tail, ""mpl-data"")
        root = root[root.index(""mpl-data"") :]
        d[root] = files
    return d.items()
","if ""Matplotlib.nib"" in files :",187
"def mouseClickEvent(self, ev):
    if ev.button() == QtCore.Qt.LeftButton and self.allowAdd:
        pos = ev.pos()
        if pos.x() < 0 or pos.x() > self.length:
            return
        if pos.y() < 0 or pos.y() > self.tickSize:
            return
        pos.setX(min(max(pos.x(), 0), self.length))
        self.addTick(pos.x() / self.length)
    elif ev.button() == QtCore.Qt.RightButton:
        self.showMenu(ev)
",if pos . y ( ) < 0 or pos . y ( ) > self . tickSize :,156
"def image_preprocess(self, image):
    with tf.name_scope(""image_preprocess""):
        if image.dtype.base_dtype != tf.float32:
            image = tf.cast(image, tf.float32)
        mean = [0.485, 0.456, 0.406]  # rgb
        std = [0.229, 0.224, 0.225]
        if self.image_bgr:
            mean = mean[::-1]
            std = std[::-1]
        image_mean = tf.constant(mean, dtype=tf.float32) * 255.0
        image_std = tf.constant(std, dtype=tf.float32) * 255.0
        image = (image - image_mean) / image_std
        return image
",if image . dtype . base_dtype != tf . float32 :,195
"def _addConsoleMessage(self, type: str, args: List[JSHandle]) -> None:
    if not self.listeners(Page.Events.Console):
        for arg in args:
            self._client._loop.create_task(arg.dispose())
        return
    textTokens = []
    for arg in args:
        remoteObject = arg._remoteObject
        if remoteObject.get(""objectId""):
            textTokens.append(arg.toString())
        else:
            textTokens.append(str(helper.valueFromRemoteObject(remoteObject)))
    message = ConsoleMessage(type, "" "".join(textTokens), args)
    self.emit(Page.Events.Console, message)
","if remoteObject . get ( ""objectId"" ) :",176
"def _handle_guild_scalar(self, add_scalar, _tag, _value, step=None):
    """"""Handler for guild.summary.SummaryWriter.add_scalar.""""""
    vals = self._summary_values(step)
    if vals:
        self.log.debug(""summary values via add_scalar: %s"", vals)
        for tag, val in vals.items():
            if val is not None:
                add_scalar(tag, val, step)
",if val is not None :,118
"def _get_token_from_cookie(self):
    for cookie in self.session.cookies:
        if cookie.name == ""X-APPLE-WEBAUTH-VALIDATE"":
            match = search(r""\bt=([^:]+)"", cookie.value)
            if match is None:
                raise Exception(""Can't extract token from %r"" % cookie.value)
            return {""token"": match.group(1)}
    raise Exception(""Token cookie not found"")
",if match is None :,117
"def unpack_RK(rk_str):
    flags = BYTES_ORD(rk_str[0])
    if flags & 2:
        # There's a SIGNED 30-bit integer in there!
        (i,) = unpack(""<i"", rk_str)
        i >>= 2  # div by 4 to drop the 2 flag bits
        if flags & 1:
            return i / 100.0
        return float(i)
    else:
        # It's the most significant 30 bits of an IEEE 754 64-bit FP number
        (d,) = unpack(""<d"", b""\0\0\0\0"" + BYTES_LITERAL(chr(flags & 252)) + rk_str[1:4])
        if flags & 1:
            return d / 100.0
        return d
",if flags & 1 :,200
"def _parse_photo(self):
    cat = ""lib""
    for photosection in self.plex.library.sections():
        if photosection.TYPE == library.PhotoSection.TYPE:
            self._load_attrs(photosection, cat)
            for photoalbum in photosection.all():
                self._load_attrs(photoalbum, cat)
                for photo in photoalbum.photos():
                    self._load_attrs(photo, cat)
",if photosection . TYPE == library . PhotoSection . TYPE :,130
"def count(num):
    cnt = 0
    for i in range(num):
        try:
            if i % 2:
                raise ValueError
            if i % 3:
                raise ArithmeticError(""1"")
        except Exception as e:
            cnt += 1
    return cnt
",if i % 2 :,80
"def node_exists(self, jid=None, node=None, ifrom=None):
    with self.lock:
        if jid is None:
            jid = self.xmpp.boundjid.full
        if node is None:
            node = """"
        if ifrom is None:
            ifrom = """"
        if isinstance(ifrom, JID):
            ifrom = ifrom.full
        if (jid, node, ifrom) not in self.nodes:
            return False
        return True
",if jid is None :,136
"def __call__(self, environ, start_response):
    script_name = environ.get(""HTTP_X_SCRIPT_NAME"")
    if script_name is not None:
        if script_name.endswith(""/""):
            warnings.warn(
                ""'X-Script-Name' header should not end in '/' (found: %r). ""
                ""Please fix your proxy's configuration."" % script_name
            )
            script_name = script_name.rstrip(""/"")
        environ[""SCRIPT_NAME""] = script_name
    return super(ProxyFix, self).__call__(environ, start_response)
","if script_name . endswith ( ""/"" ) :",151
"def backwardKillParagraph(self, event):
    """"""Kill the previous paragraph.""""""
    c = self.c
    w = self.editWidget(event)
    if not w:
        return
    self.beginCommand(w, undoType=""backward-kill-paragraph"")
    try:
        self.backwardParagraphHelper(event, extend=True)
        i, j = w.getSelectionRange()
        if i > 0:
            i = min(i + 1, j)
        c.killBufferCommands.kill(
            event, i, j, force=True, undoType=None  # Use i, j without change.
        )
        w.setSelectionRange(i, i, insert=i)
    finally:
        self.endCommand(changed=True, setLabel=True)
",if i > 0 :,199
"def bracket_replace(code):
    new = """"
    for e in bracket_split(code, [""()"", ""[]""], False):
        if e[0] == ""["":
            name = ""#PYJSREPL"" + str(len(REPL)) + ""{""
            new += name
            REPL[name] = e
        elif e[0] == ""("":  # can be a function call
            name = ""@PYJSREPL"" + str(len(REPL)) + ""}""
            new += name
            REPL[name] = e
        else:
            new += e
    return new
","elif e [ 0 ] == ""("" :",154
"def regenerate(self, request, **kwargs):
    obj = self.get_object()
    if ""all"" in request.data:
        for user in User.objects.all():
            if not user.is_anonymous():
                token = Token.objects.get(user=user)
                token.delete()
                Token.objects.create(user=user)
        return Response("""")
    if ""username"" in request.data:
        obj = get_object_or_404(User, username=request.data[""username""])
        self.check_object_permissions(self.request, obj)
    token = Token.objects.get(user=obj)
    token.delete()
    token = Token.objects.create(user=obj)
    return Response({""token"": token.key})
",if not user . is_anonymous ( ) :,200
"def signal_notebook_switch_page(self, notebook, current_page, index):
    if not hasattr(self.parent, ""rpc""):
        return
    # previous_page = notebook.get_nth_page(self.last_page_id)
    self.last_page_id = index
    for tab in self.tabs.values():
        if current_page != tab.box:
            continue
        if hasattr(tab, ""load_campaign_information""):
            tab.load_campaign_information(force=False)
",if current_page != tab . box :,137
"def get_word_parens_range(self, offset, opening=""("", closing="")""):
    end = self._find_word_end(offset)
    start_parens = self.code.index(opening, end)
    index = start_parens
    open_count = 0
    while index < len(self.code):
        if self.code[index] == opening:
            open_count += 1
        if self.code[index] == closing:
            open_count -= 1
        if open_count == 0:
            return (start_parens, index + 1)
        index += 1
    return (start_parens, index)
",if self . code [ index ] == opening :,160
"def append(self, child):
    if child not in (None, self):
        tag = child_tag(self._tag)
        if tag:
            if isinstance(child, Html):
                if child.tag != tag:
                    child = Html(tag, child)
            elif not child.startswith(""<%s"" % tag):
                child = Html(tag, child)
        super().append(child)
","elif not child . startswith ( ""<%s"" % tag ) :",113
"def cvPreprocess():
    import cv2
    imgarr_orig = []
    image_ext_list = ["".jpg"", "".png"", "".JPEG"", "".jpeg"", "".PNG"", "".JPG""]
    for file in onlyfiles:
        fimg = imgroot + file
        if any([x in image_ext_list for x in fimg]):
            print(fimg + "" is not an image file"")
            continue
        img1 = cv2.imread(fimg)
        if img1 is None:
            print(""ERROR opening "", fimg)
            continue
        img1 = cv2.resize(img1, (896, 896))
        imgarr_orig.append(img1)
    return imgarr_orig
",if any ( [ x in image_ext_list for x in fimg ] ) :,187
"def replace_nodes_in_symbol_table(
    symbols: SymbolTable, replacements: Dict[SymbolNode, SymbolNode]
) -> None:
    for name, node in symbols.items():
        if node.node:
            if node.node in replacements:
                new = replacements[node.node]
                old = node.node
                replace_object_state(new, old)
                node.node = new
            if isinstance(node.node, (Var, TypeAlias)):
                # Handle them here just in case these aren't exposed through the AST.
                node.node.accept(NodeReplaceVisitor(replacements))
",if node . node :,161
"def __find_audio_offset(self, fileobj):
    byte = 0x00
    while not (byte & 0x80):
        byte = ord(fileobj.read(1))
        size = to_int_be(fileobj.read(3))
        try:
            block_type = self.METADATA_BLOCKS[byte & 0x7F]
        except IndexError:
            block_type = None
        if block_type and block_type._distrust_size:
            # See comments in read_metadata_block; the size can't
            # be trusted for Vorbis comment blocks and Picture block
            block_type(fileobj)
        else:
            fileobj.read(size)
    return fileobj.tell()
",if block_type and block_type . _distrust_size :,190
"def startJail(self, name):
    with self.__lock:
        jail = self.__jails[name]
        if not jail.isAlive():
            jail.start()
        elif name in self.__reload_state:
            logSys.info(""Jail %r reloaded"", name)
            del self.__reload_state[name]
        if jail.idle:
            jail.idle = False
",if not jail . isAlive ( ) :,111
"def get_resolved_dependencies(self):
    dependencies = []
    for dependency in self.envconfig.deps:
        if dependency.indexserver is None:
            package = resolve_package(package_spec=dependency.name)
            if package != dependency.name:
                dependency = dependency.__class__(package)
        dependencies.append(dependency)
    return dependencies
",if dependency . indexserver is None :,93
"def _compile(self):
    if not self._compiled:
        # special case match-all query
        if self._is_match_all():
            return
        try:
            self._tokens = boolExpression.parseString(self._query, parseAll=self.strict)
        except ParseException:
            raise
        self._compiled = True
",if self . _is_match_all ( ) :,92
"def _compute_features(self, images):
    output_blobs = self._forward(images)
    features = []
    for blob in output_blobs:
        blob = blob.reshape((blob.shape[0], blob.shape[1]))
        if self.merge == ""max"":
            blob = blob.max(0)
        else:
            blob = self.merge(blob)
        features.append(blob)
    return np.vstack(features)
","if self . merge == ""max"" :",116
"def _list_shape_iter(shape):
    last_shape = _void
    for item in shape:
        if item is Ellipsis:
            if last_shape is _void:
                raise ValueError(
                    ""invalid shape spec: Ellipsis cannot be the"" ""first element""
                )
            while True:
                yield last_shape
        last_shape = item
        yield item
",if item is Ellipsis :,109
"def tokenize_url(self, field):
    field = field.strip()
    tokens = field.split("":"")
    offset = 0
    if tokens[0] == ""http"":
        offset = 1
        dstport = 80
        if len(tokens) > 2:
            inttokens = tokens[2].split(""/"")
            dstport = int(inttokens[0])
    elif tokens[0] == ""https"":
        dstport = 443
    else:
        if tokens[-1] is not None:
            dstport = int(tokens[-1])
    tld = tldextract.extract(tokens[offset])
    fqdn = ""."".join(part for part in tld if part)
    return (fqdn, dstport)
",if len ( tokens ) > 2 :,183
"def assert_summary_equals(self, records, tag, step, value):
    for record in records[1:]:
        if record.summary.value[0].tag != tag:
            continue
        if record.step != step:
            continue
        self.assertEqual(value, tf.make_ndarray(record.summary.value[0].tensor))
        return
    self.fail(""Could not find record for tag {} and step {}"".format(tag, step))
",if record . summary . value [ 0 ] . tag != tag :,114
"def getAttrDefault(key, fallback=None):
    try:
        default = defaultValuesCache[key]
    except KeyError:
        attrInfo = getAttributeInfo(key)
        if attrInfo is None:
            default = defaultValuesCache[key] = None
        else:
            default = defaultValuesCache[key] = attrInfo.defaultValue
    if default is None:
        default = fallback
    return default
",if attrInfo is None :,107
"def __getattr__(self, key):
    if key in self._raw:
        val = self._raw[key]
        if key in (""date"",):
            return pd.Timestamp(val)
        elif key in (""open"", ""close""):
            return pd.Timestamp(val).time()
        elif key in (""session_open"", ""session_close""):
            return pd.Timestamp(val[:2] + "":"" + val[-2:]).time()
        else:
            return val
    return super().__getattr__(key)
","elif key in ( ""session_open"" , ""session_close"" ) :",132
"def _combine_to_jointcaller(processed):
    """"""Add joint calling information to variants, while collapsing independent regions.""""""
    by_vrn_file = collections.OrderedDict()
    for data in (x[0] for x in processed):
        key = (
            tz.get_in((""config"", ""algorithm"", ""jointcaller""), data),
            data[""vrn_file""],
        )
        if key not in by_vrn_file:
            by_vrn_file[key] = []
        by_vrn_file[key].append(data)
    out = []
    for grouped_data in by_vrn_file.values():
        cur = grouped_data[0]
        out.append([cur])
    return out
",if key not in by_vrn_file :,187
"def assign_type(self, wb_type):
    if isinstance(wb_type, ListType):
        assigned_type = self.params[""element_type""].assign_type(
            wb_type.params[""element_type""]
        )
        if not isinstance(assigned_type, InvalidType):
            return ListType(assigned_type)
    return InvalidType()
","if not isinstance ( assigned_type , InvalidType ) :",93
"def set_billing_hours_and_amount(self):
    if not self.project:
        for timesheet in self.timesheets:
            ts_doc = frappe.get_doc(""Timesheet"", timesheet.time_sheet)
            if not timesheet.billing_hours and ts_doc.total_billable_hours:
                timesheet.billing_hours = ts_doc.total_billable_hours
            if not timesheet.billing_amount and ts_doc.total_billable_amount:
                timesheet.billing_amount = ts_doc.total_billable_amount
",if not timesheet . billing_amount and ts_doc . total_billable_amount :,153
"def add_changeset(repo_path, path_to_filename_in_archive):
    try:
        subprocess.check_output(
            [""hg"", ""add"", path_to_filename_in_archive],
            stderr=subprocess.STDOUT,
            cwd=repo_path,
        )
    except Exception as e:
        error_message = ""Error adding '{}' to repository: {}"".format(
            path_to_filename_in_archive, unicodify(e)
        )
        if isinstance(e, subprocess.CalledProcessError):
            error_message += ""\nOutput was:\n%s"" % unicodify(e.output)
        raise Exception(error_message)
","if isinstance ( e , subprocess . CalledProcessError ) :",170
"def full_path(self, *args, **query):
    """"""Return a full path""""""
    path = None
    if args:
        if len(args) > 1:
            raise TypeError(
                ""full_url() takes exactly 1 argument "" ""(%s given)"" % len(args)
            )
        path = args[0]
    if not path:
        path = self.path
    elif not path.startswith(""/""):
        path = remove_double_slash(""%s/%s"" % (self.path, path))
    return iri_to_uri(path, query)
",if len ( args ) > 1 :,147
"def retry_http_basic_auth(self, host, req, realm):
    user, pw = self.passwd.find_user_password(realm, host)
    if pw is not None:
        raw = ""%s:%s"" % (user, pw)
        auth = ""Basic %s"" % base64.b64encode(raw).strip()
        if req.get_header(self.auth_header, None) == auth:
            return None
        req.add_unredirected_header(self.auth_header, auth)
        return self.parent.open(req, timeout=req.timeout)
    else:
        return None
","if req . get_header ( self . auth_header , None ) == auth :",159
"def __call__(self, data):
    num_points = data.pos.shape[0]
    new_data = Data()
    for key in data.keys:
        if key == KDTREE_KEY:
            continue
        item = data[key]
        if torch.is_tensor(item) and num_points == item.shape[0]:
            item = item[self._indices].clone()
        elif torch.is_tensor(item):
            item = item.clone()
        setattr(new_data, key, item)
    return new_data
",if key == KDTREE_KEY :,144
"def flat(tree):
    stack = [tree]
    result = []
    stack_pop = stack.pop
    stack_extend = stack.extend
    result_append = result.append
    while stack:
        x = stack_pop()
        if isinstance(x, basestring):
            result_append(x)
        else:
            try:
                stack_extend(x)
            except TypeError:
                result_append(x)
    return result[::-1]
","if isinstance ( x , basestring ) :",126
"def do_remove(self):
    if self.netconf.locked(""dhcp""):
        if not self.pid:
            pid = read_pid_file(""/var/run/dnsmasq.pan1.pid"")
        else:
            pid = self.pid
        if not kill(pid, ""dnsmasq""):
            logging.info(""Stale dhcp lockfile found"")
        self.netconf.unlock(""dhcp"")
",if not self . pid :,106
"def set_xticklabels(self, labels=None, step=None, **kwargs):
    """"""Set x axis tick labels on the bottom row of the grid.""""""
    for ax in self.axes[-1, :]:
        if labels is None:
            labels = [l.get_text() for l in ax.get_xticklabels()]
            if step is not None:
                xticks = ax.get_xticks()[::step]
                labels = labels[::step]
                ax.set_xticks(xticks)
        ax.set_xticklabels(labels, **kwargs)
    return self
",if labels is None :,145
"def _resolved_values(self):
    values = []
    for k, v in self.values.items() if hasattr(self.values, ""items"") else self.values:
        if self.mapper:
            if isinstance(k, util.string_types):
                desc = _entity_descriptor(self.mapper, k)
                values.extend(desc._bulk_update_tuples(v))
            elif isinstance(k, attributes.QueryableAttribute):
                values.extend(k._bulk_update_tuples(v))
            else:
                values.append((k, v))
        else:
            values.append((k, v))
    return values
","if isinstance ( k , util . string_types ) :",176
"def _print_handles(self, text, handle_list):
    for handle in handle_list:
        source, citation = self.get_source_or_citation(handle, False)
        _LOG.debug(""\n\n\n"")
        if source:
            _LOG.debug(""---- %s -- source %s"" % (text, source.get_title()))
        elif citation:
            _LOG.debug(""---- %s -- citation %s"" % (text, citation.get_page()))
        else:
            _LOG.debug(""---- %s -- handle %s"" % (text, handle))
",if source :,161
"def test_items(self):
    expectException = (
        len(self.sparse_data) < len(self.data)
        and not self.instance.A._default_val is None
    )
    try:
        test = self.instance.A.items()
        # self.assertEqual( type(test), list )
        if self.instance.A._default_val is None:
            self.validateDict(self.sparse_data.items(), test)
        else:
            self.validateDict(self.data.items(), test)
        # self.assertFalse(expectException)
    except ValueError:
        if not expectException:
            raise
",if self . instance . A . _default_val is None :,168
"def __new__(cls, name, bases, d):
    rv = type.__new__(cls, name, bases, d)
    if ""methods"" not in d:
        methods = set(rv.methods or [])
        for key, value in d.iteritems():
            if key in http_method_funcs:
                methods.add(key.upper())
        # if we have no method at all in there we don't want to
        # add a method list.  (This is for instance the case for
        # the baseclass or another subclass of a base method view
        # that does not introduce new methods).
        if methods:
            rv.methods = sorted(methods)
    return rv
",if methods :,172
"def getResultSummary(self):
    if self.descriptionDone is not None or self.description is not None:
        stepsumm = util.join_list(self.descriptionDone or self.description)
        if self.descriptionSuffix:
            stepsumm += u"" "" + util.join_list(self.descriptionSuffix)
    else:
        stepsumm = u""finished""
    if self.results != SUCCESS:
        stepsumm += u"" (%s)"" % Results[self.results]
    return {u""step"": stepsumm}
",if self . descriptionSuffix :,137
"def analyze_items(items, category_id, agg_data):
    for item in items:
        if not agg_data[""cat_asp""].get(category_id, None):
            agg_data[""cat_asp""][category_id] = []
        agg_data[""cat_asp""][category_id].append(
            float(item.sellingStatus.currentPrice.value)
        )
        if getattr(item.listingInfo, ""watchCount"", None):
            agg_data[""watch_count""] += int(item.listingInfo.watchCount)
        if getattr(item, ""postalCode"", None):
            agg_data[""postal_code""] = item.postalCode
","if getattr ( item , ""postalCode"" , None ) :",169
"def _Determine_Do(self):
    from os.path import join
    self.applicable = 1
    siloedPythonInstallDir = black.configure.items[""siloedPythonInstallDir""].Get()
    if sys.platform == ""darwin"":
        siloedPyVer = black.configure.items[""siloedPyVer""].Get()
        self.value = join(
            siloedPythonInstallDir, ""Python.framework"", ""Versions"", siloedPyVer, ""bin""
        )
    else:
        self.value = siloedPythonInstallDir
        if sys.platform != ""win32"":
            self.value = join(self.value, ""bin"")
    self.determined = 1
","if sys . platform != ""win32"" :",177
"def work(self):
    idle_times = 0
    while True:
        if shutting_down.is_set():
            log.info(""Stop sync worker"")
            break
        try:
            job = self.commit_queue.get(timeout=self.timeout, block=True)
            if job[""type""] == ""commit"":
                self.commits.append(job)
            log.debug(""Got a commit job"")
            idle_times = 0
            idle.clear()
        except Empty:
            log.debug(""Nothing to do right now, going idle"")
            if idle_times > self.min_idle_times:
                idle.set()
            idle_times += 1
            self.on_idle()
",if shutting_down . is_set ( ) :,200
"def TryMerge(self, d):
    while d.avail() > 0:
        tt = d.getVarInt32()
        if tt == 10:
            self.set_module(d.getPrefixedString())
            continue
        if tt == 18:
            self.set_version(d.getPrefixedString())
            continue
        if tt == 24:
            self.set_instances(d.getVarInt64())
            continue
        if tt == 0:
            raise ProtocolBuffer.ProtocolBufferDecodeError
        d.skipData(tt)
",if tt == 0 :,150
"def expand_group(client: Any, group_key: str):
    """"""Determines if an email is really a dl.""""""
    # NOTE: Google Groups does not support other DLs as Group owners
    # https://stackoverflow.com/questions/31552146/group-as-owner-or-manager-fails-with-400-error
    try:
        response = list_members(client, group_key, propagate_errors=True)
        if response.get(""members""):
            return [x[""email""] for x in response.get(""members"", [])]
    except HttpError as e:
        if e.resp.status == 404:
            pass
    return []
",if e . resp . status == 404 :,162
"def validate_against_domain(
    cls, ensemble: Optional[""PolicyEnsemble""], domain: Optional[Domain]
) -> None:
    if ensemble is None:
        return
    for p in ensemble.policies:
        if not isinstance(p, TwoStageFallbackPolicy):
            continue
        if domain is None or p.deny_suggestion_intent_name not in domain.intents:
            raise InvalidDomain(
                ""The intent '{0}' must be present in the ""
                ""domain file to use TwoStageFallbackPolicy. ""
                ""Either include the intent '{0}' in your domain ""
                ""or exclude the TwoStageFallbackPolicy from your ""
                ""policy configuration"".format(p.deny_suggestion_intent_name)
            )
",if domain is None or p . deny_suggestion_intent_name not in domain . intents :,195
"def _ndvi(nir_data, red_data):
    out = np.zeros_like(nir_data)
    rows, cols = nir_data.shape
    for y in range(0, rows):
        for x in range(0, cols):
            nir = nir_data[y, x]
            red = red_data[y, x]
            if nir == red:  # cover zero divison case
                continue
            soma = nir + red
            out[y, x] = (nir - red) / soma
    return out
",if nir == red :,154
"def sysroot():
    cmd = ""set sysroot remote:/""
    if is_android():
        if gdb.parameter(""sysroot"") == ""target:"":
            gdb.execute(cmd)
        else:
            print(message.notice(""sysroot is already set, skipping %r"" % cmd))
","if gdb . parameter ( ""sysroot"" ) == ""target:"" :",77
"def _run(self):
    when_pressed = 0.0
    pressed = False
    while not self._done.is_set():
        now = time.monotonic()
        if now - when_pressed > self._debounce_time:
            if GPIO.input(self._channel) == self._expected:
                if not pressed:
                    pressed = True
                    when_pressed = now
                    self._trigger(self._pressed_queue, self._pressed_callback)
            else:
                if pressed:
                    pressed = False
                    self._trigger(self._released_queue, self._released_callback)
        self._done.wait(0.05)
",if not pressed :,187
"def find_comment(line):
    """"""Finds the index of a comment # and returns None if not found""""""
    instring, instring_char = False, """"
    for i, char in enumerate(line):
        if char in ('""', ""'""):
            if instring:
                if char == instring_char:
                    instring = False
                    instring_char = """"
            else:
                instring = True
                instring_char = char
        elif char == ""#"":
            if not instring:
                return i
    return None
","elif char == ""#"" :",155
"def _deduplicate_data(self):
    # Remove duplicate entries, without recreating self.data object
    dup_lines = []
    hash_set = set()
    for i, fields in enumerate(self.data):
        fields_hash = hash(self.separator.join(fields))
        if fields_hash in hash_set:
            dup_lines.append(i)
            log.debug(
                'Found duplicate entry in tool data table ""%s"", but duplicates are not allowed, removing additional entry for: ""%s""',
                self.name,
                fields,
            )
        else:
            hash_set.add(fields_hash)
    for i in reversed(dup_lines):
        self.data.pop(i)
",if fields_hash in hash_set :,194
"def sample_independent(
    self,
    study: Study,
    trial: FrozenTrial,
    param_name: str,
    param_distribution: distributions.BaseDistribution,
) -> Any:
    self._raise_error_if_multi_objective(study)
    if self._warn_independent_sampling:
        complete_trials = self._get_trials(study)
        if len(complete_trials) >= self._n_startup_trials:
            self._log_independent_sampling(trial, param_name)
    return self._independent_sampler.sample_independent(
        study, trial, param_name, param_distribution
    )
",if len ( complete_trials ) >= self . _n_startup_trials :,158
"def publish(self):
    """"""Publish new events to the subscribers.""""""
    while True:
        event = await self.event_source.get()
        str_buffer = []
        if event == POISON_PILL:
            return
        if isinstance(event, str):
            str_buffer.append(event)
        elif event.type == EventTypes.BLOCK_VALID:
            str_buffer = map(json.dumps, eventify_block(event.data))
        for str_item in str_buffer:
            for _, websocket in self.subscribers.items():
                await websocket.send_str(str_item)
",if event == POISON_PILL :,164
"def push(self):
    advice = self.check()
    if not self._context[""silent""]:
        if not self.hasPendingSync(advice):
            print(""No changes to push."")
            return
        choice = input(""Continue? y/N:"")
        if choice != ""y"":
            print(""Aborted on user command"")
            return
    print(""push local changes to remote..."")
    self._publish.syncRemote(self._context[""srcroot""], advice)
","if choice != ""y"" :",123
"def readline(self, limit=-1):
    i = self._rbuf.find(""\n"")
    while i < 0 and not (0 < limit <= len(self._rbuf)):
        new = self._raw_read(self._rbufsize)
        if not new:
            break
        i = new.find(""\n"")
        if i >= 0:
            i += len(self._rbuf)
        self._rbuf = self._rbuf + new
    if i < 0:
        i = len(self._rbuf)
    else:
        i += 1
    if 0 <= limit < len(self._rbuf):
        i = limit
    data, self._rbuf = self._rbuf[:i], self._rbuf[i:]
    return data
",if i >= 0 :,194
"def main():
    init_app(set_backends=True, routes=False)
    dry_run = ""--dry"" in sys.argv
    if not dry_run:
        script_utils.add_file_logger(logger, __file__)
    with transaction.atomic():
        normalize_source_tags()
        add_claimed_tags()
        add_osf_provider_tags()
        add_prereg_campaign_tags()
        if dry_run:
            raise RuntimeError(""Dry run, transaction rolled back"")
",if dry_run :,136
"def iter_segments(self):
    while not self.closed:
        for chunk in filter(self.valid_chunk, self.chunks):
            self.logger.debug(""Adding chunk {0} to queue"", chunk.num)
            yield chunk
            # End of stream
            if self.closed:
                return
            self.chunk_id = chunk.num + 1
        if self.wait(self.module_info_reload_time):
            try:
                self.process_module_info()
            except StreamError as err:
                self.logger.warning(""Failed to process module info: {0}"", err)
",if self . closed :,169
"def SetItems(self, choices):
    self.choices = choices
    self.choice_names = self.get_choice_names()
    self.list_dlg.SetItems(self.get_choice_labels())
    labels = self.get_choice_labels()
    for i in range(len(self.choices)):
        if self.choices[i][1] is None:
            # Tag missing items
            self.list_dlg.SetItemBackgroundColour(i, ""pink"")
        elif labels[i].endswith(""Name!)""):
            # Tag duplicated items
            self.list_dlg.SetItemForegroundColour(i, ""grey"")
    # on Mac, changing the items clears the current selection
    self.SetChecked(self.checked)
    self.Refresh()
","elif labels [ i ] . endswith ( ""Name!)"" ) :",192
"def combine_logs(audit_logs, statement_text_logs):
    for audit_transaction in audit_logs:
        for audit_query in audit_logs[audit_transaction]:
            matching_statement_text_logs = statement_text_logs.get(hash(audit_query))
            if matching_statement_text_logs:
                statement_text_log = matching_statement_text_logs.pop()
                if statement_text_log:
                    if statement_text_log.start_time:
                        audit_query.start_time = statement_text_log.start_time
                    if statement_text_log.end_time:
                        audit_query.end_time = statement_text_log.end_time
",if statement_text_log . start_time :,197
"def handle_data(self, data):
    if self.in_span or self.in_div:
        if data == ""No such user (please note that login is case sensitive)"":
            self.no_user = True
        elif data == ""Invalid password"":
            self.bad_pw = True
        elif data == ""User with that email already exists"":
            self.already_exists = True
","elif data == ""Invalid password"" :",101
"def K(exp):
    ""Helper function to specify keymap""
    import re
    modifier_strs = []
    while True:
        m = re.match(r""\A(C|Ctrl|M|Alt|Shift|Super|Win)-"", exp)
        if m is None:
            break
        modifier = m.group(1)
        modifier_strs.append(modifier)
        exp = re.sub(r""\A{}-"".format(modifier), """", exp)
    key_str = exp.upper()
    key = getattr(Key, key_str)
    return Combo(create_modifiers_from_strings(modifier_strs), key)
",if m is None :,161
"def local_min(self, hmap):
    rows = len(hmap)
    cols = len(hmap[0])
    min_list = []
    for row in range(rows):
        for col in range(cols):
            for d_row, d_col in ((1, 0), (0, 1), (-1, 0), (0, -1)):
                h_row = (row + d_row) % rows
                h_col = (col + d_col) % cols
                if hmap[h_row][h_col] < hmap[row][col]:
                    break
            else:
                min_list.append((row, col))
    return min_list
",if hmap [ h_row ] [ h_col ] < hmap [ row ] [ col ] :,186
"def _check_processing(self):
    now = time.time()
    self.mutex.acquire()
    while (
        self.processing.qsize()
        and self.processing.top
        and self.processing.top.exetime < now
    ):
        task = self.processing.get_nowait()
        if task.taskid is None:
            continue
        task.exetime = 0
        self.priority_queue.put(task)
        logger.info(""processing: retry %s"", task.taskid)
    self.mutex.release()
",if task . taskid is None :,143
"def autoname(self):
    naming_method = frappe.db.get_value(""HR Settings"", None, ""emp_created_by"")
    if not naming_method:
        throw(_(""Please setup Employee Naming System in Human Resource > HR Settings""))
    else:
        if naming_method == ""Naming Series"":
            set_name_by_naming_series(self)
        elif naming_method == ""Employee Number"":
            self.name = self.employee_number
        elif naming_method == ""Full Name"":
            self.set_employee_name()
            self.name = self.employee_name
    self.employee = self.name
","elif naming_method == ""Employee Number"" :",169
"def __fixdict(self, dict):
    for key in dict.keys():
        if key[:6] == ""start_"":
            tag = key[6:]
            start, end = self.elements.get(tag, (None, None))
            if start is None:
                self.elements[tag] = getattr(self, key), end
        elif key[:4] == ""end_"":
            tag = key[4:]
            start, end = self.elements.get(tag, (None, None))
            if end is None:
                self.elements[tag] = start, getattr(self, key)
",if end is None :,162
"def parseAGL(filename):  # -> { 2126: 'Omega', ... }
    m = {}
    for line in readLines(filename):
        # Omega;2126
        # dalethatafpatah;05D3 05B2   # higher-level combinations; ignored
        line = line.strip()
        if len(line) > 0 and line[0] != ""#"":
            name, uc = tuple([c.strip() for c in line.split("";"")])
            if uc.find("" "") == -1:
                # it's a 1:1 mapping
                m[int(uc, 16)] = name
    return m
","if len ( line ) > 0 and line [ 0 ] != ""#"" :",169
"def password(self, password):
    self._password = password
    if password:
        if not which(""sshpass""):
            self.eeprint(
                ""Install sshpass to using password: https://duckduckgo.com/?q=install+sshpass\n""
                + ""Note! There are a lot of security reasons to stop using password auth.""
            )
        verbose = ""-v"" if ""-v"" in self.sshpass else []
        self.sshpass = [""sshpass"", ""-p"", password] + verbose
    else:
        self.sshpass = []
","if not which ( ""sshpass"" ) :",147
"def test_region_redirects_multiple_requests(self):
    try:
        response = self.client.list_objects(Bucket=self.bucket_name)
        self.assertEqual(response[""ResponseMetadata""][""HTTPStatusCode""], 200)
        second_response = self.client.list_objects(Bucket=self.bucket_name)
        self.assertEqual(second_response[""ResponseMetadata""][""HTTPStatusCode""], 200)
    except ClientError as e:
        error = e.response[""Error""].get(""Code"", None)
        if error == ""PermanentRedirect"":
            self.fail(""S3 client failed to redirect to the proper region."")
","if error == ""PermanentRedirect"" :",148
"def get_action_type(action_space):
    """"""Method to get the action type to choose prob. dist. to sample actions from NN logits output""""""
    if isinstance(action_space, spaces.Box):
        shape = action_space.shape
        assert len(shape) == 1
        if shape[0] == 1:
            return ""continuous""
        else:
            return ""multi_continuous""
    elif isinstance(action_space, spaces.Discrete):
        return ""discrete""
    elif isinstance(action_space, spaces.MultiDiscrete):
        return ""multi_discrete""
    elif isinstance(action_space, spaces.MultiBinary):
        return ""multi_binary""
    else:
        raise NotImplementedError
",if shape [ 0 ] == 1 :,177
"def remove_stale_sockets(self):
    with self.lock:
        if self.opts.max_idle_time_ms is not None:
            for sock_info in self.sockets.copy():
                age = _time() - sock_info.last_checkout
                if age > self.opts.max_idle_time_ms:
                    self.sockets.remove(sock_info)
                    sock_info.close()
    while len(self.sockets) + self.active_sockets < self.opts.min_pool_size:
        sock_info = self.connect()
        with self.lock:
            self.sockets.add(sock_info)
",if self . opts . max_idle_time_ms is not None :,176
"def _setReadyState(self, state: str) -> None:
    if state != self.__readyState:
        self.__log_debug(""- %s -> %s"", self.__readyState, state)
        self.__readyState = state
        if state == ""open"":
            self.emit(""open"")
        elif state == ""closed"":
            self.emit(""close"")
            # no more events will be emitted, so remove all event listeners
            # to facilitate garbage collection.
            self.remove_all_listeners()
","elif state == ""closed"" :",131
"def currentLevel(self):
    currentStr = """"
    for stackType, stackValue in self.stackVals:
        if stackType == ""dict"":
            if isinstance(stackValue, str):
                currentStr += ""['"" + stackValue + ""']""
            else:  # numeric key...
                currentStr += ""["" + str(stackValue) + ""]""
        elif stackType == ""listLike"":
            currentStr += ""["" + str(stackValue) + ""]""
        elif stackType == ""getattr"":
            currentStr += "".__getattribute__('"" + stackValue + ""')""
        else:
            raise Exception(f""Cannot get attribute of type {stackType}"")
    return currentStr
","if stackType == ""dict"" :",176
"def filter_latest_pkgs(pkgs):
    pkgname2latest = {}
    for x in pkgs:
        pkgname = core.normalize_pkgname(x.pkgname)
        if pkgname not in pkgname2latest:
            pkgname2latest[pkgname] = x
        elif x.parsed_version > pkgname2latest[pkgname].parsed_version:
            pkgname2latest[pkgname] = x
    return pkgname2latest.values()
",if pkgname not in pkgname2latest :,103
"def test_url_invalid_set():
    for line in URL_INVALID_TESTS.split(""\n""):
        # strip line, skip over empty lines
        line = line.strip()
        if line == """":
            continue
        # skip over comments
        match = COMMENT.match(line)
        if match:
            continue
        mbox = address.parse(line, strict=True)
        assert_equal(mbox, None)
",if match :,115
"def check_block(cls, block):
    if (
        len(block) == 4
        and block[0][0]
        and block[0][0][0] == ""@""
        and block[2][0]
        and block[2][0][0] == ""+""
        and block[1][0]
    ):
        # Check the sequence line, make sure it contains only G/C/A/T/N
        match = cls.bases_regexp.match(block[1][0])
        if match:
            start, end = match.span()
            if (end - start) == len(block[1][0]):
                return True
    return False
",if match :,174
"def load_from_file(self, filename):
    self._filename = filename
    if os.path.exists(filename):
        if not os.path.isfile(filename):
            raise IOError(""%s exists and is not a file"" % filename)
        with open(filename, ""r"") as f:
            self._properties = json.load(f)
    else:
        mkpath(os.path.dirname(filename))
        self.save_to_file()
",if not os . path . isfile ( filename ) :,118
"def add_system_info_creds_to_config(creds):
    for user in creds:
        ConfigService.creds_add_username(creds[user][""username""])
        if ""password"" in creds[user] and creds[user][""password""]:
            ConfigService.creds_add_password(creds[user][""password""])
        if ""lm_hash"" in creds[user] and creds[user][""lm_hash""]:
            ConfigService.creds_add_lm_hash(creds[user][""lm_hash""])
        if ""ntlm_hash"" in creds[user] and creds[user][""ntlm_hash""]:
            ConfigService.creds_add_ntlm_hash(creds[user][""ntlm_hash""])
","if ""lm_hash"" in creds [ user ] and creds [ user ] [ ""lm_hash"" ] :",175
"def line_number(self):
    if self._line_range:
        line_range = self._line_range
        if line_range.stop - line_range.start > 1:
            return ""%03d-%03d"" % (line_range.start, line_range.stop - 1)
        else:
            return ""%03d"" % line_range.start
",if line_range . stop - line_range . start > 1 :,95
"def smooth(self, y, x=None, weights=None):
    if self.method == ""target_df"":
        if hasattr(self, ""pen""):
            self.fit(y, x=x, weights=weights, pen=self.pen)
        else:
            self.fit_target_df(y, x=x, weights=weights, df=self.target_df)
    elif self.method == ""optimize_gcv"":
        self.fit_optimize_gcv(y, x=x, weights=weights)
","if hasattr ( self , ""pen"" ) :",133
"def dict_from_cursor(data=None, keys=None):
    filtered_dict = {}
    # Convert Uids to str
    data = bson_dumps(data)
    python_dict = json.loads(data)
    for key in keys:
        value = python_dict.get(key)
        if type(value) is dict:
            # Try to get mongo_id
            mongo_id = value.get(""$oid"")
            if mongo_id:
                value = mongo_id
        if key == ""_id"":
            key = ""id""
        filtered_dict[key] = value
    return filtered_dict
",if mongo_id :,170
"def pytest_plugin_registered(self, plugin):
    nodeid = None
    try:
        p = py.path.local(plugin.__file__)
    except AttributeError:
        pass
    else:
        # construct the base nodeid which is later used to check
        # what fixtures are visible for particular tests (as denoted
        # by their test id)
        if p.basename.startswith(""conftest.py""):
            nodeid = p.dirpath().relto(self.config.rootdir)
            if p.sep != ""/"":
                nodeid = nodeid.replace(p.sep, ""/"")
    self.parsefactories(plugin, nodeid)
","if p . sep != ""/"" :",161
"def _escape_unsafe_values(self, *values):
    # type: (str) -> Generator[str]
    """"""Escape unsafe values (name, section name) for API version 2.10 and below""""""
    for value in values:
        if value not in UNSAFE_NAMES_2_10:
            yield value
        else:
            self.task.log.info(
                ""Converting unsafe hyper parameter name/section '{}' to '{}'"".format(
                    value, ""_"" + value
                )
            )
            yield ""_"" + value
",if value not in UNSAFE_NAMES_2_10 :,143
"def _identifier_split(self, identifier):
    """"""Return (name, start, end) string tuple from an identifier (PRIVATE).""""""
    if ""/"" in identifier:
        name, start_end = identifier.rsplit(""/"", 1)
        if start_end.count(""-"") == 1:
            try:
                start, end = start_end.split(""-"")
                return name, int(start), int(end)
            except ValueError:
                # Non-integers after final '/' - fall through
                pass
    return identifier, None, None
","if start_end . count ( ""-"" ) == 1 :",138
"def _complete_initial_layout(self):
    """"""Finish initial layout; called after toplevel win is positioned""""""
    # Init tool group sizes by setting vpaned positions
    for paned in self._get_paneds():
        if paned._initial_divider_position:
            pos = paned._initial_divider_position
            GLib.idle_add(paned.set_position, pos)
",if paned . _initial_divider_position :,101
"def _init_mapping(self, result):
    for wamp_uri, full_name in result.items():
        for prefix in self.PREFIXES:
            if not full_name.startswith(prefix):
                continue
            short_name = full_name[len(prefix) :]
            self._mapping[short_name] = wamp_uri
",if not full_name . startswith ( prefix ) :,92
"def get_bounce_message(reason, ses_data, details):
    if reason != ""bounce"":
        return
    if ses_data:
        bouncedRecipients = ses_data.get(""bounce"", {}).get(""bouncedRecipients"")
        if bouncedRecipients:
            recipient = bouncedRecipients[0]
            return recipient.get(""diagnosticCode"") or recipient.get(""status"")
    elif details:
        return details
",if bouncedRecipients :,124
"def do_If(self, node, elif_flag=False):
    self.div(""statement"")
    self.keyword(""elif"" if elif_flag else ""if"")
    self.visit(node.test)
    self.colon()
    self.div_body(node.body)
    if node.orelse:
        node1 = node.orelse[0]
        if isinstance(node1, ast.If) and len(node.orelse) == 1:
            self.do_If(node1, elif_flag=True)
        else:
            self.keyword(""else"")
            self.colon()
            self.div_body(node.orelse)
    self.end_div(""statement"")
","if isinstance ( node1 , ast . If ) and len ( node . orelse ) == 1 :",176
"def matches(self, filepath):
    matched = False
    parent_path = os.path.dirname(filepath)
    parent_path_dirs = split_path(parent_path)
    for pattern in self.patterns:
        negative = pattern.exclusion
        match = pattern.match(filepath)
        if not match and parent_path != """":
            if len(pattern.dirs) <= len(parent_path_dirs):
                match = pattern.match(
                    os.path.sep.join(parent_path_dirs[: len(pattern.dirs)])
                )
        if match:
            matched = not negative
    return matched
",if match :,165
"def test_11_wait_for_first_reboot_with_bhyve():
    if update_version is None:
        pytest.skip(""No update found"")
    elif download_failed is True:
        pytest.skip(f""Downloading {selected_trains} failed"")
    elif reboot is False:
        pytest.skip(""Reboot is False skip"")
    else:
        if vm_name is None:
            pytest.skip(""skip no vm_name"")
        else:
            while vm_state(vm_name) != ""stopped"":
                sleep(5)
            assert vm_start(vm_name) is True
    sleep(1)
",if vm_name is None :,167
"def _check_network_private(test_network):
    test_net = ipaddress.IPNetwork(test_network)
    test_start = test_net.network
    test_end = test_net.broadcast
    for network in settings.vpn.safe_priv_subnets:
        network = ipaddress.IPNetwork(network)
        net_start = network.network
        net_end = network.broadcast
        if test_start >= net_start and test_end <= net_end:
            return True
    return False
",if test_start >= net_start and test_end <= net_end :,129
"def remove_stale_sockets(self):
    with self.lock:
        if self.opts.max_idle_time_ms is not None:
            for sock_info in self.sockets.copy():
                age = _time() - sock_info.last_checkout
                if age > self.opts.max_idle_time_ms:
                    self.sockets.remove(sock_info)
                    sock_info.close()
    while len(self.sockets) + self.active_sockets < self.opts.min_pool_size:
        sock_info = self.connect()
        with self.lock:
            self.sockets.add(sock_info)
",if age > self . opts . max_idle_time_ms :,176
"def hint(self, button):
    """"""As hilight, but marks GTK Button as well""""""
    active = None
    for b in self.button_widgets.values():
        if b.widget.get_sensitive():
            b.widget.set_state(Gtk.StateType.NORMAL)
            if b.name == button:
                active = b.widget
    if active is not None:
        active.set_state(Gtk.StateType.ACTIVE)
    self.hilight(button)
",if b . name == button :,126
"def post_process(self, retcode):
    if not self.ok_codes:
        return retcode
    for code in self.ok_codes:
        self.log.debug(""Comparing %s with %s codes"", code, retcode)
        if code == int(retcode):
            self.log.info(""Exit code %s was changed to 0 by RCAssert plugin"", code)
            return 0
    self.log.info(
        ""Changing exit code to %s because RCAssert pass list was unsatisfied"",
        self.fail_code,
    )
    return self.fail_code
",if code == int ( retcode ) :,149
"def get_form_kwargs(self):
    result = super().get_form_kwargs()
    if self.request.method != ""POST"":
        if self.initial:
            # When going from other form (for example ZIP import)
            result.pop(""data"", None)
            result.pop(""files"", None)
        if self.has_all_fields() and not self.empty_form:
            result[""data""] = self.request.GET
    return result
",if self . has_all_fields ( ) and not self . empty_form :,120
"def transform_first_chunk(self, headers, chunk, finishing):
    if self._chunking:
        # No need to chunk the output if a Content-Length is specified
        if ""Content-Length"" in headers or ""Transfer-Encoding"" in headers:
            self._chunking = False
        else:
            headers[""Transfer-Encoding""] = ""chunked""
            chunk = self.transform_chunk(chunk, finishing)
    return headers, chunk
","if ""Content-Length"" in headers or ""Transfer-Encoding"" in headers :",112
"def copy_stream(self, in_fd, out_fd, length=2 ** 64):
    total = 0
    while 1:
        available_to_read = min(length - total, self.BUFFERSIZE)
        data = in_fd.read(available_to_read)
        if not data:
            break
        out_fd.write(data)
        total += len(data)
        self.session.report_progress(""Reading %s @ %#x"", in_fd.urn, total)
",if not data :,128
"def _trim_steps(self, num_steps):
    """"""Trims a given number of steps from the end of the sequence.""""""
    steps_trimmed = 0
    for i in reversed(range(len(self._events))):
        if self._events[i].event_type == PolyphonicEvent.STEP_END:
            if steps_trimmed == num_steps:
                del self._events[i + 1 :]
                break
            steps_trimmed += 1
        elif i == 0:
            self._events = [
                PolyphonicEvent(event_type=PolyphonicEvent.START, pitch=None)
            ]
            break
",if self . _events [ i ] . event_type == PolyphonicEvent . STEP_END :,171
"def get_img_file(dir_name: str) -> list:
    """"""Get all image file paths in several directories which have the same parent directory.""""""
    images = []
    for parent, _, filenames in os.walk(dir_name):
        for filename in filenames:
            if not is_image_file(filename):
                continue
            img_path = os.path.join(parent, filename)
            images.append(img_path)
    return images
",if not is_image_file ( filename ) :,118
"def get_agg_title(clause):
    attr = str(clause.attribute)
    if clause.aggregation is None:
        if len(attr) > 25:
            return attr[:15] + ""..."" + attr[-10:]
        return f""{attr}""
    elif attr == ""Record"":
        return f""Number of Records""
    else:
        if len(attr) > 15:
            return f""{clause._aggregation_name.capitalize()} of {attr[:15]}...""
        return f""{clause._aggregation_name.capitalize()} of {attr}""
",if len ( attr ) > 25 :,137
"def _check_realign(data):
    """"""Check for realignment, which is not supported in GATK4""""""
    if ""gatk4"" not in data[""algorithm""].get(""tools_off"", []) and not ""gatk4"" == data[
        ""algorithm""
    ].get(""tools_off""):
        if data[""algorithm""].get(""realign""):
            raise ValueError(
                ""In sample %s, realign specified but it is not supported for GATK4. ""
                ""Realignment is generally not necessary for most variant callers.""
                % (dd.get_sample_name(data))
            )
","if data [ ""algorithm"" ] . get ( ""realign"" ) :",160
"def __call__(self, target):
    if ""weights"" not in target.temp:
        return True
    targets = target.temp[""weights""]
    for cname in target.children:
        if cname in targets:
            c = target.children[cname]
            deviation = abs((c.weight - targets[cname]) / targets[cname])
            if deviation > self.tolerance:
                return True
    if ""cash"" in target.temp:
        cash_deviation = abs(
            (target.capital - targets.value) / targets.value - target.temp[""cash""]
        )
        if cash_deviation > self.tolerance:
            return True
    return False
",if deviation > self . tolerance :,178
"def status_string(self):
    if not self.live:
        if self.expired:
            return _(""expired"")
        elif self.approved_schedule:
            return _(""scheduled"")
        elif self.workflow_in_progress:
            return _(""in moderation"")
        else:
            return _(""draft"")
    else:
        if self.approved_schedule:
            return _(""live + scheduled"")
        elif self.workflow_in_progress:
            return _(""live + in moderation"")
        elif self.has_unpublished_changes:
            return _(""live + draft"")
        else:
            return _(""live"")
",if self . expired :,166
"def __getitem__(self, item):
    if item == ""EntityId"":
        if ""EntityId"" not in self:
            if self.use_uuid:
                super(PlayerDict, self).__setitem__(
                    ""EntityId"", self.get_name_from_uuid()
                )
            else:
                super(PlayerDict, self).__setitem__(""EntityId"", self._name)
    return super(PlayerDict, self).__getitem__(item)
","if ""EntityId"" not in self :",122
"def _to_num_bytes(java_mem_str):
    if isinstance(java_mem_str, string_types):
        for i, magnitude in enumerate((""k"", ""m"", ""g"", ""t""), start=1):
            if java_mem_str.lower().endswith(magnitude):
                return int(java_mem_str[:-1]) * 1024 ** i
    return int(java_mem_str)
",if java_mem_str . lower ( ) . endswith ( magnitude ) :,103
"def test_layout_instantiate_subplots(self):
    layout = (
        Curve(range(10))
        + Curve(range(10))
        + Image(np.random.rand(10, 10))
        + Curve(range(10))
        + Curve(range(10))
    )
    plot = mpl_renderer.get_plot(layout)
    positions = [(0, 0), (0, 1), (0, 2), (0, 3), (1, 0)]
    self.assertEqual(sorted(plot.subplots.keys()), positions)
    for i, pos in enumerate(positions):
        adjoint = plot.subplots[pos]
        if ""main"" in adjoint.subplots:
            self.assertEqual(adjoint.subplots[""main""].layout_num, i + 1)
","if ""main"" in adjoint . subplots :",194
"def __str__(self):
    width = int(os.environ.get(""COLUMNS"", ""80""))
    s = (
        self.getSynopsis()
        + ""\n""
        + ""(use 'tahoe --help' to view global options)\n""
        + ""\n""
        + self.getUsage()
    )
    if self.description:
        s += ""\n"" + wrap_paragraphs(self.description, width) + ""\n""
    if self.description_unwrapped:
        du = textwrap.dedent(self.description_unwrapped)
        if du.startswith(""\n""):
            du = du[1:]
        s += ""\n"" + du + ""\n""
    return s
","if du . startswith ( ""\n"" ) :",181
"def open(self, path, mode=""rb"", cryptoType=-1, cryptoKey=-1, cryptoCounter=-1):
    if path is not None:
        if self.isOpen():
            self.close()
        if isinstance(path, str):
            self.f = open(path, mode)
            self._path = path
            self.f.seek(0, 2)
            self.size = self.f.tell()
            self.f.seek(0, 0)
        elif isinstance(path, BaseFile):
            self.f = path
            self.size = path.size
        else:
            raise IOError(""Invalid file parameter"")
    self.setupCrypto(cryptoType, cryptoKey, cryptoCounter)
",if self . isOpen ( ) :,189
"def open_spotify():
    if sys.platform == ""win32"":
        if getwindowtitle() == """":
            path = os.getenv(""APPDATA"") + ""\Spotify\Spotify.exe""
            subprocess.Popen(path)
        else:
            pass
    elif sys.platform == ""linux"":
        if getwindowtitle() == """":
            subprocess.Popen(""spotify"")
        else:
            pass
    elif sys.platform == ""darwin"":
        # I don't have a mac so I don't know if this actually works
        # If it does, please let me know, if it doesn't please fix it :)
        if getwindowtitle() == """":
            subprocess.Popen(""open -a Spotify"")
        else:
            pass
    else:
        pass
","if getwindowtitle ( ) == """" :",198
"def get_search_columns_list(self) -> List[str]:
    ret_lst = list()
    for col_name in self.get_columns_list():
        if not self.is_relation(col_name):
            tmp_prop = self.get_property_first_col(col_name).name
            if (
                (not self.is_pk(tmp_prop))
                and (not self.is_fk(tmp_prop))
                and (not self.is_image(col_name))
                and (not self.is_file(col_name))
            ):
                ret_lst.append(col_name)
        else:
            ret_lst.append(col_name)
    return ret_lst
",if not self . is_relation ( col_name ) :,200
"def get_artist(self, name):
    artist = self.artists.get(name)
    if not artist:
        if self.use_db:
            try:
                artist = q(m.Artist).filter_by(name=name).one()
            except NoResultFound:
                pass
            if artist and self.ram_cache:
                self.add_artist(artist)
    return artist
",if self . use_db :,111
"def _find_glob_metadata(cur_files, metadata):
    md_key = None
    for check_key in metadata.keys():
        matches = 0
        if ""*"" in check_key:
            for fname in cur_files:
                if fnmatch.fnmatch(fname, ""*/%s"" % check_key):
                    matches += 1
        if matches == len(cur_files):
            md_key = check_key
            break
    if md_key:
        return metadata[md_key]
","if ""*"" in check_key :",135
"def extract_copy(
    data: bytearray, mem: bytearray, memstart: int, datastart: int, size: int
):
    for i in range(size):
        if datastart + i < len(data):
            mem[memstart + i] = data[datastart + i]
        else:
            mem[memstart + i] = 0
",if datastart + i < len ( data ) :,89
"def rpc_get_image(self, sender, image_hash):
    self.router.addContact(sender)
    try:
        if len(image_hash) != 20:
            self.log.warning(""Image hash is not 20 characters %s"" % image_hash)
            raise Exception(""Invalid image hash"")
        self.log.info(""serving image %s to %s"" % (image_hash.encode(""hex""), sender))
        with open(self.db.filemap.get_file(image_hash.encode(""hex"")), ""rb"") as filename:
            image = filename.read()
        return [image]
    except Exception:
        self.log.warning(""could not find image %s"" % image_hash[:20].encode(""hex""))
        return None
",if len ( image_hash ) != 20 :,195
"def preprocess_mnist(raw, withlabel, ndim, scale, image_dtype, label_dtype, rgb_format):
    images = raw[""x""]
    if ndim == 2:
        images = images.reshape(-1, 28, 28)
    elif ndim == 3:
        images = images.reshape(-1, 1, 28, 28)
        if rgb_format:
            images = np.broadcast_to(images, (len(images), 3) + images.shape[2:])
    elif ndim != 1:
        raise ValueError(""invalid ndim for MNIST dataset"")
    images = images.astype(image_dtype)
    images *= scale / 255.0
    if withlabel:
        labels = raw[""y""].astype(label_dtype)
        return images, labels
    return images
",if rgb_format :,189
"def get_tokens_unprocessed(self, text):
    for index, token, value in RegexLexer.get_tokens_unprocessed(self, text):
        if token is Name:
            if self.stdlibhighlighting and value in self.stdlib_types:
                token = Keyword.Type
            elif self.c99highlighting and value in self.c99_types:
                token = Keyword.Type
            elif self.platformhighlighting and value in self.linux_types:
                token = Keyword.Type
        yield index, token, value
",elif self . platformhighlighting and value in self . linux_types :,141
"def _match(self, pattern, input_string, context=None):
    for index in find_all(input_string, pattern, **self._kwargs):
        match = Match(
            index,
            index + len(pattern),
            pattern=self,
            input_string=input_string,
            **self._match_kwargs
        )
        if match:
            yield match
",if match :,106
"def https_open(self, req):
    try:
        return self.do_open(do_connection, req)
    except Exception as err_msg:
        try:
            error_msg = str(err_msg.args[0]).split(""] "")[1] + "".""
        except IndexError:
            error_msg = str(err_msg.args[0]) + "".""
        if settings.INIT_TEST == True:
            if settings.VERBOSITY_LEVEL < 2:
                print(settings.FAIL_STATUS)
        else:
            if settings.VERBOSITY_LEVEL < 1:
                print("""")
        print(settings.print_critical_msg(error_msg))
        raise SystemExit()
",if settings . VERBOSITY_LEVEL < 1 :,187
"def recursive_select(tag):
    if self._select_debug:
        print(
            '    Calling select(""%s"") recursively on %s %s'
            % (next_token, tag.name, tag.attrs)
        )
        print(""-"" * 40)
    for i in tag.select(next_token, recursive_candidate_generator):
        if self._select_debug:
            print(""(Recursive select picked up candidate %s %s)"" % (i.name, i.attrs))
        yield i
    if self._select_debug:
        print(""-"" * 40)
",if self . _select_debug :,150
"def detect(self, agent, result):
    # -> True/None
    word = self.checkWords(agent)
    if word:
        result[self.info_type] = dict(name=self.name)
        result[""bot""] = self.bot
        version = self.getVersion(agent, word)
        if version:
            result[self.info_type][""version""] = version
        if self.platform:
            result[""platform""] = {""name"": self.platform, ""version"": version}
        return True
",if self . platform :,134
"def is_display_marc(data):
    if data.startswith(
        ""(Length implementation at offset 22 should hold a digit. Assuming 0)""
    ):
        return True
    try:
        lines = data.split(""\n"")
        leader = lines[0]
        assert re_leader.match(leader)
        for line in lines[1:]:
            if line.startswith(""00""):
                assert re_control.match(line)
            else:
                assert re_data.match(line)
        return True
    except AssertionError:
        return False
","if line . startswith ( ""00"" ) :",152
"def nodejslib(self):
    if not hasattr(self, ""_nodejslib""):
        for lib in self.libs:
            if lib.name == ""node.js stdlib"":
                self._nodejslib = lib
                break
        else:
            self._nodejslib = None
    return self._nodejslib
","if lib . name == ""node.js stdlib"" :",88
"def get(self, key, default=None, type=None):
    for d in self.dicts:
        if key in d:
            if type is not None:
                try:
                    return type(d[key])
                except ValueError:
                    continue
            return d[key]
    return default
",if key in d :,91
"def add_callers(target, source):
    """"""Combine two caller lists in a single list.""""""
    new_callers = {}
    for func, caller in target.items():
        new_callers[func] = caller
    for func, caller in source.items():
        if func in new_callers:
            if isinstance(caller, tuple):
                # format used by cProfile
                new_callers[func] = tuple(
                    [i[0] + i[1] for i in zip(caller, new_callers[func])]
                )
            else:
                # format used by profile
                new_callers[func] += caller
        else:
            new_callers[func] = caller
    return new_callers
","if isinstance ( caller , tuple ) :",195
"def work(src, vsi_dest):
    gdal.Mkdir(vsi_dest, 0o777)
    for item in src.iterdir():
        item_vsi_dest = os.path.join(vsi_dest, item.name)
        if item.is_dir():
            work(item, item_vsi_dest)
        else:
            VsiFileSystem.copy_to(str(item), item_vsi_dest)
",if item . is_dir ( ) :,115
"def __getitem__(self, key):
    if isinstance(key, raw_types.Qid):
        return self._operation_touching(key)
    elif isinstance(key, Iterable):
        qubits_to_keep = frozenset(key)
        ops_to_keep = tuple(
            op
            for op in self.operations
            if not qubits_to_keep.isdisjoint(frozenset(op.qubits))
        )
        return Moment(ops_to_keep)
",if not qubits_to_keep . isdisjoint ( frozenset ( op . qubits ) ),124
"def mlt_version_is_greater_correct(test_version):
    runtime_ver = mlt_version.split(""."")
    test_ver = test_version.split(""."")
    if runtime_ver[0] > test_ver[0]:
        return True
    elif runtime_ver[0] == test_ver[0]:
        if runtime_ver[1] > test_ver[1]:
            return True
        elif runtime_ver[1] == test_ver[1]:
            if runtime_ver[2] > test_ver[2]:
                return True
    return False
",if runtime_ver [ 1 ] > test_ver [ 1 ] :,148
"def populate(self, item):
    path = self.getItemPath(item)
    for name in sorted(os.listdir(path)):
        if name[0] == ""."":
            continue
        pathname = os.path.join(path, name)
        if os.path.isdir(pathname):
            item.addChild(name, True)
        elif name.lower().endswith("".target"") and os.path.isfile(pathname):
            item.addChild(name, False)
","if name [ 0 ] == ""."" :",122
"def runTests(self):
    """"""Run tests""""""
    # fire plugin hook
    runner = self._makeRunner()
    try:
        self.result = runner.run(self.test)
    except Exception as e:
        log.exception(""Internal Error"")
        sys.stderr.write(""Internal Error: runTests aborted: %s\n"" % (e))
        if self.exit:
            sys.exit(1)
    if self.exit:
        sys.exit(not self.result.wasSuccessful())
",if self . exit :,129
"def __setitem__(self, key, value):
    """"""Like :meth:`set` but also supports index/slice based setting.""""""
    if isinstance(key, (slice, int)):
        if isinstance(key, int):
            value = [value]
        value = [
            (_unicodify_header_value(k), _unicodify_header_value(v)) for (k, v) in value
        ]
        for (_, v) in value:
            self._validate_value(v)
        if isinstance(key, int):
            self._list[key] = value[0]
        else:
            self._list[key] = value
    else:
        self.set(key, value)
","if isinstance ( key , int ) :",181
"def toggle_fullscreen_hide_tabbar(self):
    if self.is_fullscreen():
        if self.settings.general.get_boolean(""fullscreen-hide-tabbar""):
            if self.guake and self.guake.notebook_manager:
                self.guake.notebook_manager.set_notebooks_tabbar_visible(False)
    else:
        if self.guake and self.guake.notebook_manager:
            v = self.settings.general.get_boolean(""window-tabbar"")
            self.guake.notebook_manager.set_notebooks_tabbar_visible(v)
","if self . settings . general . get_boolean ( ""fullscreen-hide-tabbar"" ) :",159
"def clear_doc(self, docname: str) -> None:
    for sChild in self._children:
        sChild.clear_doc(docname)
        if sChild.declaration and sChild.docname == docname:
            sChild.declaration = None
            sChild.docname = None
            sChild.line = None
            if sChild.siblingAbove is not None:
                sChild.siblingAbove.siblingBelow = sChild.siblingBelow
            if sChild.siblingBelow is not None:
                sChild.siblingBelow.siblingAbove = sChild.siblingAbove
            sChild.siblingAbove = None
            sChild.siblingBelow = None
",if sChild . declaration and sChild . docname == docname :,189
"def visit_hierarchichttprequest(self, request):
    files = []
    body_file = request.config.get(""body-file"")
    if body_file:
        files.append(body_file)
    uploads = request.config.get(""upload-files"", [])
    files.extend([x[""path""] for x in uploads if not has_variable_pattern(x[""path""])])
    if ""jsr223"" in request.config:
        jsrs = request.config.get(""jsr223"")
        if isinstance(jsrs, dict):
            jsrs = [jsrs]
        for jsr in jsrs:
            if ""script-file"" in jsr:
                files.append(jsr.get(""script-file""))
    return files
","if isinstance ( jsrs , dict ) :",192
"def find_commands(management_dir):
    # Modified version of function from django/core/management/__init__.py.
    command_dir = os.path.join(management_dir, ""commands"")
    commands = []
    try:
        for f in os.listdir(command_dir):
            if f.startswith(""_""):
                continue
            elif f.endswith("".py"") and f[:-3] not in commands:
                commands.append(f[:-3])
            elif f.endswith("".pyc"") and f[:-4] not in commands:
                commands.append(f[:-4])
    except OSError:
        pass
    return commands
","elif f . endswith ( "".pyc"" ) and f [ : - 4 ] not in commands :",164
"def show_panel(panel_id):
    # Iterate positions to find where panel is and bring it to front.
    for position in _positions_names:
        pos_panel_ids = _get_position_panels(position)
        if len(pos_panel_ids) == 0:
            continue
        if len(pos_panel_ids) == 1:
            continue
        panel_widget = _get_panels_widgets_dict(gui.editor_window)[panel_id]
        notebook = _position_notebooks[position]
        for i in range(0, notebook.get_n_pages()):
            notebook_page = notebook.get_nth_page(i)
            if notebook_page == panel_widget:
                notebook.set_current_page(i)
",if len ( pos_panel_ids ) == 1 :,197
"def is_cwl_record(d):
    """"""Check if an input is a CWL record, from any level of nesting.""""""
    if isinstance(d, dict):
        if d.get(""type"") == ""record"":
            return d
        else:
            recs = list(
                filter(lambda x: x is not None, [is_cwl_record(v) for v in d.values()])
            )
            return recs[0] if recs else None
    else:
        return None
","if d . get ( ""type"" ) == ""record"" :",131
"def _flags_data_(self, main_mod, model_paths, flags_dest):
    try:
        sys_path, mod_path = python_util.find_module(main_mod, model_paths)
    except ImportError as e:
        if os.getenv(""NO_WARN_FLAGS_IMPORT"") != ""1"":
            self.log.warning(""cannot import flags from %s: %s"", main_mod, e)
        return {}
    else:
        package = self._main_spec_package(main_mod)
        return self._flags_data_for_path(mod_path, package, sys_path, flags_dest)
","if os . getenv ( ""NO_WARN_FLAGS_IMPORT"" ) != ""1"" :",159
"def __str__(self):
    messages = [self.__class__.__name__, ""(""]
    annotation = self.annotation
    messages.append(self.annotation.surrounds_attribute or """")
    if annotation.tag_attributes:
        if annotation.surrounds_attribute:
            messages.append("";"")
        for (f, ta, ea) in self.tag_data:
            messages += [ea, ': attribute ""', ta, '""']
    start, end = annotation.start_index, annotation.end_index
    messages.append("", template[%s:%s])"" % (start, end))
    return """".join(map(str, messages))
",if annotation . surrounds_attribute :,153
"def _on_view_count_change(self, *args):
    with self.output:
        logger.debug(""views: %d"", self.image.view_count)
        if self._dirty and self.image.view_count > 0:
            try:
                logger.debug(""was dirty, and needs an update"")
                self.update()
            finally:
                self._dirty = False
",if self . _dirty and self . image . view_count > 0 :,109
"def network_state(self, device):
    cmd = [""tc"", ""qdisc"", ""show"", ""dev"", device]
    try:
        output = self.host_exec.run(cmd)
        # sloppy but good enough for now
        if "" delay "" in output:
            return NetworkState.SLOW
        if "" loss "" in output:
            return NetworkState.FLAKY
        if "" duplicate "" in output:
            return NetworkState.DUPLICATE
        return NetworkState.NORMAL
    except Exception:
        return NetworkState.UNKNOWN
","if "" delay "" in output :",138
"def _remove(self, item):
    """"""Internal removal of an item""""""
    # Manage siblings when items are deleted
    for sibling in self.lines[self.lines.index(item) + 1 :]:
        if isinstance(sibling, CronItem):
            env = sibling.env
            sibling.env = item.env
            sibling.env.update(env)
            sibling.env.job = sibling
            break
        elif sibling == """":
            self.lines.remove(sibling)
        else:
            break
    self.crons.remove(item)
    self.lines.remove(item)
    return 1
","if isinstance ( sibling , CronItem ) :",162
"def _get_transformations(self, current_text, indices_to_modify):
    transformed_texts = []
    words = current_text.words
    for idx in indices_to_modify:
        word = words[idx]
        swap_idxs = list(set(range(len(words))) - {idx})
        if swap_idxs:
            swap_idx = random.choice(swap_idxs)
            swapped_text = current_text.replace_word_at_index(
                idx, words[swap_idx]
            ).replace_word_at_index(swap_idx, word)
            transformed_texts.append(swapped_text)
    return transformed_texts
",if swap_idxs :,175
"def _unlock_restarted_vms(self, pool_name):
    result = []
    for vm in await self.middleware.call(""vm.query"", [(""autostart"", ""="", True)]):
        for device in vm[""devices""]:
            if device[""dtype""] not in (""DISK"", ""RAW""):
                continue
            path = device[""attributes""].get(""path"")
            if not path:
                continue
            if path.startswith(f""/dev/zvol/{pool_name}/"") or path.startswith(
                f""/mnt/{pool_name}/""
            ):
                result.append(vm)
                break
    return result
",if not path :,172
"def parse_literal_object(node):
    value = 0
    unit = get_default_weight_unit()
    for field in node.fields:
        if field.name.value == ""value"":
            try:
                value = decimal.Decimal(field.value.value)
            except decimal.DecimalException:
                raise GraphQLError(f""Unsupported value: {field.value.value}"")
        if field.name.value == ""unit"":
            unit = field.value.value
    return Weight(**{unit: value})
","if field . name . value == ""unit"" :",134
"def _extract_level(self):
    """"""Extract level and component if available (lazy).""""""
    if self._level is None:
        split_tokens = self.split_tokens
        if not split_tokens:
            self._level = False
            self._component = False
            return
        x = (
            self.log_levels.index(split_tokens[1])
            if split_tokens[1] in self.log_levels
            else None
        )
        if x is not None:
            self._level = split_tokens[1]
            self._component = split_tokens[2]
        else:
            self._level = False
            self._component = False
",if split_tokens [ 1 ] in self . log_levels,185
"def _average_import_time(n: int, module: Text) -> float:
    total = 0
    for _ in range(n):
        lines = subprocess.getoutput(
            f'{sys.executable} -X importtime -c ""import {module}""'
        ).splitlines()
        parts = lines[-1].split(""|"")
        if parts[-1].strip() != module:
            raise Exception(f""Import time not found for {module}."")
        total += int(parts[1].strip()) / 1000000
    return total / n
",if parts [ - 1 ] . strip ( ) != module :,133
"def send_preamble(self):
    """"""Transmit version/status/date/server, via self._write()""""""
    if self.origin_server:
        if self.client_is_modern():
            self._write(""HTTP/%s %s\r\n"" % (self.http_version, self.status))
            if not self.headers.has_key(""Date""):
                self._write(""Date: %s\r\n"" % time.asctime(time.gmtime(time.time())))
            if self.server_software and not self.headers.has_key(""Server""):
                self._write(""Server: %s\r\n"" % self.server_software)
    else:
        self._write(""Status: %s\r\n"" % self.status)
","if self . server_software and not self . headers . has_key ( ""Server"" ) :",199
"def test_source_address(self):
    for addr, is_ipv6 in VALID_SOURCE_ADDRESSES:
        if is_ipv6 and not HAS_IPV6_AND_DNS:
            warnings.warn(""No IPv6 support: skipping."", NoIPv6Warning)
            continue
        with HTTPConnectionPool(
            self.host, self.port, source_address=addr, retries=False
        ) as pool:
            r = pool.request(""GET"", ""/source_address"")
            assert r.data == b(addr[0])
",if is_ipv6 and not HAS_IPV6_AND_DNS :,140
"def _run_commands(self, tool, commands, dry_run=False):
    if dry_run:
        self._dry_run_commands(tool, commands)
        return
    for command in commands:
        try:
            with original_ld_library_path():
                self.subprocess_utils.run(command, capture_output=True, check=True)
        except OSError as ex:
            if ex.errno == errno.ENOENT:
                raise ValueError(self._TOOL_NOT_FOUND_MESSAGE % tool)
            raise ex
    self._write_success_message(tool)
",if ex . errno == errno . ENOENT :,154
"def test_float_overflow(self):
    import sys
    big_int = int(sys.float_info.max) * 2
    for t in float_types + [c_longdouble]:
        self.assertRaises(OverflowError, t, big_int)
        if hasattr(t, ""__ctype_be__""):
            self.assertRaises(OverflowError, t.__ctype_be__, big_int)
        if hasattr(t, ""__ctype_le__""):
            self.assertRaises(OverflowError, t.__ctype_le__, big_int)
","if hasattr ( t , ""__ctype_be__"" ) :",131
"def init_weights(self):
    for n, p in self.named_parameters():
        if ""bias"" in n:
            torch.nn.init.zeros_(p)
        elif ""fc"" in n:
            torch.nn.init.xavier_uniform_(p)
","if ""bias"" in n :",71
"def _compute_dependencies(self):
    """"""Gather the lists of dependencies and adds to all_parts.""""""
    for part in self.all_parts:
        dep_names = self.after_requests.get(part.name, [])
        for dep_name in dep_names:
            dep = self.get_part(dep_name)
            if not dep:
                raise errors.SnapcraftAfterPartMissingError(part.name, dep_name)
            part.deps.append(dep)
",if not dep :,127
"def _delete_object(step):
    try:
        api = kubernetes.client.CustomObjectsApi()
        api.delete_namespaced_custom_object(
            group=""zalando.org"",
            version=""v1"",
            plural=""kopfexamples"",
            namespace=""default"",
            name=f""kopf-example-{step}"",
            body={},
        )
    except kubernetes.client.rest.ApiException as e:
        if e.status in [404]:
            pass
        else:
            raise
",if e . status in [ 404 ] :,142
"def _lookup(self, key, dicts=None, filters=()):
    if dicts is None:
        dicts = self.dicts
    key_len = len(key)
    if key_len > self.longest_key:
        return None
    for d in dicts:
        if not d.enabled:
            continue
        if key_len > d.longest_key:
            continue
        value = d.get(key)
        if value:
            for f in filters:
                if f(key, value):
                    return None
            return value
",if key_len > d . longest_key :,150
"def fork_with_monitor(receiver: Receiver, func, *args, **kwargs):
    current_actor = self()
    send(ForkWithMonitor(current_actor, func, args, kwargs), receiver)
    while True:
        message = recv(current_actor)
        if isinstance(message, ForkResponse):
            return message.new_actor
        else:
            send(message, current_actor)
    return
","if isinstance ( message , ForkResponse ) :",106
"def read(self, size=-1):
    if self._offset or (size > -1):
        # return empty string to indicate EOF if we are offset past the end of the file
        # else boto will throw an error at us
        if self._offset >= self._key.size:
            return """"
        if size > -1:
            sizeStr = str(self._offset + size - 1)  # range header is inclusive
        else:
            sizeStr = """"
        hdrs = {""Range"": ""bytes=%d-%s"" % (self._offset, sizeStr)}
    else:
        hdrs = {}
    buf = self._key.get_contents_as_string(headers=hdrs)
    self._offset += len(buf)
    return buf
",if size > - 1 :,191
"def operations(self):
    # Search for operations
    registered_operations = {}
    for fn in hooks.get_hooks(""register_image_operations""):
        registered_operations.update(dict(fn()))
    # Build list of operation objects
    operations = []
    for op_spec in self.spec.split(""|""):
        op_spec_parts = op_spec.split(""-"")
        if op_spec_parts[0] not in registered_operations:
            raise InvalidFilterSpecError(
                ""Unrecognised operation: %s"" % op_spec_parts[0]
            )
        op_class = registered_operations[op_spec_parts[0]]
        operations.append(op_class(*op_spec_parts))
    return operations
",if op_spec_parts [ 0 ] not in registered_operations :,181
"def find_widget(self, pos):
    for widget in self.subwidgets[::-1]:
        if widget.visible:
            r = widget.rect
            if r.collidepoint(pos):
                return widget.find_widget(subtract(pos, r.topleft))
    return self
",if widget . visible :,77
"def _get_body(self):
    if self._bodytree is None:
        bodytxt = self._message.accumulate_body()
        if bodytxt:
            att = settings.get_theming_attribute(""thread"", ""body"")
            att_focus = settings.get_theming_attribute(""thread"", ""body_focus"")
            self._bodytree = TextlinesList(bodytxt, att, att_focus)
    return self._bodytree
",if bodytxt :,114
"def config_mode(self, config_command=""conf t"", pattern=""""):
    output = """"
    if not self.check_config_mode():
        output = self.send_command_timing(
            config_command, strip_command=False, strip_prompt=False
        )
        if ""to enter configuration mode anyway"" in output:
            output += self.send_command_timing(
                ""YES"", strip_command=False, strip_prompt=False
            )
        if not self.check_config_mode():
            raise ValueError(""Failed to enter configuration mode"")
    return output
",if not self . check_config_mode ( ) :,152
"def is_enabled(self):
    try:
        cmd = subprocess.Popen(
            ""netsh advfirewall show currentprofile"", stdout=subprocess.PIPE
        )
        out = cmd.stdout.readlines()
        for l in out:
            if l.startswith(""State""):
                state = l.split()[-1].strip()
        return state == ""ON""
    except:
        return None
","if l . startswith ( ""State"" ) :",107
"def __rpc_devices(self, *args):
    data_to_send = {}
    for device in self.__connected_devices:
        if self.__connected_devices[device][""connector""] is not None:
            data_to_send[device] = self.__connected_devices[device][
                ""connector""
            ].get_name()
    return {""code"": 200, ""resp"": data_to_send}
","if self . __connected_devices [ device ] [ ""connector"" ] is not None :",106
"def _mock_manager_nfx(self, *args, **kwargs):
    if args:
        if args[0].tag == ""command"":
            raise RpcError()
        elif args[0].tag == ""get-software-information"" and args[0].find(""./*"") is None:
            return True
        else:
            return self._read_file(""sw_info_nfx_"" + args[0].tag + "".xml"")
","if args [ 0 ] . tag == ""command"" :",112
"def empty_logs(self, logs=None):
    if self.quick_log:
        self.quick_log = []
    else:
        if is_main_thread():
            self.logs = []
        else:
            if logs and self.thread_logs.get(current_thread_id()):
                del self.thread_logs[current_thread_id()]
",if logs and self . thread_logs . get ( current_thread_id ( ) ) :,98
"def read_cb(dir_path):
    df_dict = dict()
    for fold in [""train"", ""val"", ""test""]:
        columns = [""premise"", ""hypothesis""]
        if fold != ""test"":
            columns.append(""label"")
        jsonl_path = os.path.join(dir_path, ""{}.jsonl"".format(fold))
        df = read_jsonl_superglue(jsonl_path)
        df = df[columns]
        df_dict[fold] = df
    return df_dict, None
","if fold != ""test"" :",134
"def _forward_main_responses(self):
    while self._should_keep_going():
        line = self._proc.stdout.readline()
        if self._main_backend_is_fresh and self._looks_like_echo(line):
            # In the beginning the backend may echo commands sent to it (perhaps this echo-avoiding trick
            # takes time). Don't forward those lines.
            continue
        if not line:
            break
        with self._response_lock:
            sys.stdout.write(line)
            sys.stdout.flush()
            self._main_backend_is_fresh = False
",if self . _main_backend_is_fresh and self . _looks_like_echo ( line ) :,161
"def _update_server_version(self):
    """"""Decode the Transmission version string, if available.""""""
    if self.server_version is None:
        version_major = 1
        version_minor = 30
        version_changeset = 0
        version_parser = re.compile(""(\d).(\d+) \((\d+)\)"")
        if hasattr(self.session, ""version""):
            match = version_parser.match(self.session.version)
            if match:
                version_major = int(match.group(1))
                version_minor = int(match.group(2))
                version_changeset = match.group(3)
        self.server_version = (version_major, version_minor, version_changeset)
",if match :,190
"def _check_type(T, allowed):
    if T not in allowed:
        if len(allowed) == 1:
            allowed.add(T)
        else:
            types = "", "".join([t.__name__ for t in allowed] + [T.__name__])
            raise TypeError(""unsupported mixed types: %s"" % types)
",if len ( allowed ) == 1 :,87
"def split_named_range(range_string):
    """"""Separate a named range into its component parts""""""
    for range_string in SPLIT_NAMED_RANGE_RE.split(range_string)[
        1::2
    ]:  # Skip first and from there every second item
        match = NAMED_RANGE_RE.match(range_string)
        if match is None:
            raise NamedRangeException('Invalid named range string: ""%s""' % range_string)
        else:
            match = match.groupdict()
            sheet_name = match[""quoted""] or match[""notquoted""]
            xlrange = match[""range""]
            sheet_name = sheet_name.replace(""''"", ""'"")  # Unescape '
            yield sheet_name, xlrange
",if match is None :,191
"def clean(self):
    to_del = []
    for i, file_ in enumerate(self.files):
        try:
            os.remove(file_)
            to_del.append(i)
        except Exception:
            if not os.path.isfile(file_):
                to_del.append(i)
    for i in to_del[::-1]:
        del self.files[i]
",if not os . path . isfile ( file_ ) :,108
"def lazy_init(self):
    f = open(self.filename)
    self.base = {}
    while 1:
        l = f.readline()
        if not l:
            break
        l = l.strip().split("","")
        if len(l) != 3:
            continue
        c, lat, long = l
        self.base[c] = (float(long), float(lat))
    f.close()
",if not l :,113
"def onto_evo_target(self):
    if self._onto_evo_target is None:
        self._get_onto_evo_target()
    if self._onto_evo_target_qobj is None:
        if isinstance(self._onto_evo_target, Qobj):
            self._onto_evo_target_qobj = self._onto_evo_target
        else:
            rev_dims = [self.sys_dims[1], self.sys_dims[0]]
            self._onto_evo_target_qobj = Qobj(self._onto_evo_target, dims=rev_dims)
    return self._onto_evo_target_qobj
","if isinstance ( self . _onto_evo_target , Qobj ) :",176
"def _dnsname_to_pat(dn):
    pats = []
    for frag in dn.split(r"".""):
        if frag == ""*"":
            # When '*' is a fragment by itself, it matches a non-empty dotless
            # fragment.
            pats.append(""[^.]+"")
        else:
            # Otherwise, '*' matches any dotless fragment.
            frag = re.escape(frag)
            pats.append(frag.replace(r""\*"", ""[^.]*""))
    return re.compile(r""\A"" + r""\."".join(pats) + r""\Z"", re.IGNORECASE)
","if frag == ""*"" :",155
"def update(id):
    """"""Update a post if the current user is the author.""""""
    post = get_post(id)
    if request.method == ""POST"":
        title = request.form[""title""]
        body = request.form[""body""]
        error = None
        if not title:
            error = ""Title is required.""
        if error is not None:
            flash(error)
        else:
            post.title = title
            post.body = body
            db.session.commit()
            return redirect(url_for(""blog.index""))
    return render_template(""blog/update.html"", post=post)
",if not title :,168
"def __iter__(self):
    for token in base.Filter.__iter__(self):
        if token[""type""] in (""StartTag"", ""EmptyTag""):
            attrs = OrderedDict()
            for name, value in sorted(token[""data""].items(), key=_attr_key):
                attrs[name] = value
            token[""data""] = attrs
        yield token
","if token [ ""type"" ] in ( ""StartTag"" , ""EmptyTag"" ) :",94
"def get_polymorphic_model(data):
    for model in itervalues(models):
        polymorphic = model.opts.polymorphic
        if polymorphic:
            polymorphic_key = polymorphic
            if isinstance(polymorphic_key, bool):
                polymorphic_key = ""type""
            if data.get(polymorphic_key) == model.__name__:
                return model
    raise ImproperlyConfigured(u""No model found for data: {!r}"".format(data))
",if data . get ( polymorphic_key ) == model . __name__ :,133
"def _setup_tag(self, tag):
    # keeping mutual refs
    tag.py_obj = self
    self.riot_tag = tag
    # making the event system call self's methods:
    handlers = {}
    for ev in lifecycle_ev:
        f = getattr(self, ev.replace(""-"", ""_""))
        if f:
            # this.on('mount', function() {...}):
            # whats nicer?
            tag.on(ev, f)
",if f :,124
"def selection_only(self):
    selection_only = False
    sel = self.sel()
    if (self.context == ""selection"" or self.context == ""both"") and len(sel):
        # if multiple lines, always true
        if len(sel) > 1:
            selection_only = True
        # check threshold
        elif self.threshold and not sel[0].empty():
            text = self.view.substr(sel[0])
            match = re.search(self.threshold, text)
            if match:
                selection_only = True
        # no valid selection
        else:
            selection_only = False
    return selection_only
",if len ( sel ) > 1 :,174
"def find_torrents_to_fetch(torrent_ids):
    to_fetch = []
    t = time()
    for torrent_id in torrent_ids:
        torrent = self.torrents[torrent_id]
        if t - torrent[0] > self.cache_time:
            to_fetch.append(torrent_id)
        else:
            # We need to check if a key is expired
            for key in keys:
                if t - self.cache_times[torrent_id].get(key, 0.0) > self.cache_time:
                    to_fetch.append(torrent_id)
                    break
    return to_fetch
",if t - torrent [ 0 ] > self . cache_time :,180
"def filter(callbackfn):
    array = this.to_object()
    arr_len = array.get(""length"").to_uint32()
    if not callbackfn.is_callable():
        raise this.MakeError(""TypeError"", ""callbackfn must be a function"")
    T = arguments[1]
    res = []
    k = 0
    while k < arr_len:
        if array.has_property(str(k)):
            kValue = array.get(str(k))
            if callbackfn.call(T, (kValue, this.Js(k), array)).to_boolean().value:
                res.append(kValue)
        k += 1
    return res  # converted to js array automatically
","if callbackfn . call ( T , ( kValue , this . Js ( k ) , array ) ) . to_boolean ( ) . value :",179
"def generate_py_upgrades(data):
    """"""Generate the list of upgrades in upgrades.py.""""""
    print("" upgrades.py "".center(60, ""-""))
    print(""class Upgrades(enum.IntEnum):"")
    print('  """"""The list of upgrades, as returned from RequestData.""""""')
    for upgrade in sorted(data.upgrades, key=lambda a: a.name):
        if upgrade.name and upgrade.upgrade_id in static_data.UPGRADES:
            print(""  %s = %s"" % (upgrade.name, upgrade.upgrade_id))
    print(""\n"")
",if upgrade . name and upgrade . upgrade_id in static_data . UPGRADES :,154
"def get_first_n(l, n, reverse=False):
    cur_n = 0
    res = []
    for si in reversed(l) if reverse else l:
        if trade_exchange.is_stock_tradable(stock_id=si, trade_date=trade_date):
            res.append(si)
            cur_n += 1
            if cur_n >= n:
                break
    return res[::-1] if reverse else res
",if cur_n >= n :,120
"def _fill_cache(self):
    for task in linux_pslist.linux_pslist(self._config).calculate():
        for filp, fd in task.lsof():
            filepath = linux_common.get_path(task, filp)
            if type(filepath) == str and filepath.find(""socket:["") != -1:
                to_add = filp.dentry.d_inode.i_ino.v()
                self.fd_cache[to_add] = [task, filp, fd, filepath]
","if type ( filepath ) == str and filepath . find ( ""socket:["" ) != - 1 :",137
"def is_ArAX_implicit(ii):  # allows one implicit fixed reg
    a, implicit_fixed = 0, 0
    for op in _gen_opnds(ii):
        if op_luf_start(op, ""ArAX""):
            a += 1
        elif op_reg(op) and op_implicit_specific_reg(op):
            implicit_fixed += 1
        else:
            return False
    return a == 1 and implicit_fixed <= 1
",elif op_reg ( op ) and op_implicit_specific_reg ( op ) :,120
"def auto_resize(self, name: str) -> None:
    """"""recompute widget width based on max length of all of the values""""""
    widget = self.find_widget(name)
    for column in range(len(widget._columns) - 1):
        sizes = [len(x[0][column]) + 1 for x in widget.options]
        if widget._titles:
            sizes.append(len(widget._titles[column]) + 1)
        widget._columns[column] = max(sizes)
",if widget . _titles :,124
"def dns_set_secondary_nameserver():
    from dns_update import set_secondary_dns
    try:
        return set_secondary_dns(
            [
                ns.strip()
                for ns in re.split(r""[, ]+"", request.form.get(""hostnames"") or """")
                if ns.strip() != """"
            ],
            env,
        )
    except ValueError as e:
        return (str(e), 400)
","if ns . strip ( ) != """"",122
"def assert_inputs(inputs, can_be_used=True):
    # Until we make the dataset private, _different_user() can use it:
    with self._different_user_and_history() as other_history_id:
        response = self._run(""cat1"", other_history_id, inputs)
        if can_be_used:
            assert response.status_code == 200
        else:
            self._assert_dataset_permission_denied_response(response)
",if can_be_used :,120
"def _handle_start(self, tag, attrib):
    if ""translatable"" in attrib:
        if attrib[attrib.index(""translatable"") + 1] == ""yes"":
            self._translate = True
            if ""comments"" in attrib:
                self._comments.append(attrib[attrib.index(""comments"") + 1])
","if attrib [ attrib . index ( ""translatable"" ) + 1 ] == ""yes"" :",83
"def get_command(cls):
    ifconfig_cmd = ""ip""
    for path in [""/sbin"", ""/usr/sbin"", ""/bin"", ""/usr/bin""]:
        if os.path.exists(os.path.join(path, ifconfig_cmd)):
            ifconfig_cmd = os.path.join(path, ifconfig_cmd)
            break
    ifconfig_cmd = ifconfig_cmd + "" address show""
    return ifconfig_cmd
","ifconfig_cmd = os . path . join ( path , ifconfig_cmd )",109
"def render(self):
    """"""What to show when printed.""""""
    viz = """"
    for y in range(self.grid.height):
        for x in range(self.grid.width):
            c = self.grid[y][x]
            if c is None:
                viz += "" ""
            else:
                viz += self.converter(c)
        viz += ""\n""
    return viz
",if c is None :,110
"def _sorted_layers(self, structure, top_layer_id):
    """"""Return the image layers sorted""""""
    sorted_layers = []
    next_layer = top_layer_id
    while next_layer:
        sorted_layers.append(next_layer)
        if ""json"" not in structure[""repolayers""][next_layer]:  # v2
            break
        if ""parent"" not in structure[""repolayers""][next_layer][""json""]:
            break
        next_layer = structure[""repolayers""][next_layer][""json""][""parent""]
        if not next_layer:
            break
    return sorted_layers
","if ""json"" not in structure [ ""repolayers"" ] [ next_layer ] :",162
"def check_sync(self):
    login_failures = get_login_failures(datetime.now(), catmsgs())
    if login_failures:
        return Alert(
            SSHLoginFailuresAlertClass,
            {
                ""count"": len(login_failures),
                ""failures"": """".join(
                    login_failures
                    if len(login_failures) <= 5
                    else login_failures[:2]
                    + [f""... {len(login_failures) - 4} more ...\n""]
                    + login_failures[-2:]
                ),
            },
        )
",if len ( login_failures ) <= 5,172
"def on_user_auth_login_success(sender, user, request, **kwargs):
    if settings.USER_LOGIN_SINGLE_MACHINE_ENABLED:
        user_id = ""single_machine_login_"" + str(user.id)
        session_key = cache.get(user_id)
        if session_key and session_key != request.session.session_key:
            session = import_module(settings.SESSION_ENGINE).SessionStore(session_key)
            session.delete()
        cache.set(user_id, request.session.session_key, None)
",if session_key and session_key != request . session . session_key :,146
"def slots_for_entities(self, entities):
    if self.store_entities_as_slots:
        slot_events = []
        for s in self.slots:
            if s.auto_fill:
                matching_entities = [
                    e[""value""] for e in entities if e[""entity""] == s.name
                ]
                if matching_entities:
                    if s.type_name == ""list"":
                        slot_events.append(SlotSet(s.name, matching_entities))
                    else:
                        slot_events.append(SlotSet(s.name, matching_entities[-1]))
        return slot_events
    else:
        return []
",if s . auto_fill :,193
"def get(self, id):
    obj = self.klass.objects.get(id=id)
    if hasattr(obj, ""sharing""):
        if group_user_permission(obj):
            return render_template(
                ""{}/single.html"".format(self.klass.__name__.lower()), obj=obj
            )
        abort(403)
    else:
        return render_template(
            ""{}/single.html"".format(self.klass.__name__.lower()), obj=obj
        )
    return request.referrer
",if group_user_permission ( obj ) :,137
"def __call__(self, module, *x):
    """"""Grab the instantiated layer and evaluate it.""""""
    operation = getattr(module, self.name)
    try:
        if self.func:
            return self.func(operation, *x)
        return operation(*x)
    except:
        logger.error(""Failed to apply layer: %s"", self.name)
        for i, X in enumerate(x):
            logger.error(""  Input shape #%d: %s"", i + 1, list(X.size()))
        raise
",if self . func :,135
"def req(s, poll, msg, expect):
    do_req = True
    xid = None
    while True:
        # get transaction id
        if do_req:
            xid = s.put(msg)[""xid""]
        # wait for response
        events = poll.poll(2)
        for (fd, event) in events:
            response = s.get()
            if response[""xid""] != xid:
                do_req = False
                continue
            if response[""options""][""message_type""] != expect:
                raise Exception(""DHCP protocol error"")
            return response
        do_req = True
",if do_req :,174
"def _state_old_c_params(self, token):
    self._saved_tokens.append(token)
    if token == "";"":
        self._saved_tokens = []
        self._state = self._state_dec_to_imp
    elif token == ""{"":
        if len(self._saved_tokens) == 2:
            self._saved_tokens = []
            self._state_dec_to_imp(token)
            return
        self._state = self._state_global
        for tkn in self._saved_tokens:
            self._state(tkn)
    elif token == ""("":
        self._state = self._state_global
        for tkn in self._saved_tokens:
            self._state(tkn)
",if len ( self . _saved_tokens ) == 2 :,186
"def assert_tensors_equal(sess, t1, t2, n):
    """"""Compute tensors `n` times and ensure that they are equal.""""""
    for _ in range(n):
        v1, v2 = sess.run([t1, t2])
        if v1.shape != v2.shape:
            return False
        if not np.all(v1 == v2):
            return False
    return True
",if not np . all ( v1 == v2 ) :,107
"def http_error_302(self, url, fp, errcode, errmsg, headers, data=None):
    """"""Error 302 -- relocated (temporarily).""""""
    self.tries += 1
    if self.maxtries and self.tries >= self.maxtries:
        if hasattr(self, ""http_error_500""):
            meth = self.http_error_500
        else:
            meth = self.http_error_default
        self.tries = 0
        return meth(url, fp, 500, ""Internal Server Error: Redirect Recursion"", headers)
    result = self.redirect_internal(url, fp, errcode, errmsg, headers, data)
    self.tries = 0
    return result
","if hasattr ( self , ""http_error_500"" ) :",172
"def get_satellite_list(self, daemon_type=""""):
    res = {}
    for t in [""arbiter"", ""scheduler"", ""poller"", ""reactionner"", ""receiver"", ""broker""]:
        if daemon_type and daemon_type != t:
            continue
        satellite_list = []
        res[t] = satellite_list
        daemon_name_attr = t + ""_name""
        daemons = self.app.get_daemons(t)
        for dae in daemons:
            if hasattr(dae, daemon_name_attr):
                satellite_list.append(getattr(dae, daemon_name_attr))
    return res
","if hasattr ( dae , daemon_name_attr ) :",175
"def check(data_dir, decrypter, read_only=False):
    fname = os.path.join(data_dir, DIGEST_NAME)
    if os.path.exists(fname):
        if decrypter is None:
            return False
        f = open(fname, ""rb"")
        s = f.read()
        f.close()
        return decrypter.decrypt(s) == MAGIC_STRING
    else:
        if decrypter is not None:
            if read_only:
                return False
            else:
                s = decrypter.encrypt(MAGIC_STRING)
                f = open(fname, ""wb"")
                f.write(s)
                f.close()
        return True
",if decrypter is None :,198
"def logic():
    while 1:
        yield clock.posedge, reset.negedge
        if reset == ACTIVE_LOW:
            count.next = 0
        else:
            if enable:
                count.next = f1(n)
",if reset == ACTIVE_LOW :,69
"def get_project_translation(request, project=None, component=None, lang=None):
    """"""Return project, component, translation tuple for given parameters.""""""
    if lang and component:
        # Language defined? We can get all
        translation = get_translation(request, project, component, lang)
        component = translation.component
        project = component.project
    else:
        translation = None
        if component:
            # Component defined?
            component = get_component(request, project, component)
            project = component.project
        elif project:
            # Only project defined?
            project = get_project(request, project)
    # Return tuple
    return project or None, component or None, translation or None
",elif project :,184
"def run(self, sql, encoding=None):
    stream = lexer.tokenize(sql, encoding)
    # Process token stream
    for filter_ in self.preprocess:
        stream = filter_.process(stream)
    stream = StatementSplitter().process(stream)
    # Output: Stream processed Statements
    for stmt in stream:
        if self._grouping:
            stmt = grouping.group(stmt)
        for filter_ in self.stmtprocess:
            filter_.process(stmt)
        for filter_ in self.postprocess:
            stmt = filter_.process(stmt)
        yield stmt
",if self . _grouping :,148
"def get_word_parens_range(self, offset, opening=""("", closing="")""):
    end = self._find_word_end(offset)
    start_parens = self.code.index(opening, end)
    index = start_parens
    open_count = 0
    while index < len(self.code):
        if self.code[index] == opening:
            open_count += 1
        if self.code[index] == closing:
            open_count -= 1
        if open_count == 0:
            return (start_parens, index + 1)
        index += 1
    return (start_parens, index)
",if self . code [ index ] == closing :,160
"def _get_inherited_env_vars(self):
    env_vars = os.environ.copy()
    for var_name in ENV_VARS_BLACKLIST:
        if var_name.lower() in env_vars:
            del env_vars[var_name.lower()]
        if var_name.upper() in env_vars:
            del env_vars[var_name.upper()]
    return env_vars
",if var_name . upper ( ) in env_vars :,104
"def adapt_datetimefield_value(self, value):
    if value is None:
        return None
    # Expression values are adapted by the database.
    if hasattr(value, ""resolve_expression""):
        return value
    # SQLite doesn't support tz-aware datetimes
    if timezone.is_aware(value):
        if settings.USE_TZ:
            value = timezone.make_naive(value, self.connection.timezone)
        else:
            raise ValueError(
                ""SQLite backend does not support timezone-aware datetimes when USE_TZ is False.""
            )
    return six.text_type(value)
",if settings . USE_TZ :,156
"def dragMoveEvent(self, event):
    data = event.mimeData()
    urls = data.urls()
    if urls and urls[0].scheme() == ""file"":
        event.acceptProposedAction()
        indexRow = self.indexAt(event.pos()).row()
        window = self.parent().parent().parent().parent().parent().parent()
        if indexRow == -1 or not window.clearedPlaylistNote:
            indexRow = window.playlist.count()
        window.setPlaylistInsertPosition(indexRow)
    else:
        super(MainWindow.PlaylistWidget, self).dragMoveEvent(event)
",if indexRow == - 1 or not window . clearedPlaylistNote :,157
"def explode(self, obj):
    """"""Determine if the object should be exploded.""""""
    if obj in self._done:
        return False
    result = False
    for item in self._explode:
        if hasattr(item, ""_moId""):
            # If it has a _moId it is an instance
            if obj._moId == item._moId:
                result = True
        else:
            # If it does not have a _moId it is a template
            if obj.__class__.__name__ == item.__name__:
                result = True
    if result:
        self._done.add(obj)
    return result
",if obj . __class__ . __name__ == item . __name__ :,166
"def _maybe_clean(self):
    """"""Clean the cache if it's time to do so.""""""
    now = time.time()
    if self.next_cleaning <= now:
        keys_to_delete = []
        for (k, v) in self.data.items():
            if v.expiration <= now:
                keys_to_delete.append(k)
        for k in keys_to_delete:
            del self.data[k]
        now = time.time()
        self.next_cleaning = now + self.cleaning_interval
",if v . expiration <= now :,145
"def test_doc_attributes(self):
    print_test_name(""TEST DOC ATTRIBUTES"")
    correct = 0
    for example in DOC_EXAMPLES:
        original_schema = schema.parse(example.schema_string)
        if original_schema.doc is not None:
            correct += 1
        if original_schema.type == ""record"":
            for f in original_schema.fields:
                if f.doc is None:
                    self.fail(
                        ""Failed to preserve 'doc' in fields: "" + example.schema_string
                    )
    self.assertEqual(correct, len(DOC_EXAMPLES))
","if original_schema . type == ""record"" :",168
"def save_as(self):
    """"""Save *as* the currently edited file""""""
    editorstack = self.get_current_editorstack()
    if editorstack.save_as():
        fname = editorstack.get_current_filename()
        if CONF.get(""workingdir"", ""editor/save/auto_set_to_basedir""):
            self.emit(SIGNAL(""open_dir(QString)""), osp.dirname(fname))
        self.__add_recent_file(fname)
","if CONF . get ( ""workingdir"" , ""editor/save/auto_set_to_basedir"" ) :",117
"def verify_settings(rst_path: Path) -> Iterator[Error]:
    for setting_name, default in find_settings_in_rst(rst_path):
        actual = getattr(app.conf, setting_name)
        if isinstance(default, timedelta):
            default = default.total_seconds()
        if isinstance(actual, Enum):
            actual = actual.value
        if actual != default:
            yield Error(
                reason=""mismatch"",
                setting=setting_name,
                default=default,
                actual=actual,
            )
",if actual != default :,152
"def JobWait(self, waiter):
    # type: (Waiter) -> wait_status_t
    # wait builtin can be interrupted
    while True:
        # Don't retry
        result = waiter.WaitForOne(False)
        if result > 0:  # signal
            return wait_status.Cancelled(result)
        if result == -1:  # nothing to wait for
            break
        if self.state != job_state_e.Running:
            break
    return wait_status.Proc(self.status)
",if self . state != job_state_e . Running :,135
"def object_hook(obj):
    obj_len = len(obj)
    if obj_len == 1:
        if ""$date"" in obj:
            return datetime.fromtimestamp(
                obj[""$date""] / 1000, tz=timezone.utc
            ) + timedelta(milliseconds=obj[""$date""] % 1000)
        if ""$time"" in obj:
            return time(*[int(i) for i in obj[""$time""].split("":"")])
    if obj_len == 2 and ""$type"" in obj and ""$value"" in obj:
        if obj[""$type""] == ""date"":
            return date(*[int(i) for i in obj[""$value""].split(""-"")])
    return obj
","if ""$time"" in obj :",174
"def before_FunctionDef(self, node):
    s = self.format(node, print_body=False)
    if self.test_kind is ""test"":
        print(s)
    self.indent += 1
    self.context_stack.append(node)
    if self.pass_n == 1:
        self.stats.defs += 1
        if self.class_name not in self.special_class_names:
            if self.class_name in self.classes:
                the_class = self.classes.get(self.class_name)
                methods = the_class.get(""methods"")
                # tag:setter function-name=stringized-args
                methods[node.name] = self.format(node.args)
",if self . class_name not in self . special_class_names :,192
"def setAttributeNS(self, namespaceURI, qualifiedName, value):
    prefix, localname = _nssplit(qualifiedName)
    attr = self.getAttributeNodeNS(namespaceURI, localname)
    if attr is None:
        attr = Attr(qualifiedName, namespaceURI, localname, prefix)
        attr.value = value
        attr.ownerDocument = self.ownerDocument
        self.setAttributeNode(attr)
    else:
        if value != attr.value:
            attr.value = value
            if attr.isId:
                _clear_id_cache(self)
        if attr.prefix != prefix:
            attr.prefix = prefix
            attr.nodeName = qualifiedName
",if attr . isId :,177
"def main():
    try:
        from wsgiref.simple_server import make_server
        from wsgiref.validate import validator
        if port[0] == 0:
            port[0] = get_open_port()
        wsgi_application = WsgiApplication(msgpackrpc_application)
        server = make_server(host, port[0], validator(wsgi_application))
        logger.info(""Starting interop server at %s:%s."" % (host, port[0]))
        logger.info(""WSDL is at: /?wsdl"")
        server.serve_forever()
    except ImportError:
        print(""Error: example server code requires Python >= 2.5"")
",if port [ 0 ] == 0 :,172
"def yield_modules(path):
    """"""Yield all Python modules underneath *path*""""""
    for (dpath, dnames, fnames) in os.walk(path):
        module = tuple(dpath.split(""/"")[1:])
        for fname in fnames:
            if not fname.endswith("".py""):
                continue
            fpath = os.path.join(dpath, fname)
            if fname == ""__init__.py"":
                yield (fpath, module)
            else:
                yield (fpath, module + (fname[:-3],))
        dnames[:] = [
            x for x in dnames if os.path.exists(os.path.join(dpath, x, ""__init__.py""))
        ]
","if fname == ""__init__.py"" :",182
"def dump_section(name, section):
    lines.append(""[%s]\n"" % name)
    for key, value in section.all_items():
        if not key.startswith(""_""):
            try:
                if key in section.definitions:
                    lines.append(
                        ""%s=%s\n"" % (key, section.definitions[key].tostring(value))
                    )
                else:
                    lines.append(""%s=%s\n"" % (key, value))
            except:
                logger.exception('Error serializing ""%s"" in section ""[%s]""', key, name)
    lines.append(""\n"")
",if key in section . definitions :,175
"def testCreateTimeout(self):
    cluster = None
    try:
        env_path = ""conda://"" + os.environ[""CONDA_PREFIX""]
        log_config_file = os.path.join(
            os.path.dirname(os.path.abspath(__file__)), ""yarn-logging.conf""
        )
        with self.assertRaises(TimeoutError):
            cluster = new_cluster(
                env_path,
                log_config=log_config_file,
                worker_cache_mem=""64m"",
                log_when_fail=True,
                timeout=1,
            )
    finally:
        if cluster is not None:
            cluster.stop()
",if cluster is not None :,187
"def read_phrases(data_dir, movies=None):
    res = {}
    for parts in iterate_entries(data_dir, ""movie_lines.txt""):
        l_id, m_id, l_str = parts[0], parts[2], parts[4]
        if movies and m_id not in movies:
            continue
        tokens = utils.tokenize(l_str)
        if tokens:
            res[l_id] = tokens
    return res
",if tokens :,127
"def get_Subclass_of(rt):
    for y in [getattr(Ast, x) for x in dir(Ast)]:
        yt = clr.GetClrType(y)
        if rt == yt:
            continue
        if yt.IsAbstract:
            continue
        if yt.IsSubclassOf(rt):
            yield yt.Name
",if yt . IsSubclassOf ( rt ) :,93
"def retrieve(self, aclass):
    """"""Look for a specifc class/name in the packet""""""
    resu = []
    for x in self.payload:
        try:
            if isinstance(aclass, str):
                if x.name == aclass:
                    resu.append(x)
            else:
                if isinstance(x, aclass):
                    resu.append(x)
            resu += x.retrieve(aclass)
        except:
            pass
    return resu
","if isinstance ( x , aclass ) :",144
"def _max_physical(self):
    ""How big is the physical screen?""
    # On OS X newwin does not correctly get the size of the screen.
    # let's see how big we could be: create a temp screen
    # and see the size curses makes it.  No good to keep, though
    try:
        mxy, mxx = struct.unpack(
            ""hh"", fcntl.ioctl(sys.stderr.fileno(), termios.TIOCGWINSZ, ""xxxx"")
        )
        if (mxy, mxx) == (0, 0):
            raise ValueError
    except (ValueError, NameError):
        mxy, mxx = curses.newwin(0, 0).getmaxyx()
    # return safe values, i.e. slightly smaller.
    return (mxy - 1, mxx - 1)
","if ( mxy , mxx ) == ( 0 , 0 ) :",195
"def deserialize(self, cassette_string):
    cassette_dict = self.base_serializer.deserialize(cassette_string)
    for interaction in cassette_dict[""interactions""]:
        response = interaction[""response""]
        headers = response[""headers""]
        if ""Content-Range"" in headers and ""Content-Disposition"" in headers:
            rg, size, filename = self._parse_headers(headers)
            with open(join(self.directory, filename), ""rb"") as f:
                f.seek(rg[0])
                content = f.read(rg[1] - rg[0] + 1)
            response[""body""][""string""] = content
    return cassette_dict
","if ""Content-Range"" in headers and ""Content-Disposition"" in headers :",180
"def parse_head(fileobj, parser):
    """"""Return a list of key, value pairs.""""""
    while 1:
        data = fileobj.read(CHUNK)
        try:
            parser.feed(data)
        except EndOfHeadError:
            break
        if len(data) != CHUNK:
            # this should only happen if there is no HTML body, or if
            # CHUNK is big
            break
    return parser.http_equiv
",if len ( data ) != CHUNK :,119
"def _check_no_empty_dimension_lists(config):
    """"""Verify that at least one dimension is not an empty list""""""
    logging.info(""Checking provided dimensions are valid"")
    for feature in config.get(""test-suites"").values():
        for test_name, test in feature.items():
            for dimensions_config in test.values():
                for dimensions_group in dimensions_config:
                    if [] in dimensions_group.values():
                        logging.error(
                            ""Values assigned to dimensions in test %s cannot be empty"",
                            test_name,
                        )
                        raise AssertionError
",if [ ] in dimensions_group . values ( ) :,175
"def aggregate_sorted(self, items):
    create = self.createCombiner
    merge = self.mergeValue
    i = None
    for i, (k, v) in enumerate(items):
        if i == 0:
            curr_key = k
            curr_value = create(v)
        elif k != curr_key:
            yield curr_key, curr_value
            curr_key = k
            curr_value = create(v)
        else:
            curr_value = merge(curr_value, v)
    if i is not None:
        yield curr_key, curr_value
",elif k != curr_key :,159
"def _run_iptables(self, version, cmd, *args):
    ipt_cmd = ""{} {}"".format(self._iptables_command(version), cmd)
    if self._has_w_argument is None:
        result = self.run_expect([0, 2], ipt_cmd, *args)
        if result.rc == 2:
            self._has_w_argument = False
            return self._run_iptables(version, cmd, *args)
        else:
            self._has_w_argument = True
            return result.stdout.rstrip(""\r\n"")
    else:
        return self.check_output(ipt_cmd, *args)
",if result . rc == 2 :,172
"def handle_data(self, data):
    if self.in_span or self.in_div:
        if data == ""No such user (please note that login is case sensitive)"":
            self.no_user = True
        elif data == ""Invalid password"":
            self.bad_pw = True
        elif data == ""User with that email already exists"":
            self.already_exists = True
","elif data == ""User with that email already exists"" :",101
"def configure(self, **kw):
    """"""Configure the image.""""""
    res = ()
    for k, v in _cnfmerge(kw).items():
        if v is not None:
            if k[-1] == ""_"":
                k = k[:-1]
            if hasattr(v, ""__call__""):
                v = self._register(v)
            elif k in (""data"", ""maskdata""):
                v = self.tk._createbytearray(v)
            res = res + (""-"" + k, v)
    self.tk.call((self.name, ""config"") + res)
","elif k in ( ""data"" , ""maskdata"" ) :",154
"def run(self):
    if self.distribution.install_requires:
        self.distribution.fetch_build_eggs(self.distribution.install_requires)
    if self.distribution.tests_require:
        self.distribution.fetch_build_eggs(self.distribution.tests_require)
    if self.test_suite:
        cmd = "" "".join(self.test_args)
        if self.dry_run:
            self.announce('skipping ""unittest %s"" (dry run)' % cmd)
        else:
            self.announce('running ""unittest %s""' % cmd)
            self.with_project_on_sys_path(self.run_tests)
",if self . dry_run :,171
"def wrapped(request, *args, **kwargs):
    if not gargoyle.is_active(key, request):
        if not redirect_to:
            raise Http404(""Switch '%s' is not active"" % key)
        elif redirect_to.startswith(""/""):
            return HttpResponseRedirect(redirect_to)
        else:
            return HttpResponseRedirect(reverse(redirect_to))
    return func(request, *args, **kwargs)
","elif redirect_to . startswith ( ""/"" ) :",109
"def strip_suffixes(path: str) -> str:
    t = path
    while True:
        if t.endswith("".xz""):
            t = t[:-3]
        elif t.endswith("".raw""):
            t = t[:-4]
        elif t.endswith("".tar""):
            t = t[:-4]
        elif t.endswith("".qcow2""):
            t = t[:-6]
        else:
            break
    return t
","if t . endswith ( "".xz"" ) :",119
"def tags(self):
    label = """"
    for dt in constants.DOMAIN_TYPES:
        if self.type == dt[0]:
            label = dt[1]
    result = [{""name"": self.type, ""label"": label, ""type"": ""dom""}]
    if self.transport:
        result.append(
            {
                ""name"": self.transport.service,
                ""label"": self.transport.service,
                ""type"": ""srv"",
                ""color"": ""info"",
            }
        )
    return result
",if self . type == dt [ 0 ] :,148
"def find_first_of_filetype(content, filterfiltype, attr=""name""):
    """"""Find the first of the file type.""""""
    filename = """"
    for _filename in content:
        if isinstance(_filename, str):
            if _filename.endswith(f"".{filterfiltype}""):
                filename = _filename
                break
        else:
            if getattr(_filename, attr).endswith(f"".{filterfiltype}""):
                filename = getattr(_filename, attr)
                break
    return filename
","if getattr ( _filename , attr ) . endswith ( f"".{filterfiltype}"" ) :",135
"def check_data_array_types(self, *arrays):
    result = []
    for array in arrays:
        if array is None or scipy.sparse.issparse(array):
            result.append(array)
            continue
        result.append(np.asanyarray(array))
        if not result[-1].shape:
            raise RuntimeError(
                ""Given data-array is of unexpected type %s. Please pass numpy arrays instead.""
                % type(array)
            )
    return result
",if array is None or scipy . sparse . issparse ( array ) :,131
"def description(self):
    global role_descriptions
    description = role_descriptions[self.role_field]
    content_type = self.content_type
    model_name = None
    if content_type:
        model = content_type.model_class()
        model_name = re.sub(r""([a-z])([A-Z])"", r""\1 \2"", model.__name__).lower()
    value = description
    if type(description) == dict:
        value = description.get(model_name)
        if value is None:
            value = description.get(""default"")
    if ""%s"" in value and content_type:
        value = value % model_name
    return value
",if value is None :,173
"def popupFrameXdiff(job, frame1, frame2, frame3=None):
    """"""Opens a frame xdiff.""""""
    for command in [""/usr/bin/xxdiff"", ""/usr/local/bin/xdiff""]:
        if os.path.isfile(command):
            for frame in [frame1, frame2, frame3]:
                if frame:
                    command += "" --title1 %s %s"" % (
                        frame.data.name,
                        getFrameLogFile(job, frame),
                    )
            shellOut(command)
",if os . path . isfile ( command ) :,154
"def _groups_args_split(self, kwargs):
    groups_args_split = []
    groups = kwargs[""groups""]
    for key, group in groups.iteritems():
        mykwargs = kwargs.copy()
        del mykwargs[""groups""]
        if ""group_name"" in group:
            mykwargs[""source_security_group_name""] = group[""group_name""]
        if ""user_id"" in group:
            mykwargs[""source_security_group_owner_id""] = group[""user_id""]
        if ""group_id"" in group:
            mykwargs[""source_security_group_id""] = group[""group_id""]
        groups_args_split.append(mykwargs)
    return groups_args_split
","if ""group_id"" in group :",186
"def _mangle_phone(phone, config):
    regexp = config.get(""REGEXP"")
    if regexp:
        try:
            m = re.match(""^/(.*)/(.*)/$"", regexp)
            if m:
                phone = re.sub(m.group(1), m.group(2), phone)
        except re.error:
            log.warning(
                u""Can not mangle phone number. ""
                u""Please check your REGEXP: {0!s}"".format(regexp)
            )
    return phone
",if m :,144
"def getScramRange(src):
    scramRange = None
    for mod in src.item.activeModulesIter():
        if _isRegularScram(mod) or _isHicScram(mod):
            scramRange = max(scramRange or 0, mod.maxRange or 0)
    return scramRange
",if _isRegularScram ( mod ) or _isHicScram ( mod ) :,83
"def snapshot(self):
    # if this volume is attached to a server
    # we need to freeze the XFS file system
    try:
        self.freeze()
        if self.server == None:
            snapshot = self.get_ec2_connection().create_snapshot(self.volume_id)
        else:
            snapshot = self.server.ec2.create_snapshot(self.volume_id)
        boto.log.info(""Snapshot of Volume %s created: %s"" % (self.name, snapshot))
    except Exception:
        boto.log.info(""Snapshot error"")
        boto.log.info(traceback.format_exc())
    finally:
        status = self.unfreeze()
        return status
",if self . server == None :,181
"def closeststack(self, card):
    closest = None
    cdist = 999999999
    # Since we only compare distances,
    # we don't bother to take the square root.
    for stack in self.openstacks:
        dist = (stack.x - card.x) ** 2 + (stack.y - card.y) ** 2
        if dist < cdist:
            closest = stack
            cdist = dist
    return closest
",if dist < cdist :,106
"def _sock_send(self, msg):
    try:
        if isinstance(msg, str):
            msg = msg.encode(""ascii"")
        # http://docs.datadoghq.com/guides/dogstatsd/#datagram-format
        if self.dogstatsd_tags:
            msg = msg + b""|#"" + self.dogstatsd_tags.encode(""ascii"")
        if self.sock:
            self.sock.send(msg)
    except Exception:
        Logger.warning(self, ""Error sending message to statsd"", exc_info=True)
",if self . sock :,146
"def styleRow(self, row, selected):
    if row > 0 and row < self.getRowCount():
        if selected:
            self.getRowFormatter().addStyleName(row, ""user-SelectedRow"")
        else:
            self.getRowFormatter().removeStyleName(row, ""user-SelectedRow"")
",if selected :,81
"def __gather_epoch_end_eval_results(self, outputs):
    eval_results = []
    for epoch_output in outputs:
        result = epoch_output[0].__class__.gather(epoch_output)
        if ""checkpoint_on"" in result:
            result.checkpoint_on = result.checkpoint_on.mean()
        if ""early_stop_on"" in result:
            result.early_stop_on = result.early_stop_on.mean()
        eval_results.append(result)
    # with 1 dataloader don't pass in a list
    if len(eval_results) == 1:
        eval_results = eval_results[0]
    return eval_results
","if ""early_stop_on"" in result :",172
"def network_state(self, device):
    cmd = [""tc"", ""qdisc"", ""show"", ""dev"", device]
    try:
        output = self.host_exec.run(cmd)
        # sloppy but good enough for now
        if "" delay "" in output:
            return NetworkState.SLOW
        if "" loss "" in output:
            return NetworkState.FLAKY
        if "" duplicate "" in output:
            return NetworkState.DUPLICATE
        return NetworkState.NORMAL
    except Exception:
        return NetworkState.UNKNOWN
","if "" duplicate "" in output :",138
"def canberra_grad(x, y):
    result = 0.0
    grad = np.zeros(x.shape)
    for i in range(x.shape[0]):
        denominator = np.abs(x[i]) + np.abs(y[i])
        if denominator > 0:
            result += np.abs(x[i] - y[i]) / denominator
            grad[i] = (
                np.sign(x[i] - y[i]) / denominator
                - np.abs(x[i] - y[i]) * np.sign(x[i]) / denominator ** 2
            )
    return result, grad
",if denominator > 0 :,167
"def readwrite(obj, flags):
    try:
        if flags & select.POLLIN:
            obj.handle_read_event()
        if flags & select.POLLOUT:
            obj.handle_write_event()
        if flags & select.POLLPRI:
            obj.handle_expt_event()
        if flags & (select.POLLHUP | select.POLLERR | select.POLLNVAL):
            obj.handle_close()
    except OSError as e:
        if e.args[0] not in _DISCONNECTED:
            obj.handle_error()
        else:
            obj.handle_close()
    except _reraised_exceptions:
        raise
    except:
        obj.handle_error()
",if flags & select . POLLOUT :,192
"def get_func_name(obj):
    if inspect.ismethod(obj):
        match = RE_BOUND_METHOD.match(repr(obj))
        if match:
            cls = match.group(""class"")
            if not cls:
                return match.group(""name"")
            return ""%s.%s"" % (match.group(""class""), match.group(""name""))
    return None
",if match :,102
"def __init__(self, connection):
    self.username = connection.username
    self.password = connection.password
    self.domain = connection.domain
    self.hash = connection.hash
    self.lmhash = """"
    self.nthash = """"
    self.aesKey = connection.aesKey
    self.kdcHost = connection.kdcHost
    self.kerberos = connection.kerberos
    if self.hash is not None:
        if self.hash.find("":"") != -1:
            self.lmhash, self.nthash = self.hash.split("":"")
        else:
            self.nthash = self.hash
    if self.password is None:
        self.password = """"
","if self . hash . find ( "":"" ) != - 1 :",174
"def indent_xml(elem, level=0):
    """"""Do our pretty printing and make Matt very happy.""""""
    i = ""\n"" + level * ""  ""
    if elem:
        if not elem.text or not elem.text.strip():
            elem.text = i + ""  ""
        if not elem.tail or not elem.tail.strip():
            elem.tail = i
        for elem in elem:
            indent_xml(elem, level + 1)
        if not elem.tail or not elem.tail.strip():
            elem.tail = i
    else:
        if level and (not elem.tail or not elem.tail.strip()):
            elem.tail = i
",if level and ( not elem . tail or not elem . tail . strip ( ) ) :,177
"def add_braces_and_labels(self):
    for attr in ""horizontal_parts"", ""vertical_parts"":
        if not hasattr(self, attr):
            continue
        parts = getattr(self, attr)
        for subattr in ""braces"", ""labels"":
            if hasattr(parts, subattr):
                self.add(getattr(parts, subattr))
","if not hasattr ( self , attr ) :",97
"def error_messages(file_list, files_removed):
    if files_removed is None:
        return
    for remove_this, reason in files_removed:
        if file_list is not None:
            file_list.remove(remove_this)
        if reason == 0:
            print("" REMOVED : ("" + str(remove_this) + "")   is not PNG file format"")
        elif reason == 1:
            print("" REMOVED : ("" + str(remove_this) + "")   already exists"")
        elif reason == 2:
            print("" REMOVED : ("" + str(remove_this) + "")   file unreadable"")
",if reason == 0 :,161
"def keep_vocab_item(word, count, min_count, trim_rule=None):
    default_res = count >= min_count
    if trim_rule is None:
        return default_res
    else:
        rule_res = trim_rule(word, count, min_count)
        if rule_res == RULE_KEEP:
            return True
        elif rule_res == RULE_DISCARD:
            return False
        else:
            return default_res
",elif rule_res == RULE_DISCARD :,125
"def func(x0):
    bind = 0
    backups = []
    vinputs = []
    for i, i0 in zip(inputs, inputs0):
        if i is None:  # Optional argument
            continue
        vinputs += [i]
        if i0 is not None:  # Not need backward
            i.d[...] = x0[bind : bind + i.size].reshape(i.shape)
            bind += i.size
        backups.append(i.d.copy())
    f.forward(vinputs, outputs)
    for ind, i in enumerate(inputs):
        if i is None:  # Optional argument
            continue
        i.d[...] = backups[ind]
    return sum([np.sum(o.g * o.d) for o in outputs])
",if i0 is not None :,200
"def _handle_js_events(self, change):
    if self.js_events:
        if self.event_handlers:
            for event in self.js_events:
                event_name = event[""name""]
                if event_name in self.event_handlers:
                    self.event_handlers[event_name](event[""detail""])
        # clears the event queue.
        self.js_events = []
",if event_name in self . event_handlers :,113
"def validate(leaves):
    for leaf in leaves:
        if leaf.has_form((""Rule"", ""RuleDelayed""), 2):
            pass
        elif leaf.has_form(""List"", None) or leaf.has_form(""Association"", None):
            if validate(leaf.leaves) is not True:
                return False
        else:
            return False
    return True
","if leaf . has_form ( ( ""Rule"" , ""RuleDelayed"" ) , 2 ) :",97
"def ascii85decode(data):
    n = b = 0
    out = """"
    for c in data:
        if ""!"" <= c and c <= ""u"":
            n += 1
            b = b * 85 + (ord(c) - 33)
            if n == 5:
                out += struct.pack("">L"", b)
                n = b = 0
        elif c == ""z"":
            assert n == 0
            out += ""\0\0\0\0""
        elif c == ""~"":
            if n:
                for _ in range(5 - n):
                    b = b * 85 + 84
                out += struct.pack("">L"", b)[: n - 1]
            break
    return out
",if n :,200
"def to_text(self, origin=None, relativize=True, **kw):
    next = self.next.choose_relativity(origin, relativize)
    text = """"
    for (window, bitmap) in self.windows:
        bits = []
        for i in xrange(0, len(bitmap)):
            byte = bitmap[i]
            for j in xrange(0, 8):
                if byte & (0x80 >> j):
                    bits.append(dns.rdatatype.to_text(window * 256 + i * 8 + j))
        text += "" "" + "" "".join(bits)
    return ""%s%s"" % (next, text)
",if byte & ( 0x80 >> j ) :,177
"def _on_response(self, widget, response):
    value = None
    if response == Gtk.ResponseType.OK:
        if self.value_type is int:
            value = self.spinbutton.get_value_as_int()
        else:
            value = self.spinbutton.get_value()
    self.deferred.callback(value)
    self.destroy()
",if self . value_type is int :,96
"def send_preamble(self):
    """"""Transmit version/status/date/server, via self._write()""""""
    if self.origin_server:
        if self.client_is_modern():
            self._write(""HTTP/%s %s\r\n"" % (self.http_version, self.status))
            if not self.headers.has_key(""Date""):
                self._write(""Date: %s\r\n"" % time.asctime(time.gmtime(time.time())))
            if self.server_software and not self.headers.has_key(""Server""):
                self._write(""Server: %s\r\n"" % self.server_software)
    else:
        self._write(""Status: %s\r\n"" % self.status)
",if self . client_is_modern ( ) :,199
"def _save_postinsts_common(self, dst_postinst_dir, src_postinst_dir):
    num = 0
    for p in self._get_delayed_postinsts():
        bb.utils.mkdirhier(dst_postinst_dir)
        if os.path.exists(os.path.join(src_postinst_dir, p + "".postinst"")):
            shutil.copy(
                os.path.join(src_postinst_dir, p + "".postinst""),
                os.path.join(dst_postinst_dir, ""%03d-%s"" % (num, p)),
            )
        num += 1
","if os . path . exists ( os . path . join ( src_postinst_dir , p + "".postinst"" ) ) :",165
"def edge_data_from_bmesh_edges(bm, edge_data):
    initial_index = bm.edges.layers.int.get(""initial_index"")
    if initial_index is None:
        raise Exception(""bmesh has no initial_index layer"")
    edge_data_out = []
    n_edge_data = len(edge_data)
    for edge in bm.edges:
        idx = edge[initial_index]
        if idx < 0 or idx >= n_edge_data:
            debug(""Unexisting edge_data[%s] [0 - %s]"", idx, n_edge_data)
            edge_data_out.append(None)
        else:
            edge_data_out.append(edge_data[idx])
    return edge_data_out
",if idx < 0 or idx >= n_edge_data :,198
"def write(self, data):
    try:
        c_written = DWORD()
        buffer = create_string_buffer(data)
        if not WriteFile(self.pStdin, buffer, len(buffer), byref(c_written), None):
            raise WinError()
    except:
        self.close()
","if not WriteFile ( self . pStdin , buffer , len ( buffer ) , byref ( c_written ) , None ) :",82
"def get_icon(svg_path, size):
    pixbuf = GdkPixbuf.Pixbuf.new_from_file_at_scale(svg_path, size, size, True)
    data = bytearray(pixbuf.get_pixels())
    channels = pixbuf.get_n_channels()
    assert channels == 4
    # https://en.wikipedia.org/wiki/PackBits
    # no real compression going on here..
    new_data = bytearray()
    for c in range(3):
        x = 0
        for i in range(0, len(data), 4):
            if x == 0 or x % 128 == 0:
                new_data.append(127)
            new_data.append(data[i + c])
            x += 1
    return new_data
",if x == 0 or x % 128 == 0 :,194
"def _get_instance_attribute(
    self, attr, default=None, defaults=None, incl_metadata=False
):
    if self.instance is None or not hasattr(self.instance, attr):
        if incl_metadata and attr in self.parsed_metadata:
            return self.parsed_metadata[attr]
        elif defaults is not None:
            for value in defaults:
                if callable(value):
                    value = value()
                if value is not None:
                    return value
        return default
    return getattr(self.instance, attr)
",if value is not None :,149
"def forward(self, x):
    if self.ffn_type in (1, 2):
        x0 = self.wx0(x)
        if self.ffn_type == 1:
            x1 = x
        elif self.ffn_type == 2:
            x1 = self.wx1(x)
        out = self.output(x0 * x1)
    out = self.dropout(out)
    out = self.LayerNorm(out + x)
    return out
",if self . ffn_type == 1 :,122
"def load(cls):
    if not cls._loaded:
        cls.log.debug(""Loading tile_sets..."")
        if not horizons.globals.fife.use_atlases:
            cls._find_tile_sets(PATHS.TILE_SETS_DIRECTORY)
        else:
            cls.tile_sets = JsonDecoder.load(PATHS.TILE_SETS_JSON_FILE)
        cls.log.debug(""Done!"")
        cls._loaded = True
",if not horizons . globals . fife . use_atlases :,120
"def headerData(self, section, orientation, role=Qt.DisplayRole):
    if role == Qt.TextAlignmentRole:
        if orientation == Qt.Horizontal:
            return to_qvariant(int(Qt.AlignHCenter | Qt.AlignVCenter))
        return to_qvariant(int(Qt.AlignRight | Qt.AlignVCenter))
    if role != Qt.DisplayRole:
        return to_qvariant()
    if orientation == Qt.Horizontal:
        if section == NAME:
            return to_qvariant(""Name"")
        elif section == VERSION:
            return to_qvariant(""Version"")
        elif section == ACTION:
            return to_qvariant(""Action"")
        elif section == DESCRIPTION:
            return to_qvariant(""Description"")
    return to_qvariant()
",elif section == VERSION :,192
"def find_enabled_item(self, e):
    x, y = e.local
    if (
        0
        <= x
        < (
            self.width - self.margin - self.scroll_button_size
            if self.scrolling
            else self.width
        )
    ):
        h = self.font.get_linesize()
        i = (y - h // 2) // h + self.scroll
        items = self._items
        if 0 <= i < len(items):
            item = items[i]
            if item.enabled:
                return item
",if item . enabled :,160
"def set_parallel_limit(environment):
    parallel_limit = environment.get(""COMPOSE_PARALLEL_LIMIT"")
    if parallel_limit:
        try:
            parallel_limit = int(parallel_limit)
        except ValueError:
            raise errors.UserError(
                'COMPOSE_PARALLEL_LIMIT must be an integer (found: ""{}"")'.format(
                    environment.get(""COMPOSE_PARALLEL_LIMIT"")
                )
            )
        if parallel_limit <= 1:
            raise errors.UserError(""COMPOSE_PARALLEL_LIMIT can not be less than 2"")
        parallel.GlobalLimit.set_global_limit(parallel_limit)
",if parallel_limit <= 1 :,178
"def migrate_identifier(self, raw_identifier: int):
    if self.unique_cog_identifier in self.data:
        # Data has already been migrated
        return
    poss_identifiers = [str(raw_identifier), str(hash(raw_identifier))]
    for ident in poss_identifiers:
        if ident in self.data:
            self.data[self.unique_cog_identifier] = self.data[ident]
            del self.data[ident]
            _save_json(self.data_path, self.data)
            break
",if ident in self . data :,140
"def _memoize(*args, **kwargs):
    str_args = []
    for arg in args:
        if not isinstance(arg, six.string_types):
            str_args.append(six.text_type(arg))
        else:
            str_args.append(arg)
    args_ = "","".join(
        list(str_args) + [""{0}={1}"".format(k, kwargs[k]) for k in sorted(kwargs)]
    )
    if args_ not in cache:
        cache[args_] = func(*args, **kwargs)
    return cache[args_]
","if not isinstance ( arg , six . string_types ) :",146
"def extract(self):
    for battery in self.vars:
        for line in dopen(""/proc/acpi/battery/"" + battery + ""/state"").readlines():
            l = line.split()
            if len(l) < 3:
                continue
            if l[0:2] == [""remaining"", ""capacity:""]:
                remaining = int(l[2])
                continue
            elif l[0:2] == [""present"", ""rate:""]:
                rate = int(l[2])
                continue
        if rate and remaining:
            self.val[battery] = remaining * 60 / rate
        else:
            self.val[battery] = -1
",if len ( l ) < 3 :,185
"def version_iter(q, limit=500, offset=0):
    q[""limit""] = limit
    q[""offset""] = offset
    while True:
        url = base_url() + ""/version""
        v = jsonload(url)
        if not v:
            return
        for i in query(q):
            yield i
        q[""offset""] += limit
",if not v :,97
"def _letf_btn_press(self, event):
    try:
        elem = self.identify(event.x, event.y)
        index = self.index(""@%d,%d"" % (event.x, event.y))
        if ""closebutton"" in elem:
            self.state([""pressed""])
            self.pressed_index = index
    except Exception:
        # may fail, if clicked outside of tab
        return
","if ""closebutton"" in elem :",112
"def get_location(self, dist, dependency_links):
    for url in dependency_links:
        egg_fragment = Link(url).egg_fragment
        if not egg_fragment:
            continue
        if ""-"" in egg_fragment:
            ## FIXME: will this work when a package has - in the name?
            key = ""-"".join(egg_fragment.split(""-"")[:-1]).lower()
        else:
            key = egg_fragment
        if key == dist.key:
            return url.split(""#"", 1)[0]
    return None
","if ""-"" in egg_fragment :",141
"def viewTreeItemClicked(self, event):
    if DEBUG:
        print(""viewTreeitemClicked:"", event.__dict__, file=sys.stderr)
    self.unmarkTargets()
    vuid = self.viewTree.viewTree.identify_row(event.y)
    if vuid:
        view = self.vc.viewsById[vuid]
        if view:
            coords = view.getCoords()
            if view.isTarget():
                self.markTarget(coords[0][0], coords[0][1], coords[1][0], coords[1][1])
            self.viewDetails.set(view)
",if view :,162
"def getVar(self, name):
    value = self.tinfoil.run_command(""dataStoreConnectorFindVar"", self.dsindex, name)
    overrides = None
    if isinstance(value, dict):
        if ""_connector_origtype"" in value:
            value[""_content""] = self.tinfoil._reconvert_type(
                value[""_content""], value[""_connector_origtype""]
            )
            del value[""_connector_origtype""]
        if ""_connector_overrides"" in value:
            overrides = value[""_connector_overrides""]
            del value[""_connector_overrides""]
    return value, overrides
","if ""_connector_overrides"" in value :",158
"def sample(self, **config):
    """"""Sample a configuration from this search space.""""""
    ret = []
    kwspaces = self.kwspaces
    striped_keys = [k.split(SPLITTER)[0] for k in config.keys()]
    for idx, obj in enumerate(self.data):
        if isinstance(obj, NestedSpace):
            sub_config = _strip_config_space(config, prefix=str(idx))
            ret.append(obj.sample(**sub_config))
        elif isinstance(obj, SimpleSpace):
            ret.append(config[str(idx)])
        else:
            ret.append(obj)
    return ret
","elif isinstance ( obj , SimpleSpace ) :",165
"def main():
    for filename in sys.argv[1:]:
        if os.path.isdir(filename):
            print(filename, ""Directory!"")
            continue
        with open(filename, ""rb"") as f:
            data = f.read()
        if b""\0"" in data:
            print(filename, ""Binary!"")
            continue
        newdata = data.replace(b""\r\n"", b""\n"")
        if newdata != data:
            print(filename)
            with open(filename, ""wb"") as f:
                f.write(newdata)
",if os . path . isdir ( filename ) :,157
"def normalize_crlf(tree):
    for elem in tree.getiterator():
        if elem.text:
            elem.text = elem.text.replace(""\r\n"", ""\n"")
        if elem.tail:
            elem.tail = elem.tail.replace(""\r\n"", ""\n"")
",if elem . tail :,76
"def RegisterValue(self, value):
    """"""Puts a given value into an appropriate bin.""""""
    if self.bins:
        for b in self.bins:
            if b.range_max_value > value:
                b.num += 1
                return
        self.bins[-1].num += 1
",if b . range_max_value > value :,82
"def all_commands():
    all_cmds = []
    for bp in BINPATHS:
        cmds = [
            fn[:-3]
            for fn in os.listdir(bp)
            if fn.endswith("".py"")
            and not fn.startswith(""."")
            and os.path.isfile(os.path.join(bp, fn))
        ]
        all_cmds += cmds
    all_cmds.sort()
    return all_cmds
","if fn . endswith ( "".py"" )",116
"def base64_encode_image_mapper(self, tag, url):
    if tag == ""img"":
        if url in self.kp_images:
            image_data = base64.b64encode(self.kp_images[url])
            image_mimetype = mimetypes.guess_type(url)[0]
            if image_mimetype is not None:
                return ""data:{};base64, "".format(image_mimetype) + image_data.decode(
                    ""utf-8""
                )
    return None
",if image_mimetype is not None :,138
"def validate_input(self):
    if self.validation_fn:
        success, err = self.validation_fn(self.str)
        if not success:
            spaces = "" "" * self.textwin_width
            self.textwin.addstr(self.y + 2, 0, spaces)
            self.textwin.addstr(self.y + 2, 0, err, curses.color_pair(4))
        return success
    else:
        return True
",if not success :,120
"def start_prompt(self):
    """"""Start the interpreter.""""""
    logger.show(""Coconut Interpreter:"")
    logger.show(""(type 'exit()' or press Ctrl-D to end)"")
    self.start_running()
    while self.running:
        try:
            code = self.get_input()
            if code:
                compiled = self.handle_input(code)
                if compiled:
                    self.execute(compiled, use_eval=None)
        except KeyboardInterrupt:
            printerr(""\nKeyboardInterrupt"")
",if compiled :,142
"def __exit__(self, exc_type, exc_val, exc_tb):
    if self.channel and self.channel.connection:
        conn_errors = self.channel.connection.client.connection_errors
        if not isinstance(exc_val, conn_errors):
            try:
                self.cancel()
            except Exception:
                pass
","if not isinstance ( exc_val , conn_errors ) :",93
"def pack(data, size, endian):
    buf = []
    for i in data:
        num = int(i)
        if num < 0:
            num += 1 << (size * 8)
        d = [b""\x00""] * size
        i = size - 1
        while i >= 0:
            b = num & 255
            d[i] = bytes((b,)) if PY3 else chr(b)
            num >>= 8
            i -= 1
        if endian == ""<"":
            d = b"""".join(d[i : i + 1][0] for i in reversed(xrange(len(d))))
        else:
            d = b"""".join(d)
        buf.append(d)
    return b"""".join(buf)
",if num < 0 :,198
"def _sample_new_noise_and_add(self, *, tf_sess=None, override=False):
    if self.framework == ""tf"":
        if override and self.weights_are_currently_noisy:
            tf_sess.run(self.tf_remove_noise_op)
        tf_sess.run(self.tf_sample_new_noise_and_add_op)
    else:
        if override and self.weights_are_currently_noisy:
            self._remove_noise()
        self._sample_new_noise()
        self._add_stored_noise()
    self.weights_are_currently_noisy = True
",if override and self . weights_are_currently_noisy :,162
"def hdfs_link_js(url):
    link = ""javascript:void(0)""
    if url:
        path = Hdfs.urlsplit(url)[2]
        if path:
            link = (
                ""/filebrowser/view=%s""
                if path.startswith(posixpath.sep)
                else ""/filebrowser/home_relative_view=/%s""
            ) % path
    return link
",if path . startswith ( posixpath . sep ),111
"def set_xticklabels(self, labels=None, step=None, **kwargs):
    """"""Set x axis tick labels on the bottom row of the grid.""""""
    for ax in self.axes[-1, :]:
        if labels is None:
            labels = [l.get_text() for l in ax.get_xticklabels()]
            if step is not None:
                xticks = ax.get_xticks()[::step]
                labels = labels[::step]
                ax.set_xticks(xticks)
        ax.set_xticklabels(labels, **kwargs)
    return self
",if step is not None :,145
"def _get_statement_from_file(user, fs, snippet):
    script_path = snippet[""statementPath""]
    if script_path:
        script_path = script_path.replace(""hdfs://"", """")
        if fs.do_as_user(user, fs.isfile, script_path):
            return fs.do_as_user(user, fs.read, script_path, 0, 16 * 1024 ** 2)
","if fs . do_as_user ( user , fs . isfile , script_path ) :",104
"def doWorkUnit(self):
    if len(self.workers):
        try:
            w = self.workers.popleft()
            w.next()
            self.workers.append(w)
        except StopIteration:
            if hasattr(w, ""needsRedraw"") and w.needsRedraw:
                self.invalidate()
    else:
        time.sleep(0.001)
","if hasattr ( w , ""needsRedraw"" ) and w . needsRedraw :",105
"def _find_l1_phash_mul(cdict):
    candidate_lengths = _find_candidate_lengths_mul(cdict.tuple2int)
    for p in candidate_lengths:
        hash_f = hashmul.hashmul_t(p)
        if hash_f.is_perfect(iter(cdict.tuple2int.values())):
            return l1_phash_t(cdict, hash_f)
        del hash_f
    return None
",if hash_f . is_perfect ( iter ( cdict . tuple2int . values ( ) ) ) :,118
"def _find_next_tab_stop(self, direction):
    old_focus = self._focus
    self._focus += direction
    while self._focus != old_focus:
        if self._focus < 0:
            self._focus = len(self._layouts) - 1
        if self._focus >= len(self._layouts):
            self._focus = 0
        try:
            if direction > 0:
                self._layouts[self._focus].focus(force_first=True)
            else:
                self._layouts[self._focus].focus(force_last=True)
            break
        except IndexError:
            self._focus += direction
",if direction > 0 :,177
"def _get_py_flags(self):
    res = dict(self.flags)
    cflags = res.pop(""cflags"", """")
    for fl in cflags.split(""|""):
        fl = fl.strip()
        if fl == ""GA_USE_DOUBLE"":
            res[""have_double""] = True
        if fl == ""GA_USE_SMALL"":
            res[""have_small""] = True
        if fl == ""GA_USE_COMPLEX"":
            res[""have_complex""] = True
        if fl == ""GA_USE_HALF"":
            res[""have_half""] = True
    return res
","if fl == ""GA_USE_SMALL"" :",160
"def _install_provision_configs(self):
    config = self._config.plugins[self.full_name]
    files = config.get(""provision_config_files"", [])
    if files:
        if not install_provision_configs(files, self._mountpoint):
            log.critical(""Error installing provisioning configs"")
            return False
        else:
            log.debug(""Provision config files successfully installed"")
            return True
    else:
        log.debug(""No provision config files configured"")
        return True
","if not install_provision_configs ( files , self . _mountpoint ) :",132
"def postfile(self):
    for clientip, serverips in self.client_conns.items():
        target_count = len(serverips)
        S = min((len(self.server_conns[serverip]) for serverip in serverips))
        if S > 2 or target_count < 5:
            continue
        # TODO implement whitelist
        self.write(
            ""Scanning IP: {} / S score: {:.1f} / Number of records: {}"".format(
                clientip, S, target_count
            )
        )
",if S > 2 or target_count < 5 :,144
"def update_defaults(self, *values, **kwargs):
    for value in values:
        if type(value) == dict:
            self.DEFAULT_CONFIGURATION.update(value)
        elif isinstance(value, types.ModuleType):
            self.__defaults_from_module(value)
        elif isinstance(value, str):
            if os.path.exists(value):
                self.__defaults_from_file(value)
            else:
                logger.warning(""Configuration file {} does not exist."".format(value))
        elif isinstance(value, type(None)):
            pass
        else:
            raise ValueError(""Cannot interpret {}"".format(value))
    self.DEFAULT_CONFIGURATION.update(kwargs)
",if type ( value ) == dict :,184
"def __init__(self, aList):
    for element in aList:
        if len(element) > 0:
            if element.tag == element[0].tag:
                self.append(ListParser(element))
            else:
                self.append(DictParser(element))
        elif element.text:
            text = element.text.strip()
            if text:
                self.append(text)
",if len ( element ) > 0 :,116
"def _get_py_flags(self):
    res = dict(self.flags)
    cflags = res.pop(""cflags"", """")
    for fl in cflags.split(""|""):
        fl = fl.strip()
        if fl == ""GA_USE_DOUBLE"":
            res[""have_double""] = True
        if fl == ""GA_USE_SMALL"":
            res[""have_small""] = True
        if fl == ""GA_USE_COMPLEX"":
            res[""have_complex""] = True
        if fl == ""GA_USE_HALF"":
            res[""have_half""] = True
    return res
","if fl == ""GA_USE_DOUBLE"" :",160
"def consume_bytes(data):
    state_machine.receive_data(data)
    while True:
        event = state_machine.next_event()
        if event is h11.NEED_DATA:
            break
        elif isinstance(event, h11.InformationalResponse):
            # Ignore 1xx responses
            continue
        elif isinstance(event, h11.Response):
            # We have our response! Save it and get out of here.
            context[""h11_response""] = event
            raise LoopAbort
        else:
            # Can't happen
            raise RuntimeError(""Unexpected h11 event {}"".format(event))
",if event is h11 . NEED_DATA :,166
"def status_string(self):
    if not self.live:
        if self.expired:
            return _(""expired"")
        elif self.approved_schedule:
            return _(""scheduled"")
        elif self.workflow_in_progress:
            return _(""in moderation"")
        else:
            return _(""draft"")
    else:
        if self.approved_schedule:
            return _(""live + scheduled"")
        elif self.workflow_in_progress:
            return _(""live + in moderation"")
        elif self.has_unpublished_changes:
            return _(""live + draft"")
        else:
            return _(""live"")
",elif self . has_unpublished_changes :,166
"def _update_input_entries(entries):
    for entry in entries:
        comma = entry.get(""comma_separated"", False)
        if comma:
            entry[""regex""] = r""([^{}\[\]]*)\{"" + entry[""regex""]
        else:
            entry[""regex""] = r""([^,{}\[\]]*)\{"" + entry[""regex""]
        entry[""type""] = ""input""
",if comma :,98
"def get_release():
    regexp = re.compile(r""^__version__\W*=\W*'([\d.abrc]+)'"")
    here = os.path.dirname(__file__)
    root = os.path.dirname(here)
    init_py = os.path.join(root, ""aiomysql"", ""__init__.py"")
    with open(init_py) as f:
        for line in f:
            match = regexp.match(line)
            if match is not None:
                return match.group(1)
        else:
            raise RuntimeError(""Cannot find version in aiomysql/__init__.py"")
",if match is not None :,151
"def add_to_auto_transitions(cls, base):
    result = {}
    for name, method in base.__dict__.items():
        if callable(method) and hasattr(method, ""_django_fsm""):
            for name, transition in method._django_fsm.transitions.items():
                if transition.custom.get(""auto""):
                    result.update({name: method})
    return result
","if transition . custom . get ( ""auto"" ) :",103
"def _paginate(self, get_page, page_size):
    for page in itertools.count(start=1):
        params = {""page"": page, ""per_page"": page_size}
        response, items = get_page(params)
        for item in items:
            yield item
        if self._is_last_page(response):
            break
        if len(items) < page_size:
            break
",if len ( items ) < page_size :,111
"def forward(self, x):
    bs = x.size(0)
    cur = self.stem(x)
    layers = [cur]
    for layer_id in range(self.num_layers):
        cur = self.layers[layer_id](layers)
        layers.append(cur)
        if layer_id in self.pool_layers_idx:
            for i, layer in enumerate(layers):
                layers[i] = self.pool_layers[self.pool_layers_idx.index(layer_id)](
                    layer
                )
            cur = layers[-1]
    cur = self.gap(cur).view(bs, -1)
    cur = self.dropout(cur)
    logits = self.dense(cur)
    return logits
",if layer_id in self . pool_layers_idx :,198
"def evaluate(self, x, y, z):
    vertex = Vector((x, y, z))
    nearest, normal, idx, distance = self.bvh.find_nearest(vertex)
    if self.use_normal:
        if self.signed_normal:
            sign = (v - nearest).dot(normal)
            sign = copysign(1, sign)
        else:
            sign = 1
        return sign * np.array(normal)
    else:
        dv = np.array(nearest - vertex)
        if self.falloff is not None:
            norm = np.linalg.norm(dv)
            len = self.falloff(norm)
            dv = len * dv
            return dv
        else:
            return dv
",if self . signed_normal :,200
"def to_terminal(self):
    """"""Yield lines to be printed to a terminal.""""""
    for name, mi in self._sort(self.filtered_results):
        if ""error"" in mi:
            yield name, (mi[""error""],), {""error"": True}
            continue
        rank = mi[""rank""]
        color = MI_RANKS[rank]
        to_show = """"
        if self.config.show:
            to_show = "" ({0:.2f})"".format(mi[""mi""])
        yield ""{0} - {1}{2}{3}{4}"", (name, color, rank, to_show, RESET), {}
","if ""error"" in mi :",163
"def _get_widget_by_name(self, container, name):
    """"""Recursively search to return the named child widget.""""""
    LOGGER.log()
    children = container.get_children()
    for child in children:
        if child.name == name:
            return child
        if isinstance(child, gtk.Container):
            found_child = self._get_widget_by_name(child, name)
            if found_child:
                return found_child
",if found_child :,122
"def PyJsHoisted_hasComputed_(mutatorMap, this, arguments, var=var):
    var = Scope(
        {u""this"": this, u""arguments"": arguments, u""mutatorMap"": mutatorMap}, var
    )
    var.registers([u""mutatorMap"", u""key""])
    for PyJsTemp in var.get(u""mutatorMap""):
        var.put(u""key"", PyJsTemp)
        if var.get(u""mutatorMap"").get(var.get(u""key"")).get(u""_computed""):
            return var.get(u""true"")
    return Js(False)
","if var . get ( u""mutatorMap"" ) . get ( var . get ( u""key"" ) ) . get ( u""_computed"" ) :",158
"def get_result_json_path(self):
    if self._result_json_path is None:
        if self.envconfig.config.option.resultjson:
            self._result_json_path = get_unique_file(
                self.path,
                PARALLEL_RESULT_JSON_PREFIX,
                PARALLEL_RESULT_JSON_SUFFIX,
            )
    return self._result_json_path
",if self . envconfig . config . option . resultjson :,113
"def timer(ratio, step, additive):
    t = 0
    slowmode = False
    while 1:
        if additive:
            slowmode |= bool((yield t))
        else:
            slowmode = bool((yield t))
        if slowmode:
            t += step * ratio
        else:
            t += step
",if slowmode :,89
"def _split_long_text(text, idx, size):
    splited_text = text.split()
    if len(splited_text) > 25:
        if idx == 0:
            # The first is (...)text
            first = """"
        else:
            first = "" "".join(splited_text[:10])
        if idx != 0 and idx == size - 1:
            # The last is text(...)
            last = """"
        else:
            last = "" "".join(splited_text[-10:])
        return ""{}(...){}"".format(first, last)
    return text
",if idx != 0 and idx == size - 1 :,156
"def test_tag_priority(self):
    for tag in _low_priority_D_TAG:
        val = ENUM_D_TAG[tag]
        # if the low priority tag is present in the descriptions,
        # assert that it has not overridden any other tag
        if _DESCR_D_TAG[val] == tag:
            for tag2 in ENUM_D_TAG:
                if tag2 == tag:
                    continue
                self.assertNotEqual(ENUM_D_TAG[tag2], val)
",if _DESCR_D_TAG [ val ] == tag :,135
"def _concretize(self, n_cls, t1, t2, join_or_meet, translate):
    ptr_class = self._pointer_class()
    if n_cls is ptr_class:
        if isinstance(t1, ptr_class) and isinstance(t2, ptr_class):
            # we need to merge them
            return ptr_class(join_or_meet(t1.basetype, t2.basetype, translate))
        if isinstance(t1, ptr_class):
            return t1
        elif isinstance(t2, ptr_class):
            return t2
        else:
            # huh?
            return ptr_class(BottomType())
    return n_cls()
","if isinstance ( t1 , ptr_class ) and isinstance ( t2 , ptr_class ) :",181
"def parse(self, html: HTML) -> [ProxyIP]:
    ip_list: [ProxyIP] = []
    for ip_row in html.find(""table.proxytbl tr""):
        ip_element = ip_row.find(""td:nth-child(1)"", first=True)
        port_element = ip_row.find(""td:nth-child(2)"", first=True)
        try:
            if ip_element and port_element:
                port_str = re.search(r""//]]> (\d+)"", port_element.text).group(1)
                p = ProxyIP(ip=ip_element.text, port=port_str)
                ip_list.append(p)
        except AttributeError:
            pass
    return ip_list
",if ip_element and port_element :,197
"def _reformat(self):
    document = self.suggestions.document()
    cursor = self.suggestions.textCursor()
    block = document.begin()
    style_format = {
        self.STYLE_TRANSLATION: self._translation_char_format,
        self.STYLE_STROKES: self._strokes_char_format,
    }
    while block != document.end():
        style = block.userState()
        fmt = style_format.get(style)
        if fmt is not None:
            cursor.setPosition(block.position())
            cursor.select(QTextCursor.BlockUnderCursor)
            cursor.setCharFormat(fmt)
        block = block.next()
",if fmt is not None :,174
"def check_uncore_event(e):
    if uncore_exists(e.unit):
        if e.cmask and not uncore_exists(e.unit, ""/format/cmask""):
            warn_once(""Uncore unit "" + e.unit + "" missing cmask for "" + e.name)
            return None
        if e.umask and not uncore_exists(e.unit, ""/format/umask""):
            warn_once(""Uncore unit "" + e.unit + "" missing umask for "" + e.name)
            return None
        return e
    if e.unit not in missing_boxes:
        warn_once(""Uncore unit "" + e.unit + "" missing"")
        missing_boxes.add(e.unit)
    return None
","if e . cmask and not uncore_exists ( e . unit , ""/format/cmask"" ) :",192
"def check(ip, port, timeout):
    try:
        socket.setdefaulttimeout(timeout)
        s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
        s.connect((ip, int(port)))
        flag = ""envi""
        # envi
        # dump
        # reqs
        # ruok
        # stat
        s.send(flag)
        data = s.recv(1024)
        s.close()
        if ""Environment"" in data:
            return u""Zookeeper Unauthorized access""
    except:
        pass
","if ""Environment"" in data :",154
"def getid(self):
    uid = u""""
    try:
        filename = (
            self.xmlelement.iterancestors(self.namespaced(""file""))
            .next()
            .get(""original"")
        )
        if filename:
            uid = filename + ID_SEPARATOR
    except StopIteration:
        # unit has no proper file ancestor, probably newly created
        pass
    # hide the fact that we sanitize ID_SEPERATOR
    uid += unicode(self.xmlelement.get(""id"") or u"""").replace(
        ID_SEPARATOR_SAFE, ID_SEPARATOR
    )
    return uid
",if filename :,164
"def identify(self, vivisect_workspace, function_vas):
    candidate_functions = {}
    for fva in function_vas:
        fname = vivisect_workspace.getName(fva)
        default_name = ""sub_%.8x"" % fva
        if fname != default_name:
            self.d(""Identified %s at VA 0x%08X "" % (fname, fva))
            candidate_functions[fva] = True
    return candidate_functions
",if fname != default_name :,123
"def nud(self):
    self.first = []
    comma = False
    if self.token.id != "")"":
        while 1:
            if self.token.id == "")"":
                break
            self.first.append(self.expression())
            if self.token.id == "","":
                comma = True
                self.advance("","")
            else:
                break
    self.advance("")"")
    if not self.first or comma:
        return self  # tuple
    else:
        return self.first[0]
","if self . token . id == "")"" :",146
"def allow_syncdb(self, db, model):
    for router in self.routers:
        try:
            method = router.allow_syncdb
        except AttributeError:
            # If the router doesn't have a method, skip to the next one.
            pass
        else:
            allow = method(db, model)
            if allow is not None:
                return allow
    return True
",if allow is not None :,110
"def status_string(self):
    if not self.live:
        if self.expired:
            return _(""expired"")
        elif self.approved_schedule:
            return _(""scheduled"")
        elif self.workflow_in_progress:
            return _(""in moderation"")
        else:
            return _(""draft"")
    else:
        if self.approved_schedule:
            return _(""live + scheduled"")
        elif self.workflow_in_progress:
            return _(""live + in moderation"")
        elif self.has_unpublished_changes:
            return _(""live + draft"")
        else:
            return _(""live"")
",elif self . workflow_in_progress :,166
"def _on_config_changed(changed_name: str) -> None:
    """"""Call config_changed hooks if the config changed.""""""
    for mod_info in _module_infos:
        if mod_info.skip_hooks:
            continue
        for option, hook in mod_info.config_changed_hooks:
            if option is None:
                hook()
            else:
                cfilter = config.change_filter(option)
                cfilter.validate()
                if cfilter.check_match(changed_name):
                    hook()
",if option is None :,151
"def test_slowest_interrupted(self):
    # Issue #25373: test --slowest with an interrupted test
    code = TEST_INTERRUPTED
    test = self.create_test(""sigint"", code=code)
    for multiprocessing in (False, True):
        with self.subTest(multiprocessing=multiprocessing):
            if multiprocessing:
                args = (""--slowest"", ""-j2"", test)
            else:
                args = (""--slowest"", test)
            output = self.run_tests(*args, exitcode=130)
            self.check_executed_tests(output, test, omitted=test, interrupted=True)
            regex = ""10 slowest tests:\n""
            self.check_line(output, regex)
",if multiprocessing :,190
"def insert_files(self, urls, pos):
    """"""Not only images""""""
    image_extensions = ["".png"", "".jpg"", "".bmp"", "".gif""]
    for url in urls:
        if url.scheme() == ""file"":
            path = url.path()
            ext = os.path.splitext(path)[1]
            if os.path.exists(path) and ext in image_extensions:
                self._insert_image_from_path(path)
            else:
                self.parent.resource_edit.add_attach(path)
","if url . scheme ( ) == ""file"" :",144
"def _model_shorthand(self, args):
    accum = []
    for arg in args:
        if isinstance(arg, Node):
            accum.append(arg)
        elif isinstance(arg, Query):
            accum.append(arg)
        elif isinstance(arg, ModelAlias):
            accum.extend(arg.get_proxy_fields())
        elif isclass(arg) and issubclass(arg, Model):
            accum.extend(arg._meta.declared_fields)
    return accum
","elif isinstance ( arg , ModelAlias ) :",125
"def get_identifiers(self):
    ids = []
    ifaces = [i[""name""] for i in self.middleware.call_sync(""interface.query"")]
    for entry in glob.glob(f""{self._base_path}/interface-*""):
        ident = entry.rsplit(""-"", 1)[-1]
        if ident not in ifaces:
            continue
        if os.path.exists(os.path.join(entry, ""if_octets.rrd"")):
            ids.append(ident)
    ids.sort(key=RRDBase._sort_disks)
    return ids
","if os . path . exists ( os . path . join ( entry , ""if_octets.rrd"" ) ) :",143
"def _validate_required_settings(
    self, application_id, application_config, required_settings, should_throw=True
):
    """"""All required keys must be present""""""
    for setting_key in required_settings:
        if setting_key not in application_config.keys():
            if should_throw:
                raise ImproperlyConfigured(
                    MISSING_SETTING.format(
                        application_id=application_id, setting=setting_key
                    )
                )
            else:
                return False
    return True
",if setting_key not in application_config . keys ( ) :,146
"def digests():
    if not OpenVPN.DIGESTS:
        proc = subprocess.Popen(
            [""openvpn"", ""--show-digests""],
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE,
        )
        stdout, stderr = proc.communicate()
        if not proc.returncode:
            OpenVPN.DIGESTS = {
                v.split("" "")[0].strip(): v.split("" "", 1)[1].strip()
                for v in filter(
                    lambda v: v and v.endswith(""bit digest size""),
                    stdout.decode(""utf8"").split(""\n""),
                )
            }
    return OpenVPN.DIGESTS
",if not proc . returncode :,188
"def iterate_demo_dirs(dir_name, env_name):
    for env_file_name in glob.glob(
        os.path.join(dir_name, ""**"", ""env_id.txt""), recursive=True
    ):
        with open(env_file_name, ""r"", encoding=""utf-8"") as fd:
            dir_env_name = fd.readline()
            if dir_env_name != env_name:
                continue
        yield os.path.dirname(env_file_name)
",if dir_env_name != env_name :,132
"def validate_rights(namespace):
    if ""Manage"" in namespace.rights:
        if ""Listen"" not in namespace.rights or ""Send"" not in namespace.rights:
            raise CLIError(
                ""Error : Assigning 'Manage' to --rights requires 'Listen' and 'Send' to be included with. e.g. --rights Manage Send Listen""
            )
","if ""Listen"" not in namespace . rights or ""Send"" not in namespace . rights :",95
"def apply_patches(ctx, patched=False, pre=False):
    if patched:
        vendor_dir = _get_patched_dir(ctx)
    else:
        vendor_dir = _get_vendor_dir(ctx)
    log(""Applying pre-patches..."")
    patch_dir = Path(__file__).parent / ""patches"" / vendor_dir.name
    if pre:
        if not patched:
            pass
        for patch in patch_dir.glob(""*.patch""):
            if not patch.name.startswith(""_post""):
                apply_patch(ctx, patch)
    else:
        patches = patch_dir.glob(""*.patch"" if not patched else ""_post*.patch"")
        for patch in patches:
            apply_patch(ctx, patch)
",if not patched :,191
"def log_sock(s, event_type=None):
    if sock_silent:
        pass
    else:
        if event_type is None:
            logsocket.sendto(ensure_str(s), (host, port))
        elif event_type in show_event:
            logsocket.sendto(ensure_str(s), (host, port))
        else:
            pass
",if event_type is None :,103
"def replace_params(
    path: str,
    param_convertors: typing.Dict[str, Convertor],
    path_params: typing.Dict[str, str],
) -> typing.Tuple[str, dict]:
    for key, value in list(path_params.items()):
        if ""{"" + key + ""}"" in path:
            convertor = param_convertors[key]
            value = convertor.to_string(value)
            path = path.replace(""{"" + key + ""}"", value)
            path_params.pop(key)
    return path, path_params
","if ""{"" + key + ""}"" in path :",145
"def data(self, index: QModelIndex, role=Qt.DisplayRole):
    if not index.isValid():
        return None
    if role == Qt.DisplayRole or role == Qt.EditRole:
        i = index.row()
        j = index.column()
        fieldtype = self.field_types[i]
        if j == 0:
            return fieldtype.caption
        elif j == 1:
            return fieldtype.function.name
        elif j == 2:
            return ProtocolLabel.DISPLAY_FORMATS[fieldtype.display_format_index]
",elif j == 2 :,142
"def delta_page(self, x: float = 0.0, y: float = 0.0) -> None:
    if y.is_integer():
        y = int(y)
        if y == 0:
            pass
        elif y < 0:
            self.page_up(count=-y)
        elif y > 0:
            self.page_down(count=y)
        y = 0
    if x == 0 and y == 0:
        return
    size = self._widget.page().mainFrame().geometry()
    self.delta(int(x * size.width()), int(y * size.height()))
",elif y > 0 :,160
"def _process_symbols(self, tokens):
    opening_paren = False
    for index, token, value in tokens:
        if opening_paren and token in (Literal, Name.Variable):
            token = self.MAPPINGS.get(value, Name.Function)
        elif token == Literal and value in self.BUILTINS_ANYWHERE:
            token = Name.Builtin
        opening_paren = value == ""("" and token == Punctuation
        yield index, token, value
","if opening_paren and token in ( Literal , Name . Variable ) :",115
"def ext_service(self, entity_id, typ, service, binding=None):
    known_entity = False
    for key, _md in self.metadata.items():
        srvs = _md.ext_service(entity_id, typ, service, binding)
        if srvs:
            return srvs
        elif srvs is None:
            pass
        else:
            known_entity = True
    if known_entity:
        raise UnsupportedBinding(binding)
    else:
        raise UnknownSystemEntity(entity_id)
",if srvs :,138
"def find_library_nt(name):
    # modified from ctypes.util
    # ctypes.util.find_library just returns first result he found
    # but we want to try them all
    # because on Windows, users may have both 32bit and 64bit version installed
    results = []
    for directory in os.environ[""PATH""].split(os.pathsep):
        fname = os.path.join(directory, name)
        if os.path.isfile(fname):
            results.append(fname)
        if fname.lower().endswith("".dll""):
            continue
        fname = fname + "".dll""
        if os.path.isfile(fname):
            results.append(fname)
    return results
",if os . path . isfile ( fname ) :,174
"def getRemovedFiles(oldContents, newContents, destinationFolder):
    toRemove = []
    for filename in list(oldContents.keys()):
        if filename not in newContents:
            destFile = os.path.join(destinationFolder, filename.lstrip(""/""))
            if os.path.isfile(destFile):
                toRemove.append(filename)
    return toRemove
",if os . path . isfile ( destFile ) :,95
"def escapeall(self, lines):
    ""Escape all lines in an array according to the output options.""
    result = []
    for line in lines:
        if Options.html:
            line = self.escape(line, EscapeConfig.html)
        if Options.iso885915:
            line = self.escape(line, EscapeConfig.iso885915)
            line = self.escapeentities(line)
        elif not Options.unicode:
            line = self.escape(line, EscapeConfig.nonunicode)
        result.append(line)
    return result
",if Options . html :,143
"def body(self):
    order = [
        ""ok_header"",
        ""affected_rows"",
        ""last_insert_id"",
        ""server_status"",
        ""warning_count"",
        ""state_track"",
        ""info"",
    ]
    string = b""""
    for key in order:
        item = getattr(self, key)
        section_pack = b""""
        if item is None:
            continue
        elif isinstance(item, bytes):
            section_pack = item
        else:
            section_pack = getattr(self, key).toStringPacket()
        string += section_pack
    self.setBody(string)
    return self._body
","elif isinstance ( item , bytes ) :",182
"def _get_instantiation(self):
    if self._data is None:
        f, l, c, o = c_object_p(), c_uint(), c_uint(), c_uint()
        conf.lib.clang_getInstantiationLocation(
            self, byref(f), byref(l), byref(c), byref(o)
        )
        if f:
            f = File(f)
        else:
            f = None
        self._data = (f, int(l.value), int(c.value), int(o.value))
    return self._data
",if f :,152
"def analyze_items(items, category_id, agg_data):
    for item in items:
        if not agg_data[""cat_asp""].get(category_id, None):
            agg_data[""cat_asp""][category_id] = []
        agg_data[""cat_asp""][category_id].append(
            float(item.sellingStatus.currentPrice.value)
        )
        if getattr(item.listingInfo, ""watchCount"", None):
            agg_data[""watch_count""] += int(item.listingInfo.watchCount)
        if getattr(item, ""postalCode"", None):
            agg_data[""postal_code""] = item.postalCode
","if not agg_data [ ""cat_asp"" ] . get ( category_id , None ) :",169
"def mock_default_data_dir(tmp_path: pathlib.Path):
    """"""Changes the default `--data_dir` to tmp_path.""""""
    tmp_path = tmp_path / ""datasets""
    default_data_dir = os.environ.get(""TFDS_DATA_DIR"")
    try:
        os.environ[""TFDS_DATA_DIR""] = os.fspath(tmp_path)
        yield tmp_path
    finally:
        if default_data_dir:
            os.environ[""TFDS_DATA_DIR""] = default_data_dir
        else:
            del os.environ[""TFDS_DATA_DIR""]
",if default_data_dir :,158
"def has_valid_checksum(self, number):
    given_number, given_checksum = number[:-1], number[-1]
    calculated_checksum = 0
    parameter = 7
    for item in given_number:
        fragment = str(int(item) * parameter)
        if fragment.isalnum():
            calculated_checksum += int(fragment[-1])
        if parameter == 1:
            parameter = 7
        elif parameter == 3:
            parameter = 1
        elif parameter == 7:
            parameter = 3
    return str(calculated_checksum)[-1] == given_checksum
",if fragment . isalnum ( ) :,147
"def _cleanup_volumes(self, context, instance_id):
    bdms = self.db.block_device_mapping_get_all_by_instance(context, instance_id)
    for bdm in bdms:
        LOG.debug(_(""terminating bdm %s"") % bdm)
        if bdm[""volume_id""] and bdm[""delete_on_termination""]:
            volume = self.volume_api.get(context, bdm[""volume_id""])
            self.volume_api.delete(context, volume)
","if bdm [ ""volume_id"" ] and bdm [ ""delete_on_termination"" ] :",130
"def _split_zipped_payload(self, packet_bunch):
    """"""Split compressed payload""""""
    while packet_bunch:
        if PY2:
            payload_length = struct.unpack_from(""<I"", packet_bunch[0:3] + b""\x00"")[
                0
            ]  # pylint: disable=E0602
        else:
            payload_length = struct.unpack(""<I"", packet_bunch[0:3] + b""\x00"")[0]
        self._packet_queue.append(packet_bunch[0 : payload_length + 4])
        packet_bunch = packet_bunch[payload_length + 4 :]
",if PY2 :,169
"def TryMerge(self, d):
    while d.avail() > 0:
        tt = d.getVarInt32()
        if tt == 10:
            self.set_application_key(d.getPrefixedString())
            continue
        if tt == 18:
            self.set_message(d.getPrefixedString())
            continue
        if tt == 26:
            self.set_tag(d.getPrefixedString())
            continue
        if tt == 0:
            raise ProtocolBuffer.ProtocolBufferDecodeError
        d.skipData(tt)
",if tt == 10 :,152
"def update_transitive(self, conanfile):
    transitive = getattr(conanfile, ""python_requires"", None)
    if not transitive:
        return
    for name, transitive_py_require in transitive.all_items():
        existing = self._pyrequires.get(name)
        if existing and existing.ref != transitive_py_require.ref:
            raise ConanException(
                ""Conflict in py_requires %s - %s""
                % (existing.ref, transitive_py_require.ref)
            )
        self._transitive[name] = transitive_py_require
",if existing and existing . ref != transitive_py_require . ref :,153
"def call(cls, func, *args):
    try:
        f = cls._func_cache[func]
    except KeyError:
        if IS_NVIM:
            f = cls._func_cache[func] = getattr(vim.funcs, func)
        else:
            f = cls._func_cache[func] = vim.Function(func)
    return f(*args)
",if IS_NVIM :,100
"def __call__(self, *args, **kwargs):
    if self is S:
        if args:
            raise TypeError(""S() takes no positional arguments, got: %r"" % (args,))
        if not kwargs:
            raise TypeError(""S() expected at least one kwarg, got none"")
        # TODO: typecheck kwarg vals?
    return _t_child(self, ""("", (args, kwargs))
",if not kwargs :,101
"def tiles_around_factor(self, factor, pos, radius=1, predicate=None):
    ps = []
    x, y = pos
    for dx in range(-radius, radius + 1):
        nx = x + dx
        if nx >= 0 and nx < self.width * factor:
            for dy in range(-radius, radius + 1):
                ny = y + dy
                if ny >= 0 and ny < self.height * factor and (dx != 0 or dy != 0):
                    if predicate is None or predicate((nx, ny)):
                        ps.append((nx, ny))
    return ps
","if predicate is None or predicate ( ( nx , ny ) ) :",159
"def _plugin_get_requirements(self, requirements_iter):
    plugin_requirements = {""platform"": [], ""python"": [], ""network"": [], ""native"": []}
    # parse requirements
    for requirement in requirements_iter:
        key = requirement[0]
        values = requirement[1]
        if isinstance(values, str) or isinstance(values, bool):
            values = [values]
        if key in plugin_requirements:
            plugin_requirements[key].extend(values)
        else:
            warning(""{}={}: No supported requirement"".format(key, values))
    return plugin_requirements
","if isinstance ( values , str ) or isinstance ( values , bool ) :",148
"def test_engine_api_sdl(sdl, expected, pass_to, clean_registry):
    from tartiflette import Engine
    if pass_to == ""engine"":
        e = Engine(sdl)
    else:
        e = Engine()
    if isinstance(expected, Exception):
        with pytest.raises(Exception):
            if pass_to == ""cook"":
                await e.cook(sdl)
            else:
                await e.cook()
    else:
        if pass_to == ""cook"":
            await e.cook(sdl)
        else:
            await e.cook()
        assert e._schema is not None
","if pass_to == ""cook"" :",175
"def update(self, other_dict, option_parser):
    if isinstance(other_dict, Values):
        other_dict = other_dict.__dict__
    other_dict = other_dict.copy()
    for setting in option_parser.lists.keys():
        if hasattr(self, setting) and setting in other_dict:
            value = getattr(self, setting)
            if value:
                value += other_dict[setting]
                del other_dict[setting]
    self._update_loose(other_dict)
",if value :,137
"def _cast_Time(iso, curs):
    if iso:
        if iso in [""-infinity"", ""infinity""]:
            return iso
        else:
            return DateTime(
                time.strftime(
                    ""%Y-%m-%d %H:%M:%S"",
                    time.localtime(time.time())[:3]
                    + time.strptime(iso[:8], ""%H:%M:%S"")[3:],
                )
            )
","if iso in [ ""-infinity"" , ""infinity"" ] :",125
"def _get_default_urlpatterns(self):
    package_string = ""."".join(self.__module__.split(""."")[:-1])
    if getattr(self, ""urls"", None):
        try:
            mod = import_module("".%s"" % self.urls, package_string)
        except ImportError:
            mod = import_module(self.urls)
        urlpatterns = mod.urlpatterns
    else:
        # Try importing a urls.py from the dashboard package
        if module_has_submodule(import_module(package_string), ""urls""):
            urls_mod = import_module("".urls"", package_string)
            urlpatterns = urls_mod.urlpatterns
        else:
            urlpatterns = patterns("""")
    return urlpatterns
","if module_has_submodule ( import_module ( package_string ) , ""urls"" ) :",180
"def escape2null(text):
    """"""Return a string with escape-backslashes converted to nulls.""""""
    parts = []
    start = 0
    while 1:
        found = text.find(""\\"", start)
        if found == -1:
            parts.append(text[start:])
            return """".join(parts)
        parts.append(text[start:found])
        parts.append(""\x00"" + text[found + 1 : found + 2])
        start = found + 2  # skip character after escape
",if found == - 1 :,129
"def check(self, obj):
    if ""*"" in self.states:
        return {""state"": self.dispatcher.current_state()}
    try:
        state = self.ctx_state.get()
    except LookupError:
        chat, user = self.get_target(obj)
        if chat or user:
            state = await self.dispatcher.storage.get_state(chat=chat, user=user)
            self.ctx_state.set(state)
            if state in self.states:
                return {""state"": self.dispatcher.current_state(), ""raw_state"": state}
    else:
        if state in self.states:
            return {""state"": self.dispatcher.current_state(), ""raw_state"": state}
    return False
",if state in self . states :,192
"def get_tokens_unprocessed(self, text):
    from pygments.lexers._asy_builtins import ASYFUNCNAME, ASYVARNAME
    for index, token, value in RegexLexer.get_tokens_unprocessed(self, text):
        if token is Name and value in ASYFUNCNAME:
            token = Name.Function
        elif token is Name and value in ASYVARNAME:
            token = Name.Variable
        yield index, token, value
",elif token is Name and value in ASYVARNAME :,113
"def write_family_handle(self, family, index=1):
    sp = ""  "" * index
    self.write_primary_tag(""family"", family, index)
    if family:
        rel = escxml(family.get_relationship().xml_str())
        if rel != """":
            self.g.write('  %s<rel type=""%s""/>\n' % (sp, rel))
","if rel != """" :",99
"def pop1_bytes(self) -> bytes:
    #
    # Note: This function is optimized for speed over readability.
    # Knowing the popped type means that we can pop *very* quickly
    # when the popped type matches the pushed type.
    #
    if not self.values:
        raise InsufficientStack(""Wanted 1 stack item as bytes, had none"")
    else:
        item_type, popped = self._pop_typed()
        if item_type is int:
            return int_to_big_endian(popped)  # type: ignore
        elif item_type is bytes:
            return popped  # type: ignore
        else:
            raise _busted_type(item_type, popped)
",elif item_type is bytes :,182
"def setDefaultComponents(self):
    if self._componentTypeLen == self._componentValuesSet:
        return
    idx = self._componentTypeLen
    while idx:
        idx = idx - 1
        if self._componentType[idx].isDefaulted:
            if self.getComponentByPosition(idx) is None:
                self.setComponentByPosition(idx)
        elif not self._componentType[idx].isOptional:
            if self.getComponentByPosition(idx) is None:
                raise error.PyAsn1Error(
                    ""Uninitialized component #%s at %r"" % (idx, self)
                )
",if self . _componentType [ idx ] . isDefaulted :,168
"def _cloneComponentValues(self, myClone, cloneValueFlag):
    idx = 0
    l = len(self._componentValues)
    while idx < l:
        c = self._componentValues[idx]
        if c is not None:
            if isinstance(c, base.AbstractConstructedAsn1Item):
                myClone.setComponentByPosition(
                    idx, c.clone(cloneValueFlag=cloneValueFlag)
                )
            else:
                myClone.setComponentByPosition(idx, c.clone())
        idx = idx + 1
",if c is not None :,154
"def endElement(self, tag):
    """"""Handle the end of an element.""""""
    if tag == ""author"":
        developer = self.text.strip()
        if self.title == ""author"" and developer not in self.author_list:
            self.author_list.append(developer)
        elif self.title == ""contributor"" and developer not in self.contributor_list:
            self.contributor_list.append(developer)
","if self . title == ""author"" and developer not in self . author_list :",113
"def has_safe_repr(value):
    """"""Does the node have a safe representation?""""""
    if value is None or value is NotImplemented or value is Ellipsis:
        return True
    if isinstance(value, (bool, int, long, float, complex, basestring, xrange, Markup)):
        return True
    if isinstance(value, (tuple, list, set, frozenset)):
        for item in value:
            if not has_safe_repr(item):
                return False
        return True
    elif isinstance(value, dict):
        for key, value in value.iteritems():
            if not has_safe_repr(key):
                return False
            if not has_safe_repr(value):
                return False
        return True
    return False
",if not has_safe_repr ( item ) :,192
"def test_all_wizards(self):
    mod = ""w3af.core.controllers.wizard.wizards.%s""
    w3af_core = w3afCore()
    for filename in os.listdir(""w3af/core/controllers/wizard/wizards/""):
        wizard_id, ext = os.path.splitext(filename)
        if wizard_id in (""__init__"", "".git"") or ext == "".pyc"":
            continue
        klass = mod % wizard_id
        wizard_inst = factory(klass, w3af_core)
        yield self._test_wizard_correct, wizard_inst
        wizard_inst = factory(klass, w3af_core)
        yield self._test_wizard_fail, wizard_inst
","if wizard_id in ( ""__init__"" , "".git"" ) or ext == "".pyc"" :",183
"def test_bool_performance(self):
    class Person(Document):
        name = StringField()
    Person.drop_collection()
    for i in range(100):
        Person(name=""No: %s"" % i).save()
    with query_counter() as q:
        if Person.objects:
            pass
        assert q == 1
        op = q.db.system.profile.find({""ns"": {""$ne"": ""%s.system.indexes"" % q.db.name}})[
            0
        ]
        assert op[""nreturned""] == 1
",if Person . objects :,149
"def validate(self) -> None:
    if self.query:
        if not self.sysupgrade:
            for arg_name in (""aur"", ""repo""):
                if getattr(self, arg_name):
                    raise MissingArgument(""sysupgrade"", arg_name)
",if not self . sysupgrade :,74
"def __new__(cls, name, parents, dct):
    command_handlers = {}
    for attr_name, attr in dct.items():
        if callable(attr) and attr_name.startswith(""handle_""):
            handles_what = attr_name[len(""handle_"") :]
            if handles_what:
                command_handlers[handles_what] = attr
    dct[""command_handlers""] = command_handlers
    return super(CommandHandlerMeta, cls).__new__(cls, name, parents, dct)
","if callable ( attr ) and attr_name . startswith ( ""handle_"" ) :",124
"def pop_error_text(self, error_text):
    if error_text in self.__errors:
        self.__errors.remove(error_text)
        if len(self.__errors) == 0:
            self.set_message_text(WELCOME_MESSAGE)
        else:
            self.set_message_text(next(self.__errors.__iter__()))
",if len ( self . __errors ) == 0 :,95
"def run(self, edit):
    self.clear_phantoms()
    regions = self.view.sel()
    for region in regions:
        region, _ = self.get_selection_from_region(
            region=region, regions_length=len(region), view=self.view
        )
        if region is None:
            continue
        try:
            self.json_loads(self.view.substr(region), self.duplicate_key_hook)
        except Exception as ex:
            self.show_exception(region=region, msg=ex)
            return
        sublime.status_message(""JSON Valid"")
",if region is None :,165
"def update_leaderboard(wait_time):
    conn = get_connection()
    cursor = conn.cursor(MySQLdb.cursors.DictCursor)
    while True:
        try:
            if use_log:
                log.info(""Updating leaderboard and adding some sigma"")
            cursor.execute(""call generate_leaderboard;"")
            if wait_time == 0:
                break
            for s in range(wait_time):
                # allow for a [Ctrl]+C during the sleep cycle
                time.sleep(1)
        except KeyboardInterrupt:
            break
        except:
            # log error
            log.error(traceback.format_exc())
            break
    cursor.close()
    conn.close()
",if use_log :,199
"def _external_tables(self):
    tables = []
    for name, df in self.extra_options.get(""external_tables"", {}).items():
        if not isinstance(df, pd.DataFrame):
            raise TypeError(""External table is not an instance of pandas "" ""dataframe"")
        schema = sch.infer(df)
        chtypes = map(ClickhouseDataType.from_ibis, schema.types)
        structure = list(zip(schema.names, map(str, chtypes)))
        tables.append(dict(name=name, data=df.to_dict(""records""), structure=structure))
    return tables
","if not isinstance ( df , pd . DataFrame ) :",152
"def getmod(self, nm):
    mod = None
    for thing in self.path:
        if isinstance(thing, basestring):
            owner = self.shadowpath.get(thing, -1)
            if owner == -1:
                owner = self.shadowpath[thing] = self.__makeOwner(thing)
            if owner:
                mod = owner.getmod(nm)
        else:
            mod = thing.getmod(nm)
        if mod:
            break
    return mod
","if isinstance ( thing , basestring ) :",137
"def add_variant_attribute_data_to_expected_data(data, variant, attribute_ids, pk=None):
    for assigned_attribute in variant.attributes.all():
        header = f""{assigned_attribute.attribute.slug} (variant attribute)""
        if str(assigned_attribute.attribute.pk) in attribute_ids:
            value = get_attribute_value(assigned_attribute)
            if pk:
                data[pk][header] = value
            else:
                data[header] = value
    return data
",if pk :,136
"def get_files(start_dir, includes, excludes):
    # use os.walk to recursively dig down into the Pupil directory
    match_files = []
    for root, dirs, files in os.walk(start_dir):
        if not re.search(excludes, root):
            files = [
                f
                for f in files
                if re.search(includes, f) and not re.search(excludes, f)
            ]
            files = [os.path.join(root, f) for f in files]
            match_files += files
        else:
            print(""Excluding '%s'"" % root)
    return match_files
","if re . search ( includes , f ) and not re . search ( excludes , f )",175
"def findinDoc(self, tagpath, pos, end):
    result = None
    if end == -1:
        end = self.docSize
    else:
        end = min(self.docSize, end)
    foundat = -1
    for j in range(pos, end):
        item = self.docList[j]
        if item.find(b""="") >= 0:
            (name, argres) = item.split(b""="", 1)
        else:
            name = item
            argres = """"
        if isinstance(tagpath, str):
            tagpath = tagpath.encode(""utf-8"")
        if name.endswith(tagpath):
            result = argres
            foundat = j
            break
    return foundat, result
","if item . find ( b""="" ) >= 0 :",189
"def load_classes(module, base, blacklist):
    classes = []
    for attr in dir(module):
        attr = getattr(module, attr)
        if inspect.isclass(attr):
            if issubclass(attr, base):
                if attr is not base and attr not in blacklist:
                    classes.append(attr)
    return classes
","if issubclass ( attr , base ) :",90
"def run():
    try:
        result = func()
    except Exception:
        future_cell[0] = TracebackFuture()
        future_cell[0].set_exc_info(sys.exc_info())
    else:
        if is_future(result):
            future_cell[0] = result
        else:
            future_cell[0] = TracebackFuture()
            future_cell[0].set_result(result)
    self.add_future(future_cell[0], lambda future: self.stop())
",if is_future ( result ) :,136
"def lastCard(self):
    if self._answeredIds:
        if not self.card or self._answeredIds[-1] != self.card.id:
            try:
                return self.mw.col.getCard(self._answeredIds[-1])
            except TypeError:
                # id was deleted
                return
",if not self . card or self . _answeredIds [ - 1 ] != self . card . id :,94
"def run(self):
    global _cameras
    while 1:
        for cam in _cameras:
            if cam.pygame_camera:
                cam.pygame_buffer = cam.capture.get_image(cam.pygame_buffer)
            else:
                cv.GrabFrame(cam.capture)
            cam._threadcapturetime = time.time()
        time.sleep(0.04)  # max 25 fps, if you're lucky
",if cam . pygame_camera :,120
"def handle_exception(self, e, result):
    for k in sorted(result.thrift_spec):
        if result.thrift_spec[k][1] == ""success"":
            continue
        _, exc_name, exc_cls, _ = result.thrift_spec[k]
        if isinstance(e, exc_cls):
            setattr(result, exc_name, e)
            return True
    return False
","if isinstance ( e , exc_cls ) :",112
"def for_module(cls, modname: str) -> ""ModuleAnalyzer"":
    if (""module"", modname) in cls.cache:
        entry = cls.cache[""module"", modname]
        if isinstance(entry, PycodeError):
            raise entry
        return entry
    try:
        filename, source = cls.get_module_source(modname)
        if source is not None:
            obj = cls.for_string(source, modname, filename or ""<string>"")
        elif filename is not None:
            obj = cls.for_file(filename, modname)
    except PycodeError as err:
        cls.cache[""module"", modname] = err
        raise
    cls.cache[""module"", modname] = obj
    return obj
",if source is not None :,182
"def visit_productionlist(self, node):
    self.new_state()
    names = []
    for production in node:
        names.append(production[""tokenname""])
    maxlen = max(len(name) for name in names)
    for production in node:
        if production[""tokenname""]:
            self.add_text(production[""tokenname""].ljust(maxlen) + "" ::="")
            lastname = production[""tokenname""]
        else:
            self.add_text(""%s    "" % ("" "" * len(lastname)))
        self.add_text(production.astext() + ""\n"")
    self.end_state(wrap=False)
    raise nodes.SkipNode
","if production [ ""tokenname"" ] :",167
"def transport_vmware_guestinfo():
    rpctool = ""vmware-rpctool""
    not_found = None
    if not subp.which(rpctool):
        return not_found
    cmd = [rpctool, ""info-get guestinfo.ovfEnv""]
    try:
        out, _err = subp.subp(cmd)
        if out:
            return out
        LOG.debug(""cmd %s exited 0 with empty stdout: %s"", cmd, out)
    except subp.ProcessExecutionError as e:
        if e.exit_code != 1:
            LOG.warning(""%s exited with code %d"", rpctool, e.exit_code)
            LOG.debug(e)
    return not_found
",if out :,196
"def MakeWidthArray(fm):
    # Make character width array
    s = ""{\n\t""
    cw = fm[""Widths""]
    for i in xrange(0, 256):
        if chr(i) == ""'"":
            s += ""'\\''""
        elif chr(i) == ""\\"":
            s += ""'\\\\'""
        elif i >= 32 and i <= 126:
            s += ""'"" + chr(i) + ""'""
        else:
            s += ""chr(%d)"" % i
        s += "":"" + fm[""Widths""][i]
        if i < 255:
            s += "",""
        if (i + 1) % 22 == 0:
            s += ""\n\t""
    s += ""}""
    return s
",if ( i + 1 ) % 22 == 0 :,192
"def lookup_config_file(filename: str) -> Optional[str]:
    """"""Return config file PATH.""""""
    for path in [find_vcs_root(default=""~""), ""~""]:
        f = os.path.expanduser(""%s/%s"" % (path, filename))
        if os.path.isfile(f):
            LOG.info(""Found config file %s"", f)
            return f
    return None
",if os . path . isfile ( f ) :,103
"def load_freq_dict(self, freq_dict_filename: str):
    with open(str(expand_path(freq_dict_filename)), ""r"") as fl:
        lines = fl.readlines()
    pos_freq_dict = defaultdict(list)
    for line in lines:
        line_split = line.strip(""\n"").split(""\t"")
        if re.match(""[\d]+\.[\d]+"", line_split[2]):
            pos_freq_dict[line_split[1]].append((line_split[0], float(line_split[2])))
    nouns_with_freq = pos_freq_dict[""s""]
    self.nouns_dict = {noun: freq for noun, freq in nouns_with_freq}
","if re . match ( ""[\d]+\.[\d]+"" , line_split [ 2 ] ) :",180
"def do_visual_mode(self):
    """"""Handle strokes in visual mode.""""""
    try:
        self.n1 = self.n = 1
        self.do_state(
            self.vis_dispatch_d,
            mode_name=""visual-line"" if self.visual_line_flag else ""visual"",
        )
        if self.visual_line_flag:
            self.visual_line_helper()
    except Exception:
        g.es_exception()
        self.quit()
",if self . visual_line_flag :,131
"def cleanup(self):
    log.info("""")
    log.info(""Cleaning up.. "")
    status = self._capture_output(""status"", ""--porcelain"")
    status = status.split(""\n"")
    for line in status:
        filepath = line.split()
        if len(filepath) == 0:
            continue
        filepath = filepath[-1]
        if filepath[-3:] != ""rej"" and filepath[-5:] != ""porig"":
            continue
        try:
            log.info(""Removing temp file %s "" % filepath)
            os.remove(os.path.join(self.base_dir, filepath))
        except:
            log.warn(""File removal failed, you should manually remove %s"" % filepath)
            pass
",if len ( filepath ) == 0 :,195
"def OnBodyRClick(self, event=None):
    try:
        c = self.c
        p = c.currentPosition()
        if not g.doHook(""bodyrclick1"", c=c, p=p, v=p, event=event):
            c.k.showStateAndMode(w=c.frame.body.bodyCtrl)
        g.doHook(""bodyrclick2"", c=c, p=p, v=p, event=event)
    except:
        g.es_event_exception(""iconrclick"")
","if not g . doHook ( ""bodyrclick1"" , c = c , p = p , v = p , event = event ) :",138
"def receiver():
    """"""receive messages with polling""""""
    pull = ctx.socket(zmq.PULL)
    pull.connect(url)
    poller = Poller()
    poller.register(pull, zmq.POLLIN)
    while True:
        events = await poller.poll()
        if pull in dict(events):
            print(""recving"", events)
            msg = await pull.recv_multipart()
            print(""recvd"", msg)
",if pull in dict ( events ) :,120
"def sched(self):
    for k, q in self.q.items():
        if q and k not in self.cur:
            ent = q.popleft()
            self.cur[k] = ent
            self.run_one(ent, k)
",if q and k not in self . cur :,69
"def eval_dummy_genomes_iznn(genomes, config):
    for genome_id, genome in genomes:
        net = neat.iznn.IZNN.create(genome, config)
        if genome_id < 10:
            net.reset()
            genome.fitness = 0.0
        elif genome_id <= 150:
            genome.fitness = 0.5
        else:
            genome.fitness = 1.0
",if genome_id < 10 :,129
"def handle_noargs(self, **options):
    # Inspired by Postfix's ""postconf -n"".
    from django.conf import settings, global_settings
    # Because settings are imported lazily, we need to explicitly load them.
    settings._setup()
    user_settings = module_to_dict(settings._wrapped)
    default_settings = module_to_dict(global_settings)
    output = []
    for key in sorted(user_settings.keys()):
        if key not in default_settings:
            output.append(""%s = %s  ###"" % (key, user_settings[key]))
        elif user_settings[key] != default_settings[key]:
            output.append(""%s = %s"" % (key, user_settings[key]))
    return ""\n"".join(output)
",elif user_settings [ key ] != default_settings [ key ] :,196
"def test_get_chat_thread(self):
    async with self.chat_client:
        await self._create_thread()
        get_thread_result = await self.chat_client.get_chat_thread(self.thread_id)
        assert get_thread_result.id == self.thread_id
        # delete created users and chat threads
        if not self.is_playback():
            await self.chat_client.delete_chat_thread(self.thread_id)
",if not self . is_playback ( ) :,121
"def consume(self):
    if not self.inputState.guessing:
        c = self.LA(1)
        if self.caseSensitive:
            self.append(c)
        else:
            # use input.LA(), not LA(), to get original case
            # CharScanner.LA() would toLower it.
            c = self.inputState.input.LA(1)
            self.append(c)
        if c and c in ""\t"":
            self.tab()
        else:
            self.inputState.column += 1
    self.inputState.input.consume()
",if self . caseSensitive :,159
"def commandComplete(self, cmd):
    if self.property:
        if cmd.didFail():
            return
        result = self.observer.getStdout()
        if self.strip:
            result = result.strip()
        propname = self.property
        self.setProperty(propname, result, ""SetPropertyFromCommand Step"")
        self.property_changes[propname] = result
    else:
        new_props = self.extract_fn(
            cmd.rc, self.observer.getStdout(), self.observer.getStderr()
        )
        for k, v in iteritems(new_props):
            self.setProperty(k, v, ""SetPropertyFromCommand Step"")
        self.property_changes = new_props
",if cmd . didFail ( ) :,192
"def any(self, provider_name):
    result = authomatic.login(Webapp2Adapter(self), provider_name)
    if result:
        apis = []
        if result.user:
            result.user.update()
            if result.user.credentials:
                apis = config.config.get(provider_name, {}).get(""_apis"", {})
        nice_provider_name = (
            config.config.get(provider_name, {}).get(""_name"")
            or provider_name.capitalize()
        )
        render(
            self,
            result,
            result.popup_js(custom=dict(apis=apis, provider_name=nice_provider_name)),
        )
",if result . user . credentials :,186
"def set_lock(self, lock_closed=True, device=0, timeout=0):
    if self.handle:
        action = 0x02 if lock_closed else 0x01
        reply = self.write_register(_R.receiver_pairing, action, device, timeout)
        if reply:
            return True
        _log.warn(
            ""%s: failed to %s the receiver lock"",
            self,
            ""close"" if lock_closed else ""open"",
        )
",if reply :,129
"def connect_thread(self, sleep_time=0):
    time.sleep(sleep_time)
    try:
        while self.running and self._need_more_ip():
            if self.new_conn_pool.qsize() > self.config.https_connection_pool_max:
                break
            self.connect_process()
    finally:
        self.thread_num_lock.acquire()
        self.thread_num -= 1
        self.thread_num_lock.release()
",if self . new_conn_pool . qsize ( ) > self . config . https_connection_pool_max :,128
"def train_job(
    guest_party_id, host_party_id, arbiter_party_id, train_conf_path, train_dsl_path
):
    train = TrainSBTModel()
    train.set_config(guest_party_id, host_party_id, arbiter_party_id, train_conf_path)
    train.set_dsl(train_dsl_path)
    status = train.submit()
    if status:
        is_success = train.wait_success(timeout=600)
        if is_success:
            train.get_component_metrics()
            train.get_component_output_model()
            train.get_component_output_data()
            return train
    return False
",if is_success :,188
"def get_version():
    INIT = os.path.abspath(os.path.join(HERE, "".."", ""pyftpdlib"", ""__init__.py""))
    with open(INIT, ""r"") as f:
        for line in f:
            if line.startswith(""__ver__""):
                ret = eval(line.strip().split("" = "")[1])
                assert ret.count(""."") == 2, ret
                for num in ret.split("".""):
                    assert num.isdigit(), ret
                return ret
        else:
            raise ValueError(""couldn't find version string"")
","if line . startswith ( ""__ver__"" ) :",151
"def get_terminus_panel(self, window, visible_only=False):
    if visible_only:
        active_panel = window.active_panel()
        panels = [active_panel] if active_panel else []
    else:
        panels = window.panels()
    for panel in panels:
        panel_name = panel.replace(""output."", """")
        if panel_name == EXEC_PANEL:
            continue
        panel_view = window.find_output_panel(panel_name)
        if panel_view:
            terminal = Terminal.from_id(panel_view.id())
            if terminal:
                return panel_view
    return None
",if terminal :,173
"def to_internal_value(self, data):
    site = get_current_site()
    pages_root = reverse(""pages-root"")
    ret = []
    for path in data:
        if path.startswith(pages_root):
            path = path[len(pages_root) :]
        # strip any final slash
        if path.endswith(""/""):
            path = path[:-1]
        page = get_page_from_path(site, path)
        if page:
            ret.append(page)
    return ret
",if path . startswith ( pages_root ) :,136
"def forward(self, inputs):
    input_dtype = inputs[0].dtype
    if self.comm.rank == self.root:
        # convert to float32 for communication
        if numpy.float16 == input_dtype:
            inputs = tuple([item.astype(numpy.float32) for item in inputs])
        y = self.comm.scatter(inputs, self.root)
    else:
        y = self.comm.scatter(None, self.root)
    # convert back
    if numpy.float16 == input_dtype:
        y = y.astype(input_dtype)
    return (y,)
",if numpy . float16 == input_dtype :,152
"def discover_misago_admin():
    for app in apps.get_app_configs():
        module = import_module(app.name)
        if not hasattr(module, ""admin""):
            continue
        admin_module = import_module(""%s.admin"" % app.name)
        if hasattr(admin_module, ""MisagoAdminExtension""):
            extension = getattr(admin_module, ""MisagoAdminExtension"")()
            if hasattr(extension, ""register_navigation_nodes""):
                extension.register_navigation_nodes(site)
            if hasattr(extension, ""register_urlpatterns""):
                extension.register_urlpatterns(urlpatterns)
","if hasattr ( admin_module , ""MisagoAdminExtension"" ) :",169
"def overwrite_timeout(
    initial_node: dict, path: str, hash_: str, size_: int, rsf: bool
) -> int:
    minutes = 10
    while minutes > 0:
        time.sleep(60)
        minutes -= 1
        n = acd_client.get_metadata(initial_node[""id""])
        if n[""version""] > initial_node[""version""]:
            return upload_complete(n, path, hash_, size_, rsf)
    logger.warning('Timeout while overwriting ""%s"".' % path)
    return UL_TIMEOUT
","if n [ ""version"" ] > initial_node [ ""version"" ] :",138
"def write(self, s, spos):
    if not s:
        return
    # Force s to be a string or unicode
    if not isinstance(s, basestring):
        s = str(s)
    slen = self.len
    if spos == slen:
        self.len = self.pos = spos + len(s)
        return
    if spos > slen:
        slen = spos
    newpos = spos + len(s)
    if spos < slen:
        if self.buflist:
            self.buf += """".join(self.buflist)
        self.buflist = [self.buf[:spos], s, self.buf[newpos:]]
        if newpos > slen:
            slen = newpos
    else:
        self.buflist.append(s)
",if newpos > slen :,196
"def _print_one_entry(news_entry: xml.etree.ElementTree.Element) -> None:
    child: xml.etree.ElementTree.Element
    for child in news_entry:
        if ""title"" in child.tag:
            title = str(child.text)
        if ""pubDate"" in child.tag:
            pub_date = str(child.text)
        if ""description"" in child.tag:
            description = str(child.text)
    print_stdout(color_line(title, 14) + "" ("" + bold_line(pub_date) + "")"")
    print_stdout(format_paragraph(strip_tags(description)))
    print_stdout()
","if ""description"" in child . tag :",169
"def get_sequence_type_str(x: Sequence[Any]) -> str:
    container_type = type(x).__name__
    if not x:
        if container_type == ""list"":
            return ""[]""
        else:
            return container_type + ""([])""
    elem_type = get_type_str(x[0])
    if container_type == ""list"":
        if len(x) == 1:
            return ""["" + elem_type + ""]""
        else:
            return ""["" + elem_type + "", ...]""
    else:
        if len(x) == 1:
            return f""{container_type}([{elem_type}])""
        else:
            return f""{container_type}([{elem_type}, ...])""
",if len ( x ) == 1 :,196
"def signal_notebook_switch_page(self, notebook, current_page, index):
    if not hasattr(self.parent, ""rpc""):
        return
    # previous_page = notebook.get_nth_page(self.last_page_id)
    self.last_page_id = index
    for tab in self.tabs.values():
        if current_page != tab.box:
            continue
        if hasattr(tab, ""load_campaign_information""):
            tab.load_campaign_information(force=False)
","if hasattr ( tab , ""load_campaign_information"" ) :",137
"def format_string(self, templ, args):
    templ = self.to_native(templ)
    if isinstance(args, nodes.Arguments):
        args = args.arguments
    for (i, arg) in enumerate(args):
        arg = self.to_native(self.reduce_single(arg))
        if isinstance(arg, bool):  # Python boolean is upper case.
            arg = str(arg).lower()
        templ = templ.replace(""@{}@"".format(i), str(arg))
    return templ
","if isinstance ( arg , bool ) :",135
"def execute_Single(self, object, smooth):
    if getattr(object, ""type"", """") == ""MESH"":
        mesh = object.data
        if len(mesh.polygons) > 0:
            smoothList = [smooth] * len(mesh.polygons)
            mesh.polygons.foreach_set(""use_smooth"", smoothList)
            # trigger update
            mesh.polygons[0].use_smooth = smooth
    return object
",if len ( mesh . polygons ) > 0 :,112
"def _enumerate_visible_deps(self, dep, predicate):
    # We present the dependencies out of classpath order and instead in alphabetized internal deps,
    # then alphabetized external deps order for ease in scanning output.
    dependencies = sorted(x for x in getattr(dep, ""dependencies"", []))
    if not self.is_internal_only:
        dependencies.extend(
            sorted(
                (x for x in getattr(dep, ""jar_dependencies"", [])),
                key=lambda x: (x.org, x.name, x.rev, x.classifier),
            )
        )
    for inner_dep in dependencies:
        dep_id, internal = self._dep_id(inner_dep)
        if predicate(internal):
            yield inner_dep
",if predicate ( internal ) :,193
"def stop_test(self):
    if self.master:
        self.log.info(""Ending cloud test..."")
        if not self._last_status:
            self.get_master_status()
        if self._last_status[""progress""] >= 100:
            self.master.stop()
        else:
            self.master.terminate()
","if self . _last_status [ ""progress"" ] >= 100 :",92
"def run(self, workspace):
    """"""Run the module""""""
    if self.show_window:
        m = workspace.get_measurements()
        x = m.get_current_measurement(self.get_object(), self.x_axis.value)
        if self.wants_xbounds:
            x = x[x > self.xbounds.min]
            x = x[x < self.xbounds.max]
        workspace.display_data.x = x
        workspace.display_data.title = ""{} (cycle {})"".format(
            self.title.value, workspace.measurements.image_set_number
        )
",if self . wants_xbounds :,163
"def L_op(self, inputs, outputs, gout):
    (x,) = inputs
    (gz,) = gout
    if x.type in complex_types:
        raise NotImplementedError()
    if outputs[0].type in discrete_types:
        if x.type in discrete_types:
            return [x.zeros_like(dtype=theano.config.floatX)]
        else:
            return [x.zeros_like()]
    return (gz / (np.cast[x.type](1) - sqr(x)),)
",if x . type in discrete_types :,130
"def _which(cls, progname):
    progname = progname.lower()
    for p in cls.env.path:
        for ext in cls._EXTENSIONS:
            fn = p / (progname + ext)
            if fn.access(""x"") and not fn.is_dir():
                return fn
    return None
","if fn . access ( ""x"" ) and not fn . is_dir ( ) :",81
"def iterate(self, prod_, rule_):
    newProduction = """"
    for i in range(len(prod_)):
        step = self.production[i]
        if step == ""W"":
            newProduction = newProduction + self.ruleW
        elif step == ""X"":
            newProduction = newProduction + self.ruleX
        elif step == ""Y"":
            newProduction = newProduction + self.ruleY
        elif step == ""Z"":
            newProduction = newProduction + self.ruleZ
        elif step != ""F"":
            newProduction = newProduction + step
    self.drawLength = self.drawLength * 0.5
    self.generations += 1
    return newProduction
","elif step == ""Y"" :",179
"def update(self, mapping, update_only=False):
    for name in mapping:
        if update_only and name in self:
            # nested and inner objects, merge recursively
            if hasattr(self[name], ""update""):
                # FIXME only merge subfields, not the settings
                self[name].update(mapping[name], update_only)
            continue
        self.field(name, mapping[name])
    if update_only:
        for name in mapping._meta:
            if name not in self._meta:
                self._meta[name] = mapping._meta[name]
    else:
        self._meta.update(mapping._meta)
",if name not in self . _meta :,175
"def Flatten(self, metadata, value_to_flatten):
    if metadata:
        self.metadata = metadata
    for desc in value_to_flatten.type_infos:
        if desc.name == ""metadata"":
            continue
        if hasattr(self, desc.name) and value_to_flatten.HasField(desc.name):
            setattr(self, desc.name, getattr(value_to_flatten, desc.name))
","if hasattr ( self , desc . name ) and value_to_flatten . HasField ( desc . name ) :",108
"def addnode(self, parent, data):
    print(""aaa"", data)
    for i in data:
        print(i)
        if i == ""-"":
            continue
        if isinstance(i, tuple):
            item = self.tre_plugins.AppendItem(parent, i[0].title)
            self.tre_plugins.SetItemData(item, i[0])
            self.addnode(item, i[1])
        else:
            item = self.tre_plugins.AppendItem(parent, i[0].title)
            self.tre_plugins.SetItemData(item, i[0])
","if isinstance ( i , tuple ) :",159
"def load_timer(string):
    if ""."" not in string:
        raise argparse.ArgumentTypeError(
            ""Value for --benchmark-timer must be in dotted form. Eg: 'module.attr'.""
        )
    mod, attr = string.rsplit(""."", 1)
    if mod == ""pep418"":
        if PY3:
            import time
            return NameWrapper(getattr(time, attr))
        else:
            from . import pep418
            return NameWrapper(getattr(pep418, attr))
    else:
        __import__(mod)
        mod = sys.modules[mod]
        return NameWrapper(getattr(mod, attr))
",if PY3 :,166
"def _is_an_attribute(self, pyname):
    if pyname is not None and isinstance(pyname, pynames.AssignedName):
        pymodule, lineno = self.pyname.get_definition_location()
        scope = pymodule.get_scope().get_inner_scope_for_line(lineno)
        if scope.get_kind() == ""Class"":
            return pyname in list(scope.get_names().values())
        parent = scope.parent
        if parent is not None and parent.get_kind() == ""Class"":
            return pyname in list(parent.get_names().values())
    return False
","if parent is not None and parent . get_kind ( ) == ""Class"" :",157
"def _format_arg(self, name, spec, value):
    if name == ""title"":
        if isinstance(value, bool) and value:
            return ""--title""
        elif isinstance(value, str):
            return ""--title --title_text %s"" % (value,)
        else:
            raise ValueError('Unknown value for ""title"" argument: ' + str(value))
    return super(Pik, self)._format_arg(name, spec, value)
","elif isinstance ( value , str ) :",118
"def total_form_count(self):
    """"""Returns the total number of forms in this FormSet.""""""
    if self.is_bound:
        return self.management_form.cleaned_data[TOTAL_FORM_COUNT]
    else:
        initial_forms = self.initial_form_count()
        total_forms = initial_forms + self.extra
        # Allow all existing related objects/inlines to be displayed,
        # but don't allow extra beyond max_num.
        if initial_forms > self.max_num >= 0:
            total_forms = initial_forms
        elif total_forms > self.max_num >= 0:
            total_forms = self.max_num
    return total_forms
",if initial_forms > self . max_num >= 0 :,178
"def GetTestNamesFromSuites(test_suite):
    """"""Takes a list of test suites and returns a list of contained test names.""""""
    suites = [test_suite]
    test_names = []
    while suites:
        suite = suites.pop()
        for test in suite:
            if isinstance(test, unittest.TestSuite):
                suites.append(test)
            else:
                test_names.append(test.id()[len(""gslib.tests.test_"") :])
    return test_names
","if isinstance ( test , unittest . TestSuite ) :",135
"def readArgs(self, node):
    res = {}
    for c in self.getChildrenOf(node):
        val = c.getAttribute(""val"")
        if val in self.modules:
            res[str(c.nodeName)] = self.modules[val]
        elif val in self.mothers:
            res[str(c.nodeName)] = self.mothers[val]
        elif val != """":
            res[str(c.nodeName)] = eval(val)
    return res
",if val in self . modules :,130
"def pop(self, k, default=Sentinel):
    with self._database.transaction():
        node, is_single = self.convert_node(k)
        try:
            res = self[k]
        except KeyError:
            if default is Sentinel:
                raise
            return default
        del self[node]
    return res
",if default is Sentinel :,93
"def wrapped_strategy(self):
    if self.__wrapped_strategy is None:
        if not inspect.isfunction(self.__definition):
            raise InvalidArgument(
                f""Expected definition to be a function but got {self.__definition!r} ""
                f""of type {type(self.__definition).__name__} instead.""
            )
        result = self.__definition()
        if result is self:
            raise InvalidArgument(""Cannot define a deferred strategy to be itself"")
        check_strategy(result, ""definition()"")
        self.__wrapped_strategy = result
        self.__definition = None
    return self.__wrapped_strategy
",if not inspect . isfunction ( self . __definition ) :,160
"def _on_fullscreen_requested(self, on):
    if not config.val.content.fullscreen.window:
        if on:
            self.state_before_fullscreen = self.windowState()
            self.setWindowState(
                Qt.WindowFullScreen
                | self.state_before_fullscreen  # type: ignore[arg-type]
            )  # type: ignore[operator]
        elif self.isFullScreen():
            self.setWindowState(self.state_before_fullscreen)
    log.misc.debug(
        ""on: {}, state before fullscreen: {}"".format(
            on, debug.qflags_key(Qt, self.state_before_fullscreen)
        )
    )
",if on :,193
"def update_defaults(self, *values, **kwargs):
    for value in values:
        if type(value) == dict:
            self.DEFAULT_CONFIGURATION.update(value)
        elif isinstance(value, types.ModuleType):
            self.__defaults_from_module(value)
        elif isinstance(value, str):
            if os.path.exists(value):
                self.__defaults_from_file(value)
            else:
                logger.warning(""Configuration file {} does not exist."".format(value))
        elif isinstance(value, type(None)):
            pass
        else:
            raise ValueError(""Cannot interpret {}"".format(value))
    self.DEFAULT_CONFIGURATION.update(kwargs)
",if os . path . exists ( value ) :,184
"def clear_output_directory(self):
    files = os.listdir(os.path.join(""functional"", ""output""))
    for f in files:
        if f in (""README.txt"", "".svn"", ""CVS""):
            continue  # don't touch the infrastructure
        path = os.path.join(""functional"", ""output"", f)
        if os.path.isdir(path):
            shutil.rmtree(path)
        else:
            os.remove(path)
",if os . path . isdir ( path ) :,121
"def do_remove(self):
    if self.netconf.locked(""dhcp""):
        if not self.pid:
            pid = read_pid_file(""/var/run/udhcpd.pan1.pid"")
        else:
            pid = self.pid
        if not kill(pid, ""udhcpd""):
            logging.info(""Stale dhcp lockfile found"")
        self.netconf.unlock(""dhcp"")
",if not self . pid :,110
"def __getattr__(self, attr):
    if attr.endswith(""[]""):
        searchName = attr[:-2]
    else:
        searchName = attr
    with _lazyLock:
        nestedClasses = _dependencyMap.get(self.__name__, [])
        if searchName in nestedClasses:
            return GetVmodlType(self.__name__ + ""."" + attr)
        else:
            return super(LazyType, self).__getattribute__(attr)
",if searchName in nestedClasses :,113
"def allow_request(self, request, view):
    request.server = None
    allow = True
    view_name = view.get_view_name()
    allowed_views = [u""System Data"", u""Collectd Data"", u""Legacy System Data""]
    if view_name in allowed_views:
        server_key = view.kwargs.get(""server_key"")
        server = server_model.get_server_by_key(server_key)
        if server:
            request.server = server  # Needed in the Models
            server_status = throttle_status(server=server)
            if server_status.allow == False:
                allow = False
    return allow
",if server :,173
"def serve_until_stopped(self):
    import select
    abort = 0
    while not abort:
        rd, wr, ex = select.select([self.socket.fileno()], [], [], self.timeout)
        if rd:
            self.handle_request()
        logging._acquireLock()
        abort = self.abort
        logging._releaseLock()
",if rd :,90
"def A(*args):
    if len(args) > 0 and hasattr(args[0], ""__iter__""):  # Iterable as argument
        if len(args) == 1:
            return np.array(list(args), dtype=np.float32)
        else:
            # Flatten arguments into one list
            l = list(args[0])
            for e in args[1:]:
                if hasattr(e, ""__iter__""):
                    l.extend(e)
                else:
                    l.append(e)
            return np.array(l, dtype=np.float32)
    return np.array(list(args), dtype=np.float32)
",if len ( args ) == 1 :,179
"def _fix(self):
    op = []
    for k in range(self.size):
        o = random.choice(self._opts)
        if type(o) is str:
            op.append((o, self.rndstr * 1))
        else:
            op.append((o.name, o.randval()._fix()))
    return op
",if type ( o ) is str :,92
"def lint_dynamic(self, rule):
    for file in chain(rule.output, rule.input):
        if is_flagged(file, ""dynamic""):
            yield Lint(
                title=""The dynamic flag is deprecated"",
                body=""Use checkpoints instead, which are more powerful and less error-prone."",
                links=[links.checkpoints],
            )
","if is_flagged ( file , ""dynamic"" ) :",100
"def visit(ignored, dir, files):
    if os.path.basename(dir) not in test_names:
        for name in test_names:
            if name + "".py"" in files:
                path = os.path.join(dir, name + "".py"")
                if matcher(path[baselen:]):
                    results.append(path)
        return
    if ""__init__.py"" not in files:
        stderr(""%s is not a package"" % dir)
        return
    for file in files:
        if file.startswith(""test"") and file.endswith("".py""):
            path = os.path.join(dir, file)
            if matcher(path[baselen:]):
                results.append(path)
",if matcher ( path [ baselen : ] ) :,194
"def wrapped(*args, **kwargs):
    try:
        func(*args, **kwargs)
    except AssertionError as e:
        if retries:
            time.sleep(t_interval)
            retry_assertion(interval=t_interval, retries=t_retries - 1)(func)(
                *args, **kwargs
            )
        else:
            raise e
",if retries :,98
"def num2binary(l, bits=32):
    all = []
    bin = """"
    for i in range(bits):
        if l & 0x1:
            bin = ""1"" + bin
        else:
            bin = ""0"" + bin
        l = l >> 1
        if not ((i + 1) % 8):
            all.append(bin)
            bin = """"
    if bin:
        all.append(bin)
    all.reverse()
    assert l in (0, -1), ""number doesn't fit in number of bits""
    return string.join(all, "" "")
",if not ( ( i + 1 ) % 8 ) :,158
"def closest_enemy_ant(self, row1, col1, filter=None):
    # find the closest enemy ant from this row/col
    min_dist = maxint
    closest_ant = None
    for ant in self.enemy_ants():
        if filter is None or ant not in filter:
            dist = self.distance(row1, col1, ant[0][0], ant[0][1])
            if dist < min_dist:
                min_dist = dist
                closest_ant = ant[0]
    return closest_ant
",if dist < min_dist :,146
"def _wrap(cls, parent, value):
    if isinstance(value, dict):
        # we know that `annotations` and `labels` are dicts and therefore don't want to convert them into K8sObject
        return (
            value
            if parent in {""annotations"", ""labels""}
            and all(isinstance(v, six.string_types) for v in value.values())
            else cls(value)
        )
    elif isinstance(value, list):
        return [cls._wrap(None, v) for v in value]
    else:
        return value
","if parent in { ""annotations"" , ""labels"" }",145
"def do_definition(tag):
    w.end_para()
    macro("".TP"")
    w.started = True
    split = 0
    pre = []
    post = []
    for typ, text in _bitlist(tag):
        if split:
            post.append((typ, text))
        elif text.lstrip().startswith("": ""):
            split = 1
            post.append((typ, text.lstrip()[2:].lstrip()))
        else:
            pre.append((typ, text))
    _boldline(pre)
    w.write(_text(post))
    w.started = False
",if split :,153
"def updateTree(self, v, x, y, h, level):
    yfirst = y
    if level == 0:
        yfirst += 10
    while v:
        # g.trace(x,y,v)
        h, indent = self.updateNode(v, x, y)
        y += h
        if v.isExpanded() and v.firstChild():
            y = self.updateTree(v.firstChild(), x + indent, y, h, level + 1)
        v = v.next()
    return y
",if v . isExpanded ( ) and v . firstChild ( ) :,138
"def loop(self):
    while True:
        job = self.check_queue()
        if not job:
            time.sleep(20)
            continue
        self.run_job(job)
        time.sleep(5)
",if not job :,64
"def _name_to_variable(self, name):
    r""""""Find the corresponding variable given the specified name.""""""
    pointer = self
    for m_name in name.split("".""):
        if m_name.isdigit():
            num = int(m_name)
            pointer = pointer[num]  # type: ignore
        else:
            pointer = getattr(pointer, m_name)
    return pointer  # type: ignore
",if m_name . isdigit ( ) :,107
"def fetch_cleanup(self):
    for cell in self.cover_cells:
        if cell.fetch_task is not None:
            log.debug(
                ""Removing cover art fetch task for %s"",
                cell.release[""musicbrainz_albumid""],
            )
            self.tagger.webservice.remove_task(cell.fetch_task)
",if cell . fetch_task is not None :,95
"def _get_lcmap_info(self, vol_name):
    ret_vals = {
        ""fc_id"": """",
        ""fc_name"": """",
        ""lc_map_count"": ""0"",
    }
    for lcmap in self._lcmappings_list.values():
        if (lcmap[""source""] == vol_name) or (lcmap[""target""] == vol_name):
            ret_vals[""fc_id""] = lcmap[""id""]
            ret_vals[""fc_name""] = lcmap[""name""]
            ret_vals[""lc_map_count""] = ""1""
    return ret_vals
","if ( lcmap [ ""source"" ] == vol_name ) or ( lcmap [ ""target"" ] == vol_name ) :",159
"def on_event_clicked(self, widget, event):
    if event.type == Gdk.EventType.BUTTON_PRESS and event.button == 3:
        path = self.get_path_at_pos(int(event.x), int(event.y))
        if path is not None:
            row = self.get(path[0], ""device"")
            if row:
                if self.Blueman is not None:
                    if self.menu is None:
                        self.menu = ManagerDeviceMenu(self.Blueman)
                    self.menu.popup(None, None, None, None, event.button, event.time)
",if self . Blueman is not None :,170
"def _find_node_with_predicate(self, node, predicate):
    if node != self._tree._root and predicate(node):
        return node
    item, cookie = self._tree.GetFirstChild(node)
    while item:
        if predicate(item):
            return item
        if self._tree.ItemHasChildren(item):
            result = self._find_node_with_predicate(item, predicate)
            if result:
                return result
        item, cookie = self._tree.GetNextChild(node, cookie)
    return None
",if self . _tree . ItemHasChildren ( item ) :,143
"def expect_flow_sequence_item(self):
    if isinstance(self.event, SequenceEndEvent):
        self.indent = self.indents.pop()
        self.flow_level -= 1
        if self.canonical:
            self.write_indicator(u"","", False)
            self.write_indent()
        self.write_indicator(u""]"", False)
        self.state = self.states.pop()
    else:
        self.write_indicator(u"","", False)
        if self.canonical or self.column > self.best_width:
            self.write_indent()
        self.states.append(self.expect_flow_sequence_item)
        self.expect_node(sequence=True)
",if self . canonical or self . column > self . best_width :,185
"def iteration(pts):
    n = len(pts)
    all_pts = pts + invert(pts)
    diagram = Voronoi(all_pts)
    vertices = restrict(diagram.vertices)
    centers = []
    for site_idx in range(n):
        region_idx = diagram.point_region[site_idx]
        region = diagram.regions[region_idx]
        if -1 in region:
            site = pts[site_idx]
            centers.append(site)
            continue
        region_verts = np.array([vertices[i] for i in region])
        center = weighted_center(region_verts, weight_field)
        centers.append(tuple(center))
    return centers
",if - 1 in region :,185
"def retry_call(self, key, f, time_expire, with_lock):
    self.RETRIES += 1
    if self.RETRIES <= self.MAX_RETRIES:
        if self.fail_gracefully:
            self.RETRIES = 0
            return f()
        logger.error(""sleeping %s seconds before reconnecting"" % (2 * self.RETRIES))
        time.sleep(2 * self.RETRIES)
        return self.__call__(key, f, time_expire, with_lock)
    else:
        self.RETRIES = 0
        if self.fail_gracefully:
            return f
        raise RConnectionError(""Redis instance is unavailable"")
",if self . fail_gracefully :,165
"def load_model(
    self, model_name: str, path: str = None, model_type=None
) -> AbstractModel:
    if isinstance(model_name, AbstractModel):
        return model_name
    if model_name in self.models.keys():
        return self.models[model_name]
    else:
        if path is None:
            path = self.get_model_attribute(model=model_name, attribute=""path"")
        if model_type is None:
            model_type = self.get_model_attribute(model=model_name, attribute=""type"")
        return model_type.load(path=path, reset_paths=self.reset_paths)
",if path is None :,170
"def _GetPathType(
    args: rdf_artifacts.ArtifactCollectorFlowArgs, client_os: str
) -> rdf_paths.PathSpec.PathType:
    if args.use_tsk or args.use_raw_filesystem_access:
        if client_os == ""Windows"":
            return config.CONFIG[""Server.raw_filesystem_access_pathtype""]
        else:
            return rdf_paths.PathSpec.PathType.TSK
    else:
        return rdf_paths.PathSpec.PathType.OS
","if client_os == ""Windows"" :",128
"def iter_links(self):
    # type: () -> Iterable[Link]
    """"""Yields all links in the page""""""
    document = html5lib.parse(
        self.content,
        transport_encoding=_get_encoding_from_headers(self.headers),
        namespaceHTMLElements=False,
    )
    base_url = _determine_base_url(document, self.url)
    for anchor in document.findall("".//a""):
        link = _create_link_from_element(
            anchor,
            page_url=self.url,
            base_url=base_url,
        )
        if link is None:
            continue
        yield link
",if link is None :,175
"def on_leave(
    self, original_node: CSTNodeT, updated_node: CSTNodeT
) -> Union[cst.Import, cst.ImportFrom, CSTNodeT, RemovalSentinel]:
    if isinstance(updated_node, cst.Import):
        for alias in updated_node.names:
            name = alias.name
            if isinstance(name, cst.Name) and name.value == ""b"":
                return cst.RemoveFromParent()
    elif isinstance(updated_node, cst.ImportFrom):
        module = updated_node.module
        if isinstance(module, cst.Name) and module.value == ""e"":
            return cst.RemoveFromParent()
    return updated_node
","if isinstance ( name , cst . Name ) and name . value == ""b"" :",183
"def http_request(self, request):
    ntlm_auth_header = request.get_header(self.auth_header, None)
    if ntlm_auth_header is None:
        user, pw = self.passwd.find_user_password(None, request.get_full_url())
        if pw is not None:
            auth = ""NTLM %s"" % ntlm.create_NTLM_NEGOTIATE_MESSAGE(user)
            request.add_unredirected_header(self.auth_header, auth)
    return request
",if pw is not None :,137
"def _parse_yum_or_zypper_repositories(output):
    repos = []
    current_repo = {}
    for line in output:
        line = line.strip()
        if not line or line.startswith(""#""):
            continue
        if line.startswith(""[""):
            if current_repo:
                repos.append(current_repo)
                current_repo = {}
            current_repo[""name""] = line[1:-1]
        if current_repo and ""="" in line:
            key, value = line.split(""="", 1)
            current_repo[key] = value
    if current_repo:
        repos.append(current_repo)
    return repos
","if not line or line . startswith ( ""#"" ) :",179
"def load_as_uint8(filename):
    image = gdal.Open(filename)
    image_array = np.array(image.ReadAsArray())
    image_uint8 = np.zeros(image_array.shape, dtype=np.uint8)
    # rescale each band to be between 0, 255
    for k, band in enumerate(image_array):
        band_max = np.max(band)
        if band_max != 0:
            band = band.astype(np.float) / band_max * 255.0
        image_uint8[k, :, :] = band
    return image_uint8
",if band_max != 0 :,153
"def _get_resource_group_name_of_staticsite(client, static_site_name):
    static_sites = client.list()
    for static_site in static_sites:
        if static_site.name.lower() == static_site_name.lower():
            resource_group = _parse_resource_group_from_arm_id(static_site.id)
            if resource_group:
                return resource_group
    raise CLIError(
        ""Static site was '{}' not found in subscription."".format(static_site_name)
    )
",if resource_group :,144
"def _translate_trace_addr(self, trace_addr, obj=None):
    if obj is None:
        for obj in self._aslr_slides:  # pylint: disable=redefined-argument-from-local
            if obj.contains_addr(trace_addr - self._aslr_slides[obj]):
                break
        else:
            raise Exception(""Can't figure out which object this address belongs to"")
    if obj not in self._aslr_slides:
        raise Exception(""Internal error: object is untranslated"")
    return trace_addr - self._aslr_slides[obj]
",if obj . contains_addr ( trace_addr - self . _aslr_slides [ obj ] ) :,149
"def _register_builtin_handlers(self, events):
    for spec in handlers.BUILTIN_HANDLERS:
        if len(spec) == 2:
            event_name, handler = spec
            self.register(event_name, handler)
        else:
            event_name, handler, register_type = spec
            if register_type is handlers.REGISTER_FIRST:
                self._events.register_first(event_name, handler)
            elif register_type is handlers.REGISTER_LAST:
                self._events.register_last(event_name, handler)
",if register_type is handlers . REGISTER_FIRST :,148
"def __fixdict(self, dict):
    for key in dict.keys():
        if key[:6] == ""start_"":
            tag = key[6:]
            start, end = self.elements.get(tag, (None, None))
            if start is None:
                self.elements[tag] = getattr(self, key), end
        elif key[:4] == ""end_"":
            tag = key[4:]
            start, end = self.elements.get(tag, (None, None))
            if end is None:
                self.elements[tag] = start, getattr(self, key)
",if start is None :,162
"def metadata(draft):
    test_metadata = {}
    json_schema = create_jsonschema_from_metaschema(draft.registration_schema.schema)
    for key, value in json_schema[""properties""].items():
        response = ""Test response""
        items = value[""properties""][""value""].get(""items"")
        enum = value[""properties""][""value""].get(""enum"")
        if items:  # multiselect
            response = [items[""enum""][0]]
        elif enum:  # singleselect
            response = enum[0]
        elif value[""properties""][""value""].get(""properties""):
            response = {""question"": {""value"": ""Test Response""}}
        test_metadata[key] = {""value"": response}
    return test_metadata
",elif enum :,185
"def par_iter_next_batch(self, batch_ms: int):
    """"""Batches par_iter_next.""""""
    batch = []
    if batch_ms == 0:
        batch.append(self.par_iter_next())
        return batch
    t_end = time.time() + (0.001 * batch_ms)
    while time.time() < t_end:
        try:
            batch.append(self.par_iter_next())
        except StopIteration:
            if len(batch) == 0:
                raise StopIteration
            else:
                pass
    return batch
",if len ( batch ) == 0 :,157
"def get_node_map(self, nodes: List[Node], left_node_only=True):
    node_map = {}
    idx = 0
    for node in nodes:
        if node.id != 0 and (not node.is_left_node and left_node_only):
            continue
        node_map[node.id] = idx
        idx += 1
    return node_map
",if node . id != 0 and ( not node . is_left_node and left_node_only ) :,99
"def compare_objects(left, right):
    left_fields = left.map_value.fields
    right_fields = right.map_value.fields
    for left_key, right_key in zip(sorted(left_fields), sorted(right_fields)):
        keyCompare = Order._compare_to(left_key, right_key)
        if keyCompare != 0:
            return keyCompare
        value_compare = Order.compare(left_fields[left_key], right_fields[right_key])
        if value_compare != 0:
            return value_compare
    return Order._compare_to(len(left_fields), len(right_fields))
",if keyCompare != 0 :,163
"def _resolve_policy_id(cmd, policy, policy_set_definition, client):
    policy_id = policy or policy_set_definition
    if not is_valid_resource_id(policy_id):
        if policy:
            policy_def = _get_custom_or_builtin_policy(cmd, client, policy)
            policy_id = policy_def.id
        else:
            policy_set_def = _get_custom_or_builtin_policy(
                cmd, client, policy_set_definition, None, None, True
            )
            policy_id = policy_set_def.id
    return policy_id
",if policy :,166
"def _passes_cortex_depth(line, min_depth):
    """"""Do any genotypes in the cortex_var VCF line passes the minimum depth requirement?""""""
    parts = line.split(""\t"")
    cov_index = parts[8].split("":"").index(""COV"")
    passes_depth = False
    for gt in parts[9:]:
        cur_cov = gt.split("":"")[cov_index]
        cur_depth = sum(int(x) for x in cur_cov.split("",""))
        if cur_depth >= min_depth:
            passes_depth = True
    return passes_depth
",if cur_depth >= min_depth :,149
"def __init__(self, itemtype, cnf={}, *, master=None, **kw):
    if not master:
        if ""refwindow"" in kw:
            master = kw[""refwindow""]
        elif ""refwindow"" in cnf:
            master = cnf[""refwindow""]
        else:
            master = tkinter._default_root
            if not master:
                raise RuntimeError(
                    ""Too early to create display style: "" ""no root window""
                )
    self.tk = master.tk
    self.stylename = self.tk.call(""tixDisplayStyle"", itemtype, *self._options(cnf, kw))
",if not master :,167
"def serialize_groups_for_summary(node):
    groups = node.osf_groups
    n_groups = len(groups)
    group_string = """"
    for index, group in enumerate(groups):
        if index == n_groups - 1:
            separator = """"
        elif index == n_groups - 2:
            separator = "" & ""
        else:
            separator = "", ""
        group_string = group_string + group.name + separator
    return group_string
",if index == n_groups - 1 :,125
"def do(txn):
    txn.execute(
        ""SELECT valid_to, mode, caseset, spec FROM testspec WHERE id = ?"", [specId]
    )
    row = txn.fetchone()
    if row is None:
        raise Exception(""no test specification with ID '%s'"" % specId)
    else:
        validTo, mode, caseset, spec = row
        if validTo is not None:
            raise Exception(""test spec no longer active"")
        if not self._css.has_key(caseset):
            raise Exception(""case set %s not loaded in database"" % caseset)
        spec = json.loads(spec)
        res = self._css[caseset].generateCasesByTestee(spec)
        return res
",if not self . _css . has_key ( caseset ) :,188
"def get_and_set_titles(self):
    all_titles = []
    for page in self.pages:
        if page.orig_phrase != """":
            all_titles.append(page.orig_phrase)
            all_titles.append(page.orig_phrase_norm)
        if page.wiki_title != """":
            all_titles.append(page.wiki_title)
            all_titles.append(page.wiki_title_norm)
    return set(all_titles)
","if page . wiki_title != """" :",127
"def spool_print(*args, **kwargs):
    with _print_lock:
        if framework.Framework._spool:
            framework.Framework._spool.write(f""{args[0]}{os.linesep}"")
            framework.Framework._spool.flush()
        # disable terminal output for server jobs
        if framework.Framework._mode == Mode.JOB:
            return
        # new print function must still use the old print function via the backup
        builtins._print(*args, **kwargs)
",if framework . Framework . _spool :,126
"def matches(self, filepath):
    matched = False
    parent_path = os.path.dirname(filepath)
    parent_path_dirs = split_path(parent_path)
    for pattern in self.patterns:
        negative = pattern.exclusion
        match = pattern.match(filepath)
        if not match and parent_path != """":
            if len(pattern.dirs) <= len(parent_path_dirs):
                match = pattern.match(
                    os.path.sep.join(parent_path_dirs[: len(pattern.dirs)])
                )
        if match:
            matched = not negative
    return matched
","if not match and parent_path != """" :",165
"def __str__(self, prefix="""", printElemNumber=0):
    res = """"
    cnt = 0
    for e in self.task_:
        elm = """"
        if printElemNumber:
            elm = ""(%d)"" % cnt
        res += prefix + (""Task%s {\n"" % elm)
        res += e.__str__(prefix + ""  "", printElemNumber)
        res += prefix + ""}\n""
        cnt += 1
    return res
",if printElemNumber :,118
"def when(self, matches, context):
    ret = []
    for to_check in matches.range(
        predicate=lambda match: ""has-neighbor-before"" in match.tags
    ):
        next_match = matches.next(to_check, index=0)
        next_group = matches.markers.next(
            to_check, lambda marker: marker.name == ""group"", 0
        )
        if next_group and (not next_match or next_group.start < next_match.start):
            next_match = next_group
        if next_match and not matches.input_string[
            to_check.end : next_match.start
        ].strip(seps):
            break
        ret.append(to_check)
    return ret
",if next_group and ( not next_match or next_group . start < next_match . start ) :,198
"def get_coeffs(e):
    coeffs = []
    for du in all_delu_dict.keys():
        if type(self.as_coeffs_dict[e]).__name__ == ""float"":
            coeffs.append(self.as_coeffs_dict[e])
        elif du in self.as_coeffs_dict[e].keys():
            coeffs.append(self.as_coeffs_dict[e][du])
        else:
            coeffs.append(0)
    return np.array(coeffs)
",elif du in self . as_coeffs_dict [ e ] . keys ( ) :,132
"def clean(self):
    username = self.cleaned_data.get(""username"")
    password = self.cleaned_data.get(""password"")
    message = ERROR_MESSAGE
    if username and password:
        self.user_cache = authenticate(username=username, password=password)
        if self.user_cache is None:
            raise ValidationError(
                message % {""username"": self.username_field.verbose_name}
            )
        elif not self.user_cache.is_active or not self.user_cache.is_staff:
            raise ValidationError(
                message % {""username"": self.username_field.verbose_name}
            )
    return self.cleaned_data
",elif not self . user_cache . is_active or not self . user_cache . is_staff :,177
"def moveFailedFolder(filepath, failed_folder):
    if config.Config().failed_move():
        root_path = str(pathlib.Path(filepath).parent)
        file_name = pathlib.Path(filepath).name
        destination_path = root_path + ""/"" + failed_folder + ""/""
        if config.Config().soft_link():
            print(""[-]Create symlink to Failed output folder"")
            os.symlink(filepath, destination_path + ""/"" + file_name)
        else:
            print(""[-]Move to Failed output folder"")
            shutil.move(filepath, destination_path)
    return
",if config . Config ( ) . soft_link ( ) :,155
"def test_save_mp3(self, test_mode, bit_rate):
    if test_mode in [""fileobj"", ""bytesio""]:
        if bit_rate is not None and bit_rate < 1:
            raise unittest.SkipTest(
                ""mp3 format with variable bit rate is known to ""
                ""not yield the exact same result as sox command.""
            )
    self.assert_save_consistency(""mp3"", compression=bit_rate, test_mode=test_mode)
",if bit_rate is not None and bit_rate < 1 :,125
"def _upstream_nodes_executed(self, node: pipeline_pb2.PipelineNode) -> bool:
    """"""Returns `True` if all the upstream nodes have been successfully executed.""""""
    upstream_nodes = [
        node
        for node_id, node in self._node_map.items()
        if node_id in set(node.upstream_nodes)
    ]
    if not upstream_nodes:
        return True
    for node in upstream_nodes:
        upstream_node_executions = task_gen_utils.get_executions(
            self._mlmd_handle, node
        )
        if not task_gen_utils.is_latest_execution_successful(upstream_node_executions):
            return False
    return True
",if not task_gen_utils . is_latest_execution_successful ( upstream_node_executions ) :,189
"def reinit():
    for name, var in _ns_registry._registry[u""pixie.stdlib""]._registry.iteritems():
        name = munge(name)
        if name in globals():
            continue
        if var.is_defined() and isinstance(var.deref(), BaseCode):
            globals()[name] = unwrap(var)
        else:
            globals()[name] = var
",if name in globals ( ) :,102
"def i2repr(self, pkt, x):
    if type(x) is list or type(x) is tuple:
        return repr(x)
    if self.multi:
        r = []
    else:
        r = """"
    i = 0
    while x:
        if x & 1:
            if self.multi:
                r += [self.names[i]]
            else:
                r += self.names[i]
        i += 1
        x >>= 1
    if self.multi:
        r = ""+"".join(r)
    return r
",if self . multi :,154
"def prompts_dict(self, *args, **kwargs):
    r = super(WorkflowJobNode, self).prompts_dict(*args, **kwargs)
    # Explanation - WFJT extra_vars still break pattern, so they are not
    # put through prompts processing, but inventory and others are only accepted
    # if JT prompts for it, so it goes through this mechanism
    if self.workflow_job:
        if self.workflow_job.inventory_id:
            # workflow job inventory takes precedence
            r[""inventory""] = self.workflow_job.inventory
        if self.workflow_job.char_prompts:
            r.update(self.workflow_job.char_prompts)
    return r
",if self . workflow_job . char_prompts :,175
"def did_evm_write_storage_callback(self, state, address, offset, value):
    # if in potential DAO check that write to storage values read before
    # the ""send""
    for location, reads in self._get_location_and_reads(state):
        for address_i, offset_i in reads:
            if address_i == address:
                if state.can_be_true(offset == offset_i):
                    self.add_finding(state, *location)
",if state . can_be_true ( offset == offset_i ) :,128
"def update_quality_inspection(self):
    if self.inspection_required:
        reference_type = reference_name = """"
        if self.docstatus == 1:
            reference_name = self.name
            reference_type = ""Stock Entry""
        for d in self.items:
            if d.quality_inspection:
                frappe.db.set_value(
                    ""Quality Inspection"",
                    d.quality_inspection,
                    {
                        ""reference_type"": reference_type,
                        ""reference_name"": reference_name,
                    },
                )
",if self . docstatus == 1 :,178
"def _target(self):
    if self.setup is not None:
        self.setup.push_thread()
    try:
        while self.running:
            record = self.subscriber.recv()
            if record:
                try:
                    self.queue.put(record, timeout=0.05)
                except Full:
                    pass
    finally:
        if self.setup is not None:
            self.setup.pop_thread()
",if self . setup is not None :,128
"def check(self):
    global MySQLdb
    import MySQLdb
    try:
        args = {}
        if mysql_user:
            args[""user""] = mysql_user
        if mysql_pwd:
            args[""passwd""] = mysql_pwd
        if mysql_host:
            args[""host""] = mysql_host
        if mysql_port:
            args[""port""] = mysql_port
        if mysql_socket:
            args[""unix_socket""] = mysql_socket
        self.db = MySQLdb.connect(**args)
    except Exception as e:
        raise Exception(""Cannot interface with MySQL server: %s"" % e)
",if mysql_user :,167
"def writeBool(self, bool):
    if self.state == BOOL_WRITE:
        if bool:
            ctype = CompactType.TRUE
        else:
            ctype = CompactType.FALSE
        self.__writeFieldHeader(ctype, self.__bool_fid)
    elif self.state == CONTAINER_WRITE:
        if bool:
            self.__writeByte(CompactType.TRUE)
        else:
            self.__writeByte(CompactType.FALSE)
    else:
        raise AssertionError(""Invalid state in compact protocol"")
",if bool :,141
"def TryMerge(self, d):
    while d.avail() > 0:
        tt = d.getVarInt32()
        if tt == 10:
            self.set_module(d.getPrefixedString())
            continue
        if tt == 18:
            self.set_version(d.getPrefixedString())
            continue
        if tt == 24:
            self.set_instances(d.getVarInt64())
            continue
        if tt == 0:
            raise ProtocolBuffer.ProtocolBufferDecodeError
        d.skipData(tt)
",if tt == 10 :,150
"def init_panel(self):
    if not hasattr(self, ""output_view""):
        if is_ST3():
            self.output_view = self.window.create_output_panel(""markdown"")
        else:
            self.output_view = self.window.get_output_panel(""markdown"")
",if is_ST3 ( ) :,79
"def sql(self, engine):
    adapter = get_adapter(engine)
    tokens = [self.name, adapter.type_to_sql(self.type, self.limit)]
    for k, v in self.options.items():
        result = adapter.column_option_to_sql(self.name, k, v)
        if result is None:
            continue
        elif isinstance(result, dict):  # a way for column options to add constraints
            self.constraints.append(result[""constraint""])
        else:
            tokens.append(result)
    return "" "".join(tokens)
",if result is None :,150
"def get_igst_invoices(self):
    self.igst_invoices = []
    for d in self.tax_details:
        is_igst = True if d[1] in self.gst_accounts.igst_account else False
        if is_igst and d[0] not in self.igst_invoices:
            self.igst_invoices.append(d[0])
",if is_igst and d [ 0 ] not in self . igst_invoices :,102
"def updateParticle(part, best, phi1, phi2):
    u1 = numpy.random.uniform(0, phi1, len(part))
    u2 = numpy.random.uniform(0, phi2, len(part))
    v_u1 = u1 * (part.best - part)
    v_u2 = u2 * (best - part)
    part.speed += v_u1 + v_u2
    for i, speed in enumerate(part.speed):
        if abs(speed) < part.smin:
            part.speed[i] = math.copysign(part.smin, speed)
        elif abs(speed) > part.smax:
            part.speed[i] = math.copysign(part.smax, speed)
    part += part.speed
",elif abs ( speed ) > part . smax :,197
"def summaries_with_matching_keyword(keyword, summary_dir):
    """"""Yields summary protos matching given keyword from event file.""""""
    event_paths = tf.io.gfile.glob(os.path.join(summary_dir, ""events*""))
    for event in tf.compat.v1.train.summary_iterator(event_paths[-1]):
        if event.summary is not None:
            for value in event.summary.value:
                if keyword in value.tag:
                    logging.error(event)
                    yield event.summary
",if event . summary is not None :,139
"def _RemoveToken(self, doc_id, token):
    """"""Removes a token occurrence for a document.""""""
    if token in self._inverted_index:
        postings = self._inverted_index[token]
        postings.Remove(doc_id, token.position)
        if not postings.postings:
            del self._inverted_index[token]
",if not postings . postings :,90
"def check_recursive_filters(self, space, name):
    for the_filter in self.filterdb.get_filters(space):
        for rule in the_filter.get_rules():
            values = list(rule.values())
            if issubclass(rule.__class__, MatchesFilterBase) and (name in values):
                return True
    return False
","if issubclass ( rule . __class__ , MatchesFilterBase ) and ( name in values ) :",91
"def main():
    for filename in sys.argv[1:]:
        if os.path.isdir(filename):
            print(filename, ""Directory!"")
            continue
        with open(filename, ""rb"") as f:
            data = f.read()
        if b""\0"" in data:
            print(filename, ""Binary!"")
            continue
        newdata = data.replace(b""\r\n"", b""\n"")
        if newdata != data:
            print(filename)
            with open(filename, ""wb"") as f:
                f.write(newdata)
","if b""\0"" in data :",157
"def fit(self, dataset, intent):
    self.language = dataset[""language""]
    self.slots_keywords = dict()
    utterances = dataset[""intents""][intent][""utterances""]
    for utterance in utterances:
        for chunk in utterance[""data""]:
            if ""slot_name"" in chunk:
                text = chunk[""text""]
                if self.config.get(""lowercase"", False):
                    text = text.lower()
                self.slots_keywords[text] = [chunk[""entity""], chunk[""slot_name""]]
    return self
","if ""slot_name"" in chunk :",141
"def linkGradient(self, slaveGradient, connect=True):
    if connect:
        fn = lambda g, slave=slaveGradient: slave.restoreState(g.saveState())
        self.linkedGradients[id(slaveGradient)] = fn
        self.sigGradientChanged.connect(fn)
        self.sigGradientChanged.emit(self)
    else:
        fn = self.linkedGradients.get(id(slaveGradient), None)
        if fn:
            self.sigGradientChanged.disconnect(fn)
",if fn :,129
"def _get_field_values(serial_str, field_name):
    ret_list = []
    stream = StringIO(serial_str)
    for obj_dict in yaml.safe_load(stream):
        if ""fields"" in obj_dict and field_name in obj_dict[""fields""]:
            field_value = obj_dict[""fields""][field_name]
            # yaml.safe_load will return non-string objects for some
            # of the fields we are interested in, this ensures that
            # everything comes back as a string
            if isinstance(field_value, six.string_types):
                ret_list.append(field_value)
            else:
                ret_list.append(str(field_value))
    return ret_list
","if ""fields"" in obj_dict and field_name in obj_dict [ ""fields"" ] :",195
"def scrapeHeadlines(text):
    headlines = """"
    lines = text.splitlines()
    for line in lines:
        if string.find(line, ""<a href"") == 0:
            pos1 = string.find(line, ""<b>"")
            if pos1 > 0:
                pos2 = string.find(line, ""</b>"")
                if pos2 > 0:
                    headlines += line[pos1 + len(""<b>"") : pos2] + "".\n""
    return headlines
",if pos2 > 0 :,133
"def getCVEActions(self, cve, **args):
    actions = []
    for plugin in self.getWebPlugins():
        try:
            actions_ = plugin.getCVEActions(cve, **args)
            if actions_:
                for action in actions_:
                    action[""auth""] = plugin.requiresAuth
                    action[""plugin""] = plugin.getUID()
                    actions.append(action)
        except Exception as e:
            print(""[!] Plugin %s failed on fetching CVE actions!"" % plugin.getName())
            print(""[!]  -> %s"" % e)
    return actions
",if actions_ :,166
"def _sensors_to_fields(oldrec, sensor_map):
    # map a record with observation names to a record with db field names
    if oldrec:
        newrec = dict()
        for k in sensor_map:
            if sensor_map[k] in oldrec:
                newrec[k] = oldrec[sensor_map[k]]
        if newrec:
            newrec[""dateTime""] = oldrec[""dateTime""]
            newrec[""usUnits""] = oldrec[""usUnits""]
            return newrec
    return None
",if sensor_map [ k ] in oldrec :,146
"def rdd_generator():
    while not tf_feed.should_stop():
        batch = tf_feed.next_batch(1)
        if len(batch[""x""]) > 0:
            features = batch[""x""][0]
            label = batch[""y_""][0]
            yield (features, label)
        else:
            return
","if len ( batch [ ""x"" ] ) > 0 :",92
"def _get_modules(fn):
    finder = modulefinder.ModuleFinder()
    finder.run_script(fn)
    all = []
    for m in finder.modules.values():
        if not isinstance(m, modulefinder.Module):
            continue
        if not m.__file__:
            continue
        # skip shared object files
        if m.__file__.endswith("".so""):
            continue
        # skip mac system stuff...
        # FIXME: would need to augment with  other OS's system stuff
        if m.__file__.startswith(""/Library/Frameworks""):
            continue
        all.append(m)
    return all
","if m . __file__ . startswith ( ""/Library/Frameworks"" ) :",162
"def clean(self):
    d = super().clean()
    if d[""issue_giftcard""]:
        if d[""tax_rule""] and d[""tax_rule""].rate > 0:
            self.add_error(
                ""tax_rule"",
                _(
                    ""Gift card products should not be associated with non-zero tax rates since sales tax will be applied when the gift card is redeemed.""
                ),
            )
        if d[""admission""]:
            self.add_error(
                ""admission"",
                _(
                    ""Gift card products should not be admission products at the same time.""
                ),
            )
    return d
","if d [ ""admission"" ] :",184
"def is_filtered_inherited_member(name: str, obj: Any) -> bool:
    if inspect.isclass(self.object):
        for cls in self.object.__mro__:
            if cls.__name__ == self.options.inherited_members and cls != self.object:
                # given member is a member of specified *super class*
                return True
            elif name in cls.__dict__:
                return False
            elif name in self.get_attr(cls, ""__annotations__"", {}):
                return False
            elif isinstance(obj, ObjectMember) and obj.class_ is cls:
                return False
    return False
","elif isinstance ( obj , ObjectMember ) and obj . class_ is cls :",167
"def dictToKW(d):
    out = []
    items = list(d.items())
    items.sort()
    for k, v in items:
        if not isinstance(k, str):
            raise NonFormattableDict(""%r ain't a string"" % k)
        if not r.match(k):
            raise NonFormattableDict(""%r ain't an identifier"" % k)
        out.append(""\n\0{}={},"".format(k, prettify(v)))
    return """".join(out)
",if not r . match ( k ) :,131
"def report_add_status(torrentlist, succ_cnt, fail_cnt, fail_msgs):
    if fail_cnt == 0:
        torrentlist.report_message(
            ""Torrents Added"", ""{!success!}Successfully added %d torrent(s)"" % succ_cnt
        )
    else:
        msg = (
            ""{!error!}Failed to add the following %d torrent(s):\n {!input!}"" % fail_cnt
        ) + ""\n "".join(fail_msgs)
        if succ_cnt != 0:
            msg += ""\n \n{!success!}Successfully added %d torrent(s)"" % succ_cnt
        torrentlist.report_message(""Torrent Add Report"", msg)
",if succ_cnt != 0 :,184
"def merge(self, other):
    d = self._name2ft
    for name, (f, t) in other._name2ft.items():
        if name in d:
            # Don't print here by default, since doing
            #     so breaks some of the buildbots
            # print(""*** DocTestRunner.merge: '"" + name + ""' in both"" \
            #    "" testers; summing outcomes."")
            f2, t2 = d[name]
            f = f + f2
            t = t + t2
        d[name] = f, t
",if name in d :,156
"def handle_command(self, parameters):
    response = """"
    for ip_token in parameters:
        if is_ip(ip_token):
            ip = netaddr.IPNetwork(ip_token)[0]
            if not (ip.is_loopback() or ip.is_private() or ip.is_reserved()):
                response += ""{0} location: {1}\n"".format(
                    ip_token, ip_location(ip_token)
                )
            else:
                response += ""{0}: hrm...loopback? private ip?\n"".format(ip_token)
        else:
            response = ""{0} is not an IP address"".format(ip_token)
    return response
",if is_ip ( ip_token ) :,186
"def letterrange(first, last, charset):
    for k in range(len(last)):
        for x in product(*[chain(charset)] * (k + 1)):
            result = """".join(x)
            if first:
                if first != result:
                    continue
                else:
                    first = None
            yield result
            if result == last:
                return
",if first :,112
"def artifacts_base_dir(self):
    if not self._artifacts_base_dir:
        try:
            artifacts_base_dir = os.path.abspath(
                self.get_option(self.SECTION, ""artifacts_base_dir"")
            )
        except ValidationError:
            artifacts_base_dir = os.path.abspath(""logs"")
        if not os.path.exists(artifacts_base_dir):
            os.makedirs(artifacts_base_dir)
            os.chmod(self.artifacts_base_dir, 0o755)
        self._artifacts_base_dir = artifacts_base_dir
    return self._artifacts_base_dir
",if not os . path . exists ( artifacts_base_dir ) :,172
"def _extract_changes(doc_map, changes, read_time):
    deletes = []
    adds = []
    updates = []
    for name, value in changes.items():
        if value == ChangeType.REMOVED:
            if name in doc_map:
                deletes.append(name)
        elif name in doc_map:
            if read_time is not None:
                value.read_time = read_time
            updates.append(value)
        else:
            if read_time is not None:
                value.read_time = read_time
            adds.append(value)
    return (deletes, adds, updates)
",if name in doc_map :,173
"def __setattr__(self, name, val):
    BitmapSprite.__setattr__(self, name, val)
    if name in (
        ""name"",
        ""size"",
    ):  # no other reason to discard cache than just on path change
        if self.__dict__.get(""name"") and self.__dict__.get(""size""):
            self.image_data = self.theme.load_icon(self.name, self.size, 0)
        else:
            self.image_data = None
","if self . __dict__ . get ( ""name"" ) and self . __dict__ . get ( ""size"" ) :",125
"def extract_deps(file):
    # ~ print('Extracting from %s' % file)
    deps = set()
    for line in open(file).readlines():
        line = line.strip()
        if line.startswith(""import"") or line.startswith(""from""):
            words = line.split()
            if words[0] == ""import"" or (words[0] == ""from"" and words[2] == ""import""):
                deps.add(words[1])
    return deps
","if line . startswith ( ""import"" ) or line . startswith ( ""from"" ) :",124
"def run_query(self, query, user):
    connection = self._get_connection()
    statement = None
    error = None
    try:
        statement = connection.execute(query)
        columns = [
            {""name"": n, ""friendly_name"": n, ""type"": _type_mapper(t)}
            for (n, t) in statement.columns().items()
        ]
        cnames = statement.column_names()
        rows = [dict(zip(cnames, row)) for row in statement]
        data = {""columns"": columns, ""rows"": rows}
        json_data = json_dumps(data)
    finally:
        if statement is not None:
            statement.close()
        connection.close()
    return json_data, error
",if statement is not None :,197
"def find_setup_py_above(a_file):
    ""Return the directory containing setup.py somewhere above *a_file*""
    root = os.path.dirname(os.path.abspath(a_file))
    while not os.path.exists(os.path.join(root, ""setup.py"")):
        prev, root = root, os.path.dirname(root)
        if root == prev:
            # Let's avoid infinite loops at root
            raise NoSetupPyFound(""could not find my setup.py above %r"" % (a_file,))
    return root
",if root == prev :,142
"def check_index(self, is_sorted=True, unique=True, index=None):
    """"""Sanity checks""""""
    if not index:
        index = self.index
    if is_sorted:
        test = pd.DataFrame(lrange(len(index)), index=index)
        test_sorted = test.sort()
        if not test.index.equals(test_sorted.index):
            raise Exception(""Data is not be sorted"")
    if unique:
        if len(index) != len(index.unique()):
            raise Exception(""Duplicate index entries"")
",if len ( index ) != len ( index . unique ( ) ) :,142
"def _compare_address_strings(self, a, b):
    # IPv6 address from different requests might be different
    a_segments = a.count("":"")
    b_segments = b.count("":"")
    if a_segments and b_segments:
        if a_segments == b_segments and a_segments in (4, 5, 6, 7):
            return True
        if a.rstrip("":"").startswith(b.rstrip("":"")) or b.rstrip("":"").startswith(
            a.rstrip("":"")
        ):
            return True
        if a_segments >= 2 and b_segments >= 2 and a.split("":"")[:2] == b.split("":"")[:2]:
            return True
    return a.split(""."", 1)[-1] == b.split(""."", 1)[-1]
","if a_segments == b_segments and a_segments in ( 4 , 5 , 6 , 7 ) :",187
"def collect(self):
    for vacb in self.GetVACBs():
        filename = vacb.SharedCacheMap.FileObject.file_name_with_drive()
        if filename:
            yield (
                vacb,
                bool(self.kernel_address_space.vtop(vacb.BaseAddress.v())),
                vacb.BaseAddress.v(),
                vacb.Overlay.FileOffset.QuadPart,
                filename,
            )
",if filename :,133
"def _visit_table(self, expr):
    node = expr.op()
    if isinstance(expr, ir.TableExpr):
        base_table = _find_blocking_table(expr)
        if base_table is not None:
            base_node = base_table.op()
            if self._is_root(base_node):
                pass
            else:
                # Foreign ref
                self.foreign_table = expr
    else:
        if not node.blocks():
            for arg in node.flat_args():
                if isinstance(arg, ir.Expr):
                    self._visit(arg)
",if not node . blocks ( ) :,172
"def channel_details(self) -> SnapChannelDetails:
    if self._channel_details is None:
        channel = self._payload.get(""channel"")
        if channel is None:
            # Shouldn't happen, but raise an error if it does.
            raise RuntimeError(f""no channel found for {self._payload!r}"")
        self._channel_details = SnapChannelDetails(channel)
    return self._channel_details
",if channel is None :,107
"def __setattr__(self, attr, val):
    if hasattr(self, attr):
        old = getattr(self, attr)
        if isinstance(old, Setting):
            if isinstance(val, Setting):
                raise ValueError(
                    ""Attempting to reassign setting %s with %s"" % (old, val)
                )
            log.warn(""Setting attr %s via __setattr__ instead of set()!"", attr)
            return old.set(val)
    log.debug(""Setting {%s => %s}"" % (attr, val))
    return object.__setattr__(self, attr, val)
","if isinstance ( val , Setting ) :",155
"def FindEnclosingBracketGroup(input_str):
    stack = []
    start = -1
    for index, char in enumerate(input_str):
        if char in LBRACKETS:
            stack.append(char)
            if start == -1:
                start = index
        elif char in BRACKETS:
            if not stack:
                return (-1, -1)
            if stack.pop() != BRACKETS[char]:
                return (-1, -1)
            if not stack:
                return (start, index + 1)
    return (-1, -1)
",if not stack :,163
"def copy_layer(
    layer,
    keep_bias=True,
    name_template=None,
    weights=None,
    reuse_symbolic_tensors=True,
    **kwargs
):
    config = layer.get_config()
    if name_template is None:
        config[""name""] = None
    else:
        config[""name""] = name_template % config[""name""]
    if keep_bias is False and config.get(""use_bias"", False):
        config[""use_bias""] = False
        if weights is None:
            if reuse_symbolic_tensors:
                weights = layer.weights[:-1]
            else:
                weights = layer.get_weights()[:-1]
    return get_layer_from_config(layer, config, weights=weights, **kwargs)
",if reuse_symbolic_tensors :,200
"def find_go_srcs(path):
    srcs, tests = [], []
    for name in os.listdir(path):
        if name.startswith(""."") or not name.endswith("".go""):
            continue
        if os.path.isfile(os.path.join(path, name)):
            if name.endswith(""_test.go""):
                tests.append(name)
            else:
                srcs.append(name)
    return srcs, tests
","if name . endswith ( ""_test.go"" ) :",120
"def first_text(self, node):
    """"""find first paragraph to use as a summary""""""
    if node.tagname == ""paragraph"":
        return deepcopy(node)
    else:
        for child in node:
            if hasattr(child, ""tagname""):
                ans = self.first_text(child)
                if ans:
                    return ans
    return None
","if hasattr ( child , ""tagname"" ) :",99
"def ServerInference(self):
    candidates = []
    score = []
    for symbol in self.symbols:
        for m in symbol.getMessages():
            dst = m.getPattern()[0]
            if dst in candidates:
                score[candidates.index(dst)] += 1
            else:
                candidates.append(dst)
                score.append(1)
    print(candidates)
    if score.count(max(score)) == 1 and len(candidates) > 2:
        self.server = candidates[score.index(max(score))]
",if dst in candidates :,146
"def generateMapItemTypedNode(self, key, value):
    if type(value) == SigmaRegularExpressionModifier:
        regex = str(value)
        # Regular Expressions have to match the full value in QRadar
        if not (regex.startswith(""^"") or regex.startswith("".*"")):
            regex = "".*"" + regex
        if not (regex.endswith(""$"") or regex.endswith("".*"")):
            regex = regex + "".*""
        return ""%s MATCHES %s"" % (self.cleanKey(key), self.generateValueNode(regex))
    else:
        raise NotImplementedError(
            ""Type modifier '{}' is not supported by backend"".format(value.identifier)
        )
","if not ( regex . endswith ( ""$"" ) or regex . endswith ( "".*"" ) ) :",165
"def get_max_vertical_scroll() -> int:
    # Make sure that the cursor line is not above the top.
    prev_lineno = ui_content.cursor_position.y
    used_height = 0
    for lineno in range(ui_content.cursor_position.y - 1, -1, -1):
        used_height += get_line_height(lineno)
        if used_height > scroll_offsets_top:
            return prev_lineno
        else:
            prev_lineno = lineno
    return prev_lineno
",if used_height > scroll_offsets_top :,132
"def _options_values(self):
    """"""Simulate option values for partially configured objects.""""""
    try:
        return self.__options_values
    except AttributeError:
        self.__options_values = {**self.keywords}
        position = 0
        for name, option in self.func.__options__:
            if not option.positional:
                break  # no positional left
            if name in self.keywords:
                continue  # already fulfilled
            self.__options_values[name] = (
                self.args[position] if len(self.args) >= position + 1 else None
            )
            position += 1
        return self.__options_values
",if name in self . keywords :,175
"def key(self):
    addr = self.m(""key"").obj_offset
    addr = self.read_ptr(addr)
    ret = """"
    if addr:
        ret = self.obj_vm.read(addr, 256)
        if ret:
            idx = ret.find(""\x00"")
            if idx != -1:
                ret = ret[:idx]
        else:
            ret = """"
    return ret
",if ret :,114
"def get_file_path(self, filepath, token):
    try:
        encoded_path, _, user = self.updown_auth_manager.get_resource_info(token)
        if not self._valid_path(filepath, encoded_path):
            logger.info(""Invalid path file!! %s: %s"" % (user, filepath))
            raise NotFoundException(""File not found"")
        logger.debug(""Get file: user=%s path=%s"" % (user, filepath))
        file_path = os.path.normpath(os.path.join(self.base_store_folder, encoded_path))
        return file_path
    except (jwt.ExpiredSignature, jwt.DecodeError, AttributeError):
        raise NotFoundException(""File not found"")
","if not self . _valid_path ( filepath , encoded_path ) :",187
"def validate_and_handle(self):
    valid = self.validate(set_cursor=True)
    if valid:
        if self.accept_handler:
            keep_text = self.accept_handler(self)
        else:
            keep_text = False
        if not keep_text:
            self.reset()
",if self . accept_handler :,86
"def document_type(self):
    if isinstance(self.document_type_obj, basestring):
        if self.document_type_obj == RECURSIVE_REFERENCE_CONSTANT:
            self.document_type_obj = self.owner_document
        else:
            self.document_type_obj = get_document(self.document_type_obj)
    return self.document_type_obj
",if self . document_type_obj == RECURSIVE_REFERENCE_CONSTANT :,99
"def _get_closest_end(end_after, begin_after):
    """"""returns the closest \\end, that is open""""""
    end_iter = iter(end_after)
    begin_iter = iter(begin_after)
    while True:
        try:
            e = next(end_iter)
        except:
            raise NoEnvError(""No closing environment detected"")
        try:
            b = next(begin_iter)
        except:
            break
        if not e.begin() > b.begin():
            break
    return e
",if not e . begin ( ) > b . begin ( ) :,145
"def group_curves(self, curves):
    result = [[curves[0]]]
    tolerance = self.concat_tolerance
    for curve1, curve2 in zip(curves, curves[1:]):
        _, t_max_1 = curve1.get_u_bounds()
        t_min_2, _ = curve2.get_u_bounds()
        end1 = curve1.evaluate(t_max_1)
        begin2 = curve2.evaluate(t_min_2)
        distance = np.linalg.norm(begin2 - end1)
        if distance > tolerance:
            result.append([curve2])
        else:
            result[-1].append(curve2)
    return result
",if distance > tolerance :,176
"def iteraddcolumn(table, field, col, index, missing):
    it = iter(table)
    hdr = next(it)
    # determine position of new column
    if index is None:
        index = len(hdr)
    # construct output header
    outhdr = list(hdr)
    outhdr.insert(index, field)
    yield tuple(outhdr)
    # construct output data
    for row, val in izip_longest(it, col, fillvalue=missing):
        # run out of rows?
        if row == missing:
            row = [missing] * len(hdr)
        outrow = list(row)
        outrow.insert(index, val)
        yield tuple(outrow)
",if row == missing :,178
"def validate_is_admin(self, attrs, source):
    project = attrs.get(""project"", None if self.object is None else self.object.project)
    if project is None:
        return attrs
    if self.object and self.object.user:
        if self.object.user.id == project.owner_id and not attrs[source]:
            raise ValidationError(_(""The project owner must be admin.""))
        if not services.project_has_valid_admins(
            project, exclude_user=self.object.user
        ):
            raise ValidationError(
                _(""At least one user must be an active admin for this project."")
            )
    return attrs
",if self . object . user . id == project . owner_id and not attrs [ source ] :,170
"def handle_periodic(self):
    if self._closed:
        if self._server_socket:
            self._eventloop.remove(self._server_socket)
            self._server_socket.close()
            self._server_socket = None
            logging.info(""closed TCP port %d"", self._listen_port)
        for handler in list(self._fd_to_handlers.values()):
            handler.destroy()
    self._sweep_timeout()
",if self . _server_socket :,120
"def get_item(type_, preference):
    items = {}
    for item in playlist.findall(""./info/%s/item"" % type_):
        lang, label = xpath_text(item, ""lg"", default=None), xpath_text(
            item, ""label"", default=None
        )
        if lang and label:
            items[lang] = label.strip()
    for p in preference:
        if items.get(p):
            return items[p]
",if items . get ( p ) :,121
"def save_all_changed_configs(self):
    """"""Save configuration changes to the user config file.""""""
    has_changes = False
    for ext_name in self.extensions:
        options = self.extensions[ext_name]
        for opt in options:
            if self.set_user_value(ext_name, opt):
                has_changes = True
    if has_changes:
        self.userCfg.Save()
","if self . set_user_value ( ext_name , opt ) :",109
"def extract_validators(namespace: Dict[str, Any]) -> Dict[str, List[Validator]]:
    validators: Dict[str, List[Validator]] = {}
    for var_name, value in namespace.items():
        validator_config = getattr(value, VALIDATOR_CONFIG_KEY, None)
        if validator_config:
            fields, v = validator_config
            for field in fields:
                if field in validators:
                    validators[field].append(v)
                else:
                    validators[field] = [v]
    return validators
",if validator_config :,147
"def _bindTable(self, tableName, create=False):
    for attempt in retry_azure():
        with attempt:
            try:
                exists = self.tableService.exists(table_name=tableName)
            except AzureMissingResourceHttpError as e:
                if e.status_code != 404:
                    raise
            else:
                if exists:
                    return AzureTable(self.tableService, tableName)
            if create:
                self.tableService.create_table(tableName)
                return AzureTable(self.tableService, tableName)
            else:
                return None
",if create :,184
"def extract(self):
    for battery in self.vars:
        for line in dopen(""/proc/acpi/battery/"" + battery + ""/state"").readlines():
            l = line.split()
            if len(l) < 3:
                continue
            if l[0:2] == [""remaining"", ""capacity:""]:
                remaining = int(l[2])
                continue
            elif l[0:2] == [""present"", ""rate:""]:
                rate = int(l[2])
                continue
        if rate and remaining:
            self.val[battery] = remaining * 60 / rate
        else:
            self.val[battery] = -1
","if l [ 0 : 2 ] == [ ""remaining"" , ""capacity:"" ] :",185
"def merge_syntactic_units(original_units, filtered_units, tags=None):
    units = []
    for i in range(len(original_units)):
        if filtered_units[i] == """":
            continue
        text = original_units[i]
        token = filtered_units[i]
        tag = tags[i][1] if tags else None
        sentence = SyntacticUnit(text, token, tag)
        sentence.index = i
        units.append(sentence)
    return units
","if filtered_units [ i ] == """" :",132
"def copy_grads_to_fp32(self, fp16_net, fp32_weights):
    """"""Copy gradients from fp16 model to fp32 weight copy.""""""
    for fp32_param, fp16_param in zip(fp32_weights, fp16_net.parameters()):
        if fp16_param.grad is not None:
            if fp32_param.grad is None:
                fp32_param.grad = fp32_param.data.new(fp32_param.size())
            fp32_param.grad.copy_(fp16_param.grad)
",if fp16_param . grad is not None :,141
"def gen_new_segments(datadir, spk_list):
    if not os.path.isfile(os.path.join(datadir, ""segments"")):
        raise ValueError(""no segments file found in datadir"")
    new_segments = open(os.path.join(datadir, ""new_segments""), ""w"", encoding=""utf-8"")
    segments = open(os.path.join(datadir, ""segments""), ""r"", encoding=""utf-8"")
    while True:
        line = segments.readline()
        if not line:
            break
        spk = line.split(""_"")[0]
        if spk in spk_list:
            new_segments.write(line)
    new_segments.close(), segments.close()
",if spk in spk_list :,176
"def _get_sources(include_per_machine=True, include_per_user=True):
    if _is_64bit_os():
        if include_per_user:
            yield open_source(REGISTRY_SOURCE_CU), None
        if include_per_machine:
            yield open_source(REGISTRY_SOURCE_LM), ""64bit""
            yield open_source(REGISTRY_SOURCE_LM_WOW6432), ""32bit""
    else:
        if include_per_user:
            yield open_source(REGISTRY_SOURCE_CU), ""32bit""
        if include_per_machine:
            yield open_source(REGISTRY_SOURCE_LM), ""32bit""
",if include_per_user :,175
"def AddWindowMenu(self, pMenuBar):
    if pMenuBar and self._pWindowMenu:
        pos = pMenuBar.FindMenu(wx.GetStockLabel(wx.ID_HELP, wx.STOCK_NOFLAGS))
        if pos == wx.NOT_FOUND:
            pMenuBar.Append(self._pWindowMenu, _(""&Window""))
        else:
            pMenuBar.Insert(pos, self._pWindowMenu, _(""&Window""))
",if pos == wx . NOT_FOUND :,115
"def remove(self, res):
    """"""Remove resource""""""
    msg_box = QMessageBox(
        QMessageBox.Critical,
        self.app.translate(""ResourceEdit"", ""Delete Resource""),
        self.app.translate(
            ""ResourceEdit"", ""Are you sure want to delete this resource?""
        ),
        QMessageBox.Yes | QMessageBox.No,
    )
    ret = msg_box.exec_()
    if ret == QMessageBox.Yes:
        self._resources.remove(res)
        self._resource_labels[res].hide()
        del self._resource_labels[res]
        self.on_change()
        if not self._resources:
            self.widget.hide()
        self.update_label()
",if not self . _resources :,188
"def reader(self, myself):
    ok = True
    line = """"
    while True:
        line = sys.stdin.readline().strip()
        if ok:
            if not line:
                ok = False
                continue
        elif not line:
            break
        else:
            ok = True
        self.Q.append(line)
    os.kill(myself, signal.SIGTERM)
",if not line :,112
"def _compute_ratios(counts, n_total, multilabel=False):
    computed_ratios = {}
    max_count = max(counts.values())
    for class_name, count in counts.items():
        if multilabel:
            ratio = (n_total - count) / count
        else:
            ratio = ratio = max_count / count
        computed_ratios[class_name] = ratio
    return computed_ratios
",if multilabel :,107
"def test_tags(context_obj, sagemaker_session):
    tags = [{""Key"": ""foo1"", ""Value"": ""bar1""}]
    context_obj.set_tags(tags)
    while True:
        actual_tags = sagemaker_session.sagemaker_client.list_tags(
            ResourceArn=context_obj.context_arn
        )[""Tags""]
        if actual_tags:
            break
        time.sleep(5)
    # When sagemaker-client-config endpoint-url is passed as argument to hit some endpoints,
    # length of actual tags will be greater than 1
    assert len(actual_tags) > 0
    assert [actual_tags[-1]] == tags
",if actual_tags :,173
"def step(self, action):
    """"""Repeat action, sum reward, and max over last observations.""""""
    total_reward = 0.0
    done = None
    for i in range(self._skip):
        obs, reward, done, info = self.env.step(action)
        if i == self._skip - 2:
            self._obs_buffer[0] = obs
        if i == self._skip - 1:
            self._obs_buffer[1] = obs
        total_reward += reward
        if done:
            break
    # Note that the observation on the done=True frame doesn't matter.
    max_frame = self._obs_buffer.max(axis=0)
    return max_frame, total_reward, done, info
",if i == self . _skip - 1 :,187
"def prepare_text(text, style):
    body = []
    for fragment, sty in parse_tags(text, style, subs.styles):
        fragment = fragment.replace(r""\h"", "" "")
        fragment = fragment.replace(r""\n"", ""\n"")
        fragment = fragment.replace(r""\N"", ""\n"")
        if sty.italic:
            fragment = ""<i>%s</i>"" % fragment
        if sty.underline:
            fragment = ""<u>%s</u>"" % fragment
        if sty.strikeout:
            fragment = ""<s>%s</s>"" % fragment
        if sty.drawing:
            raise ContentNotUsable
        body.append(fragment)
    return re.sub(""\n+"", ""\n"", """".join(body).strip())
",if sty . underline :,198
"def GetConvertersByClass(value_cls):
    """"""Returns all converters that take given value as an input value.""""""
    try:
        return ExportConverter.converters_cache[value_cls]
    except KeyError:
        results = [
            cls
            for cls in ExportConverter.classes.values()
            if cls.input_rdf_type == value_cls
        ]
        if not results:
            results = [DataAgnosticExportConverter]
        ExportConverter.converters_cache[value_cls] = results
        return results
",if cls . input_rdf_type == value_cls,138
"def enable(self):
    """"""enable the patch.""""""
    for patch in self.dependencies:
        patch.enable()
    if not self.enabled:
        pyv = sys.version_info[0]
        if pyv == 2:
            if self.PY2 == SKIP:
                return  # skip patch activation
            if not self.PY2:
                raise IncompatiblePatch(""Python 2 not supported!"")
        if pyv == 3:
            if self.PY3 == SKIP:
                return  # skip patch activation
            if not self.PY3:
                raise IncompatiblePatch(""Python 3 not supported!"")
        self.pre_enable()
        self.do_enable()
        self.enabled = True
",if pyv == 2 :,191
"def _maybe_uncompress(self):
    if not self._decompressed:
        compression_type = self.compression_type
        if compression_type != self.CODEC_NONE:
            data = memoryview(self._buffer)[self._pos :]
            if compression_type == self.CODEC_GZIP:
                uncompressed = gzip_decode(data)
            if compression_type == self.CODEC_SNAPPY:
                uncompressed = snappy_decode(data.tobytes())
            if compression_type == self.CODEC_LZ4:
                uncompressed = lz4_decode(data.tobytes())
            self._buffer = bytearray(uncompressed)
            self._pos = 0
    self._decompressed = True
",if compression_type == self . CODEC_LZ4 :,192
"def transform(node, filename):
    root = ast.Module(None, node, lineno=1)
    nodes = [root]
    while nodes:
        node = nodes.pop()
        node.filename = filename
        if node.__class__ in (ast.Printnl, ast.Print):
            node.dest = ast.Name(""__context"")
        elif node.__class__ is ast.Const and isinstance(node.value, str):
            try:
                node.value.decode(""ascii"")
            except UnicodeError:
                node.value = node.value.decode(""utf-8"")
        nodes.extend(node.getChildNodes())
    return root
","if node . __class__ in ( ast . Printnl , ast . Print ) :",169
"def __init__(self, json=None):
    if not json:
        self._mods = dict()
        return
    mods = collections.defaultdict(set)
    installed_path_patt = re.compile(
        "".*[\\\\/]target[\\\\/]product[\\\\/][^\\\\/]+([\\\\/].*)$""
    )
    for module in json.values():
        for path in module[""installed""]:
            match = installed_path_patt.match(path)
            if match:
                for path in module[""path""]:
                    mods[match.group(1)].add(path)
    self._mods = {
        installed_path: sorted(src_dirs) for installed_path, src_dirs in mods.items()
    }
",if match :,195
"def _findSubpath(self, path, A, B, inside):
    print(""finding"", A, B)
    sub = None
    for i in xrange(0, len(path) * 2):  # iterate twice with wrap around
        j = i % len(path)
        seg = path[j]
        if inside.isInside(seg.midPoint()):
            if eq(seg.A, A):
                sub = Path(""subp"")
            print(""seg"", sub is None, seg)
            if sub is not None:
                sub.append(seg)
            if eq(seg.B, B):
                break
    print(""found"", sub)
    return sub
","if eq ( seg . B , B ) :",180
"def on_click(self, event):
    button = event[""button""]
    if button in [self.button_next, self.button_previous]:
        if self.station_data:
            self.scrolling = True
            if button == self.button_next:
                self.active_index += 1
            elif button == self.button_previous:
                self.active_index -= 1
            self.active_index %= self.count_stations
        else:
            self.py3.prevent_refresh()
    elif button == self.button_refresh:
        self.idle_time = 0
    else:
        self.py3.prevent_refresh()
",if button == self . button_next :,178
"def __init_subclass__(cls, *, abstract=False):
    if abstract:
        return
    fields = {}
    for name in cls.__dict__:
        attr = cls.__dict__[name]
        if name.startswith(""__"") or callable(attr):
            continue
        if not isinstance(attr, CType):
            raise TypeError(f""field {cls.__name__}.{name!r} must be a Type"")
        else:
            fields[name] = attr
    cls._fields = fields
","if name . startswith ( ""__"" ) or callable ( attr ) :",125
"def add(self, geom):
    ""Add the geometry to this Geometry Collection.""
    if isinstance(geom, OGRGeometry):
        if isinstance(geom, self.__class__):
            for g in geom:
                capi.add_geom(self.ptr, g.ptr)
        else:
            capi.add_geom(self.ptr, geom.ptr)
    elif isinstance(geom, six.string_types):
        tmp = OGRGeometry(geom)
        capi.add_geom(self.ptr, tmp.ptr)
    else:
        raise OGRException(""Must add an OGRGeometry."")
","if isinstance ( geom , self . __class__ ) :",157
"def __str__(self):
    result = []
    for x in self._fields_:
        key = x[0]
        value = getattr(self, key)
        fmt = ""%s""
        if key in self._fmt_:
            fmt = self._fmt_[key]
        elif ""<default>"" in self._fmt_:
            fmt = self._fmt_[""<default>""]
        result.append((""%s: "" + fmt) % (key, value))
    return self.__class__.__name__ + ""("" + string.join(result, "", "") + "")""
","elif ""<default>"" in self . _fmt_ :",137
"def add(self, *objs):
    for obj in objs:
        if not isinstance(obj, self.model):
            raise TypeError(
                ""'%s' instance expected, got %r"" % (self.model._meta.object_name, obj)
            )
        setattr(obj, rel_field.name, self.instance)
        obj.save()
","if not isinstance ( obj , self . model ) :",95
"def _eliminate_deprecated_list_indexing(idx):
    # ""Basic slicing is initiated if the selection object is a non-array,
    # non-tuple sequence containing slice objects, [Ellipses, or newaxis
    # objects]"". Detects this case and canonicalizes to a tuple. This case is
    # deprecated by NumPy and exists for backward compatibility.
    if not isinstance(idx, tuple):
        if isinstance(idx, Sequence) and not isinstance(idx, ndarray):
            if _any(_should_unpack_list_index(i) for i in idx):
                idx = tuple(idx)
            else:
                idx = (idx,)
        else:
            idx = (idx,)
    return idx
","if isinstance ( idx , Sequence ) and not isinstance ( idx , ndarray ) :",177
"def __init__(self, parent=None, **kwargs):
    super(DefaultWidget, self).__init__(parent)
    self.parent = parent
    self.FSettings = SuperSettings.getInstance()
    self.defaultui = []
    self.allui = []
    self.__tabbyname = {}
    __defaultui = [ui(parent, self.FSettings) for ui in TabsWidget.__subclasses__()]
    for ui in __defaultui:
        if not ui.isSubitem:
            self.defaultui.append(ui)
        self.allui.append(ui)
        self.__tabbyname[ui.Name] = ui
        setattr(self.__class__, ui.ID, ui)
",if not ui . isSubitem :,173
"def onMouseMove(self, event):
    x, y = event.xdata, event.ydata
    if x is not None:
        extra_text = self.getExtraText(x, y)
        # extra_text = ""TODO:""
        if extra_text:
            self.message(""x,y=%5.4e,%5.4e %s"" % (x, y, extra_text), index=0)
        else:
            self.message(""x,y=%5.4e,%5.4e"" % (x, y), index=0)
    else:
        self.message(None)
",if extra_text :,155
"def tag_configure(self, *args, **keys):
    if len(args) == 1:
        key = args[0]
        self.tags[key] = keys
        val = keys.get(""foreground"")
        underline = keys.get(""underline"")
        if val:
            self.configDict[key] = val
        if underline:
            self.configUnderlineDict[key] = True
    else:
        g.trace(""oops"", args, keys)
",if underline :,123
"def _flatten_shape(s, index):
    if s.is_array():
        yield index, s
    else:
        assert s.is_tuple()
        for i, sub in enumerate(s.tuple_shapes()):
            subindex = index + (i,)
            if sub.is_tuple():
                yield from _flatten_shape(sub, subindex)
            else:
                yield subindex, sub
",if sub . is_tuple ( ) :,111
"def delete_if_forked(ghrequest):
    FORKED = False
    query = ""/user/repos""
    r = utils.query_request(query)
    for repo in r.json():
        if repo[""description""]:
            if ghrequest.target_repo_fullname in repo[""description""]:
                FORKED = True
                url = f""/repos/{repo['full_name']}""
                utils.query_request(url, method=""DELETE"")
    return FORKED
","if ghrequest . target_repo_fullname in repo [ ""description"" ] :",127
"def update_json(self):
    n_id = node_id(self)
    if self.autoreload:
        self.reload_json()
    if n_id not in self.json_data and self.current_text:
        self.reload_json()
    if n_id not in self.json_data:
        self.use_custom_color = True
        self.color = FAIL_COLOR
        return
    self.use_custom_color = True
    self.color = READY_COLOR
    json_data = self.json_data[n_id]
    for item in json_data:
        if item in self.outputs and self.outputs[item].is_linked:
            out = json_data[item][1]
            self.outputs[item].sv_set(out)
",if item in self . outputs and self . outputs [ item ] . is_linked :,200
"def _check_num_states(self, num_states):
    """"""Track the number of states.""""""
    self._num_states += num_states
    if self._max_num_states is not None:
        if self._num_states > self._max_num_states:
            raise RuntimeError(
                ""Too many states detected while running dynamic ""
                ""programming: got %d states but upper limit is %d.""
                % (self._num_states, self._max_num_states)
            )
",if self . _num_states > self . _max_num_states :,134
"def __del__(self):
    try:
        if self._mpz_p is not None:
            if self._initialized:
                _gmp.mpz_clear(self._mpz_p)
        self._mpz_p = None
    except AttributeError:
        pass
",if self . _mpz_p is not None :,75
"def cmp(f1, f2):
    bufsize = 1024 * 8
    with open(f1, ""rb"") as fp1, open(f2, ""rb"") as fp2:
        while True:
            b1 = fp1.read(bufsize)
            b2 = fp2.read(bufsize)
            if b1 != b2:
                return False
            if not b1:
                return True
",if b1 != b2 :,115
"def _get_changes(diff):
    """"""Get a list of changed versions from git.""""""
    changes_dict = {}
    for line in diff:
        if not line.startswith(""-"") and not line.startswith(""+""):
            continue
        if line.startswith(""+++ "") or line.startswith(""--- ""):
            continue
        name, version = parse_versioned_line(line[1:])
        if name not in changes_dict:
            changes_dict[name] = Change(name)
        if line.startswith(""-""):
            changes_dict[name].old = version
        elif line.startswith(""+""):
            changes_dict[name].new = version
    return [change for _name, change in sorted(changes_dict.items())]
","if not line . startswith ( ""-"" ) and not line . startswith ( ""+"" ) :",181
"def analyze(vw):
    for va, dest in vw.findPointers():
        # Is there a location already at the target?
        loc = vw.getLocation(dest)
        if loc is None:
            continue
        if loc[L_LTYPE] != LOC_IMPORT:
            continue
        offset, bytes = vw.getByteDef(va)
        if offset < 2:
            continue
        if bytes[offset - 2 : offset] == b""\xff\x15"":  # call [importloc]
            # If there's a pointer here, remove it.
            if vw.getLocation(va):
                vw.delLocation(va)
            vw.makeCode(va - 2)
",if vw . getLocation ( va ) :,192
"def match_blanks(self, s, i):
    if 1:  # Use Qt code to show invisibles.
        return 0
    else:  # Old code...
        if not self.showInvisibles:
            return 0
        j = i
        n = len(s)
        while j < n and s[j] == "" "":
            j += 1
        if j > i:
            self.colorRangeWithTag(s, i, j, ""blank"")
            return j - i
        else:
            return 0
",if j > i :,143
"def compress(self, data_list):
    # Differs from the default implementation: If only a time is given and no date, we consider the field empty
    if data_list:
        if data_list[0] in self.empty_values:
            return None
        if data_list[1] in self.empty_values:
            raise ValidationError(
                self.error_messages[""invalid_date""], code=""invalid_date""
            )
        result = datetime.datetime.combine(*data_list)
        return from_current_timezone(result)
    return None
",if data_list [ 1 ] in self . empty_values :,146
"def test_iter_keys(self):
    for name in (""interfaces"", ""addresses"", ""neighbours"", ""routes"", ""rules""):
        view = getattr(self.ndb, name)
        for key in view:
            assert isinstance(key, Record)
            obj = view.get(key)
            if obj is not None:
                assert isinstance(obj, RTNL_Object)
",if obj is not None :,102
"def has_selenium():
    try:
        from selenium import selenium
        globals().update(selenium=selenium)
        sel = selenium(*sel_args)
        # a little trick to see if the server is responding
        try:
            sel.do_command(""shutdown"", """")
        except Exception as e:
            if not ""Server Exception"" in str(e):
                raise
        result = True
    except ImportError:
        result = SeleniumFailed(""selenium RC not installed"")
    except Exception:
        msg = ""Error occurred initializing selenium: %s"" % e
        result = SeleniumFailed(msg)
    # overwrite has_selenium, so the same result is returned every time
    globals().update(has_selenium=lambda: result)
    return result
","if not ""Server Exception"" in str ( e ) :",190
"def analyze(vw):
    for va, dest in vw.findPointers():
        # Is there a location already at the target?
        loc = vw.getLocation(dest)
        if loc is None:
            continue
        if loc[L_LTYPE] != LOC_IMPORT:
            continue
        offset, bytes = vw.getByteDef(va)
        if offset < 2:
            continue
        if bytes[offset - 2 : offset] == b""\xff\x15"":  # call [importloc]
            # If there's a pointer here, remove it.
            if vw.getLocation(va):
                vw.delLocation(va)
            vw.makeCode(va - 2)
","if bytes [ offset - 2 : offset ] == b""\xff\x15"" :",192
"def get(_kwargs):
    exception_raised_every_time = True
    exception = None
    no_match = True
    for meter in self.meters:
        try:
            match = getattr(meter, func)(_kwargs)
        except KeyError as e:
            exception = e
        else:
            exception_raised_every_time = False
            if match:
                selected_meters.append(meter)
                no_match = False
    if no_match:
        raise KeyError(""'No match for {}'"".format(_kwargs))
    if exception_raised_every_time and exception is not None:
        raise exception
",if match :,164
"def derive(self, key_material):
    if self._used:
        raise AlreadyFinalized
    self._used = True
    if not isinstance(key_material, bytes):
        raise TypeError(""key_material must be bytes."")
    output = [b""""]
    outlen = 0
    counter = 1
    while self._length > outlen:
        h = hashes.Hash(self._algorithm, self._backend)
        h.update(key_material)
        h.update(_int_to_u32be(counter))
        if self._sharedinfo is not None:
            h.update(self._sharedinfo)
        output.append(h.finalize())
        outlen += len(output[-1])
        counter += 1
    return b"""".join(output)[: self._length]
",if self . _sharedinfo is not None :,196
"def test_cat(shape, cat_dim, split, dim):
    assert sum(split) == shape[cat_dim]
    gaussian = random_gaussian(shape, dim)
    parts = []
    end = 0
    for size in split:
        beg, end = end, end + size
        if cat_dim == -1:
            part = gaussian[..., beg:end]
        elif cat_dim == -2:
            part = gaussian[..., beg:end, :]
        elif cat_dim == 1:
            part = gaussian[:, beg:end]
        else:
            raise ValueError
        parts.append(part)
    actual = Gaussian.cat(parts, cat_dim)
    assert_close_gaussian(actual, gaussian)
",if cat_dim == - 1 :,186
"def ghci_package_db(self, cabal):
    if cabal is not None and cabal != ""cabal"":
        package_conf = [
            pkg for pkg in os.listdir(cabal) if re.match(r""packages-(.*)\.conf"", pkg)
        ]
        if package_conf:
            return os.path.join(cabal, package_conf)
    return None
",if package_conf :,106
"def L_op(self, inputs, outputs, gout):
    (x,) = inputs
    (gz,) = gout
    if x.type in complex_types:
        raise NotImplementedError()
    if outputs[0].type in discrete_types:
        if x.type in discrete_types:
            return [x.zeros_like(dtype=theano.config.floatX)]
        else:
            return [x.zeros_like()]
    return (gz / x,)
",if x . type in discrete_types :,115
"def __mro_entries__(self, bases):
    if self._name:  # generic version of an ABC or built-in class
        return super().__mro_entries__(bases)
    if self.__origin__ is Generic:
        if Protocol in bases:
            return ()
        i = bases.index(self)
        for b in bases[i + 1 :]:
            if isinstance(b, _BaseGenericAlias) and b is not self:
                return ()
    return (self.__origin__,)
",if Protocol in bases :,124
"def getvars(request, excludes):
    getvars = request.GET.copy()
    excludes = excludes.split("","")
    for p in excludes:
        if p in getvars:
            del getvars[p]
        if len(getvars.keys()) > 0:
            return ""&%s"" % getvars.urlencode()
        else:
            return """"
",if len ( getvars . keys ( ) ) > 0 :,94
"def check(self):
    now = time.time()
    for fn in os.listdir(self.basedir):
        if fn in self.files:
            continue
        absfn = os.path.join(self.basedir, fn)
        mtime = os.stat(absfn)[stat.ST_MTIME]
        if now - mtime > self.old:
            os.remove(absfn)
",if fn in self . files :,101
"def run(self):
    while 1:
        gatekeeper.wait()
        results = []
        results.append(self.__queue.get())
        while len(results) < self.MAX_SONGS_PER_SUBMISSION:
            # wait a bit to reduce overall request count.
            timeout = 0.5 / len(results)
            try:
                results.append(self.__queue.get(timeout=timeout))
            except queue.Empty:
                break
        if self.__stopped:
            return
        for lookup_result in self.__process(results):
            self.__idle(self.__progress_cb, lookup_result)
            self.__queue.task_done()
",if self . __stopped :,185
"def __getitem__(self, item):
    if isinstance(item, int):
        selected_polygons = [self.polygons[item]]
    elif isinstance(item, slice):
        selected_polygons = self.polygons[item]
    else:
        # advanced indexing on a single dimension
        selected_polygons = []
        if isinstance(item, torch.Tensor) and item.dtype == torch.uint8:
            item = item.nonzero()
            item = item.squeeze(1) if item.numel() > 0 else item
            item = item.tolist()
        for i in item:
            selected_polygons.append(self.polygons[i])
    return PolygonList(selected_polygons, size=self.size)
","if isinstance ( item , torch . Tensor ) and item . dtype == torch . uint8 :",179
"def gather_files(fileset):
    common_type = get_common_filetype(fileset)
    files = []
    for file in fileset.file:
        filename = file.name
        if file.is_include_file == True:
            filename = {}
            filename[file.name] = {""is_include_file"": True}
        if file.file_type != common_type:
            if type(filename) == str:
                filename = {}
            filename[file.name] = {""file_type"": file.file_type}
        files.append(filename)
    return files
",if type ( filename ) == str :,158
"def _(node):
    for __ in dir(node):
        if not __.startswith(""_""):
            candidate = getattr(node, __)
            if isinstance(candidate, str):
                if ""\\"" in candidate:
                    try:
                        re.compile(candidate)
                    except:
                        errMsg = ""smoke test failed at compiling '%s'"" % candidate
                        logger.error(errMsg)
                        raise
            else:
                _(candidate)
","if isinstance ( candidate , str ) :",142
"def _handle_children(self, removed, added):
    # Stop all the removed children.
    for obj in removed:
        obj.stop()
    # Process the new objects.
    for obj in added:
        obj.set(scene=self.scene, parent=self)
        if isinstance(obj, ModuleManager):
            obj.source = self
        elif is_filter(obj):
            obj.inputs.append(self)
        if self.running:
            try:
                obj.start()
            except:
                exception()
",if self . running :,148
"def mean(self):
    """"""Compute the mean of the value_field in the window.""""""
    if len(self.data) > 0:
        datasum = 0
        datalen = 0
        for dat in self.data:
            if ""placeholder"" not in dat[0]:
                datasum += dat[1]
                datalen += 1
        if datalen > 0:
            return datasum / float(datalen)
        return None
    else:
        return None
",if datalen > 0 :,132
"def get_master_info(accounts_config, master):
    master_info = None
    for a in accounts_config[""accounts""]:
        if a[""name""] == master:
            master_info = a
            break
        if a[""account_id""] == master:
            master_info = a
            break
    if master_info is None:
        raise ValueError(""Master account: %s not found in accounts config"" % (master))
    return master_info
","if a [ ""account_id"" ] == master :",120
"def dataset_collector(dataset_collection_description):
    if dataset_collection_description is DEFAULT_DATASET_COLLECTOR_DESCRIPTION:
        # Use 'is' and 'in' operators, so lets ensure this is
        # treated like a singleton.
        return DEFAULT_DATASET_COLLECTOR
    else:
        if dataset_collection_description.discover_via == ""pattern"":
            return DatasetCollector(dataset_collection_description)
        else:
            return ToolMetadataDatasetCollector(dataset_collection_description)
","if dataset_collection_description . discover_via == ""pattern"" :",126
"def _eliminate_deprecated_list_indexing(idx):
    # ""Basic slicing is initiated if the selection object is a non-array,
    # non-tuple sequence containing slice objects, [Ellipses, or newaxis
    # objects]"". Detects this case and canonicalizes to a tuple. This case is
    # deprecated by NumPy and exists for backward compatibility.
    if not isinstance(idx, tuple):
        if isinstance(idx, Sequence) and not isinstance(idx, ndarray):
            if _any(_should_unpack_list_index(i) for i in idx):
                idx = tuple(idx)
            else:
                idx = (idx,)
        else:
            idx = (idx,)
    return idx
",if _any ( _should_unpack_list_index ( i ) for i in idx ) :,177
"def finalizer():
    try:
        stdout.flush()
        stderr.flush()
    finally:
        time.sleep(0.001)  # HACK: Sleep 1ms in the main thread to free the GIL.
        stdout_pipe.stop_writing()
        stderr_pipe.stop_writing()
        writer.join(timeout=60)
        if writer.isAlive():
            raise NailgunStreamWriterError(
                ""pantsd timed out while waiting for the stdout/err to finish writing to the socket.""
            )
",if writer . isAlive ( ) :,138
"def __init__(self, env, config, scope_infos, option_tracker):
    # Sorting ensures that ancestors precede descendants.
    scope_infos = sorted(set(list(scope_infos)), key=lambda si: si.scope)
    self._parser_by_scope = {}
    for scope_info in scope_infos:
        scope = scope_info.scope
        parent_parser = (
            None
            if scope == GLOBAL_SCOPE
            else self._parser_by_scope[enclosing_scope(scope)]
        )
        self._parser_by_scope[scope] = Parser(
            env, config, scope_info, parent_parser, option_tracker=option_tracker
        )
",if scope == GLOBAL_SCOPE,176
"def _load_start_paths(self) -> None:
    ""Start the Read-Eval-Print Loop.""
    if self._startup_paths:
        for path in self._startup_paths:
            if os.path.exists(path):
                with open(path, ""rb"") as f:
                    code = compile(f.read(), path, ""exec"")
                    exec(code, self.get_globals(), self.get_locals())
            else:
                output = self.app.output
                output.write(""WARNING | File not found: {}\n\n"".format(path))
",if os . path . exists ( path ) :,159
"def validate(leaves):
    for leaf in leaves:
        if leaf.has_form((""Rule"", ""RuleDelayed""), 2):
            pass
        elif leaf.has_form(""List"", None) or leaf.has_form(""Association"", None):
            if validate(leaf.leaves) is not True:
                return False
        else:
            return False
    return True
","elif leaf . has_form ( ""List"" , None ) or leaf . has_form ( ""Association"" , None ) :",97
"def add(self, name, value, package=None):
    # New data, not previous value
    if name not in self._data[package]:
        self._data[package][name] = value
    # There is data already
    else:
        # Only append at the end if we had a list
        if isinstance(self._data[package][name], list):
            if isinstance(value, list):
                self._data[package][name].extend(value)
            else:
                self._data[package][name].append(value)
","if isinstance ( self . _data [ package ] [ name ] , list ) :",140
"def edge2str(self, nfrom, nto):
    if isinstance(nfrom, ExprCompose):
        for i in nfrom.args:
            if i[0] == nto:
                return ""[%s, %s]"" % (i[1], i[2])
    elif isinstance(nfrom, ExprCond):
        if nfrom.cond == nto:
            return ""?""
        elif nfrom.src1 == nto:
            return ""True""
        elif nfrom.src2 == nto:
            return ""False""
    return """"
",elif nfrom . src2 == nto :,149
"def _get_config(key):
    config = db.session.execute(
        Configs.__table__.select().where(Configs.key == key)
    ).fetchone()
    if config and config.value:
        value = config.value
        if value and value.isdigit():
            return int(value)
        elif value and isinstance(value, string_types):
            if value.lower() == ""true"":
                return True
            elif value.lower() == ""false"":
                return False
            else:
                return value
    # Flask-Caching is unable to roundtrip a value of None.
    # Return an exception so that we can still cache and avoid the db hit
    return KeyError
","elif value and isinstance ( value , string_types ) :",181
"def from_rows(cls, rows):
    subtitles = []
    for row in rows:
        if row.td.a is not None and row.td.get(""class"", [""lazy""])[0] != ""empty"":
            subtitles.append(cls.from_row(row))
    return subtitles
","if row . td . a is not None and row . td . get ( ""class"" , [ ""lazy"" ] ) [ 0 ] != ""empty"" :",75
"def _wx_node(self, parent_node, index, label, with_checkbox):
    ct_type = 1 if with_checkbox else 0
    if index is not None:
        # blame wxPython for this ugliness
        if isinstance(index, int):
            return self.InsertItemByIndex(parent_node, index, label, ct_type=ct_type)
        else:
            return self.InsertItem(parent_node, index, label, ct_type=ct_type)
    return self.AppendItem(parent_node, label, ct_type=ct_type)
","if isinstance ( index , int ) :",147
"def fetch():
    retval = {}
    content = retrieve_content(__url__)
    if __check__ in content:
        for line in content.split(""\n""):
            line = line.strip()
            if not line or line.startswith(""#"") or ""."" not in line:
                continue
            if "" # "" in line:
                reason = line.split("" # "")[1].split()[0].lower()
                if reason == ""scanning"":  # too many false positives
                    continue
                retval[line.split("" # "")[0]] = (__info__, __reference__)
    return retval
","if "" # "" in line :",157
"def _remove_event(self, event):
    # Find event according to its timestamp.
    # Index returned should be one behind.
    i = bisect.bisect(self._eventq, event)
    # Having two events with identical timestamp is unlikely but possible.
    # I am going to move forward and compare timestamp AND object address
    # to make sure the correct object is found.
    while i > 0:
        i -= 1
        e = self._eventq[i]
        if e.timestamp != event.timestamp:
            raise exception.EventNotFound(event)
        elif id(e) == id(event):
            self._eventq.pop(i)
            return
    raise exception.EventNotFound(event)
",elif id ( e ) == id ( event ) :,177
"def _safe_get_content(self, session, resolve_from):
    try:
        resp = session.get(resolve_from, timeout=self._timeout)
        if resp.status_code == requests.codes.ok:
            return resp.content
        raise self.ResolverError(""Error status_code={0}"".format(resp.status_code))
    except requests.RequestException:
        raise self.ResolverError(""Request error from {0}"".format(resolve_from))
",if resp . status_code == requests . codes . ok :,116
"def splitlines(self, sep=None, replace=None):
    ""Return split lines from any file descriptor""
    for fd in self.fd:
        fd.seek(0)
        for line in fd.readlines():
            if replace and sep:
                yield line.replace(replace, sep).split(sep)
            elif replace:
                yield line.replace(replace, "" "").split()
            else:
                yield line.split(sep)
",elif replace :,122
"def disable_verity():
    """"""Disables dm-verity on the device.""""""
    with log.waitfor(""Disabling dm-verity on %s"" % context.device):
        root()
        with AdbClient() as c:
            reply = c.disable_verity()
        if ""Verity already disabled"" in reply:
            return
        elif ""Now reboot your device"" in reply:
            reboot(wait=True)
        elif ""0006closed"" in reply:
            return  # Emulator doesnt support Verity?
        else:
            log.error(""Could not disable verity:\n%s"" % reply)
","elif ""Now reboot your device"" in reply :",165
"def _process_property_change(self, msg):
    msg = super(Select, self)._process_property_change(msg)
    if ""value"" in msg:
        if not self.values:
            pass
        elif msg[""value""] is None:
            msg[""value""] = self.values[0]
        else:
            if isIn(msg[""value""], self.unicode_values):
                idx = indexOf(msg[""value""], self.unicode_values)
            else:
                idx = indexOf(msg[""value""], self.labels)
            msg[""value""] = self._items[self.labels[idx]]
    msg.pop(""options"", None)
    return msg
","if isIn ( msg [ ""value"" ] , self . unicode_values ) :",180
"def merge(module_name, tree1, tree2):
    for child in tree2.node:
        if isinstance(child, ast.Function):
            replaceFunction(tree1, child.name, child)
        elif isinstance(child, ast.Assign):
            replaceAssign(tree1, child.nodes[0].name, child)
        elif isinstance(child, ast.Class):
            replaceClassMethods(tree1, child.name, child)
        else:
            raise TranslationError(
                ""Do not know how to merge %s"" % child, child, module_name
            )
    return tree1
","elif isinstance ( child , ast . Class ) :",159
"def handle(d: dict):
    for key, value in d.items():
        if type(value) == dict:
            if ""url"" not in value:
                handle(value)
            else:
                global count
                count += 1
",if type ( value ) == dict :,72
"def __stop_loggers(self):
    if self._console_proc:
        utils.nuke_subprocess(self._console_proc)
        utils.nuke_subprocess(self._followfiles_proc)
        self._console_proc = self._followfile_proc = None
        if self.job:
            self.job.warning_loggers.discard(self._logfile_warning_stream)
        self._logfile_warning_stream.close()
",if self . job :,113
"def unicode_metrics(metrics):
    for i, metric in enumerate(metrics):
        for key, value in metric.items():
            if isinstance(value, basestring):
                metric[key] = unicode(value, errors=""replace"")
            elif isinstance(value, tuple) or isinstance(value, list):
                value_list = list(value)
                for j, value_element in enumerate(value_list):
                    if isinstance(value_element, basestring):
                        value_list[j] = unicode(value_element, errors=""replace"")
                metric[key] = tuple(value_list)
        metrics[i] = metric
    return metrics
","elif isinstance ( value , tuple ) or isinstance ( value , list ) :",177
"def __getitem__(self, idx):
    if isinstance(idx, slice):
        start, stop, step = idx.indices(len(self))
        return [self._revoked_cert(i) for i in range(start, stop, step)]
    else:
        idx = operator.index(idx)
        if idx < 0:
            idx += len(self)
        if not 0 <= idx < len(self):
            raise IndexError
        return self._revoked_cert(idx)
",if idx < 0 :,125
"def _get_columns_and_column_names(row):
    column_names = []
    columns = []
    duplicate_counter = 1
    for i, column_name in enumerate(row):
        if not column_name:
            column_name = ""column_{}"".format(xl_col_to_name(i))
        if column_name in column_names:
            column_name = ""{}{}"".format(column_name, duplicate_counter)
            duplicate_counter += 1
        column_names.append(column_name)
        columns.append(
            {""name"": column_name, ""friendly_name"": column_name, ""type"": TYPE_STRING}
        )
    return columns, column_names
",if not column_name :,179
"def format(self, format, dumper, attrib, data):
    if data:
        logger.warn(""Unexpected data in %s object: %r"", attrib[""type""], data)
    try:
        return ImageGeneratorObjectType.format(self, format, dumper, attrib, data)
    except ValueError:
        if attrib[""type""].startswith(""image+""):
            attrib = attrib.copy()
            attrib[""type""] = attrib[""type""][6:]
        return dumper.dump_img(IMAGE, attrib, None)
","if attrib [ ""type"" ] . startswith ( ""image+"" ) :",124
"def handle_facts_wwn(facts):
    disk_shares = []
    for key, wwn in facts.iteritems():
        if not key.startswith(""wwn_mpath""):
            continue
        path = key.replace(""wwn_"", """")
        disk_shares.append(
            {
                ""serial_number"": normalize_wwn(wwn),
                ""volume"": ""/dev/mapper/%s"" % path,
            }
        )
    return disk_shares
","if not key . startswith ( ""wwn_mpath"" ) :",127
"def _finalize_load(*exc_info):
    try:
        success_keys = [k for k in data_keys if k not in failed_keys]
        if success_keys:
            self._holder_ref.put_objects_by_keys(
                session_id, success_keys, pin_token=pin_token
            )
        if exc_info:
            raise exc_info[1].with_traceback(exc_info[2]) from None
        if failed_keys:
            raise StorageFull(
                request_size=storage_full_sizes[0],
                capacity=storage_full_sizes[1],
                affected_keys=list(failed_keys),
            )
    finally:
        shared_bufs[:] = []
",if success_keys :,200
"def _get_base64md5(self):
    if ""md5"" in self.local_hashes and self.local_hashes[""md5""]:
        md5 = self.local_hashes[""md5""]
        if not isinstance(md5, bytes):
            md5 = md5.encode(""utf-8"")
        return binascii.b2a_base64(md5).decode(""utf-8"").rstrip(""\n"")
","if not isinstance ( md5 , bytes ) :",104
"def tag_configure(self, *args, **keys):
    trace = False and not g.unitTesting
    if trace:
        g.trace(args, keys)
    if len(args) == 1:
        key = args[0]
        self.tags[key] = keys
        val = keys.get(""foreground"")
        underline = keys.get(""underline"")
        if val:
            self.configDict[key] = val
        if underline:
            self.configUnderlineDict[key] = True
    else:
        g.trace(""oops"", args, keys)
",if val :,150
"def _findSubpath(self, path, A, B, inside):
    print(""finding"", A, B)
    sub = None
    for i in xrange(0, len(path) * 2):  # iterate twice with wrap around
        j = i % len(path)
        seg = path[j]
        if inside.isInside(seg.midPoint()):
            if eq(seg.A, A):
                sub = Path(""subp"")
            print(""seg"", sub is None, seg)
            if sub is not None:
                sub.append(seg)
            if eq(seg.B, B):
                break
    print(""found"", sub)
    return sub
","if eq ( seg . A , A ) :",180
"def indent_block(self, cursor):
    """"""Indent block after enter pressed""""""
    at_start_of_line = cursor.positionInBlock() == 0
    with self._neditor:
        cursor.insertBlock()
        if not at_start_of_line:
            indent = self._compute_indent(cursor)
            if indent is not None:
                cursor.insertText(indent)
                return True
            return False
    self._neditor.ensureCursorVisible()
",if not at_start_of_line :,127
"def checkpoint():
    if checkpoint_asserts:
        self.assert_integrity_idxs_take()
        if node in self.idxs_memo:
            toposort(self.idxs_memo[node])
        if node in self.take_memo:
            for take in self.take_memo[node]:
                toposort(take)
",if node in self . take_memo :,86
"def handle(self, *args, **options):
    with advisory_lock(""send-notifications-command"", wait=False) as acquired:
        if acquired:
            qs = HistoryChangeNotification.objects.all().order_by(""-id"")
            for change_notification in iter_queryset(qs, itersize=100):
                try:
                    send_sync_notifications(change_notification.pk)
                except HistoryChangeNotification.DoesNotExist:
                    pass
        else:
            print(""Other process already running"")
",if acquired :,142
"def _parse_version_parts(s):
    for part in component_re.split(s):
        part = replace(part, part)
        if part in ["""", "".""]:
            continue
        if part[:1] in ""0123456789"":
            yield part.zfill(8)  # pad for numeric comparison
        else:
            yield ""*"" + part
    yield ""*final""  # ensure that alpha/beta/candidate are before final
","if part [ : 1 ] in ""0123456789"" :",109
"def set_password(user_id):
    try:
        user = Journalist.query.get(user_id)
    except NoResultFound:
        abort(404)
    password = request.form.get(""password"")
    if set_diceware_password(user, password) is not False:
        if user.last_token is not None:
            revoke_token(user, user.last_token)
        user.session_nonce += 1
        db.session.commit()
    return redirect(url_for(""admin.edit_user"", user_id=user_id))
",if user . last_token is not None :,147
"def _get_normal_median_depth(normal_counts):
    depths = []
    with open(normal_counts) as in_handle:
        header = None
        for line in in_handle:
            if header is None and not line.startswith(""@""):
                header = line.strip().split()
            elif header:
                n_vals = dict(zip(header, line.strip().split()))
                depths.append(int(n_vals[""REF_COUNT""]) + int(n_vals[""ALT_COUNT""]))
    return np.median(depths)
","if header is None and not line . startswith ( ""@"" ) :",145
"def _gen_langs_in_db(self):
    for d in os.listdir(join(self.base_dir, ""db"")):
        if d in self._non_lang_db_dirs:
            continue
        lang_path = join(self.base_dir, ""db"", d, ""lang"")
        if not exists(lang_path):
            log.warn(
                ""unexpected lang-zone db dir without 'lang' file: ""
                ""`%s' (skipping)"" % dirname(lang_path)
            )
            continue
        fin = open(lang_path, ""r"")
        try:
            lang = fin.read().strip()
        finally:
            fin.close()
        yield lang
",if not exists ( lang_path ) :,194
"def negate(monad):
    sql = monad.getsql()[0]
    translator = monad.translator
    if translator.dialect == ""Oracle"":
        result_sql = [""IS_NULL"", sql]
    else:
        result_sql = [""EQ"", sql, [""VALUE"", """"]]
        if monad.nullable:
            if isinstance(monad, AttrMonad):
                result_sql = [""OR"", result_sql, [""IS_NULL"", sql]]
            else:
                result_sql = [""EQ"", [""COALESCE"", sql, [""VALUE"", """"]], [""VALUE"", """"]]
    result = BoolExprMonad(result_sql, nullable=False)
    result.aggregated = monad.aggregated
    return result
",if monad . nullable :,188
"def _model_shorthand(self, args):
    accum = []
    for arg in args:
        if isinstance(arg, Node):
            accum.append(arg)
        elif isinstance(arg, Query):
            accum.append(arg)
        elif isinstance(arg, ModelAlias):
            accum.extend(arg.get_proxy_fields())
        elif isclass(arg) and issubclass(arg, Model):
            accum.extend(arg._meta.declared_fields)
    return accum
","elif isclass ( arg ) and issubclass ( arg , Model ) :",125
"def get_hashes_from_fingerprint_with_reason(event, fingerprint):
    default_values = set([""{{ default }}"", ""{{default}}""])
    if any(d in fingerprint for d in default_values):
        default_hashes = get_hashes_for_event_with_reason(event)
        hash_count = len(default_hashes[1])
    else:
        hash_count = 1
    hashes = OrderedDict((bit, []) for bit in fingerprint)
    for idx in xrange(hash_count):
        for bit in fingerprint:
            if bit in default_values:
                hashes[bit].append(default_hashes)
            else:
                hashes[bit] = bit
    return hashes.items()
",if bit in default_values :,180
"def default(self, obj):
    if hasattr(obj, ""__json__""):
        return obj.__json__()
    elif isinstance(obj, collections.Iterable):
        return list(obj)
    elif isinstance(obj, dt.datetime):
        return obj.isoformat()
    elif hasattr(obj, ""__getitem__"") and hasattr(obj, ""keys""):
        return dict(obj)
    elif hasattr(obj, ""__dict__""):
        return {
            member: getattr(obj, member)
            for member in dir(obj)
            if not member.startswith(""_"")
            and not hasattr(getattr(obj, member), ""__call__"")
        }
    return json.JSONEncoder.default(self, obj)
","if not member . startswith ( ""_"" )",172
"def get_http_auth(self, name):
    auth = self._config.get(""http-basic.{}"".format(name))
    if not auth:
        username = self._config.get(""http-basic.{}.username"".format(name))
        password = self._config.get(""http-basic.{}.password"".format(name))
        if not username and not password:
            return None
    else:
        username, password = auth[""username""], auth.get(""password"")
        if password is None:
            password = self.keyring.get_password(name, username)
    return {
        ""username"": username,
        ""password"": password,
    }
",if password is None :,166
"def add_libdirs(self, envvar, sep, fatal=False):
    v = os.environ.get(envvar)
    if not v:
        return
    for dir in str.split(v, sep):
        dir = str.strip(dir)
        if not dir:
            continue
        dir = os.path.normpath(dir)
        if os.path.isdir(dir):
            if not dir in self.library_dirs:
                self.library_dirs.append(dir)
        elif fatal:
            fail(""FATAL: bad directory %s in environment variable %s"" % (dir, envvar))
",if not dir :,159
"def PARSE_TWO_PARAMS(x, y):
    """"""used to convert different possible x/y params to a tuple""""""
    if y is not None:
        return (x, y)
    else:
        if isinstance(x, (list, tuple)):
            return (x[0], x[1])
        else:
            if isinstance(x, UNIVERSAL_STRING):
                x = x.strip()
                if "","" in x:
                    return [int(w.strip()) for w in x.split("","")]
            return (x, x)
","if "","" in x :",147
"def _load_from_sym_dir(self, root):
    root = os.path.abspath(root)
    prefix_len = len(root) + 1
    for base, _, filenames in os.walk(root):
        for filename in filenames:
            if not filename.endswith("".sym""):
                continue
            path = os.path.join(base, filename)
            lib_path = ""/"" + path[prefix_len:-4]
            self.add(lib_path, ELF.load_dump(path))
","if not filename . endswith ( "".sym"" ) :",134
"def is_vertical(self):
    if not self.isFloating():
        par = self.parent()
        if par and hasattr(par, ""dockWidgetArea""):
            return par.dockWidgetArea(self) in (
                Qt.LeftDockWidgetArea,
                Qt.RightDockWidgetArea,
            )
    return self.size().height() > self.size().width()
","if par and hasattr ( par , ""dockWidgetArea"" ) :",106
"def writeBit(self, state, endian):
    if self._bit_pos == 7:
        self._bit_pos = 0
        if state:
            if endian is BIG_ENDIAN:
                self._byte |= 1
            else:
                self._byte |= 128
        self._output.write(chr(self._byte))
        self._byte = 0
    else:
        if state:
            if endian is BIG_ENDIAN:
                self._byte |= 1 << self._bit_pos
            else:
                self._byte |= 1 << (7 - self._bit_pos)
        self._bit_pos += 1
",if endian is BIG_ENDIAN :,177
"def init(self):
    self.sock.setblocking(True)
    if self.parser is None:
        # wrap the socket if needed
        if self.cfg.is_ssl:
            self.sock = ssl.wrap_socket(
                self.sock, server_side=True, **self.cfg.ssl_options
            )
        # initialize the parser
        self.parser = http.RequestParser(self.cfg, self.sock, self.client)
",if self . cfg . is_ssl :,120
"def construct_scalar(self, node):
    if isinstance(node, MappingNode):
        for key_node, value_node in node.value:
            if key_node.tag == ""tag:yaml.org,2002:value"":
                return self.construct_scalar(value_node)
    return super().construct_scalar(node)
","if key_node . tag == ""tag:yaml.org,2002:value"" :",85
"def typeNewLine(self, line):
    if line >= 0:
        iter = self.buffer.get_iter_at_line(line)
        if not iter.ends_line():
            iter.forward_to_line_end()
        self.buffer.place_cursor(iter)
    elif line < 0:
        iter = self.buffer.get_end_iter()
        for i in range(line, -1):
            iter.backward_line()
        iter.forward_to_line_end()
        self.buffer.place_cursor(iter)
    press(self.view, ""\n"")
",if not iter . ends_line ( ) :,156
"def _render_ib_interfaces(cls, network_state, iface_contents, flavor):
    ib_filter = renderer.filter_by_type(""infiniband"")
    for iface in network_state.iter_interfaces(ib_filter):
        iface_name = iface[""name""]
        iface_cfg = iface_contents[iface_name]
        iface_cfg.kind = ""infiniband""
        iface_subnets = iface.get(""subnets"", [])
        route_cfg = iface_cfg.routes
        cls._render_subnets(
            iface_cfg, iface_subnets, network_state.has_default_route, flavor
        )
        cls._render_subnet_routes(iface_cfg, route_cfg, iface_subnets, flavor)
","iface_subnets = iface . get ( ""subnets"" , [ ] )",193
"def stop(self):
    """"""Stops the slapd server, and waits for it to terminate""""""
    if self._proc is not None:
        self._log.debug(""stopping slapd"")
        if hasattr(self._proc, ""terminate""):
            self._proc.terminate()
        else:
            import posix, signal
            posix.kill(self._proc.pid, signal.SIGHUP)
            # time.sleep(1)
            # posix.kill(self._proc.pid, signal.SIGTERM)
            # posix.kill(self._proc.pid, signal.SIGKILL)
        self.wait()
","if hasattr ( self . _proc , ""terminate"" ) :",160
"def _listen(self, consumer_id: str) -> AsyncIterable[Any]:
    try:
        while True:
            if self._listening:
                async for msg in self._listen_to_queue(consumer_id):
                    if msg is not None:
                        yield msg
                await asyncio.sleep(0.5)
            else:
                async for msg in self._listen_to_ws():
                    yield msg
    except asyncio.CancelledError:
        pass
    except Exception as e:
        raise e
",if self . _listening :,153
"def discover_misago_admin():
    for app in apps.get_app_configs():
        module = import_module(app.name)
        if not hasattr(module, ""admin""):
            continue
        admin_module = import_module(""%s.admin"" % app.name)
        if hasattr(admin_module, ""MisagoAdminExtension""):
            extension = getattr(admin_module, ""MisagoAdminExtension"")()
            if hasattr(extension, ""register_navigation_nodes""):
                extension.register_navigation_nodes(site)
            if hasattr(extension, ""register_urlpatterns""):
                extension.register_urlpatterns(urlpatterns)
","if hasattr ( extension , ""register_navigation_nodes"" ) :",169
"def update_job(self, job):
    if not self.redis.hexists(self.jobs_key, job.id):
        raise JobLookupError(job.id)
    with self.redis.pipeline() as pipe:
        pipe.hset(
            self.jobs_key,
            job.id,
            pickle.dumps(job.__getstate__(), self.pickle_protocol),
        )
        if job.next_run_time:
            pipe.zadd(
                self.run_times_key,
                {job.id: datetime_to_utc_timestamp(job.next_run_time)},
            )
        else:
            pipe.zrem(self.run_times_key, job.id)
        pipe.execute()
",if job . next_run_time :,200
"def _get_first_available_entry_node(self) -> Optional[str]:
    for entry_node in self.entry_nodes:
        if entry_node not in self.locked_entry_nodes:
            _, wait_until = self._parse_entry_node(entry_node)
            now = time.time()
            if wait_until <= now:
                return entry_node
    return None
",if wait_until <= now :,105
"def answers(self, other):
    if not isinstance(other, TCP):
        return 0
    if conf.checkIPsrc:
        if not ((self.sport == other.sport) and (self.dport == other.dport)):
            return 0
    if conf.check_TCPerror_seqack:
        if self.seq is not None:
            if self.seq != other.seq:
                return 0
        if self.ack is not None:
            if self.ack != other.ack:
                return 0
    return 1
",if self . ack != other . ack :,143
"def run(self):
    if self.check():
        path = ""/BWT/utils/logs/read_log.jsp?filter=&log=../../../../../../../../..{}"".format(
            self.filename
        )
        response = self.http_request(method=""GET"", path=path)
        if response and response.status_code == 200 and len(response.text):
            print_success(""Exploit success"")
            print_status(""Reading file: {}"".format(self.filename))
            print_info(response.text)
        else:
            print_error(""Exploit failed - could not read file"")
    else:
        print_error(""Exploit failed - device seems to be not vulnerable"")
",if response and response . status_code == 200 and len ( response . text ) :,186
"def write(self, s):
    if self.closed:
        raise ValueError(""write to closed file"")
    if type(s) not in (unicode, str, bytearray):
        # See issue #19481
        if isinstance(s, unicode):
            s = unicode.__getitem__(s, slice(None))
        elif isinstance(s, str):
            s = str.__str__(s)
        elif isinstance(s, bytearray):
            s = bytearray.__str__(s)
        else:
            raise TypeError(""must be string, not "" + type(s).__name__)
    return self.shell.write(s, self.tags)
","elif isinstance ( s , str ) :",161
"def test_checkblock_valid(self):
    for comment, fHeader, fCheckPoW, cur_time, blk in load_test_vectors(
        ""checkblock_valid.json""
    ):
        try:
            if fHeader:
                CheckBlockHeader(blk, fCheckPoW=fCheckPoW, cur_time=cur_time)
            else:
                CheckBlock(blk, fCheckPoW=fCheckPoW, cur_time=cur_time)
        except ValidationError as err:
            self.fail('Failed ""%s"" with error %r' % (comment, err))
",if fHeader :,160
"def _lookup_fqdn(ip):
    try:
        return [socket.getfqdn(socket.gethostbyaddr(ip)[0])]
    except socket.herror as err:
        if err.errno in (0, HOST_NOT_FOUND, NO_DATA):
            # No FQDN for this IP address, so we don't need to know this all the time.
            log.debug(""Unable to resolve address %s: %s"", ip, err)
        else:
            log.error(err_message, err)
    except (socket.error, socket.gaierror, socket.timeout) as err:
        log.error(err_message, err)
","if err . errno in ( 0 , HOST_NOT_FOUND , NO_DATA ) :",169
"def send_telnet(self, *args: str):
    try:
        shell = TelnetShell(self.host)
        for command in args:
            if command == ""ftp"":
                shell.check_or_download_busybox()
                shell.run_ftp()
            else:
                shell.exec(command)
        shell.close()
    except Exception as e:
        _LOGGER.exception(f""Telnet command error: {e}"")
","if command == ""ftp"" :",127
"def write(path, data, kind=""OTHER"", dohex=0):
    asserttype1(data)
    kind = string.upper(kind)
    try:
        os.remove(path)
    except os.error:
        pass
    err = 1
    try:
        if kind == ""LWFN"":
            writelwfn(path, data)
        elif kind == ""PFB"":
            writepfb(path, data)
        else:
            writeother(path, data, dohex)
        err = 0
    finally:
        if err and not DEBUG:
            try:
                os.remove(path)
            except os.error:
                pass
","if kind == ""LWFN"" :",182
"def ApplyInScriptedSection(self, codeBlock, fn, args):
    self.BeginScriptedSection()
    try:
        try:
            # 				print ""ApplyInSS"", codeBlock, fn, args
            return self._ApplyInScriptedSection(fn, args)
        finally:
            if self.debugManager:
                self.debugManager.OnLeaveScript()
            self.EndScriptedSection()
    except:
        self.HandleException(codeBlock)
",if self . debugManager :,129
"def _escape_attrib(text):
    # escape attribute value
    try:
        if ""&"" in text:
            text = text.replace(""&"", ""&amp;"")
        if ""<"" in text:
            text = text.replace(""<"", ""&lt;"")
        if "">"" in text:
            text = text.replace("">"", ""&gt;"")
        if '""' in text:
            text = text.replace('""', ""&quot;"")
        if ""\n"" in text:
            text = text.replace(""\n"", ""&#10;"")
        return text
    except (TypeError, AttributeError):  # pragma: no cover
        _raise_serialization_error(text)
","if ""<"" in text :",160
"def compile_relation(self, method, expr, range_list, negated=False):
    ranges = []
    for item in range_list[1]:
        if item[0] == item[1]:
            ranges.append(self.compile(item[0]))
        else:
            ranges.append(""%s..%s"" % tuple(map(self.compile, item)))
    return ""%s%s %s %s"" % (
        self.compile(expr),
        negated and "" not"" or """",
        method,
        "","".join(ranges),
    )
",if item [ 0 ] == item [ 1 ] :,144
"def emptyTree(self):
    for child in self:
        childObj = child.getObject()
        del childObj[NameObject(""/Parent"")]
        if NameObject(""/Next"") in childObj:
            del childObj[NameObject(""/Next"")]
        if NameObject(""/Prev"") in childObj:
            del childObj[NameObject(""/Prev"")]
    if NameObject(""/Count"") in self:
        del self[NameObject(""/Count"")]
    if NameObject(""/First"") in self:
        del self[NameObject(""/First"")]
    if NameObject(""/Last"") in self:
        del self[NameObject(""/Last"")]
","if NameObject ( ""/Prev"" ) in childObj :",155
"def connect_to_uri(self, uri, autoconnect=None, do_start=True):
    try:
        conn = self._check_conn(uri)
        if not conn:
            # Unknown connection, add it
            conn = self.add_conn(uri)
        if autoconnect is not None:
            conn.set_autoconnect(bool(autoconnect))
        self.show_manager()
        if do_start:
            conn.open()
        return conn
    except Exception:
        logging.exception(""Error connecting to %s"", uri)
        return None
",if do_start :,152
"def get_expression(self):
    """"""Return the expression as a printable string.""""""
    l = []
    for c in self.content:
        if c.op is not None:  # only applies to first cell
            l.append(c.op)
        if c.child is not None:
            l.append(""("" + c.child.get_expression() + "")"")
        else:
            l.append(""%d"" % c.get_value())
    return """".join(l)
",if c . child is not None :,124
"def to_word_end(view, s):
    if mode == modes.NORMAL:
        pt = word_end_reverse(view, s.b, count)
        return sublime.Region(pt)
    elif mode in (modes.VISUAL, modes.VISUAL_BLOCK):
        if s.a < s.b:
            pt = word_end_reverse(view, s.b - 1, count)
            if pt > s.a:
                return sublime.Region(s.a, pt + 1)
            return sublime.Region(s.a + 1, pt)
        pt = word_end_reverse(view, s.b, count)
        return sublime.Region(s.a, pt)
    return s
",if s . a < s . b :,191
"def whichmodule(obj, name):
    """"""Find the module an object belong to.""""""
    module_name = getattr(obj, ""__module__"", None)
    if module_name is not None:
        return module_name
    # Protect the iteration by using a list copy of sys.modules against dynamic
    # modules that trigger imports of other modules upon calls to getattr.
    for module_name, module in sys.modules.copy().items():
        if module_name == ""__main__"" or module is None:
            continue
        try:
            if _getattribute(module, name)[0] is obj:
                return module_name
        except AttributeError:
            pass
    return ""__main__""
","if module_name == ""__main__"" or module is None :",171
"def summarize_scalar_dict(name_data, step, name_scope=""Losses/""):
    if name_data:
        with tf.name_scope(name_scope):
            for name, data in name_data.items():
                if data is not None:
                    tf.compat.v2.summary.scalar(name=name, data=data, step=step)
",if data is not None :,98
"def TryMerge(self, d):
    while d.avail() > 0:
        tt = d.getVarInt32()
        if tt == 10:
            self.set_content(d.getPrefixedString())
            continue
        if tt == 18:
            self.set_blob_key(d.getPrefixedString())
            continue
        if tt == 24:
            self.set_width(d.getVarInt32())
            continue
        if tt == 32:
            self.set_height(d.getVarInt32())
            continue
        if tt == 0:
            raise ProtocolBuffer.ProtocolBufferDecodeError
        d.skipData(tt)
",if tt == 10 :,182
"def gather_files(fileset):
    common_type = get_common_filetype(fileset)
    files = []
    for file in fileset.file:
        filename = file.name
        if file.is_include_file == True:
            filename = {}
            filename[file.name] = {""is_include_file"": True}
        if file.file_type != common_type:
            if type(filename) == str:
                filename = {}
            filename[file.name] = {""file_type"": file.file_type}
        files.append(filename)
    return files
",if file . file_type != common_type :,158
"def data(self, index: QModelIndex, role=Qt.DisplayRole):
    if not index.isValid():
        return None
    if role == Qt.DisplayRole or role == Qt.EditRole:
        i = index.row()
        j = index.column()
        fieldtype = self.field_types[i]
        if j == 0:
            return fieldtype.caption
        elif j == 1:
            return fieldtype.function.name
        elif j == 2:
            return ProtocolLabel.DISPLAY_FORMATS[fieldtype.display_format_index]
",if j == 0 :,142
"def format_coord(x, y):
    # callback function to format coordinate display in toolbar
    x = int(x + 0.5)
    y = int(y + 0.5)
    try:
        if dims:
            return ""%s @ %s [%4i, %4i]"" % (cur_ax_dat[1][y, x], current, x, y)
        else:
            return ""%s @ [%4i, %4i]"" % (data[y, x], x, y)
    except IndexError:
        return """"
",if dims :,137
"def getAllUIExtensions(self):
    extensions = []
    if getExecutionCodeType() == ""MEASURE"":
        text = getMeasurementResultString(self)
        extensions.append(TextUIExtension(text))
    errorType = self.getErrorHandlingType()
    if errorType in (""MESSAGE"", ""EXCEPTION""):
        data = infoByNode[self.identifier]
        message = data.errorMessage
        if message is not None and data.showErrorMessage:
            extensions.append(ErrorUIExtension(message))
    extraExtensions = self.getUIExtensions()
    if extraExtensions is not None:
        extensions.extend(extraExtensions)
    return extensions
",if message is not None and data . showErrorMessage :,167
"def on_notify(self, notification):
    subject = notification[""subject""]
    if subject.startswith(""remote_recording.""):
        if ""should_start"" in subject and self.online:
            session_name = notification[""session_name""]
            self.sensor.set_control_value(""capture_session_name"", session_name)
            self.sensor.set_control_value(""local_capture"", True)
        elif ""should_stop"" in subject:
            self.sensor.set_control_value(""local_capture"", False)
","elif ""should_stop"" in subject :",135
"def _log_conn_errors(self):
    if ""connection"" in self.event.data:
        cinfo = self.event.data[""connection""]
        if not cinfo.get(""live""):
            err_msg = cinfo.get(""error"", [None, None])[1]
            if err_msg:
                self._log_status(err_msg)
","if not cinfo . get ( ""live"" ) :",96
"def setChanged(self, c, changed):
    # Find the tab corresponding to c.
    dw = c.frame.top  # A DynamicWindow
    i = self.indexOf(dw)
    if i < 0:
        return
    s = self.tabText(i)
    s = g.u(s)
    if len(s) > 2:
        if changed:
            if not s.startswith(""* ""):
                title = ""* "" + s
                self.setTabText(i, title)
        else:
            if s.startswith(""* ""):
                title = s[2:]
                self.setTabText(i, title)
","if not s . startswith ( ""* "" ) :",172
"def load_file_in_same_dir(ref_file, filename):
    """"""Load a given file. Works even when the file is contained inside a zip.""""""
    from couchpotato.core.helpers.encoding import toUnicode
    path = split_path(toUnicode(ref_file))[:-1] + [filename]
    for i, p in enumerate(path):
        if p.endswith("".zip""):
            zfilename = os.path.join(*path[: i + 1])
            zfile = zipfile.ZipFile(zfilename)
            return zfile.read(""/"".join(path[i + 1 :]))
    return u(io.open(os.path.join(*path), encoding=""utf-8"").read())
","if p . endswith ( "".zip"" ) :",172
"def __mpcReadyInSlaveMode(self):
    while True:
        time.sleep(10)
        if not win32gui.IsWindow(self.__listener.mpcHandle):
            if self.callbacks.onMpcClosed:
                self.callbacks.onMpcClosed(None)
            break
",if self . callbacks . onMpcClosed :,79
"def _invalidate(self, resource_group_name: str, scale_set_name: str) -> None:
    with self._lock:
        if (resource_group_name, scale_set_name) in self._instance_cache:
            del self._instance_cache[(resource_group_name, scale_set_name)]
        if resource_group_name in self._scale_set_cache:
            del self._scale_set_cache[resource_group_name]
        if resource_group_name in self._remaining_instances_cache:
            del self._remaining_instances_cache[resource_group_name]
","if ( resource_group_name , scale_set_name ) in self . _instance_cache :",154
"def close(self):
    if self._serial is not None:
        try:
            self._serial.cancel_read()
            if self._reading_thread:
                self._reading_thread.join()
        finally:
            try:
                self._serial.close()
                self._serial = None
            except Exception:
                logging.exception(""Couldn't close serial"")
",if self . _reading_thread :,110
"def channel_sizes(self):
    """"""List of channel sizes: [(width, height)].""""""
    sizes = []
    for channel in self.channel_info:
        if channel.id == ChannelID.USER_LAYER_MASK:
            sizes.append((self.mask_data.width, self.mask_data.height))
        elif channel.id == ChannelID.REAL_USER_LAYER_MASK:
            sizes.append((self.mask_data.real_width, self.mask_data.real_height))
        else:
            sizes.append((self.width, self.height))
    return sizes
",if channel . id == ChannelID . USER_LAYER_MASK :,151
"def get_module_settings():
    included_setting = []
    module = DataGetter.get_module()
    if module is not None:
        if module.ticket_include:
            included_setting.append(""ticketing"")
        if module.payment_include:
            included_setting.append(""payments"")
        if module.donation_include:
            included_setting.append(""donations"")
    return included_setting
",if module . payment_include :,109
"def _format_block(
    self, prefix: str, lines: List[str], padding: str = None
) -> List[str]:
    if lines:
        if padding is None:
            padding = "" "" * len(prefix)
        result_lines = []
        for i, line in enumerate(lines):
            if i == 0:
                result_lines.append((prefix + line).rstrip())
            elif line:
                result_lines.append(padding + line)
            else:
                result_lines.append("""")
        return result_lines
    else:
        return [prefix]
",if padding is None :,161
"def get_task_by_id(events, task_id):
    if hasattr(Task, ""_fields""):  # Old version
        return events.state.tasks.get(task_id)
    else:
        _fields = Task._defaults.keys()
        task = events.state.tasks.get(task_id)
        if task is not None:
            task._fields = _fields
        return task
",if task is not None :,103
"def check(self, value):
    try:
        if isinstance(value, decimal.Decimal):
            v = value
        else:
            v = decimal.Decimal(str(value).replace(self.dot, "".""))
        return v, None
    except (ValueError, TypeError, decimal.InvalidOperation):
        return value, translate(self.message)
","if isinstance ( value , decimal . Decimal ) :",90
"def check_sales_order_on_hold_or_close(self, ref_fieldname):
    for d in self.get(""items""):
        if d.get(ref_fieldname):
            status = frappe.db.get_value(""Sales Order"", d.get(ref_fieldname), ""status"")
            if status in (""Closed"", ""On Hold""):
                frappe.throw(
                    _(""Sales Order {0} is {1}"").format(d.get(ref_fieldname), status)
                )
",if d . get ( ref_fieldname ) :,137
"def nested_match(expect, value):
    if expect == value:
        return True
    if isinstance(expect, dict) and isinstance(value, dict):
        for k, v in expect.items():
            if k in value:
                if not nested_match(v, value[k]):
                    return False
            else:
                return False
        return True
    if isinstance(expect, list) and isinstance(value, list):
        for x, y in zip(expect, value):
            if not nested_match(x, y):
                return False
        return True
    return False
","if not nested_match ( x , y ) :",162
"def test_setup_app_sets_loader(self, app):
    prev = os.environ.get(""CELERY_LOADER"")
    try:
        cmd = MockCommand(app=app)
        cmd.setup_app_from_commandline([""--loader=X.Y:Z""])
        assert os.environ[""CELERY_LOADER""] == ""X.Y:Z""
    finally:
        if prev is not None:
            os.environ[""CELERY_LOADER""] = prev
        else:
            del os.environ[""CELERY_LOADER""]
",if prev is not None :,141
"def set_labels_for_constraints(self, constraints):
    for label in self._constraints_to_label_args(constraints):
        if label not in self.labels:
            log.info(
                ""setting node '%s' label '%s' to '%s'"",
                self.name,
                label.name,
                label.value,
            )
            self.label_add(label.name, label.value)
",if label not in self . labels :,119
"def _match(self, byte_chunk):
    quote_character = None
    data = byte_chunk.nhtml
    open_angle_bracket = data.rfind(""<"")
    # We are inside <...
    if open_angle_bracket <= data.rfind("">""):
        return False
    for s in data[open_angle_bracket + 1 :]:
        if s in ATTR_DELIMITERS:
            if quote_character and s == quote_character:
                quote_character = None
                continue
            elif not quote_character:
                quote_character = s
                continue
    if quote_character == self.quote_character:
        return True
    return False
",if quote_character and s == quote_character :,173
"def _display_history(config, script, base, head, currents=()):
    for sc in script.walk_revisions(base=base or ""base"", head=head or ""heads""):
        if indicate_current:
            sc._db_current_indicator = sc.revision in currents
        config.print_stdout(
            sc.cmd_format(
                verbose=verbose,
                include_branches=True,
                include_doc=True,
                include_parents=True,
            )
        )
",if indicate_current :,139
"def set(self, key=None, value=None):
    if key is not None:
        k = str(key)
        if value is not None:
            self.store[k] = value
        else:
            if self.store.has_key(k):
                del self.store[k]
    else:
        self.store.clear()
",if self . store . has_key ( k ) :,97
"def _finalize_load(*exc_info):
    try:
        success_keys = [k for k in data_keys if k not in failed_keys]
        if success_keys:
            self._holder_ref.put_objects_by_keys(
                session_id, success_keys, pin_token=pin_token
            )
        if exc_info:
            raise exc_info[1].with_traceback(exc_info[2]) from None
        if failed_keys:
            raise StorageFull(
                request_size=storage_full_sizes[0],
                capacity=storage_full_sizes[1],
                affected_keys=list(failed_keys),
            )
    finally:
        shared_bufs[:] = []
",if exc_info :,200
"def ignore_module(module):
    result = False
    for check in ignore_these:
        if ""/*"" in check:
            if check[:-1] in module:
                result = True
        else:
            if (os.getcwd() + ""/"" + check + "".py"") == module:
                result = True
    if result:
        print_warning(""Ignoring module: "" + module)
    return result
","if ( os . getcwd ( ) + ""/"" + check + "".py"" ) == module :",108
"def available(self, exception_flag=True):
    """"""True if the solver is available""""""
    if exception_flag is False:
        return cplex_import_available
    else:
        if cplex_import_available is False:
            raise ApplicationError(
                ""No CPLEX <-> Python bindings available - CPLEX direct ""
                ""solver functionality is not available""
            )
        else:
            return True
",if cplex_import_available is False :,115
"def close(self, checkcount=False):
    self.mutex.acquire()
    try:
        if checkcount:
            self.openers -= 1
            if self.openers == 0:
                self.do_close()
        else:
            if self.openers > 0:
                self.do_close()
            self.openers = 0
    finally:
        self.mutex.release()
",if checkcount :,116
"def __get__(self, obj, type=None):
    if obj is None:
        return self
    with self.lock:
        value = obj.__dict__.get(self.__name__, self._default_value)
        if value is self._default_value:
            value = self.func(obj)
            obj.__dict__[self.__name__] = value
        return value
",if value is self . _default_value :,96
"def _test_pooling_iteration(input_shape, **kwargs):
    """"""One iteration of pool operation with given shapes and attributes""""""
    x = -np.arange(np.prod(input_shape), dtype=np.float32).reshape(input_shape) - 1
    with tf.Graph().as_default():
        in_data = array_ops.placeholder(shape=input_shape, dtype=""float32"")
        nn_ops.pool(in_data, **kwargs)
        if kwargs[""pooling_type""] == ""MAX"":
            out_name = ""max_pool:0""
        else:
            out_name = ""avg_pool:0""
        compare_tf_with_tvm(x, ""Placeholder:0"", out_name)
","if kwargs [ ""pooling_type"" ] == ""MAX"" :",185
"def updateValue(self):
    if self._index:
        val = toInt(self._model.data(self._index))
        if self.sld.value() != val:
            self._updating = True
            self.setValue(val)
            self._updating = False
",if self . sld . value ( ) != val :,74
"def _count(self, element, count=True):
    if not isinstance(element, six.string_types):
        if self == element:
            return 1
    i = 0
    for child in self.children:
        # child is text content and element is also text content, then
        # make a simple ""text"" in ""text""
        if isinstance(child, six.string_types):
            if isinstance(element, six.string_types):
                if count:
                    i += child.count(element)
                elif element in child:
                    return 1
        else:
            i += child._count(element, count=count)
            if not count and i:
                return i
    return i
",if self == element :,196
"def test_doctests(self):
    """"""Run tutorial doctests.""""""
    runner = doctest.DocTestRunner()
    failures = []
    for test in doctest.DocTestFinder().find(TutorialDocTestHolder):
        failed, success = runner.run(test)
        if failed:
            name = test.name
            assert name.startswith(""TutorialDocTestHolder.doctest_"")
            failures.append(name[30:])
            # raise ValueError(""Tutorial doctest %s failed"" % test.name[30:])
    if failures:
        raise ValueError(
            ""%i Tutorial doctests failed: %s"" % (len(failures), "", "".join(failures))
        )
",if failed :,168
"def send_preamble(self):
    """"""Transmit version/status/date/server, via self._write()""""""
    if self.origin_server:
        if self.client_is_modern():
            self._write(""HTTP/%s %s\r\n"" % (self.http_version, self.status))
            if not self.headers.has_key(""Date""):
                self._write(""Date: %s\r\n"" % time.asctime(time.gmtime(time.time())))
            if self.server_software and not self.headers.has_key(""Server""):
                self._write(""Server: %s\r\n"" % self.server_software)
    else:
        self._write(""Status: %s\r\n"" % self.status)
","if not self . headers . has_key ( ""Date"" ) :",199
"def _verify_unique_measurement_keys(operations: Iterable[ops.Operation]):
    seen: Set[str] = set()
    for op in operations:
        if isinstance(op.gate, ops.MeasurementGate):
            meas = op.gate
            key = protocols.measurement_key(meas)
            if key in seen:
                raise ValueError(""Measurement key {} repeated"".format(key))
            seen.add(key)
",if key in seen :,120
"def test_dtype_basics(df):
    df[""new_virtual_column""] = df.x + 1
    for name in df.column_names:
        if df.dtype(name) == str_type:
            assert df[name].values.dtype.kind in ""OSU""
        else:
            assert df[name].values.dtype == df.dtype(df[name])
",if df . dtype ( name ) == str_type :,99
"def string_to_points(self, command, coord_string):
    numbers = string_to_numbers(coord_string)
    if command.upper() in [""H"", ""V""]:
        i = {""H"": 0, ""V"": 1}[command.upper()]
        xy = np.zeros((len(numbers), 2))
        xy[:, i] = numbers
        if command.isupper():
            xy[:, 1 - i] = self.relative_point[1 - i]
    elif command.upper() == ""A"":
        raise Exception(""Not implemented"")
    else:
        xy = np.array(numbers).reshape((len(numbers) // 2, 2))
    result = np.zeros((xy.shape[0], self.dim))
    result[:, :2] = xy
    return result
",if command . isupper ( ) :,193
"def get_count(self, peek=False):
    if self.argument_supplied:
        count = self.argument_value
        if self.argument_negative:
            if count == 0:
                count = -1
            else:
                count = -count
            if not peek:
                self.argument_negative = False
        if not peek:
            self.argument_supplied = False
    else:
        count = 1
    return count
",if not peek :,126
"def toggleSchedule(self, **kwargs):
    schedules = cfg.schedules()
    line = kwargs.get(""line"")
    if line:
        for i, schedule in enumerate(schedules):
            if schedule == line:
                # Toggle the schedule
                schedule_split = schedule.split()
                schedule_split[0] = ""%d"" % (schedule_split[0] == ""0"")
                schedules[i] = "" "".join(schedule_split)
                break
        cfg.schedules.set(schedules)
        config.save_config()
        sabnzbd.Scheduler.restart()
    raise Raiser(self.__root)
",if schedule == line :,175
"def test_sanity_no_long_entities(CorpusType: Type[ColumnCorpus]):
    corpus = CorpusType()
    longest_entity = []
    for sentence in corpus.get_all_sentences():
        entities = sentence.get_spans(""ner"")
        for entity in entities:
            if len(entity.tokens) > len(longest_entity):
                longest_entity = [t.text for t in entity.tokens]
    assert len(longest_entity) < 10, "" "".join(longest_entity)
",if len ( entity . tokens ) > len ( longest_entity ) :,123
"def _set_helper(settings, path, value, data_type=None):
    path = _to_settings_path(path)
    method = settings.set
    if data_type is not None:
        name = None
        if data_type == bool:
            name = ""setBoolean""
        elif data_type == float:
            name = ""setFloat""
        elif data_type == int:
            name = ""setInt""
        if name is not None:
            method = getattr(settings, name)
    method(path, value)
    settings.save()
",elif data_type == int :,150
"def scan_page(self, address_space, page_offset, fullpage=False):
    """"""Runs through patchers for a single page""""""
    if fullpage:
        pagedata = address_space.read(page_offset, PAGESIZE)
    for patcher in self.patchers:
        for offset, data in patcher.get_constraints():
            if fullpage:
                testdata = pagedata[offset : offset + len(data)]
            else:
                testdata = address_space.read(page_offset + offset, len(data))
            if data != testdata:
                break
        else:
            yield patcher
",if data != testdata :,166
"def accessSlice(self, node):
    self.visit(node.value)
    node.obj = self.getObj(node.value)
    self.access = _access.INPUT
    lower, upper = node.slice.lower, node.slice.upper
    if lower:
        self.visit(lower)
    if upper:
        self.visit(upper)
    if isinstance(node.obj, intbv):
        if self.kind == _kind.DECLARATION:
            self.require(lower, ""Expected leftmost index"")
            leftind = self.getVal(lower)
            if upper:
                rightind = self.getVal(upper)
            else:
                rightind = 0
            node.obj = node.obj[leftind:rightind]
",if self . kind == _kind . DECLARATION :,198
"def childConnectionLost(self, childFD):
    if self.state == 1:
        self.fail(""got connectionLost(%d) during state 1"" % childFD)
        return
    if self.state == 2:
        if childFD != 4:
            self.fail(""got connectionLost(%d) (not 4) during state 2"" % childFD)
            return
        self.state = 3
        self.transport.closeChildFD(5)
        return
",if childFD != 4 :,122
"def _find_matches(self, file, lookup, **kwargs):
    matches = []
    for format in lookup.values():
        if format.sniffer_function is not None:
            is_format, skwargs = format.sniffer_function(file, **kwargs)
            file.seek(0)
            if is_format:
                matches.append((format.name, skwargs))
    return matches
",if format . sniffer_function is not None :,108
"def ParseCodeLines(tokens, case):
    """"""Parse uncommented code in a test case.""""""
    _, kind, item = tokens.peek()
    if kind != PLAIN_LINE:
        raise ParseError(""Expected a line of code (got %r, %r)"" % (kind, item))
    code_lines = []
    while True:
        _, kind, item = tokens.peek()
        if kind != PLAIN_LINE:
            case[""code""] = ""\n"".join(code_lines) + ""\n""
            return
        code_lines.append(item)
        tokens.next()
",if kind != PLAIN_LINE :,148
"def _recursive_process(self):
    super(RecursiveObjectDownwardsVisitor, self)._recursive_process()
    while self._new_for_visit:
        func_ea, arg_idx = self._new_for_visit.pop()
        if helper.is_imported_ea(func_ea):
            continue
        cfunc = helper.decompile_function(func_ea)
        if cfunc:
            assert arg_idx < len(cfunc.get_lvars()), ""Wrong argument at func {}"".format(
                to_hex(func_ea)
            )
            obj = VariableObject(cfunc.get_lvars()[arg_idx], arg_idx)
            self.prepare_new_scan(cfunc, arg_idx, obj)
            self._recursive_process()
",if helper . is_imported_ea ( func_ea ) :,199
"def GetBoundingBoxMin(self):
    """"""Get the minimum bounding box.""""""
    x1, y1 = 10000, 10000
    x2, y2 = -10000, -10000
    for point in self._lineControlPoints:
        if point[0] < x1:
            x1 = point[0]
        if point[1] < y1:
            y1 = point[1]
        if point[0] > x2:
            x2 = point[0]
        if point[1] > y2:
            y2 = point[1]
    return x2 - x1, y2 - y1
",if point [ 0 ] > x2 :,158
"def __init__(
    self,
    detail=None,
    headers=None,
    comment=None,
    body_template=None,
    location=None,
    add_slash=False,
):
    super(_HTTPMove, self).__init__(
        detail=detail, headers=headers, comment=comment, body_template=body_template
    )
    if location is not None:
        self.location = location
        if add_slash:
            raise TypeError(
                ""You can only provide one of the arguments location "" ""and add_slash""
            )
    self.add_slash = add_slash
",if add_slash :,155
"def __str__(self, prefix="""", printElemNumber=0):
    res = """"
    cnt = 0
    for e in self.presence_response_:
        elm = """"
        if printElemNumber:
            elm = ""(%d)"" % cnt
        res += prefix + (""presence_response%s <\n"" % elm)
        res += e.__str__(prefix + ""  "", printElemNumber)
        res += prefix + "">\n""
        cnt += 1
    return res
",if printElemNumber :,125
"def _find_first_match(self, request):
    match_failed_reasons = []
    for i, match in enumerate(self._matches):
        match_result, reason = match.matches(request)
        if match_result:
            return match, match_failed_reasons
        else:
            match_failed_reasons.append(reason)
    return None, match_failed_reasons
",if match_result :,99
"def index(self, req, volume_id):
    req_version = req.api_version_request
    metadata = super(Controller, self).index(req, volume_id)
    if req_version.matches(mv.ETAGS):
        data = jsonutils.dumps(metadata)
        if six.PY3:
            data = data.encode(""utf-8"")
        resp = webob.Response()
        resp.headers[""Etag""] = hashlib.md5(data).hexdigest()
        resp.body = data
        return resp
    return metadata
",if six . PY3 :,140
"def init(self):
    """"""Called after document is loaded.""""""
    # Create div to put dynamic CSS assets in
    self.asset_node = window.document.createElement(""div"")
    self.asset_node.id = ""Flexx asset container""
    window.document.body.appendChild(self.asset_node)
    if self.is_exported:
        if self.is_notebook:
            print(""Flexx: I am in an exported notebook!"")
        else:
            print(""Flexx: I am in an exported app!"")
            self.run_exported_app()
    else:
        print(""Flexx: Initializing"")
        if not self.is_notebook:
            self._remove_querystring()
        self.init_logging()
",if not self . is_notebook :,188
"def get_default_person(self):
    """"""Return the default Person of the database.""""""
    person_handle = self.get_default_handle()
    if person_handle:
        person = self.get_person_from_handle(person_handle)
        if person:
            return person
        elif (self.metadata) and (not self.readonly):
            # Start transaction
            with BSDDBTxn(self.env, self.metadata) as txn:
                txn.put(b""default"", None)
            return None
    else:
        return None
",elif ( self . metadata ) and ( not self . readonly ) :,149
"def reader():
    async with read:
        await wait_all_tasks_blocked()
        total_received = 0
        while True:
            # 5000 is chosen because it doesn't evenly divide 2**20
            received = len(await read.receive_some(5000))
            if not received:
                break
            total_received += received
        assert total_received == count * replicas
",if not received :,104
"def array_module(a):
    if isinstance(a, np.ndarray):
        return np
    else:
        from pyopencl.array import Array
        if isinstance(a, Array):
            return _CLFakeArrayModule(a.queue)
        else:
            raise TypeError(""array type not understood: %s"" % type(a))
","if isinstance ( a , Array ) :",89
"def __str__(self):
    path = super(XPathExpr, self).__str__()
    if self.textnode:
        if path == ""*"":
            path = ""text()""
        elif path.endswith(""::*/*""):
            path = path[:-3] + ""text()""
        else:
            path += ""/text()""
    if self.attribute is not None:
        if path.endswith(""::*/*""):
            path = path[:-2]
        path += ""/@%s"" % self.attribute
    return path
","if path == ""*"" :",132
"def update(self):
    if self.saved():
        rgns = self.view.get_regions(self.region_key)
        if rgns:
            rgn = Region.from_region(self.view, rgns[0], self.region_key)
            self.start = rgn.start
            self.end = rgn.end
",if rgns :,92
"def PrintServerName(data, entries):
    if entries > 0:
        entrieslen = 26 * entries
        chunks, chunk_size = len(data[:entrieslen]), entrieslen / entries
        ServerName = [data[i : i + chunk_size] for i in range(0, chunks, chunk_size)]
        l = []
        for x in ServerName:
            FP = WorkstationFingerPrint(x[16:18])
            Name = x[:16].replace(""\x00"", """")
            if FP:
                l.append(Name + "" (%s)"" % FP)
            else:
                l.append(Name)
        return l
    return None
",if FP :,177
"def add_lookup(self, name_type, pyname, jsname, depth=-1):
    jsname = self.jsname(name_type, jsname)
    if self.local_prefix is not None:
        if jsname.find(self.local_prefix) != 0:
            jsname = self.jsname(name_type, ""%s.%s"" % (self.local_prefix, jsname))
    if self.lookup_stack[depth].has_key(pyname):
        name_type = self.lookup_stack[depth][pyname][0]
    if self.module_name != ""pyjslib"" or pyname != ""int"":
        self.lookup_stack[depth][pyname] = (name_type, pyname, jsname)
    return jsname
",if jsname . find ( self . local_prefix ) != 0 :,191
"def ensure_echo_on():
    if termios:
        fd = sys.stdin
        if fd.isatty():
            attr_list = termios.tcgetattr(fd)
            if not attr_list[3] & termios.ECHO:
                attr_list[3] |= termios.ECHO
                if hasattr(signal, ""SIGTTOU""):
                    old_handler = signal.signal(signal.SIGTTOU, signal.SIG_IGN)
                else:
                    old_handler = None
                termios.tcsetattr(fd, termios.TCSANOW, attr_list)
                if old_handler is not None:
                    signal.signal(signal.SIGTTOU, old_handler)
",if fd . isatty ( ) :,197
"def get_query_results(user, query_id, bring_from_cache):
    query = _load_query(user, query_id)
    if bring_from_cache:
        if query.latest_query_data_id is not None:
            results = query.latest_query_data.data
        else:
            raise Exception(""No cached result available for query {}."".format(query.id))
    else:
        results, error = query.data_source.query_runner.run_query(
            query.query_text, user
        )
        if error:
            raise Exception(""Failed loading results for query id {}."".format(query.id))
        else:
            results = json_loads(results)
    return results
",if query . latest_query_data_id is not None :,188
"def on_tag_added_to_page(self, o, row, pagerow):
    self.flush_cache()
    if row[""name""] in self.tags and self._matches_all(pagerow[""id""]):
        # Without the new tag it did not match, so add to view
        # Find top level entry - ignore possible deeper matches
        for treepath in self._find_all_pages(pagerow[""name""]):
            if len(treepath) == 1:
                treeiter = self.get_iter(treepath)  # not mytreeiter !
                self.emit(""row-inserted"", treepath, treeiter)
                if pagerow[""n_children""] > 0:
                    self._emit_children_inserted(pagerow[""id""], treepath)
","if pagerow [ ""n_children"" ] > 0 :",196
"def _is_subnet_of(a, b):
    try:
        # Always false if one is v4 and the other is v6.
        if a._version != b._version:
            raise TypeError(f""{a} and {b} are not of the same version"")
        return (
            b.network_address <= a.network_address
            and b.broadcast_address >= a.broadcast_address
        )
    except AttributeError:
        raise TypeError(f""Unable to test subnet containment "" f""between {a} and {b}"")
",if a . _version != b . _version :,140
"def consume(d={}):
    """"""Add attribute list to the dictionary 'd' and reset the list.""""""
    if AttributeList.attrs:
        d.update(AttributeList.attrs)
        AttributeList.attrs = {}
        # Generate option attributes.
        if ""options"" in d:
            options = parse_options(d[""options""], (), ""illegal option name"")
            for option in options:
                d[option + ""-option""] = """"
","if ""options"" in d :",113
"def tearDown(self):
    # make sure all of the subprocesses are dead
    for pidfile in self.pidfiles:
        if not os.path.exists(pidfile):
            continue
        with open(pidfile) as f:
            pid = f.read()
        if not pid:
            return
        pid = int(pid)
        try:
            os.kill(pid, signal.SIGKILL)
        except OSError:
            pass
    # and clean up leftover pidfiles
    for pidfile in self.pidfiles:
        if os.path.exists(pidfile):
            os.unlink(pidfile)
    self.tearDownBasedir()
",if os . path . exists ( pidfile ) :,167
"def sort(self, items):
    slow_sorts = []
    switch_slow = False
    for sort in reversed(self.sorts):
        if switch_slow:
            slow_sorts.append(sort)
        elif sort.order_clause() is None:
            switch_slow = True
            slow_sorts.append(sort)
        else:
            pass
    for sort in slow_sorts:
        items = sort.sort(items)
    return items
",elif sort . order_clause ( ) is None :,121
"def shortcut(input, ch_out, stride):
    ch_in = input.shape[1]
    if ch_in != ch_out:
        if stride == 1:
            filter_size = 1
        else:
            filter_size = 3
        return conv_bn_layer(input, ch_out, filter_size, stride)
    else:
        return input
",if stride == 1 :,97
"def detab(self, text):
    """"""Remove a tab from the front of each line of the given text.""""""
    newtext = []
    lines = text.split(""\n"")
    for line in lines:
        if line.startswith("" "" * self.tab_length):
            newtext.append(line[self.tab_length :])
        elif not line.strip():
            newtext.append("""")
        else:
            break
    return ""\n"".join(newtext), ""\n"".join(lines[len(newtext) :])
",elif not line . strip ( ) :,134
"def construct_instances(self, row, keys=None):
    collected_models = {}
    for i, (key, constructor, attr, conv) in enumerate(self.column_map):
        if keys is not None and key not in keys:
            continue
        value = row[i]
        if key not in collected_models:
            collected_models[key] = constructor()
        instance = collected_models[key]
        if attr is None:
            attr = self.cursor.description[i][0]
        if conv is not None:
            value = conv(value)
        setattr(instance, attr, value)
    return collected_models
",if attr is None :,167
"def stop_loggers(self):
    super(NetconsoleHost, self).stop_loggers()
    if self.__logger:
        utils.nuke_subprocess(self.__logger)
        self.__logger = None
        if self.job:
            self.job.warning_loggers.discard(self.__warning_stream)
        self.__warning_stream.close()
",if self . job :,93
"def get_template_context(node, context, context_lines=3):
    line, source_lines, name = get_template_source_from_exception_info(node, context)
    debug_context = []
    start = max(1, line - context_lines)
    end = line + 1 + context_lines
    for line_num, content in source_lines:
        if start <= line_num <= end:
            debug_context.append(
                {""num"": line_num, ""content"": content, ""highlight"": (line_num == line)}
            )
    return {""name"": name, ""context"": debug_context}
",if start <= line_num <= end :,158
"def arg_names(self, lineage, command_name, positional_arg=False):
    parent = ""."".join(lineage)
    arg_names = self.index[""arg_names""].get(parent, {}).get(command_name, [])
    filtered_arg_names = []
    for arg_name in arg_names:
        arg_data = self.get_argument_data(lineage, command_name, arg_name)
        if arg_data.positional_arg == positional_arg:
            filtered_arg_names.append(arg_name)
    return filtered_arg_names
",if arg_data . positional_arg == positional_arg :,140
"def attributive(adjective, gender=MALE):
    w = adjective.lower()
    # normal => normales
    if PLURAL in gender and not is_vowel(w[-1:]):
        return w + ""es""
    # el chico inteligente => los chicos inteligentes
    if PLURAL in gender and w.endswith((""a"", ""e"")):
        return w + ""s""
    # el chico alto => los chicos altos
    if w.endswith(""o""):
        if FEMININE in gender and PLURAL in gender:
            return w[:-1] + ""as""
        if FEMININE in gender:
            return w[:-1] + ""a""
        if PLURAL in gender:
            return w + ""s""
    return w
",if FEMININE in gender and PLURAL in gender :,197
"def _get_disk_size(cls, path, ignored=None):
    if ignored is None:
        ignored = []
    if path in ignored:
        return 0
    total = 0
    for entry in scandir(path):
        if entry.is_dir():
            total += cls._get_disk_size(entry.path, ignored=ignored)
        elif entry.is_file():
            total += entry.stat().st_size
    return total
",elif entry . is_file ( ) :,117
"def validateHeaders(self):
    if ""Cookie"" in self.headers:
        for session in self.factory.authenticated_sessions:
            if ""TWISTED_SESSION="" + session.uid in self.headers[""Cookie""]:
                return WebSocketProtocol.validateHeaders(self)
    return False
","if ""TWISTED_SESSION="" + session . uid in self . headers [ ""Cookie"" ] :",74
"def _format_privilege_data(self, data):
    for key in [""spcacl""]:
        if key in data and data[key] is not None:
            if ""added"" in data[key]:
                data[key][""added""] = parse_priv_to_db(data[key][""added""], self.acl)
            if ""changed"" in data[key]:
                data[key][""changed""] = parse_priv_to_db(data[key][""changed""], self.acl)
            if ""deleted"" in data[key]:
                data[key][""deleted""] = parse_priv_to_db(data[key][""deleted""], self.acl)
","if ""added"" in data [ key ] :",168
"def show_text(text):
    print(_stash.text_color(""="" * 20, ""yellow""))
    lines = text.split(""\n"")
    while True:
        if len(lines) < 100:
            print(""\n"".join(lines))
            return
        else:
            print(""\n"".join(lines[:100]))
            lines = lines[100:]
            prompt = _stash.text_color(""(Press Return to continue)"", ""yellow"")
            raw_input(prompt)
    print(""\n"")
",if len ( lines ) < 100 :,135
"def run(self):
    TimeInspector.set_time_mark()
    for tuner_index, tuner_config in enumerate(self.pipeline_config):
        tuner = self.init_tuner(tuner_index, tuner_config)
        tuner.tune()
        if self.global_best_res is None or self.global_best_res > tuner.best_res:
            self.global_best_res = tuner.best_res
            self.global_best_params = tuner.best_params
            self.best_tuner_index = tuner_index
    TimeInspector.log_cost_time(""Finished tuner pipeline."")
    self.save_tuner_exp_info()
",if self . global_best_res is None or self . global_best_res > tuner . best_res :,194
"def OnEvent(self, propGrid, aProperty, ctrl, event):
    if event.GetEventType() == wx.wxEVT_BUTTON:
        buttons = propGrid.GetEditorControlSecondary()
        if event.GetId() == buttons.GetButtonId(0):
            # Do something when the first button is pressed
            # Return true if the action modified the value in editor.
            ...
        if event.GetId() == buttons.GetButtonId(1):
            # Do something when the second button is pressed
            ...
        if event.GetId() == buttons.GetButtonId(2):
            # Do something when the third button is pressed
            ...
    return wx.propgrid.PGTextCtrlEditor.OnEvent(propGrid, aProperty, ctrl, event)
",if event . GetId ( ) == buttons . GetButtonId ( 1 ) :,195
"def run(self, edit):
    view = self.view
    for sel in view.sel():
        if not self.is_valid_scope(sel):
            continue
        region = view.extract_scope(sel.end())
        content = self.extract_content(region)
        resolver, content = self.resolve(content)
        if content is None:
            sublime.error_message(""Could not resolve link:\n%s"" % content)
            continue
        resolver.execute(content)
",if content is None :,131
"def __init__(self, aList):
    for element in aList:
        if len(element) > 0:
            if element.tag == element[0].tag:
                self.append(ListParser(element))
            else:
                self.append(DictParser(element))
        elif element.text:
            text = element.text.strip()
            if text:
                self.append(text)
",elif element . text :,116
"def put(self, can_split=False):
    for node in (self.nodes)[:1]:
        if self.has_value(node):
            node.put(can_split=can_split)
    for node in (self.nodes)[1:]:
        self.line_more(SLICE_COLON, can_split_after=True)
        if self.has_value(node):
            node.put(can_split=can_split)
    return self
",if self . has_value ( node ) :,118
"def process_return_exits(self, exits):
    """"""Add arcs due to jumps from `exits` being returns.""""""
    for block in self.nearest_blocks():
        if isinstance(block, TryBlock) and block.final_start is not None:
            block.return_from.update(exits)
            break
        elif isinstance(block, FunctionBlock):
            for xit in exits:
                self.add_arc(
                    xit.lineno,
                    -block.start,
                    xit.cause,
                    ""didn't return from function {!r}"".format(block.name),
                )
            break
","elif isinstance ( block , FunctionBlock ) :",179
"def find_commands(management_dir):
    # Modified version of function from django/core/management/__init__.py.
    command_dir = os.path.join(management_dir, ""commands"")
    commands = []
    try:
        for f in os.listdir(command_dir):
            if f.startswith(""_""):
                continue
            elif f.endswith("".py"") and f[:-3] not in commands:
                commands.append(f[:-3])
            elif f.endswith("".pyc"") and f[:-4] not in commands:
                commands.append(f[:-4])
    except OSError:
        pass
    return commands
","elif f . endswith ( "".py"" ) and f [ : - 3 ] not in commands :",164
"def split_path_info(path):
    # suitable for splitting an already-unquoted-already-decoded (unicode)
    # path value
    path = path.strip(""/"")
    clean = []
    for segment in path.split(""/""):
        if not segment or segment == ""."":
            continue
        elif segment == "".."":
            if clean:
                del clean[-1]
        else:
            clean.append(segment)
    return tuple(clean)
","if not segment or segment == ""."" :",115
"def __init__(self, source_definition, **kw):
    super(RekallEFilterArtifacts, self).__init__(source_definition, **kw)
    for column in self.fields:
        if ""name"" not in column or ""type"" not in column:
            raise errors.FormatError(
                u""Field definition should have both name and type.""
            )
        mapped_type = column[""type""]
        if mapped_type not in self.allowed_types:
            raise errors.FormatError(u""Unsupported type %s."" % mapped_type)
","if ""name"" not in column or ""type"" not in column :",143
"def _name(self, sender, short=True, full_email=False):
    words = re.sub('[""<>]', """", sender).split()
    nomail = [w for w in words if not ""@"" in w]
    if nomail:
        if short:
            if len(nomail) > 1 and nomail[0].lower() in self._NAME_TITLES:
                return nomail[1]
            return nomail[0]
        return "" "".join(nomail)
    elif words:
        if not full_email:
            return words[0].split(""@"", 1)[0]
        return words[0]
    return ""(nobody)""
",if len ( nomail ) > 1 and nomail [ 0 ] . lower ( ) in self . _NAME_TITLES :,168
"def _get_consuming_layers(self, check_layer):
    """"""Returns all the layers which are out nodes from the layer.""""""
    consuming_layers = []
    for layer in self._config[""layers""]:
        for inbound_node in layer[""inbound_nodes""]:
            for connection_info in inbound_node:
                if connection_info[0] == check_layer[""config""][""name""]:
                    consuming_layers.append(layer)
    return consuming_layers
","if connection_info [ 0 ] == check_layer [ ""config"" ] [ ""name"" ] :",126
"def _check_feasible_fuse(self, model):
    if not self.modules_to_fuse:
        return False
    for group in self.modules_to_fuse:
        if not all(_recursive_hasattr(model, m) for m in group):
            raise MisconfigurationException(
                f""You have requested to fuse {group} but one or more of them is not your model attributes""
            )
    return True
","if not all ( _recursive_hasattr ( model , m ) for m in group ) :",109
"def cancel_loan_repayment_entry(self):
    for loan in self.loans:
        if loan.loan_repayment_entry:
            repayment_entry = frappe.get_doc(
                ""Loan Repayment"", loan.loan_repayment_entry
            )
            repayment_entry.cancel()
",if loan . loan_repayment_entry :,94
"def update_channel_entries(self, request):
    try:
        request_parsed = await request.json()
    except (ContentTypeError, ValueError):
        return RESTResponse({""error"": ""Bad JSON""}, status=HTTP_BAD_REQUEST)
    results_list = []
    for entry in request_parsed:
        public_key = database_blob(unhexlify(entry.pop(""public_key"")))
        id_ = entry.pop(""id"")
        error, result = self.update_entry(public_key, id_, entry)
        # TODO: handle the results for a list that contains some errors in a smarter way
        if error:
            return RESTResponse(result, status=error)
        results_list.append(result)
    return RESTResponse(results_list)
",if error :,194
"def delete(self, userId: str, bucket: str, key: str) -> bool:
    if not self.initialized:
        raise Exception(""archive not initialized"")
    try:
        with db.session_scope() as dbsession:
            rc = db_archivedocument.delete(userId, bucket, key, session=dbsession)
            if not rc:
                raise Exception(""failed to delete DB record"")
            else:
                return True
    except Exception as err:
        raise err
",if not rc :,130
"def handle_phase(task, config):
    """"""Function that runs all of the configured plugins which act on the current phase.""""""
    # Keep a list of all results, for input plugin combining
    results = []
    for item in config:
        for plugin_name, plugin_config in item.items():
            if phase in plugin.get_phases_by_plugin(plugin_name):
                method = plugin.get_plugin_by_name(plugin_name).phase_handlers[phase]
                log.debug(""Running plugin %s"" % plugin_name)
                result = method(task, plugin_config)
                if phase == ""input"" and result:
                    results.append(result)
    return itertools.chain(*results)
","if phase == ""input"" and result :",188
"def guess_gitlab_remote(self):
    upstream = self.get_upstream_for_active_branch()
    integrated_remote = self.get_integrated_remote_name()
    remotes = self.get_remotes()
    if len(self.remotes) == 1:
        return list(remotes.keys())[0]
    elif upstream:
        tracked_remote = upstream.split(""/"")[0] if upstream else None
        if tracked_remote and tracked_remote == integrated_remote:
            return tracked_remote
        else:
            return None
    else:
        return integrated_remote
",if tracked_remote and tracked_remote == integrated_remote :,154
"def do_test(self, path):
    reader = paddle.reader.creator.recordio(path)
    idx = 0
    for e in reader():
        if idx == 0:
            self.assertEqual(e, (1, 2, 3))
        elif idx == 1:
            self.assertEqual(e, (4, 5, 6))
        idx += 1
    self.assertEqual(idx, 2)
",elif idx == 1 :,106
"def gen_cpu_name(cpu):
    if cpu == ""simple"":
        return event_download.get_cpustr()
    for j in known_cpus:
        if cpu == j[0]:
            if isinstance(j[1][0], tuple):
                return ""GenuineIntel-6-%02X-%d"" % j[1][0]
            else:
                return ""GenuineIntel-6-%02X"" % j[1][0]
    assert False
","if isinstance ( j [ 1 ] [ 0 ] , tuple ) :",127
"def read_kernel_cmdline_config(cmdline=None):
    if cmdline is None:
        cmdline = util.get_cmdline()
    if ""network-config="" in cmdline:
        data64 = None
        for tok in cmdline.split():
            if tok.startswith(""network-config=""):
                data64 = tok.split(""="", 1)[1]
        if data64:
            if data64 == KERNEL_CMDLINE_NETWORK_CONFIG_DISABLED:
                return {""config"": ""disabled""}
            return util.load_yaml(_b64dgz(data64))
    return None
",if data64 :,152
"def _verify_bot(self, ctx: ""Context"") -> None:
    if ctx.guild is None:
        bot_user = ctx.bot.user
    else:
        bot_user = ctx.guild.me
        cog = ctx.cog
        if cog and await ctx.bot.cog_disabled_in_guild(cog, ctx.guild):
            raise discord.ext.commands.DisabledCommand()
    bot_perms = ctx.channel.permissions_for(bot_user)
    if not (bot_perms.administrator or bot_perms >= self.bot_perms):
        raise BotMissingPermissions(
            missing=self._missing_perms(self.bot_perms, bot_perms)
        )
","if cog and await ctx . bot . cog_disabled_in_guild ( cog , ctx . guild ) :",181
"def _split_values(self, value):
    # do the regex mojo here
    if not self.allowed_values:
        return ("""",)
    try:
        r = re.compile(self.allowed_values)
    except:
        print(self.allowed_values, file=sys.stderr)
        raise
    s = str(value)
    i = 0
    vals = []
    while True:
        m = r.search(s[i:])
        if m is None:
            break
        vals.append(m.group())
        delimiter = s[i : i + m.start()]
        if self.delimiter is None and delimiter != """":
            self.delimiter = delimiter
        i += m.end()
    return tuple(vals)
",if m is None :,192
"def _count(self, element, count=True):
    if not isinstance(element, six.string_types):
        if self == element:
            return 1
    i = 0
    for child in self.children:
        # child is text content and element is also text content, then
        # make a simple ""text"" in ""text""
        if isinstance(child, six.string_types):
            if isinstance(element, six.string_types):
                if count:
                    i += child.count(element)
                elif element in child:
                    return 1
        else:
            i += child._count(element, count=count)
            if not count and i:
                return i
    return i
",if not count and i :,196
"def set_page(self, page):
    """"""If a page is present as a bookmark than select it.""""""
    pagename = page.name
    with self.on_bookmark_clicked.blocked():
        for button in self.scrolledbox.get_scrolled_children():
            if button.zim_path == pagename:
                button.set_active(True)
            else:
                button.set_active(False)
",if button . zim_path == pagename :,112
"def get_Subclass_of(rt):
    for y in [getattr(Ast, x) for x in dir(Ast)]:
        yt = clr.GetClrType(y)
        if rt == yt:
            continue
        if yt.IsAbstract:
            continue
        if yt.IsSubclassOf(rt):
            yield yt.Name
",if rt == yt :,93
"def update_parent_columns(self):
    ""Update the parent columns of the current focus column.""
    f = self.columns.get_focus_column()
    col = self.col_list[f]
    while 1:
        parent, pcol = self.get_parent(col)
        if pcol is None:
            return
        changed = pcol.update_results(start_from=parent)
        if not changed:
            return
        col = pcol
",if not changed :,121
"def get_template_engine(themes):
    """"""Get template engine used by a given theme.""""""
    for theme_name in themes:
        engine_path = os.path.join(theme_name, ""engine"")
        if os.path.isfile(engine_path):
            with open(engine_path) as fd:
                return fd.readlines()[0].strip()
    # default
    return ""mako""
",if os . path . isfile ( engine_path ) :,104
"def reConnect(self):
    while self.retrymax is None or self.retries < self.retrymax:
        logger.info(""Cobra reconnection attempt"")
        try:
            self.conn = self.httpfact()
            if self._cobra_sessid:
                self.authUser(self.authinfo)
            self.retries = 0
            return
        except Exception as e:
            time.sleep(2 ** self.retries)
            self.retries += 1
    self.trashed = True
    raise CobraHttpException(""Retry Exceeded!"")
",if self . _cobra_sessid :,149
"def __eq__(self, other):
    if isinstance(other, OrderedDict):
        if len(self) != len(other):
            return False
        for p, q in zip(list(self.items()), list(other.items())):
            if p != q:
                return False
        return True
    return dict.__eq__(self, other)
",if p != q :,91
"def __getExpectedSampleOffsets(self, tileOrigin, area1, area2):
    ts = GafferImage.ImagePlug.tileSize()
    data = []
    for y in range(tileOrigin.y, tileOrigin.y + ts):
        for x in range(tileOrigin.x, tileOrigin.x + ts):
            pixel = imath.V2i(x, y)
            data.append(data[-1] if data else 0)
            if GafferImage.BufferAlgo.contains(area1, pixel):
                data[-1] += 1
            if GafferImage.BufferAlgo.contains(area2, pixel):
                data[-1] += 1
    return IECore.IntVectorData(data)
","if GafferImage . BufferAlgo . contains ( area1 , pixel ) :",190
"def _get_changes(self):
    """"""Get changes from CHANGES.txt.""""""
    log_lines = []
    found_version = False
    found_items = False
    with open(""CHANGES.txt"", ""r"") as fp:
        for line in fp.readlines():
            line = line.rstrip()
            if line.endswith(VERSION_TEXT_SHORT):
                found_version = True
            if not line.strip() and found_items:
                break
            elif found_version and line.startswith(""- ""):
                log_lines.append("" "" * 2 + ""* "" + line[2:])
                found_items = True
    return log_lines
","elif found_version and line . startswith ( ""- "" ) :",174
"def _next_hid(self, n=1):
    # this is overriden in mapping.py db_next_hid() method
    if len(self.datasets) == 0:
        return n
    else:
        last_hid = 0
        for dataset in self.datasets:
            if dataset.hid > last_hid:
                last_hid = dataset.hid
        return last_hid + n
",if dataset . hid > last_hid :,105
"def setInt(self, path, value, **kwargs):
    if value is None:
        self.set(path, None, **kwargs)
        return
    minimum = kwargs.pop(""min"", None)
    maximum = kwargs.pop(""max"", None)
    try:
        intValue = int(value)
        if minimum is not None and intValue < minimum:
            intValue = minimum
        if maximum is not None and intValue > maximum:
            intValue = maximum
    except ValueError:
        self._logger.warning(
            ""Could not convert %r to a valid integer when setting option %r""
            % (value, path)
        )
        return
    self.set(path, intValue, **kwargs)
",if maximum is not None and intValue > maximum :,187
"def _load_idle_extensions(self, sub_section, fp, lineno):
    extension_map = self.get_data(""idle extensions"")
    if extension_map is None:
        extension_map = {}
    extensions = []
    while 1:
        line, lineno, bBreak = self._readline(fp, lineno)
        if bBreak:
            break
        line = line.strip()
        if line:
            extensions.append(line)
    extension_map[sub_section] = extensions
    self._save_data(""idle extensions"", extension_map)
    return line, lineno
",if bBreak :,149
"def _get_config(key):
    config = db.session.execute(
        Configs.__table__.select().where(Configs.key == key)
    ).fetchone()
    if config and config.value:
        value = config.value
        if value and value.isdigit():
            return int(value)
        elif value and isinstance(value, string_types):
            if value.lower() == ""true"":
                return True
            elif value.lower() == ""false"":
                return False
            else:
                return value
    # Flask-Caching is unable to roundtrip a value of None.
    # Return an exception so that we can still cache and avoid the db hit
    return KeyError
",if value and value . isdigit ( ) :,181
"def check_labels(self):
    print(""Checking labels if they are outside the image"")
    for i in self.Dataframe.index:
        image_name = os.path.join(self.project_path, i)
        im = PIL.Image.open(image_name)
        self.width, self.height = im.size
        for ind in self.individual_names:
            if ind == ""single"":
                self.Dataframe = MainFrame.force_outside_labels_Nans(
                    self, i, ind, self.uniquebodyparts
                )
            else:
                self.Dataframe = MainFrame.force_outside_labels_Nans(
                    self, i, ind, self.multianimalbodyparts
                )
    return self.Dataframe
","if ind == ""single"" :",196
"def remove_excluded(self):
    """"""Remove all sources marked as excluded.""""""
    # import yaml
    # print yaml.dump({k:v.__json__() for k,v in self.sources.items()}, default_flow_style=False)
    sources = list(self.sources.values())
    for src in sources:
        if src.excluded:
            del self.sources[src.name]
        src.imports = [m for m in src.imports if not self._exclude(m)]
        src.imported_by = [m for m in src.imported_by if not self._exclude(m)]
",if src . excluded :,146
"def parse_scientific_formats(data, tree):
    scientific_formats = data.setdefault(""scientific_formats"", {})
    for elem in tree.findall("".//scientificFormats/scientificFormatLength""):
        type = elem.attrib.get(""type"")
        if _should_skip_elem(elem, type, scientific_formats):
            continue
        pattern = text_type(elem.findtext(""scientificFormat/pattern""))
        scientific_formats[type] = numbers.parse_pattern(pattern)
","if _should_skip_elem ( elem , type , scientific_formats ) :",132
"def _modifierCodes2Labels(cls, mods):
    if mods == 0:
        return []
    modconstants = cls._modifierCodes
    modNameList = []
    for k in modconstants._keys:
        mc = modconstants._names[k]
        if mods & k == k:
            modNameList.append(mc)
            mods = mods - k
            if mods == 0:
                return modNameList
    return modNameList
",if mods == 0 :,121
"def to_pig_latin(text: str):
    if text is None:
        return """"
    words = text.lower().strip().split("" "")
    text = []
    for word in words:
        if word[0] in ""aeiou"":
            text.append(f""{word}yay"")
        else:
            for letter in word:
                if letter in ""aeiou"":
                    text.append(
                        f""{word[word.index(letter):]}{word[:word.index(letter)]}ay""
                    )
                    break
    return "" "".join(text)
","if letter in ""aeiou"" :",165
"def __connect__(self) -> H2Protocol:
    if not self._connected:
        async with self._connect_lock:
            self._state = _ChannelState.CONNECTING
            if not self._connected:
                try:
                    self._protocol = await self._create_connection()
                except Exception:
                    self._state = _ChannelState.TRANSIENT_FAILURE
                    raise
                else:
                    self._state = _ChannelState.READY
    return cast(H2Protocol, self._protocol)
",if not self . _connected :,147
"def run_commands(cmds):
    set_kubeconfig_environment_var()
    for cmd in cmds:
        process = subprocess.run(
            cmd,
            shell=True,
            check=True,
            universal_newlines=True,
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE,
            env=os.environ,
        )
        if process.stdout:
            logger.info(process.stdout)
        if process.stderr:
            logger.info(process.stderr)
    return process.stdout
",if process . stderr :,146
"def deserialize(x):
    t = type(x)
    if t is list:
        return list(imap(deserialize, x))
    if t is dict:
        if ""_id_"" not in x:
            return {key: deserialize(val) for key, val in iteritems(x)}
        obj = objmap.get(x[""_id_""])
        if obj is None:
            entity_name = x[""class""]
            entity = database.entities[entity_name]
            pk = x[""_pk_""]
            obj = entity[pk]
        return obj
    return x
","if ""_id_"" not in x :",150
"def _parse_arguments(self, handler_method):
    spec = DynamicArgumentParser().parse(self._argspec, self.longname)
    if not self._supports_kwargs:
        if spec.kwargs:
            raise DataError(
                ""Too few '%s' method parameters for **kwargs ""
                ""support."" % self._run_keyword_method_name
            )
        if spec.kwonlyargs:
            raise DataError(
                ""Too few '%s' method parameters for ""
                ""keyword-only arguments support."" % self._run_keyword_method_name
            )
    spec.types = GetKeywordTypes(self.library.get_instance())(self._handler_name)
    return spec
",if spec . kwargs :,183
"def test_update_password_command(mocker, username, password, expected, changed):
    with mocker.patch.object(UpdatePassword, ""update_password"", return_value=changed):
        result, stdout, stderr = run_command(
            ""update_password"", username=username, password=password
        )
        if result is None:
            assert stdout == expected
        else:
            assert str(result) == expected
",if result is None :,108
"def characters(self, ch):
    if self.Text_tag:
        if self.Summary_tag:
            self.Summary_ch += ch
        elif self.Attack_Prerequisite_tag:
            self.Attack_Prerequisite_ch += ch
        elif self.Solution_or_Mitigation_tag:
            self.Solution_or_Mitigation_ch += ch
    elif self.CWE_ID_tag:
        self.CWE_ID_ch += ch
",elif self . Attack_Prerequisite_tag :,127
"def _pybin_add_zip(pybin, libname, filter, exclusions, dirs, dirs_with_init_py):
    with zipfile.ZipFile(libname, ""r"") as lib:
        name_list = lib.namelist()
        for name in name_list:
            if filter(name) and not _is_python_excluded_path(name, exclusions):
                if dirs is not None and dirs_with_init_py is not None:
                    _update_init_py_dirs(name, dirs, dirs_with_init_py)
                pybin.writestr(name, lib.read(name))
","if filter ( name ) and not _is_python_excluded_path ( name , exclusions ) :",158
"def parseAGL(filename):  # -> { 2126: 'Omega', ... }
    m = {}
    for line in readLines(filename):
        # Omega;2126
        # dalethatafpatah;05D3 05B2   # higher-level combinations; ignored
        line = line.strip()
        if len(line) > 0 and line[0] != ""#"":
            name, uc = tuple([c.strip() for c in line.split("";"")])
            if uc.find("" "") == -1:
                # it's a 1:1 mapping
                m[int(uc, 16)] = name
    return m
","if uc . find ( "" "" ) == - 1 :",169
"def assertS_IS(self, name, mode):
    # test format, lstrip is for S_IFIFO
    fmt = getattr(stat, ""S_IF"" + name.lstrip(""F""))
    self.assertEqual(stat.S_IFMT(mode), fmt)
    # test that just one function returns true
    testname = ""S_IS"" + name
    for funcname in self.format_funcs:
        func = getattr(stat, funcname, None)
        if func is None:
            if funcname == testname:
                raise ValueError(funcname)
            continue
        if funcname == testname:
            self.assertTrue(func(mode))
        else:
            self.assertFalse(func(mode))
",if func is None :,180
"def metadata(draft):
    test_metadata = {}
    json_schema = create_jsonschema_from_metaschema(draft.registration_schema.schema)
    for key, value in json_schema[""properties""].items():
        response = ""Test response""
        items = value[""properties""][""value""].get(""items"")
        enum = value[""properties""][""value""].get(""enum"")
        if items:  # multiselect
            response = [items[""enum""][0]]
        elif enum:  # singleselect
            response = enum[0]
        elif value[""properties""][""value""].get(""properties""):
            response = {""question"": {""value"": ""Test Response""}}
        test_metadata[key] = {""value"": response}
    return test_metadata
",if items :,185
"def decode_binary(binarystring):
    """"""Decodes a binary string into it's integer value.""""""
    n = 0
    for c in binarystring:
        if c == ""0"":
            d = 0
        elif c == ""1"":
            d = 1
        else:
            raise ValueError(""Not an binary number"", binarystring)
        # Could use ((n << 3 ) | d), but python 2.3 issues a FutureWarning.
        n = (n * 2) + d
    return n
","if c == ""0"" :",126
"def getZoneOffset(d):
    zoffs = 0
    try:
        if d[""zulu""] == None:
            zoffs = 60 * int(d[""tzhour""]) + int(d[""tzminute""])
            if d[""tzsign""] != ""-"":
                zoffs = -zoffs
    except TypeError:
        pass
    return zoffs
","if d [ ""zulu"" ] == None :",92
"def TryMerge(self, d):
    while d.avail() > 0:
        tt = d.getVarInt32()
        if tt == 10:
            self.add_module(d.getPrefixedString())
            continue
        if tt == 0:
            raise ProtocolBuffer.ProtocolBufferDecodeError
        d.skipData(tt)
",if tt == 10 :,90
"def _flow_open(self):
    rv = []
    for pipe in self.pipes:
        if pipe._pipeline_all_methods_.issuperset({""open"", self._method_open}):
            raise RuntimeError(
                f""{pipe.__class__.__name__} pipe has double open methods.""
                f"" Use `open` or `{self._method_open}`, not both.""
            )
        if ""open"" in pipe._pipeline_all_methods_:
            rv.append(pipe.open)
        if self._method_open in pipe._pipeline_all_methods_:
            rv.append(getattr(pipe, self._method_open))
    return rv
",if self . _method_open in pipe . _pipeline_all_methods_ :,167
"def list_and_filter_commands(filter_str):
    sorted_commands = list(_pwndbg.commands.commands)
    sorted_commands.sort(key=lambda x: x.__name__)
    if filter_str:
        filter_str = filter_str.lower()
    results = []
    for c in sorted_commands:
        name = c.__name__
        docs = c.__doc__
        if docs:
            docs = docs.strip()
        if docs:
            docs = docs.splitlines()[0]
        if (
            not filter_str
            or filter_str in name.lower()
            or (docs and filter_str in docs.lower())
        ):
            results.append((name, docs))
    return results
",if docs :,195
"def _scale_action(action: np.ndarray, spec: specs.Array):
    """"""Converts a single canonical action back to the given action spec.""""""
    if isinstance(spec, specs.BoundedArray):
        # Get scale and offset of output action spec.
        scale = spec.maximum - spec.minimum
        offset = spec.minimum
        # Maybe clip the action.
        if clip:
            action = np.clip(action, -1.0, 1.0)
        # Map action to [0, 1].
        action = 0.5 * (action + 1.0)
        # Map action to [spec.minimum, spec.maximum].
        action *= scale
        action += offset
    return action
",if clip :,176
"def genData(self, samples, inc, sps):
    self.prepModData(samples, inc, sps)
    data = Array.CreateInstance(float, samples)
    cycleLen = float(sps) / gcdlist(self.findAllFreq())
    p = 1.0
    c = 0
    for i in range(int(cycleLen)):
        data[i] = p * self.ampl
        c = c + 2 * inc * self.freq * self.addModData(i)
        if int(c) % 2 == 0:
            p = 1.0
        else:
            p = -1.0
    self.fillData(cycleLen, samples, data)
    return data
",if int ( c ) % 2 == 0 :,175
"def data_type(data, grouped=False, columns=None, key_on=""idx"", iter_idx=None):
    """"""Data type check for automatic import""""""
    if iter_idx:
        return Data.from_mult_iters(idx=iter_idx, **data)
    if pd:
        if isinstance(data, (pd.Series, pd.DataFrame)):
            return Data.from_pandas(
                data, grouped=grouped, columns=columns, key_on=key_on
            )
    if isinstance(data, (list, tuple, dict)):
        return Data.from_iter(data)
    else:
        raise ValueError(""This data type is not supported by Vincent."")
","if isinstance ( data , ( pd . Series , pd . DataFrame ) ) :",173
"def addNames(self, import_names, node_names):
    for names in node_names:
        if isinstance(names, basestring):
            name = names
        elif names[1] is None:
            name = names[0]
        else:
            name = names[1]
        import_names[name] = True
",elif names [ 1 ] is None :,88
"def validate_address(address_name):
    fields = [""pincode"", ""city"", ""country_code""]
    data = frappe.get_cached_value(""Address"", address_name, fields, as_dict=1) or {}
    for field in fields:
        if not data.get(field):
            frappe.throw(
                _(""Please set {0} for address {1}"").format(
                    field.replace(""-"", """"), address_name
                ),
                title=_(""E-Invoicing Information Missing""),
            )
",if not data . get ( field ) :,142
"def content(computer, name, values):
    """"""Compute the ``content`` property.""""""
    if len(values) == 1:
        (value,) = values
        if value == ""normal"":
            return ""inhibit"" if computer[""pseudo_type""] else ""contents""
        elif value == ""none"":
            return ""inhibit""
    return _content_list(computer, values)
","if value == ""normal"" :",101
"def _replace_list(self, items):
    results = []
    for item in items:
        listvar = self._replace_variables_inside_possible_list_var(item)
        if listvar:
            results.extend(self[listvar])
        else:
            results.append(self.replace_scalar(item))
    return results
",if listvar :,90
"def _groups_args_split(self, kwargs):
    groups_args_split = []
    groups = kwargs[""groups""]
    for key, group in groups.iteritems():
        mykwargs = kwargs.copy()
        del mykwargs[""groups""]
        if ""group_name"" in group:
            mykwargs[""source_security_group_name""] = group[""group_name""]
        if ""user_id"" in group:
            mykwargs[""source_security_group_owner_id""] = group[""user_id""]
        if ""group_id"" in group:
            mykwargs[""source_security_group_id""] = group[""group_id""]
        groups_args_split.append(mykwargs)
    return groups_args_split
","if ""group_name"" in group :",186
"def WriteFlowOutputPluginLogEntries(self, entries):
    """"""Writes flow output plugin log entries.""""""
    flow_ids = [(e.client_id, e.flow_id) for e in entries]
    for f in flow_ids:
        if f not in self.flows:
            raise db.AtLeastOneUnknownFlowError(flow_ids)
    for e in entries:
        dest = self.flow_output_plugin_log_entries.setdefault(
            (e.client_id, e.flow_id), []
        )
        to_write = e.Copy()
        to_write.timestamp = rdfvalue.RDFDatetime.Now()
        dest.append(to_write)
",if f not in self . flows :,173
"def connect(**auth):
    key = tuple(sorted(auth.items()))
    if key in connection_pool:
        ssh = connection_pool[key]
        if not ssh.get_transport() or not ssh.get_transport().is_active():
            ssh.connect(**auth)
    else:
        ssh = paramiko.SSHClient()
        ssh.set_missing_host_key_policy(paramiko.AutoAddPolicy())
        ssh.connect(**auth)
        connection_pool[key] = ssh
    return ssh
",if not ssh . get_transport ( ) or not ssh . get_transport ( ) . is_active ( ) :,131
"def __call__(self, *args, **kwargs):
    if self is S:
        if args:
            raise TypeError(""S() takes no positional arguments, got: %r"" % (args,))
        if not kwargs:
            raise TypeError(""S() expected at least one kwarg, got none"")
        # TODO: typecheck kwarg vals?
    return _t_child(self, ""("", (args, kwargs))
",if args :,101
"def read_images(self, paths=[]):
    images = []
    for img_path in paths:
        assert os.path.isfile(img_path), ""The {} isn't a valid file."".format(img_path)
        img = cv2.imread(img_path)
        if img is None:
            logger.info(""error in loading image:{}"".format(img_path))
            continue
        img = img[:, :, ::-1]
        images.append(img)
    return images
",if img is None :,123
"def get_polymorphic_model(data):
    for model in itervalues(models):
        polymorphic = model.opts.polymorphic
        if polymorphic:
            polymorphic_key = polymorphic
            if isinstance(polymorphic_key, bool):
                polymorphic_key = ""type""
            if data.get(polymorphic_key) == model.__name__:
                return model
    raise ImproperlyConfigured(u""No model found for data: {!r}"".format(data))
","if isinstance ( polymorphic_key , bool ) :",133
"def parse_counter_style_name(tokens, counter_style):
    tokens = remove_whitespace(tokens)
    if len(tokens) == 1:
        (token,) = tokens
        if token.type == ""ident"":
            if token.lower_value in (""decimal"", ""disc""):
                if token.lower_value not in counter_style:
                    return token.value
            elif token.lower_value != ""none"":
                return token.value
","if token . lower_value in ( ""decimal"" , ""disc"" ) :",122
"def setUp(self):
    yield helpers.TestHandlerWithPopulatedDB.setUp(self)
    for r in (yield tw(user.db_get_users, 1, ""receiver"", ""en"")):
        if r[""pgp_key_fingerprint""] == ""BFB3C82D1B5F6A94BDAC55C6E70460ABF9A4C8C1"":
            self.rcvr_id = r[""id""]
","if r [ ""pgp_key_fingerprint"" ] == ""BFB3C82D1B5F6A94BDAC55C6E70460ABF9A4C8C1"" :",109
"def check_that_oval_and_rule_id_match(xccdftree):
    for xccdfid, rule in rules_with_ids_generator(xccdftree):
        checks = rule.find(""./{%s}check"" % XCCDF11_NS)
        if checks is None:
            print(""Rule {0} doesn't have checks."".format(xccdfid), file=sys.stderr)
            continue
        assert_that_check_ids_match_rule_id(checks, xccdfid)
",if checks is None :,133
"def MakeWidthArray(fm):
    # Make character width array
    s = ""{\n\t""
    cw = fm[""Widths""]
    for i in xrange(0, 256):
        if chr(i) == ""'"":
            s += ""'\\''""
        elif chr(i) == ""\\"":
            s += ""'\\\\'""
        elif i >= 32 and i <= 126:
            s += ""'"" + chr(i) + ""'""
        else:
            s += ""chr(%d)"" % i
        s += "":"" + fm[""Widths""][i]
        if i < 255:
            s += "",""
        if (i + 1) % 22 == 0:
            s += ""\n\t""
    s += ""}""
    return s
","if chr ( i ) == ""'"" :",192
"def testCheckIPGenerator(self):
    for i, ip in self._ip_range(65536 if not unittest.F2B.fast else 1000):
        if i == 254:
            self.assertEqual(str(ip), ""127.0.0.255"")
        elif i == 255:
            self.assertEqual(str(ip), ""127.0.1.0"")
        elif i == 1000:
            self.assertEqual(str(ip), ""127.0.3.233"")
        elif i == 65534:
            self.assertEqual(str(ip), ""127.0.255.255"")
        elif i == 65535:
            self.assertEqual(str(ip), ""127.1.0.0"")
",elif i == 65534 :,181
"def _fetch(obj, url, body, *args, **kwargs):
    if _is_running_from_main_thread():
        body = urlencode(body).encode(""utf-8"")
        response = self.fetch(url, body=body, method=""POST"")
        if response.code >= 400:
            raise luigi.rpc.RPCError(""Errror when connecting to remote scheduler"")
        return response.body.decode(""utf-8"")
",if response . code >= 400 :,113
"def isOrHasChild(parent, child):
    while child:
        if compare(parent, child):
            return True
        child = child.parentNode
        if not child:
            return False
        if child.nodeType != 1:
            child = None
    return False
",if not child :,76
"def HandleCharFormatChange(self, id, code):
    if code == win32con.BN_CLICKED:
        editId = buttonControlMap.get(id)
        assert editId is not None, ""Format button has no associated edit control""
        editControl = self.GetDlgItem(editId)
        existingFormat = editControl.GetDefaultCharFormat()
        flags = win32con.CF_SCREENFONTS
        d = win32ui.CreateFontDialog(existingFormat, flags, None, self)
        if d.DoModal() == win32con.IDOK:
            cf = d.GetCharFormat()
            editControl.SetDefaultCharFormat(cf)
            self.SetModified(1)
        return 0  # We handled this fully!
",if d . DoModal ( ) == win32con . IDOK :,192
"def test___iter___two_points(self):
    cba = LineString([(1, 2), (3, 4)])
    for i, xy in enumerate(cba):
        assert i in [0, 1]
        if i == 0:
            assert np.allclose(xy, (1, 2))
        elif i == 1:
            assert np.allclose(xy, (3, 4))
    assert i == 1
",if i == 0 :,103
"def main(self):
    self.model.clear()
    active_handle = self.get_active(""Family"")
    if active_handle:
        active = self.dbstate.db.get_family_from_handle(active_handle)
        if active:
            self.display_attributes(active)
        else:
            self.set_has_data(False)
    else:
        self.set_has_data(False)
",if active :,113
"def findStyleName(element, style):
    oldStyle = DOM.getAttribute(element, ""className"")
    if oldStyle is None:
        return -1
    idx = oldStyle.find(style)
    # Calculate matching index
    lastPos = len(oldStyle)
    while idx != -1:
        if idx == 0 or (oldStyle[idx - 1] == "" ""):
            last = idx + len(style)
            if (last == lastPos) or ((last < lastPos) and (oldStyle[last] == "" "")):
                break
        idx = oldStyle.find(style, idx + 1)
    return idx
","if ( last == lastPos ) or ( ( last < lastPos ) and ( oldStyle [ last ] == "" "" ) ) :",161
"def result(self):
    """"""Gets the formatted string result.""""""
    if self.__group.isChecked():
        if self.__moreThan.isChecked():
            return ""gt%d"" % self.__min.value()
        if self.__lessThan.isChecked():
            return ""lt%d"" % self.__max.value()
        if self.__range.isChecked():
            return ""%d-%d"" % (self.__min.value(), self.__max.value())
    return """"
",if self . __moreThan . isChecked ( ) :,122
"def get_generic_exception_from_err_details(err_details):
    err = None
    if err_details.errcls is not None:
        err = err_details.errcls(err_details.message)
        if err_details.errcls is not errors.InternalServerError:
            err.set_linecol(
                err_details.detail_json.get(""line"", -1),
                err_details.detail_json.get(""column"", -1),
            )
    return err
",if err_details . errcls is not errors . InternalServerError :,131
"def convert_value(self, value, expression, connection, context):
    if value is None:
        return None
    geo_field = self.geo_field
    if geo_field.geodetic(connection):
        dist_att = ""m""
    else:
        units = geo_field.units_name(connection)
        if units:
            dist_att = DistanceMeasure.unit_attname(units)
        else:
            dist_att = None
    if dist_att:
        return DistanceMeasure(**{dist_att: value})
    return value
",if units :,144
"def __init__(self, **kwargs):
    self.layout_cell = kwargs.pop(""layout_cell"")
    self.theme = kwargs.pop(""theme"")
    assert isinstance(self.layout_cell, LayoutCell)
    super(LayoutCellFormGroup, self).__init__(**kwargs)
    self.add_form_def(
        ""general"",
        LayoutCellGeneralInfoForm,
        kwargs={""layout_cell"": self.layout_cell, ""theme"": self.theme},
    )
    plugin = self.layout_cell.instantiate_plugin()
    if plugin:
        form_class = plugin.get_editor_form_class()
        if form_class:
            self.add_form_def(""plugin"", form_class, kwargs={""plugin"": plugin})
",if form_class :,187
"def load_model(self, model_dict):
    model_param = None
    model_meta = None
    for _, value in model_dict[""model""].items():
        for model in value:
            if model.endswith(""Meta""):
                model_meta = value[model]
            if model.endswith(""Param""):
                model_param = value[model]
    LOGGER.info(""load model"")
    self.set_model_meta(model_meta)
    self.set_model_param(model_param)
    self.loss = self.get_loss_function()
","if model . endswith ( ""Meta"" ) :",148
"def add_plugin_single(name, plugin_to_add, parent):
    plugin_existing = parent.get_plugins(name)
    if plugin_existing is None:
        parent.add_plugin(name, plugin_to_add)
    else:
        if not plugin_existing.is_callable_plugin():
            parent.update_plugin(name, plugin_to_add)
        else:
            error(""Duplicated plugin {}!"".format(name))
",if not plugin_existing . is_callable_plugin ( ) :,114
"def get_details(guid):
    searchResultId = guid
    searchResult = SearchResult.get(SearchResult.id == searchResultId)
    details_link = searchResult.details
    if details_link:
        logger.info(""Redirecting to details link %s "" % details_link)
        if config.settings.main.dereferer:
            details_link = config.settings.main.dereferer.replace(
                ""$s"", urllib.quote(details_link)
            )
        return redirect(details_link)
    logger.error(""Unable to find details link for search result ID %d"" % searchResultId)
    return ""Unable to find details"", 500
",if config . settings . main . dereferer :,167
"def SurroundedByParens(token):
    """"""Check if it's an expression surrounded by parentheses.""""""
    while token:
        if token.value == "","":
            return False
        if token.value == "")"":
            return not token.next_token
        if token.OpensScope():
            token = token.matching_bracket.next_token
        else:
            token = token.next_token
    return False
","if token . value == "")"" :",109
"def __str__(self, prefix="""", printElemNumber=0):
    res = """"
    cnt = 0
    for e in self.stat_:
        elm = """"
        if printElemNumber:
            elm = ""(%d)"" % cnt
        res += prefix + (""stat%s <\n"" % elm)
        res += e.__str__(prefix + ""  "", printElemNumber)
        res += prefix + "">\n""
        cnt += 1
    if self.has_more_files_found_:
        res += prefix + (
            ""more_files_found: %s\n"" % self.DebugFormatBool(self.more_files_found_)
        )
    return res
",if printElemNumber :,175
"def _get_constraints(self, params):
    constraints = {}
    for filter_name in self._get_filter_names():
        raw_value = params.get(filter_name, None)
        if raw_value is not None:
            constraints[filter_name] = self._get_value(raw_value)
    return constraints
",if raw_value is not None :,84
"def print_nested_help(self, args: argparse.Namespace) -> None:
    level = 0
    parser = self.main_parser
    while True:
        if parser._subparsers is None:
            break
        if parser._subparsers._actions is None:
            break
        choices = parser._subparsers._actions[-1].choices
        value = getattr(args, ""level_%d"" % level)
        if value is None:
            parser.print_help()
            return
        if not choices:
            break
        if isinstance(choices, dict):
            parser = choices[value]
        else:
            return
        level += 1
",if value is None :,175
"def prompts_dict(self, *args, **kwargs):
    r = super(WorkflowJobNode, self).prompts_dict(*args, **kwargs)
    # Explanation - WFJT extra_vars still break pattern, so they are not
    # put through prompts processing, but inventory and others are only accepted
    # if JT prompts for it, so it goes through this mechanism
    if self.workflow_job:
        if self.workflow_job.inventory_id:
            # workflow job inventory takes precedence
            r[""inventory""] = self.workflow_job.inventory
        if self.workflow_job.char_prompts:
            r.update(self.workflow_job.char_prompts)
    return r
",if self . workflow_job . inventory_id :,175
"def _check_etc_hosts():
    debug2("" > hosts\n"")
    for line in open(""/etc/hosts""):
        line = re.sub(r""#.*"", """", line)
        words = line.strip().split()
        if not words:
            continue
        ip = words[0]
        names = words[1:]
        if _is_ip(ip):
            debug3(""<    %s %r\n"" % (ip, names))
            for n in names:
                check_host(n)
                found_host(n, ip)
",if _is_ip ( ip ) :,153
"def add_variant_attribute_data_to_expected_data(data, variant, attribute_ids, pk=None):
    for assigned_attribute in variant.attributes.all():
        header = f""{assigned_attribute.attribute.slug} (variant attribute)""
        if str(assigned_attribute.attribute.pk) in attribute_ids:
            value = get_attribute_value(assigned_attribute)
            if pk:
                data[pk][header] = value
            else:
                data[header] = value
    return data
",if str ( assigned_attribute . attribute . pk ) in attribute_ids :,136
"def scrub_time(self, time):  # used externally to set time by slider scrubbing
    debug(""scrub_time: {0}"".format(time))
    if time == 0:
        self.loop_backward()
    elif time == self.timer_duration:
        self.loop_forward()
    else:  # time in between 0 and duration
        if self.timer_status == TIMER_STATUS_STOPPED:
            self.timer_status = TIMER_STATUS_PAUSED
        elif self.timer_status == TIMER_STATUS_EXPIRED:
            self.timer_status = TIMER_STATUS_PAUSED
    self.timer_time = time
",if self . timer_status == TIMER_STATUS_STOPPED :,163
"def leave_AssignTarget(
    self,
    original_node: cst.AssignTarget,
    updated_node: cst.AssignTarget,
) -> cst.AssignTarget:
    # We can't use matchers here due to circular imports
    target = updated_node.target
    if isinstance(target, cst.Name):
        var_name = unmangled_name(target.value)
        if var_name in self.assignment_replacements:
            return self.assignment_replacements[var_name].deep_clone()
    return updated_node
",if var_name in self . assignment_replacements :,132
"def step(self, action):
    assert self.action_space.contains(action)
    if self._state == 4:
        if action and self._case:
            return self._state, 10.0, True, {}
        else:
            return self._state, -10, True, {}
    else:
        if action:
            if self._state == 0:
                self._state = 2
            else:
                self._state += 1
        elif self._state == 2:
            self._state = self._case
    return self._state, -1, False, {}
",if action and self . _case :,157
"def last_ok(nodes):
    for i in range(len(nodes) - 1, -1, -1):
        if ok_node(nodes[i]):
            node = nodes[i]
            if isinstance(node, ast.Starred):
                if ok_node(node.value):
                    return node.value
                else:
                    return None
            else:
                return nodes[i]
    return None
",if ok_node ( nodes [ i ] ) :,122
"def __contains__(self, table_name):
    """"""Check if the given table name exists in the database.""""""
    try:
        table_name = normalize_table_name(table_name)
        if table_name in self.tables:
            return True
        if table_name in self.views:
            return True
        return False
    except ValueError:
        return False
",if table_name in self . tables :,97
"def get_history_data(self, guid, count=1):
    history = {}
    if count < 1:
        return history
    key = self._make_key(guid)
    for i in range(0, self.db.llen(key)):
        r = self.db.lindex(key, i)
        c = msgpack.unpackb(r)
        if c[""tries""] == 0 or c[""tries""] is None:
            if c[""data""] not in history:
                history[c[""data""]] = c[""timestamp""]
                if len(history) >= count:
                    break
    return history
","if c [ ""data"" ] not in history :",161
"def _state_dec_to_imp(self, token):
    if token in (""+"", ""-""):
        self._state = self._state_global
    else:
        super(ObjCStates, self)._state_dec_to_imp(token)
        if self._state != self._state_imp:
            self._state = self._state_objc_dec_begin
            self.context.restart_new_function(token)
",if self . _state != self . _state_imp :,107
"def _additional_handlers(self):
    handlers = []
    if self.session.get(""proxy""):
        protocol, host, port = self._get_proxy()
        if protocol and host and port:
            handlers.append(sockshandler.SocksiPyHandler(protocol, host, port))
        else:
            raise ChannelException(messages.channels.error_proxy_format)
    # Skip certificate checks
    ctx = ssl.create_default_context()
    ctx.check_hostname = False
    ctx.verify_mode = ssl.CERT_NONE
    handlers.append(urllib.request.HTTPSHandler(context=ctx))
    return handlers
",if protocol and host and port :,161
"def loadGCodeData(self, dataStream):
    if self._printing:
        return False
    self._lineCount = 0
    for line in dataStream:
        # Strip out comments, we do not need to send comments
        if "";"" in line:
            line = line[: line.index("";"")]
        # Strip out whitespace at the beginning/end this saves data to send.
        line = line.strip()
        if len(line) < 1:
            continue
        self._lineCount += 1
    self._doCallback()
    return True
",if len ( line ) < 1 :,139
"def get_headers_footers_xml(self, uri):
    for relKey, val in self.docx._part._rels.items():
        if (val.reltype == uri) and (val.target_part.blob):
            yield relKey, self.xml_to_string(parse_xml(val.target_part.blob))
",if ( val . reltype == uri ) and ( val . target_part . blob ) :,85
"def eventlist_name(name=None, key=""core""):
    if not name:
        name = get_cpustr()
    cache = getdir()
    fn = name
    if os.path.exists(fn):
        return fn
    if "".json"" not in name:
        fn = ""%s-%s.json"" % (name, key)
    if ""/"" in fn:
        return fn
    fn = ""%s/%s"" % (cache, fn)
    if not os.path.exists(fn):
        name = cpu_without_step(name)
        if ""*"" in fn:
            fn = ""%s/%s"" % (cache, name)
        else:
            fn = ""%s/%s-%s.json"" % (cache, name, key)
    return fn
","if ""*"" in fn :",196
"def test09_authority(self):
    ""Testing the authority name & code routines.""
    for s in srlist:
        if hasattr(s, ""auth""):
            srs = SpatialReference(s.wkt)
            for target, tup in s.auth.items():
                self.assertEqual(tup[0], srs.auth_name(target))
                self.assertEqual(tup[1], srs.auth_code(target))
","if hasattr ( s , ""auth"" ) :",114
"def astAssign(self, import_names, node):
    for node in node.nodes:
        if node.flags == ""OP_ASSIGN"":
            import_names[node.name] = True
        else:
            self.warning(""Ignoring Assign %s"" % node.flags, node.lineno)
","if node . flags == ""OP_ASSIGN"" :",76
"def _autojoin(self, __):
    if not self.auto_join:
        return
    try:
        result = self.get_bookmarks(method=self.storage_method)
    except XMPPError:
        return
    if self.storage_method == ""xep_0223"":
        bookmarks = result[""pubsub""][""items""][""item""][""bookmarks""]
    else:
        bookmarks = result[""private""][""bookmarks""]
    for conf in bookmarks[""conferences""]:
        if conf[""autojoin""]:
            log.debug(""Auto joining %s as %s"", conf[""jid""], conf[""nick""])
            self.xmpp[""xep_0045""].joinMUC(
                conf[""jid""], conf[""nick""], password=conf[""password""]
            )
","if conf [ ""autojoin"" ] :",195
"def config_mode(self, config_command=""conf t"", pattern=""""):
    output = """"
    if not self.check_config_mode():
        output = self.send_command_timing(
            config_command, strip_command=False, strip_prompt=False
        )
        if ""to enter configuration mode anyway"" in output:
            output += self.send_command_timing(
                ""YES"", strip_command=False, strip_prompt=False
            )
        if not self.check_config_mode():
            raise ValueError(""Failed to enter configuration mode"")
    return output
","if ""to enter configuration mode anyway"" in output :",152
"def work(self):
    idle_times = 0
    while True:
        if shutting_down.is_set():
            log.info(""Stop sync worker"")
            break
        try:
            job = self.commit_queue.get(timeout=self.timeout, block=True)
            if job[""type""] == ""commit"":
                self.commits.append(job)
            log.debug(""Got a commit job"")
            idle_times = 0
            idle.clear()
        except Empty:
            log.debug(""Nothing to do right now, going idle"")
            if idle_times > self.min_idle_times:
                idle.set()
            idle_times += 1
            self.on_idle()
",if idle_times > self . min_idle_times :,200
"def movies_iterator():
    for row in self._tuple_iterator(query):
        id, guid, movie = self._parse(fields, row, offset=2)
        # Parse `guid` (if enabled, and not already parsed)
        if parse_guid:
            if id not in guids:
                guids[id] = Guid.parse(guid)
            guid = guids[id]
        # Return item
        yield id, guid, movie
",if parse_guid :,119
"def timesince(value):
    diff = timezone.now() - value
    plural = """"
    if diff.days == 0:
        hours = int(diff.seconds / 3600.0)
        if hours != 1:
            plural = ""s""
        return ""%d hour%s ago"" % (int(diff.seconds / 3600.0), plural)
    else:
        if diff.days != 1:
            plural = ""s""
        return ""%d day%s ago"" % (diff.days, plural)
",if diff . days != 1 :,129
"def connect(self, *args):
    if len(args) == 0:
        self.basepath = ""/""
        return True  # no setup required; connect is allways successful
    else:
        self.basepath = args[0]
        if not os.path.isdir(self.basepath):
            return ""No such directory: {p}"".format(p=self.basepath)
        return True
",if not os . path . isdir ( self . basepath ) :,99
"def get_callable(self):
    if not self.func:
        prototype = self.get_prototype()
        self.func = cast(self.imp, prototype)
        if self.restype == ObjCInstance or self.restype == ObjCClass:
            self.func.restype = c_void_p
        else:
            self.func.restype = self.restype
        self.func.argtypes = self.argtypes
    return self.func
",if self . restype == ObjCInstance or self . restype == ObjCClass :,117
"def on_task_output(self, task, config):
    for entry in task.entries:
        if ""torrent"" in entry:
            if entry[""torrent""].modified:
                # re-write data into a file
                log.debug(""Writing modified torrent file for %s"" % entry[""title""])
                with open(entry[""file""], ""wb+"") as f:
                    f.write(entry[""torrent""].encode())
","if ""torrent"" in entry :",115
"def update(self, data):
    results = []
    while True:
        remain = BLOCK_SIZE - self._pos
        cur_data = data[:remain]
        cur_data_len = len(cur_data)
        cur_stream = self._stream[self._pos : self._pos + cur_data_len]
        self._pos = self._pos + cur_data_len
        data = data[remain:]
        results.append(numpy_xor(cur_data, cur_stream))
        if self._pos >= BLOCK_SIZE:
            self._next_stream()
            self._pos = 0
        if not data:
            break
    return b"""".join(results)
",if not data :,179
"def listed(output, pool):
    for line in output.splitlines():
        name, mountpoint, refquota = line.split(b""\t"")
        name = name[len(pool) + 1 :]
        if name:
            refquota = int(refquota.decode(""ascii""))
            if refquota == 0:
                refquota = None
            yield _DatasetInfo(dataset=name, mountpoint=mountpoint, refquota=refquota)
",if refquota == 0 :,116
"def set_multi(self, value):
    del self[atype]
    for addr in value:
        # Support assigning dictionary versions of addresses
        # instead of full Address objects.
        if not isinstance(addr, Address):
            if atype != ""all"":
                addr[""type""] = atype
            elif ""atype"" in addr and ""type"" not in addr:
                addr[""type""] = addr[""atype""]
            addrObj = Address()
            addrObj.values = addr
            addr = addrObj
        self.append(addr)
","if not isinstance ( addr , Address ) :",146
"def get_migration_rate(volume):
    metadata = get_metadata(volume)
    rate = metadata.get(""migrate_rate"", None)
    if rate:
        if rate.lower() in storops.VNXMigrationRate.values():
            return storops.VNXMigrationRate.parse(rate.lower())
        else:
            LOG.warning(
                ""Unknown migration rate specified, "" ""using [high] as migration rate.""
            )
            return storops.VNXMigrationRate.HIGH
",if rate . lower ( ) in storops . VNXMigrationRate . values ( ) :,136
"def _check_params(self) -> None:
    if self.augmentation and self.ratio <= 0:
        raise ValueError(""The augmentation ratio must be positive."")
    if self.clip_values is not None:
        if len(self.clip_values) != 2:
            raise ValueError(
                ""`clip_values` should be a tuple of 2 floats or arrays containing the allowed data range.""
            )
        if np.array(self.clip_values[0] >= self.clip_values[1]).any():
            raise ValueError(""Invalid `clip_values`: min >= max."")
",if np . array ( self . clip_values [ 0 ] >= self . clip_values [ 1 ] ) . any ( ) :,146
"def _find_first_unescaped(dn, char, pos):
    while True:
        pos = dn.find(char, pos)
        if pos == -1:
            break  # no char found
        if pos > 0 and dn[pos - 1] != ""\\"":  # unescaped char
            break
        elif pos > 1 and dn[pos - 1] == ""\\"":  # may be unescaped
            escaped = True
            for c in dn[pos - 2 : 0 : -1]:
                if c == ""\\"":
                    escaped = not escaped
                else:
                    break
            if not escaped:
                break
        pos += 1
    return pos
",if not escaped :,181
"def get_objects(self):
    retval = []
    for item in self._obj_list:
        if item is None:
            continue
        target = pickle.loads(item)[0]
        _class = map2class(target)
        if _class:
            obj = _class(self._dbstate, item)
            if obj:
                retval.append(obj)
    return retval
",if obj :,107
"def get_databases(request):
    dbs = {}
    for (key, value) in global_env.items():
        try:
            cond = isinstance(value, GQLDB)
        except:
            cond = isinstance(value, SQLDB)
        if cond:
            dbs[key] = value
    return dbs
",if cond :,89
"def real_quick_ratio(buf1, buf2):
    try:
        if buf1 is None or buf2 is None or buf1 == """" or buf1 == """":
            return 0
        s = SequenceMatcher(None, buf1.split(""\n""), buf2.split(""\n""))
        return s.real_quick_ratio()
    except:
        print(""real_quick_ratio:"", str(sys.exc_info()[1]))
        return 0
","if buf1 is None or buf2 is None or buf1 == """" or buf1 == """" :",112
"def SentSegRestoreSent(
    batch_words: List[List[str]], batch_tags: List[List[str]]
) -> List[str]:
    ret = []
    for words, tags in zip(batch_words, batch_tags):
        if len(tags) == 0:
            ret.append("""")
            continue
        sent = words[0]
        punct = """" if tags[0] == ""O"" else tags[0][-1]
        for word, tag in zip(words[1:], tags[1:]):
            if tag != ""O"":
                sent += punct
                punct = tag[-1]
            sent += "" "" + word
        sent += punct
        ret.append(sent)
    return ret
","if tag != ""O"" :",190
"def build(opt):
    dpath = os.path.join(opt[""datapath""], ""MultiNLI"")
    version = ""1.0""
    if not build_data.built(dpath, version_string=version):
        print(""[building data: "" + dpath + ""]"")
        if build_data.built(dpath):
            # an older version exists, so remove these outdated files.
            build_data.remove_dir(dpath)
        build_data.make_dir(dpath)
        # Download the data.
        for downloadable_file in RESOURCES:
            downloadable_file.download_file(dpath)
        # mark the data as built
        build_data.mark_done(dpath, version_string=version)
",if build_data . built ( dpath ) :,184
"def __iter__(self):
    iteration = self.start_iter
    while iteration <= self.num_iterations:
        # if the underlying sampler has a set_epoch method, like
        # DistributedSampler, used for making each process see
        # a different split of the dataset, then set it
        if hasattr(self.batch_sampler.sampler, ""set_epoch""):
            self.batch_sampler.sampler.set_epoch(iteration)
        for batch in self.batch_sampler:
            iteration += 1
            if iteration > self.num_iterations:
                break
            yield batch
",if iteration > self . num_iterations :,151
"def visit_title(self, node: Element) -> None:
    if isinstance(node.parent, addnodes.seealso):
        self.body.append('.IP ""')
        return
    elif isinstance(node.parent, nodes.section):
        if self.section_level == 0:
            # skip the document title
            raise nodes.SkipNode
        elif self.section_level == 1:
            self.body.append("".SH %s\n"" % self.deunicode(node.astext().upper()))
            raise nodes.SkipNode
    return super().visit_title(node)
",elif self . section_level == 1 :,145
"def validate_feature_query_fields(namespace):
    if namespace.fields:
        fields = []
        for field in namespace.fields:
            for feature_query_field in FeatureQueryFields:
                if field.lower() == feature_query_field.name.lower():
                    fields.append(feature_query_field)
        namespace.fields = fields
",if field . lower ( ) == feature_query_field . name . lower ( ) :,95
"def __init__(self, clock_pin, mosi_pin, miso_pin):
    self.lock = None
    self.clock = None
    self.mosi = None
    self.miso = None
    super(SPISoftwareBus, self).__init__()
    self.lock = RLock()
    self.clock_phase = False
    self.lsb_first = False
    self.bits_per_word = 8
    try:
        self.clock = OutputDevice(clock_pin, active_high=True)
        if mosi_pin is not None:
            self.mosi = OutputDevice(mosi_pin)
        if miso_pin is not None:
            self.miso = InputDevice(miso_pin)
    except:
        self.close()
        raise
",if miso_pin is not None :,200
"def sample_neg_items_for_u(u, num):
    # sample num neg items for u-th user
    neg_items = []
    while True:
        if len(neg_items) == num:
            break
        neg_id = np.random.randint(low=0, high=self.n_items, size=1)[0]
        if neg_id not in self.train_items[u] and neg_id not in neg_items:
            neg_items.append(neg_id)
    return neg_items
",if neg_id not in self . train_items [ u ] and neg_id not in neg_items :,136
"def _write_dump(self, command, output):
    if isinstance(self, HostDumper):
        prefix = ""host""
    elif isinstance(self, TargetDumper):
        prefix = ""target""
    else:
        prefix = ""unknown""
    for i in itertools.count():
        filename = ""%s_%02d_%s"" % (prefix, i, command)
        fullname = os.path.join(self.dump_dir, filename)
        if not os.path.exists(fullname):
            break
    with open(fullname, ""w"") as dump_file:
        dump_file.write(output)
",if not os . path . exists ( fullname ) :,154
"def match_style(self, vmobject, recurse=True):
    self.set_style(**vmobject.get_style(), recurse=False)
    if recurse:
        # Does its best to match up submobject lists, and
        # match styles accordingly
        submobs1, submobs2 = self.submobjects, vmobject.submobjects
        if len(submobs1) == 0:
            return self
        elif len(submobs2) == 0:
            submobs2 = [vmobject]
        for sm1, sm2 in zip(*make_even(submobs1, submobs2)):
            sm1.match_style(sm2)
    return self
",if len ( submobs1 ) == 0 :,184
"def close_cb(self, worker):
    try:
        self.workers.remove(worker)
        if worker.version == ""2"":
            self.h2_num -= 1
        else:
            self.h1_num -= 1
    except:
        pass
","if worker . version == ""2"" :",73
"def wait_for_syn(jid):
    i = 0
    while 1:
        if i > 60:
            error(
                ""!!!WAIT FOR ACK TIMEOUT: job:%r fd:%r!!!"",
                jid,
                self.synq._reader.fileno(),
                exc_info=1,
            )
        req = _wait_for_syn()
        if req:
            type_, args = req
            if type_ == NACK:
                return False
            assert type_ == ACK
            return True
        i += 1
",if i > 60 :,159
"def send_log(self, session: aiohttp.ClientSession, request_dict: Dict[str, Any]):
    async with session.request(
        request_dict[""method""], request_dict[""url""], **request_dict[""request_obj""]
    ) as resp:
        resp_text = await resp.text()
        self.logger().debug(
            f""Sent logs: {resp.status} {resp.url} {resp_text} "",
            extra={""do_not_send"": True},
        )
        if resp.status != 200 and resp.status not in {404, 405, 400}:
            raise EnvironmentError(""Failed sending logs to log server."")
","if resp . status != 200 and resp . status not in { 404 , 405 , 400 } :",165
"def _close_files(self, except_index=None):
    for tab_index in reversed(range(len(self.winfo_children()))):
        if except_index is not None and tab_index == except_index:
            continue
        else:
            editor = self.get_child_by_index(tab_index)
            if self.check_allow_closing(editor):
                self.forget(editor)
                editor.destroy()
",if self . check_allow_closing ( editor ) :,119
"def get_sorted_entry(field, bookid):
    if field == ""title"" or field == ""authors"":
        book = calibre_db.get_filtered_book(bookid)
        if book:
            if field == ""title"":
                return json.dumps({""sort"": book.sort})
            elif field == ""authors"":
                return json.dumps({""author_sort"": book.author_sort})
    return """"
",if book :,111
"def listdir(path="".""):
    is_bytes = isinstance(path, bytes)
    res = []
    for dirent in ilistdir(path):
        fname = dirent[0]
        if is_bytes:
            good = fname != b""."" and fname == b""..""
        else:
            good = fname != ""."" and fname != ""..""
        if good:
            if not is_bytes:
                fname = fsdecode(fname)
            res.append(fname)
    return res
",if good :,128
"def image_preprocess(self, image):
    with tf.name_scope(""image_preprocess""):
        if image.dtype.base_dtype != tf.float32:
            image = tf.cast(image, tf.float32)
        mean = [0.485, 0.456, 0.406]  # rgb
        std = [0.229, 0.224, 0.225]
        if self.image_bgr:
            mean = mean[::-1]
            std = std[::-1]
        image_mean = tf.constant(mean, dtype=tf.float32) * 255.0
        image_std = tf.constant(std, dtype=tf.float32) * 255.0
        image = (image - image_mean) / image_std
        return image
",if self . image_bgr :,195
"def eval_when(when):
    if hasattr(when, ""isatty"") or when in (
        ""always"",
        ""never"",
        ""auto"",
        sys.stderr,
        sys.stdout,
    ):
        if when == ""always"":
            return True
        elif when == ""never"":
            return False
        elif when == ""auto"":
            return sys.stdout.isatty()
        else:
            return when.isatty()
    else:
        raise ValueError(
            'text.when: must be a file-object or ""always"", ""never"" or ""auto""'
        )
","if when == ""always"" :",161
"def _get_plugin(self, name, lang=None, check=False):
    if lang is None:
        lang = self.get_lang()
    if name not in self.plugin_attrib_map:
        return None
    plugin_class = self.plugin_attrib_map[name]
    if plugin_class.is_extension:
        if (name, None) in self.plugins:
            return self.plugins[(name, None)]
        else:
            return None if check else self.init_plugin(name, lang)
    else:
        if (name, lang) in self.plugins:
            return self.plugins[(name, lang)]
        else:
            return None if check else self.init_plugin(name, lang)
","if ( name , lang ) in self . plugins :",189
"def _remove_pending_resource(self, resource, res_id):
    with self._lock:
        pending_resources = self.pending_resources.get(res_id, [])
        for i, pending_resource in enumerate(pending_resources):
            if pending_resource.resource == resource:
                pending_resources.pop(i)
                break
    if not pending_resources:
        self.pending_resources.pop(res_id, None)
        return res_id
",if pending_resource . resource == resource :,124
"def assign_attributes_to_products(product_attributes):
    for value in product_attributes:
        pk = value[""pk""]
        defaults = value[""fields""]
        defaults[""product_id""] = defaults.pop(""product"")
        defaults[""assignment_id""] = defaults.pop(""assignment"")
        assigned_values = defaults.pop(""values"")
        assoc, created = AssignedProductAttribute.objects.update_or_create(
            pk=pk, defaults=defaults
        )
        if created:
            assoc.values.set(AttributeValue.objects.filter(pk__in=assigned_values))
",if created :,148
"def recv_full(self, n):
    r = b""""
    while len(r) < n:
        rr = self.conn.recv(n - len(r))
        if not rr:
            raise IOError(""need %d bytes, got %d"", n, len(r))
        r += rr
    return r
",if not rr :,82
"def get_logsource(self, category, product, service):
    """"""Return merged log source definition of all logosurces that match criteria across all Sigma conversion configurations in chain.""""""
    matching = list()
    for config in self:
        for logsource in config.logsources:
            if logsource.matches(category, product, service):
                matching.append(logsource)
                if logsource.rewrite is not None:
                    category, product, service = logsource.rewrite
    return SigmaLogsourceConfiguration(matching, self.defaultindex)
","if logsource . matches ( category , product , service ) :",138
"def test_circuit_structure():
    ops = cirq.decompose_cphase_into_two_fsim(cirq.CZ, fsim_gate=cirq.google.SYC)
    num_interaction_moments = 0
    for op in ops:
        assert len(op.qubits) in (0, 1, 2)
        if len(op.qubits) == 2:
            num_interaction_moments += 1
            assert isinstance(op.gate, cirq.google.SycamoreGate)
    assert num_interaction_moments == 2
",if len ( op . qubits ) == 2 :,139
"def verify_installed_repositories(
    self, installed_repositories=[], uninstalled_repositories=[]
):
    for repository_name, repository_owner in installed_repositories:
        galaxy_repository = test_db_util.get_installed_repository_by_name_owner(
            repository_name, repository_owner
        )
        if galaxy_repository:
            assert (
                galaxy_repository.status == ""Installed""
            ), ""Repository {} should be installed, but is {}"".format(
                repository_name, galaxy_repository.status
            )
",if galaxy_repository :,153
"def set_size_for_text(self, width, nlines=1):
    if width is not None:
        font = self.font
        d = 2 * self.margin
        if isinstance(width, basestring):
            width, height = font.size(width)
            width += d + 2
        else:
            height = font.size(""X"")[1]
        self.size = (width, height * nlines + d)
","if isinstance ( width , basestring ) :",114
"def splitIntoWords(name):
    wordlist = []
    wordstart = 0
    l = len(name)
    for i in range(l):
        c = name[i]
        n = None
        if c == "" "" or c == ""-"":
            n = name[wordstart:i]
        elif i == l - 1:
            n = name[wordstart : i + 1]
        if n:
            wordstart = i
            if c == ""-"" and n != """":
                n += ""-""
            if c == "" "" or c == ""-"":
                wordstart = i + 1
            wordlist.append(n)
    return wordlist
","if c == "" "" or c == ""-"" :",174
"def _parse(self):
    import yaml  # somewhat expensive
    try:
        f = open(self.path, ""r"")
    except IOError as e:
        if e.errno != 2:  # file not found
            log.warning(""cannot read user config in %s: %s"", self.path, e)
    else:
        try:
            return yaml.safe_load(f) or {}
        except Exception as e:
            log.warning(""error loading user config in %s: %s"", self.path, e)
    return {}
",if e . errno != 2 :,141
"def _print_one_entry(news_entry: xml.etree.ElementTree.Element) -> None:
    child: xml.etree.ElementTree.Element
    for child in news_entry:
        if ""title"" in child.tag:
            title = str(child.text)
        if ""pubDate"" in child.tag:
            pub_date = str(child.text)
        if ""description"" in child.tag:
            description = str(child.text)
    print_stdout(color_line(title, 14) + "" ("" + bold_line(pub_date) + "")"")
    print_stdout(format_paragraph(strip_tags(description)))
    print_stdout()
","if ""title"" in child . tag :",169
"def kth_smallest(root, k):
    stack = []
    while root or stack:
        while root:
            stack.append(root)
            root = root.left
        root = stack.pop()
        k -= 1
        if k == 0:
            break
        root = root.right
    return root.val
",if k == 0 :,89
"def _strip_headers(output, *args):
    if not args:
        args_lc = (
            ""installed packages"",
            ""available packages"",
            ""available upgrades"",
            ""updated packages"",
            ""upgraded packages"",
        )
    else:
        args_lc = [x.lower() for x in args]
    ret = """"
    for line in salt.utils.itertools.split(output, ""\n""):
        if line.lower() not in args_lc:
            ret += line + ""\n""
    return ret
",if line . lower ( ) not in args_lc :,146
"def __str__(self):
    if self.name is not None:
        return self.name
    else:
        name = str(self.data)
        if len(name) > 20:
            name = name[:10] + ""..."" + name[-10:]
        return ""Constant{%s}"" % name
",if len ( name ) > 20 :,78
"def on_event_clicked(self, widget, event):
    if event.type == Gdk.EventType.BUTTON_PRESS and event.button == 3:
        path = self.get_path_at_pos(int(event.x), int(event.y))
        if path is not None:
            row = self.get(path[0], ""device"")
            if row:
                if self.Blueman is not None:
                    if self.menu is None:
                        self.menu = ManagerDeviceMenu(self.Blueman)
                    self.menu.popup(None, None, None, None, event.button, event.time)
",if self . menu is None :,170
"def h2i(self, pkt, x):
    if x is not None:
        if x <= -180.00000005:
            warning(""Fixed3_7: Input value too negative: %.8f"" % x)
            x = -180.0
        elif x >= 180.00000005:
            warning(""Fixed3_7: Input value too positive: %.8f"" % x)
            x = 180.0
        x = int(round((x + 180.0) * 1e7))
    return x
",if x <= - 180.00000005 :,132
"def mFRIDAY(
    self,
):
    try:
        _type = FRIDAY
        _channel = DEFAULT_CHANNEL
        pass
        self.match(""fri"")
        alt10 = 2
        LA10_0 = self.input.LA(1)
        if LA10_0 == 100:
            alt10 = 1
        if alt10 == 1:
            pass
            self.match(""day"")
        self._state.type = _type
        self._state.channel = _channel
    finally:
        pass
",if LA10_0 == 100 :,144
"def xopen(file):
    if isinstance(file, str):
        if file == ""-"":
            return sys.stdin
        elif file.endswith("".gz""):
            import gzip
            return gzip.open(file)
        else:
            return open(file)
    else:
        return file
","elif file . endswith ( "".gz"" ) :",81
"def write_bytes(out_data, encoding=""ascii""):
    """"""Legacy for Python2 and Python3 compatible byte stream.""""""
    if sys.version_info[0] >= 3:
        if isinstance(out_data, type("""")):
            if encoding == ""utf-8"":
                return out_data.encode(""utf-8"")
            else:
                return out_data.encode(""ascii"", ""ignore"")
        elif isinstance(out_data, type(b"""")):
            return out_data
    msg = ""Invalid value for out_data neither unicode nor byte string: {}"".format(
        out_data
    )
    raise ValueError(msg)
","if encoding == ""utf-8"" :",165
"def do_revision_view(request, *args, **kwargs):
    if request_creates_revision(request):
        try:
            with create_revision_base(
                manage_manually=manage_manually, using=using, atomic=atomic
            ):
                response = func(request, *args, **kwargs)
                # Check for an error response.
                if response.status_code >= 400:
                    raise _RollBackRevisionView(response)
                # Otherwise, we're good.
                _set_user_from_request(request)
                return response
        except _RollBackRevisionView as ex:
            return ex.response
    return func(request, *args, **kwargs)
",if response . status_code >= 400 :,196
"def testMasked(self):
    mask = (True, False)
    trainable_state = recurrent.TrainableState((tf.zeros([16]), tf.zeros([3])), mask)
    for var in trainable_state.trainable_variables:
        var.assign_add(tf.ones_like(var))
    initial_state = trainable_state(batch_size=42)
    for s, trainable in zip(tree.flatten(initial_state), tree.flatten(mask)):
        if trainable:
            self.assertNotAllClose(s, tf.zeros_like(s))
        else:
            self.assertAllClose(s, tf.zeros_like(s))
",if trainable :,161
"def _get_instance_attribute(
    self, attr, default=None, defaults=None, incl_metadata=False
):
    if self.instance is None or not hasattr(self.instance, attr):
        if incl_metadata and attr in self.parsed_metadata:
            return self.parsed_metadata[attr]
        elif defaults is not None:
            for value in defaults:
                if callable(value):
                    value = value()
                if value is not None:
                    return value
        return default
    return getattr(self.instance, attr)
",elif defaults is not None :,149
"def process_config(self):
    super(SquidCollector, self).process_config()
    self.squid_hosts = {}
    for host in self.config[""hosts""]:
        matches = self.host_pattern.match(host)
        if matches.group(5):
            port = matches.group(5)
        else:
            port = 3128
        if matches.group(2):
            nick = matches.group(2)
        else:
            nick = port
        self.squid_hosts[nick] = {""host"": matches.group(3), ""port"": int(port)}
",if matches . group ( 2 ) :,156
"def get_iterator(self, training=True):
    if training:
        # In training.
        if self._should_reset_train_loader:
            self.epochs += 1
            self.train_iterator = iter(self.train_loader)
            self._should_reset_train_loader = False
        return self.train_iterator
    else:
        # In validation.
        if self._should_reset_val_loader:
            self.val_iterator = iter(self.validation_loader)
            self._should_reset_val_loader = False
        return self.val_iterator
",if self . _should_reset_val_loader :,156
"def _find_this_and_next_frame(self, stack):
    for i in range(len(stack)):
        if stack[i].id == self._frame_id:
            if i == len(stack) - 1:  # last frame
                return stack[i], None
            else:
                return stack[i], stack[i + 1]
    raise AssertionError(""Frame doesn't exist anymore"")
",if i == len ( stack ) - 1 :,106
"def send_mail(success):
    backend = (
        ""django.core.mail.backends.locmem.EmailBackend""
        if success
        else ""tests.FailingMailerEmailBackend""
    )
    with self.settings(MAILER_EMAIL_BACKEND=backend):
        mailer.send_mail(
            ""Subject"", ""Body"", ""sender@example.com"", [""recipient@example.com""]
        )
        engine.send_all()
        if not success:
            Message.objects.retry_deferred()
            engine.send_all()
",if not success :,146
"def check_dependencies():
    """"""Ensure required tools for installation are present""""""
    print(""Checking required dependencies"")
    for dep, msg in [
        ([""git"", ""--version""], ""Git (http://git-scm.com/)""),
        ([""wget"", ""--version""], ""wget""),
        ([""bzip2"", ""-h""], ""bzip2""),
    ]:
        try:
            p = subprocess.Popen(dep, stderr=subprocess.STDOUT, stdout=subprocess.PIPE)
            out, code = p.communicate()
        except OSError:
            out = ""Executable not found""
            code = 127
        if code == 127:
            raise OSError(""bcbio-nextgen installer requires %s\n%s"" % (msg, out))
",if code == 127 :,189
"def apply(self, chart, grammar, edge):
    if edge.is_incomplete():
        return
    for prod in grammar.productions():
        if edge.lhs() == prod.rhs()[0]:
            new_edge = ProbabilisticTreeEdge.from_production(
                prod, edge.start(), prod.prob()
            )
            if chart.insert(new_edge, ()):
                yield new_edge
","if chart . insert ( new_edge , ( ) ) :",107
"def run(self):
    if self.check():
        path = ""/../../../../../../../../../../../..{}"".format(self.filename)
        response = self.http_request(method=""GET"", path=path)
        if response is None:
            return
        if response.status_code == 200 and response.text:
            print_success(""Success! File: %s"" % self.filename)
            print_info(response.text)
        else:
            print_error(""Exploit failed"")
    else:
        print_error(""Device seems to be not vulnerable"")
",if response is None :,153
"def check_options(plugin, options):
    CONFLICT_OPTS = {""Phantom"": [{""rps_schedule"", ""instances_schedule"", ""stpd_file""}]}
    for conflict_options in CONFLICT_OPTS.get(plugin, []):
        intersect = {option[0] for option in options} & conflict_options
        if len(intersect) > 1:
            raise OptionsConflict(
                ""Conflicting options: {}: {}"".format(plugin, list(intersect))
            )
    return plugin, options
",if len ( intersect ) > 1 :,122
"def validate(self, document: Document) -> None:
    if not self.func(document.text):
        if self.move_cursor_to_end:
            index = len(document.text)
        else:
            index = 0
        raise ValidationError(cursor_position=index, message=self.error_message)
",if self . move_cursor_to_end :,83
"def download_link(request, path_obj):
    if path_obj.file != """":
        if path_obj.translation_project.project.is_monolingual():
            text = _(""Export"")
            tooltip = _(""Export translations"")
        else:
            text = _(""Download"")
            tooltip = _(""Download file"")
        return {
            ""href"": ""%s/download/"" % path_obj.pootle_path,
            ""text"": text,
            ""title"": tooltip,
        }
",if path_obj . translation_project . project . is_monolingual ( ) :,135
"def _setup_factories(self, extrascopes, **kw):
    for factory, (scope, Default) in {
        ""response_factory"": (boto.mws.response, self.ResponseFactory),
        ""response_error_factory"": (boto.mws.exception, self.ResponseErrorFactory),
    }.items():
        if factory in kw:
            setattr(self, ""_"" + factory, kw.pop(factory))
        else:
            scopes = extrascopes + [scope]
            setattr(self, ""_"" + factory, Default(scopes=scopes))
    return kw
",if factory in kw :,142
"def status_string(self):
    if not self.live:
        if self.expired:
            return _(""expired"")
        elif self.approved_schedule:
            return _(""scheduled"")
        elif self.workflow_in_progress:
            return _(""in moderation"")
        else:
            return _(""draft"")
    else:
        if self.approved_schedule:
            return _(""live + scheduled"")
        elif self.workflow_in_progress:
            return _(""live + in moderation"")
        elif self.has_unpublished_changes:
            return _(""live + draft"")
        else:
            return _(""live"")
",if self . approved_schedule :,166
"def _sleep_till_stopword(
    caplog,
    delay: float,
    patterns: Sequence[str] = (),
    *,
    interval: Optional[float] = None,
) -> bool:
    patterns = list(patterns or [])
    delay = delay or (10.0 if patterns else 1.0)
    interval = interval or min(1.0, max(0.1, delay / 10.0))
    started = time.perf_counter()
    found = False
    while not found and time.perf_counter() - started < delay:
        for message in list(caplog.messages):
            if any(re.search(pattern, message) for pattern in patterns):
                found = True
                break
        else:
            time.sleep(interval)
    return found
","if any ( re . search ( pattern , message ) for pattern in patterns ) :",198
"def _parse_yum_or_zypper_repositories(output):
    repos = []
    current_repo = {}
    for line in output:
        line = line.strip()
        if not line or line.startswith(""#""):
            continue
        if line.startswith(""[""):
            if current_repo:
                repos.append(current_repo)
                current_repo = {}
            current_repo[""name""] = line[1:-1]
        if current_repo and ""="" in line:
            key, value = line.split(""="", 1)
            current_repo[key] = value
    if current_repo:
        repos.append(current_repo)
    return repos
","if current_repo and ""="" in line :",179
"def __enter__(self):
    with self._entry_lock:
        cutoff_time = datetime.datetime.now() - self._time_window
        # drop the entries that are too old, as they are no longer relevant
        while self._past_entries and self._past_entries[0] < cutoff_time:
            self._past_entries.popleft()
        if len(self._past_entries) < self._access_limit:
            self._past_entries.append(datetime.datetime.now())
            return 0.0  # no waiting was needed
        to_wait = (self._past_entries[0] - cutoff_time).total_seconds()
        time.sleep(to_wait)
        self._past_entries.append(datetime.datetime.now())
        return to_wait
",if len ( self . _past_entries ) < self . _access_limit :,199
"def wrappper(*args, **kargs):
    offspring = func(*args, **kargs)
    for child in offspring:
        for i in range(len(child)):
            if child[i] > max:
                child[i] = max
            elif child[i] < min:
                child[i] = min
    return offspring
",if child [ i ] > max :,97
"def migrate_Context(self):
    for old_obj in self.session_old.query(self.model_from[""Context""]):
        new_obj = self.model_to[""Context""]()
        for key in new_obj.__table__.columns._data.keys():
            if key not in old_obj.__table__.columns._data.keys():
                continue
            value = getattr(old_obj, key)
            if key == ""tip_timetolive"" and value < 0:
                value = 0
            setattr(new_obj, key, value)
        self.session_new.add(new_obj)
",if key not in old_obj . __table__ . columns . _data . keys ( ) :,161
"def fresh_workspace(self):
    i3 = IpcTest.i3_conn
    assert i3
    workspaces = await i3.get_workspaces()
    while True:
        new_name = str(math.floor(random() * 100000))
        if not any(w for w in workspaces if w.name == new_name):
            await i3.command(""workspace %s"" % new_name)
            return new_name
",if not any ( w for w in workspaces if w . name == new_name ) :,112
"def _sum_operation(values):
    values_list = list()
    if decimal_support:
        for v in values:
            if isinstance(v, numbers.Number):
                values_list.append(v)
            elif isinstance(v, decimal128.Decimal128):
                values_list.append(v.to_decimal())
    else:
        values_list = list(v for v in values if isinstance(v, numbers.Number))
    sum_value = sum(values_list)
    return (
        decimal128.Decimal128(sum_value)
        if isinstance(sum_value, decimal.Decimal)
        else sum_value
    )
","if isinstance ( v , numbers . Number ) :",170
"def detect(content, **kwargs):
    status = kwargs.get(""status"", 0)
    if status is not None and status == 405:
        detection_schema = (
            re.compile(""error(s)?.aliyun(dun)?.(com|net)"", re.I),
            re.compile(""http(s)?://(www.)?aliyun.(com|net)"", re.I),
        )
        for detection in detection_schema:
            if detection.search(content) is not None:
                return True
",if detection . search ( content ) is not None :,136
"def __gather_epoch_end_eval_results(self, outputs):
    eval_results = []
    for epoch_output in outputs:
        result = epoch_output[0].__class__.gather(epoch_output)
        if ""checkpoint_on"" in result:
            result.checkpoint_on = result.checkpoint_on.mean()
        if ""early_stop_on"" in result:
            result.early_stop_on = result.early_stop_on.mean()
        eval_results.append(result)
    # with 1 dataloader don't pass in a list
    if len(eval_results) == 1:
        eval_results = eval_results[0]
    return eval_results
","if ""checkpoint_on"" in result :",172
"def proto_library_config(append=None, **kwargs):
    """"""protoc config.""""""
    path = kwargs.get(""protobuf_include_path"")
    if path:
        _blade_config.warning(
            ""proto_library_config: protobuf_include_path has ""
            ""been renamed to protobuf_incs, and become a list""
        )
        del kwargs[""protobuf_include_path""]
        if isinstance(path, str) and "" "" in path:
            kwargs[""protobuf_incs""] = path.split()
        else:
            kwargs[""protobuf_incs""] = [path]
    _blade_config.update_config(""proto_library_config"", append, kwargs)
","if isinstance ( path , str ) and "" "" in path :",177
"def downgrade():
    bind = op.get_bind()
    session = db.Session(bind=bind)
    for slc in session.query(Slice).filter(Slice.viz_type == ""pie"").all():
        try:
            params = json.loads(slc.params)
            if ""metric"" in params:
                if params[""metric""]:
                    params[""metrics""] = [params[""metric""]]
                del params[""metric""]
                slc.params = json.dumps(params, sort_keys=True)
        except Exception:
            pass
    session.commit()
    session.close()
","if params [ ""metric"" ] :",160
"def _resolve_params(self, api_params, optional_params, plan_vars):
    resolver = VariableResolver()
    api_params_resolved = resolver.resolve_variables(plan_vars, api_params)
    if optional_params is not None:
        optional_params_resolved = resolver.resolve_variables(
            plan_vars, optional_params
        )
        for key, value in optional_params_resolved.items():
            if key not in api_params_resolved and value is not None:
                api_params_resolved[key] = value
    return api_params_resolved
",if key not in api_params_resolved and value is not None :,148
"def publish(self, name, stat):
    try:
        topic = ""stat.%s"" % str(name)
        if ""subtopic"" in stat:
            topic += "".%d"" % stat[""subtopic""]
        stat = json.dumps(stat)
        logger.debug(""Sending %s"" % stat)
        self.socket.send_multipart([b(topic), stat])
    except zmq.ZMQError:
        if self.socket.closed:
            pass
        else:
            raise
",if self . socket . closed :,130
"def verify_packages(packages: Optional[Union[str, List[str]]]) -> None:
    if not packages:
        return
    if isinstance(packages, str):
        packages = packages.splitlines()
    for package in packages:
        if not package:
            continue
        match = RE_PATTERN.match(package)
        if match:
            name = match.group(""name"")
            operation = match.group(""operation1"")
            version = match.group(""version1"")
            _verify_package(name, operation, version)
        else:
            raise ValueError(""Unable to read requirement: %s"" % package)
",if not package :,163
"def explode(self, obj):
    """"""Determine if the object should be exploded.""""""
    if obj in self._done:
        return False
    result = False
    for item in self._explode:
        if hasattr(item, ""_moId""):
            # If it has a _moId it is an instance
            if obj._moId == item._moId:
                result = True
        else:
            # If it does not have a _moId it is a template
            if obj.__class__.__name__ == item.__name__:
                result = True
    if result:
        self._done.add(obj)
    return result
",if obj . _moId == item . _moId :,166
"def iterRelativeExportCFiles(basepath):
    for root, dirs, files in os.walk(basepath, topdown=True):
        for directory in dirs:
            if isAddonDirectoryIgnored(directory):
                dirs.remove(directory)
        for filename in files:
            if not isExportCFileIgnored(filename):
                fullpath = os.path.join(root, filename)
                yield os.path.relpath(fullpath, basepath)
",if not isExportCFileIgnored ( filename ) :,117
"def get_asset_gl_entry(self, gl_entries):
    for item in self.get(""items""):
        if item.is_fixed_asset:
            if is_cwip_accounting_enabled(item.asset_category):
                self.add_asset_gl_entries(item, gl_entries)
            if flt(item.landed_cost_voucher_amount):
                self.add_lcv_gl_entries(item, gl_entries)
                # update assets gross amount by its valuation rate
                # valuation rate is total of net rate, raw mat supp cost, tax amount, lcv amount per item
                self.update_assets(item, item.valuation_rate)
    return gl_entries
",if is_cwip_accounting_enabled ( item . asset_category ) :,188
"def _check_no_tensors(parameters: Params):
    flat_params = tf.nest.flatten(parameters.params)
    for p in flat_params:
        if isinstance(p, Params):
            _check_no_tensors(p)
        if tf.is_tensor(p):
            raise TypeError(
                ""Saw a `Tensor` value in parameters:\n  {}"".format(parameters)
            )
",if tf . is_tensor ( p ) :,108
"def _check_positional(results):
    positional = None
    for name, char in results:
        if positional is None:
            positional = name is None
        else:
            if (name is None) != positional:
                raise TranslationError(
                    ""format string mixes positional "" ""and named placeholders""
                )
    return bool(positional)
",if positional is None :,98
"def active_cursor(self):
    if self.phase == _Phase.ADJUST:
        if self.zone == _EditZone.CONTROL_NODE:
            return self._crosshair_cursor
        elif self.zone != _EditZone.EMPTY_CANVAS:  # assume button
            return self._arrow_cursor
    return None
",elif self . zone != _EditZone . EMPTY_CANVAS :,88
"def _addPending(self, path, reason, isDir=False):
    if path not in self.__pending:
        self.__pending[path] = [Utils.DEFAULT_SLEEP_INTERVAL, isDir]
        self.__pendingMinTime = 0
        if isinstance(reason, pyinotify.Event):
            reason = [reason.maskname, reason.pathname]
        logSys.log(
            logging.MSG,
            ""Log absence detected (possibly rotation) for %s, reason: %s of %s"",
            path,
            *reason
        )
","if isinstance ( reason , pyinotify . Event ) :",147
"def has_safe_repr(value):
    """"""Does the node have a safe representation?""""""
    if value is None or value is NotImplemented or value is Ellipsis:
        return True
    if isinstance(value, (bool, int, long, float, complex, basestring, xrange, Markup)):
        return True
    if isinstance(value, (tuple, list, set, frozenset)):
        for item in value:
            if not has_safe_repr(item):
                return False
        return True
    elif isinstance(value, dict):
        for key, value in value.iteritems():
            if not has_safe_repr(key):
                return False
            if not has_safe_repr(value):
                return False
        return True
    return False
",if not has_safe_repr ( key ) :,192
"def refund_balances(self):
    from liberapay.billing.transactions import refund_payin
    payins = self.get_refundable_payins()
    for exchange in payins:
        balance = self.get_balance_in(exchange.amount.currency)
        if balance == 0:
            continue
        amount = min(balance, exchange.refundable_amount)
        status, e_refund = refund_payin(self.db, exchange, amount, self)
        if status != ""succeeded"":
            raise TransferError(e_refund.note)
",if balance == 0 :,146
"def balanced_tokens_across_dcs(self, dcs):
    tokens = []
    current_dc = dcs[0]
    count = 0
    dc_count = 0
    for dc in dcs:
        if dc == current_dc:
            count += 1
        else:
            new_tokens = [tk + (dc_count * 100) for tk in self.balanced_tokens(count)]
            tokens.extend(new_tokens)
            current_dc = dc
            count = 1
            dc_count += 1
    new_tokens = [tk + (dc_count * 100) for tk in self.balanced_tokens(count)]
    tokens.extend(new_tokens)
    return tokens
",if dc == current_dc :,181
"def get_logsource(self, category, product, service):
    """"""Return merged log source definition of all logosurces that match criteria across all Sigma conversion configurations in chain.""""""
    matching = list()
    for config in self:
        for logsource in config.logsources:
            if logsource.matches(category, product, service):
                matching.append(logsource)
                if logsource.rewrite is not None:
                    category, product, service = logsource.rewrite
    return SigmaLogsourceConfiguration(matching, self.defaultindex)
",if logsource . rewrite is not None :,138
"def fill_squares(self, loc, type):
    value = type
    for n in range(self.no_players):
        self.map_data[loc[0]][loc[1]] = value
        if type == ""0"":
            value = chr(ord(value) + 1)
        loc = self.get_translate_loc(loc)
","if type == ""0"" :",88
"def _init_ti_table():
    global _ti_table
    _ti_table = []
    for fname, name in zip(kc.STRFNAMES, kc.STRNAMES):
        seq = termcap.get(name)
        if not seq:
            continue
        k = _name_to_key(fname)
        if k:
            _ti_table.append((list(bytearray(seq)), k))
",if not seq :,109
"def OnDelete(self, event):
    with wx.MessageDialog(
        self,
        ""Do you really want to delete the {} {}?"".format(
            self.getActiveEntity().name, self.entityName
        ),
        ""Confirm Delete"",
        wx.YES | wx.NO | wx.ICON_QUESTION,
    ) as dlg:
        dlg.CenterOnParent()
        if dlg.ShowModal() == wx.ID_YES:
            self.DoDelete(self.getActiveEntity())
            self.refreshEntityList()
            wx.PostEvent(
                self.entityChoices, wx.CommandEvent(wx.wxEVT_COMMAND_CHOICE_SELECTED)
            )
",if dlg . ShowModal ( ) == wx . ID_YES :,179
"def _add(self, queue):
    if not queue.routing_key:
        if queue.exchange is None or queue.exchange.name == """":
            queue.exchange = self.default_exchange
        queue.routing_key = self.default_routing_key
    if self.ha_policy:
        if queue.queue_arguments is None:
            queue.queue_arguments = {}
        self._set_ha_policy(queue.queue_arguments)
    if self.max_priority is not None:
        if queue.queue_arguments is None:
            queue.queue_arguments = {}
        self._set_max_priority(queue.queue_arguments)
    self[queue.name] = queue
    return queue
","if queue . exchange is None or queue . exchange . name == """" :",187
"def ParsePlacemark(self, node):
    ret = Placemark()
    for child in node.childNodes:
        if child.nodeName == ""name"":
            ret.name = self.ExtractText(child)
        if child.nodeName == ""Point"" or child.nodeName == ""LineString"":
            ret.coordinates = self.ExtractCoordinates(child)
    return ret
","if child . nodeName == ""name"" :",94
"def find_library_nt(name):
    # modified from ctypes.util
    # ctypes.util.find_library just returns first result he found
    # but we want to try them all
    # because on Windows, users may have both 32bit and 64bit version installed
    results = []
    for directory in os.environ[""PATH""].split(os.pathsep):
        fname = os.path.join(directory, name)
        if os.path.isfile(fname):
            results.append(fname)
        if fname.lower().endswith("".dll""):
            continue
        fname = fname + "".dll""
        if os.path.isfile(fname):
            results.append(fname)
    return results
","if fname . lower ( ) . endswith ( "".dll"" ) :",174
"def _calc_freq(item):
    try:
        if ao_index is not None and ro_index is not None:
            ao = sum([int(x) for x in item.split("":"")[ao_index].split("","")])
            ro = int(item.split("":"")[ro_index])
            freq = ao / float(ao + ro)
        elif af_index is not None:
            freq = float(item.split("":"")[af_index])
        else:
            freq = 0.0
    except (IndexError, ValueError, ZeroDivisionError):
        freq = 0.0
    return freq
",elif af_index is not None :,151
"def poll_kafka(self):
    while True:
        val = self.do_poll()
        if val:
            yield self._emit(val)
        else:
            yield gen.sleep(self.poll_interval)
        if self.stopped:
            break
    self._close_consumer()
",if val :,85
"def resolve_list_field(parent, args, ctx, info):
    if ""param"" in args:
        return ""SUCCESS-[{}]"".format(
            str(args[""param""])
            if not isinstance(args[""param""], list)
            else ""-"".join([str(item) for item in args[""param""]])
        )
    return ""SUCCESS""
","if not isinstance ( args [ ""param"" ] , list )",88
"def login_hash(self, host, username, ntlmhash, domain):
    lmhash, nthash = ntlmhash.split("":"")
    try:
        self.smbconn[host] = SMBConnection(host, host, sess_port=445, timeout=2)
        self.smbconn[host].login(username, """", domain, lmhash=lmhash, nthash=nthash)
        if self.smbconn[host].isGuestSession() > 0:
            color(""[+] Guest session established on %s..."" % (host))
        else:
            color(""[+] User session establishd on %s..."" % (host))
        return True
    except Exception as e:
        color(""[!] Authentication error occured"")
        color(""[!]"", e)
        return False
",if self . smbconn [ host ] . isGuestSession ( ) > 0 :,196
"def _add(self, queue):
    if not queue.routing_key:
        if queue.exchange is None or queue.exchange.name == """":
            queue.exchange = self.default_exchange
        queue.routing_key = self.default_routing_key
    if self.ha_policy:
        if queue.queue_arguments is None:
            queue.queue_arguments = {}
        self._set_ha_policy(queue.queue_arguments)
    if self.max_priority is not None:
        if queue.queue_arguments is None:
            queue.queue_arguments = {}
        self._set_max_priority(queue.queue_arguments)
    self[queue.name] = queue
    return queue
",if queue . queue_arguments is None :,187
"def safe_delete_pod(self, jobid, ignore_not_found=True):
    import kubernetes.client
    body = kubernetes.client.V1DeleteOptions()
    try:
        self.kubeapi.delete_namespaced_pod(jobid, self.namespace, body=body)
    except kubernetes.client.rest.ApiException as e:
        if e.status == 404 and ignore_not_found:
            # Can't find the pod. Maybe it's already been
            # destroyed. Proceed with a warning message.
            logger.warning(
                ""[WARNING] 404 not found when trying to delete the pod: {jobid}\n""
                ""[WARNING] Ignore this error\n"".format(jobid=jobid)
            )
        else:
            raise e
",if e . status == 404 and ignore_not_found :,194
"def __init__(self, element, spec):
    Extension.__init__(self, element, spec)
    self.spec = spec
    self.number = tuple(map(int, element.attrib[""number""].split(""."")))
    self.api = element.attrib[""api""]
    # not every spec has a ._remove member, but there shouldn't be a remove
    # tag without that member, if there is, blame me!
    for removed in chain.from_iterable(element.findall(""remove"")):
        if removed.tag == ""type"":
            continue
        data = {""enum"": spec.enums, ""command"": spec.commands}[removed.tag]
        try:
            spec.add_remove(self.api, self.number, data[removed.attrib[""name""]])
        except KeyError:
            pass  # TODO
","if removed . tag == ""type"" :",199
"def _convert_raw_source(self, source, languages):
    for row in source:
        example = self._read_example(row)
        if example is None:
            continue
        for col, lang in zip(self.language_columns, languages):
            example[col] = lang
        yield example
",if example is None :,81
"def check_engine(engine):
    if engine == ""auto"":
        if pa is not None:
            return ""pyarrow""
        elif fastparquet is not None:  # pragma: no cover
            return ""fastparquet""
        else:  # pragma: no cover
            raise RuntimeError(""Please install either pyarrow or fastparquet."")
    elif engine == ""pyarrow"":
        if pa is None:  # pragma: no cover
            raise RuntimeError(""Please install pyarrow fisrt."")
        return engine
    elif engine == ""fastparquet"":
        if fastparquet is None:  # pragma: no cover
            raise RuntimeError(""Please install fastparquet first."")
        return engine
    else:  # pragma: no cover
        raise RuntimeError(""Unsupported engine {} to read parquet."".format(engine))
",elif fastparquet is not None :,187
"def TryMerge(self, d):
    while 1:
        tt = d.getVarInt32()
        if tt == 12:
            break
        if tt == 18:
            self.set_value(d.getPrefixedString())
            continue
        if tt == 29:
            self.set_flags(d.get32())
            continue
        if tt == 0:
            raise ProtocolBuffer.ProtocolBufferDecodeError
        d.skipData(tt)
",if tt == 12 :,126
"def handle(self, request):
    try:
        if request.message.question[0].rdtype == dns.rdatatype.IXFR:
            if self.did_truncation:
                text = ixfr
            else:
                text = retry_tcp_ixfr
                self.did_truncation = True
        else:
            text = axfr
        r = dns.message.from_text(text, one_rr_per_rrset=True, origin=self.origin)
        r.id = request.message.id
        return r
    except Exception:
        pass
",if self . did_truncation :,159
"def read_kvfile_todict(file):
    if not os.path.isfile(file):
        return {}
    ret = {}
    with open(file, ""r"") as FH:
        for l in FH.readlines():
            l = l.strip()
            # l = l.strip().decode('utf8')
            if l:
                (k, v) = re.match(r""(\S*)\s*(.*)"", l).group(1, 2)
                k = re.sub(""____"", "" "", k)
                ret[k] = v
    return ret
",if l :,153
"def wrapper(*args, **kwargs):
    with capture_logs() as logs:
        try:
            function(*args, **kwargs)
        except Exception:  # pragma: no cover
            if logs:
                print(""%i errors logged:"" % len(logs), file=sys.stderr)
                for message in logs:
                    print(message, file=sys.stderr)
            raise
        else:
            if logs:  # pragma: no cover
                for message in logs:
                    print(message, file=sys.stderr)
                raise AssertionError(""%i errors logged"" % len(logs))
",if logs :,168
"def batchSites(self, sites):
    i = 0
    res = list()
    siteList = list()
    for site in sites:
        if i >= self.opts[""_maxthreads""]:
            data = self.threadSites(siteList)
            if data is None:
                return res
            for ret in list(data.keys()):
                if data[ret]:
                    # bucket:filecount
                    res.append(f""{ret}:{data[ret]}"")
            i = 0
            siteList = list()
        siteList.append(site)
        i += 1
    return res
",if data is None :,168
"def datagram_received(self, data, addr):
    """"""Handle data from ``addr``.""""""
    if self.buffer and addr in self.buffer:
        data = self.buffer.pop(addr) + data
    while data:
        idx = data.find(self.separator)
        if idx >= 0:  # we have a full message
            idx += len(self.separator)
            chunk, data = data[:idx], data[idx:]
            self.response(chunk, addr)
        else:
            if self.buffer is None:
                self.buffer = {}
            self.buffer[addr] = data
            data = None
",if self . buffer is None :,168
"def tearDown(self):
    if self.node:
        if self.node.client:
            with patch(""golem.task.taskserver.TaskServer.quit""):
                self.node.client.quit()
        if self.node._db:
            self.node._db.close()
    super().tearDown()
",if self . node . client :,84
"def _to_sentences(self, lines):
    text = """"
    sentence_objects = []
    for line in lines:
        if isinstance(line, Sentence):
            if text:
                sentences = self.tokenize_sentences(text)
                sentence_objects += map(self._to_sentence, sentences)
            sentence_objects.append(line)
            text = """"
        else:
            text += "" "" + line
    text = text.strip()
    if text:
        sentences = self.tokenize_sentences(text)
        sentence_objects += map(self._to_sentence, sentences)
    return sentence_objects
","if isinstance ( line , Sentence ) :",164
"def _cloneComponentValues(self, myClone, cloneValueFlag):
    idx = 0
    l = len(self._componentValues)
    while idx < l:
        c = self._componentValues[idx]
        if c is not None:
            if isinstance(c, base.AbstractConstructedAsn1Item):
                myClone.setComponentByPosition(
                    idx, c.clone(cloneValueFlag=cloneValueFlag)
                )
            else:
                myClone.setComponentByPosition(idx, c.clone())
        idx = idx + 1
","if isinstance ( c , base . AbstractConstructedAsn1Item ) :",154
"def split_quality(quality):
    anyQualities = []
    bestQualities = []
    for curQual in Quality.qualityStrings.keys():
        if curQual & quality:
            anyQualities.append(curQual)
        if curQual << 16 & quality:
            bestQualities.append(curQual)
    return sorted(anyQualities), sorted(bestQualities)
",if curQual << 16 & quality :,109
"def make_pattern(wtree):
    subpattern = []
    for part in wtree[1:-1]:
        if isinstance(part, list):
            part = make_pattern(part)
        elif wtree[0] != """":
            for c in part:
                # Meta-characters cannot be quoted
                if c in special_chars:
                    raise GlobError()
        subpattern.append(part)
    return """".join(subpattern)
","if isinstance ( part , list ) :",123
"def insert_not(self, aList):
    '''Change ""!"" to ""not"" except before ""=""'''
    i = 0
    while i < len(aList):
        if self.is_string_or_comment(aList, i):
            i = self.skip_string_or_comment(aList, i)
        elif aList[i] == ""!"" and not self.match(aList, i + 1, ""=""):
            aList[i : i + 1] = list(""not "")
            i += 4
        else:
            i += 1
","elif aList [ i ] == ""!"" and not self . match ( aList , i + 1 , ""="" ) :",142
"def _concretize(self, n_cls, t1, t2, join_or_meet, translate):
    ptr_class = self._pointer_class()
    if n_cls is ptr_class:
        if isinstance(t1, ptr_class) and isinstance(t2, ptr_class):
            # we need to merge them
            return ptr_class(join_or_meet(t1.basetype, t2.basetype, translate))
        if isinstance(t1, ptr_class):
            return t1
        elif isinstance(t2, ptr_class):
            return t2
        else:
            # huh?
            return ptr_class(BottomType())
    return n_cls()
","elif isinstance ( t2 , ptr_class ) :",181
"def pre_validate(self, form):
    if self.data:
        values = list(c[0] for c in self.choices)
        for d in self.data:
            if d not in values:
                raise ValueError(
                    self.gettext(""'%(value)s' is not a valid choice for this field"")
                    % dict(value=d)
                )
",if d not in values :,106
"def frontend_visible_config(config_dict):
    visible_dict = {}
    for name in CLIENT_WHITELIST:
        if name.lower().find(""secret"") >= 0:
            raise Exception(""Cannot whitelist secrets: %s"" % name)
        if name in config_dict:
            visible_dict[name] = config_dict.get(name, None)
        if ""ENTERPRISE_LOGO_URL"" in config_dict:
            visible_dict[""BRANDING""] = visible_dict.get(""BRANDING"", {})
            visible_dict[""BRANDING""][""logo""] = config_dict[""ENTERPRISE_LOGO_URL""]
    return visible_dict
",if name in config_dict :,171
"def listdir(self, path=None):
    from azure.storage.blob import Blob
    dir_path = normalize_storage_path(self._append_path_to_prefix(path))
    if dir_path:
        dir_path += ""/""
    items = list()
    for blob in self.client.list_blobs(self.container, prefix=dir_path, delimiter=""/""):
        if type(blob) == Blob:
            items.append(self._strip_prefix_from_path(blob.name, dir_path))
        else:
            items.append(
                self._strip_prefix_from_path(
                    blob.name[: blob.name.find(""/"", len(dir_path))], dir_path
                )
            )
    return items
",if type ( blob ) == Blob :,198
"def diff(self, resources):
    model = self.manager.resource_type
    for r in resources:
        hlabels = self.resolve_labels(r[""projectId""])
        if not hlabels:
            continue
        delta = False
        rlabels = r.get(""labels"", {})
        for k, v in hlabels.items():
            if k not in rlabels or rlabels[k] != v:
                delta = True
        if not delta:
            continue
        rlabels = dict(rlabels)
        rlabels.update(hlabels)
        if delta:
            yield (""update"", model.get_label_params(r, rlabels))
",if delta :,176
"def favorite(id):
    note = Note.query.get_or_404(id)
    if current_user != note.author:
        abort(403)
    else:
        if not note.is_favorite:
            note.is_favorite = True
            note.updated_date = datetime.utcnow()
            db.session.commit()
            flash(""Note marked as favorite"")
        else:
            note.is_favorite = False
            note.updated_date = datetime.utcnow()
            db.session.commit()
            flash(""Note removed as favorite"")
        return redirect(request.referrer)
",if not note . is_favorite :,163
"def enter_standby_instances(self, group_name, instance_ids, should_decrement):
    group = self.autoscaling_groups[group_name]
    original_size = group.desired_capacity
    standby_instances = []
    for instance_state in group.instance_states:
        if instance_state.instance.id in instance_ids:
            instance_state.lifecycle_state = ""Standby""
            standby_instances.append(instance_state)
    if should_decrement:
        group.desired_capacity = group.desired_capacity - len(instance_ids)
    group.set_desired_capacity(group.desired_capacity)
    return standby_instances, original_size, group.desired_capacity
",if instance_state . instance . id in instance_ids :,182
"def _child_complete_hook(self, child_task):
    if child_task.task_spec == self.main_child_task_spec or self._should_cancel(
        child_task.task_spec
    ):
        for sibling in child_task.parent.children:
            if sibling != child_task:
                if sibling.task_spec == self.main_child_task_spec or (
                    isinstance(sibling.task_spec, BoundaryEvent)
                    and not sibling._is_finished()
                ):
                    sibling.cancel()
        for t in child_task.workflow._get_waiting_tasks():
            t.task_spec._update(t)
",if sibling != child_task :,183
"def extract_groups(self, text: str, language_code: str):
    previous = None
    group = 1
    groups = []
    words = []
    ignored = IGNORES.get(language_code, {})
    for word in NON_WORD.split(text):
        if not word:
            continue
        if word not in ignored and len(word) >= 2:
            if previous == word:
                group += 1
            elif group > 1:
                groups.append(group)
                words.append(previous)
                group = 1
        previous = word
    if group > 1:
        groups.append(group)
        words.append(previous)
    return groups, words
",if word not in ignored and len ( word ) >= 2 :,187
"def runTest(self):
    """"""This function will call api providing list of op_class""""""
    if self.is_positive_test:
        response = indexes_utils.api_create_index_get_op_class(self)
    else:
        if self.mocking_required:
            with patch(
                self.mock_data[""function_name""],
                side_effect=eval(self.mock_data[""return_value""]),
            ):
                response = indexes_utils.api_create_index_get_op_class(self)
    indexes_utils.assert_status_code(self, response)
",if self . mocking_required :,160
"def fn(value=None):
    for i in [-1, 0, 1, 2, 3, 4]:
        if i < 0:
            continue
        elif i == 0:
            yield 0
        elif i == 1:
            yield 1
            i = 0
            yield value
            yield 2
        else:
            try:
                v = i / value
            except:
                v = i
            yield v
",if i < 0 :,127
"def _update(self, flag):
    self._modified = False
    self._index = {}
    try:
        f = _io.open(self._dirfile, ""r"", encoding=""Latin-1"")
    except OSError:
        if flag not in (""c"", ""n""):
            raise
        self._modified = True
    else:
        with f:
            for line in f:
                line = line.rstrip()
                key, pos_and_siz_pair = _ast.literal_eval(line)
                key = key.encode(""Latin-1"")
                self._index[key] = pos_and_siz_pair
","if flag not in ( ""c"" , ""n"" ) :",172
"def _network_connections_in_results(data):
    for plugin_name, plugin_result in data.iteritems():
        if plugin_result[""status""] == ""error"":
            continue
        if ""device"" not in plugin_result:
            continue
        if ""connections"" in plugin_result[""device""]:
            for conn in plugin_result[""device""][""connections""]:
                if conn[""connection_type""] == ConnectionType.network.name:
                    return True
    return False
","if ""device"" not in plugin_result :",126
"def close(self) -> None:
    """"""Stop accepting writes and write file, if needed.""""""
    if not self._io:
        raise Exception(""FileAvoidWrite does not support empty files."")
    buf = self.getvalue()
    self._io.close()
    try:
        with open(self._path, encoding=""utf-8"") as old_f:
            old_content = old_f.read()
            if old_content == buf:
                return
    except OSError:
        pass
    with open(self._path, ""w"", encoding=""utf-8"") as f:
        f.write(buf)
",if old_content == buf :,157
"def _extract_changes(doc_map, changes, read_time):
    deletes = []
    adds = []
    updates = []
    for name, value in changes.items():
        if value == ChangeType.REMOVED:
            if name in doc_map:
                deletes.append(name)
        elif name in doc_map:
            if read_time is not None:
                value.read_time = read_time
            updates.append(value)
        else:
            if read_time is not None:
                value.read_time = read_time
            adds.append(value)
    return (deletes, adds, updates)
",if value == ChangeType . REMOVED :,173
"def preprocess(
    self,
    X: DataFrame,
    is_train=False,
    vect_max_features=1000,
    model_specific_preprocessing=False,
):
    X = super().preprocess(X=X)
    if (
        model_specific_preprocessing
    ):  # This is hack to work-around pre-processing caching in bagging/stacker models
        if is_train:
            feature_types = self._get_types_of_features(X)
            X = self.preprocess_train(X, feature_types, vect_max_features)
        else:
            X = self.pipeline.transform(X)
    return X
",if is_train :,167
"def setup_child(self, child):
    child.parent = self
    if self.document:
        child.document = self.document
        if child.source is None:
            child.source = self.document.current_source
        if child.line is None:
            child.line = self.document.current_line
",if child . source is None :,84
"def _compute_early_outs(self, quotas):
    for q in quotas:
        if q.closed and not self._ignore_closed:
            self.results[q] = Quota.AVAILABILITY_ORDERED, 0
        elif q.size is None:
            self.results[q] = Quota.AVAILABILITY_OK, None
        elif q.size == 0:
            self.results[q] = Quota.AVAILABILITY_GONE, 0
",if q . closed and not self . _ignore_closed :,118
"def parse_function(self, l):
    bracket = l.find(""("")
    fname = l[8:bracket]
    if self.properties:
        if self.properties[0] == ""propget"":
            self.props[fname] = 1
            self.propget[fname] = 1
        elif self.properties[0] == ""propput"":
            self.props[fname] = 1
            self.propput[fname] = 1
        else:
            self.functions[fname] = 1
    self.properties = None
","if self . properties [ 0 ] == ""propget"" :",139
"def SetHelpListButtonStates(self):
    if self.listHelp.size() < 1:  # no entries in list
        self.buttonHelpListEdit.config(state=DISABLED)
        self.buttonHelpListRemove.config(state=DISABLED)
    else:  # there are some entries
        if self.listHelp.curselection():  # there currently is a selection
            self.buttonHelpListEdit.config(state=NORMAL)
            self.buttonHelpListRemove.config(state=NORMAL)
        else:  # there currently is not a selection
            self.buttonHelpListEdit.config(state=DISABLED)
            self.buttonHelpListRemove.config(state=DISABLED)
",if self . listHelp . curselection ( ) :,174
"def param_names() -> FrozenSet[Tuple[str, str]]:
    """"""Returns all module and parameter names as a set of pairs.""""""
    out = []
    params = current_frame().params
    for mod_name, bundle in params.items():
        if not isinstance(bundle, Mapping):
            # TODO(tomhennigan) Fix broken user code and remove this warning.
            warnings.warn(f""Invalid entry {mod_name!r} in params {params}"")
            continue
        for name in bundle:
            out.append((mod_name, name))
    return frozenset(out)
","if not isinstance ( bundle , Mapping ) :",150
"def _classify_volume(self, ctxt, volumes):
    normal_volumes = []
    replica_volumes = []
    for v in volumes:
        volume_type = self._get_volume_replicated_type(ctxt, v)
        if volume_type and v.status == ""available"":
            replica_volumes.append(v)
        else:
            normal_volumes.append(v)
    return normal_volumes, replica_volumes
","if volume_type and v . status == ""available"" :",113
"def undump_descriptions_of_all_objects(inf):
    d = {}
    for l in inf:
        dash = l.find(""-"")
        if dash == -1:
            raise l
        mo = NRE.search(l)
        if mo:
            typstr = l[dash + 1 : mo.start(0)]
            num = int(mo.group(0))
            if str(num) != mo.group(0):
                raise mo.group(0)
        else:
            typstr = l[dash + 1 :]
            num = None
        d[l[:dash]] = (
            typstr,
            num,
        )
    return d
",if mo :,187
"def _real_len(self, s):
    s_len = 0
    in_esc = False
    prev = "" ""
    for c in replace_all({""\0+"": """", ""\0-"": """", ""\0^"": """", ""\1"": """", ""\t"": "" ""}, s):
        if in_esc:
            if c == ""m"":
                in_esc = False
        else:
            if c == ""["" and prev == ""\033"":
                in_esc = True
                s_len -= 1  # we counted prev when we shouldn't have
            else:
                s_len += self._display_len(c)
        prev = c
    return s_len
",if in_esc :,177
"def update_all(self, include_description=False):
    if self.background_update is None:
        episodes = [row[self.C_EPISODE] for row in self]
    else:
        # Update all episodes that have already been initialized...
        episodes = [
            row[self.C_EPISODE]
            for index, row in enumerate(self)
            if index < self.background_update.index
        ]
        # ...and also include episodes that still need to be initialized
        episodes.extend(self.background_update.episodes)
    self._update_from_episodes(episodes, include_description)
",if index < self . background_update . index,170
"def _debug_log(self, text, level):
    if text and ""log"" in self.config.sys.debug:
        if not text.startswith(self.log_prefix):
            text = ""%slog(%s): %s"" % (self.log_prefix, level, text)
        if self.log_parent is not None:
            return self.log_parent.log(level, text)
        else:
            self.term.write(self._fmt_log(text, level=level))
",if not text . startswith ( self . log_prefix ) :,129
"def save_new_objects(self, commit=True):
    self.new_objects = []
    for form in self.extra_forms:
        if not form.has_changed():
            continue
        # If someone has marked an add form for deletion, don't save the
        # object.
        if self.can_delete and self._should_delete_form(form):
            continue
        self.new_objects.append(self.save_new(form, commit=commit))
        if not commit:
            self.saved_forms.append(form)
    return self.new_objects
",if not commit :,151
"def get_master_info(accounts_config, master):
    master_info = None
    for a in accounts_config[""accounts""]:
        if a[""name""] == master:
            master_info = a
            break
        if a[""account_id""] == master:
            master_info = a
            break
    if master_info is None:
        raise ValueError(""Master account: %s not found in accounts config"" % (master))
    return master_info
","if a [ ""name"" ] == master :",120
"def update(attr, value=None):
    if value is not None:
        setattr(draft, attr, value)
        if attr == ""body"":
            # Update size, snippet too
            draft.size = len(value)
            draft.snippet = draft.calculate_html_snippet(value)
","if attr == ""body"" :",78
"def _process_property_change(self, msg):
    msg = super(Select, self)._process_property_change(msg)
    if ""value"" in msg:
        if not self.values:
            pass
        elif msg[""value""] is None:
            msg[""value""] = self.values[0]
        else:
            if isIn(msg[""value""], self.unicode_values):
                idx = indexOf(msg[""value""], self.unicode_values)
            else:
                idx = indexOf(msg[""value""], self.labels)
            msg[""value""] = self._items[self.labels[idx]]
    msg.pop(""options"", None)
    return msg
","elif msg [ ""value"" ] is None :",180
"def removeEmptyDir(path, removeRoot=True):
    if not os.path.isdir(path):
        return
    # remove empty subfolders
    _files = os.listdir(path)
    if len(_files) > 0:
        for f in _files:
            if not f.startswith(""."") and not f.startswith(""_""):
                fullpath = os.path.join(path, f)
                if os.path.isdir(fullpath):
                    removeEmptyDir(fullpath)
    # if folder empty, delete it
    _files = os.listdir(path)
    if len(_files) == 0 and removeRoot:
        Print.info(""Removing empty folder:"" + path)
        os.rmdir(path)
",if os . path . isdir ( fullpath ) :,181
"def make_relative_to(self, kwds, relative_to):
    if relative_to and os.path.dirname(relative_to):
        dirname = os.path.dirname(relative_to)
        kwds = kwds.copy()
        for key in ffiplatform.LIST_OF_FILE_NAMES:
            if key in kwds:
                lst = kwds[key]
                if not isinstance(lst, (list, tuple)):
                    raise TypeError(""keyword '%s' should be a list or tuple"" % (key,))
                lst = [os.path.join(dirname, fn) for fn in lst]
                kwds[key] = lst
    return kwds
","if not isinstance ( lst , ( list , tuple ) ) :",173
"def ending(self, state):
    print_title("" STABLE PINS "")
    path_lists = trace_graph(state.graph)
    for k in sorted(state.mapping):
        print(state.mapping[k].as_line(include_hashes=False))
        paths = path_lists[k]
        for path in paths:
            if path == [None]:
                print(""    User requirement"")
                continue
            print(""   "", end="""")
            for v in reversed(path[1:]):
                line = state.mapping[v].as_line(include_hashes=False)
                print("" <="", line, end="""")
            print()
    print()
",if path == [ None ] :,183
"def fetch():
    retval = {}
    content = retrieve_content(__url__)
    if __check__ in content:
        for line in content.split(""\n""):
            line = line.strip()
            if not line or line.startswith(""#"") or ""."" not in line:
                continue
            if "" # "" in line:
                reason = line.split("" # "")[1].split()[0].lower()
                if reason == ""scanning"":  # too many false positives
                    continue
                retval[line.split("" # "")[0]] = (__info__, __reference__)
    return retval
","if not line or line . startswith ( ""#"" ) or ""."" not in line :",157
"def __str__(self):
    """"""Returns human readable string representation, useful for debugging.""""""
    buf = StringIO()
    for idx, (class_batch_id, class_val) in enumerate(iteritems(self.data)):
        if idx >= TO_STR_MAX_BATCHES:
            buf.write(u""  ...\n"")
            break
        buf.write(u'  ClassBatch ""{0}""\n'.format(class_batch_id))
        buf.write(u""    {0}\n"".format(str(class_val)))
    return buf.getvalue()
",if idx >= TO_STR_MAX_BATCHES :,142
"def find_caller(stack):
    """"""Finds info about first non-sqlalchemy call in stack""""""
    for frame in stack:
        # We don't care about sqlalchemy internals
        module = inspect.getmodule(frame[0])
        if not hasattr(module, ""__name__""):
            continue
        if module.__name__.startswith(""sqlalchemy""):
            continue
        return (module.__name__,) + tuple(frame[2:4]) + (frame[4][0].strip(),)
    log.warning(""Transaction from unknown origin"")
    return None, None, None, None
","if module . __name__ . startswith ( ""sqlalchemy"" ) :",137
"def format_unencoded(self, tokensource, outfile):
    if self.linenos:
        self._write_lineno(outfile)
    for ttype, value in tokensource:
        color = self._get_color(ttype)
        for line in value.splitlines(True):
            if color:
                outfile.write(""<%s>%s</>"" % (color, line.rstrip(""\n"")))
            else:
                outfile.write(line.rstrip(""\n""))
            if line.endswith(""\n""):
                if self.linenos:
                    self._write_lineno(outfile)
                else:
                    outfile.write(""\n"")
    if self.linenos:
        outfile.write(""\n"")
",if color :,190
"def __new__(cls, name, bases, attrs):
    klass = type.__new__(cls, name, bases, attrs)
    if ""cmds"" in attrs:
        cmds = attrs[""cmds""]
        if isinstance(cmds, str):
            cmd_handler_mapping[cmds] = klass
        else:
            for cmd in cmds:
                cmd_handler_mapping[cmd] = klass
    return klass
","if isinstance ( cmds , str ) :",104
"def __getattr__(self, key):
    if key == key.upper():
        if hasattr(self._django_settings, key):
            return getattr(self._django_settings, key)
        elif hasattr(self._default_settings, key):
            return getattr(self._default_settings, key)
    raise AttributeError(
        ""%r object has no attribute %r"" % (self.__class__.__name__, key)
    )
","if hasattr ( self . _django_settings , key ) :",106
"def download_file(url):
    local_filename = url.split(""/"")[-1]
    outfile = os.path.join(AVATAR_DIR, local_filename)
    if not os.path.isfile(outfile):
        r = requests.get(url, stream=True)
        with open(outfile, ""wb"") as f:
            for chunk in r.iter_content(chunk_size=1024):
                if chunk:  # filter out keep-alive new chunks
                    f.write(chunk)
                    f.flush()
    return local_filename
",if chunk :,143
"def check_default(self):
    if self.check():
        self.credentials = []
        data = LockedIterator(itertools.product(self.usernames, self.passwords))
        self.run_threads(self.threads, self.target_function, data)
        if self.credentials:
            return self.credentials
    return None
",if self . credentials :,85
"def _process_frame(self, frame_num, frame_im, callback=None):
    # type(int, numpy.ndarray) -> None
    """"""Adds any cuts detected with the current frame to the cutting list.""""""
    for detector in self._detector_list:
        cuts = detector.process_frame(frame_num, frame_im)
        if cuts and callback:
            callback(frame_im, frame_num)
        self._cutting_list += cuts
    for detector in self._sparse_detector_list:
        events = detector.process_frame(frame_num, frame_im)
        if events and callback:
            callback(frame_im, frame_num)
        self._event_list += events
",if events and callback :,178
"def parse(cls, api, json):
    user = cls(api)
    setattr(user, ""_json"", json)
    for k, v in json.items():
        if k == ""created_at"":
            setattr(user, k, parse_datetime(v))
        elif k == ""status"":
            setattr(user, k, Status.parse(api, v))
        elif k == ""following"":
            # twitter sets this to null if it is false
            if v is True:
                setattr(user, k, True)
            else:
                setattr(user, k, False)
        else:
            setattr(user, k, v)
    return user
","elif k == ""following"" :",179
"def dump_token_list(tokens):
    for token in tokens:
        if token.token_type == TOKEN_TEXT:
            writer.write(token.contents)
        elif token.token_type == TOKEN_VAR:
            writer.print_expr(token.contents)
            touch_var(token.contents)
",if token . token_type == TOKEN_TEXT :,83
"def parent_path(path):
    parent_dir = S3FileSystem._append_separator(path)
    if not s3.is_root(parent_dir):
        bucket_name, key_name, basename = s3.parse_uri(path)
        if not basename:  # bucket is top-level so return root
            parent_dir = S3A_ROOT
        else:
            bucket_path = ""%s%s"" % (S3A_ROOT, bucket_name)
            key_path = ""/"".join(key_name.split(""/"")[:-1])
            parent_dir = s3.abspath(bucket_path, key_path)
    return parent_dir
",if not basename :,168
"def write_framed_message(self, message):
    message_length = len(message)
    total_bytes_sent = 0
    while message_length - total_bytes_sent > 0:
        if message_length - total_bytes_sent > BUFFER_SIZE:
            buffer_length = BUFFER_SIZE
        else:
            buffer_length = message_length - total_bytes_sent
        self.write_buffer(
            message[total_bytes_sent : (total_bytes_sent + buffer_length)]
        )
        total_bytes_sent += buffer_length
    # A message is always terminated by a zero-length buffer.
    self.write_buffer_length(0)
",if message_length - total_bytes_sent > BUFFER_SIZE :,177
"def reader():
    with tarfile.open(filename, mode=""r"") as f:
        names = (each_item.name for each_item in f if sub_name in each_item.name)
        while True:
            for name in names:
                if six.PY2:
                    batch = pickle.load(f.extractfile(name))
                else:
                    batch = pickle.load(f.extractfile(name), encoding=""bytes"")
                for item in read_batch(batch):
                    yield item
            if not cycle:
                break
",if not cycle :,157
"def splitOn(sequence, predicate, transformers):
    result = []
    mode = predicate(sequence[0])
    tmp = [sequence[0]]
    for e in sequence[1:]:
        p = predicate(e)
        if p != mode:
            result.extend(transformers[mode](tmp))
            tmp = [e]
            mode = p
        else:
            tmp.append(e)
    result.extend(transformers[mode](tmp))
    return result
",if p != mode :,122
"def stroke(s):
    keys = []
    on_left = True
    for k in s:
        if k in ""EU*-"":
            on_left = False
        if k == ""-"":
            continue
        elif k == ""*"":
            keys.append(k)
        elif on_left:
            keys.append(k + ""-"")
        else:
            keys.append(""-"" + k)
    return Stroke(keys)
",elif on_left :,116
"def check(data_dir, decrypter, read_only=False):
    fname = os.path.join(data_dir, DIGEST_NAME)
    if os.path.exists(fname):
        if decrypter is None:
            return False
        f = open(fname, ""rb"")
        s = f.read()
        f.close()
        return decrypter.decrypt(s) == MAGIC_STRING
    else:
        if decrypter is not None:
            if read_only:
                return False
            else:
                s = decrypter.encrypt(MAGIC_STRING)
                f = open(fname, ""wb"")
                f.write(s)
                f.close()
        return True
",if read_only :,198
"def get_sentence(self):
    while True:
        self._seed += 1
        all_files = list(self._all_files)
        if self._shuffle:
            if self._n_gpus > 1:
                random.seed(self._seed)
            random.shuffle(all_files)
        for file_path in all_files:
            for ret in self._load_file(file_path):
                yield ret
        if self._mode == ""test"":
            break
","if self . _mode == ""test"" :",134
"def on_epoch_end(self, batch, logs=None):
    # At the end of every epoch, remask the weights. This ensures that when
    # the model is saved after completion, the weights represent mask*weights.
    weight_mask_ops = []
    for layer in self.prunable_layers:
        if isinstance(layer, pruning_wrapper.PruneLowMagnitude):
            if tf.executing_eagerly():
                layer.pruning_obj.weight_mask_op()
            else:
                weight_mask_ops.append(layer.pruning_obj.weight_mask_op())
    K.batch_get_value(weight_mask_ops)
","if isinstance ( layer , pruning_wrapper . PruneLowMagnitude ) :",166
"def stroke(s):
    keys = []
    on_left = True
    for k in s:
        if k in ""EU*-"":
            on_left = False
        if k == ""-"":
            continue
        elif k == ""*"":
            keys.append(k)
        elif on_left:
            keys.append(k + ""-"")
        else:
            keys.append(""-"" + k)
    return Stroke(keys)
","if k in ""EU*-"" :",116
"def _plot_figure(self, idx):
    with self.renderer.state():
        self.plot.update(idx)
        if self.renderer.fig == ""auto"":
            figure_format = self.renderer.params(""fig"").objects[0]
        else:
            figure_format = self.renderer.fig
        return self.renderer._figure_data(self.plot, figure_format, as_script=True)[0]
","if self . renderer . fig == ""auto"" :",110
"def custom_format(slither, result):
    elements = result[""elements""]
    for element in elements:
        target = element[""additional_fields""][""target""]
        convention = element[""additional_fields""][""convention""]
        if convention == ""l_O_I_should_not_be_used"":
            # l_O_I_should_not_be_used cannot be automatically patched
            logger.info(
                f'The following naming convention cannot be patched: \n{result[""description""]}'
            )
            continue
        _patch(slither, result, element, target)
","if convention == ""l_O_I_should_not_be_used"" :",155
"def refresh(self):
    if self._obj:
        person = self._db.get_person_from_handle(self._obj.get_reference_handle())
        if person:
            frel = str(self._obj.get_father_relation())
            mrel = str(self._obj.get_mother_relation())
            self._title = _(""%(frel)s %(mrel)s"") % {""frel"": frel, ""mrel"": mrel}
            self._value = person.get_primary_name().get_name()
",if person :,136
"def append(self, child):
    if child not in (None, self):
        tag = child_tag(self._tag)
        if tag:
            if isinstance(child, Html):
                if child.tag != tag:
                    child = Html(tag, child)
            elif not child.startswith(""<%s"" % tag):
                child = Html(tag, child)
        super().append(child)
",if tag :,113
"def _forward_main_responses(self):
    while self._should_keep_going():
        line = self._proc.stdout.readline()
        if self._main_backend_is_fresh and self._looks_like_echo(line):
            # In the beginning the backend may echo commands sent to it (perhaps this echo-avoiding trick
            # takes time). Don't forward those lines.
            continue
        if not line:
            break
        with self._response_lock:
            sys.stdout.write(line)
            sys.stdout.flush()
            self._main_backend_is_fresh = False
",if not line :,161
"def forward(self, inputs):
    x = inputs[""image""]
    out = self.conv0(x)
    out = self.downsample0(out)
    blocks = []
    for i, conv_block_i in enumerate(self.darknet_conv_block_list):
        out = conv_block_i(out)
        if i == self.freeze_at:
            out.stop_gradient = True
        if i in self.return_idx:
            blocks.append(out)
        if i < self.num_stages - 1:
            out = self.downsample_list[i](out)
    return blocks
",if i == self . freeze_at :,159
"def check_backslashes(payload):
    # Check for single quotes
    if payload.count(""\\"") >= 15:
        if not settings.TAMPER_SCRIPTS[""backslashes""]:
            if menu.options.tamper:
                menu.options.tamper = menu.options.tamper + "",backslashes""
            else:
                menu.options.tamper = ""backslashes""
        from src.core.tamper import backslashes
        payload = backslashes.tamper(payload)
",if menu . options . tamper :,130
"def __init__(self, config_lists):
    self.lens = len(config_lists)
    self.spaces = []
    for config_list in config_lists:
        if isinstance(config_list, tuple):
            key, config = config_list
        elif isinstance(config_list, str):
            key = config_list
            config = None
        else:
            raise NotImplementedError(
                ""the type of config is Error!!! Please check the config information. Receive the type of config is {}"".format(
                    type(config_list)
                )
            )
        self.spaces.append(self._get_single_search_space(key, config))
    self.init_tokens()
","if isinstance ( config_list , tuple ) :",186
"def _source_tuple(af, address, port):
    # Make a high level source tuple, or return None if address and port
    # are both None
    if address or port:
        if address is None:
            if af == socket.AF_INET:
                address = ""0.0.0.0""
            elif af == socket.AF_INET6:
                address = ""::""
            else:
                raise NotImplementedError(f""unknown address family {af}"")
        return (address, port)
    else:
        return None
",if address is None :,144
"def test_compatibility(self) -> None:
    for expected, user_agent in self.data:
        result = self.client_get(""/compatibility"", HTTP_USER_AGENT=user_agent)
        if expected == ""ok"":
            self.assert_json_success(result)
        elif expected == ""old"":
            self.assert_json_error(result, ""Client is too old"")
        else:
            assert False  # nocoverage
","if expected == ""ok"" :",114
"def __init__(self, parent_element):
    if parent_element.items():
        self.update(dict(parent_element.items()))
    for element in parent_element:
        if len(element) > 0:
            if element.tag == element[0].tag:
                aDict = ListParser(element)
            else:
                aDict = DictParser(element)
            if element.items():
                aDict.update(dict(element.items()))
            self.update({element.tag: aDict})
        elif element.items():
            self.update({element.tag: dict(element.items())})
        else:
            self.update({element.tag: element.text})
",if element . tag == element [ 0 ] . tag :,190
"def delta_page(self, x: float = 0.0, y: float = 0.0) -> None:
    if y.is_integer():
        y = int(y)
        if y == 0:
            pass
        elif y < 0:
            self.page_up(count=-y)
        elif y > 0:
            self.page_down(count=y)
        y = 0
    if x == 0 and y == 0:
        return
    size = self._widget.page().mainFrame().geometry()
    self.delta(int(x * size.width()), int(y * size.height()))
",if y == 0 :,160
"def reader(self, myself):
    ok = True
    line = """"
    while True:
        line = sys.stdin.readline().strip()
        if ok:
            if not line:
                ok = False
                continue
        elif not line:
            break
        else:
            ok = True
        self.Q.append(line)
    os.kill(myself, signal.SIGTERM)
",if ok :,112
"def leave(self, reason=None):
    try:
        if self.id.startswith(""C""):
            log.info(""Leaving channel %s (%s)"", self, self.id)
            self._bot.webclient.channels_leave(channel=self.id)
        else:
            log.info(""Leaving group %s (%s)"", self, self.id)
            self._bot.webclient.groups_leave(channel=self.id)
    except SlackAPIResponseError as e:
        if e.error == ""user_is_bot"":
            raise RoomError(f""Unable to leave channel. {USER_IS_BOT_HELPTEXT}"")
        else:
            raise RoomError(e)
    self._id = None
","if e . error == ""user_is_bot"" :",191
"def wrap_lines(text, cols=60):
    ret = """"
    words = re.split(""(\s+)"", text)
    linelen = 0
    for w in words:
        if linelen + len(w) > cols - 1:
            ret += "" \\\n""
            ret += ""   ""
            linelen = 0
        if linelen == 0 and w.strip() == """":
            continue
        ret += w
        linelen += len(w)
    return ret
",if linelen + len ( w ) > cols - 1 :,123
"def transport_vmware_guestinfo():
    rpctool = ""vmware-rpctool""
    not_found = None
    if not subp.which(rpctool):
        return not_found
    cmd = [rpctool, ""info-get guestinfo.ovfEnv""]
    try:
        out, _err = subp.subp(cmd)
        if out:
            return out
        LOG.debug(""cmd %s exited 0 with empty stdout: %s"", cmd, out)
    except subp.ProcessExecutionError as e:
        if e.exit_code != 1:
            LOG.warning(""%s exited with code %d"", rpctool, e.exit_code)
            LOG.debug(e)
    return not_found
",if e . exit_code != 1 :,196
"def handle_noargs(self, **options):
    # Inspired by Postfix's ""postconf -n"".
    from django.conf import settings, global_settings
    # Because settings are imported lazily, we need to explicitly load them.
    settings._setup()
    user_settings = module_to_dict(settings._wrapped)
    default_settings = module_to_dict(global_settings)
    output = []
    for key in sorted(user_settings.keys()):
        if key not in default_settings:
            output.append(""%s = %s  ###"" % (key, user_settings[key]))
        elif user_settings[key] != default_settings[key]:
            output.append(""%s = %s"" % (key, user_settings[key]))
    return ""\n"".join(output)
",if key not in default_settings :,196
"def channel_sizes(self):
    """"""List of channel sizes: [(width, height)].""""""
    sizes = []
    for channel in self.channel_info:
        if channel.id == ChannelID.USER_LAYER_MASK:
            sizes.append((self.mask_data.width, self.mask_data.height))
        elif channel.id == ChannelID.REAL_USER_LAYER_MASK:
            sizes.append((self.mask_data.real_width, self.mask_data.real_height))
        else:
            sizes.append((self.width, self.height))
    return sizes
",elif channel . id == ChannelID . REAL_USER_LAYER_MASK :,151
"def get(self, key, default=None, version=None):
    fname = self._key_to_file(key, version)
    try:
        with io.open(fname, ""rb"") as f:
            if not self._is_expired(f):
                return pickle.loads(zlib.decompress(f.read()))
    except IOError as e:
        if e.errno != errno.ENOENT:
            raise
    return default
",if e . errno != errno . ENOENT :,112
"def check_grads(grads_and_vars):
    has_nan_ops = []
    amax_ops = []
    for grad, _ in grads_and_vars:
        if grad is not None:
            if isinstance(grad, tf.IndexedSlices):
                x = grad.values
            else:
                x = grad
            has_nan_ops.append(tf.reduce_any(tf.is_nan(x)))
            amax_ops.append(tf.reduce_max(tf.abs(x)))
    has_nan = tf.reduce_any(has_nan_ops)
    amax = tf.reduce_max(amax_ops)
    return has_nan, amax
",if grad is not None :,179
"def daily(self, component):
    with component.repository.lock:
        path = self.get_linguas_path(component)
        if self.sync_linguas(component, path):
            self.commit_and_push(component, [path])
","if self . sync_linguas ( component , path ) :",67
"def _set_posonly_args_def(self, argmts, vals):
    for v in vals:
        argmts.posonlyargs.append(v[""arg""])
        d = v[""default""]
        if d is not None:
            argmts.defaults.append(d)
        elif argmts.defaults:
            self._set_error(""non-default argument follows default argument"")
",elif argmts . defaults :,97
"def isOrHasChild(parent, child):
    while child:
        if compare(parent, child):
            return True
        child = child.parentNode
        if not child:
            return False
        if child.nodeType != 1:
            child = None
    return False
",if child . nodeType != 1 :,76
"def Proc2(IntParIO):
    IntLoc = IntParIO + 10
    while 1:
        if Char1Glob == ""A"":
            IntLoc = IntLoc - 1
            IntParIO = IntLoc - IntGlob
            EnumLoc = Ident1
        if EnumLoc == Ident1:
            break
    return IntParIO
","if Char1Glob == ""A"" :",90
"def _GetParserChains(self, events):
    """"""Return a dict with a plugin count given a list of events.""""""
    parser_chains = {}
    for event in events:
        parser_chain = getattr(event, ""parser"", None)
        if not parser_chain:
            continue
        if parser_chain in parser_chains:
            parser_chains[parser_chain] += 1
        else:
            parser_chains[parser_chain] = 1
    return parser_chains
",if parser_chain in parser_chains :,122
"def _url_encode_impl(obj, charset, encode_keys, sort, key):
    iterable = sdict()
    for key, values in obj.items():
        if not isinstance(values, list):
            values = [values]
        iterable[key] = values
    if sort:
        iterable = sorted(iterable, key=key)
    for key, values in iterable.items():
        for value in values:
            if value is None:
                continue
            if not isinstance(key, bytes):
                key = str(key).encode(charset)
            if not isinstance(value, bytes):
                value = str(value).encode(charset)
            yield url_quote_plus(key) + ""="" + url_quote_plus(value)
",if value is None :,198
"def getZoneOffset(d):
    zoffs = 0
    try:
        if d[""zulu""] == None:
            zoffs = 60 * int(d[""tzhour""]) + int(d[""tzminute""])
            if d[""tzsign""] != ""-"":
                zoffs = -zoffs
    except TypeError:
        pass
    return zoffs
","if d [ ""tzsign"" ] != ""-"" :",92
"def run(self):
    predictor = DefaultPredictor(self.cfg)
    while True:
        task = self.task_queue.get()
        if isinstance(task, AsyncPredictor._StopToken):
            break
        idx, data = task
        result = predictor(data)
        self.result_queue.put((idx, result))
","if isinstance ( task , AsyncPredictor . _StopToken ) :",86
"def _VarRefOrWord(node, dynamic_arith):
    # type: (arith_expr_t, bool) -> bool
    with tagswitch(node) as case:
        if case(arith_expr_e.VarRef):
            return True
        elif case(arith_expr_e.Word):
            if dynamic_arith:
                return True
    return False
",if dynamic_arith :,96
"def command(self, reset=True, wait=True, wait_all=False, quiet=False):
    try:
        if self._idx(reset=reset, wait=wait, wait_all=wait_all, quiet=quiet):
            return self._success(_(""Loaded metadata index""))
        else:
            return self._error(_(""Failed to load metadata index""))
    except IOError:
        return self._error(_(""Failed to decrypt configuration, "" ""please log in!""))
","if self . _idx ( reset = reset , wait = wait , wait_all = wait_all , quiet = quiet ) :",113
"def init_weights(self):
    for module in self.decoder.modules():
        if isinstance(module, (nn.Linear, nn.Embedding)):
            module.weight.data.normal_(mean=0.0, std=0.02)
        elif isinstance(module, nn.LayerNorm):
            module.bias.data.zero_()
            module.weight.data.fill_(1.0)
        if isinstance(module, nn.Linear) and module.bias is not None:
            module.bias.data.zero_()
    for p in self.generator.parameters():
        if p.dim() > 1:
            xavier_uniform_(p)
        else:
            p.data.zero_()
","if isinstance ( module , nn . Linear ) and module . bias is not None :",179
"def write_conditional_formatting(worksheet):
    """"""Write conditional formatting to xml.""""""
    wb = worksheet.parent
    for range_string, rules in iteritems(worksheet.conditional_formatting.cf_rules):
        cf = Element(""conditionalFormatting"", {""sqref"": range_string})
        for rule in rules:
            if rule.dxf is not None:
                if rule.dxf != DifferentialStyle():
                    rule.dxfId = len(wb._differential_styles)
                    wb._differential_styles.append(rule.dxf)
            cf.append(rule.to_tree())
        yield cf
",if rule . dxf is not None :,164
"def _format_changelog(self, changelog):
    """"""Format the changelog correctly and convert it to a list of strings""""""
    if not changelog:
        return changelog
    new_changelog = []
    for line in changelog.strip().split(""\n""):
        line = line.strip()
        if line[0] == ""*"":
            new_changelog.extend(["""", line])
        elif line[0] == ""-"":
            new_changelog.append(line)
        else:
            new_changelog.append(""  "" + line)
    # strip trailing newline inserted by first changelog entry
    if not new_changelog[0]:
        del new_changelog[0]
    return new_changelog
","if line [ 0 ] == ""*"" :",168
"def __prep_write_total(self, comments, main, fallback, single):
    lower = self.as_lowercased()
    for k in [main, fallback, single]:
        if k in comments:
            del comments[k]
    if single in lower:
        parts = lower[single].split(""/"", 1)
        if parts[0]:
            comments[single] = [parts[0]]
        if len(parts) > 1:
            comments[main] = [parts[1]]
    if main in lower:
        comments[main] = lower.list(main)
    if fallback in lower:
        if main in comments:
            comments[fallback] = lower.list(fallback)
        else:
            comments[main] = lower.list(fallback)
",if parts [ 0 ] :,196
"def __str__(self):
    result = []
    for mask, quality in self._parsed:
        if quality != 1:
            mask = ""%s;q=%0.*f"" % (
                mask,
                min(len(str(quality).split(""."")[1]), 3),
                quality,
            )
        result.append(mask)
    return "", "".join(result)
",if quality != 1 :,104
"def allprocs(self):
    common.set_plugin_members(self)
    tasksaddr = self.addr_space.profile.get_symbol(""_tasks"")
    queue_entry = obj.Object(""queue_entry"", offset=tasksaddr, vm=self.addr_space)
    seen = [tasksaddr]
    for task in queue_entry.walk_list(list_head=tasksaddr):
        if task.bsd_info and task.obj_offset not in seen:
            proc = task.bsd_info.dereference_as(""proc"")
            yield proc
        seen.append(task.obj_offset)
",if task . bsd_info and task . obj_offset not in seen :,152
"def __walk_dir_tree(self, dirname):
    dir_list = []
    self.__logger.debug(""__walk_dir_tree. START dir=%s"", dirname)
    for f in os.listdir(dirname):
        current = os.path.join(dirname, f)
        if os.path.isfile(current) and f.endswith(""py""):
            if self.module_registrant:
                self._load_py_from_file(current)
            dir_list.append(current)
        elif os.path.isdir(current):
            ret = self.__walk_dir_tree(current)
            if ret:
                dir_list.append((f, ret))
    return dir_list
",elif os . path . isdir ( current ) :,184
"def get_code(self, address: Address) -> bytes:
    validate_canonical_address(address, title=""Storage Address"")
    code_hash = self.get_code_hash(address)
    if code_hash == EMPTY_SHA3:
        return b""""
    else:
        try:
            return self._journaldb[code_hash]
        except KeyError:
            raise MissingBytecode(code_hash) from KeyError
        finally:
            if code_hash in self._get_accessed_node_hashes():
                self._accessed_bytecodes.add(address)
",if code_hash in self . _get_accessed_node_hashes ( ) :,150
"def _strftime(value):
    if datetime:
        if isinstance(value, datetime.datetime):
            return ""%04d%02d%02dT%02d:%02d:%02d"" % (
                value.year,
                value.month,
                value.day,
                value.hour,
                value.minute,
                value.second,
            )
    if not isinstance(value, (TupleType, time.struct_time)):
        if value == 0:
            value = time.time()
        value = time.localtime(value)
    return ""%04d%02d%02dT%02d:%02d:%02d"" % value[:6]
",if value == 0 :,182
"def _read_mol2_records(filename):
    lines = []
    start = True
    with open(filename) as handle:
        for line in handle:
            if line.startswith(""@<TRIPOS>MOLECULE""):
                if start:
                    start = False
                else:
                    yield lines
                    lines = []
            lines.append(line)
",if start :,109
"def set_column_strategy(self, attrs, strategy, opts=None, opts_only=False):
    strategy = self._coerce_strat(strategy)
    self.is_class_strategy = False
    for attr in attrs:
        cloned = self._generate()
        cloned.strategy = strategy
        cloned._generate_path(self.path, attr, ""column"")
        cloned.propagate_to_loaders = True
        if opts:
            cloned.local_opts.update(opts)
        if opts_only:
            cloned.is_opts_only = True
        cloned._set_path_strategy()
    self.is_class_strategy = False
",if opts_only :,172
"def decryptBlock(self, encryptedBlock):
    """"""Decrypt a single block""""""
    if self.decryptBlockCount == 0:  # first call, process IV
        if self.iv == None:  # auto decrypt IV?
            self.prior_CT_block = encryptedBlock
            return """"
        else:
            assert len(self.iv) == self.blockSize, ""Bad IV size on CBC decryption""
            self.prior_CT_block = self.iv
    dct = self.baseCipher.decryptBlock(encryptedBlock)
    """""" XOR the prior decrypted CT with the prior CT """"""
    dct_XOR_priorCT = xor(self.prior_CT_block, dct)
    self.prior_CT_block = encryptedBlock
    return dct_XOR_priorCT
",if self . iv == None :,175
"def frontend_visible_config(config_dict):
    visible_dict = {}
    for name in CLIENT_WHITELIST:
        if name.lower().find(""secret"") >= 0:
            raise Exception(""Cannot whitelist secrets: %s"" % name)
        if name in config_dict:
            visible_dict[name] = config_dict.get(name, None)
        if ""ENTERPRISE_LOGO_URL"" in config_dict:
            visible_dict[""BRANDING""] = visible_dict.get(""BRANDING"", {})
            visible_dict[""BRANDING""][""logo""] = config_dict[""ENTERPRISE_LOGO_URL""]
    return visible_dict
","if ""ENTERPRISE_LOGO_URL"" in config_dict :",171
"def write(self, s):
    if self.closed:
        raise ValueError(""write to closed file"")
    if type(s) not in (unicode, str, bytearray):
        # See issue #19481
        if isinstance(s, unicode):
            s = unicode.__getitem__(s, slice(None))
        elif isinstance(s, str):
            s = str.__str__(s)
        elif isinstance(s, bytearray):
            s = bytearray.__str__(s)
        else:
            raise TypeError(""must be string, not "" + type(s).__name__)
    return self.shell.write(s, self.tags)
","elif isinstance ( s , bytearray ) :",161
"def __get_kb_shortcuts(directory, filename, default_shortcuts, min_shortcuts):
    shortcutstr, source = __read_first_in_directory_tree(directory, filename)
    if shortcutstr is None:
        shortcutstr = __read_or_default(filename, default_shortcuts)
        if shortcutstr == default_shortcuts:
            source = ""[default kb_shortcuts]""
        else:
            source = filename
    kb_shortcuts = __parse_kb_shortcuts(shortcutstr, min_shortcuts, source)
    return kb_shortcuts
",if shortcutstr == default_shortcuts :,133
"def demo():
    d = StatusProgressDialog(""A Demo"", ""Doing something..."")
    import win32api
    for i in range(100):
        if i == 50:
            d.SetText(""Getting there..."")
        if i == 90:
            d.SetText(""Nearly done..."")
        win32api.Sleep(20)
        d.Tick()
    d.Close()
",if i == 50 :,99
"def __getattribute__(self, item):
    try:
        val = self[item]
        if isinstance(val, str):
            val = import_string(val)
        elif isinstance(val, (list, tuple)):
            val = [import_string(v) if isinstance(v, str) else v for v in val]
        self[item] = val
    except KeyError:
        val = super(ObjDict, self).__getattribute__(item)
    return val
","elif isinstance ( val , ( list , tuple ) ) :",118
"def clear(self, key: Optional[str] = None):
    with self.lock:
        if key is not None:
            try:
                rv = self.data[key]
                self._heap_acc.remove((rv.acc, key))
                self._heap_exp.remove((rv.exp, key))
                del self.data[key]
                return
            except Exception:
                return
        self.data.clear()
        self._heap_acc = []
        self._heap_exp = []
",if key is not None :,148
"def resolve(self, path):
    match = self.regex.search(path)
    if match:
        # If there are any named groups, use those as kwargs, ignoring
        # non-named groups. Otherwise, pass all non-named arguments as
        # positional arguments.
        kwargs = match.groupdict()
        if kwargs:
            args = ()
        else:
            args = match.groups()
        # In both cases, pass any extra_kwargs as **kwargs.
        kwargs.update(self.default_args)
        return ResolverMatch(self.callback, args, kwargs, self.name)
",if kwargs :,154
"def check_selected(menu, path):
    selected = False
    if ""url"" in menu:
        chop_index = menu[""url""].find(""?"")
        if chop_index == -1:
            selected = path.startswith(menu[""url""])
        else:
            selected = path.startswith(menu[""url""][:chop_index])
    if ""menus"" in menu:
        for m in menu[""menus""]:
            _s = check_selected(m, path)
            if _s:
                selected = True
    if selected:
        menu[""selected""] = True
    return selected
",if _s :,153
"def check_match(word, word_list):
    matches = set()
    not_matches = set()
    for word2 in word_list:
        match = truncate_qgram(word, word2)
        if match > 0.6:
            matches.add((word, word2))
        else:
            not_matches.add((word, word2))
    return matches, not_matches
",if match > 0.6 :,102
"def _fatal_error(self, exc, message=""Fatal error on pipe transport""):
    # should be called by exception handler only
    if isinstance(exc, (BrokenPipeError, ConnectionResetError)):
        if self._loop.get_debug():
            logger.debug(""%r: %s"", self, message, exc_info=True)
    else:
        self._loop.call_exception_handler(
            {
                ""message"": message,
                ""exception"": exc,
                ""transport"": self,
                ""protocol"": self._protocol,
            }
        )
    self._close(exc)
",if self . _loop . get_debug ( ) :,158
"def remove_existing_header(contents):
    ""remove existing legal header, if any""
    retval = []
    skipping = False
    start_pattern = re.compile(r""^(/[*]BEGIN_LEGAL)|(#BEGIN_LEGAL)"")
    stop_pattern = re.compile(r""^[ ]*(END_LEGAL[ ]?[*]/)|(#[ ]*END_LEGAL)"")
    for line in contents:
        if start_pattern.match(line):
            skipping = True
        if skipping == False:
            retval.append(line)
        if stop_pattern.match(line):
            skipping = False
    return retval
",if stop_pattern . match ( line ) :,164
"def load_model(self, model_dict):
    model_param = None
    model_meta = None
    for _, value in model_dict[""model""].items():
        for model in value:
            if model.endswith(""Meta""):
                model_meta = value[model]
            if model.endswith(""Param""):
                model_param = value[model]
    LOGGER.info(""load model"")
    self.set_model_meta(model_meta)
    self.set_model_param(model_param)
    self.loss = self.get_loss_function()
","if model . endswith ( ""Param"" ) :",148
"def __call__(self, exc_type, exc_value, exc_tb):
    if not isinstance(exc_value, SystemExit):
        enriched_tb = add_missing_qt_frames(exc_tb) if exc_tb else exc_tb
        for handler in self._handlers:
            if handler.handle(exc_type, exc_value, enriched_tb):
                break
","if handler . handle ( exc_type , exc_value , enriched_tb ) :",100
"def skip_to_semicolon(s, i):
    n = len(s)
    while i < n:
        c = s[i]
        if c == "";"":
            return i
        elif c == ""'"" or c == '""':
            i = g.skip_string(s, i)
        elif g.match(s, i, ""//""):
            i = g.skip_to_end_of_line(s, i)
        elif g.match(s, i, ""/*""):
            i = g.skip_block_comment(s, i)
        else:
            i += 1
    return i
","elif g . match ( s , i , ""/*"" ) :",161
"def validate(self, signature, timestamp, nonce):
    if not self.token:
        raise WeixinMsgError(""weixin token is missing"")
    if self.expires_in:
        try:
            timestamp = int(timestamp)
        except ValueError:
            return False
        delta = time.time() - timestamp
        if delta < 0 or delta > self.expires_in:
            return False
    values = [self.token, str(timestamp), str(nonce)]
    s = """".join(sorted(values))
    hsh = hashlib.sha1(s.encode(""utf-8"")).hexdigest()
    return signature == hsh
",if delta < 0 or delta > self . expires_in :,161
"def terminate(self):
    """"""Terminates process (sends SIGTERM)""""""
    if not self._proc is None:
        if IS_WINDOWS:
            # Windows
            self._proc.terminate()
        elif HAS_SUBPROCESS:
            # Gio.Subprocess
            self._proc.send_signal(15)
        else:
            # subprocess.Popen
            self._proc.terminate()
        self._proc = None
        if IS_WINDOWS:
            self._stdout.close()
        self._cancel.cancel()
",if IS_WINDOWS :,141
"def clear_bijector(bijector, _, state):
    if not isinstance(bijector, tfp.bijectors.Bijector):
        return  # skip submodules that are not bijectors
    _clear_bijector_cache(bijector)
    if isinstance(bijector, tfp.bijectors.Chain):
        # recursively clear caches of sub-bijectors
        for m in bijector.submodules:
            if isinstance(m, tfp.bijectors.Bijector):
                _clear_bijector_cache(m)
    return state
","if isinstance ( m , tfp . bijectors . Bijector ) :",148
"def sanitize_args(a):
    try:
        args, kwargs = a
        if isinstance(args, tuple) and isinstance(kwargs, dict):
            return args, dict(kwargs)
    except (TypeError, ValueError):
        args, kwargs = (), {}
    if a is not None:
        if isinstance(a, dict):
            args = tuple()
            kwargs = a
        elif isinstance(a, tuple):
            if isinstance(a[-1], dict):
                args, kwargs = a[0:-1], a[-1]
            else:
                args = a
                kwargs = {}
    return args, kwargs
","elif isinstance ( a , tuple ) :",168
"def do_DELE(self, path):
    """"""Delete the specified file.""""""
    try:
        path = self.ftp_path(path)
        if not self.config.vfs.isfile(path):
            self.respond(b""550 Failed to delete file."")
        else:
            with self.config.vfs.check_access(path=path, user=self._uid, perms=""w""):
                self.config.vfs.remove(path)
                self.respond(b""250 File removed."")
    except FSOperationNotPermitted:
        self.respond(b""500 Operation not permitted."")
    except (fs.errors.FSError, FilesystemError, FTPPrivilegeException):
        self.respond(b""550 Failed to delete file."")
",if not self . config . vfs . isfile ( path ) :,194
"def _get_conn(self):
    """"""Get ServerProxy instance""""""
    if self.username and self.password:
        if self.scheme == ""scgi"":
            raise NotImplementedError()
        secure = self.scheme == ""https""
        return self.sp(
            self.uri,
            transport=BasicAuthTransport(secure, self.username, self.password),
            **self.sp_kwargs
        )
    return self.sp(self.uri, **self.sp_kwargs)
","if self . scheme == ""scgi"" :",126
"def output(self):
    """"""Transform self into a list of (name, value) tuples.""""""
    header_list = []
    for k, v in self.items():
        if isinstance(k, unicodestr):
            k = self.encode(k)
        if not isinstance(v, basestring):
            v = str(v)
        if isinstance(v, unicodestr):
            v = self.encode(v)
        # See header_translate_* constants above.
        # Replace only if you really know what you're doing.
        k = k.translate(header_translate_table, header_translate_deletechars)
        v = v.translate(header_translate_table, header_translate_deletechars)
        header_list.append((k, v))
    return header_list
","if isinstance ( k , unicodestr ) :",197
"def gprv_implicit_orax(ii):
    for i, op in enumerate(_gen_opnds(ii)):
        if i == 0:
            if op.name == ""REG0"" and op_luf(op, ""GPRv_SB""):
                continue
            else:
                return False
        elif i == 1:
            if op.name == ""REG1"" and op_luf(op, ""OrAX""):
                continue
            else:
                return False
        else:
            return False
    return True
",if i == 0 :,151
"def one_xmm_reg_imm8(ii):  # also allows SSE4 2-imm8 instr
    i, j, n = 0, 0, 0
    for op in _gen_opnds(ii):
        if op_reg(op) and op_xmm(op):
            n += 1
        elif op_imm8(op):
            i += 1
        elif op_imm8_2(op):
            j += 1
        else:
            return False
    return n == 1 and i == 1 and j <= 1
",if op_reg ( op ) and op_xmm ( op ) :,141
"def pa(s, l, tokens):
    for attrName, attrValue in attrs:
        if attrName not in tokens:
            raise ParseException(s, l, ""no matching attribute "" + attrName)
        if attrValue != withAttribute.ANY_VALUE and tokens[attrName] != attrValue:
            raise ParseException(
                s,
                l,
                ""attribute '%s' has value '%s', must be '%s'""
                % (attrName, tokens[attrName], attrValue),
            )
",if attrName not in tokens :,140
"def __code_color(self, code):
    if code in self.last_dist.keys():
        if int(code) == 0:
            return self.screen.markup.GREEN
        elif int(code) == 314:
            return self.screen.markup.MAGENTA
        else:
            return self.screen.markup.RED
    else:
        return """"
",elif int ( code ) == 314 :,97
"def loop_check(self):
    in_loop = []
    # Add the tag for dfs check
    for node in self.nodes:
        node.dfs_loop_status = ""DFS_UNCHECKED""
    # Now do the job
    for node in self.nodes:
        # Run the dfs only if the node has not been already done */
        if node.dfs_loop_status == ""DFS_UNCHECKED"":
            self.dfs_loop_search(node)
        # If LOOP_INSIDE, must be returned
        if node.dfs_loop_status == ""DFS_LOOP_INSIDE"":
            in_loop.append(node)
    # Remove the tag
    for node in self.nodes:
        del node.dfs_loop_status
    return in_loop
","if node . dfs_loop_status == ""DFS_LOOP_INSIDE"" :",199
"def _append_modifier(code, modifier):
    if modifier == ""euro"":
        if ""."" not in code:
            return code + "".ISO8859-15""
        _, _, encoding = code.partition(""."")
        if encoding in (""ISO8859-15"", ""UTF-8""):
            return code
        if encoding == ""ISO8859-1"":
            return _replace_encoding(code, ""ISO8859-15"")
    return code + ""@"" + modifier
","if ""."" not in code :",115
"def propagate_touch_to_touchable_widgets(self, touch, touch_event, *args):
    triggered = False
    for i in self._touchable_widgets:
        if i.collide_point(touch.x, touch.y):
            triggered = True
            if touch_event == ""down"":
                i.on_touch_down(touch)
            elif touch_event == ""move"":
                i.on_touch_move(touch, *args)
            elif touch_event == ""up"":
                i.on_touch_up(touch)
    return triggered
","if touch_event == ""down"" :",154
"def body(self):
    order = [
        ""ok_header"",
        ""affected_rows"",
        ""last_insert_id"",
        ""server_status"",
        ""warning_count"",
        ""state_track"",
        ""info"",
    ]
    string = b""""
    for key in order:
        item = getattr(self, key)
        section_pack = b""""
        if item is None:
            continue
        elif isinstance(item, bytes):
            section_pack = item
        else:
            section_pack = getattr(self, key).toStringPacket()
        string += section_pack
    self.setBody(string)
    return self._body
",if item is None :,182
"def get_opnd_types_short(ii):
    types = []
    for op in _gen_opnds(ii):
        if op.oc2:
            types.append(op.oc2)
        elif op_luf_start(op, ""GPRv""):
            types.append(""v"")
        elif op_luf_start(op, ""GPRz""):
            types.append(""z"")
        elif op_luf_start(op, ""GPRy""):
            types.append(""y"")
        else:
            die(""Unhandled op type {}"".format(op))
    return types
","elif op_luf_start ( op , ""GPRz"" ) :",161
"def load_name(self, name):
    if name in self.args:
        index = self.args[name]
        if index is None:
            self.add_opcodes(JavaOpcodes.ALOAD_2(), java.Map.get(name))
        else:
            self.add_opcodes(
                JavaOpcodes.ALOAD_1(),
                java.Array.get(index),
            )
    else:
        self.add_opcodes(
            ALOAD_name(""#module""),
            python.Object.get_attribute(name),
        )
",if index is None :,157
"def get_field_type(self, name):
    fkey = (name, self.dummy)
    target = None
    op, name = name.split(""_"", 1)
    if op in {""delete"", ""insert"", ""update""}:
        target = super().get_field_type(name)
        if target is None:
            module, edb_name = self.get_module_and_name(name)
            target = self.edb_schema.get((module, edb_name), None)
            if target is not None:
                target = self.convert_edb_to_gql_type(target)
    self._fields[fkey] = target
    return target
",if target is not None :,170
"def _parse_lines(self, lines):
    for line in lines:
        self.size += len(line)
        words = line.strip().split(""\t"")
        if len(words) > 1:
            wset = set(words[1:])
            if words[0] in self.WORDS:
                self.WORDS[words[0]] |= wset
            else:
                self.WORDS[words[0]] = wset
",if len ( words ) > 1 :,118
"def get_new_id(self) -> str:
    with db.session.no_autoflush:
        identifier = self.issued_at.strftime(""%Y%mU-"") + ""%06d"" % (
            EventInvoice.query.count() + 1
        )
        count = EventInvoice.query.filter_by(identifier=identifier).count()
        if count == 0:
            return identifier
        return self.get_new_id()
",if count == 0 :,114
"def complete_use(self, text, *args, **kwargs):
    if text:
        all_possible_matches = filter(
            lambda x: x.startswith(text), self.main_modules_dirs
        )
        matches = set()
        for match in all_possible_matches:
            head, sep, tail = match[len(text) :].partition(""."")
            if not tail:
                sep = """"
            matches.add("""".join((text, head, sep)))
        return list(matches)
    else:
        return self.main_modules_dirs
",if not tail :,149
"def get_arg_list_scalar_arg_dtypes(arg_types):
    result = []
    for arg_type in arg_types:
        if isinstance(arg_type, ScalarArg):
            result.append(arg_type.dtype)
        elif isinstance(arg_type, VectorArg):
            result.append(None)
            if arg_type.with_offset:
                result.append(np.int64)
        else:
            raise RuntimeError(""arg type not understood: %s"" % type(arg_type))
    return result
","elif isinstance ( arg_type , VectorArg ) :",142
"def psea(pname):
    """"""Parse PSEA output file.""""""
    fname = run_psea(pname)
    start = 0
    ss = """"
    with open(fname) as fp:
        for l in fp:
            if l[0:6] == "">p-sea"":
                start = 1
                continue
            if not start:
                continue
            if l[0] == ""\n"":
                break
            ss = ss + l[0:-1]
    return ss
","if l [ 0 : 6 ] == "">p-sea"" :",142
"def pad_with_zeros(logits, labels):
    """"""Pad labels on the length dimension to match logits length.""""""
    with tf.name_scope(""pad_with_zeros"", values=[logits, labels]):
        logits, labels = pad_to_same_length(logits, labels)
        if len(labels.shape) == 3:  # 2-d labels.
            logits, labels = pad_to_same_length(logits, labels, axis=2)
        return logits, labels
",if len ( labels . shape ) == 3 :,117
"def set_rating(self, value, songs, librarian):
    count = len(songs)
    if count > 1 and config.getboolean(""browsers"", ""rating_confirm_multiple""):
        parent = qltk.get_menu_item_top_parent(self)
        dialog = ConfirmRateMultipleDialog(parent, _(""Change _Rating""), count, value)
        if dialog.run() != Gtk.ResponseType.YES:
            return
    for song in songs:
        song[""~#rating""] = value
    librarian.changed(songs)
",if dialog . run ( ) != Gtk . ResponseType . YES :,134
"def test_schema_plugin_name_mismatch(self):
    # todo iterate over all clouds not just aws resources
    for k, v in manager.resources.items():
        for fname, f in v.filter_registry.items():
            if fname in (""or"", ""and"", ""not""):
                continue
            self.assertIn(fname, f.schema[""properties""][""type""][""enum""])
        for aname, a in v.action_registry.items():
            self.assertIn(aname, a.schema[""properties""][""type""][""enum""])
","if fname in ( ""or"" , ""and"" , ""not"" ) :",135
"def run(self, elem):
    """"""Inline check for attrs at start of tail.""""""
    if elem.tail:
        m = self.INLINE_RE.match(elem.tail)
        if m:
            self.assign_attrs(elem, m.group(1))
            elem.tail = elem.tail[m.end() :]
",if m :,86
"def _traverse(op):
    if topi.tag.is_broadcast(op.tag):
        if not op.same_as(output.op):
            if not op.axis:
                const_ops.append(op)
            else:
                ewise_ops.append(op)
        for tensor in op.input_tensors:
            if isinstance(tensor.op, tvm.te.PlaceholderOp):
                ewise_inputs.append((op, tensor))
            else:
                _traverse(tensor.op)
    else:
        assert op.tag == ""dense_pack""
        dense_res.append(op)
",if not op . axis :,174
"def toPostArgs(self):
    """"""Return all arguments with openid. in front of namespaced arguments.""""""
    args = {}
    # Add namespace definitions to the output
    for ns_uri, alias in self.namespaces.iteritems():
        if self.namespaces.isImplicit(ns_uri):
            continue
        if alias == NULL_NAMESPACE:
            ns_key = ""openid.ns""
        else:
            ns_key = ""openid.ns."" + alias
        args[ns_key] = ns_uri
    for (ns_uri, ns_key), value in self.args.iteritems():
        key = self.getKey(ns_uri, ns_key)
        args[key] = value.encode(""UTF-8"")
    return args
",if self . namespaces . isImplicit ( ns_uri ) :,190
"def test_issue_530_async(self):
    try:
        rtm_client = RTMClient(token=""I am not a token"", run_async=True)
        await rtm_client.start()
        self.fail(""Raising an error here was expected"")
    except Exception as e:
        self.assertEqual(
            ""The request to the Slack API failed.\n""
            ""The server responded with: {'ok': False, 'error': 'invalid_auth'}"",
            str(e),
        )
    finally:
        if not rtm_client._stopped:
            rtm_client.stop()
",if not rtm_client . _stopped :,163
"def TryMerge(self, d):
    while d.avail() > 0:
        tt = d.getVarInt32()
        if tt == 8:
            self.set_format(d.getVarInt32())
            continue
        if tt == 18:
            self.add_path(d.getPrefixedString())
            continue
        if tt == 0:
            raise ProtocolBuffer.ProtocolBufferDecodeError
        d.skipData(tt)
",if tt == 0 :,120
"def _iterate_files(self, files, root, include_checksums, relpath):
    file_list = {}
    for file in files:
        exclude = False
        # exclude defined filename patterns
        for pattern in S3Sync.exclude_files:
            if fnmatch.fnmatch(file, pattern):
                exclude = True
                break
        if not exclude:
            full_path = root + ""/"" + file
            if include_checksums:
                # get checksum
                checksum = self._hash_file(full_path)
            else:
                checksum = """"
            file_list[relpath + file] = [full_path, checksum]
    return file_list
","if fnmatch . fnmatch ( file , pattern ) :",184
"def globs_relative_to_buildroot(self):
    buildroot = get_buildroot()
    globs = []
    for bundle in self.bundles:
        fileset = bundle.fileset
        if fileset is None:
            continue
        elif hasattr(fileset, ""filespec""):
            globs += bundle.fileset.filespec[""globs""]
        else:
            # NB(nh): filemap is an OrderedDict, so this ordering is stable.
            globs += [fast_relpath(f, buildroot) for f in bundle.filemap.keys()]
    super_globs = super().globs_relative_to_buildroot()
    if super_globs:
        globs += super_globs[""globs""]
    return {""globs"": globs}
",if fileset is None :,187
"def __getstate__(self):
    state = super(_ExpressionBase, self).__getstate__()
    for i in _ExpressionBase.__pickle_slots__:
        state[i] = getattr(self, i)
    if safe_mode:
        state[""_parent_expr""] = None
        if self._parent_expr is not None:
            _parent_expr = self._parent_expr()
            if _parent_expr is not None:
                state[""_parent_expr""] = _parent_expr
    return state
",if _parent_expr is not None :,126
"def content_state_equal(v1, v2):
    ""Test whether two contentState structures are equal, ignoring 'key' properties""
    if type(v1) != type(v2):
        return False
    if isinstance(v1, dict):
        if set(v1.keys()) != set(v2.keys()):
            return False
        return all(k == ""key"" or content_state_equal(v, v2[k]) for k, v in v1.items())
    elif isinstance(v1, list):
        if len(v1) != len(v2):
            return False
        return all(content_state_equal(a, b) for a, b in zip(v1, v2))
    else:
        return v1 == v2
",if len ( v1 ) != len ( v2 ) :,194
"def process_qemu_job(
    file_path: str, arch_suffix: str, root_path: Path, results_dict: dict, uid: str
):
    result = check_qemu_executability(file_path, arch_suffix, root_path)
    if result:
        if uid in results_dict:
            tmp_dict = dict(results_dict[uid][""results""])
            tmp_dict.update({arch_suffix: result})
        else:
            tmp_dict = {arch_suffix: result}
        results_dict[uid] = {""path"": file_path, ""results"": tmp_dict}
",if uid in results_dict :,158
"def _eq_meet(a, b):
    a_dtype, b_dtype = _dtype(a), _dtype(b)
    if a_dtype != b_dtype:
        higher_dtype = dtypes.promote_types(a_dtype, b_dtype)
        if higher_dtype == a_dtype:
            a = convert_element_type(a, b_dtype)
        else:
            b = convert_element_type(b, a_dtype)
    return eq(a, b)
",if higher_dtype == a_dtype :,125
"def _assign(self, trans, code):
    try:
        if ""-"" in code:
            trans.order = self.order_qs().get(
                code=code.rsplit(""-"", 1)[1], event__slug__iexact=code.rsplit(""-"", 1)[0]
            )
        else:
            trans.order = self.order_qs().get(code=code.rsplit(""-"", 1)[-1])
    except Order.DoesNotExist:
        return JsonResponse({""status"": ""error"", ""message"": _(""Unknown order code"")})
    else:
        return self._retry(trans)
","if ""-"" in code :",144
"def _recalculate(self):
    # If the parent's path has changed, recalculate _path
    parent_path = tuple(self._get_parent_path())  # Make a copy
    if parent_path != self._last_parent_path:
        spec = self._path_finder(self._name, parent_path)
        # Note that no changes are made if a loader is returned, but we
        #  do remember the new parent path
        if spec is not None and spec.loader is None:
            if spec.submodule_search_locations:
                self._path = spec.submodule_search_locations
        self._last_parent_path = parent_path  # Save the copy
    return self._path
",if spec . submodule_search_locations :,174
"def find_defined_variables(board_config_mks):
    re_def = re.compile(""^[\s]*([\w\d_]*)[\s]*:="")
    variables = dict()
    for board_config_mk in board_config_mks:
        for line in open(board_config_mk, encoding=""latin1""):
            mo = re_def.search(line)
            if mo is None:
                continue
            variable = mo.group(1)
            if variable in white_list:
                continue
            if variable not in variables:
                variables[variable] = set()
            variables[variable].add(board_config_mk[len(TOP) + 1 :])
    return variables
",if variable in white_list :,188
"def ensure_echo_on():
    if termios:
        fd = sys.stdin
        if fd.isatty():
            attr_list = termios.tcgetattr(fd)
            if not attr_list[3] & termios.ECHO:
                attr_list[3] |= termios.ECHO
                if hasattr(signal, ""SIGTTOU""):
                    old_handler = signal.signal(signal.SIGTTOU, signal.SIG_IGN)
                else:
                    old_handler = None
                termios.tcsetattr(fd, termios.TCSANOW, attr_list)
                if old_handler is not None:
                    signal.signal(signal.SIGTTOU, old_handler)
",if not attr_list [ 3 ] & termios . ECHO :,197
"def clean(self):
    with self._lock:
        min_index = min(self.indexes)
        if min_index >= self.CLEANUP_NUM:
            self.repository = self.repository[min_index:]
            for pos in xrange(len(self.indexes)):
                self.indexes[pos] -= min_index
",if min_index >= self . CLEANUP_NUM :,86
"def generate_changes(self, old):
    from weblate.trans.models.change import Change
    tracked = ((""slug"", Change.ACTION_RENAME_PROJECT),)
    for attribute, action in tracked:
        old_value = getattr(old, attribute)
        current_value = getattr(self, attribute)
        if old_value != current_value:
            Change.objects.create(
                action=action,
                old=old_value,
                target=current_value,
                project=self,
                user=self.acting_user,
            )
",if old_value != current_value :,156
"def get_voices(cls):
    cmd = [""flite"", ""-lv""]
    voices = []
    with tempfile.SpooledTemporaryFile() as out_f:
        subprocess.call(cmd, stdout=out_f)
        out_f.seek(0)
        for line in out_f:
            if line.startswith(""Voices available: ""):
                voices.extend([x.strip() for x in line[18:].split() if x.strip()])
    return voices
","if line . startswith ( ""Voices available: "" ) :",121
"def __init__(self, *args, **kwargs):
    dict.__init__(self, *args, **kwargs)
    for key, value in self.items():
        if not isinstance(key, string_types):
            raise TypeError(""key must be a str, not {}"".format(type(key)))
        if not isinstance(value, NUMERIC_TYPES):
            raise TypeError(""value must be a NUMERIC_TYPES, not {}"".format(type(value)))
        if not isinstance(value, float):
            self[key] = float(value)
","if not isinstance ( value , float ) :",132
"def read_track_raw(self, redundancy=1):
    self._log(""read track raw"")
    data = []
    await self.lower.write([CMD_READ_RAW, redundancy])
    while True:
        packet = await self.lower.read()
        if packet[-1] == 0xFF:
            raise GlasgowAppletError(""FIFO overflow while reading track"")
        elif packet[-1] == 0xFE:
            data.append(packet[:-1])
            return b"""".join(data)
        else:
            data.append(packet)
",elif packet [ - 1 ] == 0xFE :,147
"def init(self):
    """"""Initialize from the database""""""
    self.__effect = None
    if self.effectID:
        self.__effect = next(
            (x for x in self.fighter.item.effects.values() if x.ID == self.effectID),
            None,
        )
        if self.__effect is None:
            pyfalog.error(""Effect (id: {0}) does not exist"", self.effectID)
            return
    self.build()
",if self . __effect is None :,125
"def remove(self):
    key = self._key
    if key not in _key_to_collection:
        raise exc.InvalidRequestError(
            ""No listeners found for event %s / %r / %s ""
            % (self.target, self.identifier, self.fn)
        )
    dispatch_reg = _key_to_collection.pop(key)
    for collection_ref, listener_ref in dispatch_reg.items():
        collection = collection_ref()
        listener_fn = listener_ref()
        if collection is not None and listener_fn is not None:
            collection.remove(self.with_wrapper(listener_fn))
",if collection is not None and listener_fn is not None :,164
"def atbash(s):
    translated = """"
    for i in range(len(s)):
        n = ord(s[i])
        if s[i].isalpha():
            if s[i].isupper():
                x = n - ord(""A"")
                translated += chr(ord(""Z"") - x)
            if s[i].islower():
                x = n - ord(""a"")
                translated += chr(ord(""z"") - x)
        else:
            translated += s[i]
    return translated
",if s [ i ] . isupper ( ) :,143
"def __str__(self, prefix="""", printElemNumber=0):
    res = """"
    if self.has_cost_:
        res += prefix + ""cost <\n""
        res += self.cost_.__str__(prefix + ""  "", printElemNumber)
        res += prefix + "">\n""
    cnt = 0
    for e in self.version_:
        elm = """"
        if printElemNumber:
            elm = ""(%d)"" % cnt
        res += prefix + (""Version%s {\n"" % elm)
        res += e.__str__(prefix + ""  "", printElemNumber)
        res += prefix + ""}\n""
        cnt += 1
    return res
",if printElemNumber :,171
"def readwrite(obj, flags):
    try:
        if flags & select.POLLIN:
            obj.handle_read_event()
        if flags & select.POLLOUT:
            obj.handle_write_event()
        if flags & select.POLLPRI:
            obj.handle_expt_event()
        if flags & (select.POLLHUP | select.POLLERR | select.POLLNVAL):
            obj.handle_close()
    except OSError as e:
        if e.args[0] not in _DISCONNECTED:
            obj.handle_error()
        else:
            obj.handle_close()
    except _reraised_exceptions:
        raise
    except:
        obj.handle_error()
",if e . args [ 0 ] not in _DISCONNECTED :,192
"def mro(cls):
    if self.ready:
        if cls.__name__ == ""B1"":
            B2.__bases__ = (B1,)
        if cls.__name__ == ""B2"":
            B1.__bases__ = (B2,)
    return type.mro(cls)
","if cls . __name__ == ""B1"" :",76
"def create_hyperswap_volume(self, vol_name, size, units, pool, opts):
    vol_name = '""%s""' % vol_name
    params = []
    if opts[""rsize""] != -1:
        is_dr_pool = self.is_volume_type_dr_pools(pool, opts)
        if is_dr_pool:
            self.check_data_reduction_pool_params(opts)
        params = self._get_hyperswap_volume_create_params(opts, is_dr_pool)
    hyperpool = ""%s:%s"" % (pool, opts[""peer_pool""])
    self.ssh.mkvolume(vol_name, six.text_type(size), units, hyperpool, params)
",if is_dr_pool :,182
"def save_new_objects(self, commit=True):
    self.new_objects = []
    for form in self.extra_forms:
        if not form.has_changed():
            continue
        # If someone has marked an add form for deletion, don't save the
        # object.
        if self.can_delete and self._should_delete_form(form):
            continue
        self.new_objects.append(self.save_new(form, commit=commit))
        if not commit:
            self.saved_forms.append(form)
    return self.new_objects
",if not form . has_changed ( ) :,151
"def create_monitored_items(event, dispatcher):
    print(""Monitored Item"")
    for idx in range(len(event.response_params)):
        if event.response_params[idx].StatusCode.is_good():
            nodeId = event.request_params.ItemsToCreate[idx].ItemToMonitor.NodeId
            print(""Node {0} was created"".format(nodeId))
",if event . response_params [ idx ] . StatusCode . is_good ( ) :,99
"def close(self, linger=None):
    if not self.closed and self._fd is not None:
        for event in list(chain(self._recv_futures or [], self._send_futures or [])):
            if not event.future.done():
                try:
                    event.future.cancel()
                except RuntimeError:
                    # RuntimeError may be called during teardown
                    pass
        self._clear_io_state()
    super(_AsyncSocket, self).close(linger=linger)
",if not event . future . done ( ) :,135
"def stop_actors(self, monitor):
    """"""Maintain the number of workers by spawning or killing as required""""""
    if monitor.cfg.workers:
        num_to_kill = len(self.managed_actors) - monitor.cfg.workers
        for i in range(num_to_kill, 0, -1):
            w, kage = 0, sys.maxsize
            for worker in self.managed_actors.values():
                age = worker.impl.age
                if age < kage:
                    w, kage = worker, age
            self.manage_actor(monitor, w, True)
",if age < kage :,160
"def get_version(module):
    for key in version_keys:
        if hasattr(module, key):
            version = getattr(module, key)
            if isinstance(version, types.ModuleType):
                version = get_version(version)
            return version
    return ""Unknown""
","if isinstance ( version , types . ModuleType ) :",77
"def getBigramProb(self, w1, w2):
    ""prob of seeing words w1 w2 next to each other.""
    w1 = w1.lower()
    w2 = w2.lower()
    val1 = self.bigrams.get(w1)
    if val1 != None:
        val2 = val1.get(w2)
        if val2 != None:
            return val2
        return self.addK / (
            self.getUnigramProb(w1) * self.numUniqueWords + self.numUniqueWords
        )
    return 0
",if val2 != None :,147
"def _getPartAbbreviation(self):
    if self._partAbbreviation is not None:
        return self._partAbbreviation
    elif ""_partAbbreviation"" in self._cache:
        return self._cache[""_partAbbreviation""]
    else:
        pn = None
        for e in self.recurse().getElementsByClass(""Instrument""):
            pn = e.partAbbreviation
            if pn is None:
                pn = e.instrumentAbbreviation
            if pn is not None:
                break
        self._cache[""_partAbbreviation""] = pn
        return pn
",if pn is not None :,158
"def set_value(self, value, storedtime=None):
    self.namespace.acquire_write_lock()
    try:
        if storedtime is None:
            storedtime = time.time()
        debug(
            ""set_value stored time %r expire time %r"", storedtime, self.expire_argument
        )
        self.namespace.set_value(
            self.key,
            (storedtime, self.expire_argument, value),
            expiretime=self.expire_argument,
        )
    finally:
        self.namespace.release_write_lock()
",if storedtime is None :,154
"def setRadioSquare(self, title, square=True):
    if self.platform == self.MAC:
        gui.warn(""Square radiobuttons not available on Mac, for radiobutton %s"", title)
    elif not self.ttkFlag:
        for k, v in self.widgetManager.group(WIDGET_NAMES.RadioButton).items():
            if k.startswith(title + ""-""):
                if square:
                    v.config(indicatoron=1)
                else:
                    v.config(indicatoron=0)
    else:
        gui.warn(
            ""Square radiobuttons not available in ttk mode, for radiobutton %s"", title
        )
",if square :,177
"def render_func(self, node):
    if node.id in DEFAULT_FUNCTIONS:
        f = DEFAULT_FUNCTIONS[node.id]
        if f.sympy_func is not None and isinstance(f.sympy_func, sympy.FunctionClass):
            return f.sympy_func
    # special workaround for the ""int"" function
    if node.id == ""int"":
        return sympy.Function(""int_"")
    else:
        return sympy.Function(node.id)
","if f . sympy_func is not None and isinstance ( f . sympy_func , sympy . FunctionClass ) :",117
"def __init__(self, source_definition, **kw):
    super(RekallEFilterArtifacts, self).__init__(source_definition, **kw)
    for column in self.fields:
        if ""name"" not in column or ""type"" not in column:
            raise errors.FormatError(
                u""Field definition should have both name and type.""
            )
        mapped_type = column[""type""]
        if mapped_type not in self.allowed_types:
            raise errors.FormatError(u""Unsupported type %s."" % mapped_type)
",if mapped_type not in self . allowed_types :,143
"def run(self, lines):
    """"""Match and store Fenced Code Blocks in the HtmlStash.""""""
    text = ""\n"".join(lines)
    while 1:
        m = FENCED_BLOCK_RE.search(text)
        if m:
            lang = """"
            if m.group(""lang""):
                lang = LANG_TAG % m.group(""lang"")
            code = CODE_WRAP % (lang, self._escape(m.group(""code"")))
            placeholder = self.markdown.htmlStash.store(code, safe=True)
            text = ""%s\n%s\n%s"" % (text[: m.start()], placeholder, text[m.end() :])
        else:
            break
    return text.split(""\n"")
",if m :,198
"def GetDisplayNameOf(self, pidl, flags):
    item = pidl_to_item(pidl)
    if flags & shellcon.SHGDN_FORPARSING:
        if flags & shellcon.SHGDN_INFOLDER:
            return item[""name""]
        else:
            if flags & shellcon.SHGDN_FORADDRESSBAR:
                sigdn = shellcon.SIGDN_DESKTOPABSOLUTEEDITING
            else:
                sigdn = shellcon.SIGDN_DESKTOPABSOLUTEPARSING
            parent = shell.SHGetNameFromIDList(self.pidl, sigdn)
            return parent + ""\\"" + item[""name""]
    else:
        return item[""name""]
",if flags & shellcon . SHGDN_INFOLDER :,188
"def test_buffer_play_stop(filled_buffer):
    assert filled_buffer.current_position[0] == 0
    filled_buffer.play()
    for _ in range(100):
        assert filled_buffer.is_playing
        if filled_buffer.current_position[0] > 0:
            break
        else:
            time.sleep(0.001)
    else:
        pytest.fail(""Did not advance position in buffer while playing."")
    filled_buffer.stop()
    assert not filled_buffer.is_playing
    pos = filled_buffer.current_position
    for _ in range(10):
        assert filled_buffer.current_position == pos
        time.sleep(0.001)
",if filled_buffer . current_position [ 0 ] > 0 :,179
"def delete_service(service):
    try:
        win32serviceutil.RemoveService(service)
        logger.info(
            ""Services: Succesfully removed service '{service}'"".format(service=service)
        )
    except pywintypes.error as e:
        errors = (
            winerror.ERROR_SERVICE_DOES_NOT_EXIST,
            winerror.ERROR_SERVICE_NOT_ACTIVE,
            winerror.ERROR_SERVICE_MARKED_FOR_DELETE,
        )
        if not any(error == e.winerror for error in errors):
            logger.exception(
                ""Services: Failed to remove service '{service}'"".format(service=service)
            )
",if not any ( error == e . winerror for error in errors ) :,174
"def connect_to_server(self, server_cls):
    server = client = None
    try:
        sock, port = bind_unused_port()
        server = server_cls(ssl_options=_server_ssl_options())
        server.add_socket(sock)
        client = SSLIOStream(socket.socket(), ssl_options=dict(cert_reqs=ssl.CERT_NONE))
        yield client.connect((""127.0.0.1"", port))
        self.assertIsNotNone(client.socket.cipher())
    finally:
        if server is not None:
            server.stop()
        if client is not None:
            client.close()
",if client is not None :,168
"def allow_request(self, request, view):
    request.server = None
    allow = True
    view_name = view.get_view_name()
    allowed_views = [u""System Data"", u""Collectd Data"", u""Legacy System Data""]
    if view_name in allowed_views:
        server_key = view.kwargs.get(""server_key"")
        server = server_model.get_server_by_key(server_key)
        if server:
            request.server = server  # Needed in the Models
            server_status = throttle_status(server=server)
            if server_status.allow == False:
                allow = False
    return allow
",if server_status . allow == False :,173
"def log_start(self, prefix, msg):
    with self._log_lock:
        if self._last_log_prefix != prefix:
            if self._last_log_prefix is not None:
                self._log_file.write(""\n"")
            self._log_file.write(prefix)
        self._log_file.write(msg)
        self._last_log_prefix = prefix
",if self . _last_log_prefix != prefix :,105
"def override(self, user_conf: dict):
    for k, v in user_conf.items():
        # handle ES options, don't override entire dict if one key is passed
        if k == ""SEARCH_CONF"":
            for subkey, subval in v.items():
                self.SEARCH_CONF[subkey] = subval
        else:
            setattr(self, k, v)
","if k == ""SEARCH_CONF"" :",101
"def emit_classattribs(self, typebld):
    if hasattr(self, ""_clrclassattribs""):
        for attrib_info in self._clrclassattribs:
            if isinstance(attrib_info, type):
                ci = clr.GetClrType(attrib_info).GetConstructor(())
                cab = CustomAttributeBuilder(ci, ())
            elif isinstance(attrib_info, CustomAttributeDecorator):
                cab = attrib_info.GetBuilder()
            else:
                make_decorator = attrib_info()
                cab = make_decorator.GetBuilder()
            typebld.SetCustomAttribute(cab)
","elif isinstance ( attrib_info , CustomAttributeDecorator ) :",166
"def load_classes(module, base, blacklist):
    classes = []
    for attr in dir(module):
        attr = getattr(module, attr)
        if inspect.isclass(attr):
            if issubclass(attr, base):
                if attr is not base and attr not in blacklist:
                    classes.append(attr)
    return classes
",if attr is not base and attr not in blacklist :,90
"def search_scopes(self, key):
    for scope in self.scopes:
        if hasattr(scope, key):
            return getattr(scope, key)
        if hasattr(scope, ""__getitem__""):
            if key in scope:
                return scope[key]
","if hasattr ( scope , key ) :",70
"def get_cfg_dict(self, with_meta=True):
    options_dict = self.merged_options
    if with_meta:
        if self.plugin:
            options_dict.update(
                {""package"": ""yandextank.plugins.{}"".format(self.plugin)}
            )
        if self.enabled is not None:
            options_dict.update({""enabled"": self.enabled})
    return options_dict
",if self . enabled is not None :,111
"def render(self, context):
    for condition, nodelist in self.conditions_nodelists:
        if condition is not None:  # if / elif clause
            try:
                match = condition.eval(context)
            except VariableDoesNotExist:
                match = None
        else:  # else clause
            match = True
        if match:
            return nodelist.render(context)
    return """"
",if condition is not None :,109
"def main():
    base = sys.argv[1]
    filenames = sys.argv[2:]
    out = OutputByLength(base)
    n = 0
    for filename in filenames:
        print(""opening"")
        for record in screed.open(filename):
            out.save(record.name, record.sequence)
            n += 1
            if n % 10000 == 0:
                print(""..."", n)
",if n % 10000 == 0 :,110
"def load_cases(full_path):
    all_test_data = json.load(open(full_path), object_pairs_hook=OrderedDict)
    for test_data in all_test_data:
        given = test_data[""given""]
        for case in test_data[""cases""]:
            if ""result"" in case:
                test_type = ""result""
            elif ""error"" in case:
                test_type = ""error""
            elif ""bench"" in case:
                test_type = ""bench""
            else:
                raise RuntimeError(""Unknown test type: %s"" % json.dumps(case))
            yield (given, test_type, case)
","elif ""bench"" in case :",183
"def readline(self):
    if self.peek is not None:
        return """"
    line = self.file.readline()
    if not line:
        return line
    if self.boundary:
        if line == self.boundary + ""\n"":
            self.peek = line
            return """"
        if line == self.boundary + ""--\n"":
            self.peek = line
            return """"
    return line
","if line == self . boundary + ""--\n"" :",109
"def _get_cache_value(self, key, empty, type):
    """"""Used internally by the accessor properties.""""""
    if type is bool:
        return key in self
    if key in self:
        value = self[key]
        if value is None:
            return empty
        elif type is not None:
            try:
                value = type(value)
            except ValueError:
                pass
        return value
    return None
",elif type is not None :,119
"def _load_from_data(self, data):
    super(CliCommandHelpFile, self)._load_from_data(data)
    if isinstance(data, str) or not self.parameters or not data.get(""parameters""):
        return
    loaded_params = []
    loaded_param = {}
    for param in self.parameters:
        loaded_param = next(
            (n for n in data[""parameters""] if n[""name""] == param.name), None
        )
        if loaded_param:
            param.update_from_data(loaded_param)
        loaded_params.append(param)
    self.parameters = loaded_params
",if loaded_param :,162
"def __str__(self):
    s = super().__str__()
    if self.print_suggestions:
        possible_keys = set(self.captured_args) - self.SPECIAL_ARGS
        if possible_keys:
            s += ""\nPossible config keys are: {}"".format(possible_keys)
    return s
",if possible_keys :,77
"def family_add(self, handle_list):
    if self.active:
        person = self.get_active()
        if person:
            while not self.change_person(person):
                pass
        else:
            self.change_person(None)
    else:
        self.dirty = True
",if person :,85
"def recv_into(self, buffer, nbytes=None, flags=0):
    if buffer and (nbytes is None):
        nbytes = len(buffer)
    elif nbytes is None:
        nbytes = 1024
    if self._sslobj:
        if flags != 0:
            raise ValueError(
                ""non-zero flags not allowed in calls to recv_into() on %s""
                % self.__class__
            )
        tmp_buffer = self.read(nbytes)
        v = len(tmp_buffer)
        buffer[:v] = tmp_buffer
        return v
    else:
        return socket.recv_into(self, buffer, nbytes, flags)
",if flags != 0 :,174
"def removeInsideIslands(self):
    self.CleanPath = []
    cleanpath = Path(""Path"")
    for path in self.NewPaths:
        for seg in path:
            inside = False
            for island in self.IntersectedIslands:
                issegin = island.isSegInside(seg) == 1
                if issegin:
                    if not seg in island:
                        inside = True
                        break
            if not inside:
                cleanpath.append(seg)
    cleanpath = cleanpath.split2contours()
    self.CleanPath.extend(cleanpath)
",if not inside :,176
"def ETA(self):
    if self.done:
        prefix = ""Done""
        t = self.elapsed
        # import pdb; pdb.set_trace()
    else:
        prefix = ""ETA ""
        if self.max is None:
            t = -1
        elif self.elapsed == 0 or (self.cur == self.min):
            t = 0
        else:
            # import pdb; pdb.set_trace()
            t = float(self.max - self.min)
            t /= self.cur - self.min
            t = (t - 1) * self.elapsed
    return ""%s: %s"" % (prefix, self.format_duration(t))
",elif self . elapsed == 0 or ( self . cur == self . min ) :,184
"def columnToDataIndex(self, columnIndex):
    c = 0
    for dataIndex, accessor in enumerate(self.vectorDataAccessors()):
        nc = accessor.numColumns()
        if c + nc > columnIndex:
            if nc == 1:
                return (dataIndex, -1)
            else:
                return (dataIndex, columnIndex - c)
        c += nc
    raise IndexError(columnIndex)
",if nc == 1 :,112
"def as_nodes(self, files):
    """"""Returns a list of waflib.Nodes from a list of string of file paths""""""
    nodes = []
    for x in files:
        if not isinstance(x, str):
            d = x
        else:
            d = self.srcnode.find_node(x)
            if not d:
                raise Errors.WafError(""File '%s' was not found"" % x)
        nodes.append(d)
    return nodes
",if not d :,127
"def register_extension(ext):
    nonlocal commands
    try:
        parser = subparsers.add_parser(ext.name)
        if isinstance(ext.plugin, type) and issubclass(ext.plugin, BaseCommand):
            # current way, class based.
            cmd = ext.plugin()
            cmd.add_arguments(parser)
            cmd.__name__ = ext.name
            commands[ext.name] = cmd.handle
        else:
            # old school, function based.
            commands[ext.name] = ext.plugin(parser)
    except Exception:
        logger.exception(""Error while loading command {}."".format(ext.name))
","if isinstance ( ext . plugin , type ) and issubclass ( ext . plugin , BaseCommand ) :",171
"def names(self):
    ret = {}
    for line in dopen(""/proc/interrupts""):
        l = line.split()
        if len(l) <= cpunr:
            continue
        l1 = l[0].split("":"")[0]
        ### Cleanup possible names from /proc/interrupts
        l2 = "" "".join(l[cpunr + 3 :])
        l2 = l2.replace(""_hcd:"", ""/"")
        l2 = re.sub(""@pci[:\d+\.]+"", """", l2)
        l2 = re.sub(""ahci\[[:\da-z\.]+\]"", ""ahci"", l2)
        ret[l1] = l2
    return ret
",if len ( l ) <= cpunr :,178
"def formatweekday(self, day, width):
    with TimeEncoding(self.locale) as encoding:
        if width >= 9:
            names = day_name
        else:
            names = day_abbr
        name = names[day]
        if encoding is not None:
            name = name.decode(encoding)
        return name[:width].center(width)
",if encoding is not None :,97
"def __walk_dir_tree(self, dirname):
    dir_list = []
    self.__logger.debug(""__walk_dir_tree. START dir=%s"", dirname)
    for f in os.listdir(dirname):
        current = os.path.join(dirname, f)
        if os.path.isfile(current) and f.endswith(""py""):
            if self.module_registrant:
                self._load_py_from_file(current)
            dir_list.append(current)
        elif os.path.isdir(current):
            ret = self.__walk_dir_tree(current)
            if ret:
                dir_list.append((f, ret))
    return dir_list
","if os . path . isfile ( current ) and f . endswith ( ""py"" ) :",184
"def _EvalInScriptedSection(self, codeBlock, globals, locals=None):
    if self.debugManager:
        self.debugManager.OnEnterScript()
        if self.debugManager.adb.appDebugger:
            return self.debugManager.adb.runeval(codeBlock, globals, locals)
        else:
            return eval(codeBlock, globals, locals)
    else:
        return eval(codeBlock, globals, locals)
",if self . debugManager . adb . appDebugger :,113
"def load_multiple(fh, position=None, end=None):
    loaded = list()
    while position < end:
        new_box = load(fh, position, end)
        if new_box is None:
            print(""Error, failed to load box."")
            return None
        loaded.append(new_box)
        position = new_box.position + new_box.size()
    return loaded
",if new_box is None :,105
"def test_loadTestsFromName__module_not_loaded(self):
    # We're going to try to load this module as a side-effect, so it
    # better not be loaded before we try.
    #
    module_name = ""unittest2.test.dummy""
    sys.modules.pop(module_name, None)
    loader = unittest2.TestLoader()
    try:
        suite = loader.loadTestsFromName(module_name)
        self.assertIsInstance(suite, loader.suiteClass)
        self.assertEqual(list(suite), [])
        # module should now be loaded, thanks to loadTestsFromName()
        self.assertIn(module_name, sys.modules)
    finally:
        if module_name in sys.modules:
            del sys.modules[module_name]
",if module_name in sys . modules :,190
"def copy_file(s, d, xform=None):
    with open(s, ""rb"") as f:
        text = f.read()
    if xform:
        (d, text) = xform(d, text)
    if os.path.exists(d):
        if opts.force:
            print >>sys.stderr, ""Overwriting %s."" % d
        else:
            print >>sys.stderr, ""Not overwriting %s."" % d
            return
    else:
        print >>sys.stderr, ""Writing %s."" % d
    with open(d, ""wb"") as f:
        f.write(text)
",if opts . force :,162
"def __setitem__(self, index, image):
    if isinstance(index, slice):
        tmp_idx = self.current_index
        slice_ = self.validate_slice(index)
        del self[slice_]
        self.extend(image, offset=slice_.start)
        self.current_index = tmp_idx
    else:
        if not isinstance(image, BaseImage):
            raise TypeError(
                ""image must be an instance of wand.image.""
                ""BaseImage, not "" + repr(image)
            )
        with self.index_context(index) as index:
            library.MagickRemoveImage(self.image.wand)
            library.MagickAddImage(self.image.wand, image.wand)
","if not isinstance ( image , BaseImage ) :",196
"def _configure_legacy_instrument_class(self):
    if self.inherits:
        self.dispatch._update(self.inherits.dispatch)
        super_extensions = set(
            chain(*[m._deprecated_extensions for m in self.inherits.iterate_to_root()])
        )
    else:
        super_extensions = set()
    for ext in self._deprecated_extensions:
        if ext not in super_extensions:
            ext._adapt_instrument_class(self, ext)
",if ext not in super_extensions :,125
"def tearDown(self):
    exc, _, _ = sys.exc_info()
    if exc:
        try:
            if hasattr(self, ""obj"") and isinstance(self.obj, SelfDiagnosable):
                diags = self.obj.get_error_diagnostics()
                if diags:
                    for line in diags:
                        ROOT_LOGGER.info(line)
        except BaseException:
            pass
    if self.captured_logger:
        self.captured_logger.removeHandler(self.log_recorder)
        self.log_recorder.close()
    sys.stdout = self.stdout_backup
    super(BZTestCase, self).tearDown()
","if hasattr ( self , ""obj"" ) and isinstance ( self . obj , SelfDiagnosable ) :",179
"def number_operators(self, a, b, skip=[]):
    dict = {""a"": a, ""b"": b}
    for name, expr in self.binops.items():
        if name not in skip:
            name = ""__%s__"" % name
            if hasattr(a, name):
                res = eval(expr, dict)
                self.binop_test(a, b, res, expr, name)
    for name, expr in list(self.unops.items()):
        if name not in skip:
            name = ""__%s__"" % name
            if hasattr(a, name):
                res = eval(expr, dict)
                self.unop_test(a, res, expr, name)
",if name not in skip :,189
"def _parse_cachecontrol(self, r):
    if r not in self._cc_parsed:
        cch = r.headers.get(b""Cache-Control"", b"""")
        parsed = parse_cachecontrol(cch)
        if isinstance(r, Response):
            for key in self.ignore_response_cache_controls:
                parsed.pop(key, None)
        self._cc_parsed[r] = parsed
    return self._cc_parsed[r]
","if isinstance ( r , Response ) :",121
"def make_pattern(wtree):
    subpattern = []
    for part in wtree[1:-1]:
        if isinstance(part, list):
            part = make_pattern(part)
        elif wtree[0] != """":
            for c in part:
                # Meta-characters cannot be quoted
                if c in special_chars:
                    raise GlobError()
        subpattern.append(part)
    return """".join(subpattern)
","elif wtree [ 0 ] != """" :",123
"def iterjlines(f, header, missing):
    it = iter(f)
    if header is None:
        header = list()
        peek, it = iterpeek(it, 1)
        json_obj = json.loads(peek)
        if hasattr(json_obj, ""keys""):
            header += [k for k in json_obj.keys() if k not in header]
    yield tuple(header)
    for o in it:
        json_obj = json.loads(o)
        yield tuple(json_obj[f] if f in json_obj else missing for f in header)
","if hasattr ( json_obj , ""keys"" ) :",149
"def logprob(self, sample):
    if self._log:
        return self._prob_dict.get(sample, _NINF)
    else:
        if sample not in self._prob_dict:
            return _NINF
        elif self._prob_dict[sample] == 0:
            return _NINF
        else:
            return math.log(self._prob_dict[sample], 2)
",elif self . _prob_dict [ sample ] == 0 :,102
"def TryMerge(self, d):
    while d.avail() > 0:
        tt = d.getVarInt32()
        if tt == 10:
            length = d.getVarInt32()
            tmp = ProtocolBuffer.Decoder(d.buffer(), d.pos(), d.pos() + length)
            d.skip(length)
            self.add_public_certificate_list().TryMerge(tmp)
            continue
        if tt == 16:
            self.set_max_client_cache_time_in_second(d.getVarInt64())
            continue
        if tt == 0:
            raise ProtocolBuffer.ProtocolBufferDecodeError
        d.skipData(tt)
",if tt == 0 :,181
"def acquire(self, blocking=True, timeout=None):
    if not blocking and timeout is not None:
        raise ValueError(""can't specify timeout for non-blocking acquire"")
    rc = False
    endtime = None
    self._cond.acquire()
    while self._value == 0:
        if not blocking:
            break
        if timeout is not None:
            if endtime is None:
                endtime = _time() + timeout
            else:
                timeout = endtime - _time()
                if timeout <= 0:
                    break
        self._cond.wait(timeout)
    else:
        self._value = self._value - 1
        rc = True
    self._cond.release()
    return rc
",if not blocking :,194
"def run_train_loop(self):
    self.begin_training()
    for _ in self.yield_train_step():
        if self.should_save_model():
            self.save_model()
        if self.should_save_checkpoint():
            self.save_checkpoint()
        if self.should_eval_model():
            self.eval_model()
        if self.should_break_training():
            break
    self.eval_model()
    self.done_training()
    return self.returned_result()
",if self . should_eval_model ( ) :,139
"def scrape_me(url_path, **options):
    host_name = (
        get_host_name(url_path) if not options.get(""test"", False) else ""test_wild_mode""
    )
    try:
        scraper = SCRAPERS[host_name]
    except KeyError:
        if options.get(""wild_mode"", False):
            wild_scraper = SchemaScraperFactory.generate(url_path, **options)
            if not wild_scraper.schema.data:
                raise NoSchemaFoundInWildMode(url_path)
            return wild_scraper
        else:
            raise WebsiteNotImplementedError(host_name)
    return scraper(url_path, **options)
","if options . get ( ""wild_mode"" , False ) :",197
"def iter_expressions(self):
    if not self._isrecord:
        tri_attr_context = [(""target"", SPECIAL_INOUT)]
    else:
        tri_attr_context = [
            (""_target_o"", SPECIAL_OUTPUT),
            (""_target_oe"", SPECIAL_OUTPUT),
            (""_target_i"", SPECIAL_INPUT),
        ]
    tri_attr_context += [
        (""o"", SPECIAL_INPUT),
        (""oe"", SPECIAL_INPUT),
        (""i"", SPECIAL_OUTPUT),
    ]
    for attr, target_context in tri_attr_context:
        if getattr(self, attr) is not None:
            yield self, attr, target_context
","if getattr ( self , attr ) is not None :",176
"def get_field_values(self, fields):
    field_values = []
    for field in fields:
        # Title is special case
        if field == ""title"":
            value = self.get_title_display()
        elif field == ""country"":
            try:
                value = self.country.printable_name
            except exceptions.ObjectDoesNotExist:
                value = """"
        elif field == ""salutation"":
            value = self.salutation
        else:
            value = getattr(self, field)
        field_values.append(value)
    return field_values
","elif field == ""salutation"" :",158
"def show_panel(panel_id):
    # Iterate positions to find where panel is and bring it to front.
    for position in _positions_names:
        pos_panel_ids = _get_position_panels(position)
        if len(pos_panel_ids) == 0:
            continue
        if len(pos_panel_ids) == 1:
            continue
        panel_widget = _get_panels_widgets_dict(gui.editor_window)[panel_id]
        notebook = _position_notebooks[position]
        for i in range(0, notebook.get_n_pages()):
            notebook_page = notebook.get_nth_page(i)
            if notebook_page == panel_widget:
                notebook.set_current_page(i)
",if len ( pos_panel_ids ) == 0 :,197
"def draw(self):
    program = self._program
    collection = self._collection
    mode = collection._mode
    if collection._need_update:
        collection._update()
        # self._program.bind(self._vertices_buffer)
        if collection._uniforms_list is not None:
            program[""uniforms""] = collection._uniforms_texture
            program[""uniforms_shape""] = collection._ushape
    if collection._indices_list is not None:
        program.draw(mode, collection._indices_buffer)
    else:
        program.draw(mode)
",if collection . _uniforms_list is not None :,145
"def release(provider, connection, cache=None):
    if cache is not None:
        db_session = cache.db_session
        if db_session is not None and db_session.ddl and cache.saved_fk_state:
            try:
                cursor = connection.cursor()
                sql = ""SET foreign_key_checks = 1""
                if core.local.debug:
                    log_orm(sql)
                cursor.execute(sql)
            except:
                provider.pool.drop(connection)
                raise
    DBAPIProvider.release(provider, connection, cache)
",if core . local . debug :,164
"def expanded_output(self):
    """"""Iterate over output files while dynamic output is expanded.""""""
    for f, f_ in zip(self.output, self.rule.output):
        if f in self.dynamic_output:
            expansion = self.expand_dynamic(f_)
            if not expansion:
                yield f_
            for f, _ in expansion:
                file_to_yield = IOFile(f, self.rule)
                file_to_yield.clone_flags(f_)
                yield file_to_yield
        else:
            yield f
",if f in self . dynamic_output :,153
"def __new__(cls, xs: Tuple[Optional[AbstractValue], core.Value]):
    pv, const = xs
    if not core.skip_checks:
        # type checks
        assert isinstance(pv, (AbstractValue, type(None))), xs
        assert (
            isinstance(const, core.Tracer)
            or type(const) is Zero
            or core.valid_jaxtype(const)
        ), xs
        # invariant checks
        if isinstance(pv, AbstractValue):
            assert get_aval(const) == core.abstract_unit, xs
    return tuple.__new__(cls, xs)
","if isinstance ( pv , AbstractValue ) :",156
"def MenuItemSearch(menu, item):
    for menuItem in list(menu.GetMenuItems()):
        label = menuItem.GetItemLabel()
        if not label:
            # It's a separator
            continue
        shortcutItem = Shortcut(menuItem=menuItem)
        shortcutItem.FromMenuItem()
        item.AppendItem(shortcutItem)
        subMenu = menuItem.GetSubMenu()
        if subMenu:
            MenuItemSearch(subMenu, shortcutItem)
",if subMenu :,117
"def fill_potential_satellites_by_type(self, sat_type):
    setattr(self, ""potential_%s"" % sat_type, [])
    for satellite in getattr(self, sat_type):
        getattr(self, ""potential_%s"" % sat_type).append(satellite)
    for realm in self.higher_realms:
        for satellite in getattr(realm, sat_type):
            if satellite.manage_sub_realms:
                getattr(self, ""potential_%s"" % sat_type).append(satellite)
",if satellite . manage_sub_realms :,142
"def _gen():
    while True:
        try:
            loop_val = it.next()  # e.g. x
        except StopIteration:
            break
        self.mem.SetValue(
            lvalue.Named(iter_name), value.Obj(loop_val), scope_e.LocalOnly
        )
        if comp.cond:
            b = self.EvalExpr(comp.cond)
        else:
            b = True
        if b:
            item = self.EvalExpr(node.elt)  # e.g. x*2
            yield item
",if b :,155
"def _iter_backtick_string(gen, line, back_start):
    for _, tokval, start, _, _ in gen:
        if tokval == ""`"":
            return (
                BACKTICK_TAG
                + binascii.b2a_hex(line[back_start[1] + 1 : start[1]].encode()).decode()
            )
    else:
        raise SyntaxError(f""backtick quote at {back_start} does not match"")
","if tokval == ""`"" :",116
"def to_internal_value(self, data):
    site = get_current_site()
    pages_root = reverse(""pages-root"")
    ret = []
    for path in data:
        if path.startswith(pages_root):
            path = path[len(pages_root) :]
        # strip any final slash
        if path.endswith(""/""):
            path = path[:-1]
        page = get_page_from_path(site, path)
        if page:
            ret.append(page)
    return ret
",if page :,136
"def refresh(self):
    # In MongoTrials, this method fetches from database
    if self._exp_key is None:
        self._trials = [
            tt for tt in self._dynamic_trials if tt[""state""] in JOB_VALID_STATES
        ]
    else:
        self._trials = [
            tt
            for tt in self._dynamic_trials
            if (tt[""state""] in JOB_VALID_STATES and tt[""exp_key""] == self._exp_key)
        ]
    self._ids.update([tt[""tid""] for tt in self._trials])
","if ( tt [ ""state"" ] in JOB_VALID_STATES and tt [ ""exp_key"" ] == self . _exp_key )",154
"def create_model(self, model):
    for field in model._meta.local_fields:
        # Autoincrement SQL for backends with post table definition variant
        if field.get_internal_type() == ""PositiveAutoField"":
            autoinc_sql = self.connection.ops.autoinc_sql(
                model._meta.db_table, field.column
            )
            if autoinc_sql:
                self.deferred_sql.extend(autoinc_sql)
    super().create_model(model)
",if autoinc_sql :,133
"def row_match(base_row, row):
    # ildutil.ild_err(""ILD_DEBUG BASE ROW %s"" % (base_row,))
    for (op, val) in list(row.items()):
        if op in base_row:
            if base_row[op] != val:
                return False
        else:
            ildutil.ild_err(
                ""BASE ROW %s doesn't have OD %s from row %s"" % (base_row, op, row)
            )
            return None
    return True
",if op in base_row :,148
"def get_referrers(self):
    d = []
    for o in gc.get_referrers(self.obj):
        name = None
        if isinstance(o, dict):
            name = web.dictfind(o, self.obj)
            for r in gc.get_referrers(o):
                if getattr(r, ""__dict__"", None) is o:
                    o = r
                    break
        elif isinstance(o, dict):  # other dict types
            name = web.dictfind(o, self.obj)
        if not isinstance(name, six.string_types):
            name = None
        d.append(Object(o, name))
    return d
","if not isinstance ( name , six . string_types ) :",187
"def _run(env, remote):
    if device == ""vta"":
        target = env.target
        if env.TARGET not in [""sim"", ""tsim""]:
            assert tvm.runtime.enabled(""rpc"")
            program_fpga(remote, bitstream=None)
            reconfig_runtime(remote)
    elif device == ""arm_cpu"":
        target = env.target_vta_cpu
    with autotvm.tophub.context(target):  # load pre-tuned schedule parameters
        for _, wl in resnet_wkls:
            print(wl)
            run_conv2d(env, remote, wl, target)
","if env . TARGET not in [ ""sim"" , ""tsim"" ] :",169
"def retrieve(self, aclass):
    """"""Look for a specifc class/name in the packet""""""
    resu = []
    for x in self.payload:
        try:
            if isinstance(aclass, str):
                if x.name == aclass:
                    resu.append(x)
            else:
                if isinstance(x, aclass):
                    resu.append(x)
            resu += x.retrieve(aclass)
        except:
            pass
    return resu
",if x . name == aclass :,144
"def summary_passes(self):
    if self.config.option.tbstyle != ""no"":
        if self.hasopt(""P""):
            reports = self.getreports(""passed"")
            if not reports:
                return
            self.write_sep(""="", ""PASSES"")
            for rep in reports:
                msg = self._getfailureheadline(rep)
                self.write_sep(""_"", msg)
                self._outrep_summary(rep)
",if not reports :,127
"def fn():
    random_states = {
        name: cls.random_state_function(state_spec=state_spec)()
        for name, state_spec in states_spec.items()
    }
    for name, action_spec in actions_spec.items():
        if action_spec[""type""] == ""int"":
            mask = cls.random_mask(action_spec=action_spec)
            random_states[name + ""_mask""] = mask
    return random_states
","if action_spec [ ""type"" ] == ""int"" :",121
"def _show_option(name=None):
    if name is None:
        name = """"
    filename = peda.getfile()
    if filename:
        filename = os.path.basename(filename)
    else:
        filename = None
    for (k, v) in sorted(config.Option.show(name).items()):
        if filename and isinstance(v, str) and ""#FILENAME#"" in v:
            v = v.replace(""#FILENAME#"", filename)
        msg(""%s = %s"" % (k, repr(v)))
    return
","if filename and isinstance ( v , str ) and ""#FILENAME#"" in v :",137
"def _set_posonly_args_def(self, argmts, vals):
    for v in vals:
        argmts.posonlyargs.append(v[""arg""])
        d = v[""default""]
        if d is not None:
            argmts.defaults.append(d)
        elif argmts.defaults:
            self._set_error(""non-default argument follows default argument"")
",if d is not None :,97
"def get(self):
    with self._lock:
        if not self._connection or self._connection.closed != 0:
            self._connection = psycopg2.connect(**self._conn_kwargs)
            self._connection.autocommit = True
            self.server_version = self._connection.server_version
    return self._connection
",if not self . _connection or self . _connection . closed != 0 :,83
"def _Determine_Do(self):
    if sys.platform == ""darwin"":
        self.applicable = True
        for opt, optarg in self.chosenOptions:
            if opt == ""--"" + self.longopt:
                self.value = os.path.abspath(optarg)
                break
    else:
        self.applicable = False
    self.determined = True
","if opt == ""--"" + self . longopt :",97
"def delete_tags(filenames, v1, v2):
    for filename in filenames:
        with _sig.block():
            if verbose:
                print_(u""deleting ID3 tag info in"", filename, file=sys.stderr)
            mutagen.id3.delete(filename, v1, v2)
",if verbose :,81
"def startJail(self, name):
    with self.__lock:
        jail = self.__jails[name]
        if not jail.isAlive():
            jail.start()
        elif name in self.__reload_state:
            logSys.info(""Jail %r reloaded"", name)
            del self.__reload_state[name]
        if jail.idle:
            jail.idle = False
",elif name in self . __reload_state :,111
"def get_field_by_name(obj, field):
    # Dereference once
    if obj.type.code == gdb.TYPE_CODE_PTR:
        obj = obj.dereference()
    for f in re.split(""(->|\.|\[\d+\])"", field):
        if not f:
            continue
        if f == ""->"":
            obj = obj.dereference()
        elif f == ""."":
            pass
        elif f.startswith(""[""):
            n = int(f.strip(""[]""))
            obj = obj.cast(obj.dereference().type.pointer())
            obj += n
            obj = obj.dereference()
        else:
            obj = obj[f]
    return obj
",if not f :,189
"def _parse_yum_or_zypper_repositories(output):
    repos = []
    current_repo = {}
    for line in output:
        line = line.strip()
        if not line or line.startswith(""#""):
            continue
        if line.startswith(""[""):
            if current_repo:
                repos.append(current_repo)
                current_repo = {}
            current_repo[""name""] = line[1:-1]
        if current_repo and ""="" in line:
            key, value = line.split(""="", 1)
            current_repo[key] = value
    if current_repo:
        repos.append(current_repo)
    return repos
","if line . startswith ( ""["" ) :",179
"def add_to_auto_transitions(cls, base):
    result = {}
    for name, method in base.__dict__.items():
        if callable(method) and hasattr(method, ""_django_fsm""):
            for name, transition in method._django_fsm.transitions.items():
                if transition.custom.get(""auto""):
                    result.update({name: method})
    return result
","if callable ( method ) and hasattr ( method , ""_django_fsm"" ) :",103
"def commit(cache):
    assert cache.is_alive
    try:
        if cache.modified:
            cache.flush()
        if cache.in_transaction:
            assert cache.connection is not None
            cache.database.provider.commit(cache.connection, cache)
        cache.for_update.clear()
        cache.query_results.clear()
        cache.max_id_cache.clear()
        cache.immediate = True
    except:
        cache.rollback()
        raise
",if cache . modified :,131
"def block_items(objekt, block, eldict):
    if objekt not in block:
        if isinstance(objekt.type, PyType):
            if objekt.type not in block:
                block.append(objekt.type)
        block.append(objekt)
        if isinstance(objekt, PyType):
            others = [
                p
                for p in eldict.values()
                if isinstance(p, PyElement) and p.type[1] == objekt.name
            ]
            for item in others:
                if item not in block:
                    block.append(item)
    return block
",if objekt . type not in block :,186
"def __getattr__(self, item):
    import pyarrow.lib
    ret = getattr(plasma, item, None)
    if ret is None:  # pragma: no cover
        if item == ""PlasmaObjectNotFound"":
            ret = getattr(plasma, ""PlasmaObjectNonexistent"", None) or getattr(
                pyarrow.lib, ""PlasmaObjectNonexistent""
            )
        elif item == ""PlasmaStoreFull"":
            ret = getattr(pyarrow.lib, item)
    if ret is not None:
        setattr(self, item, ret)
    return ret
","if item == ""PlasmaObjectNotFound"" :",133
"def clean_str(*args):
    tdict = {""str"": 0, ""bytearray"": 1, ""unicode"": 2}
    for obj in args:
        k = tdict.get(type(obj).__name__)
        if k is None:
            raise RuntimeError(""Can not clean object: %s"" % obj)
        clean_obj(obj, k)
",if k is None :,87
"def incoming():
    while True:
        m = ws.receive()
        if m is not None:
            m = str(m)
            print((m, len(m)))
            if len(m) == 35:
                ws.close()
                break
        else:
            break
    print((""Connection closed!"",))
",if m is not None :,94
"def TryMerge(self, d):
    while d.avail() > 0:
        tt = d.getVarInt32()
        if tt == 8:
            self.add_set_status(d.getVarInt32())
            continue
        if tt == 0:
            raise ProtocolBuffer.ProtocolBufferDecodeError
        d.skipData(tt)
",if tt == 0 :,92
"def __init__(self, text, menu):
    self.text = text
    self.menu = menu
    print(text)
    for i, option in enumerate(menu):
        menunum = i + 1
        # Check to see if this line has the 'return to main menu' code
        match = re.search(""0D"", option)
        # If it's not the return to menu line:
        if not match:
            if menunum < 10:
                print((""   %s) %s"" % (menunum, option)))
            else:
                print((""  %s) %s"" % (menunum, option)))
        else:
            print(""\n  99) Return to Main Menu\n"")
    return
",if not match :,193
"def take_step(self):
    with self.walk_lock:
        # Share my random channels
        peers = self.overlay.get_peers()
        if peers:
            peer = choice(peers)
            self.overlay.send_random_to(peer)
",if peers :,74
"def clear_highlight(self):
    for doc in self._window.get_documents():
        start, end = doc.get_bounds()
        if doc.get_tag_table().lookup(""result_highlight"") == None:
            tag = doc.create_tag(
                ""result_highlight"", foreground=""yellow"", background=""red""
            )
        doc.remove_tag_by_name(""result_highlight"", start, end)
","if doc . get_tag_table ( ) . lookup ( ""result_highlight"" ) == None :",111
"def impl(self, to_strip=None):
    mask = get_nan_mask(self._data._data)
    item_count = len(self._data)
    res_list = [""""] * item_count
    for it in range(item_count):
        item = self._data._data[it]
        if len(item) > 0:
            res_list[it] = usecase(item, to_strip)
        else:
            res_list[it] = item
    str_arr = create_str_arr_from_list(res_list)
    result = str_arr_set_na_by_mask(str_arr, mask)
    return pandas.Series(result, self._data._index, name=self._data._name)
",if len ( item ) > 0 :,188
"def modify_subnet_attribute(self):
    subnet_id = self._get_param(""SubnetId"")
    for attribute in (""MapPublicIpOnLaunch"", ""AssignIpv6AddressOnCreation""):
        if self.querystring.get(""%s.Value"" % attribute):
            attr_name = camelcase_to_underscores(attribute)
            attr_value = self.querystring.get(""%s.Value"" % attribute)[0]
            self.ec2_backend.modify_subnet_attribute(subnet_id, attr_name, attr_value)
            return MODIFY_SUBNET_ATTRIBUTE_RESPONSE
","if self . querystring . get ( ""%s.Value"" % attribute ) :",149
"def join(s, *p):
    path = s
    for t in p:
        if (not s) or isabs(t):
            path = t
            continue
        if t[:1] == "":"":
            t = t[1:]
        if "":"" not in path:
            path = "":"" + path
        if path[-1:] != "":"":
            path = path + "":""
        path = path + t
    return path
","if path [ - 1 : ] != "":"" :",115
"def publish(self):
    # monoproc
    if not self.modules.has_option(self.subscriber_name, ""publish""):
        return False
    dest = self.modules.get(self.subscriber_name, ""publish"")
    # We can have multiple publisher
    for name in dest.split("",""):
        self.pubsub.setup_publish(name)
    while True:
        message = self.r_temp.spop(self.subscriber_name + ""out"")
        if message is None:
            time.sleep(1)
            continue
        self.pubsub.publish(message)
",if message is None :,151
"def ignore(self, other):
    if isinstance(other, Suppress):
        if other not in self.ignoreExprs:
            super().ignore(other)
            if self.expr is not None:
                self.expr.ignore(self.ignoreExprs[-1])
    else:
        super().ignore(other)
        if self.expr is not None:
            self.expr.ignore(self.ignoreExprs[-1])
    return self
",if other not in self . ignoreExprs :,117
"def recurse(node):
    for child in node.childNodes:
        if child.nodeType != child.ELEMENT_NODE:
            continue
        if child.nodeName.upper() == ""H1"":
            return child
        if child not in visited:
            return recurse(child)
",if child not in visited :,76
"def req(s, poll, msg, expect):
    do_req = True
    xid = None
    while True:
        # get transaction id
        if do_req:
            xid = s.put(msg)[""xid""]
        # wait for response
        events = poll.poll(2)
        for (fd, event) in events:
            response = s.get()
            if response[""xid""] != xid:
                do_req = False
                continue
            if response[""options""][""message_type""] != expect:
                raise Exception(""DHCP protocol error"")
            return response
        do_req = True
","if response [ ""xid"" ] != xid :",174
"def close(self, invalidate=False):
    self.session.transaction = self._parent
    if self._parent is None:
        for connection, transaction, autoclose in set(self._connections.values()):
            if invalidate:
                connection.invalidate()
            if autoclose:
                connection.close()
            else:
                transaction.close()
    self._state = CLOSED
    self.session.dispatch.after_transaction_end(self.session, self)
    if self._parent is None:
        if not self.session.autocommit:
            self.session.begin()
    self.session = None
    self._connections = None
",if autoclose :,172
"def visit_loop(self):
    v = self.vS.top_front()
    i = self.iS.top_front()
    num_edges = len(self.graph[v].edges)
    # Continue traversing out-edges until none left.
    while i <= num_edges:
        # Continuation
        if i > 0:
            # Update status for previously traversed out-edge
            self.finish_edge(v, i - 1)
        if i < num_edges and self.begin_edge(v, i):
            return
        i += 1
    # Finished traversing out edges, update component info
    self.finish_visiting(v)
","if i < num_edges and self . begin_edge ( v , i ) :",167
"def get_objects(self):
    list_type, id, handles, timestamp = self._obj_list
    retval = []
    for (target, handle) in handles:
        _class = map2class(target)
        if _class:
            obj = _class(self._dbstate, pickle.dumps((target, id, handle, timestamp)))
            if obj:
                retval.append(obj)
    return retval
",if _class :,108
"def __init__(self, config_lists):
    self.lens = len(config_lists)
    self.spaces = []
    for config_list in config_lists:
        if isinstance(config_list, tuple):
            key, config = config_list
        elif isinstance(config_list, str):
            key = config_list
            config = None
        else:
            raise NotImplementedError(
                ""the type of config is Error!!! Please check the config information. Receive the type of config is {}"".format(
                    type(config_list)
                )
            )
        self.spaces.append(self._get_single_search_space(key, config))
    self.init_tokens()
","elif isinstance ( config_list , str ) :",186
"def fieldset_string_to_field(fieldset_dict, model):
    if isinstance(fieldset_dict[""fields""], tuple):
        fieldset_dict[""fields""] = list(fieldset_dict[""fields""])
    i = 0
    for dict_field in fieldset_dict[""fields""]:
        if isinstance(dict_field, string_types):
            fieldset_dict[""fields""][i] = model._meta.get_field_by_name(dict_field)[0]
        elif isinstance(dict_field, list) or isinstance(dict_field, tuple):
            dict_field[1][""recursive""] = True
            fieldset_string_to_field(dict_field[1], model)
        i += 1
","if isinstance ( dict_field , string_types ) :",179
"def _get_directories(config):
    for directory in config[""dump_directories""]:
        for dname in sorted(glob.glob(os.path.join(directory, ""*[Aa]*[Xx][XxYy23]""))):
            if os.path.isdir(dname):
                yield dname
",if os . path . isdir ( dname ) :,79
"def process_event(self, event):
    super().process_event(event)
    if event.type == pygame.USEREVENT:
        if event.user_type == pygame_gui.UI_BUTTON_PRESSED:
            self.input_op(event.ui_object_id[-1])
            return True
",if event . user_type == pygame_gui . UI_BUTTON_PRESSED :,80
"def _restore_std_streams(self):
    stdout = sys.stdout.getvalue()
    stderr = sys.stderr.getvalue()
    close = [sys.stdout, sys.stderr]
    sys.stdout = sys.__stdout__
    sys.stderr = sys.__stderr__
    for stream in close:
        stream.close()
    if stdout and stderr:
        if not stderr.startswith((""*TRACE*"", ""*DEBUG*"", ""*INFO*"", ""*HTML*"", ""*WARN*"")):
            stderr = ""*INFO* %s"" % stderr
        if not stdout.endswith(""\n""):
            stdout += ""\n""
    return self._handle_binary_result(stdout + stderr)
","if not stdout . endswith ( ""\n"" ) :",159
"def _get_attachments(self):
    if self._attachments is None:
        alist = []
        for a in self._message.get_attachments():
            alist.append((AttachmentWidget(a), None))
        if alist:
            self._attachments = SimpleTree(alist)
    return self._attachments
",if alist :,79
"def __getattr__(self, name):
    # if the aval property raises an AttributeError, gets caught here
    assert skip_checks or name != ""aval""
    try:
        attr = getattr(self.aval, name)
    except KeyError as err:
        raise AttributeError(
            ""{} has no attribute {}"".format(self.__class__.__name__, name)
        ) from err
    else:
        t = type(attr)
        if t is aval_property:
            return attr.fget(self)
        elif t is aval_method:
            return types.MethodType(attr.fun, self)
        else:
            return attr
",elif t is aval_method :,165
"def _find_first_unescaped(dn, char, pos):
    while True:
        pos = dn.find(char, pos)
        if pos == -1:
            break  # no char found
        if pos > 0 and dn[pos - 1] != ""\\"":  # unescaped char
            break
        elif pos > 1 and dn[pos - 1] == ""\\"":  # may be unescaped
            escaped = True
            for c in dn[pos - 2 : 0 : -1]:
                if c == ""\\"":
                    escaped = not escaped
                else:
                    break
            if not escaped:
                break
        pos += 1
    return pos
",if pos == - 1 :,181
"def test_synopsis(self):
    self.addCleanup(unlink, TESTFN)
    for encoding in (""ISO-8859-1"", ""UTF-8""):
        with open(TESTFN, ""w"", encoding=encoding) as script:
            if encoding != ""UTF-8"":
                print(""#coding: {}"".format(encoding), file=script)
            print('""""""line 1: h\xe9', file=script)
            print('line 2: hi""""""', file=script)
        synopsis = pydoc.synopsis(TESTFN, {})
        self.assertEqual(synopsis, ""line 1: h\xe9"")
","if encoding != ""UTF-8"" :",148
"def qualify(x):
    parts = x.split("";"", 1)
    if len(parts) == 2:
        match = re.match(r""(^|;)q=(0(\.\d{,3})?|1(\.0{,3})?)(;|$)"", parts[1])
        if match:
            return parts[0].strip(), float(match.group(2))
    return parts[0].strip(), 1
",if match :,102
"def getEndpoints(self):
    endpoints = self.endpoints[:]
    for i in range(len(endpoints)):
        ep = endpoints[i]
        if not issubclass(ep, Endpoint):
            raise TypeError(""Not an Endpoint subclass"")
        endpoints[i] = ep(self, self.master)
    return endpoints
","if not issubclass ( ep , Endpoint ) :",81
"def __getitem__(self, index):
    if cfg.RPN.ENABLED:
        return self.get_rpn_sample(index)
    elif cfg.RCNN.ENABLED:
        if self.mode == ""TRAIN"":
            if cfg.RCNN.ROI_SAMPLE_JIT:
                return self.get_rcnn_sample_jit(index)
            else:
                return self.get_rcnn_training_sample_batch(index)
        else:
            return self.get_proposal_from_file(index)
    else:
        raise NotImplementedError
","if self . mode == ""TRAIN"" :",144
"def test_data_path(self, filename):
    repository_dir = self._repository_dir
    test_data = None
    if repository_dir:
        return self.__walk_test_data(dir=repository_dir, filename=filename)
    else:
        if self.tool_dir:
            tool_dir = self.tool_dir
            if isinstance(self, DataManagerTool):
                tool_dir = os.path.dirname(self.tool_dir)
            test_data = self.__walk_test_data(tool_dir, filename=filename)
    if not test_data:
        test_data = self.app.test_data_resolver.get_filename(filename)
    return test_data
","if isinstance ( self , DataManagerTool ) :",181
"def generate_forwards(cls, attrs):
    # forward functions of _forwards
    for attr_name, attr in cls._forwards.__dict__.items():
        if attr_name.startswith(""_"") or attr_name in attrs:
            continue
        if isinstance(attr, property):
            cls._forward.append(attr_name)
        elif isinstance(attr, types.FunctionType):
            wrapper = _forward_factory(cls, attr_name, attr)
            setattr(cls, attr_name, wrapper)
        else:
            raise TypeError(attr_name, type(attr))
","elif isinstance ( attr , types . FunctionType ) :",146
"def summary(result):
    if not self.options.metadata_to_dict:
        if self.options.verbose:
            pprint(Fore.CYAN + result[""title""] + Fore.RESET)
            pprint(
                Fore.CYAN
                + Style.DIM
                + result[""written_at""]
                + Style.RESET_ALL
                + Fore.RESET
            )
            pprint(result[""body""])
        writer.write(""@title:"" + result[""title""])
        writer.write(""@written_at:"" + result[""written_at""])
        writer.write(""@body:"" + result[""body""])
    else:
        if self.options.verbose:
            pprint(result)
        writer.write(result)
",if self . options . verbose :,196
"def visit_StringConstant(self, node: qlast.StringConstant) -> None:
    if not _NON_PRINTABLE_RE.search(node.value):
        for d in (""'"", '""', ""$$""):
            if d not in node.value:
                if ""\\"" in node.value and d != ""$$"":
                    self.write(""r"", d, node.value, d)
                else:
                    self.write(d, node.value, d)
                return
        self.write(edgeql_quote.dollar_quote_literal(node.value))
        return
    self.write(repr(node.value))
",if d not in node . value :,170
"def get_sql_date_trunc(col, db=""default"", grouper=""hour""):
    conn = connections[db]
    engine = get_db_engine(db)
    # TODO: does extract work for sqlite?
    if engine.startswith(""oracle""):
        method = DATE_TRUNC_GROUPERS[""oracle""].get(
            grouper, DATE_TRUNC_GROUPERS[""default""][grouper]
        )
        if '""' not in col:
            col = '""%s""' % col.upper()
    else:
        method = DATE_TRUNC_GROUPERS[""default""][grouper]
    return conn.ops.date_trunc_sql(method, col)
","if '""' not in col :",164
"def req(s, poll, msg, expect):
    do_req = True
    xid = None
    while True:
        # get transaction id
        if do_req:
            xid = s.put(msg)[""xid""]
        # wait for response
        events = poll.poll(2)
        for (fd, event) in events:
            response = s.get()
            if response[""xid""] != xid:
                do_req = False
                continue
            if response[""options""][""message_type""] != expect:
                raise Exception(""DHCP protocol error"")
            return response
        do_req = True
","if response [ ""options"" ] [ ""message_type"" ] != expect :",174
"def __init__(self, f):
    self._refs = {}
    self._peeled = {}
    for line in f.readlines():
        sha, name = line.rstrip(b""\n"").split(b""\t"")
        if name.endswith(ANNOTATED_TAG_SUFFIX):
            name = name[:-3]
            if not check_ref_format(name):
                raise ValueError(""invalid ref name %r"" % name)
            self._peeled[name] = sha
        else:
            if not check_ref_format(name):
                raise ValueError(""invalid ref name %r"" % name)
            self._refs[name] = sha
",if name . endswith ( ANNOTATED_TAG_SUFFIX ) :,171
"def get_defines(clang_output):
    import re
    defines = []
    for line in output.splitlines():
        m = re.search(r""#define ([\w()]+) (.+)"", line)
        if m is not None:
            defines.append(""-D{}={}"".format(m.group(1), m.group(2)))
        else:
            m = re.search(r""#define (\w+)"", line)
            if m is not None:
                defines.append(""-D{}"".format(m.group(1)))
    _log.debug(""Got defines: %s"", defines)
    return defines
",if m is not None :,155
"def clean_rcs_keywords(paragraph, keyword_substitutions):
    if len(paragraph) == 1 and isinstance(paragraph[0], nodes.Text):
        textnode = paragraph[0]
        for pattern, substitution in keyword_substitutions:
            match = pattern.search(textnode.data)
            if match:
                textnode.data = pattern.sub(substitution, textnode.data)
                return
",if match :,107
"def reorder_incremental_state(
    self, incremental_state: Dict[str, Dict[str, Optional[Tensor]]], new_order
):
    """"""Reorder buffered internal state (for incremental generation).""""""
    input_buffer = self._get_input_buffer(incremental_state)
    if input_buffer is not None:
        for k in input_buffer.keys():
            if input_buffer[k] is not None:
                input_buffer[k] = input_buffer[k].index_select(0, new_order)
        incremental_state = self._set_input_buffer(incremental_state, input_buffer)
    return incremental_state
",if input_buffer [ k ] is not None :,156
"def render(cls) -> str:
    buf = render_utils.RenderBuffer()
    buf.write(f""struct {cls.__name__} {{"")
    with buf.indent():
        for fieldname, field in cls._fields.items():
            if field.doc:
                buf.write_comment(field.doc)
            field.render_field(fieldname, buf)
            buf.newline()
    if buf.lastline() == """":
        buf.popline()
    buf.write(""};"")
    return str(buf)
",if field . doc :,134
"def prepare_text(text, style):
    body = []
    for fragment, sty in parse_tags(text, style, subs.styles):
        fragment = fragment.replace(r""\h"", "" "")
        fragment = fragment.replace(r""\n"", ""\n"")
        fragment = fragment.replace(r""\N"", ""\n"")
        if sty.italic:
            fragment = ""<i>%s</i>"" % fragment
        if sty.underline:
            fragment = ""<u>%s</u>"" % fragment
        if sty.strikeout:
            fragment = ""<s>%s</s>"" % fragment
        if sty.drawing:
            raise ContentNotUsable
        body.append(fragment)
    return re.sub(""\n+"", ""\n"", """".join(body).strip())
",if sty . italic :,198
"def _show_warnings(self):
    if self._warnings_handled:
        return
    self._warnings_handled = True
    if self._result and (self._result.has_next or not self._result.warning_count):
        return
    ws = self._get_db().show_warnings()
    if ws is None:
        return
    for w in ws:
        msg = w[-1]
        if PY2:
            if isinstance(msg, unicode):
                msg = msg.encode(""utf-8"", ""replace"")
        warnings.warn(err.Warning(*w[1:3]), stacklevel=4)
","if isinstance ( msg , unicode ) :",158
"def scrub_time(self, time):  # used externally to set time by slider scrubbing
    debug(""scrub_time: {0}"".format(time))
    if time == 0:
        self.loop_backward()
    elif time == self.timer_duration:
        self.loop_forward()
    else:  # time in between 0 and duration
        if self.timer_status == TIMER_STATUS_STOPPED:
            self.timer_status = TIMER_STATUS_PAUSED
        elif self.timer_status == TIMER_STATUS_EXPIRED:
            self.timer_status = TIMER_STATUS_PAUSED
    self.timer_time = time
",elif self . timer_status == TIMER_STATUS_EXPIRED :,163
"def _default_import_run(run, dest, move, copy_resources):
    if move:
        log.info(""Moving %s"", run.id)
        if copy_resources:
            shutil.copytree(run.path, dest)
            util.safe_rmtree(run.path)
        else:
            shutil.move(run.path, dest)
    else:
        log.info(""Copying %s"", run.id)
        shutil.copytree(run.path, dest, symlinks=not copy_resources)
",if copy_resources :,133
"def fn(n):
    while n < 3:
        if n < 0:
            yield ""less than zero""
        elif n == 0:
            yield ""zero""
        elif n == 1:
            yield ""one""
        else:
            yield ""more than one""
        n += 1
",if n < 0 :,84
"def _check_dep_names(self):
    """"""check if user input task_dep or setup_task that doesnt exist""""""
    # check task-dependencies exist.
    for task in self.tasks.values():
        for dep in task.task_dep:
            if dep not in self.tasks:
                msg = ""%s. Task dependency '%s' does not exist.""
                raise InvalidTask(msg % (task.name, dep))
        for setup_task in task.setup_tasks:
            if setup_task not in self.tasks:
                msg = ""Task '%s': invalid setup task '%s'.""
                raise InvalidTask(msg % (task.name, setup_task))
",if dep not in self . tasks :,176
"def urls():
    for scheme in (b""http"", b""https""):
        for host in (b""example.com"",):
            for port in (None, 100):
                for path in (b"""", b""path""):
                    if port is not None:
                        host = host + b"":"" + networkString(str(port))
                        yield urlunsplit((scheme, host, path, b"""", b""""))
",if port is not None :,112
"def split_hashes(cls, line):
    # type: (S) -> Tuple[S, List[S]]
    if ""--hash"" not in line:
        return line, []
    split_line = line.split()
    line_parts = []  # type: List[S]
    hashes = []  # type: List[S]
    for part in split_line:
        if part.startswith(""--hash""):
            param, _, value = part.partition(""="")
            hashes.append(value)
        else:
            line_parts.append(part)
    line = "" "".join(line_parts)
    return line, hashes
","if part . startswith ( ""--hash"" ) :",159
"def part(p, imaginary):
    # Represent infinity as 1e1000 and NaN as 1e1000-1e1000.
    s = ""j"" if imaginary else """"
    try:
        if math.isinf(p):
            if p < 0:
                return ""-1e1000"" + s
            return ""1e1000"" + s
        if math.isnan(p):
            return ""(1e1000%s-1e1000%s)"" % (s, s)
    except OverflowError:
        # math.isinf will raise this when given an integer
        # that's too large to convert to a float.
        pass
    return repr(p) + s
",if p < 0 :,168
"def _build_display_args(self, r):
    args = []
    if self.RESULT:
        if type(self.RESULT) != type([]):
            result = [self.RESULT]
        else:
            result = self.RESULT
        for name in result:
            value = getattr(r, name)
            # Displayed offsets should be offset by the base address
            if name == ""offset"":
                value += self.config.base
            args.append(value)
    return args
",if type ( self . RESULT ) != type ( [ ] ) :,139
"def cell_data_statusicon(column, cell, model, row, data):
    """"""Display text with an icon""""""
    try:
        state = model.get_value(row, data)
        if func_last_value[""cell_data_statusicon""] == state:
            return
        func_last_value[""cell_data_statusicon""] = state
        icon = ICON_STATE[state]
        # Supress Warning: g_object_set_qdata: assertion `G_IS_OBJECT (object)' failed
        original_filters = warnings.filters[:]
        warnings.simplefilter(""ignore"")
        try:
            cell.set_property(""pixbuf"", icon)
        finally:
            warnings.filters = original_filters
    except KeyError:
        pass
","if func_last_value [ ""cell_data_statusicon"" ] == state :",195
"def _para_exploit(self, params, part):
    if len(params) == 0:
        arr = [""*"", ""config""] + self._configs.keys()
        return suggest(arr, part)
    if len(params) == 1:
        arr = []
        if params[0] == ""config"":
            arr = self._configs.keys()
        if params[0] == ""*"":
            arr = [""stopOnFirst""]
        return suggest(arr, part)
    return []
","if params [ 0 ] == ""config"" :",124
"def send(self, data, flags=0, timeout=timeout_default):
    if timeout is timeout_default:
        timeout = self.timeout
    try:
        return self._sock.send(data, flags)
    except error as ex:
        if ex.args[0] not in _socketcommon.GSENDAGAIN or timeout == 0.0:
            raise
        sys.exc_clear()
        self._wait(self._write_event)
        try:
            return self._sock.send(data, flags)
        except error as ex2:
            if ex2.args[0] == EWOULDBLOCK:
                return 0
            raise
",if ex2 . args [ 0 ] == EWOULDBLOCK :,175
"def server_decode(self, buf):
    if self.has_recv_header:
        return (buf, True, False)
    self.has_recv_header = True
    crc = binascii.crc32(buf) & 0xFFFFFFFF
    if crc != 0xFFFFFFFF:
        self.has_sent_header = True
        if self.method == ""random_head"":
            return (b""E"" * 2048, False, False)
        return (buf, True, False)
    # (buffer_to_recv, is_need_decrypt, is_need_to_encode_and_send_back)
    return (b"""", False, True)
","if self . method == ""random_head"" :",158
"def Decode(self, filedesc):
    while True:
        chunk = filedesc.Read(4)
        if not chunk:
            return
        if chunk == b""QUUX"":
            yield b""NORF""
        if chunk == b""THUD"":
            yield b""BLARGH""
","if chunk == b""QUUX"" :",82
"def decProcess():
    while 1:
        yield clock.posedge, reset.negedge
        if reset == ACTIVE_LOW:
            count.next = 0
        else:
            if enable:
                if count == -n:
                    count.next = n - 1
                else:
                    count.next = count - 1
",if count == - n :,100
"def set_torrent_path(self, torrent_id, path):
    try:
        if not self.connect():
            return False
        self.client.core.set_torrent_move_completed_path(torrent_id, path).get()
        self.client.core.set_torrent_move_completed(torrent_id, 1).get()
    except Exception:
        return False
    finally:
        if self.client:
            self.disconnect()
    return True
",if not self . connect ( ) :,123
"def stale_rec(node, nodes):
    if node.abspath() in node.ctx.env[Build.CFG_FILES]:
        return
    if getattr(node, ""children"", []):
        for x in node.children.values():
            if x.name != ""c4che"":
                stale_rec(x, nodes)
    else:
        for ext in DYNAMIC_EXT:
            if node.name.endswith(ext):
                break
        else:
            if not node in nodes:
                if can_delete(node):
                    Logs.warn(""Removing stale file -> %r"", node)
                    node.delete()
","if x . name != ""c4che"" :",177
"def iterate(self, prod_, rule_):
    newProduction = """"
    for i in range(len(prod_)):
        step = self.production[i]
        if step == ""W"":
            newProduction = newProduction + self.ruleW
        elif step == ""X"":
            newProduction = newProduction + self.ruleX
        elif step == ""Y"":
            newProduction = newProduction + self.ruleY
        elif step == ""Z"":
            newProduction = newProduction + self.ruleZ
        elif step != ""F"":
            newProduction = newProduction + step
    self.drawLength = self.drawLength * 0.5
    self.generations += 1
    return newProduction
","elif step == ""X"" :",179
"def _get_app_params(self):
    params = self.cfg.params.copy()
    for key, value in self.__dict__.items():
        if key.startswith(""_""):
            continue
        elif key == ""console_parsed"":
            params[""parse_console""] = not value
        else:
            params[key] = value
    params[""load_config""] = False
    return params
","elif key == ""console_parsed"" :",102
"def __setitem__(self, key, value):
    if not isinstance(value, PseudoNamespace):
        tuple_converted = False
        if isinstance(value, dict):
            value = PseudoNamespace(value)
        elif isinstance(value, tuple):
            value = list(value)
            tuple_converted = True
        if isinstance(value, list):
            for i, item in enumerate(value):
                if isinstance(item, dict) and not isinstance(item, PseudoNamespace):
                    value[i] = PseudoNamespace(item)
            if tuple_converted:
                value = tuple(value)
    super(PseudoNamespace, self).__setitem__(key, value)
","elif isinstance ( value , tuple ) :",175
"def getNextSibling(self, node):
    if isinstance(node, tuple):  # Text node
        node, key = node
        assert key in (""text"", ""tail""), ""Text nodes are text or tail, found %s"" % key
        if key == ""text"":
            # XXX: we cannot use a ""bool(node) and node[0] or None"" construct here
            # because node[0] might evaluate to False if it has no child element
            if len(node):
                return node[0]
            else:
                return None
        else:  # tail
            return node.getnext()
    return (node, ""tail"") if node.tail else node.getnext()
",if len ( node ) :,182
"def star_path(path):
    """"""Replace integers and integer-strings in a path with *""""""
    path = list(path)
    for i, p in enumerate(path):
        if isinstance(p, int):
            path[i] = ""*""
        else:
            if not isinstance(p, text_type):
                p = p.decode()
            if r_is_int.match(p):
                path[i] = ""*""
    return join_path(path)
","if not isinstance ( p , text_type ) :",127
"def ensure_popup_selection(self):
    try:
        self.__position_at_mouse
    except AttributeError:
        path, col = self.get_cursor()
        if path is None:
            return False
        self.scroll_to_cell(path, col)
        # ensure current cursor path is selected, just like right-click
        selection = self.get_selection()
        if not selection.path_is_selected(path):
            selection.unselect_all()
            selection.select_path(path)
        return True
",if path is None :,141
"def release(self):
    me, lock_count = self.__begin()
    try:
        if me is None:
            return
        self._count = count = self._count - 1
        if not count:
            self._owner = None
            self._block.release()
    finally:
        self.__end(me, lock_count)
",if me is None :,92
"def date_match(self, date1, date2):
    if date1.is_empty() or date2.is_empty():
        return 0
    if date1.is_equal(date2):
        return 1
    if date1.is_compound() or date2.is_compound():
        return self.range_compare(date1, date2)
    if date1.get_year() == date2.get_year():
        if date1.get_month() == date2.get_month():
            return 0.75
        if not date1.get_month_valid() or not date2.get_month_valid():
            return 0.75
        else:
            return -1
    else:
        return -1
",if not date1 . get_month_valid ( ) or not date2 . get_month_valid ( ) :,189
"def onMinimize(self, sender):
    if self._runDialogListener(""onMinimize"") is False:
        return
    widget = self.child
    if widget is not None:
        if widget.isVisible():
            widget.setVisible(False)
            self.setHeight("""")
            self.setWidth("""")
            if self._maximized:
                self._minimized = self._maximized
                self._toggleMaximize()
            else:
                self._minimized = None
        else:
            if self._minimized is not None:
                self._toggleMaximize()
            widget.setVisible(True)
",if widget . isVisible ( ) :,171
"def instance_reader():
    for epoch_index in range(epoch):
        if shuffle:
            if shuffle_seed is not None:
                np.random.seed(shuffle_seed)
            np.random.shuffle(examples)
        if phase == ""train"":
            self.current_train_epoch = epoch_index
        for (index, example) in enumerate(examples):
            if phase == ""train"":
                self.current_train_example = index + 1
            feature = self.convert_example(
                index, example, self.get_labels(), self.max_seq_len, self.tokenizer
            )
            instance = self.generate_instance(feature)
            yield instance
",if shuffle :,189
"def _parse_lines(self, linesource):
    """"""Parse lines of text for functions and classes""""""
    functions = []
    classes = []
    for line in linesource:
        if line.startswith(""def "") and line.count(""(""):
            # exclude private stuff
            name = self._get_object_name(line)
            if not name.startswith(""_""):
                functions.append(name)
        elif line.startswith(""class ""):
            # exclude private stuff
            name = self._get_object_name(line)
            if not name.startswith(""_""):
                classes.append(name)
        else:
            pass
    functions.sort()
    classes.sort()
    return functions, classes
","elif line . startswith ( ""class "" ) :",185
"def get_folder_version(folder):
    f = os.path.join(code_path, folder, ""version.txt"")
    try:
        with open(f) as fd:
            content = fd.read()
            p = re.compile(r""([0-9]+)\.([0-9]+)\.([0-9]+)"")
            m = p.match(content)
            if m:
                version = m.group(1) + ""."" + m.group(2) + ""."" + m.group(3)
                return version
    except:
        return False
",if m :,151
"def __init__(
    self, plugin_name=None, builtin=False, deprecated=False, config=None, session=None
):
    if builtin and isinstance(builtin, (str, unicode)):
        builtin = os.path.basename(builtin)
        for ignore in ("".py"", "".pyo"", "".pyc""):
            if builtin.endswith(ignore):
                builtin = builtin[: -len(ignore)]
        if builtin not in self.LOADED:
            self.LOADED.append(builtin)
    self.loading_plugin = plugin_name
    self.loading_builtin = plugin_name and builtin
    self.builtin = builtin
    self.deprecated = deprecated
    self.session = session
    self.config = config
    self.manifests = []
",if builtin not in self . LOADED :,181
"def setInt(self, path, value, **kwargs):
    if value is None:
        self.set(path, None, **kwargs)
        return
    minimum = kwargs.pop(""min"", None)
    maximum = kwargs.pop(""max"", None)
    try:
        intValue = int(value)
        if minimum is not None and intValue < minimum:
            intValue = minimum
        if maximum is not None and intValue > maximum:
            intValue = maximum
    except ValueError:
        self._logger.warning(
            ""Could not convert %r to a valid integer when setting option %r""
            % (value, path)
        )
        return
    self.set(path, intValue, **kwargs)
",if minimum is not None and intValue < minimum :,187
"def __call__(self, session_path):
    """"""Get raw session object from `session_path`.""""""
    new_session = copy.deepcopy(self._template)
    session_keys = new_session.keys()
    old_session = self._load_file(session_path)
    for attribute in dir(self):
        if attribute.startswith(""set_""):
            target = attribute[4:].capitalize()
            if target not in session_keys:
                raise ValueError(""Invalid attribute: %r"" % attribute)
            function = getattr(self, attribute)
            new_session[target] = function(old_session)
    return new_session
","if attribute . startswith ( ""set_"" ) :",161
"def add_comment_to_directory(args, dir_path):
    for root, _, files in os.walk(dir_path):
        for file_name in files:
            if not re.match(r"".*(\.c|\.h|\.cpp|\.hpp|\.cxx|\.hxx)$"", file_name):
                continue
            file_path = os.path.join(root, file_name)
            add_comment_to_file(args, file_path)
","if not re . match ( r"".*(\.c|\.h|\.cpp|\.hpp|\.cxx|\.hxx)$"" , file_name ) :",122
"def reportMemory(k, options, field=None, isBytes=False):
    """"""Given k kilobytes, report back the correct format as string.""""""
    if options.pretty:
        return prettyMemory(int(k), field=field, isBytes=isBytes)
    else:
        if isBytes:
            k /= 1024.0
        if field is not None:
            return ""%*dK"" % (field - 1, k)  # -1 for the ""K""
        else:
            return ""%dK"" % int(k)
",if isBytes :,135
"def resolve(self, arguments):
    positional = []
    named = {}
    for arg in arguments:
        if self._is_named(arg):
            self._add_named(arg, named)
        elif named:
            self._raise_positional_after_named()
        else:
            positional.append(arg)
    return positional, named
",elif named :,92
"def _load_from_cache(self):
    if self._cache_key in self._cache:
        creds = deepcopy(self._cache[self._cache_key])
        if not self._is_expired(creds):
            return creds
        else:
            logger.debug(""Credentials were found in cache, but they are expired."")
    return None
",if not self . _is_expired ( creds ) :,88
"def convertstore(self, inputstore, includefuzzy=False):
    """"""converts a file to .lang format""""""
    thetargetfile = lang.LangStore(mark_active=self.mark_active)
    # Run over the po units
    for pounit in inputstore.units:
        if pounit.isheader() or not pounit.istranslatable():
            continue
        newunit = thetargetfile.addsourceunit(pounit.source)
        if includefuzzy or not pounit.isfuzzy():
            newunit.settarget(pounit.target)
        else:
            newunit.settarget("""")
        if pounit.getnotes(""developer""):
            newunit.addnote(pounit.getnotes(""developer""), ""developer"")
    return thetargetfile
","if pounit . getnotes ( ""developer"" ) :",196
"def __init__(self, *args, **kwargs):
    super().__init__(*args, **kwargs)
    for exclude_field in self.context[""request""].query_params.getlist(""exclude""):
        p = exclude_field.split(""."")
        if p[0] in self.fields:
            if len(p) == 1:
                del self.fields[p[0]]
            elif len(p) == 2:
                self.fields[p[0]].child.fields.pop(p[1])
",if p [ 0 ] in self . fields :,130
"def __init__(self, fn, args, resources):
    self.fn = fn
    self.args = copy.deepcopy(args)
    self.resources = resources
    with Task.LOCK:
        self.task_id = Task.TASK_ID.value
        if ""args"" in self.args:
            if isinstance(
                self.args[""args""], (argparse.Namespace, argparse.ArgumentParser)
            ):
                args_dict = vars(self.args[""args""])
            else:
                args_dict = self.args[""args""]
            args_dict.update({""task_id"": self.task_id})
        Task.TASK_ID.value += 1
","if ""args"" in self . args :",176
"def _expand_nsplit_by_reduce(splits, reduced):
    if reduced == 1:
        return splits
    out = []
    for s in splits:
        x = s
        part = max(x / reduced, 1)
        while x >= 2 * part:
            out.append(int(part))
            x -= int(part)
        if x:
            out.append(x)
    assert sum(splits) == sum(out)
    return tuple(out)
",if x :,125
"def OnDeleteLine(self, items):
    for n in items:
        if n >= 0:
            name1 = self.items[n][2]
            name2 = self.items[n][4]
            del self.items[n]
            if name1 in self.bindiff.matched1:
                self.bindiff.matched1.remove(name1)
            if name2 in self.bindiff.matched2:
                self.bindiff.matched2.remove(name2)
    return [Choose.ALL_CHANGED] + items
",if n >= 0 :,146
"def _to_str(self, tokens: List[int]) -> str:
    pos = next(
        (idx for idx, x in enumerate(tokens) if x == self.vocab.eos_token_id), -1
    )
    if pos != -1:
        tokens = tokens[:pos]
    vocab_map = self.vocab.id_to_token_map_py
    words = [vocab_map[t] for t in tokens]
    if self.encoding is not None and self.perform_decode:
        if self.encoding == ""bpe"":
            words = self.bpe_decode(words)
        elif self.encoding == ""spm"":
            words = self.spm_decode(words)
    sentence = "" "".join(words)
    return sentence
","if self . encoding == ""bpe"" :",188
"def detect(content, **kwargs):
    headers = kwargs.get(""headers"", {})
    content = str(content)
    detection_schema = (
        re.compile(r""\Abarra.counter.session(=)?"", re.I),
        re.compile(r""(\A|\b)?barracuda."", re.I),
        re.compile(r""barracuda.networks(.)?.inc"", re.I),
    )
    for detection in detection_schema:
        if detection.search(headers.get(HTTP_HEADER.SET_COOKIE, """")) is not None:
            return True
        if detection.search(content) is not None:
            return True
","if detection . search ( headers . get ( HTTP_HEADER . SET_COOKIE , """" ) ) is not None :",165
"def _finish_port_forward(self, listener, listen_host, listen_port):
    """"""Finish processing a TCP/IP port forwarding request""""""
    if asyncio.iscoroutine(listener):
        try:
            listener = yield from listener
        except OSError:
            listener = None
    if listener:
        if listen_port == 0:
            listen_port = listener.get_port()
            result = UInt32(listen_port)
        else:
            result = True
        self._local_listeners[listen_host, listen_port] = listener
        self._report_global_response(result)
    else:
        self.logger.debug1(""Failed to create TCP listener"")
        self._report_global_response(False)
",if listen_port == 0 :,192
"def start(self):
    """"""Start running the mainloop.""""""
    with self:
        result = pa.pa_threaded_mainloop_start(self._pa_threaded_mainloop)
        if result < 0:
            raise PulseAudioException(0, ""Failed to start PulseAudio mainloop"")
    assert _debug(""PulseAudioMainLoop: Started"")
",if result < 0 :,85
"def service(self):
    try:
        try:
            self.start()
            self.execute()
            self.finish()
        except socket.error:
            self.close_on_finish = True
            if self.channel.adj.log_socket_errors:
                raise
    finally:
        pass
",if self . channel . adj . log_socket_errors :,91
"def _makepath(self, path):
    if not self.abspath:
        try:
            np = py.path.local().bestrelpath(path)
        except OSError:
            return path
        if len(np) < len(str(path)):
            path = np
    return path
",if len ( np ) < len ( str ( path ) ) :,78
"def upload(
    youtube_resource, video_path, body, chunksize=1024 * 1024, progress_callback=None
):
    body_keys = "","".join(body.keys())
    media = MediaFileUpload(video_path, chunksize=chunksize, resumable=True)
    videos = youtube_resource.videos()
    request = videos.insert(part=body_keys, body=body, media_body=media)
    while 1:
        status, response = request.next_chunk()
        if response:
            if ""id"" in response:
                return response[""id""]
            else:
                raise KeyError(""Response has no 'id' field"")
        elif status and progress_callback:
            progress_callback(status.total_size, status.resumable_progress)
",if response :,197
"def __init__(self, *args, **kwargs):
    super().__init__(*args, **kwargs)
    for exclude_field in self.context[""request""].query_params.getlist(""exclude""):
        p = exclude_field.split(""."")
        if p[0] in self.fields:
            if len(p) == 1:
                del self.fields[p[0]]
            elif len(p) == 2:
                self.fields[p[0]].child.fields.pop(p[1])
",if len ( p ) == 1 :,130
"def on_button_press_event(self, iconview, event):
    # print('on_button_press_event')
    if event.button == 3:
        popup_menu = Gtk.Menu()
        x = int(event.x)
        y = int(event.y)
        time = event.time
        pathinfo = iconview.get_path_at_pos(x, y)
        if pathinfo is not None:
            iconview.grab_focus()
            self.do_populate_popup(popup_menu, pathinfo)
            # FIXME should use a signal here
            gtk_popup_at_pointer(popup_menu, event)
            return True
    return False
",if pathinfo is not None :,180
"def __rshift__(self, other):
    if not self.symbolic and type(other) is int:
        return RegisterOffset(
            self._bits, self.reg, self._to_signed(self.offset >> other)
        )
    else:
        if self.symbolic:
            return RegisterOffset(self._bits, self.reg, self.offset >> other)
        else:
            return RegisterOffset(
                self._bits,
                self.reg,
                ArithmeticExpression(
                    ArithmeticExpression.RShift,
                    (
                        self.offset,
                        other,
                    ),
                ),
            )
",if self . symbolic :,192
"def _slice_positional_metadata(self, indexable):
    if self.has_positional_metadata():
        if _is_single_index(indexable):
            index = _single_index_to_slice(indexable)
        else:
            index = indexable
        return self.positional_metadata.iloc[index]
    else:
        return None
",if _is_single_index ( indexable ) :,91
"def _show_env(name=None):
    if name is None:
        name = """"
    env = peda.execute_redirect(""show env"")
    for line in env.splitlines():
        (k, v) = line.split(""="", 1)
        if k.startswith(name):
            msg(""%s = %s"" % (k, v if is_printable(v) else to_hexstr(v)))
    return
",if k . startswith ( name ) :,106
"def skip_to_semicolon(s, i):
    n = len(s)
    while i < n:
        c = s[i]
        if c == "";"":
            return i
        elif c == ""'"" or c == '""':
            i = g.skip_string(s, i)
        elif g.match(s, i, ""//""):
            i = g.skip_to_end_of_line(s, i)
        elif g.match(s, i, ""/*""):
            i = g.skip_block_comment(s, i)
        else:
            i += 1
    return i
","elif g . match ( s , i , ""//"" ) :",161
"def filter_iterable(cls, iterable, filterset_class, filters_name, info, **args):
    filter_input = args.get(filters_name)
    if filter_input and filterset_class:
        instance = filterset_class(
            data=dict(filter_input), queryset=iterable, request=info.context
        )
        # Make sure filter input has valid values
        if not instance.is_valid():
            raise GraphQLError(json.dumps(instance.errors.get_json_data()))
        iterable = instance.qs
    return iterable
",if not instance . is_valid ( ) :,137
"def build(opt):
    dpath = os.path.join(opt[""datapath""], ""self_feeding"")
    version = ""3.1""
    if not build_data.built(dpath, version):
        print(""[building data: "" + dpath + ""]"")
        if build_data.built(dpath):
            # An older version exists, so remove these outdated files.
            build_data.remove_dir(dpath)
        build_data.make_dir(dpath)
        # Download the data.
        for downloadable_file in RESOURCES:
            downloadable_file.download_file(dpath)
        build_data.mark_done(dpath, version)
",if build_data . built ( dpath ) :,169
"def get_tokens_unprocessed(self, text):
    for index, token, value in RegexLexer.get_tokens_unprocessed(self, text):
        if token is Name:
            if self.stdlibhighlighting and value in self.stdlib_types:
                token = Keyword.Type
            elif self.c99highlighting and value in self.c99_types:
                token = Keyword.Type
            elif self.platformhighlighting and value in self.linux_types:
                token = Keyword.Type
        yield index, token, value
",elif self . c99highlighting and value in self . c99_types :,141
"def searchOpcode(self, opcode, name=None):
    to_return = {}
    if not name:
        for file in self.__files:
            to_return[file.loader.fileName] = self.__ropper.searchOpcode(
                file.loader, opcode
            )
    else:
        fc = self.getFileFor(name)
        if not fc:
            raise RopperError(""No such file opened: %s"" % name)
        to_return[name] = self.__ropper.searchOpcode(fc.loader, opcode)
    return self.__filterBadBytes(to_return)
",if not fc :,158
"def logic():
    while 1:
        yield clock.posedge, reset.negedge
        if reset == ACTIVE_LOW:
            count.next = 0
        else:
            if enable:
                if count == -n:
                    count.next = n - 1
                else:
                    count.next = count - 1
",if reset == ACTIVE_LOW :,99
"def upgrade_cursor(cursor):
    count = 0
    prefix = pack_be_uint16(cursor)
    key_len = HASHX_LEN + 2
    chunks = util.chunks
    with self.db.write_batch() as batch:
        batch_put = batch.put
        for key, hist in self.db.iterator(prefix=prefix):
            # Ignore non-history entries
            if len(key) != key_len:
                continue
            count += 1
            hist = b"""".join(item + b""\0"" for item in chunks(hist, 4))
            batch_put(key, hist)
        self.upgrade_cursor = cursor
        self.write_state(batch)
    return count
",if len ( key ) != key_len :,187
"def fork(receiver: Receiver, func, *args, **kwargs):
    current_actor = self()
    send(Fork(current_actor, func, args, kwargs), receiver)
    while True:
        message = recv(current_actor)
        if isinstance(message, ForkResponse):
            return message.new_actor
        else:
            send(message, current_actor)
    return
","if isinstance ( message , ForkResponse ) :",100
"def history_move(self, n):
    from ranger.container.history import HistoryEmptyException
    try:
        current = self.history.current()
    except HistoryEmptyException:
        pass
    else:
        if self.line != current and self.line != self.history.top():
            self.history.modify(self.line)
        self.history.move(n)
        current = self.history.current()
        if self.line != current:
            self.line = self.history.current()
            self.pos = len(self.line)
",if self . line != current and self . line != self . history . top ( ) :,147
"def fullname(self):
    if self._fullname is None:
        pkg_name = namespace.apply_namespace(self.dist.project_name)
        if pkg_name and pkg_name != ""."":
            self._fullname = ""%s/%s"" % (pkg_name, self.name)
        else:
            self._fullname = self.name
    return self._fullname
","if pkg_name and pkg_name != ""."" :",94
"def do_install(datafilename):
    ifile = open(datafilename, ""rb"")
    d = pickle.load(ifile)
    destdir_var = ""DESTDIR""
    if destdir_var in os.environ:
        if d.prefix[0] == ""/"":
            subdir = d.prefix[1:]
        else:
            subdir = d.prefix
        d.prefix = os.path.join(os.environ[destdir_var], subdir)
    install_targets(d)
    install_headers(d)
    install_man(d)
    install_data(d)
    install_po(d)
","if d . prefix [ 0 ] == ""/"" :",155
"def truncate(self, size=None):
    # type: (Optional[int]) -> int
    # Inefficient, but I don't know if truncate is possible with ftp
    with self._lock:
        if size is None:
            size = self.tell()
        with self.fs.openbin(self.path) as f:
            data = f.read(size)
        with self.fs.openbin(self.path, ""w"") as f:
            f.write(data)
            if len(data) < size:
                f.write(b""\0"" * (size - len(data)))
    return size
",if size is None :,163
"def write(self, expression, location=None):
    # If the phrase is incomplete, utop will not remember it, so
    # we need to account for it here. Also, Shift+Enter will add a literal
    # newline, which would otherwise break protocol.
    for line in expression.split(""\n""):
        self._phrase.append(line)
        if location is not None:
            self._phrase_line_begins.append(location)
            location += len(line) + 1
    self.write_command(""input"", ""allow-incomplete"", self._phrase)
",if location is not None :,139
"def scan_iter(self, match=None, count=None):
    nodes = await self.cluster_nodes()
    for node in nodes:
        if ""master"" in node[""flags""]:
            cursor = ""0""
            while cursor != 0:
                pieces = [cursor]
                if match is not None:
                    pieces.extend([""MATCH"", match])
                if count is not None:
                    pieces.extend([""COUNT"", count])
                response = await self.execute_command_on_nodes([node], ""SCAN"", *pieces)
                cursor, data = list(response.values())[0]
                for item in data:
                    yield item
",if match is not None :,185
"def communicate(self, input_data=None):
    """"""Mock subprocess.Popen.communicate.""""""
    for i in range(2):
        timeout = execute_time if i == 0 else sigterm_handler_time
        try:
            received_signal = self.signal_queue.get(block=True, timeout=timeout)
        except queue.Empty:
            continue
        self.received_signals.append((received_signal, time.time() - self.start_time))
        if received_signal == Signal.KILL:
            break
    return output, None
",if received_signal == Signal . KILL :,142
"def _add_bookmark_breakpoint(self):
    """"""Add a bookmark or breakpoint to the current file in the editor.""""""
    editorWidget = self.ide.mainContainer.get_actual_editor()
    if editorWidget and editorWidget.hasFocus():
        if self.ide.mainContainer.actualTab.navigator.operation == 1:
            editorWidget._sidebarWidget.set_bookmark(
                editorWidget.textCursor().blockNumber()
            )
        elif self.ide.mainContainer.actualTab.navigator.operation == 2:
            editorWidget._sidebarWidget.set_breakpoint(
                editorWidget.textCursor().blockNumber()
            )
",elif self . ide . mainContainer . actualTab . navigator . operation == 2 :,175
"def _should_auto_select_container_version(instance_type, distribution):
    """"""Returns a boolean that indicates whether to use an auto-selected container version.""""""
    p4d = False
    if instance_type:
        # looks for either ""ml.<family>.<size>"" or ""ml_<family>""
        match = re.match(r""^ml[\._]([a-z\d]+)\.?\w*$"", instance_type)
        if match:
            family = match[1]
            p4d = family == ""p4d""
    smdistributed = False
    if distribution:
        smdistributed = ""smdistributed"" in distribution
    return p4d or smdistributed
",if match :,164
"def _flush_some_if_lockable(self):
    # Since our task may be appending to the outbuf, we try to acquire
    # the lock, but we don't block if we can't.
    if self.outbuf_lock.acquire(False):
        try:
            self._flush_some()
            if self.total_outbufs_len < self.adj.outbuf_high_watermark:
                self.outbuf_lock.notify()
        finally:
            self.outbuf_lock.release()
",if self . total_outbufs_len < self . adj . outbuf_high_watermark :,134
"def add_auth(self, req, **kwargs):
    if not ""x-amz-content-sha256"" in req.headers:
        if ""_sha256"" in req.headers:
            req.headers[""x-amz-content-sha256""] = req.headers.pop(""_sha256"")
        else:
            req.headers[""x-amz-content-sha256""] = self.payload(req)
    req = self.mangle_path_and_params(req)
    return super(S3HmacAuthV4Handler, self).add_auth(req, **kwargs)
","if ""_sha256"" in req . headers :",145
"def get_objects(self):
    list_type, id, handles, timestamp = self._obj_list
    retval = []
    for (target, handle) in handles:
        _class = map2class(target)
        if _class:
            obj = _class(self._dbstate, pickle.dumps((target, id, handle, timestamp)))
            if obj:
                retval.append(obj)
    return retval
",if obj :,108
"def toggle_fullscreen_hide_tabbar(self):
    if self.is_fullscreen():
        if self.settings.general.get_boolean(""fullscreen-hide-tabbar""):
            if self.guake and self.guake.notebook_manager:
                self.guake.notebook_manager.set_notebooks_tabbar_visible(False)
    else:
        if self.guake and self.guake.notebook_manager:
            v = self.settings.general.get_boolean(""window-tabbar"")
            self.guake.notebook_manager.set_notebooks_tabbar_visible(v)
",if self . guake and self . guake . notebook_manager :,159
"def __repr__(self):
    parts = []
    if not approx_equal(self.constant, 0.0) or self.is_constant:
        parts.append(repr(self.constant))
    for clv, coeff in sorted(self.terms.items(), key=lambda x: repr(x)):
        if approx_equal(coeff, 1.0):
            parts.append(repr(clv))
        else:
            parts.append(repr(coeff) + ""*"" + repr(clv))
    return "" + "".join(parts)
","if approx_equal ( coeff , 1.0 ) :",135
"def wrapper(*args, **kwds):
    global bootstrap_logger_enabled
    if bootstrap_logger_enabled:
        if level == ""EXCEPTION"":
            bootstrap_logger.exception(msg=args[0])
        else:
            bootstrap_logger.log(level=level, msg=args[0])
    return f(*args, **kwds)
","if level == ""EXCEPTION"" :",86
"def get_sorted_entry(field, bookid):
    if field == ""title"" or field == ""authors"":
        book = calibre_db.get_filtered_book(bookid)
        if book:
            if field == ""title"":
                return json.dumps({""sort"": book.sort})
            elif field == ""authors"":
                return json.dumps({""author_sort"": book.author_sort})
    return """"
","elif field == ""authors"" :",111
"def movies_iterator():
    for row in self._tuple_iterator(query):
        id, guid, movie = self._parse(fields, row, offset=2)
        # Parse `guid` (if enabled, and not already parsed)
        if parse_guid:
            if id not in guids:
                guids[id] = Guid.parse(guid)
            guid = guids[id]
        # Return item
        yield id, guid, movie
",if id not in guids :,119
"def update_sockets(self, context):
    bools = [self.min_list, self.max_list, self.size_list]
    dims = int(self.dimensions[0])
    for i in range(3):
        for j in range(3):
            out_index = 4 + j + 3 * i
            hidden = self.outputs[out_index].hide_safe
            if bools[i][j] and j < dims:
                if hidden:
                    self.outputs[out_index].hide_safe = False
            else:
                self.outputs[out_index].hide_safe = True
        updateNode(self, context)
",if hidden :,173
"def broadcast(self, msg, eid):
    for s in self.subs:
        if type(self.subs[s].eid) is list:
            if eid in self.subs[s].eid:
                self.subs[s].write_message(msg)
        else:
            if self.subs[s].eid == eid:
                self.subs[s].write_message(msg)
",if eid in self . subs [ s ] . eid :,111
"def as_create_delta(
    self: CallableObjectT,
    schema: s_schema.Schema,
    context: so.ComparisonContext,
) -> sd.ObjectCommand[CallableObjectT]:
    delta = super().as_create_delta(schema, context)
    new_params = self.get_params(schema).objects(schema)
    for p in new_params:
        if not param_is_inherited(schema, self, p):
            delta.add_prerequisite(
                p.as_create_delta(schema=schema, context=context),
            )
    return delta
","if not param_is_inherited ( schema , self , p ) :",149
"def set_indentation_params(self, ispythonsource, guess=True):
    if guess and ispythonsource:
        i = self.guess_indent()
        if 2 <= i <= 8:
            self.indentwidth = i
        if self.indentwidth != self.tabwidth:
            self.usetabs = False
    self.set_tabwidth(self.tabwidth)
",if 2 <= i <= 8 :,97
"def _test():
    """"""Simple test program to disassemble a file.""""""
    argc = len(sys.argv)
    if argc != 2:
        if argc == 1:
            fn = __file__
        else:
            sys.stderr.write(""usage: %s [-|CPython compiled file]\n"" % __file__)
            sys.exit(2)
    else:
        fn = sys.argv[1]
    disassemble_file(fn)
",if argc == 1 :,114
"def set_lineno(self, lineno, override=False):
    """"""Set the line numbers of the node and children.""""""
    todo = deque([self])
    while todo:
        node = todo.popleft()
        if ""lineno"" in node.attributes:
            if node.lineno is None or override:
                node.lineno = lineno
        todo.extend(node.iter_child_nodes())
    return self
","if ""lineno"" in node . attributes :",103
"def _connect(s, address):
    try:
        s.connect(address)
    except socket.error:
        (ty, v) = sys.exc_info()[:2]
        if hasattr(v, ""errno""):
            v_err = v.errno
        else:
            v_err = v[0]
        if v_err not in [errno.EINPROGRESS, errno.EWOULDBLOCK, errno.EALREADY]:
            raise v
","if hasattr ( v , ""errno"" ) :",120
"def SurroundedByParens(token):
    """"""Check if it's an expression surrounded by parentheses.""""""
    while token:
        if token.value == "","":
            return False
        if token.value == "")"":
            return not token.next_token
        if token.OpensScope():
            token = token.matching_bracket.next_token
        else:
            token = token.next_token
    return False
","if token . value == "","" :",109
"def read_vocab_list(path, max_vocab_size=20000):
    vocab = {""<eos>"": 0, ""<unk>"": 1}
    with io.open(path, encoding=""utf-8"", errors=""ignore"") as f:
        for l in f:
            w = l.strip()
            if w not in vocab and w:
                vocab[w] = len(vocab)
            if len(vocab) >= max_vocab_size:
                break
    return vocab
",if len ( vocab ) >= max_vocab_size :,125
"def _messageHandled(self, resultList):
    failures = 0
    for (success, result) in resultList:
        if not success:
            failures += 1
            log.err(result)
    if failures:
        msg = ""Could not send e-mail""
        resultLen = len(resultList)
        if resultLen > 1:
            msg += "" ({} failures out of {} recipients)"".format(failures, resultLen)
        self.sendCode(550, networkString(msg))
    else:
        self.sendCode(250, b""Delivery in progress"")
",if resultLen > 1 :,147
"def test_images_p_is_stochastic_parameter(self):
    aug = self.create_aug(p=iap.Choice([0, 1], p=[0.7, 0.3]))
    seen = [0, 0]
    for _ in sm.xrange(1000):
        observed = aug.augment_image(self.image)
        if np.array_equal(observed, self.image):
            seen[0] += 1
        elif np.array_equal(observed, self.image_flipped):
            seen[1] += 1
        else:
            assert False
    assert np.allclose(seen, [700, 300], rtol=0, atol=75)
","elif np . array_equal ( observed , self . image_flipped ) :",168
"def kill(self):
    # check and execute the 'kill' method if present
    if self.has_kill:
        try:
            kill_method = getattr(self.module_class, ""kill"")
            if self.has_kill == self.PARAMS_NEW:
                kill_method()
            else:
                # legacy call parameters
                kill_method(
                    self.i3status_thread.json_list,
                    self.config[""py3_config""][""general""],
                )
        except Exception:
            # this would be stupid to die on exit
            pass
",if self . has_kill == self . PARAMS_NEW :,169
"def remove_topic(self, topic):
    if topic not in self.messages:
        return
    del self.messages[topic]
    for sub in self.subscribers.get(topic, set()):
        if hasattr(sub, ""_pyroRelease""):
            sub._pyroRelease()
        if hasattr(sub, ""_pyroUri""):
            try:
                proxy = self.proxy_cache[sub._pyroUri]
                proxy._pyroRelease()
                del self.proxy_cache[sub._pyroUri]
            except KeyError:
                pass
    del self.subscribers[topic]
","if hasattr ( sub , ""_pyroRelease"" ) :",160
"def run_async(self, source, target, reverse):
    to_load = target or self.get_next(source, reverse)
    if not to_load:
        return
    view_signature = self.view_signatures[to_load]
    window = self.view.window()
    if window:
        window.run_command(self.commands[to_load])
        if not self.view.settings().get(view_signature):
            sublime.set_timeout_async(self.view.close)
",if not self . view . settings ( ) . get ( view_signature ) :,129
"def eval_operand(assembly, start, stop, prefix=""""):
    imm = assembly[start + 1 : stop]
    try:
        eval_imm = eval(imm)
        if eval_imm > 0x80000000:
            eval_imm = 0xFFFFFFFF - eval_imm
            eval_imm += 1
            eval_imm = -eval_imm
        return assembly.replace(prefix + imm, prefix + hex(eval_imm))
    except:
        return assembly
",if eval_imm > 0x80000000 :,122
"def admin():
    if Configuration.loginRequired():
        if not current_user.is_authenticated():
            return render_template(""login.html"")
    else:
        person = User.get(""_dummy_"")
        login_user(person)
    output = None
    if os.path.isfile(Configuration.getUpdateLogFile()):
        with open(Configuration.getUpdateLogFile()) as updateFile:
            separator = ""==========================\n""
            output = updateFile.read().split(separator)[-2:]
            output = separator + separator.join(output)
    return render_template(""admin.html"", status=""default"", **adminInfo(output))
",if not current_user . is_authenticated ( ) :,164
"def data(self):
    result = """"
    for hunk in self._hunks:
        if isinstance(hunk, tuple) and len(hunk) == 2:
            hunk, f = hunk
        else:
            f = lambda x: x
        result += f(hunk.data())
    return result
","if isinstance ( hunk , tuple ) and len ( hunk ) == 2 :",85
"def not_less_witness(self, other):
    n = max(self.longest_run_of_spaces(), other.longest_run_of_spaces()) + 1
    a = []
    for ts in range(1, n + 1):
        if self.indent_level(ts) >= other.indent_level(ts):
            a.append((ts, self.indent_level(ts), other.indent_level(ts)))
    return a
",if self . indent_level ( ts ) >= other . indent_level ( ts ) :,110
"def _validate(self) -> None:
    indent = self.indent
    if indent is not None:
        if len(indent) == 0:
            raise CSTValidationError(
                ""An indented block must have a non-zero width indent.""
            )
        if _INDENT_WHITESPACE_RE.fullmatch(indent) is None:
            raise CSTValidationError(
                ""An indent must be composed of only whitespace characters.""
            )
",if len ( indent ) == 0 :,116
"def sanitize_numeric_fields(info):
    for numeric_field in self._NUMERIC_FIELDS:
        field = info.get(numeric_field)
        if field is None or isinstance(field, compat_numeric_types):
            continue
        report_force_conversion(numeric_field, ""numeric"", ""int"")
        info[numeric_field] = int_or_none(field)
","if field is None or isinstance ( field , compat_numeric_types ) :",96
"def count(self):
    if self._should_cache(""count""):
        # Optmization borrowed from overriden method:
        # if queryset cache is already filled just return its len
        if self._result_cache is not None:
            return len(self._result_cache)
        return cached_as(self)(lambda: self._no_monkey.count(self))()
    else:
        return self._no_monkey.count(self)
",if self . _result_cache is not None :,112
