cleaned_method,target_block,tokens_in_method
"def read(self, count=True, timeout=None, ignore_non_errors=True, ignore_timeouts=True):
    try:
        return self._read(count, timeout)
    except usb.USBError as e:
        if DEBUG_COMM:
            log.info(
                ""read: e.errno=%s e.strerror=%s e.message=%s repr=%s""
                % (e.errno, e.strerror, e.message, repr(e))
            )
        if ignore_timeouts and is_timeout(e):
            return []
        if ignore_non_errors and is_noerr(e):
            return []
        raise
",if ignore_timeouts and is_timeout ( e ) :,174
"def _cache_mem(curr_out, prev_mem, mem_len, reuse_len=None):
    """"""cache hidden states into memory.""""""
    if mem_len is None or mem_len == 0:
        return None
    else:
        if reuse_len is not None and reuse_len > 0:
            curr_out = curr_out[:reuse_len]
        if prev_mem is None:
            new_mem = curr_out[-mem_len:]
        else:
            new_mem = tf.concat([prev_mem, curr_out], 0)[-mem_len:]
    new_mem.stop_gradient = True
    return new_mem
",if prev_mem is None :,165
"def filtered(gen):
    for example in gen:
        example_len = length_fn(example)
        # Checking max length boundary.
        if max_length is not None:
            if example_len > max_length:
                continue
        # Checking min length boundary.
        if min_length is not None:
            if example_len < min_length:
                continue
        # Within bounds.
        yield example
",if example_len > max_length :,117
"def search(self, query):
    # ""Search.ashx?query="" + query + filterVal
    if not query:
        logger.debug(""Empty search query"")
        return []
    logger.debug('Searching TuneIn for ""%s""' % query)
    args = ""&query="" + query
    search_results = self._tunein(""Search.ashx"", args)
    results = []
    for item in self._flatten(search_results):
        if item.get(""type"", """") == ""audio"":
            # Only return stations
            self._stations[item[""guide_id""]] = item
            results.append(item)
    return results
","if item . get ( ""type"" , """" ) == ""audio"" :",163
"def _check_script(self, script, directive):
    for var in compile_script(script):
        if var.must_contain(""/""):
            # Skip variable checks
            return False
        if var.can_contain("".""):
            # Yay! Our variable can contain any symbols!
            reason = (
                'At least variable ""${var}"" can contain untrusted user input'.format(
                    var=var.name
                )
            )
            self.add_issue(directive=[directive] + var.providers, reason=reason)
            return True
    return False
","if var . must_contain ( ""/"" ) :",157
"def getAllDataLinkIDs():
    linkDataIDs = set()
    dataType = _forestData.dataTypeBySocket
    for socketID, linkedIDs in _forestData.linkedSockets.items():
        for linkedID in linkedIDs:
            if socketID[1]:  # check which one is origin/target
                linkDataIDs.add(
                    (socketID, linkedID, dataType[socketID], dataType[linkedID])
                )
            else:
                linkDataIDs.add(
                    (linkedID, socketID, dataType[linkedID], dataType[socketID])
                )
    return linkDataIDs
",if socketID [ 1 ] :,174
"def _stderr_supports_color():
    try:
        if hasattr(sys.stderr, ""isatty"") and sys.stderr.isatty():
            if curses:
                curses.setupterm()
                if curses.tigetnum(""colors"") > 0:
                    return True
            elif colorama:
                if sys.stderr is getattr(
                    colorama.initialise, ""wrapped_stderr"", object()
                ):
                    return True
    except Exception:
        # Very broad exception handling because it's always better to
        # fall back to non-colored logs than to break at startup.
        pass
    return False
","if curses . tigetnum ( ""colors"" ) > 0 :",170
"def offsets(self):
    offsets = {}
    offset_so_far = 0
    for name, ty in self.fields.items():
        if isinstance(ty, SimTypeBottom):
            l.warning(
                ""Found a bottom field in struct %s. Ignore and increment the offset using the default ""
                ""element size."",
                self.name,
            )
            continue
        if not self._pack:
            align = ty.alignment
            if offset_so_far % align != 0:
                offset_so_far += align - offset_so_far % align
        offsets[name] = offset_so_far
        offset_so_far += ty.size // self._arch.byte_width
    return offsets
",if offset_so_far % align != 0 :,196
"def Restore(self):
    picker, obj = self._window, self._pObject
    value = obj.RestoreValue(PERSIST_FILEDIRPICKER_PATH)
    if value is not None:
        if issubclass(picker.__class__, wx.FileDialog):
            if type(value) == list:
                value = value[-1]
        picker.SetPath(value)
        return True
    return False
",if type ( value ) == list :,102
"def dt_s_tup_to_string(dt_s_tup):
    dt_string = dt_s_tup[0]  # string for identifying the file to parse.
    if dt_s_tup[1] > 0:  # if there are seasons in the model
        if ""co"" in dt_string or ""ci"" in dt_string or ""nc"" in dt_string:
            dt_string = dt_string[:2] + ""s"" + dt_string[2:]
        else:
            dt_string = ""s"" + dt_string
    return dt_string
","if ""co"" in dt_string or ""ci"" in dt_string or ""nc"" in dt_string :",146
"def writer(stream, items):
    sep = """"
    for item in items:
        stream.write(sep)
        sep = "" ""
        if not isinstance(item, str):
            item = str(item)
        if not PY3K:
            if not isinstance(item, unicode):
                item = str(item)
        stream.write(item)
    stream.write(""\n"")
","if not isinstance ( item , str ) :",106
"def _get_result_keys(self, config):
    result_key = config.get(""result_key"")
    if result_key is not None:
        if not isinstance(result_key, list):
            result_key = [result_key]
        result_key = [jmespath.compile(rk) for rk in result_key]
        return result_key
","if not isinstance ( result_key , list ) :",92
"def _download_build_artifacts(self, build: Dict[str, Any]) -> None:
    arch = build[""arch_tag""]
    snap_build = self._lp_load_url(build[""self_link""])
    urls = snap_build.getFileUrls()
    if not urls:
        logger.error(f""Snap file not available for arch {arch!r}."")
        return
    for url in urls:
        file_name = _get_url_basename(url)
        self._download_file(url=url, dst=file_name)
        if file_name.endswith("".snap""):
            logger.info(f""Snapped {file_name}"")
        else:
            logger.info(f""Fetched {file_name}"")
","if file_name . endswith ( "".snap"" ) :",187
"def _add_custom_statement(self, custom_statements):
    if custom_statements is None:
        return
    self.resource_policy[""Version""] = ""2012-10-17""
    if self.resource_policy.get(""Statement"") is None:
        self.resource_policy[""Statement""] = custom_statements
    else:
        if not isinstance(custom_statements, list):
            custom_statements = [custom_statements]
        statement = self.resource_policy[""Statement""]
        if not isinstance(statement, list):
            statement = [statement]
        for s in custom_statements:
            if s not in statement:
                statement.append(s)
        self.resource_policy[""Statement""] = statement
",if s not in statement :,184
"def display_failures_for_single_test(result: TestResult) -> None:
    """"""Display a failure for a single method / endpoint.""""""
    display_subsection(result)
    checks = _get_unique_failures(result.checks)
    for idx, check in enumerate(checks, 1):
        message: Optional[str]
        if check.message:
            message = f""{idx}. {check.message}""
        else:
            message = None
        example = cast(Case, check.example)  # filtered in `_get_unique_failures`
        display_example(example, check.name, message, result.seed)
        # Display every time except the last check
        if idx != len(checks):
            click.echo(""\n"")
",if check . message :,188
"def build(opt):
    dpath = os.path.join(opt[""datapath""], ""qangaroo"")
    version = ""v1.1""
    if not build_data.built(dpath, version_string=version):
        print(""[building data: "" + dpath + ""]"")
        if build_data.built(dpath):
            # An older version exists, so remove these outdated files.
            build_data.remove_dir(dpath)
        build_data.make_dir(dpath)
        # Download the data.
        for downloadable_file in RESOURCES:
            downloadable_file.download_file(dpath)
        # Mark the data as built.
        build_data.mark_done(dpath, version_string=version)
",if build_data . built ( dpath ) :,188
"def call(self, step_input, states):
    new_states = []
    for i in range(self.num_layers):
        out, new_state = self.lstm_cells[i](step_input, states[i])
        step_input = (
            layers.dropout(
                out, self.dropout_prob, dropout_implementation=""upscale_in_train""
            )
            if self.dropout_prob > 0.0
            else out
        )
        new_states.append(new_state)
    return step_input, new_states
",if self . dropout_prob > 0.0,148
"def jupyter_progress_bar(min=0, max=1.0):
    """"""Returns an ipywidget progress bar or None if we can't import it""""""
    widgets = wandb.util.get_module(""ipywidgets"")
    try:
        if widgets is None:
            # TODO: this currently works in iPython but it's deprecated since 4.0
            from IPython.html import widgets  # type: ignore
        assert hasattr(widgets, ""VBox"")
        assert hasattr(widgets, ""Label"")
        assert hasattr(widgets, ""FloatProgress"")
        return ProgressWidget(widgets, min=min, max=max)
    except (ImportError, AssertionError):
        return None
",if widgets is None :,168
"def _record_event(self, path, fsevent_handle, filename, events, error):
    with self.lock:
        self.events[path].append(events)
        if events | pyuv.fs.UV_RENAME:
            if not os.path.exists(path):
                self.watches.pop(path).close()
",if events | pyuv . fs . UV_RENAME :,89
"def _get_v1_id_from_tags(self, tags_obj, tag):
    """"""Get image id from array of tags""""""
    if isinstance(tags_obj, dict):
        try:
            return tags_obj[tag]
        except KeyError:
            pass
    elif isinstance(tags_obj, []):
        try:
            for tag_dict in tags_obj:
                if tag_dict[""name""] == tag:
                    return tag_dict[""layer""]
        except KeyError:
            pass
    return """"
","if tag_dict [ ""name"" ] == tag :",142
"def query_lister(domain, query="""", max_items=None, attr_names=None):
    more_results = True
    num_results = 0
    next_token = None
    while more_results:
        rs = domain.connection.query_with_attributes(
            domain, query, attr_names, next_token=next_token
        )
        for item in rs:
            if max_items:
                if num_results == max_items:
                    raise StopIteration
            yield item
            num_results += 1
        next_token = rs.next_token
        more_results = next_token != None
",if max_items :,166
"def filter(this, args):
    array = to_object(this, args.space)
    callbackfn = get_arg(args, 0)
    arr_len = js_arr_length(array)
    if not is_callable(callbackfn):
        raise MakeError(""TypeError"", ""callbackfn must be a function"")
    _this = get_arg(args, 1)
    k = 0
    res = []
    while k < arr_len:
        if array.has_property(unicode(k)):
            kValue = array.get(unicode(k))
            if to_boolean(callbackfn.call(_this, (kValue, float(k), array))):
                res.append(kValue)
        k += 1
    return args.space.ConstructArray(res)
",if array . has_property ( unicode ( k ) ) :,194
"def every_one_is(self, dst):
    msg = ""all members of %r should be %r, but the %dth is %r""
    for index, item in enumerate(self._src):
        if self._range:
            if index < self._range[0] or index > self._range[1]:
                continue
        error = msg % (self._src, dst, index, item)
        if item != dst:
            raise AssertionError(error)
    return True
",if item != dst :,124
"def schedule_logger(job_id=None, delete=False):
    if not job_id:
        return getLogger(""fate_flow_schedule"")
    else:
        if delete:
            with LoggerFactory.lock:
                try:
                    for key in LoggerFactory.schedule_logger_dict.keys():
                        if job_id in key:
                            del LoggerFactory.schedule_logger_dict[key]
                except:
                    pass
            return True
        key = job_id + ""schedule""
        if key in LoggerFactory.schedule_logger_dict:
            return LoggerFactory.schedule_logger_dict[key]
        return LoggerFactory.get_schedule_logger(job_id)
",if job_id in key :,198
"def Tokenize(s):
    # type: (str) -> Iterator[Token]
    for item in TOKEN_RE.findall(s):
        # The type checker can't know the true type of item!
        item = cast(TupleStr4, item)
        if item[0]:
            typ = ""number""
            val = item[0]
        elif item[1]:
            typ = ""name""
            val = item[1]
        elif item[2]:
            typ = item[2]
            val = item[2]
        elif item[3]:
            typ = item[3]
            val = item[3]
        yield Token(typ, val)
",elif item [ 2 ] :,181
"def _read_data_from_all_categories(self, directory, config, categories):
    lines = []
    for category in categories:
        data_file = os.path.join(directory, _DATASET_VERSION, category, config)
        if os.path.exists(data_file):
            with open(data_file) as f:
                ls = f.read().split(""\n"")
                for l in ls[::-1]:
                    if not l:
                        ls.remove(l)
                lines.extend(ls)
    return lines
",if os . path . exists ( data_file ) :,150
"def find_handlers(self, forms):
    handlers = {}
    for form in forms.itervalues():
        for action_name, _action_label in form.actions:
            if action_name not in handlers:
                handlers[action_name] = form
            else:
                raise HandlerError(
                    ""More than one form defines the handler %s"" % action_name
                )
    return handlers
",if action_name not in handlers :,112
"def get_story_task_completed_body(payload: Dict[str, Any]) -> Optional[str]:
    action = get_action_with_primary_id(payload)
    kwargs = {
        ""task_description"": action[""description""],
    }
    story_id = action[""story_id""]
    for ref in payload[""references""]:
        if ref[""id""] == story_id:
            kwargs[""name_template""] = STORY_NAME_TEMPLATE.format(
                name=ref[""name""],
                app_url=ref[""app_url""],
            )
    if action[""changes""][""complete""][""new""]:
        return STORY_TASK_COMPLETED_TEMPLATE.format(**kwargs)
    else:
        return None
","if ref [ ""id"" ] == story_id :",188
"def _create_valid_graph(graph):
    nodes = graph.nodes()
    for i in range(len(nodes)):
        for j in range(len(nodes)):
            if i == j:
                continue
            edge = (nodes[i], nodes[j])
            if graph.has_edge(edge):
                graph.del_edge(edge)
            graph.add_edge(edge, 1)
",if i == j :,112
"def _post_order(op):
    if isinstance(op, tvm.tir.Allocate):
        lift_stmt[-1].append(op)
        return op.body
    if isinstance(op, tvm.tir.AttrStmt):
        if op.attr_key == ""storage_scope"":
            lift_stmt[-1].append(op)
            return op.body
        if op.attr_key == ""virtual_thread"":
            return _merge_block(lift_stmt.pop() + [op], op.body)
        return op
    if isinstance(op, tvm.tir.For):
        return _merge_block(lift_stmt.pop() + [op], op.body)
    raise RuntimeError(""not reached"")
","if op . attr_key == ""storage_scope"" :",188
"def format_lazy_import(names):
    """"""Formats lazy import lines""""""
    lines = """"
    for _, name, asname in names:
        pkg, _, _ = name.partition(""."")
        if asname is None:
            line = ""{pkg} = _LazyModule.load({pkg!r}, {mod!r})\n""
        else:
            line = ""{asname} = _LazyModule.load({pkg!r}, {mod!r}, {asname!r})\n""
        lines += line.format(pkg=pkg, mod=name, asname=asname)
    return lines
",if asname is None :,140
"def evaluateWord(self, argument):
    wildcard_count = argument[0].count(""*"")
    if wildcard_count > 0:
        if wildcard_count == 1 and argument[0].startswith(""*""):
            return self.GetWordWildcard(argument[0][1:], method=""endswith"")
        if wildcard_count == 1 and argument[0].endswith(""*""):
            return self.GetWordWildcard(argument[0][:-1], method=""startswith"")
        else:
            _regex = argument[0].replace(""*"", "".+"")
            matched = False
            for w in self.words:
                matched = bool(re.search(_regex, w))
                if matched:
                    break
            return matched
    return self.GetWord(argument[0])
",if matched :,194
"def setup(self, ir: ""IR"", aconf: Config) -> bool:
    if self.kind == ""ConsulResolver"":
        self.resolve_with = ""consul""
        if not self.get(""datacenter""):
            self.post_error(""ConsulResolver is required to have a datacenter"")
            return False
    elif self.kind == ""KubernetesServiceResolver"":
        self.resolve_with = ""k8s""
    elif self.kind == ""KubernetesEndpointResolver"":
        self.resolve_with = ""k8s""
    else:
        self.post_error(f""Resolver kind {self.kind} unknown"")
        return False
    return True
","if not self . get ( ""datacenter"" ) :",170
"def get_success_url(self):
    """"""Continue to the flow index or redirect according `?back` parameter.""""""
    if ""back"" in self.request.GET:
        back_url = self.request.GET[""back""]
        if not is_safe_url(url=back_url, allowed_hosts={self.request.get_host()}):
            back_url = ""/""
        return back_url
    return reverse(self.success_url)
","if not is_safe_url ( url = back_url , allowed_hosts = { self . request . get_host ( ) } ) :",111
"def download_main(
    download, download_playlist, urls, playlist, output_dir, merge, info_only
):
    for url in urls:
        if url.startswith(""https://""):
            url = url[8:]
        if not url.startswith(""http://""):
            url = ""http://"" + url
        if playlist:
            download_playlist(
                url, output_dir=output_dir, merge=merge, info_only=info_only
            )
        else:
            download(url, output_dir=output_dir, merge=merge, info_only=info_only)
","if not url . startswith ( ""http://"" ) :",155
"def __str__(self):
    buf = [""""]
    if self.fileName:
        buf.append(self.fileName + "":"")
    if self.line != -1:
        if not self.fileName:
            buf.append(""line "")
        buf.append(str(self.line))
        if self.column != -1:
            buf.append("":"" + str(self.column))
        buf.append("":"")
    buf.append("" "")
    return str("""").join(buf)
",if not self . fileName :,124
"def parse_bash_set_output(output):
    """"""Parse Bash-like 'set' output""""""
    if not sys.platform.startswith(""win""):
        # Replace ""\""-continued lines in *Linux* environment dumps.
        # Cannot do this on Windows because a ""\"" at the end of the
        # line does not imply a continuation.
        output = output.replace(""\\\n"", """")
    environ = {}
    for line in output.splitlines(0):
        line = line.rstrip()
        if not line:
            continue  # skip black lines
        item = _ParseBashEnvStr(line)
        if item:
            environ[item[0]] = item[1]
    return environ
",if item :,177
"def remove_selected(self):
    """"""Removes selected items from list.""""""
    to_delete = []
    for i in range(len(self)):
        if self[i].selected:
            to_delete.append(i)
    to_delete.reverse()
    for i in to_delete:
        self.pop(i)
    if len(to_delete) > 0:
        first_to_delete = to_delete[-1]
        if first_to_delete == 0 and len(self) > 0:
            self[0].selected = True
        elif first_to_delete > 0:
            self[first_to_delete - 1].selected = True
",if first_to_delete == 0 and len ( self ) > 0 :,169
"def update(self, update_tracks=True):
    self.enable_update_metadata_images(False)
    old_album_title = self.metadata[""album""]
    self.metadata[""album""] = config.setting[""nat_name""]
    for track in self.tracks:
        if old_album_title == track.metadata[""album""]:
            track.metadata[""album""] = self.metadata[""album""]
        for file in track.linked_files:
            track.update_file_metadata(file)
    self.enable_update_metadata_images(True)
    super().update(update_tracks)
","if old_album_title == track . metadata [ ""album"" ] :",149
"def on_input(self, target, message):
    if message.strip() == """":
        self.panel(""No commit message provided"")
        return
    if target:
        command = [""git"", ""add""]
        if target == ""*"":
            command.append(""--all"")
        else:
            command.extend((""--"", target))
        self.run_command(command, functools.partial(self.add_done, message))
    else:
        self.add_done(message, """")
","if target == ""*"" :",125
"def go_to_last_edit_location(self):
    if self.last_edit_cursor_pos is not None:
        filename, position = self.last_edit_cursor_pos
        if not osp.isfile(filename):
            self.last_edit_cursor_pos = None
            return
        else:
            self.load(filename)
            editor = self.get_current_editor()
            if position < editor.document().characterCount():
                editor.set_cursor_position(position)
",if not osp . isfile ( filename ) :,135
"def returnByType(self, results):
    new_results = {}
    for r in results:
        type_name = r.get(""type"", ""movie"") + ""s""
        if type_name not in new_results:
            new_results[type_name] = []
        new_results[type_name].append(r)
    # Combine movies, needs a cleaner way..
    if ""movies"" in new_results:
        new_results[""movies""] = self.combineOnIMDB(new_results[""movies""])
    return new_results
",if type_name not in new_results :,144
"def cache_sns_topics_across_accounts() -> bool:
    function: str = f""{__name__}.{sys._getframe().f_code.co_name}""
    # First, get list of accounts
    accounts_d: list = async_to_sync(get_account_id_to_name_mapping)()
    for account_id in accounts_d.keys():
        if config.get(""environment"") == ""prod"":
            cache_sns_topics_for_account.delay(account_id)
        else:
            if account_id in config.get(""celery.test_account_ids"", []):
                cache_sns_topics_for_account.delay(account_id)
    stats.count(f""{function}.success"")
    return True
","if account_id in config . get ( ""celery.test_account_ids"" , [ ] ) :",185
"def get(self, subject, topic):
    """"""Handles GET requests.""""""
    if subject in feconf.AVAILABLE_LANDING_PAGES:
        if topic in feconf.AVAILABLE_LANDING_PAGES[subject]:
            self.render_template(""topic-landing-page.mainpage.html"")
        else:
            raise self.PageNotFoundException
    else:
        raise self.PageNotFoundException
",if topic in feconf . AVAILABLE_LANDING_PAGES [ subject ] :,100
"def callback(compiled):
    if destpath is None:
        logger.show_tabulated(
            ""Compiled"", showpath(codepath), ""without writing to file.""
        )
    else:
        with univ_open(destpath, ""w"") as opened:
            writefile(opened, compiled)
        logger.show_tabulated(""Compiled to"", showpath(destpath), ""."")
    if self.show:
        print(compiled)
    if run:
        if destpath is None:
            self.execute(compiled, path=codepath, allow_show=False)
        else:
            self.execute_file(destpath)
",if destpath is None :,166
"def _find_start_index(self, string, start, end):
    while True:
        index = string.find(""{"", start, end) - 1
        if index < 0:
            return -1
        if self._start_index_is_ok(string, index):
            return index
        start = index + 2
","if self . _start_index_is_ok ( string , index ) :",84
"def _get_nlu_target_format(export_path: Text) -> Text:
    guessed_format = loading.guess_format(export_path)
    if guessed_format not in {MARKDOWN, RASA, RASA_YAML}:
        if rasa.shared.data.is_likely_json_file(export_path):
            guessed_format = RASA
        elif rasa.shared.data.is_likely_markdown_file(export_path):
            guessed_format = MARKDOWN
        elif rasa.shared.data.is_likely_yaml_file(export_path):
            guessed_format = RASA_YAML
    return guessed_format
",elif rasa . shared . data . is_likely_yaml_file ( export_path ) :,166
"def moveToThreadNext(self):
    """"""Move a position to threadNext position.""""""
    p = self
    if p.v:
        if p.v.children:
            p.moveToFirstChild()
        elif p.hasNext():
            p.moveToNext()
        else:
            p.moveToParent()
            while p:
                if p.hasNext():
                    p.moveToNext()
                    break  # found
                p.moveToParent()
            # not found.
    return p
",if p . hasNext ( ) :,150
"def copy_attributes(info_add, obj, name_fmt, attributes, formatter=None):
    for attr in attributes:
        value = getattr(obj, attr, None)
        if value is None:
            continue
        name = name_fmt % attr
        if formatter is not None:
            value = formatter(attr, value)
        info_add(name, value)
",if value is None :,97
"def getElement(self, aboutUri, namespace, name):
    for desc in self.rdfRoot.getElementsByTagNameNS(RDF_NAMESPACE, ""Description""):
        if desc.getAttributeNS(RDF_NAMESPACE, ""about"") == aboutUri:
            attr = desc.getAttributeNodeNS(namespace, name)
            if attr != None:
                yield attr
            for element in desc.getElementsByTagNameNS(namespace, name):
                yield element
","if desc . getAttributeNS ( RDF_NAMESPACE , ""about"" ) == aboutUri :",113
"def run(self):
    while not self.completed:
        if self.block:
            time.sleep(self.period)
        else:
            self._completed.wait(self.period)
        self.counter += 1
        try:
            self.callback(self.counter)
        except Exception:
            self.stop()
        if self.timeout is not None:
            dt = time.time() - self._start_time
            if dt > self.timeout:
                self.stop()
        if self.counter == self.count:
            self.stop()
",if self . timeout is not None :,159
"def _parse_fixits(message, titer, line):
    """"""Parses fixit messages.""""""
    while (
        OutputParser.message_line_re.match(line) is None
        and OutputParser.note_line_re.match(line) is None
    ):
        message_text = line.strip()
        if message_text != """":
            message.fixits.append(
                Note(
                    message.path,
                    message.line,
                    line.find(message_text) + 1,
                    message_text,
                )
            )
        line = next(titer)
    return line
","if message_text != """" :",177
"def _connect_db(self, force_reconnect=False):
    thread_id = thread.get_ident()
    if force_reconnect and thread_id in ENGINES:
        del ENGINES[thread_id]
    conn = None
    try:
        engine = ENGINES[thread_id]
        conn = engine.connect()
        _test = conn.execute(""SELECT 1"")
        _test.fetchall()
    except (KeyError, MySQLdb.OperationalError):
        if conn:
            conn.close()
        engine = sqla.create_engine(self.db_url, pool_recycle=3600)
        ENGINES[thread_id] = engine
        conn = engine.connect()
    return conn
",if conn :,183
"def read(self, n):
    if self.current_frame:
        data = self.current_frame.read(n)
        if not data and n != 0:
            self.current_frame = None
            return self.file_read(n)
        if len(data) < n:
            raise UnpicklingError(""pickle exhausted before end of frame"")
        return data
    else:
        return self.file_read(n)
",if not data and n != 0 :,115
"def __setLoadCmd(self):
    base = self.__rawLoadCmd
    for _ in range(self.__machHeader.ncmds):
        command = LOAD_COMMAND.from_buffer_copy(base)
        if command.cmd == MACHOFlags.LC_SEGMENT:
            segment = SEGMENT_COMMAND.from_buffer_copy(base)
            self.__setSections(segment, base[56:], 32)
        elif command.cmd == MACHOFlags.LC_SEGMENT_64:
            segment = SEGMENT_COMMAND64.from_buffer_copy(base)
            self.__setSections(segment, base[72:], 64)
        base = base[command.cmdsize :]
",if command . cmd == MACHOFlags . LC_SEGMENT :,174
"def emit_post_sync_signal(created_models, verbosity, interactive, db):
    # Emit the post_sync signal for every application.
    for app in models.get_apps():
        app_name = app.__name__.split(""."")[-2]
        if verbosity >= 2:
            print(""Running post-sync handlers for application %s"" % app_name)
        models.signals.post_syncdb.send(
            sender=app,
            app=app,
            created_models=created_models,
            verbosity=verbosity,
            interactive=interactive,
            db=db,
        )
",if verbosity >= 2 :,158
"def git_pull(args):
    if len(args) <= 1:
        repo = _get_repo()
        _confirm_dangerous()
        url = args[0] if len(args) == 1 else repo.remotes.get(""origin"", """")
        if url in repo.remotes:
            origin = url
            url = repo.remotes.get(origin)
        if url:
            repo.pull(origin_uri=url)
        else:
            print(""No pull URL."")
    else:
        print(command_help[""git pull""])
",if url :,147
"def version(self):
    try:
        return self._version
    except AttributeError:
        for line in self._get_metadata(self.PKG_INFO):
            if line.lower().startswith(""version:""):
                self._version = safe_version(line.split("":"", 1)[1].strip())
                return self._version
        else:
            tmpl = ""Missing 'Version:' header and/or %s file""
            raise ValueError(tmpl % self.PKG_INFO, self)
","if line . lower ( ) . startswith ( ""version:"" ) :",127
"def increment(self, metric, labels, delta):
    """"""Increment a value by |delta|.""""""
    with self._lock:
        key = self._get_key(metric.name, labels)
        if key in self._store:
            start_time = self._store[key].start_time
            value = self._store[key].value + delta
        else:
            start_time = time.time()
            value = metric.default_value + delta
        self._store[key] = _StoreValue(metric, labels, start_time, value)
",if key in self . _store :,143
"def get_current_connections(session):
    """"""Retrieves open connections using the the given session""""""
    # Use Show process list to count the open sesions.
    res = session.sql(""SHOW PROCESSLIST"").execute()
    rows = res.fetch_all()
    connections = {}
    for row in rows:
        if row.get_string(""User"") not in connections:
            connections[row.get_string(""User"")] = [row.get_string(""Host"")]
        else:
            connections[row.get_string(""User"")].append(row.get_string(""Host""))
    return connections
","if row . get_string ( ""User"" ) not in connections :",148
"def asset(*paths):
    for path in paths:
        fspath = www_root + ""/assets/"" + path
        etag = """"
        try:
            if env.cache_static:
                etag = asset_etag(fspath)
            else:
                os.stat(fspath)
        except FileNotFoundError as e:
            if path == paths[-1]:
                if not os.path.exists(fspath + "".spt""):
                    tell_sentry(e, {})
            else:
                continue
        except Exception as e:
            tell_sentry(e, {})
        return asset_url + path + (etag and ""?etag="" + etag)
",if env . cache_static :,182
"def thread_loop(self) -> None:
    while not self.stop_event.is_set():
        time.sleep(1)
        new_trials = self.study.trials
        with self.lock:
            need_to_add_callback = self.new_trials is None
            self.new_trials = new_trials
            if need_to_add_callback:
                self.doc.add_next_tick_callback(self.update_callback)
",if need_to_add_callback :,122
"def _cache_db_tables_iterator(tables, cache_alias, db_alias):
    no_tables = not tables
    cache_aliases = settings.CACHES if cache_alias is None else (cache_alias,)
    db_aliases = settings.DATABASES if db_alias is None else (db_alias,)
    for db_alias in db_aliases:
        if no_tables:
            tables = connections[db_alias].introspection.table_names()
        if tables:
            for cache_alias in cache_aliases:
                yield cache_alias, db_alias, tables
",if tables :,145
"def remove_subscriber(self, topic, subscriber):
    if subscriber in self.subscribers[topic]:
        if hasattr(subscriber, ""_pyroRelease""):
            subscriber._pyroRelease()
        if hasattr(subscriber, ""_pyroUri""):
            try:
                proxy = self.proxy_cache[subscriber._pyroUri]
                proxy._pyroRelease()
                del self.proxy_cache[subscriber._pyroUri]
            except KeyError:
                pass
        self.subscribers[topic].discard(subscriber)
","if hasattr ( subscriber , ""_pyroUri"" ) :",139
"def test_constructor(job_id):
    with patch(""apscheduler.job.Job._modify"") as _modify:
        scheduler_mock = MagicMock(BaseScheduler)
        job = Job(scheduler_mock, id=job_id)
        assert job._scheduler is scheduler_mock
        assert job._jobstore_alias is None
        modify_kwargs = _modify.call_args[1]
        if job_id is None:
            assert len(modify_kwargs[""id""]) == 32
        else:
            assert modify_kwargs[""id""] == job_id
",if job_id is None :,141
"def get_connection(self):
    if self.config.proxy_host != """":
        return httplib.HTTPConnection(self.config.proxy_host, self.config.proxy_port)
    else:
        if self.config.use_https:
            return httplib.HTTPSConnection(self.config.simpledb_host)
        else:
            return httplib.HTTPConnection(self.config.simpledb_host)
",if self . config . use_https :,107
"def notify_login(self, ipaddress=""""):
    if app.NOTIFY_ON_LOGIN:
        update_text = common.notifyStrings[common.NOTIFY_LOGIN_TEXT]
        title = common.notifyStrings[common.NOTIFY_LOGIN]
        if update_text and title and ipaddress:
            self._notify_pht(title, update_text.format(ipaddress))
",if update_text and title and ipaddress :,93
"def _getItemHeight(self, item, ctrl=None):
    """"""Returns the full height of the item to be inserted in the form""""""
    if type(ctrl) == psychopy.visual.TextBox2:
        return ctrl.size[1]
    if type(ctrl) == psychopy.visual.Slider:
        # Set radio button layout
        if item[""layout""] == ""horiz"":
            return 0.03 + ctrl.labelHeight * 3
        elif item[""layout""] == ""vert"":
            # for vertical take into account the nOptions
            return ctrl.labelHeight * len(item[""options""])
","elif item [ ""layout"" ] == ""vert"" :",155
"def _get_errors_lines(self):
    """"""Return the number of lines that contains errors to highlight.""""""
    errors_lines = []
    block = self.document().begin()
    while block.isValid():
        user_data = get_user_data(block)
        if user_data.error:
            errors_lines.append(block.blockNumber())
        block = block.next()
    return errors_lines
",if user_data . error :,105
"def set_pbar_fraction(self, frac, progress, stage=None):
    gtk.gdk.threads_enter()
    try:
        self.is_pulsing = False
        self.set_stage_text(stage or _(""Processing...""))
        self.pbar.set_text(progress)
        if frac > 1:
            frac = 1.0
        if frac < 0:
            frac = 0
        self.pbar.set_fraction(frac)
    finally:
        gtk.gdk.threads_leave()
",if frac < 0 :,135
"def list_files(basedir):
    """"""List files in the directory rooted at |basedir|.""""""
    if not os.path.isdir(basedir):
        raise NoSuchDirectory(basedir)
    directories = [""""]
    while directories:
        d = directories.pop()
        for basename in os.listdir(os.path.join(basedir, d)):
            filename = os.path.join(d, basename)
            if os.path.isdir(os.path.join(basedir, filename)):
                directories.append(filename)
            elif os.path.exists(os.path.join(basedir, filename)):
                yield filename
","elif os . path . exists ( os . path . join ( basedir , filename ) ) :",159
"def assistive(self):
    """"""Detects if item can be used as assistance""""""
    # Make sure we cache results
    if self.__assistive is None:
        assistive = False
        # Go through all effects and find first assistive
        for effect in self.effects.values():
            if effect.isAssistance is True:
                # If we find one, stop and mark item as assistive
                assistive = True
                break
        self.__assistive = assistive
    return self.__assistive
",if effect . isAssistance is True :,141
"def closest_unseen(self, row1, col1, filter=None):
    # find the closest unseen from this row/col
    min_dist = maxint
    closest_unseen = None
    for row in range(self.height):
        for col in range(self.width):
            if filter is None or (row, col) not in filter:
                if self.map[row][col] == UNSEEN:
                    dist = self.distance(row1, col1, row, col)
                    if dist < min_dist:
                        min_dist = dist
                        closest_unseen = (row, col)
    return closest_unseen
",if dist < min_dist :,174
"def _maybe_has_default_route(self):
    for route in self.iter_routes():
        if self._is_default_route(route):
            return True
    for iface in self.iter_interfaces():
        for subnet in iface.get(""subnets"", []):
            for route in subnet.get(""routes"", []):
                if self._is_default_route(route):
                    return True
    return False
",if self . _is_default_route ( route ) :,113
"def data(self, data):
    if data is None:
        raise Exception(""Data cannot be None"")
    val = []
    for d in data:
        if isinstance(d, str):
            val.append(bytes(d, ""utf-8""))
        elif isinstance(d, bytes):
            val.append(d)
        else:
            raise Exception(
                ""Invalid type, data can only be an str or a bytes not {}: {}"".format(
                    type(data), d
                )
            )
    self.__data = val
","elif isinstance ( d , bytes ) :",149
"def get_one_segment_function(data, context, echoerr):
    ext = data[""ext""]
    function_name = context[-2][1].get(""function"")
    if function_name:
        module, function_name = get_function_strings(function_name, context, ext)
        func = import_segment(function_name, data, context, echoerr, module=module)
        if func:
            yield func
",if func :,107
"def generic_visit(self, node, parents=None):
    parents = (parents or []) + [node]
    for field, value in iter_fields(node):
        if isinstance(value, list):
            for item in value:
                if isinstance(item, AST):
                    self.visit(item, parents)
        elif isinstance(value, AST):
            self.visit(value, parents)
","if isinstance ( item , AST ) :",106
"def find_scintilla_constants(f):
    lexers = []
    states = []
    for name in f.order:
        v = f.features[name]
        if v[""Category""] != ""Deprecated"":
            if v[""FeatureType""] == ""val"":
                if name.startswith(""SCE_""):
                    states.append((name, v[""Value""]))
                elif name.startswith(""SCLEX_""):
                    lexers.append((name, v[""Value""]))
    return (lexers, states)
","if v [ ""Category"" ] != ""Deprecated"" :",137
"def things(self, query):
    limit = query.pop(""limit"", 100)
    offset = query.pop(""offset"", 0)
    keys = set(self.docs)
    for k, v in query.items():
        if isinstance(v, dict):
            # query keys need to be flattened properly,
            # this corrects any nested keys that have been included
            # in values.
            flat = common.flatten_dict(v)[0]
            k += ""."" + web.rstrips(flat[0], "".key"")
            v = flat[1]
        keys = set(k for k in self.filter_index(self.index, k, v) if k in keys)
    keys = sorted(keys)
    return keys[offset : offset + limit]
","if isinstance ( v , dict ) :",194
"def del_(self, key):
    initial_hash = hash_ = self.hash(key)
    while True:
        if self._keys[hash_] is self._empty:
            # That key was never assigned
            return None
        elif self._keys[hash_] == key:
            # key found, assign with deleted sentinel
            self._keys[hash_] = self._deleted
            self._values[hash_] = self._deleted
            self._len -= 1
            return
        hash_ = self._rehash(hash_)
        if initial_hash == hash_:
            # table is full and wrapped around
            return None
",elif self . _keys [ hash_ ] == key :,166
"def test_204_invalid_content_length(self):
    # 204 status with non-zero content length is malformed
    with ExpectLog(gen_log, "".*Response with code 204 should not have body""):
        response = self.fetch(""/?error=1"")
        if not self.http1:
            self.skipTest(""requires HTTP/1.x"")
        if self.http_client.configured_class != SimpleAsyncHTTPClient:
            self.skipTest(""curl client accepts invalid headers"")
        self.assertEqual(response.code, 599)
",if not self . http1 :,136
"def __str__(self) -> str:
    text = ""\n""
    for k, r in self.result.items():
        text += ""{}\n"".format(""#"" * 40)
        if r.failed:
            text += ""# {} (failed)\n"".format(k)
        else:
            text += ""# {} (succeeded)\n"".format(k)
        text += ""{}\n"".format(""#"" * 40)
        for sub_r in r:
            text += ""**** {}\n"".format(sub_r.name)
            text += ""{}\n"".format(sub_r)
    return text
",if r . failed :,153
"def DeleteTask():
    oid = request.form.get(""oid"", """")
    if oid:
        result = Mongo.coll[""Task""].delete_one({""_id"": ObjectId(oid)})
        if result.deleted_count > 0:
            result = Mongo.coll[""Result""].delete_many({""task_id"": ObjectId(oid)})
            if result:
                return ""success""
    return ""fail""
",if result . deleted_count > 0 :,108
"def _replace_vars(self, line, extracted, env_variables):
    for e in extracted:
        if e in env_variables:
            value = env_variables.get(e)
            if isinstance(value, dict) or isinstance(value, list):
                value = pprint.pformat(value)
            decorated = self._decorate_var(e)
            line = line.replace(decorated, str(value))
    return line
",if e in env_variables :,113
"def should_include(service):
    for f in filt:
        if f == ""status"":
            state = filt[f]
            containers = project.containers([service.name], stopped=True)
            if not has_container_with_state(containers, state):
                return False
        elif f == ""source"":
            source = filt[f]
            if source == ""image"" or source == ""build"":
                if source not in service.options:
                    return False
            else:
                raise UserError(""Invalid value for source filter: %s"" % source)
        else:
            raise UserError(""Invalid filter: %s"" % f)
    return True
",if source not in service . options :,184
"def state_callback_loop():
    if usercallback:
        when = 1
        while (
            when
            and not self.future_removed.done()
            and not self.session.shutdownstarttime
        ):
            result = usercallback(self.get_state())
            when = (await result) if iscoroutine(result) else result
            if when > 0.0 and not self.session.shutdownstarttime:
                await sleep(when)
",if when > 0.0 and not self . session . shutdownstarttime :,122
"def __get_new_timeout(self, timeout):
    """"""When using --timeout_multiplier=#.#""""""
    self.__check_scope()
    try:
        timeout_multiplier = float(self.timeout_multiplier)
        if timeout_multiplier <= 0.5:
            timeout_multiplier = 0.5
        timeout = int(math.ceil(timeout_multiplier * timeout))
        return timeout
    except Exception:
        # Wrong data type for timeout_multiplier (expecting int or float)
        return timeout
",if timeout_multiplier <= 0.5 :,126
"def readexactly(self, n):
    buf = b""""
    while n:
        yield IORead(self.s)
        res = self.s.read(n)
        assert res is not None
        if not res:
            yield IOReadDone(self.s)
            break
        buf += res
        n -= len(res)
    return buf
",if not res :,99
"def contract_rendering_pane(event):
    """"""Expand the rendering pane.""""""
    c = event.get(""c"")
    if c:
        vr = c.frame.top.findChild(QtWidgets.QWidget, ""viewrendered_pane"")
        if vr:
            vr.contract()
        else:
            # Just open the pane.
            viewrendered(event)
",if vr :,103
"def translate_headers(self, environ):
    """"""Translate CGI-environ header names to HTTP header names.""""""
    for cgiName in environ:
        # We assume all incoming header keys are uppercase already.
        if cgiName in self.headerNames:
            yield self.headerNames[cgiName], environ[cgiName]
        elif cgiName[:5] == ""HTTP_"":
            # Hackish attempt at recovering original header names.
            translatedHeader = cgiName[5:].replace(""_"", ""-"")
            yield translatedHeader, environ[cgiName]
",if cgiName in self . headerNames :,134
"def get_value_from_string(self, string_value):
    """"""Return internal representation starting from CFN/user-input value.""""""
    param_value = self.get_default_value()
    try:
        if string_value is not None:
            string_value = str(string_value).strip()
            if string_value != ""NONE"":
                param_value = int(string_value)
    except ValueError:
        self.pcluster_config.warn(
            ""Unable to convert the value '{0}' to an Integer. ""
            ""Using default value for parameter '{1}'"".format(string_value, self.key)
        )
    return param_value
",if string_value is not None :,172
"def monitor_filter(self):
    """"""Return filtered service objects list""""""
    services = self.client.services.list(filters={""label"": ""com.ouroboros.enable""})
    monitored_services = []
    for service in services:
        ouro_label = service.attrs[""Spec""][""Labels""].get(""com.ouroboros.enable"")
        if not self.config.label_enable or ouro_label.lower() in [""true"", ""yes""]:
            monitored_services.append(service)
    self.data_manager.monitored_containers[self.socket] = len(monitored_services)
    self.data_manager.set(self.socket)
    return monitored_services
","if not self . config . label_enable or ouro_label . lower ( ) in [ ""true"" , ""yes"" ] :",176
"def nextEditable(self):
    """"""Moves focus of the cursor to the next editable window""""""
    if self.currentEditable is None:
        if len(self._editableChildren):
            self._currentEditableRef = self._editableChildren[0]
    else:
        for ref in weakref.getweakrefs(self.currentEditable):
            if ref in self._editableChildren:
                cei = self._editableChildren.index(ref)
                nei = cei + 1
                if nei >= len(self._editableChildren):
                    nei = 0
                self._currentEditableRef = self._editableChildren[nei]
    return self.currentEditable
",if nei >= len ( self . _editableChildren ) :,179
"def linkify_cm_by_tp(self, timeperiods):
    for rm in self:
        mtp_name = rm.modulation_period.strip()
        # The new member list, in id
        mtp = timeperiods.find_by_name(mtp_name)
        if mtp_name != """" and mtp is None:
            err = (
                ""Error: the business impact modulation '%s' got an unknown ""
                ""modulation_period '%s'"" % (rm.get_name(), mtp_name)
            )
            rm.configuration_errors.append(err)
        rm.modulation_period = mtp
","if mtp_name != """" and mtp is None :",169
"def close_open_fds(keep=None):  # noqa
    keep = [maybe_fileno(f) for f in (keep or []) if maybe_fileno(f) is not None]
    for fd in reversed(range(get_fdmax(default=2048))):
        if fd not in keep:
            try:
                os.close(fd)
            except OSError as exc:
                if exc.errno != errno.EBADF:
                    raise
",if fd not in keep :,120
"def _append_child_from_unparsed_xml(father_node, unparsed_xml):
    """"""Append child xml nodes to a node.""""""
    dom_tree = parseString(unparsed_xml)
    if dom_tree.hasChildNodes():
        first_child = dom_tree.childNodes[0]
        if first_child.hasChildNodes():
            child_nodes = first_child.childNodes
            for _ in range(len(child_nodes)):
                childNode = child_nodes.item(0)
                father_node.appendChild(childNode)
            return
    raise DistutilsInternalError(
        ""Could not Append append elements to "" ""the Windows msi descriptor.""
    )
",if first_child . hasChildNodes ( ) :,178
"def process_request(self, request):
    for old, new in self.names_name:
        request.uri = request.uri.replace(old, new)
        if is_text_payload(request) and request.body:
            body = six.ensure_str(request.body)
            if old in body:
                request.body = body.replace(old, new)
    return request
",if is_text_payload ( request ) and request . body :,103
"def __init__(self, **options):
    self.func_name_highlighting = get_bool_opt(options, ""func_name_highlighting"", True)
    self.disabled_modules = get_list_opt(options, ""disabled_modules"", [])
    self._functions = set()
    if self.func_name_highlighting:
        from pygments.lexers._luabuiltins import MODULES
        for mod, func in MODULES.iteritems():
            if mod not in self.disabled_modules:
                self._functions.update(func)
    RegexLexer.__init__(self, **options)
",if mod not in self . disabled_modules :,153
"def GetBestSizeForParentSize(self, parentSize):
    """"""Finds the best width and height given the parent's width and height.""""""
    if len(self.GetChildren()) == 1:
        win = self.GetChildren()[0]
        if isinstance(win, RibbonControl):
            temp_dc = wx.ClientDC(self)
            childSize = win.GetBestSizeForParentSize(parentSize)
            clientParentSize = self._art.GetPanelClientSize(
                temp_dc, self, wx.Size(*parentSize), None
            )
            overallSize = self._art.GetPanelSize(
                temp_dc, self, wx.Size(*clientParentSize), None
            )
            return overallSize
    return self.GetSize()
","if isinstance ( win , RibbonControl ) :",199
"def pid_from_name(name):
    processes = []
    for pid in os.listdir(""/proc""):
        try:
            pid = int(pid)
            pname, cmdline = SunProcess._name_args(pid)
            if name in pname:
                return pid
            if name in cmdline.split("" "", 1)[0]:
                return pid
        except:
            pass
    raise ProcessException(""No process with such name: %s"" % name)
",if name in pname :,126
"def __get_file_by_num(self, num, file_list, idx=0):
    for element in file_list:
        if idx == num:
            return element
        if element[3] and element[4]:
            i = self.__get_file_by_num(num, element[3], idx + 1)
            if not isinstance(i, int):
                return i
            idx = i
        else:
            idx += 1
    return idx
",if element [ 3 ] and element [ 4 ] :,127
"def scan_block_scalar_indentation(self):
    # See the specification for details.
    chunks = []
    max_indent = 0
    end_mark = self.get_mark()
    while self.peek() in "" \r\n\x85\u2028\u2029"":
        if self.peek() != "" "":
            chunks.append(self.scan_line_break())
            end_mark = self.get_mark()
        else:
            self.forward()
            if self.column > max_indent:
                max_indent = self.column
    return chunks, max_indent, end_mark
",if self . column > max_indent :,161
"def ant_map(m):
    tmp = ""rows %s\ncols %s\n"" % (len(m), len(m[0]))
    players = {}
    for row in m:
        tmp += ""m ""
        for col in row:
            if col == LAND:
                tmp += "".""
            elif col == BARRIER:
                tmp += ""%""
            elif col == FOOD:
                tmp += ""*""
            elif col == UNSEEN:
                tmp += ""?""
            else:
                players[col] = True
                tmp += chr(col + 97)
        tmp += ""\n""
    tmp = (""players %s\n"" % len(players)) + tmp
    return tmp
",elif col == FOOD :,199
"def prepare_data(entry):
    branch_wise_entries = {}
    gross_pay = 0
    for d in entry:
        gross_pay += d.gross_pay
        if branch_wise_entries.get(d.branch):
            branch_wise_entries[d.branch][d.mode_of_payment] = d.net_pay
        else:
            branch_wise_entries.setdefault(d.branch, {}).setdefault(
                d.mode_of_payment, d.net_pay
            )
    return branch_wise_entries, gross_pay
",if branch_wise_entries . get ( d . branch ) :,146
"def __init__(self, uuid=None, cluster_state=None, children=None, **kwargs):
    self.uuid = uuid
    self.cluster_state = cluster_state
    if self.cluster_state is not None:
        self.children = WeakSet(
            self.cluster_state.tasks.get(task_id)
            for task_id in children or ()
            if task_id in self.cluster_state.tasks
        )
    else:
        self.children = WeakSet()
    self._serializer_handlers = {
        ""children"": self._serializable_children,
        ""root"": self._serializable_root,
        ""parent"": self._serializable_parent,
    }
    if kwargs:
        self.__dict__.update(kwargs)
",if task_id in self . cluster_state . tasks,192
"def listdir(self, d):
    try:
        return [
            p
            for p in os.listdir(d)
            if os.path.basename(p) != ""CVS"" and os.path.isdir(os.path.join(d, p))
        ]
    except OSError:
        return []
","if os . path . basename ( p ) != ""CVS"" and os . path . isdir ( os . path . join ( d , p ) )",84
"def send_packed_command(self, command, check_health=True):
    if not self._sock:
        self.connect()
    try:
        if isinstance(command, str):
            command = [command]
        for item in command:
            self._sock.sendall(item)
    except socket.error as e:
        self.disconnect()
        if len(e.args) == 1:
            _errno, errmsg = ""UNKNOWN"", e.args[0]
        else:
            _errno, errmsg = e.args
        raise ConnectionError(
            ""Error %s while writing to socket. %s."" % (_errno, errmsg)
        )
    except Exception:
        self.disconnect()
        raise
","if isinstance ( command , str ) :",188
"def run(self):
    """"""Start the scanner""""""
    logging.info(""Dirscanner starting up"")
    self.shutdown = False
    while not self.shutdown:
        # Wait to be woken up or triggered
        with self.loop_condition:
            self.loop_condition.wait(self.dirscan_speed)
        if self.dirscan_speed and not self.shutdown:
            self.scan()
",if self . dirscan_speed and not self . shutdown :,104
"def __aexit__(
    self, exc_type: type, exc_value: BaseException, tb: TracebackType
) -> None:
    if exc_type is not None:
        await self.close()
    await self._task
    while not self._receive_queue.empty():
        data = await self._receive_queue.get()
        if isinstance(data, bytes):
            self.response_data.extend(data)
        elif not isinstance(data, HTTPDisconnect):
            raise data
","elif not isinstance ( data , HTTPDisconnect ) :",121
"def f(msg):
    text = extractor(msg)
    for px in prefix:
        if text.startswith(px):
            chunks = text[len(px) :].split(separator)
            return chunks[0], (chunks[1:],) if pass_args else ()
    return ((None,),)  # to distinguish with `None`
",if text . startswith ( px ) :,83
"def _flatten(*args):
    ahs = set()
    if len(args) > 0:
        for item in args:
            if type(item) is ActionHandle:
                ahs.add(item)
            elif type(item) in (list, tuple, dict, set):
                for ah in item:
                    if type(ah) is not ActionHandle:  # pragma:nocover
                        raise ActionManagerError(""Bad argument type %s"" % str(ah))
                    ahs.add(ah)
            else:  # pragma:nocover
                raise ActionManagerError(""Bad argument type %s"" % str(item))
    return ahs
","elif type ( item ) in ( list , tuple , dict , set ) :",183
"def find_class(self, module, name):
    # Subclasses may override this.
    sys.audit(""pickle.find_class"", module, name)
    if self.proto < 3 and self.fix_imports:
        if (module, name) in _compat_pickle.NAME_MAPPING:
            module, name = _compat_pickle.NAME_MAPPING[(module, name)]
        elif module in _compat_pickle.IMPORT_MAPPING:
            module = _compat_pickle.IMPORT_MAPPING[module]
    __import__(module, level=0)
    if self.proto >= 4:
        return _getattribute(sys.modules[module], name)[0]
    else:
        return getattr(sys.modules[module], name)
",elif module in _compat_pickle . IMPORT_MAPPING :,178
"def _send_until_done(self, data):
    while True:
        try:
            return self.connection.send(data)
        except OpenSSL.SSL.WantWriteError:
            wr = util.wait_for_write(self.socket, self.socket.gettimeout())
            if not wr:
                raise timeout()
            continue
        except OpenSSL.SSL.SysCallError as e:
            raise SocketError(str(e))
",if not wr :,120
"def __new__(cls, *args, **kwargs):
    """"""Hack to ensure method defined as async are implemented as such.""""""
    coroutines = inspect.getmembers(BaseManager, predicate=inspect.iscoroutinefunction)
    for coroutine in coroutines:
        implemented_method = getattr(cls, coroutine[0])
        if not inspect.iscoroutinefunction(implemented_method):
            raise RuntimeError(""The method %s must be a coroutine"" % implemented_method)
    return super().__new__(cls, *args, **kwargs)
",if not inspect . iscoroutinefunction ( implemented_method ) :,120
"def add_directive(self, name, obj, content=None, arguments=None, **options):
    if isinstance(obj, clstypes) and issubclass(obj, Directive):
        if content or arguments or options:
            raise ExtensionError(
                ""when adding directive classes, no "" ""additional arguments may be given""
            )
        directives.register_directive(name, directive_dwim(obj))
    else:
        obj.content = content
        obj.arguments = arguments
        obj.options = options
        directives.register_directive(name, obj)
",if content or arguments or options :,144
"def create(self, w):
    if w.use_eventloop:
        # does not use dedicated timer thread.
        w.timer = _Timer(max_interval=10.0)
    else:
        if not w.timer_cls:
            # Default Timer is set by the pool, as for example, the
            # eventlet pool needs a custom timer implementation.
            w.timer_cls = w.pool_cls.Timer
        w.timer = self.instantiate(
            w.timer_cls,
            max_interval=w.timer_precision,
            on_error=self.on_timer_error,
            on_tick=self.on_timer_tick,
        )
",if not w . timer_cls :,182
"def _config(_molecule_file, request):
    with open(_molecule_file) as f:
        d = util.safe_load(f)
    if hasattr(request, ""param""):
        if isinstance(request.getfixturevalue(request.param), str):
            d2 = util.safe_load(request.getfixturevalue(request.param))
        else:
            d2 = request.getfixturevalue(request.param)
        # print(100, d)
        # print(200, d2)
        d = util.merge_dicts(d, d2)
        # print(300, d)
    return d
","if isinstance ( request . getfixturevalue ( request . param ) , str ) :",164
"def _instrument_model(self, model):
    for key, value in list(
        model.__dict__.items()
    ):  # avoid ""dictionary keys changed during iteration""
        if isinstance(value, tf.keras.layers.Layer):
            new_layer = self._instrument(value)
            if new_layer is not value:
                setattr(model, key, new_layer)
        elif isinstance(value, list):
            for i, item in enumerate(value):
                if isinstance(item, tf.keras.layers.Layer):
                    value[i] = self._instrument(item)
    return model
","if isinstance ( value , tf . keras . layers . Layer ) :",164
"def is_accepted_drag_event(self, event):
    if event.source() == self.table:
        return True
    mime = event.mimeData()
    if mime.hasUrls():
        for url in mime.urls():
            # Only support local files.
            if not url.isLocalFile():
                break
            # And only allow supported extensions.
            filename = url.toLocalFile()
            extension = os.path.splitext(filename)[1].lower()[1:]
            if extension not in _dictionary_formats():
                break
        else:
            return True
    return False
",if not url . isLocalFile ( ) :,163
"def explain(self, other, depth=0):
    exp = super(UnionType, self).explain(other, depth)
    for ndx, subtype in enumerate(self.params[""allowed_types""]):
        if ndx > 0:
            exp += ""\n{}and"".format("""".join([""\t""] * depth))
        exp += ""\n"" + subtype.explain(other, depth=depth + 1)
    return exp
",if ndx > 0 :,101
"def test_k_is_stochastic_parameter(self):
    # k as stochastic parameter
    aug = iaa.MedianBlur(k=iap.Choice([3, 5]))
    seen = [False, False]
    for i in sm.xrange(100):
        observed = aug.augment_image(self.base_img)
        if np.array_equal(observed, self.blur3x3):
            seen[0] += True
        elif np.array_equal(observed, self.blur5x5):
            seen[1] += True
        else:
            raise Exception(""Unexpected result in MedianBlur@2"")
        if all(seen):
            break
    assert np.all(seen)
","elif np . array_equal ( observed , self . blur5x5 ) :",176
"def test_get_message(self):
    async with self.chat_client:
        await self._create_thread()
        async with self.chat_thread_client:
            message_id = await self._send_message()
            message = await self.chat_thread_client.get_message(message_id)
            assert message.id == message_id
            assert message.type == ChatMessageType.TEXT
            assert message.content.message == ""hello world""
        # delete chat threads
        if not self.is_playback():
            await self.chat_client.delete_chat_thread(self.thread_id)
",if not self . is_playback ( ) :,163
"def do_write_property(self, device, callback=None):
    try:
        iocb = (
            device
            if isinstance(device, IOCB)
            else self.form_iocb(device, request_type=""writeProperty"")
        )
        deferred(self.request_io, iocb)
        self.requests_in_progress.update({iocb: {""callback"": callback}})
        iocb.add_callback(self.__general_cb)
    except Exception as error:
        log.exception(""exception: %r"", error)
","if isinstance ( device , IOCB )",146
"def fit(self, dataset, force_retrain):
    if force_retrain:
        self.sub_unit_1[""fitted""] = True
        self.sub_unit_1[""calls""] += 1
        self.sub_unit_2[""fitted""] = True
        self.sub_unit_2[""calls""] += 1
    else:
        if not self.sub_unit_1[""fitted""]:
            self.sub_unit_1[""fitted""] = True
            self.sub_unit_1[""calls""] += 1
        if not self.sub_unit_2[""fitted""]:
            self.sub_unit_2[""fitted""] = True
            self.sub_unit_2[""calls""] += 1
    return self
","if not self . sub_unit_2 [ ""fitted"" ] :",183
"def _insert_with_loop(self):
    id_list = []
    last_id = None
    return_id_list = self._return_id_list
    for row in self._rows:
        last_id = InsertQuery(self.model_class, row).upsert(self._upsert).execute()
        if return_id_list:
            id_list.append(last_id)
    if return_id_list:
        return id_list
    else:
        return last_id
",if return_id_list :,126
"def merge_block(self):
    """"""merges a block in the map""""""
    for i in range(self.block.x):
        for j in range(self.block.x):
            c = self.block.get(i, j)
            if c:
                self.map[(i + self.block.pos.x, j + self.block.pos.y)] = c
",if c :,99
"def configure_plex(config):
    core.PLEX_SSL = int(config[""Plex""][""plex_ssl""])
    core.PLEX_HOST = config[""Plex""][""plex_host""]
    core.PLEX_PORT = config[""Plex""][""plex_port""]
    core.PLEX_TOKEN = config[""Plex""][""plex_token""]
    plex_section = config[""Plex""][""plex_sections""] or []
    if plex_section:
        if isinstance(plex_section, list):
            plex_section = "","".join(plex_section)  # fix in case this imported as list.
        plex_section = [tuple(item.split("","")) for item in plex_section.split(""|"")]
    core.PLEX_SECTION = plex_section
","if isinstance ( plex_section , list ) :",182
"def select(self):
    e = xlib.XEvent()
    while xlib.XPending(self._display):
        xlib.XNextEvent(self._display, e)
        # Key events are filtered by the xlib window event
        # handler so they get a shot at the prefiltered event.
        if e.xany.type not in (xlib.KeyPress, xlib.KeyRelease):
            if xlib.XFilterEvent(e, e.xany.window):
                continue
        try:
            dispatch = self._window_map[e.xany.window]
        except KeyError:
            continue
        dispatch(e)
","if e . xany . type not in ( xlib . KeyPress , xlib . KeyRelease ) :",171
"def format_message(self):
    bits = [self.message]
    if self.possibilities:
        if len(self.possibilities) == 1:
            bits.append(""Did you mean %s?"" % self.possibilities[0])
        else:
            possibilities = sorted(self.possibilities)
            bits.append(""(Possible options: %s)"" % "", "".join(possibilities))
    return ""  "".join(bits)
",if len ( self . possibilities ) == 1 :,106
"def _collect_logs(model):
    page_token = None
    all_logs = []
    while True:
        paginated_logs = model.lookup_logs(now, later, page_token=page_token)
        page_token = paginated_logs.next_page_token
        all_logs.extend(paginated_logs.logs)
        if page_token is None:
            break
    return all_logs
",if page_token is None :,105
"def run(self):
    while True:
        context_id_list_tuple = self._inflated_addresses.get(block=True)
        if context_id_list_tuple is _SHUTDOWN_SENTINEL:
            break
        c_id, inflated_address_list = context_id_list_tuple
        inflated_value_map = dict(inflated_address_list)
        if c_id in self._contexts:
            self._contexts[c_id].set_from_tree(inflated_value_map)
",if context_id_list_tuple is _SHUTDOWN_SENTINEL :,135
"def _setup_prefix(self):
    # we assume here that our metadata may be nested inside a ""basket""
    # of multiple eggs; that's why we use module_path instead of .archive
    path = self.module_path
    old = None
    while path != old:
        if path.lower().endswith("".egg""):
            self.egg_name = os.path.basename(path)
            self.egg_info = os.path.join(path, ""EGG-INFO"")
            self.egg_root = path
            break
        old = path
        path, base = os.path.split(path)
","if path . lower ( ) . endswith ( "".egg"" ) :",160
"def get_filename(self, prompt):
    okay = False
    val = """"
    while not okay:
        val = raw_input(""%s: %s"" % (prompt, val))
        val = os.path.expanduser(val)
        if os.path.isfile(val):
            okay = True
        elif os.path.isdir(val):
            path = val
            val = self.choose_from_list(os.listdir(path))
            if val:
                val = os.path.join(path, val)
                okay = True
            else:
                val = """"
        else:
            print(""Invalid value: %s"" % val)
            val = """"
    return val
",elif os . path . isdir ( val ) :,194
"def versions(self, sitename, data):
    # handle the query of type {""query"": '{""key"": ""/books/ia:foo00bar"", ...}}
    if ""query"" in data:
        q = json.loads(data[""query""])
        itemid = self._get_itemid(q.get(""key""))
        if itemid:
            key = q[""key""]
            return json.dumps([self.dummy_edit(key)])
    # if not just go the default way
    return ConnectionMiddleware.versions(self, sitename, data)
",if itemid :,133
"def read_stanza(self):
    while True:
        try:
            stanza_end = self._buffer.index(b""\n"")
            stanza = self.decoder.decode(self._buffer[:stanza_end])
            self._buffer = self._buffer[stanza_end + 1 :]
            colon = stanza.index("":"")
            return stanza[:colon], stanza[colon + 1 :]
        except ValueError:
            bytes = self.read_bytes()
            if not bytes:
                return None
            else:
                self._buffer += bytes
",if not bytes :,164
"def decodeattrs(attrs):
    names = []
    for bit in range(16):
        mask = 1 << bit
        if attrs & mask:
            if attrnames.has_key(mask):
                names.append(attrnames[mask])
            else:
                names.append(hex(mask))
    return names
",if attrs & mask :,88
"def _set_http_cookie():
    if conf.cookie:
        if isinstance(conf.cookie, dict):
            conf.http_headers[HTTP_HEADER.COOKIE] = ""; "".join(
                map(lambda x: ""="".join(x), conf.cookie.items())
            )
        else:
            conf.http_headers[HTTP_HEADER.COOKIE] = conf.cookie
","if isinstance ( conf . cookie , dict ) :",101
"def __ne__(self, other):
    if isinstance(other, WeakMethod):
        if not self._alive or not other._alive:
            return self is not other
        return weakref.ref.__ne__(self, other) or self._func_ref != other._func_ref
    return True
",if not self . _alive or not other . _alive :,72
"def update_unread(self, order_id, reset=False):
    conn = Database.connect_database(self.PATH)
    with conn:
        cursor = conn.cursor()
        if reset is False:
            cursor.execute(
                """"""UPDATE sales SET unread = unread + 1 WHERE id=?;"""""", (order_id,)
            )
        else:
            cursor.execute(""""""UPDATE sales SET unread=0 WHERE id=?;"""""", (order_id,))
        conn.commit()
    conn.close()
",if reset is False :,131
"def _get_field_value(self, test, key, match):
    if test.ver == ofproto_v1_0.OFP_VERSION:
        members = inspect.getmembers(match)
        for member in members:
            if member[0] == key:
                field_value = member[1]
            elif member[0] == ""wildcards"":
                wildcards = member[1]
        if key == ""nw_src"":
            field_value = test.nw_src_to_str(wildcards, field_value)
        elif key == ""nw_dst"":
            field_value = test.nw_dst_to_str(wildcards, field_value)
    else:
        field_value = match[key]
    return field_value
","elif key == ""nw_dst"" :",200
"def nested_filter(self, items, mask):
    keep_current = self.current_mask(mask)
    keep_nested_lookup = self.nested_masks(mask)
    for k, v in items:
        keep_nested = keep_nested_lookup.get(k)
        if k in keep_current:
            if keep_nested is not None:
                if isinstance(v, dict):
                    yield k, dict(self.nested_filter(v.items(), keep_nested))
            else:
                yield k, v
",if k in keep_current :,142
"def goToPrevMarkedHeadline(self, event=None):
    """"""Select the next marked node.""""""
    c = self
    p = c.p
    if not p:
        return
    p.moveToThreadBack()
    wrapped = False
    while 1:
        if p and p.isMarked():
            break
        elif p:
            p.moveToThreadBack()
        elif wrapped:
            break
        else:
            wrapped = True
            p = c.rootPosition()
    if not p:
        g.blue(""done"")
    c.treeSelectHelper(p)  # Sets focus.
",if p and p . isMarked ( ) :,164
"def sample(self, **config):
    """"""Sample a configuration from this search space.""""""
    ret = {}
    ret.update(self.data)
    kwspaces = self.kwspaces
    kwspaces.update(config)
    striped_keys = [k.split(SPLITTER)[0] for k in config.keys()]
    for k, v in kwspaces.items():
        if k in striped_keys:
            if isinstance(v, NestedSpace):
                sub_config = _strip_config_space(config, prefix=k)
                ret[k] = v.sample(**sub_config)
            else:
                ret[k] = v
    return ret
",if k in striped_keys :,172
"def update_gradients_full(self, dL_dK, X, X2=None):
    if self.ARD:
        phi1 = self.phi(X)
        if X2 is None or X is X2:
            self.variance.gradient = np.einsum(""ij,iq,jq->q"", dL_dK, phi1, phi1)
        else:
            phi2 = self.phi(X2)
            self.variance.gradient = np.einsum(""ij,iq,jq->q"", dL_dK, phi1, phi2)
    else:
        self.variance.gradient = np.einsum(""ij,ij"", dL_dK, self._K(X, X2)) * self.beta
",if X2 is None or X is X2 :,185
"def post(self):
    host_json = json.loads(request.data)
    host_os = host_json.get(""os"")
    if host_os:
        result = get_monkey_executable(host_os.get(""type""), host_os.get(""machine""))
        if result:
            # change resulting from new base path
            executable_filename = result[""filename""]
            real_path = MonkeyDownload.get_executable_full_path(executable_filename)
            if os.path.isfile(real_path):
                result[""size""] = os.path.getsize(real_path)
                return result
    return {}
",if os . path . isfile ( real_path ) :,167
"def _encode_data(
    self,
    data,
    content_type,
):
    if content_type is MULTIPART_CONTENT:
        return encode_multipart(BOUNDARY, data)
    else:
        # Encode the content so that the byte representation is correct.
        match = CONTENT_TYPE_RE.match(content_type)
        if match:
            charset = match.group(1)
        else:
            charset = settings.DEFAULT_CHARSET
        return force_bytes(data, encoding=charset)
",if match :,130
"def _merge_scientific_float_tokens(tokens: Iterable[str]) -> List[str]:
    tokens = list(tokens)
    i = 0
    while ""e"" in tokens[i + 1 :]:
        i = tokens.index(""e"", i + 1)
        s = i - 1
        e = i + 1
        if not re.match(""[0-9]"", str(tokens[s])):
            continue
        if re.match(""[+-]"", str(tokens[e])):
            e += 1
        if re.match(""[0-9]"", str(tokens[e])):
            e += 1
            tokens[s:e] = ["""".join(tokens[s:e])]
            i -= 1
    return tokens
","if re . match ( ""[0-9]"" , str ( tokens [ e ] ) ) :",184
"def convert_with_key(self, key, value, replace=True):
    result = self.configurator.convert(value)
    # If the converted value is different, save for next time
    if value is not result:
        if replace:
            self[key] = result
        if type(result) in (ConvertingDict, ConvertingList, ConvertingTuple):
            result.parent = self
            result.key = key
    return result
",if replace :,111
"def OnListEndLabelEdit(self, std, extra):
    item = extra[0]
    text = item[4]
    if text is None:
        return
    item_id = self.GetItem(item[0])[6]
    from bdb import Breakpoint
    for bplist in Breakpoint.bplist.itervalues():
        for bp in bplist:
            if id(bp) == item_id:
                if text.strip().lower() == ""none"":
                    text = None
                bp.cond = text
                break
    self.RespondDebuggerData()
",if id ( bp ) == item_id :,151
"def add(self, url: str, future_nzo: NzbObject, when: Optional[int] = None):
    """"""Add an URL to the URLGrabber queue, 'when' is seconds from now""""""
    if future_nzo and when:
        # Always increase counter
        future_nzo.url_tries += 1
        # Too many tries? Cancel
        if future_nzo.url_tries > cfg.max_url_retries():
            self.fail_to_history(future_nzo, url, T(""Maximum retries""))
            return
        future_nzo.url_wait = time.time() + when
    self.queue.put((url, future_nzo))
",if future_nzo . url_tries > cfg . max_url_retries ( ) :,172
"def _is_datetime_string(series):
    if series.dtype == object:
        not_numeric = False
        try:
            pd.to_numeric(series)
        except Exception as e:
            not_numeric = True
        datetime_col = None
        if not_numeric:
            try:
                datetime_col = pd.to_datetime(series)
            except Exception as e:
                return False
        if datetime_col is not None:
            return True
    return False
",if not_numeric :,138
"def _getEventAndObservers(self, event):
    if isinstance(event, xpath.XPathQuery):
        # Treat as xpath
        observers = self._xpathObservers
    else:
        if self.prefix == event[: len(self.prefix)]:
            # Treat as event
            observers = self._eventObservers
        else:
            # Treat as xpath
            event = xpath.internQuery(event)
            observers = self._xpathObservers
    return event, observers
",if self . prefix == event [ : len ( self . prefix ) ] :,131
"def test_wildcard_import():
    bonobo = __import__(""bonobo"")
    assert bonobo.__version__
    for name in dir(bonobo):
        # ignore attributes starting by underscores
        if name.startswith(""_""):
            continue
        attr = getattr(bonobo, name)
        if inspect.ismodule(attr):
            continue
        assert name in bonobo.__all__
",if inspect . ismodule ( attr ) :,97
"def relint_views(wid=None):
    windows = [sublime.Window(wid)] if wid else sublime.windows()
    for window in windows:
        for view in window.views():
            if view.buffer_id() in persist.assigned_linters and view.is_primary():
                hit(view, ""relint_views"")
",if view . buffer_id ( ) in persist . assigned_linters and view . is_primary ( ) :,93
"def _check_for_unknown_gender(self):
    if self.obj.get_gender() == Person.UNKNOWN:
        d = GenderDialog(parent=self.window)
        gender = d.run()
        d.destroy()
        if gender >= 0:
            self.obj.set_gender(gender)
",if gender >= 0 :,83
"def add_to_path(self, fnames):
    """"""Add fnames to path""""""
    indexes = []
    for path in fnames:
        project = self.get_source_project(path)
        if project.add_to_pythonpath(path):
            self.parent_widget.emit(SIGNAL(""pythonpath_changed()""))
            indexes.append(self.get_index(path))
    if indexes:
        self.reset_icon_provider()
        for index in indexes:
            self.update(index)
",if project . add_to_pythonpath ( path ) :,132
"def validate(self, value):
    if value.grid_id is not None:
        if not isinstance(value, self.proxy_class):
            self.error(""FileField only accepts GridFSProxy values"")
        if not isinstance(value.grid_id, ObjectId):
            self.error(""Invalid GridFSProxy value"")
","if not isinstance ( value . grid_id , ObjectId ) :",82
"def shortcut(self, input, ch_out, stride, name, if_first=False):
    ch_in = input.shape[1]
    if ch_in != ch_out or stride != 1:
        if if_first:
            return self.conv_bn_layer(input, ch_out, 1, stride, name=name)
        else:
            return self.conv_bn_layer_new(input, ch_out, 1, stride, name=name)
    else:
        return input
",if if_first :,127
"def convert_path(ctx, tpath):
    for points, code in tpath.iter_segments():
        if code == Path.MOVETO:
            ctx.move_to(*points)
        elif code == Path.LINETO:
            ctx.line_to(*points)
        elif code == Path.CURVE3:
            ctx.curve_to(
                points[0], points[1], points[0], points[1], points[2], points[3]
            )
        elif code == Path.CURVE4:
            ctx.curve_to(*points)
        elif code == Path.CLOSEPOLY:
            ctx.close_path()
",elif code == Path . CURVE4 :,172
"def _get_build_status(self, job_name, build_number):
    try:
        build_info = self.server.get_build_info(job_name, build_number)
        if build_info[""building""]:
            return ""building""
        else:
            return ""built""
    except jenkins.NotFoundException:
        return ""not found""
","if build_info [ ""building"" ] :",96
"def _parse_param_value(name, datatype, default):
    if datatype == ""bool"":
        if default.lower() == ""true"":
            return True
        elif default.lower() == ""false"":
            return False
        else:
            _s = ""{}: Invalid default value '{}' for bool parameter {}""
            raise SyntaxError(_s.format(self.name, default, p))
    elif datatype == ""int"":
        if type(default) == int:
            return default
        else:
            return int(default, 0)
    elif datatype == ""real"":
        if type(default) == float:
            return default
        else:
            return float(default)
    else:
        return str(default)
",if type ( default ) == float :,191
"def get_fills(self, exchange_order_id):
    async with aiohttp.ClientSession() as client:
        response: aiohttp.ClientResponse = await client.get(
            f""{BASE_URL}{FILLS_ROUTE}"",
            params={""orderId"": exchange_order_id, ""limit"": 100},
        )
        if response.status >= 300:
            try:
                msg = await response.json()
            except ValueError:
                msg = await response.text()
            raise DydxAsyncAPIError(response.status, msg)
        return await response.json()
",if response . status >= 300 :,156
"def semanticTags(self, semanticTags):
    if semanticTags is None:
        self.__semanticTags = OrderedDict()
    # check
    for key, value in list(semanticTags.items()):
        if not isinstance(key, int):
            raise TypeError(""At least one key is not a valid int position"")
        if not isinstance(value, list):
            raise TypeError(
                ""At least one value of the provided dict is not a list of string""
            )
        for x in value:
            if not isinstance(x, str):
                raise TypeError(
                    ""At least one value of the provided dict is not a list of string""
                )
    self.__semanticTags = semanticTags
","if not isinstance ( x , str ) :",184
"def start_cutting_tool(self, event, axis, direction):
    toggle = event.EventObject
    self.cutting = toggle.Value
    if toggle.Value:
        # Disable the other toggles
        for child in self.cutsizer.Children:
            child = child.Window
            if child != toggle:
                child.Value = False
        self.cutting_axis = axis
        self.cutting_direction = direction
    else:
        self.cutting_axis = None
        self.cutting_direction = None
    self.cutting_dist = None
",if child != toggle :,150
"def decoration_helper(self, patched, args, keywargs):
    extra_args = []
    with contextlib.ExitStack() as exit_stack:
        for patching in patched.patchings:
            arg = exit_stack.enter_context(patching)
            if patching.attribute_name is not None:
                keywargs.update(arg)
            elif patching.new is DEFAULT:
                extra_args.append(arg)
        args += tuple(extra_args)
        yield (args, keywargs)
",elif patching . new is DEFAULT :,134
"def decodeattrs(attrs):
    names = []
    for bit in range(16):
        mask = 1 << bit
        if attrs & mask:
            if attrnames.has_key(mask):
                names.append(attrnames[mask])
            else:
                names.append(hex(mask))
    return names
",if attrnames . has_key ( mask ) :,88
"def pytest_collection_modifyitems(items):
    for item in items:
        if item.nodeid.startswith(""tests/params""):
            if ""stage"" not in item.keywords:
                item.add_marker(pytest.mark.stage(""unit""))
            if ""init"" not in item.keywords:
                item.add_marker(pytest.mark.init(rng_seed=123))
","if ""init"" not in item . keywords :",102
"def handle_socket(self, request):
    conn = request.connection
    while True:
        chunk = conn.recv(4)
        if len(chunk) < 4:
            break
        slen = struct.unpack("">L"", chunk)[0]
        chunk = conn.recv(slen)
        while len(chunk) < slen:
            chunk = chunk + conn.recv(slen - len(chunk))
        obj = pickle.loads(chunk)
        record = logging.makeLogRecord(obj)
        self.log_output += record.msg + ""\n""
        self.handled.release()
",if len ( chunk ) < 4 :,156
"def on_source_foreach(self, model, path, iter, id):
    m_id = model.get_value(iter, self.COLUMN_ID)
    if m_id == id:
        if self._foreach_mode == ""get"":
            self._foreach_take = model.get_value(iter, self.COLUMN_ENABLED)
        elif self._foreach_mode == ""set"":
            self._foreach_take = iter
","elif self . _foreach_mode == ""set"" :",113
"def parts():
    for l in lists.leaves:
        head_name = l.get_head_name()
        if head_name == ""System`List"":
            yield l.leaves
        elif head_name != ""System`Missing"":
            raise MessageException(""Catenate"", ""invrp"", l)
","elif head_name != ""System`Missing"" :",78
"def __fill_counter_values(self, command: str):
    result = []
    regex = r""(item[0-9]+\.counter_value)""
    for token in re.split(regex, command):
        if re.match(regex, token) is not None:
            try:
                result.append(str(self.simulator_config.item_dict[token].value))
            except (KeyError, ValueError, AttributeError):
                logger.error(""Could not get counter value for "" + token)
        else:
            result.append(token)
    return """".join(result)
","if re . match ( regex , token ) is not None :",152
"def IMPORTFROM(self, node):
    if node.module == ""__future__"":
        if not self.futuresAllowed:
            self.report(messages.LateFutureImport, node, [n.name for n in node.names])
    else:
        self.futuresAllowed = False
    for alias in node.names:
        if alias.name == ""*"":
            self.scope.importStarred = True
            self.report(messages.ImportStarUsed, node, node.module)
            continue
        name = alias.asname or alias.name
        importation = Importation(name, node)
        if node.module == ""__future__"":
            importation.used = (self.scope, node)
        self.addBinding(node, importation)
","if node . module == ""__future__"" :",190
"def _split_batch_list(args, batch_list):
    new_list = []
    for batch in batch_list.batches:
        new_list.append(batch)
        if len(new_list) == args.batch_size_limit:
            yield batch_pb2.BatchList(batches=new_list)
            new_list = []
    if new_list:
        yield batch_pb2.BatchList(batches=new_list)
",if len ( new_list ) == args . batch_size_limit :,116
"def get_branch_or_use_upstream(branch_name, arg, repo):
    if not branch_name:  # use upstream branch
        current_b = repo.current_branch
        upstream_b = current_b.upstream
        if not upstream_b:
            raise ValueError(
                ""No {0} branch specified and the current branch has no upstream ""
                ""branch set"".format(arg)
            )
        ret = current_b.upstream
    else:
        ret = get_branch(branch_name, repo)
    return ret
",if not upstream_b :,148
"def __init__(self, **settings):
    default_settings = self.get_default_settings()
    for name, value in default_settings.items():
        if not hasattr(self, name):
            setattr(self, name, value)
    for name, value in settings.items():
        if name not in default_settings:
            raise ImproperlyConfigured(
                ""Invalid setting '{}' for {}"".format(
                    name,
                    self.__class__.__name__,
                )
            )
        setattr(self, name, value)
","if not hasattr ( self , name ) :",144
"def _declare(self, name, obj, included=False, quals=0):
    if name in self._declarations:
        prevobj, prevquals = self._declarations[name]
        if prevobj is obj and prevquals == quals:
            return
        if not self._override:
            raise api.FFIError(
                ""multiple declarations of %s (for interactive usage, ""
                ""try cdef(xx, override=True))"" % (name,)
            )
    assert ""__dotdotdot__"" not in name.split()
    self._declarations[name] = (obj, quals)
    if included:
        self._included_declarations.add(obj)
",if not self . _override :,174
"def include_file(name, fdir=tmp_dir, b64=False):
    try:
        if fdir is None:
            fdir = """"
        if b64:
            with io.open(os.path.join(fdir, name), ""rb"") as f:
                return base64.b64encode(f.read()).decode(""utf-8"")
        else:
            with io.open(os.path.join(fdir, name), ""r"", encoding=""utf-8"") as f:
                return f.read()
    except (OSError, IOError) as e:
        logger.error(""Could not include file '{}': {}"".format(name, e))
",if b64 :,174
"def to_raw_json(self):
    parts = {}
    for p in self.parts:
        if p[0] not in parts:
            parts[p[0]] = []
        parts[p[0]].append({""value"": p[2], ""parameters"": p[1]})
    children = [x.to_raw_json() for x in self.children]
    return {
        ""type"": self.__class__.__name__,
        ""children"": children,
        ""parts"": parts,
    }
",if p [ 0 ] not in parts :,127
"def process_output(
    output: str, filename: str, start_line: int
) -> Tuple[Optional[str], bool]:
    error_found = False
    for line in output.splitlines():
        t = get_revealed_type(line, filename, start_line)
        if t:
            return t, error_found
        elif ""error:"" in line:
            error_found = True
    return None, True  # finding no reveal_type is an error
",if t :,117
"def __init__(
    self, resize_keyboard=None, one_time_keyboard=None, selective=None, row_width=3
):
    if row_width > self.max_row_keys:
        # Todo: Will be replaced with Exception in future releases
        if not DISABLE_KEYLEN_ERROR:
            logger.error(
                ""Telegram does not support reply keyboard row width over %d.""
                % self.max_row_keys
            )
        row_width = self.max_row_keys
    self.resize_keyboard = resize_keyboard
    self.one_time_keyboard = one_time_keyboard
    self.selective = selective
    self.row_width = row_width
    self.keyboard = []
",if not DISABLE_KEYLEN_ERROR :,188
"def realizeElementExpressions(innerElement):
    elementHasBeenRealized = False
    for exp in innerElement.expressions:
        if not hasattr(exp, ""realize""):
            continue
        # else:
        before, during, after = exp.realize(innerElement)
        elementHasBeenRealized = True
        for n in before:
            newStream.append(n)
        if during is not None:
            newStream.append(during)
        for n in after:
            newStream.append(n)
    if elementHasBeenRealized is False:
        newStream.append(innerElement)
",if during is not None :,164
"def lex_number(self, pos):
    # numeric literal
    start = pos
    found_dot = False
    while pos < len(self.string) and (
        self.string[pos].isdigit() or self.string[pos] == "".""
    ):
        if self.string[pos] == ""."":
            if found_dot is True:
                raise ValueError(""Invalid number. Found multiple '.'"")
            found_dot = True
        # technically we allow more than one ""."" and let float()'s parsing
        # complain later
        pos += 1
    val = self.string[start:pos]
    return Token(TokenType.LNUM, val, len(val))
","if self . string [ pos ] == ""."" :",168
"def rename(src, dst):
    # Try atomic or pseudo-atomic rename
    if _rename(src, dst):
        return
    # Fall back to ""move away and replace""
    try:
        os.rename(src, dst)
    except OSError as e:
        if e.errno != errno.EEXIST:
            raise
        old = ""%s-%08x"" % (dst, random.randint(0, sys.maxsize))
        os.rename(dst, old)
        os.rename(src, dst)
        try:
            os.unlink(old)
        except Exception:
            pass
",if e . errno != errno . EEXIST :,156
"def _the_callback(widget, event_id):
    point = widget.GetCenter()
    index = widget.WIDGET_INDEX
    if hasattr(callback, ""__call__""):
        if num > 1:
            args = [point, index]
        else:
            args = [point]
        if pass_widget:
            args.append(widget)
        try_callback(callback, *args)
    return
",if pass_widget :,109
"def run(self):
    for _ in range(self.n):
        error = True
        try:
            self.collection.insert_one({""test"": ""insert""})
            error = False
        except:
            if not self.expect_exception:
                raise
        if self.expect_exception:
            assert error
",if not self . expect_exception :,91
"def handle(self, *args: Any, **options: Any) -> None:
    realm = self.get_realm(options)
    if options[""all""]:
        if realm is None:
            raise CommandError(
                ""You must specify a realm if you choose the --all option.""
            )
        self.fix_all_users(realm)
        return
    self.fix_emails(realm, options[""emails""])
",if realm is None :,108
"def recv_tdi(self, nbits, pos):
    bits = 0
    for n in range(nbits * 2):
        yield from self._wait_for_tck()
        if (yield self.tck.o) == pos:
            bits = (bits << 1) | (yield self.tdi.o)
    return bits
",if ( yield self . tck . o ) == pos :,86
"def _split_head(self):
    if not hasattr(self, ""_severed_head""):
        if self._tree:
            tree = self._tree.copy()
            head = tree.get_heading_text()
            tree.remove_heading()
            self._severed_head = (head, tree)
        else:
            self._severed_head = (None, None)
    return self._severed_head
",if self . _tree :,113
"def buildSearchTrie(self, choices):
    searchtrie = trie.Trie()
    for choice in choices:
        for token in self.tokenizeChoice(choice):
            if not searchtrie.has_key(token):
                searchtrie[token] = []
            searchtrie[token].append(choice)
    return searchtrie
",if not searchtrie . has_key ( token ) :,85
"def format_sql(sql, params):
    rv = []
    if isinstance(params, dict):
        # convert sql with named parameters to sql with unnamed parameters
        conv = _FormatConverter(params)
        if params:
            sql = sql_to_string(sql)
            sql = sql % conv
            params = conv.params
        else:
            params = ()
    for param in params or ():
        if param is None:
            rv.append(""NULL"")
        param = safe_repr(param)
        rv.append(param)
    return sql, rv
",if param is None :,151
"def on_completed2():
    doner[0] = True
    if not qr:
        if len(ql) > 0:
            observer.on_next(False)
            observer.on_completed()
        elif donel[0]:
            observer.on_next(True)
            observer.on_completed()
",elif donel [ 0 ] :,86
"def notify_digest(self, frequency, changes):
    notifications = defaultdict(list)
    users = {}
    for change in changes:
        for user in self.get_users(frequency, change):
            if change.project is None or user.can_access_project(change.project):
                notifications[user.pk].append(change)
                users[user.pk] = user
    for user in users.values():
        self.send_digest(
            user.profile.language,
            user.email,
            notifications[user.pk],
            subscription=user.current_subscription,
        )
",if change . project is None or user . can_access_project ( change . project ) :,163
"def _any_listener_using(self, target_group_arn):
    for load_balancer in self.load_balancers.values():
        for listener in load_balancer.listeners.values():
            for rule in listener.rules:
                for action in rule.actions:
                    if action.data.get(""target_group_arn"") == target_group_arn:
                        return True
    return False
","if action . data . get ( ""target_group_arn"" ) == target_group_arn :",110
"def train_dict(self, triples):
    """"""Train a dict lemmatizer given training (word, pos, lemma) triples.""""""
    # accumulate counter
    ctr = Counter()
    ctr.update([(p[0], p[1], p[2]) for p in triples])
    # find the most frequent mappings
    for p, _ in ctr.most_common():
        w, pos, l = p
        if (w, pos) not in self.composite_dict:
            self.composite_dict[(w, pos)] = l
        if w not in self.word_dict:
            self.word_dict[w] = l
    return
",if w not in self . word_dict :,158
"def parse_git_config(path):
    """"""Parse git config file.""""""
    config = dict()
    section = None
    with open(os.path.join(path, ""config""), ""r"") as f:
        for line in f:
            line = line.strip()
            if line.startswith(""[""):
                section = line[1:-1].strip()
                config[section] = dict()
            elif section:
                key, value = line.replace("" "", """").split(""="")
                config[section][key] = value
    return config
","if line . startswith ( ""["" ) :",146
"def send_signal(self, pid, signum):
    if pid in self.processes:
        process = self.processes[pid]
        hook_result = self.call_hook(""before_signal"", pid=pid, signum=signum)
        if signum != signal.SIGKILL and not hook_result:
            logger.debug(
                ""before_signal hook didn't return True ""
                ""=> signal %i is not sent to %i"" % (signum, pid)
            )
        else:
            process.send_signal(signum)
        self.call_hook(""after_signal"", pid=pid, signum=signum)
    else:
        logger.debug(""process %s does not exist"" % pid)
",if signum != signal . SIGKILL and not hook_result :,186
"def validate_pos_return(self):
    if self.is_pos and self.is_return:
        total_amount_in_payments = 0
        for payment in self.payments:
            total_amount_in_payments += payment.amount
        invoice_total = self.rounded_total or self.grand_total
        if total_amount_in_payments < invoice_total:
            frappe.throw(
                _(""Total payments amount can't be greater than {}"").format(
                    -invoice_total
                )
            )
",if total_amount_in_payments < invoice_total :,144
"def delete(key, inner_key=None):
    if inner_key is not None:
        try:
            del cache[key][inner_key]
            del use_count[key][inner_key]
            if not cache[key]:
                del cache[key]
                del use_count[key]
            wrapper.cache_size -= 1
        except KeyError:
            return False
        else:
            return True
    else:
        try:
            wrapper.cache_size -= len(cache[key])
            del cache[key]
            del use_count[key]
        except KeyError:
            return False
        else:
            return True
",if not cache [ key ] :,189
"def insertionsort(array):
    size = array.getsize()
    array.reset(""Insertion sort"")
    for i in range(1, size):
        j = i - 1
        while j >= 0:
            if array.compare(j, j + 1) <= 0:
                break
            array.swap(j, j + 1)
            j = j - 1
    array.message(""Sorted"")
","if array . compare ( j , j + 1 ) <= 0 :",109
"def publish_state(cls, payload, state):
    try:
        if isinstance(payload, LiveActionDB):
            if state == action_constants.LIVEACTION_STATUS_REQUESTED:
                cls.process(payload)
            else:
                worker.get_worker().process(payload)
    except Exception:
        traceback.print_exc()
        print(payload)
",if state == action_constants . LIVEACTION_STATUS_REQUESTED :,99
"def change_opacity_function(self, new_f):
    self.opacity_function = new_f
    dr = self.radius / self.num_levels
    sectors = []
    for submob in self.submobjects:
        if type(submob) == AnnularSector:
            sectors.append(submob)
    for (r, submob) in zip(np.arange(0, self.radius, dr), sectors):
        if type(submob) != AnnularSector:
            # it's the shadow, don't dim it
            continue
        alpha = self.opacity_function(r)
        submob.set_fill(opacity=alpha)
",if type ( submob ) != AnnularSector :,180
"def is_suppressed_warning(
    type: str, subtype: str, suppress_warnings: List[str]
) -> bool:
    """"""Check the warning is suppressed or not.""""""
    if type is None:
        return False
    for warning_type in suppress_warnings:
        if ""."" in warning_type:
            target, subtarget = warning_type.split(""."", 1)
        else:
            target, subtarget = warning_type, None
        if target == type:
            if (
                subtype is None
                or subtarget is None
                or subtarget == subtype
                or subtarget == ""*""
            ):
                return True
    return False
","if ""."" in warning_type :",178
"def set_many(self, mapping, timeout=None):
    timeout = self._normalize_timeout(timeout)
    # Use transaction=False to batch without calling redis MULTI
    # which is not supported by twemproxy
    pipe = self._client.pipeline(transaction=False)
    for key, value in _items(mapping):
        dump = self.dump_object(value)
        if timeout == -1:
            pipe.set(name=self.key_prefix + key, value=dump)
        else:
            pipe.setex(name=self.key_prefix + key, value=dump, time=timeout)
    return pipe.execute()
",if timeout == - 1 :,160
"def maybe_relative_path(path):
    if not os.path.isabs(path):
        return path  # already relative
    dir = path
    names = []
    while True:
        prevdir = dir
        dir, name = os.path.split(prevdir)
        if dir == prevdir or not dir:
            return path  # failed to make it relative
        names.append(name)
        try:
            if samefile(dir, os.curdir):
                names.reverse()
                return os.path.join(*names)
        except OSError:
            pass
","if samefile ( dir , os . curdir ) :",155
"def word_range(word):
    for ind in range(len(word)):
        temp = word[ind]
        for c in [chr(x) for x in range(ord(""a""), ord(""z"") + 1)]:
            if c != temp:
                yield word[:ind] + c + word[ind + 1 :]
",if c != temp :,83
"def validate(self):
    self.update_soil_edit(""sand_composition"")
    for soil_type in self.soil_types:
        if self.get(soil_type) > 100 or self.get(soil_type) < 0:
            frappe.throw(_(""{0} should be a value between 0 and 100"").format(soil_type))
    if sum(self.get(soil_type) for soil_type in self.soil_types) != 100:
        frappe.throw(_(""Soil compositions do not add up to 100""))
",if self . get ( soil_type ) > 100 or self . get ( soil_type ) < 0 :,142
"def on_click(self, event):
    run = self._is_running()
    if event[""button""] == self.button_activate:
        self.py3.command_run([""xscreensaver-command"", ""-activate""])
    if event[""button""] == self.button_toggle:
        if run:
            self.py3.command_run([""xscreensaver-command"", ""-exit""])
        else:
            # Because we want xscreensaver to continue running after
            # exit, we instead use preexec_fn=setpgrp here.
            Popen(
                [""xscreensaver"", ""-no-splash"", ""-no-capture-stderr""],
                stdout=PIPE,
                stderr=PIPE,
                preexec_fn=setpgrp,
            )
",if run :,199
"def maybe_relative_path(path):
    if not os.path.isabs(path):
        return path  # already relative
    dir = path
    names = []
    while True:
        prevdir = dir
        dir, name = os.path.split(prevdir)
        if dir == prevdir or not dir:
            return path  # failed to make it relative
        names.append(name)
        try:
            if samefile(dir, os.curdir):
                names.reverse()
                return os.path.join(*names)
        except OSError:
            pass
",if dir == prevdir or not dir :,155
"def _format_micros(self, datestring):
    parts = datestring[:-1].split(""."")
    if len(parts) == 1:
        if datestring.endswith(""Z""):
            return datestring[:-1] + "".000000Z""
        else:
            return datestring + "".000000Z""
    else:
        micros = parts[-1][:6] if len(parts[-1]) > 6 else parts[-1]
        return ""."".join(parts[:-1] + [""{:06d}"".format(int(micros))]) + ""Z""
","if datestring . endswith ( ""Z"" ) :",135
"def preprocess_raw_enwik9(input_filename, output_filename):
    with open(input_filename, ""r"") as f1:
        with open(output_filename, ""w"") as f2:
            while True:
                line = f1.readline()
                if not line:
                    break
                line = list(enwik9_norm_transform([line]))[0]
                if line != "" "" and line != """":
                    if line[0] == "" "":
                        line = line[1:]
                    f2.writelines(line + ""\n"")
","if line != "" "" and line != """" :",164
"def set(self, item, data):
    if not type(item) is slice:
        item = slice(item, item + len(data), None)
    virt_item = self.item2virtitem(item)
    if not virt_item:
        return
    off = 0
    for s, n_item in virt_item:
        if isinstance(s, ProgBits):
            i = slice(off, n_item.stop + off - n_item.start, n_item.step)
            data_slice = data.__getitem__(i)
            s.content.__setitem__(n_item, data_slice)
            off = i.stop
        else:
            raise ValueError(""TODO XXX"")
    return
","if isinstance ( s , ProgBits ) :",184
"def walk(msg, callback, data):
    partnum = 0
    for part in msg.walk():
        # multipart/* are just containers
        if part.get_content_maintype() == ""multipart"":
            continue
        ctype = part.get_content_type()
        if ctype is None:
            ctype = OCTET_TYPE
        filename = part.get_filename()
        if not filename:
            filename = PART_FN_TPL % (partnum)
        headers = dict(part)
        LOG.debug(headers)
        headers[""Content-Type""] = ctype
        payload = util.fully_decoded_payload(part)
        callback(data, filename, payload, headers)
        partnum = partnum + 1
",if not filename :,190
"def _run_wes(args):
    """"""Run CWL using a Workflow Execution Service (WES) endpoint""""""
    main_file, json_file, project_name = _get_main_and_json(args.directory)
    main_file = _pack_cwl(main_file)
    if args.host and ""stratus"" in args.host:
        _run_wes_stratus(args, main_file, json_file)
    else:
        opts = [""--no-wait""]
        if args.host:
            opts += [""--host"", args.host]
        if args.auth:
            opts += [""--auth"", args.auth]
        cmd = [""wes-client""] + opts + [main_file, json_file]
        _run_tool(cmd)
",if args . host :,197
"def insertTestData(self, rows):
    for row in rows:
        if isinstance(row, Worker):
            self.workers[row.id] = dict(
                id=row.id, name=row.name, paused=0, graceful=0, info=row.info
            )
        elif isinstance(row, ConfiguredWorker):
            row.id = row.buildermasterid * 10000 + row.workerid
            self.configured[row.id] = dict(
                buildermasterid=row.buildermasterid, workerid=row.workerid
            )
        elif isinstance(row, ConnectedWorker):
            self.connected[row.id] = dict(masterid=row.masterid, workerid=row.workerid)
","elif isinstance ( row , ConfiguredWorker ) :",194
"def local_shape_to_shape_i(node):
    if node.op == T.shape:
        # This optimization needs ShapeOpt and fgraph.shape_feature
        if not hasattr(node.fgraph, ""shape_feature""):
            return
        shape_feature = node.fgraph.shape_feature
        ret = shape_feature.make_vector_shape(node.inputs[0])
        # We need to copy over stack trace from input to output
        copy_stack_trace(node.outputs[0], ret)
        return [ret]
","if not hasattr ( node . fgraph , ""shape_feature"" ) :",136
"def get_config():
    """"""Get INI parser with version.ini data.""""""
    # TODO(hanuszczak): See comment in `setup.py` for `grr-response-proto`.
    ini_path = os.path.join(THIS_DIRECTORY, ""version.ini"")
    if not os.path.exists(ini_path):
        ini_path = os.path.join(THIS_DIRECTORY, ""../../version.ini"")
        if not os.path.exists(ini_path):
            raise RuntimeError(""Couldn't find version.ini"")
    config = configparser.ConfigParser()
    config.read(ini_path)
    return config
",if not os . path . exists ( ini_path ) :,156
"def init_weights(self, pretrained=None):
    if isinstance(pretrained, str):
        logger = logging.getLogger()
        load_checkpoint(self, pretrained, strict=False, logger=logger)
    elif pretrained is None:
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                kaiming_init(m)
            elif isinstance(m, (_BatchNorm, nn.GroupNorm)):
                constant_init(m, 1)
    else:
        raise TypeError(""pretrained must be a str or None"")
","if isinstance ( m , nn . Conv2d ) :",141
"def isValidDateString(config_param_name, value, valid_value):
    try:
        if value == ""DD-MM-YYYY"":
            return value
        day, month, year = value.split(""-"")
        if int(day) < 1 or int(day) > 31:
            raise DateStringValueError(config_param_name, value)
        if int(month) < 1 or int(month) > 12:
            raise DateStringValueError(config_param_name, value)
        if int(year) < 1900 or int(year) > 2013:
            raise DateStringValueError(config_param_name, value)
        return value
    except Exception:
        raise DateStringValueError(config_param_name, value)
","if value == ""DD-MM-YYYY"" :",187
"def from_obj(cls, py_obj):
    if not isinstance(py_obj, Image):
        raise TypeError(""py_obj must be a wandb.Image"")
    else:
        if hasattr(py_obj, ""_boxes"") and py_obj._boxes:
            box_keys = list(py_obj._boxes.keys())
        else:
            box_keys = []
        if hasattr(py_obj, ""masks"") and py_obj.masks:
            mask_keys = list(py_obj.masks.keys())
        else:
            mask_keys = []
        return cls(box_keys, mask_keys)
","if hasattr ( py_obj , ""_boxes"" ) and py_obj . _boxes :",164
"def _path_type(st, lst):
    parts = []
    if st:
        if stat.S_ISREG(st.st_mode):
            parts.append(""file"")
        elif stat.S_ISDIR(st.st_mode):
            parts.append(""dir"")
        else:
            parts.append(""other"")
    if lst:
        if stat.S_ISLNK(lst.st_mode):
            parts.append(""link"")
    return "" "".join(parts)
",elif stat . S_ISDIR ( st . st_mode ) :,130
"def is_destructive(queries):
    """"""Returns if any of the queries in *queries* is destructive.""""""
    keywords = (""drop"", ""shutdown"", ""delete"", ""truncate"", ""alter"")
    for query in sqlparse.split(queries):
        if query:
            if query_starts_with(query, keywords) is True:
                return True
            elif query_starts_with(
                query, [""update""]
            ) is True and not query_has_where_clause(query):
                return True
    return False
","if query_starts_with ( query , keywords ) is True :",136
"def _store_gsuite_membership_post(self):
    """"""Flush storing gsuite memberships.""""""
    if not self.member_cache:
        return
    self.session.flush()
    # session.execute automatically flushes
    if self.membership_items:
        if get_sql_dialect(self.session) == ""sqlite"":
            # SQLite doesn't support bulk insert
            for item in self.membership_items:
                stmt = self.dao.TBL_MEMBERSHIP.insert(item)
                self.session.execute(stmt)
        else:
            stmt = self.dao.TBL_MEMBERSHIP.insert(self.membership_items)
            self.session.execute(stmt)
","if get_sql_dialect ( self . session ) == ""sqlite"" :",182
"def forward(self, inputs: paddle.Tensor):
    outputs = []
    blocks = self.block(inputs)
    route = None
    for i, block in enumerate(blocks):
        if i > 0:
            block = paddle.concat([route, block], axis=1)
        route, tip = self.yolo_blocks[i](block)
        block_out = self.block_outputs[i](tip)
        outputs.append(block_out)
        if i < 2:
            route = self.route_blocks_2[i](route)
            route = self.upsample(route)
    return outputs
",if i > 0 :,163
"def deep_dict(self, root=None):
    if root is None:
        root = self
    result = {}
    for key, value in root.items():
        if isinstance(value, dict):
            result[key] = self.deep_dict(root=self.__class__._get_next(key, root))
        else:
            result[key] = value
    return result
","if isinstance ( value , dict ) :",99
"def _parse_param_list(self, content):
    r = Reader(content)
    params = []
    while not r.eof():
        header = r.read().strip()
        if "" : "" in header:
            arg_name, arg_type = header.split("" : "")[:2]
        else:
            arg_name, arg_type = header, """"
        desc = r.read_to_next_unindented_line()
        desc = dedent_lines(desc)
        params.append((arg_name, arg_type, desc))
    return params
","if "" : "" in header :",147
"def _ungroup(sequence, groups=None):
    for v in sequence:
        if isinstance(v, (list, tuple)):
            if groups is not None:
                groups.append(list(_ungroup(v, groups=None)))
            for v in _ungroup(v, groups):
                yield v
        else:
            yield v
","if isinstance ( v , ( list , tuple ) ) :",95
"def _add_resource_group(obj):
    if isinstance(obj, list):
        for array_item in obj:
            _add_resource_group(array_item)
    elif isinstance(obj, dict):
        try:
            if ""resourcegroup"" not in [x.lower() for x in obj.keys()]:
                if obj[""id""]:
                    obj[""resourceGroup""] = _parse_id(obj[""id""])[""resource-group""]
        except (KeyError, IndexError, TypeError):
            pass
        for item_key in obj:
            if item_key != ""sourceVault"":
                _add_resource_group(obj[item_key])
","if item_key != ""sourceVault"" :",175
"def haslayer(self, cls):
    """"""true if self has a layer that is an instance of cls. Superseded by ""cls in self"" syntax.""""""
    if self.__class__ == cls or self.__class__.__name__ == cls:
        return 1
    for f in self.packetfields:
        fvalue_gen = self.getfieldval(f.name)
        if fvalue_gen is None:
            continue
        if not f.islist:
            fvalue_gen = SetGen(fvalue_gen, _iterpacket=0)
        for fvalue in fvalue_gen:
            if isinstance(fvalue, Packet):
                ret = fvalue.haslayer(cls)
                if ret:
                    return ret
    return self.payload.haslayer(cls)
","if isinstance ( fvalue , Packet ) :",199
"def _post_attachment(self, message, channel, color, sub_fields=None):
    if channel is None:
        message_channels = self.channels
    else:
        message_channels = [channel]
    for message_channel in message_channels:
        attachment = {
            ""fallback"": message,
            ""text"": message,
            ""color"": color,
        }
        if sub_fields is not None:
            attachment[""fields""] = sub_fields
        self.slack_client.api_call(
            ""chat.postMessage"",
            channel=message_channel,
            attachments=[attachment],
            as_user=True,
        )
",if sub_fields is not None :,178
"def create(cls, repository, args):
    key = cls()
    passphrase = os.environ.get(""ATTIC_PASSPHRASE"")
    if passphrase is not None:
        passphrase2 = passphrase
    else:
        passphrase, passphrase2 = 1, 2
    while passphrase != passphrase2:
        passphrase = getpass(""Enter passphrase: "")
        if not passphrase:
            print(""Passphrase must not be blank"")
            continue
        passphrase2 = getpass(""Enter same passphrase again: "")
        if passphrase != passphrase2:
            print(""Passphrases do not match"")
    key.init(repository, passphrase)
    if passphrase:
        print(""Remember your passphrase. Your data will be inaccessible without it."")
    return key
",if not passphrase :,184
"def _generate_create_date(self):
    if self.timezone is not None:
        # First, assume correct capitalization
        tzinfo = tz.gettz(self.timezone)
        if tzinfo is None:
            # Fall back to uppercase
            tzinfo = tz.gettz(self.timezone.upper())
        if tzinfo is None:
            raise util.CommandError(""Can't locate timezone: %s"" % self.timezone)
        create_date = (
            datetime.datetime.utcnow().replace(tzinfo=tz.tzutc()).astimezone(tzinfo)
        )
    else:
        create_date = datetime.datetime.now()
    return create_date
",if tzinfo is None :,168
"def _read_header_lines(fp):
    """"""Read lines with headers until the start of body""""""
    lines = deque()
    for line in fp:
        if is_empty(line):
            break
        # tricky case if it's not a header and not an empty line
        # usually means that user forgot to separate the body and newlines
        # so ""unread"" this line here, what means to treat it like a body
        if not _RE_HEADER.match(line):
            fp.seek(fp.tell() - len(line))
            break
        lines.append(line)
    return lines
",if not _RE_HEADER . match ( line ) :,153
"def _media_files_drag_received(widget, context, x, y, data, info, timestamp):
    uris = data.get_uris()
    files = []
    for uri in uris:
        try:
            uri_tuple = GLib.filename_from_uri(uri)
        except:
            continue
        uri, unused = uri_tuple
        if os.path.exists(uri) == True:
            if utils.is_media_file(uri) == True:
                files.append(uri)
    if len(files) == 0:
        return
    open_dropped_files(files)
",if os . path . exists ( uri ) == True :,159
"def remove_importlib(frame, options):
    if frame is None:
        return None
    for child in frame.children:
        remove_importlib(child, options=options)
        if ""<frozen importlib._bootstrap"" in child.file_path:
            # remove this node, moving the self_time and children up to the parent
            frame.self_time += child.self_time
            frame.add_children(child.children, after=child)
            child.remove_from_parent()
    return frame
","if ""<frozen importlib._bootstrap"" in child . file_path :",132
"def __call__(self, graph):
    for layer_name, data in self.params:
        if layer_name in graph:
            node = graph.get_node(layer_name)
            node.data = self.adjust_parameters(node, data)
        else:
            print_stderr(""Ignoring parameters for non-existent layer: %s"" % layer_name)
    return graph
",if layer_name in graph :,99
"def test_with_three_points(self):
    cba = ia.Polygon([(1, 2), (3, 4), (5, 5)])
    for i, xy in enumerate(cba):
        assert i in [0, 1, 2]
        if i == 0:
            assert np.allclose(xy, (1, 2))
        elif i == 1:
            assert np.allclose(xy, (3, 4))
        elif i == 2:
            assert np.allclose(xy, (5, 5))
    assert i == 2
",elif i == 1 :,136
"def _serve(self):
    self._conn = self.manager.request(REQUEST_DNS_LISTENER, self.domain)
    conn = MsgPackMessages(self._conn)
    while self.active:
        request = conn.recv()
        if not request:
            logger.warning(""DNS: Recieved empty request. Shutdown"")
            self.stop()
            break
        now = time.time()
        response = self.handler.process(request)
        if not response:
            response = []
        used = time.time() - now
        if used > 1:
            logger.warning(""DNS: Slow processing speed (%s)s"", used)
        conn.send(response)
",if used > 1 :,180
"def read(cls, fp, **kwargs):
    major_version, minor_version, count = read_fmt(""2HI"", fp)
    items = []
    for _ in range(count):
        length = read_fmt(""I"", fp)[0] - 4
        if length > 0:
            with io.BytesIO(fp.read(length)) as f:
                items.append(Annotation.read(f))
    return cls(major_version=major_version, minor_version=minor_version, items=items)
",if length > 0 :,129
"def save_uploaded_files():
    files = []
    unzip = bool(request.form.get(""unzip"") in [""true"", ""on""])
    for uploaded_file in request.files.getlist(""files""):
        if unzip and zipfile.is_zipfile(uploaded_file):
            with zipfile.ZipFile(uploaded_file, ""r"") as zf:
                for info in zf.infolist():
                    name = info.filename
                    size = info.file_size
                    data = zf.read(name)
                    if size > 0:
                        files.append(save_file(data, filename=name.split(""/"")[-1]))
        else:
            files.append(save_file(uploaded_file))
    return files
",if unzip and zipfile . is_zipfile ( uploaded_file ) :,195
"def analyze_string_content(self, string, line_num, filename):
    output = {}
    if self.keyword_exclude and self.keyword_exclude.search(string):
        return output
    for identifier in self.secret_generator(
        string,
        filetype=determine_file_type(filename),
    ):
        if self.is_secret_false_positive(identifier):
            continue
        secret = PotentialSecret(
            self.secret_type,
            filename,
            identifier,
            line_num,
        )
        output[secret] = secret
    return output
",if self . is_secret_false_positive ( identifier ) :,157
"def _validate_and_set_default_hyperparameters(self):
    """"""Placeholder docstring""""""
    # Check if all the required hyperparameters are set. If there is a default value
    # for one, set it.
    for name, definition in self.hyperparameter_definitions.items():
        if name not in self.hyperparam_dict:
            spec = definition[""spec""]
            if ""DefaultValue"" in spec:
                self.hyperparam_dict[name] = spec[""DefaultValue""]
            elif ""IsRequired"" in spec and spec[""IsRequired""]:
                raise ValueError(""Required hyperparameter: %s is not set"" % name)
","elif ""IsRequired"" in spec and spec [ ""IsRequired"" ] :",158
"def get_code(self, fullname=None):
    fullname = self._fix_name(fullname)
    if self.code is None:
        mod_type = self.etc[2]
        if mod_type == imp.PY_SOURCE:
            source = self.get_source(fullname)
            self.code = compile(source, self.filename, ""exec"")
        elif mod_type == imp.PY_COMPILED:
            self._reopen()
            try:
                self.code = read_code(self.file)
            finally:
                self.file.close()
        elif mod_type == imp.PKG_DIRECTORY:
            self.code = self._get_delegate().get_code()
    return self.code
",elif mod_type == imp . PKG_DIRECTORY :,196
"def eigh_abstract_eval(operand, lower):
    if isinstance(operand, ShapedArray):
        if operand.ndim < 2 or operand.shape[-2] != operand.shape[-1]:
            raise ValueError(
                ""Argument to symmetric eigendecomposition must have shape [..., n, n],""
                ""got shape {}"".format(operand.shape)
            )
        batch_dims = operand.shape[:-2]
        n = operand.shape[-1]
        v = ShapedArray(batch_dims + (n, n), operand.dtype)
        w = ShapedArray(batch_dims + (n,), lax.lax._complex_basetype(operand.dtype))
    else:
        v, w = operand, operand
    return v, w
",if operand . ndim < 2 or operand . shape [ - 2 ] != operand . shape [ - 1 ] :,191
"def conninfo_parse(dsn):
    ret = {}
    length = len(dsn)
    i = 0
    while i < length:
        if dsn[i].isspace():
            i += 1
            continue
        param_match = PARAMETER_RE.match(dsn[i:])
        if not param_match:
            return
        param = param_match.group(1)
        i += param_match.end()
        if i >= length:
            return
        value, end = read_param_value(dsn[i:])
        if value is None:
            return
        i += end
        ret[param] = value
    return ret
",if not param_match :,175
"def load_weights_from_unsupervised(self, unsupervised_model):
    update_state_dict = copy.deepcopy(self.network.state_dict())
    for param, weights in unsupervised_model.network.state_dict().items():
        if param.startswith(""encoder""):
            # Convert encoder's layers name to match
            new_param = ""tabnet."" + param
        else:
            new_param = param
        if self.network.state_dict().get(new_param) is not None:
            # update only common layers
            update_state_dict[new_param] = weights
    self.network.load_state_dict(update_state_dict)
",if self . network . state_dict ( ) . get ( new_param ) is not None :,170
"def viewer_setup(self):
    for key, value in DEFAULT_CAMERA_CONFIG.items():
        if isinstance(value, np.ndarray):
            getattr(self.viewer.cam, key)[:] = value
        else:
            setattr(self.viewer.cam, key, value)
","if isinstance ( value , np . ndarray ) :",75
"def colormap_changed(change):
    if change[""new""]:
        cmap_colors = [
            color[1:] for color in cmap.step.__dict__[""_schemes""][colormap.value]
        ]
        palette.value = "", "".join(cmap_colors)
        colorbar = getattr(cmap.step, colormap.value)
        colorbar_output = self.colorbar_widget
        with colorbar_output:
            colorbar_output.clear_output()
            display(colorbar)
        if len(palette.value) > 0 and "","" in palette.value:
            labels = [f""Class {i+1}"" for i in range(len(palette.value.split("","")))]
            legend_labels.value = "", "".join(labels)
","if len ( palette . value ) > 0 and "","" in palette . value :",185
"def invalidate(self, layers=None):
    if layers is None:
        layers = Layer.AllLayers
    if layers:
        layers = set(layers)
        self.invalidLayers.update(layers)
        blockRenderers = [
            br
            for br in self.blockRenderers
            if br.layer is Layer.Blocks or br.layer not in layers
        ]
        if len(blockRenderers) < len(self.blockRenderers):
            self.forgetDisplayLists()
        self.blockRenderers = blockRenderers
        if self.renderer.showRedraw and Layer.Blocks in layers:
            self.needsRedisplay = True
",if len ( blockRenderers ) < len ( self . blockRenderers ) :,184
"def fromstring(cls, input):
    productions = []
    for linenum, line in enumerate(input.split(""\n"")):
        line = line.strip()
        if line.startswith(""#"") or line == """":
            continue
        try:
            productions += _read_dependency_production(line)
        except ValueError:
            raise ValueError(""Unable to parse line %s: %s"" % (linenum, line))
    if len(productions) == 0:
        raise ValueError(""No productions found!"")
    return DependencyGrammar(productions)
","if line . startswith ( ""#"" ) or line == """" :",130
"def repl(m, base_path, rel_path=None):
    if m.group(""comments""):
        tag = m.group(""comments"")
    else:
        tag = m.group(""open"")
        if rel_path is None:
            tag += RE_TAG_LINK_ATTR.sub(
                lambda m2: repl_absolute(m2, base_path), m.group(""attr"")
            )
        else:
            tag += RE_TAG_LINK_ATTR.sub(
                lambda m2: repl_relative(m2, base_path, rel_path), m.group(""attr"")
            )
        tag += m.group(""close"")
    return tag
",if rel_path is None :,179
"def encode(path):
    if isinstance(path, str_cls):
        try:
            path = path.encode(fs_encoding, ""strict"")
        except UnicodeEncodeError:
            if not platform.is_linux():
                raise
            path = path.encode(fs_fallback_encoding, ""strict"")
    return path
",if not platform . is_linux ( ) :,86
"def __iter__(self):
    base_iterator = super(ProcessIterable, self).__iter__()
    if getattr(self.queryset, ""_coerced"", False):
        for process in base_iterator:
            if isinstance(process, self.queryset.model):
                process = coerce_to_related_instance(
                    process, process.flow_class.process_class
                )
            yield process
    else:
        for process in base_iterator:
            yield process
","if isinstance ( process , self . queryset . model ) :",125
"def footnotes_under(n: Element) -> Iterator[nodes.footnote]:
    if isinstance(n, nodes.footnote):
        yield n
    else:
        for c in n.children:
            if isinstance(c, addnodes.start_of_file):
                continue
            elif isinstance(c, nodes.Element):
                yield from footnotes_under(c)
","if isinstance ( c , addnodes . start_of_file ) :",99
"def _process_submissions(self) -> None:
    """"""Process all submissions which have not been processed yet.""""""
    while self._to_be_processed:
        job = self._to_be_processed[0]
        job.process()  # trigger computation
        if not self.batch_mode:
            heapq.heappush(
                self._steady_priority_queue,
                OrderedJobs(job.release_time, self._order, job),
            )
        self._to_be_processed.popleft()  # remove right after it is added to the heap queue
        self._order += 1
",if not self . batch_mode :,156
"def valid_localparts(strip_delimiters=False):
    for line in ABRIDGED_LOCALPART_VALID_TESTS.split(""\n""):
        # strip line, skip over empty lines
        line = line.strip()
        if line == """":
            continue
        # skip over comments or empty lines
        match = COMMENT.match(line)
        if match:
            continue
        # skip over localparts with delimiters
        if strip_delimiters:
            if "","" in line or "";"" in line:
                continue
        yield line
",if match :,145
"def _get_payload_hash(self, method, data=None):
    if method in (""POST"", ""PUT""):
        if data:
            if hasattr(data, ""next"") or hasattr(data, ""__next__""):
                # File upload; don't try to read the entire payload
                return UNSIGNED_PAYLOAD
            return _hash(data)
        else:
            return UNSIGNED_PAYLOAD
    else:
        return _hash("""")
","if hasattr ( data , ""next"" ) or hasattr ( data , ""__next__"" ) :",118
"def get_download_info(self):
    try:
        download_info = self.api.get_download_info(self.game)
        result = True
    except NoDownloadLinkFound as e:
        print(e)
        if Config.get(""current_download"") == self.game.id:
            Config.unset(""current_download"")
        GLib.idle_add(
            self.parent.parent.show_error,
            _(""Download error""),
            _(
                ""There was an error when trying to fetch the download link!\n{}"".format(
                    e
                )
            ),
        )
        download_info = False
        result = False
    return result, download_info
","if Config . get ( ""current_download"" ) == self . game . id :",191
"def find_id(self, doc_id):
    self._lock.acquire()
    try:
        doc = self._docs.get(doc_id)
        if doc:
            doc = copy.deepcopy(doc)
            doc[""id""] = doc_id
            return doc
    finally:
        self._lock.release()
",if doc :,88
"def assign_art(self, session, task):
    """"""Place the discovered art in the filesystem.""""""
    if task in self.art_candidates:
        candidate = self.art_candidates.pop(task)
        self._set_art(task.album, candidate, not self.src_removed)
        if self.src_removed:
            task.prune(candidate.path)
",if self . src_removed :,93
"def _replace_named(self, named, replace_scalar):
    for item in named:
        for name, value in self._get_replaced_named(item, replace_scalar):
            if not is_string(name):
                raise DataError(""Argument names must be strings."")
            yield name, value
",if not is_string ( name ) :,79
"def qtTypeIdent(conn, *args):
    # We're not using the conn object at the moment, but - we will
    # modify the
    # logic to use the server version specific keywords later.
    res = None
    value = None
    for val in args:
        # DataType doesn't have len function then convert it to string
        if not hasattr(val, ""__len__""):
            val = str(val)
        if len(val) == 0:
            continue
        value = val
        if Driver.needsQuoting(val, True):
            value = value.replace('""', '""""')
            value = '""' + value + '""'
        res = ((res and res + ""."") or """") + value
    return res
","if Driver . needsQuoting ( val , True ) :",181
"def _update_tileable_and_chunk_shape(self, tileable_graph, chunk_result, failed_ops):
    for n in tileable_graph:
        if n.op in failed_ops:
            continue
        tiled_n = get_tiled(n)
        if has_unknown_shape(tiled_n):
            if any(c.key not in chunk_result for c in tiled_n.chunks):
                # some of the chunks has been fused
                continue
            new_nsplits = self.get_tileable_nsplits(n, chunk_result=chunk_result)
            for node in (n, tiled_n):
                node._update_shape(tuple(sum(nsplit) for nsplit in new_nsplits))
            tiled_n._nsplits = new_nsplits
",if n . op in failed_ops :,200
"def _read_filter(self, data):
    if data:
        if self.expected_inner_sha256:
            self.inner_sha.update(data)
        if self.expected_inner_md5sum:
            self.inner_md5.update(data)
    return data
",if self . expected_inner_sha256 :,76
"def find_previous_editable(self, *args):
    if self.editw == 0:
        if self._active_page > 0:
            self.switch_page(self._active_page - 1)
    if not self.editw == 0:
        # remember that xrange does not return the 'last' value,
        # so go to -1, not 0! (fence post error in reverse)
        for n in range(self.editw - 1, -1, -1):
            if self._widgets__[n].editable and not self._widgets__[n].hidden:
                self.editw = n
                break
",if self . _widgets__ [ n ] . editable and not self . _widgets__ [ n ] . hidden :,161
"def _get_event_for_message(self, message_id):
    with self.event_lock:
        if message_id not in self._events:
            raise RuntimeError(
                ""Event for message[{}] should have been created before accessing"".format(
                    message_id
                )
            )
        return self._events[message_id]
",if message_id not in self . _events :,97
"def _get_deepest(self, t):
    if isinstance(t, list):
        if len(t) == 1:
            return t[0]
        else:
            for part in t:
                res = self._get_deepest(part)
                if res:
                    return res
            return None
    return None
",if res :,95
"def _get_notify(self, action_node):
    if action_node.name not in self._skip_notify_tasks:
        if action_node.notify:
            task_notify = NotificationsHelper.to_model(action_node.notify)
            return task_notify
        elif self._chain_notify:
            return self._chain_notify
    return None
",if action_node . notify :,95
"def __init__(self, centered=None, shape_params=()):
    assert centered is None or isinstance(centered, (float, torch.Tensor))
    assert isinstance(shape_params, (tuple, list))
    assert all(isinstance(name, str) for name in shape_params)
    if is_validation_enabled():
        if isinstance(centered, float):
            assert 0 <= centered and centered <= 1
        elif isinstance(centered, torch.Tensor):
            assert (0 <= centered).all()
            assert (centered <= 1).all()
        else:
            assert centered is None
    self.centered = centered
    self.shape_params = shape_params
","elif isinstance ( centered , torch . Tensor ) :",163
"def collect(self):
    for nickname in self.squid_hosts.keys():
        squid_host = self.squid_hosts[nickname]
        fulldata = self._getData(squid_host[""host""], squid_host[""port""])
        if fulldata is not None:
            fulldata = fulldata.splitlines()
            for data in fulldata:
                matches = self.stat_pattern.match(data)
                if matches:
                    self.publish_counter(
                        ""%s.%s"" % (nickname, matches.group(1)), float(matches.group(2))
                    )
",if matches :,166
"def test_len(self):
    eq = self.assertEqual
    eq(base64MIME.base64_len(""hello""), len(base64MIME.encode(""hello"", eol="""")))
    for size in range(15):
        if size == 0:
            bsize = 0
        elif size <= 3:
            bsize = 4
        elif size <= 6:
            bsize = 8
        elif size <= 9:
            bsize = 12
        elif size <= 12:
            bsize = 16
        else:
            bsize = 20
        eq(base64MIME.base64_len(""x"" * size), bsize)
",elif size <= 9 :,160
"def wait_for_initial_conf(self, timeout=1.0):
    logger.info(""Waiting for initial configuration"")
    cur_timeout = timeout
    # Arbiter do not already set our have_conf param
    while not self.new_conf and not self.interrupted:
        elapsed, _, _ = self.handleRequests(cur_timeout)
        if elapsed:
            cur_timeout -= elapsed
            if cur_timeout > 0:
                continue
            cur_timeout = timeout
        sys.stdout.write(""."")
        sys.stdout.flush()
",if cur_timeout > 0 :,142
"def __init__(self, querylist=None):
    self.query_id = -1
    if querylist is None:
        self.querylist = []
    else:
        self.querylist = querylist
        for query in self.querylist:
            if self.query_id == -1:
                self.query_id = query.query_id
            else:
                if self.query_id != query.query_id:
                    raise ValueError(""query in list must be same query_id"")
",if self . query_id != query . query_id :,137
"def candidates() -> Generator[""Symbol"", None, None]:
    s = self
    if Symbol.debug_lookup:
        Symbol.debug_print(""searching in self:"")
        print(s.to_string(Symbol.debug_indent + 1), end="""")
    while True:
        if matchSelf:
            yield s
        if recurseInAnon:
            yield from s.children_recurse_anon
        else:
            yield from s._children
        if s.siblingAbove is None:
            break
        s = s.siblingAbove
        if Symbol.debug_lookup:
            Symbol.debug_print(""searching in sibling:"")
            print(s.to_string(Symbol.debug_indent + 1), end="""")
",if recurseInAnon :,190
"def get_default_params(problem_type: str, penalty: str):
    # TODO: get seed from seeds provider
    if problem_type == REGRESSION:
        default_params = {""C"": None, ""random_state"": 0, ""fit_intercept"": True}
        if penalty == L2:
            default_params[""solver""] = ""auto""
    else:
        default_params = {
            ""C"": None,
            ""random_state"": 0,
            ""solver"": _get_solver(problem_type),
            ""n_jobs"": -1,
            ""fit_intercept"": True,
        }
    model_params = list(default_params.keys())
    return model_params, default_params
",if penalty == L2 :,187
"def _UploadDirectory(local_dir: str, gcs_bucket: storage.Bucket, gcs_dir: str):
    """"""Upload the contents of a local directory to a GCS Bucket.""""""
    for file_name in os.listdir(local_dir):
        path = os.path.join(local_dir, file_name)
        if not os.path.isfile(path):
            logging.info(""Skipping %s as it's not a file."", path)
            continue
        logging.info(""Uploading: %s"", path)
        gcs_blob = gcs_bucket.blob(f""{gcs_dir}/{file_name}"")
        gcs_blob.upload_from_filename(path)
",if not os . path . isfile ( path ) :,173
"def decode_query_ids(self, trans, conditional):
    if conditional.operator == ""and"":
        self.decode_query_ids(trans, conditional.left)
        self.decode_query_ids(trans, conditional.right)
    else:
        left_base = conditional.left.split(""."")[0]
        if left_base in self.FIELDS:
            field = self.FIELDS[left_base]
            if field.id_decode:
                conditional.right = trans.security.decode_id(conditional.right)
",if field . id_decode :,135
"def data_dir(self) -> Path:
    try:
        from appdirs import user_data_dir
    except ImportError:
        # linux
        path = Path.home() / "".local"" / ""share""
        if path.exists():
            return path / ""dephell""
        # mac os
        path = Path.home() / ""Library"" / ""Application Support""
        if path.exists():
            return path / ""dephell""
        self.pip_main([""install"", ""appdirs""])
        from appdirs import user_data_dir
    return Path(user_data_dir(""dephell""))
",if path . exists ( ) :,157
"def setGameCard(self, isGameCard=False):
    if isGameCard:
        targetValue = 1
    else:
        targetValue = 0
    for nca in self:
        if isinstance(nca, Nca):
            if nca.header.getIsGameCard() == targetValue:
                continue
            Print.info(""writing isGameCard for %s, %d"" % (str(nca._path), targetValue))
            nca.header.setIsGameCard(targetValue)
",if nca . header . getIsGameCard ( ) == targetValue :,132
"def check_apns_certificate(ss):
    mode = ""start""
    for s in ss.split(""\n""):
        if mode == ""start"":
            if ""BEGIN RSA PRIVATE KEY"" in s or ""BEGIN PRIVATE KEY"" in s:
                mode = ""key""
        elif mode == ""key"":
            if ""END RSA PRIVATE KEY"" in s or ""END PRIVATE KEY"" in s:
                mode = ""end""
                break
            elif s.startswith(""Proc-Type"") and ""ENCRYPTED"" in s:
                raise ImproperlyConfigured(
                    ""Encrypted APNS private keys are not supported""
                )
    if mode != ""end"":
        raise ImproperlyConfigured(""The APNS certificate doesn't contain a private key"")
","elif mode == ""key"" :",195
"def register_aggregate_groups(conn, *groups):
    seen = set()
    for group in groups:
        klasses = AGGREGATE_COLLECTION[group]
        for klass in klasses:
            name = getattr(klass, ""name"", klass.__name__)
            if name not in seen:
                seen.add(name)
                conn.create_aggregate(name, -1, klass)
",if name not in seen :,106
"def _impl(inputs, input_types):
    data = inputs[0]
    axis = None
    keepdims = False
    if len(inputs) > 2:  # default, torch have only data, axis=None, keepdims=False
        if isinstance(inputs[1], int):
            axis = int(inputs[1])
        elif _is_int_seq(inputs[1]):
            axis = inputs[1]
        else:
            axis = list(_infer_shape(inputs[1]))
        keepdims = bool(inputs[2])
    return get_relay_op(name)(data, axis=axis, keepdims=keepdims)
",elif _is_int_seq ( inputs [ 1 ] ) :,158
"def walks_generator():
    if filelist is not None:
        bucket = []
        for filename in filelist:
            with io.open(filename) as inf:
                for line in inf:
                    walk = [int(x) for x in line.strip(""\n"").split("" "")]
                    bucket.append(walk)
                    if len(bucket) == batch_size:
                        yield bucket
                        bucket = []
        if len(bucket):
            yield bucket
    else:
        for _ in range(epoch):
            for nodes in graph.node_batch_iter(batch_size):
                walks = graph.random_walk(nodes, walk_len)
                yield walks
",if len ( bucket ) :,198
"def _calculate_runtimes(states):
    results = {""runtime"": 0.00, ""num_failed_states"": 0, ""num_passed_states"": 0}
    for state, resultset in states.items():
        if isinstance(resultset, dict) and ""duration"" in resultset:
            # Count the pass vs failures
            if resultset[""result""]:
                results[""num_passed_states""] += 1
            else:
                results[""num_failed_states""] += 1
            # Count durations
            results[""runtime""] += resultset[""duration""]
    log.debug(""Parsed state metrics: {}"".format(results))
    return results
","if isinstance ( resultset , dict ) and ""duration"" in resultset :",167
"def _replicator_primary_device() -> snt_replicator.Replicator:
    # NOTE: The explicit device list is required since currently Replicator
    # only considers CPU and GPU devices. This means on TPU by default we only
    # mirror on the local CPU.
    for device_type in (""TPU"", ""GPU"", ""CPU""):
        devices = tf.config.experimental.list_logical_devices(device_type=device_type)
        if devices:
            devices = [d.name for d in devices]
            logging.info(""Replicating over %s"", devices)
            return snt_replicator.Replicator(devices=devices)
    assert False, ""No TPU/GPU or CPU found""
",if devices :,180
"def get_tag_values(self, event):
    http = event.interfaces.get(""sentry.interfaces.Http"")
    if not http:
        return []
    if not http.headers:
        return []
    headers = http.headers
    # XXX: transitional support for workers
    if isinstance(headers, dict):
        headers = headers.items()
    output = []
    for key, value in headers:
        if key != ""User-Agent"":
            continue
        ua = Parse(value)
        if not ua:
            continue
        result = self.get_tag_from_ua(ua)
        if result:
            output.append(result)
    return output
","if key != ""User-Agent"" :",176
"def general(metadata, value):
    if metadata.get(""commands"") and value:
        if not metadata.get(""nargs""):
            v = quote(value)
        else:
            v = value
        return u""{0} {1}"".format(metadata[""commands""][0], v)
    else:
        if not value:
            return None
        elif not metadata.get(""nargs""):
            return quote(value)
        else:
            return value
","if not metadata . get ( ""nargs"" ) :",122
"def _actions_read(self, c):
    self.action_input.handle_read(c)
    if c in [curses.KEY_ENTER, util.KEY_ENTER2]:
        # take action
        if self.action_input.selected_index == 0:  # Cancel
            self.back_to_parent()
        elif self.action_input.selected_index == 1:  # Apply
            self._apply_prefs()
            client.core.get_config().addCallback(self._update_preferences)
        elif self.action_input.selected_index == 2:  # OK
            self._apply_prefs()
            self.back_to_parent()
",elif self . action_input . selected_index == 2 :,174
"def logic():
    if reset == 1:
        lfsr.next = 1
    else:
        if enable:
            # lfsr.next[24:1] = lfsr[23:0]
            lfsr.next = lfsr << 1
            lfsr.next[0] = lfsr[23] ^ lfsr[22] ^ lfsr[21] ^ lfsr[16]
",if enable :,110
"def action_delete(self, request, attachments):
    deleted_attachments = []
    desynced_posts = []
    for attachment in attachments:
        if attachment.post:
            deleted_attachments.append(attachment.pk)
            desynced_posts.append(attachment.post_id)
    if desynced_posts:
        with transaction.atomic():
            for post in Post.objects.filter(id__in=desynced_posts):
                self.delete_from_cache(post, deleted_attachments)
    for attachment in attachments:
        attachment.delete()
    message = _(""Selected attachments have been deleted."")
    messages.success(request, message)
",if attachment . post :,165
"def __getitem__(self, index):
    if self._check():
        if isinstance(index, int):
            if index < 0 or index >= len(self.features):
                raise IndexError(index)
            if self.features[index] is None:
                feature = self.device.feature_request(FEATURE.FEATURE_SET, 0x10, index)
                if feature:
                    (feature,) = _unpack(""!H"", feature[:2])
                    self.features[index] = FEATURE[feature]
            return self.features[index]
        elif isinstance(index, slice):
            indices = index.indices(len(self.features))
            return [self.__getitem__(i) for i in range(*indices)]
","elif isinstance ( index , slice ) :",195
"def _skip_start(self):
    start, stop = self.start, self.stop
    for chunk in self.app_iter:
        self._pos += len(chunk)
        if self._pos < start:
            continue
        elif self._pos == start:
            return b""""
        else:
            chunk = chunk[start - self._pos :]
            if stop is not None and self._pos > stop:
                chunk = chunk[: stop - self._pos]
                assert len(chunk) == stop - start
            return chunk
    else:
        raise StopIteration()
",if self . _pos < start :,156
"def get_files(d):
    f = []
    for root, dirs, files in os.walk(d):
        for name in files:
            if ""meta-environment"" in root or ""cross-canadian"" in root:
                continue
            if ""qemux86copy-"" in root or ""qemux86-"" in root:
                continue
            if ""do_build"" not in name and ""do_populate_sdk"" not in name:
                f.append(os.path.join(root, name))
    return f
","if ""meta-environment"" in root or ""cross-canadian"" in root :",143
"def _load_windows_store_certs(self, storename, purpose):
    certs = bytearray()
    try:
        for cert, encoding, trust in enum_certificates(storename):
            # CA certs are never PKCS#7 encoded
            if encoding == ""x509_asn"":
                if trust is True or purpose.oid in trust:
                    certs.extend(cert)
    except PermissionError:
        warnings.warn(""unable to enumerate Windows certificate store"")
    if certs:
        self.load_verify_locations(cadata=certs)
    return certs
","if encoding == ""x509_asn"" :",145
"def test_tokenizer_identifier_with_correct_config(self):
    for tokenizer_class in [BertTokenizer, BertTokenizerFast, AutoTokenizer]:
        tokenizer = tokenizer_class.from_pretrained(""wietsedv/bert-base-dutch-cased"")
        self.assertIsInstance(tokenizer, (BertTokenizer, BertTokenizerFast))
        if isinstance(tokenizer, BertTokenizer):
            self.assertEqual(tokenizer.basic_tokenizer.do_lower_case, False)
        else:
            self.assertEqual(tokenizer.do_lower_case, False)
        self.assertEqual(tokenizer.model_max_length, 512)
","if isinstance ( tokenizer , BertTokenizer ) :",146
"def run(self):
    global WAITING_BEFORE_START
    time.sleep(WAITING_BEFORE_START)
    while self.keep_alive:
        path_id, module, resolve = self.queue_receive.get()
        if path_id is None:
            continue
        self.lock.acquire()
        self.modules[path_id] = module
        self.lock.release()
        if resolve:
            resolution = self._resolve_with_other_modules(resolve)
            self._relations[path_id] = []
            for package in resolution:
                self._relations[path_id].append(resolution[package])
            self.queue_send.put((path_id, module, False, resolution))
",if resolve :,190
"def __new__(mcs, name, bases, attrs):
    include_profile = include_trace = include_garbage = True
    bases = list(bases)
    if name == ""SaltLoggingClass"":
        for base in bases:
            if hasattr(base, ""trace""):
                include_trace = False
            if hasattr(base, ""garbage""):
                include_garbage = False
    if include_profile:
        bases.append(LoggingProfileMixin)
    if include_trace:
        bases.append(LoggingTraceMixin)
    if include_garbage:
        bases.append(LoggingGarbageMixin)
    return super(LoggingMixinMeta, mcs).__new__(mcs, name, tuple(bases), attrs)
","if hasattr ( base , ""garbage"" ) :",176
"def __str__(self, prefix="""", printElemNumber=0):
    res = """"
    if self.has_owner_:
        res += prefix + (""owner: %s\n"" % self.DebugFormatString(self.owner_))
    cnt = 0
    for e in self.entries_:
        elm = """"
        if printElemNumber:
            elm = ""(%d)"" % cnt
        res += prefix + (""entries%s <\n"" % elm)
        res += e.__str__(prefix + ""  "", printElemNumber)
        res += prefix + "">\n""
        cnt += 1
    return res
",if printElemNumber :,154
"def parse_tag(self):
    buf = []
    escaped = False
    for c in self.get_next_chars():
        if escaped:
            buf.append(c)
        elif c == ""\\"":
            escaped = True
        elif c == "">"":
            return """".join(buf)
        else:
            buf.append(c)
    raise Exception(""Unclosed tag "" + """".join(buf))
","elif c == "">"" :",110
"def get_batches(train_nodes, train_labels, batch_size=64, shuffle=True):
    if shuffle:
        random.shuffle(train_nodes)
    total = train_nodes.shape[0]
    for i in range(0, total, batch_size):
        if i + batch_size <= total:
            cur_nodes = train_nodes[i : i + batch_size]
            cur_labels = train_labels[cur_nodes]
            yield cur_nodes, cur_labels
",if i + batch_size <= total :,127
"def _get_all_info_lines(data):
    infos = []
    for row in data:
        splitrow = row.split()
        if len(splitrow) > 0:
            if splitrow[0] == ""INFO:"":
                infos.append("" "".join(splitrow[1:]))
    return infos
","if splitrow [ 0 ] == ""INFO:"" :",82
"def _validate_client_public_key(self, username, key_data):
    """"""Validate a client public key for the specified user""""""
    try:
        key = decode_ssh_public_key(key_data)
    except KeyImportError:
        return None
    options = None
    if self._client_keys:
        options = self._client_keys.validate(key, self._peer_addr)
    if options is None:
        result = self._owner.validate_public_key(username, key)
        if asyncio.iscoroutine(result):
            result = yield from result
        if not result:
            return None
        options = {}
    self._key_options = options
    return key
",if not result :,177
"def attach_related_versions(addons, addon_dict=None):
    if addon_dict is None:
        addon_dict = {addon.id: addon for addon in addons}
    all_ids = set(filter(None, (addon._current_version_id for addon in addons)))
    versions = list(Version.objects.filter(id__in=all_ids).order_by())
    for version in versions:
        try:
            addon = addon_dict[version.addon_id]
        except KeyError:
            log.info(""Version %s has an invalid add-on id."" % version.id)
            continue
        if addon._current_version_id == version.id:
            addon._current_version = version
        version.addon = addon
",if addon . _current_version_id == version . id :,200
"def move_view(obj, evt):
    position = obj.GetCurrentCursorPosition()
    for other_axis, axis_number in self._axis_names.iteritems():
        if other_axis == axis_name:
            continue
        ipw3d = getattr(self, ""ipw_3d_%s"" % other_axis)
        ipw3d.ipw.slice_position = position[axis_number]
",if other_axis == axis_name :,104
"def func_wrapper(*args, **kwargs):
    warnings.simplefilter(""always"", DeprecationWarning)  # turn off filter
    for old, new in arg_mapping.items():
        if old in kwargs:
            warnings.warn(
                f""Keyword argument '{old}' has been ""
                f""deprecated in favour of '{new}'. ""
                f""'{old}' will be removed in a future version."",
                category=DeprecationWarning,
                stacklevel=2,
            )
            val = kwargs.pop(old)
            kwargs[new] = val
    # reset filter
    warnings.simplefilter(""default"", DeprecationWarning)
    return func(*args, **kwargs)
",if old in kwargs :,173
"def inner_connection_checker(self, *args, **kwargs):
    LOG.debug(""in _connection_checker"")
    for attempts in range(5):
        try:
            return func(self, *args, **kwargs)
        except exception.VolumeBackendAPIException as e:
            pattern = re.compile(r"".*Session id expired$"")
            matches = pattern.match(six.text_type(e))
            if matches:
                if attempts < 4:
                    LOG.debug(""Session might have expired."" "" Trying to relogin"")
                    self._login()
                    continue
            LOG.error(""Re-throwing Exception %s"", e)
            raise
",if attempts < 4 :,182
"def set(self, pcount):
    """"""Set channel prefetch_count setting.""""""
    if pcount != self.prev:
        new_value = pcount
        if pcount > PREFETCH_COUNT_MAX:
            logger.warning(
                ""QoS: Disabled: prefetch_count exceeds %r"", PREFETCH_COUNT_MAX
            )
            new_value = 0
        logger.debug(""basic.qos: prefetch_count->%s"", new_value)
        self.callback(prefetch_count=new_value)
        self.prev = pcount
    return pcount
",if pcount > PREFETCH_COUNT_MAX :,146
"def _build_gcs_object_key(self, key):
    if self.platform_specific_separator:
        if self.prefix:
            gcs_object_key = os.path.join(
                self.prefix, self._convert_key_to_filepath(key)
            )
        else:
            gcs_object_key = self._convert_key_to_filepath(key)
    else:
        if self.prefix:
            gcs_object_key = ""/"".join((self.prefix, self._convert_key_to_filepath(key)))
        else:
            gcs_object_key = self._convert_key_to_filepath(key)
    return gcs_object_key
",if self . prefix :,185
"def number_operators(self, a, b, skip=[]):
    dict = {""a"": a, ""b"": b}
    for name, expr in self.binops.items():
        if name not in skip:
            name = ""__%s__"" % name
            if hasattr(a, name):
                res = eval(expr, dict)
                self.binop_test(a, b, res, expr, name)
    for name, expr in self.unops.items():
        if name not in skip:
            name = ""__%s__"" % name
            if hasattr(a, name):
                res = eval(expr, dict)
                self.unop_test(a, res, expr, name)
",if name not in skip :,187
"def isCurveMonotonic(set_):
    for i in range(len(set_) - 1):
        # ==== added by zli =======
        if set_[i][0] >= set_[i + 1][0]:
            return False
        # ==== added by zli =======
        # ==== added by zli =======
        # if set_[i][1] > set_[i + 1][1]:
        if set_[i][1] >= set_[i + 1][1]:
            # ==== added by zli =======
            return False
    return True
",if set_ [ i ] [ 0 ] >= set_ [ i + 1 ] [ 0 ] :,141
"def show_topics():
    """"""prints all available miscellaneous help topics.""""""
    print(_stash.text_color(""Miscellaneous Topics:"", ""yellow""))
    for pp in PAGEPATHS:
        if not os.path.isdir(pp):
            continue
        content = os.listdir(pp)
        for pn in content:
            if ""."" in pn:
                name = pn[: pn.index(""."")]
            else:
                name = pn
            print(name)
",if not os . path . isdir ( pp ) :,125
"def test_send_error(self):
    allow_transfer_encoding_codes = (205, 304)
    for code in (101, 102, 204, 205, 304):
        self.con.request(""SEND_ERROR"", ""/{}"".format(code))
        res = self.con.getresponse()
        self.assertEqual(code, res.status)
        self.assertEqual(None, res.getheader(""Content-Length""))
        self.assertEqual(None, res.getheader(""Content-Type""))
        if code not in allow_transfer_encoding_codes:
            self.assertEqual(None, res.getheader(""Transfer-Encoding""))
        data = res.read()
        self.assertEqual(b"""", data)
",if code not in allow_transfer_encoding_codes :,173
"def _length_hint(obj):
    """"""Returns the length hint of an object.""""""
    try:
        return len(obj)
    except (AttributeError, TypeError):
        try:
            get_hint = type(obj).__length_hint__
        except AttributeError:
            return None
        try:
            hint = get_hint(obj)
        except TypeError:
            return None
        if hint is NotImplemented or not isinstance(hint, int_types) or hint < 0:
            return None
        return hint
","if hint is NotImplemented or not isinstance ( hint , int_types ) or hint < 0 :",135
"def _rmtree(self, path):
    # Essentially a stripped down version of shutil.rmtree.  We can't
    # use globals because they may be None'ed out at shutdown.
    for name in self._listdir(path):
        fullname = self._path_join(path, name)
        try:
            isdir = self._isdir(fullname)
        except self._os_error:
            isdir = False
        if isdir:
            self._rmtree(fullname)
        else:
            try:
                self._remove(fullname)
            except self._os_error:
                pass
    try:
        self._rmdir(path)
    except self._os_error:
        pass
",if isdir :,183
"def get_sources(self, sources=None):
    """"""Returns all sources from this provider.""""""
    self._load()
    if sources is None:
        sources = list(self.data.keys())
    elif not isinstance(sources, (list, tuple)):
        sources = [sources]
    for source in sources:
        if source not in self.data:
            raise KeyError(
                ""Invalid data key: {}. Valid keys are: {}"".format(
                    source, "", "".join(str(k) for k in self.data)
                )
            )
    return {k: self.data[k] for k in sources}
",if source not in self . data :,163
"def do_shorts(
    opts: List[Tuple[str, str]], optstring: str, shortopts: str, args: List[str]
) -> Tuple[List[Tuple[str, str]], List[str]]:
    while optstring != """":
        opt, optstring = optstring[0], optstring[1:]
        if short_has_arg(opt, shortopts):
            if optstring == """":
                if not args:
                    raise GetoptError(""option -%s requires argument"" % opt, opt)
                optstring, args = args[0], args[1:]
            optarg, optstring = optstring, """"
        else:
            optarg = """"
        opts.append((""-"" + opt, optarg))
    return opts, args
","if optstring == """" :",183
"def _sanitize_dict(self, config_dict, allow_val_change=None, ignore_keys: set = None):
    sanitized = {}
    for k, v in six.iteritems(config_dict):
        if ignore_keys and k in ignore_keys:
            continue
        k, v = self._sanitize(k, v, allow_val_change)
        sanitized[k] = v
    return sanitized
",if ignore_keys and k in ignore_keys :,104
"def x(data):
    count = 0
    while count < 10:
        data.start_example(SOME_LABEL)
        b = data.draw_bits(1)
        if b:
            count += 1
        data.stop_example(discard=not b)
    data.mark_interesting()
",if b :,79
"def prompt_for_resume(config):
    logger = logging.getLogger(""changeme"")
    logger.error(
        ""A previous scan was interrupted. Type R to resume or F to start a fresh scan""
    )
    answer = """"
    while not (answer == ""R"" or answer == ""F""):
        prompt = ""(R/F)> ""
        answer = """"
        try:
            answer = raw_input(prompt)
        except NameError:
            answer = input(prompt)
        if answer.upper() == ""F"":
            logger.debug(""Forcing a fresh scan"")
        elif answer.upper() == ""R"":
            logger.debug(""Resuming previous scan"")
            config.resume = True
    return config.resume
","elif answer . upper ( ) == ""R"" :",189
"def _evaluate_local_single(self, iterator):
    for batch in iterator:
        in_arrays = convert._call_converter(self.converter, batch, self.device)
        with function.no_backprop_mode():
            if isinstance(in_arrays, tuple):
                results = self.calc_local(*in_arrays)
            elif isinstance(in_arrays, dict):
                results = self.calc_local(**in_arrays)
            else:
                results = self.calc_local(in_arrays)
        if self._progress_hook:
            self._progress_hook(batch)
        yield results
","if isinstance ( in_arrays , tuple ) :",166
"def _send_until_done(self, data):
    while True:
        try:
            return self.connection.send(data)
        except OpenSSL.SSL.WantWriteError:
            if not util.wait_for_write(self.socket, self.socket.gettimeout()):
                raise timeout()
            continue
        except OpenSSL.SSL.SysCallError as e:
            raise SocketError(str(e))
","if not util . wait_for_write ( self . socket , self . socket . gettimeout ( ) ) :",112
"def _read_jtl_chunk(self, jtl):
    data = jtl.read(1024 * 1024 * 10)
    if data:
        parts = data.rsplit(""\n"", 1)
        if len(parts) > 1:
            ready_chunk = self.buffer + parts[0] + ""\n""
            self.buffer = parts[1]
            df = string_to_df(ready_chunk)
            self.stat_queue.put(df)
            return df
        else:
            self.buffer += parts[0]
    else:
        if self.jmeter_finished:
            self.agg_finished = True
        jtl.readline()
    return None
",if len ( parts ) > 1 :,182
"def __new__(mcl, classname, bases, dictionary):
    slots = list(dictionary.get(""__slots__"", []))
    for getter_name in [key for key in dictionary if key.startswith(""get_"")]:
        name = getter_name
        slots.append(""__"" + name)
        getter = dictionary.pop(getter_name)
        setter = dictionary.get(setter_name, None)
        if setter is not None and isinstance(setter, collections.Callable):
            del dictionary[setter_name]
        dictionary[name] = property(getter.setter)
        dictionary[""__slots__""] = tuple(slots)
        return super().__new__(mcl, classname, bases, dictionary)
","if setter is not None and isinstance ( setter , collections . Callable ) :",166
"def tex_coords(self):
    """"""Array of texture coordinate data.""""""
    if ""multi_tex_coords"" not in self.domain.attribute_names:
        if self._tex_coords_cache_version != self.domain._version:
            domain = self.domain
            attribute = domain.attribute_names[""tex_coords""]
            self._tex_coords_cache = attribute.get_region(
                attribute.buffer, self.start, self.count
            )
            self._tex_coords_cache_version = domain._version
        region = self._tex_coords_cache
        region.invalidate()
        return region.array
    else:
        return None
",if self . _tex_coords_cache_version != self . domain . _version :,173
"def index(self, sub, start=0):
    """"""Returns the index of the closing bracket""""""
    br = ""([{<""["")]}>"".index(sub)]
    count = 0
    for i in range(start, len(self.string)):
        char = self.string[i]
        if char == br:
            count += 1
        elif char == sub:
            if count > 0:
                count -= 1
            else:
                return i
    err = ""Closing bracket {!r} missing in string {!r}"".format(
        sub, """".join(self.original)
    )
    raise ParseError(err)
",if char == br :,161
"def test_createFile(self):
    text = ""This is a test!""
    path = tempfile.mktemp()
    try:
        koDoc = self._koDocFromPath(path, load=False)
        koDoc.buffer = text
        koDoc.save(0)
        del koDoc
        koDoc2 = self._koDocFromPath(path)
        assert koDoc2.buffer == text
    finally:
        if os.path.exists(path):
            os.unlink(path)  # clean up
",if os . path . exists ( path ) :,134
"def __editScopeHasEdit(self, attributeHistory):
    with attributeHistory.context:
        tweak = GafferScene.EditScopeAlgo.acquireParameterEdit(
            attributeHistory.scene.node(),
            attributeHistory.context[""scene:path""],
            attributeHistory.attributeName,
            IECoreScene.ShaderNetwork.Parameter("""", self.__parameter),
            createIfNecessary=False,
        )
        if tweak is None:
            return False
        return tweak[""enabled""].getValue()
",if tweak is None :,139
"def mail_migrator(app, schema_editor):
    Event_SettingsStore = app.get_model(""pretixbase"", ""Event_SettingsStore"")
    for ss in Event_SettingsStore.objects.filter(
        key__in=[
            ""mail_text_order_approved"",
            ""mail_text_order_placed"",
            ""mail_text_order_placed_require_approval"",
        ]
    ):
        chgd = ss.value.replace(""{date}"", ""{expire_date}"")
        if chgd != ss.value:
            ss.value = chgd
            ss.save()
            cache.delete(""hierarkey_{}_{}"".format(""event"", ss.object_id))
",if chgd != ss . value :,179
"def __get_limits(self):
    dimension = len(self.__tree.get_root().data)
    nodes = self.__get_all_nodes()
    max, min = [float(""-inf"")] * dimension, [float(""+inf"")] * dimension
    for node in nodes:
        for d in range(dimension):
            if max[d] < node.data[d]:
                max[d] = node.data[d]
            if min[d] > node.data[d]:
                min[d] = node.data[d]
    return min, max
",if min [ d ] > node . data [ d ] :,145
"def get_complete_position(self, context: UserContext) -> int:
    # Check member prefix pattern.
    for prefix_pattern in convert2list(
        self.get_filetype_var(context[""filetype""], ""prefix_patterns"")
    ):
        m = re.search(self._object_pattern + prefix_pattern + r""\w*$"", context[""input""])
        if m is None or prefix_pattern == """":
            continue
        self._prefix = re.sub(r""\w*$"", """", m.group(0))
        m = re.search(r""\w*$"", context[""input""])
        if m:
            return m.start()
    return -1
","if m is None or prefix_pattern == """" :",166
"def _stderr_supports_color():
    try:
        if hasattr(sys.stderr, ""isatty"") and sys.stderr.isatty():
            if curses:
                curses.setupterm()
                if curses.tigetnum(""colors"") > 0:
                    return True
            elif colorama:
                if sys.stderr is getattr(
                    colorama.initialise, ""wrapped_stderr"", object()
                ):
                    return True
    except Exception:
        # Very broad exception handling because it's always better to
        # fall back to non-colored logs than to break at startup.
        pass
    return False
",elif colorama :,170
"def setLabelColumnWidth(self, panel, width):
    for child in panel.GetChildren():
        if isinstance(child, wx.lib.stattext.GenStaticText):
            size = child.GetSize()
            size[0] = width
            child.SetBestSize(size)
","if isinstance ( child , wx . lib . stattext . GenStaticText ) :",74
"def update(self, other):
    if other.M is None:
        if self.M is None:
            self.items.update(other.items)
        else:
            for i in other.items:
                self.add(i)
        return
    if self.M is None:
        self.convert()
    self.M = array.array(""B"", list(map(max, list(zip(self.M, other.M)))))
",if self . M is None :,119
"def on_end_epoch(self, state):
    if self.write_epoch_metrics:
        if self.visdom:
            self.writer.add_text(
                ""epoch"",
                ""<h4>Epoch {}</h4>"".format(state[torchbearer.EPOCH])
                + self.table_formatter(str(state[torchbearer.METRICS])),
                1,
            )
        else:
            self.writer.add_text(
                ""epoch"",
                self.table_formatter(str(state[torchbearer.METRICS])),
                state[torchbearer.EPOCH],
            )
",if self . visdom :,174
"def is_listening_for_message(conversation_id: Text, endpoint: EndpointConfig) -> bool:
    """"""Check if the conversation is in need for a user message.""""""
    tracker = await retrieve_tracker(endpoint, conversation_id, EventVerbosity.APPLIED)
    for i, e in enumerate(reversed(tracker.get(""events"", []))):
        if e.get(""event"") == UserUttered.type_name:
            return False
        elif e.get(""event"") == ActionExecuted.type_name:
            return e.get(""name"") == ACTION_LISTEN_NAME
    return False
","if e . get ( ""event"" ) == UserUttered . type_name :",154
"def filter_ports(self, dpid, in_port, nw_id, allow_nw_id_external=None):
    assert nw_id != self.nw_id_unknown
    ret = []
    for port in self.get_ports(dpid):
        nw_id_ = port.network_id
        if port.port_no == in_port:
            continue
        if nw_id_ == nw_id:
            ret.append(port.port_no)
        elif allow_nw_id_external is not None and nw_id_ == allow_nw_id_external:
            ret.append(port.port_no)
    return ret
",if port . port_no == in_port :,167
"def next_month(billing_cycle_anchor: datetime, dt: datetime) -> datetime:
    estimated_months = round((dt - billing_cycle_anchor).days * 12.0 / 365)
    for months in range(max(estimated_months - 1, 0), estimated_months + 2):
        proposed_next_month = add_months(billing_cycle_anchor, months)
        if 20 < (proposed_next_month - dt).days < 40:
            return proposed_next_month
    raise AssertionError(
        ""Something wrong in next_month calculation with ""
        f""billing_cycle_anchor: {billing_cycle_anchor}, dt: {dt}""
    )
",if 20 < ( proposed_next_month - dt ) . days < 40 :,165
"def wait_complete(self):
    """"""Wait for futures complete done.""""""
    for future in concurrent.futures.as_completed(self._futures.keys()):
        try:
            error = future.exception()
        except concurrent.futures.CancelledError:
            break
        name = self._futures[future]
        if error is not None:
            err_msg = 'Extracting ""{0}"", got: {1}'.format(name, error)
            logger.error(err_msg)
",if error is not None :,124
"def _accept_with(cls, orm, target):
    if target is orm.mapper:
        return mapperlib.Mapper
    elif isinstance(target, type):
        if issubclass(target, mapperlib.Mapper):
            return target
        else:
            mapper = _mapper_or_none(target)
            if mapper is not None:
                return mapper
            else:
                return _MapperEventsHold(target)
    else:
        return target
",if mapper is not None :,123
"def gvariant_args(args: List[Any]) -> str:
    """"""Convert args into gvariant.""""""
    gvariant = """"
    for arg in args:
        if isinstance(arg, bool):
            gvariant += "" {}"".format(str(arg).lower())
        elif isinstance(arg, (int, float)):
            gvariant += f"" {arg}""
        elif isinstance(arg, str):
            gvariant += f' ""{arg}""'
        else:
            gvariant += f"" {arg!s}""
    return gvariant.lstrip()
","if isinstance ( arg , bool ) :",139
"def _list_cases(suite):
    for test in suite:
        if isinstance(test, unittest.TestSuite):
            _list_cases(test)
        elif isinstance(test, unittest.TestCase):
            if support.match_test(test):
                print(test.id())
","elif isinstance ( test , unittest . TestCase ) :",75
"def get_and_set_all_disambiguation(self):
    all_disambiguations = []
    for page in self.pages:
        if page.relations.disambiguation_links_norm is not None:
            all_disambiguations.extend(page.relations.disambiguation_links_norm)
        if page.relations.disambiguation_links is not None:
            all_disambiguations.extend(page.relations.disambiguation_links)
    return set(all_disambiguations)
",if page . relations . disambiguation_links_norm is not None :,113
"def test_decode_invalid(self):
    testcases = [
        (b""xn--w&"", ""strict"", UnicodeError()),
        (b""xn--w&"", ""ignore"", ""xn-""),
    ]
    for puny, errors, expected in testcases:
        with self.subTest(puny=puny, errors=errors):
            if isinstance(expected, Exception):
                self.assertRaises(UnicodeError, puny.decode, ""punycode"", errors)
            else:
                self.assertEqual(puny.decode(""punycode"", errors), expected)
","if isinstance ( expected , Exception ) :",144
"def find_globs(walker, patterns, matches):
    for root, dirs, files in walker:
        for d in dirs:
            d = join(root, d)
            for pattern in patterns:
                for p in Path(d).glob(pattern):
                    matches.add(str(p))
        sub_files = set()
        for p in matches:
            if root.startswith(p):
                for f in files:
                    sub_files.add(join(root, f))
        matches.update(sub_files)
",if root . startswith ( p ) :,149
"def parse_stack_trace(self, it, line):
    """"""Iterate over lines and parse stack traces.""""""
    events = []
    stack_traces = []
    while self.stack_trace_re.match(line):
        event = self.parse_stack_trace_line(line)
        if event:
            events.append(event)
        stack_traces.append(line)
        line = get_next(it)
    events.reverse()
    return stack_traces, events, line
",if event :,123
"def process(self):
    """"""Do processing necessary, storing result in feature.""""""
    summation = 0  # count of all
    histo = self.data[""flat.notes.quarterLengthHistogram""]
    if not histo:
        raise NativeFeatureException(""input lacks notes"")
    maxKey = 0  # max found for any one key
    for key in histo:
        # all defined keys should be greater than zero, but just in case
        if histo[key] > 0:
            summation += histo[key]
            if histo[key] >= maxKey:
                maxKey = histo[key]
    self.feature.vector[0] = maxKey / summation
",if histo [ key ] >= maxKey :,169
"def load_resource(name):
    """"""return file contents for files within the package root folder""""""
    try:
        if is_ST3():
            return sublime.load_resource(""Packages/Markdown Preview/{0}"".format(name))
        else:
            filename = os.path.join(
                sublime.packages_path(), INSTALLED_DIRECTORY, os.path.normpath(name)
            )
            return load_utf8(filename)
    except:
        print(""Error while load_resource('%s')"" % name)
        traceback.print_exc()
        return """"
",if is_ST3 ( ) :,154
"def get_password(self, service, repo_url):
    if self.is_unlocked:
        asyncio.set_event_loop(asyncio.new_event_loop())
        collection = secretstorage.get_default_collection(self.connection)
        attributes = {""application"": ""Vorta"", ""service"": service, ""repo_url"": repo_url}
        items = list(collection.search_items(attributes))
        logger.debug(""Found %i passwords matching repo URL."", len(items))
        if len(items) > 0:
            return items[0].get_secret().decode(""utf-8"")
    return None
",if len ( items ) > 0 :,156
"def get_files(d):
    res = []
    for p in glob.glob(os.path.join(d, ""*"")):
        if not p:
            continue
        (pth, fname) = os.path.split(p)
        if fname == ""output"":
            continue
        if fname == ""PureMVC_Python_1_0"":
            continue
        if fname[-4:] == "".pyc"":  # ehmm.. no.
            continue
        if os.path.isdir(p):
            get_dir(p)
        else:
            res.append(p)
    return res
",if os . path . isdir ( p ) :,162
"def test_nic_names(self):
    p = subprocess.Popen([""ipconfig"", ""/all""], stdout=subprocess.PIPE)
    out = p.communicate()[0]
    if PY3:
        out = str(out, sys.stdout.encoding)
    nics = psutil.net_io_counters(pernic=True).keys()
    for nic in nics:
        if ""pseudo-interface"" in nic.replace("" "", ""-"").lower():
            continue
        if nic not in out:
            self.fail(""%r nic wasn't found in 'ipconfig /all' output"" % nic)
","if ""pseudo-interface"" in nic . replace ( "" "" , ""-"" ) . lower ( ) :",145
"def vexop_to_simop(op, extended=True, fp=True):
    res = operations.get(op)
    if res is None and extended:
        attrs = op_attrs(op)
        if attrs is None:
            raise UnsupportedIROpError(""Operation not implemented"")
        res = SimIROp(op, **attrs)
    if res is None:
        raise UnsupportedIROpError(""Operation not implemented"")
    if res._float and not fp:
        raise UnsupportedIROpError(""Floating point support disabled"")
    return res
",if attrs is None :,141
"def rule_builder_add_value(self, value, screenshot_name=None):
    rule_builder = self.components.rule_builder
    rule_builder.menu_button_column.wait_for_and_click()
    with self.rule_builder_rule_editor(""add-column-value"") as editor_element:
        filter_input = editor_element.find_element_by_css_selector(""input[type='text']"")
        filter_input.clear()
        filter_input.send_keys(value)
        if screenshot_name:
            self.screenshot(screenshot_name)
",if screenshot_name :,147
"def make_open_socket(self):
    s = socket.socket()
    try:
        s.bind(DEFAULT_BIND_ADDR_TUPLE)
        if WIN or greentest.LINUX:
            # Windows and linux (with psutil) doesn't show as open until
            # we call listen (linux with lsof accepts either)
            s.listen(1)
        self.assert_open(s, s.fileno())
    except:
        s.close()
        s = None
        raise
    return s
",if WIN or greentest . LINUX :,135
"def handle_ray_task_error(e):
    for s in e.traceback_str.split(""\n"")[::-1]:
        if ""Error"" in s or ""Exception"" in s:
            try:
                raise getattr(builtins, s.split("":"")[0])("""".join(s.split("":"")[1:]))
            except AttributeError as att_err:
                if ""module"" in str(att_err) and builtins.__name__ in str(att_err):
                    pass
                else:
                    raise att_err
    raise e
","if ""Error"" in s or ""Exception"" in s :",144
"def compare_multiple_events(i, expected_results, actual_results):
    events_in_a_row = []
    j = i
    while j < len(expected_results) and isinstance(
        actual_results[j], actual_results[i].__class__
    ):
        events_in_a_row.append(actual_results[j])
        j += 1
    message = """"
    for event in events_in_a_row:
        for k in range(i, j):
            passed, message = compare_events(expected_results[k], event)
            if passed:
                expected_results[k] = None
                break
        else:
            return i, False, message
    return j, True, """"
",if passed :,192
"def ListSubscriptions(self, params):
    queryreturn = sqlQuery(""""""SELECT label, address, enabled FROM subscriptions"""""")
    data = '{""subscriptions"":['
    for row in queryreturn:
        label, address, enabled = row
        label = shared.fixPotentiallyInvalidUTF8Data(label)
        if len(data) > 20:
            data += "",""
        data += json.dumps(
            {
                ""label"": label.encode(""base64""),
                ""address"": address,
                ""enabled"": enabled == 1,
            },
            indent=4,
            separators=("","", "": ""),
        )
    data += ""]}""
    return data
",if len ( data ) > 20 :,177
"def compile(self, args):
    compiled_args = {}
    for key, value in six.iteritems(args):
        if key in self.clean_args:
            compiled_args[key] = str(value)
        else:
            compiled_args[key] = sjson_dumps(value)
    return self._minified_code % compiled_args
",if key in self . clean_args :,91
"def insert(self, pack_id, data):
    if (pack_id not in self.queue) and pack_id > self.begin_id:
        self.queue[pack_id] = PacketInfo(data)
        if self.end_id == pack_id:
            self.end_id = pack_id + 1
        elif self.end_id < pack_id:
            eid = self.end_id
            while eid < pack_id:
                self.miss_queue.add(eid)
                eid += 1
            self.end_id = pack_id + 1
        else:
            self.miss_queue.remove(pack_id)
",elif self . end_id < pack_id :,182
"def _target_generator(self):
    # since we do not have predictions yet, so we ignore sampling here
    if self._internal_target_generator is None:
        if self._anchors_none:
            return None
        from ....model_zoo.ssd.target import SSDTargetGenerator
        self._internal_target_generator = SSDTargetGenerator(
            iou_thresh=self._iou_thresh,
            stds=self._box_norm,
            negative_mining_ratio=-1,
            **self._kwargs
        )
        return self._internal_target_generator
    else:
        return self._internal_target_generator
",if self . _anchors_none :,166
"def test_heapsort(self):
    # Exercise everything with repeated heapsort checks
    for trial in range(100):
        size = random.randrange(50)
        data = [random.randrange(25) for i in range(size)]
        if trial & 1:  # Half of the time, use heapify
            heap = data[:]
            self.module.heapify(heap)
        else:  # The rest of the time, use heappush
            heap = []
            for item in data:
                self.module.heappush(heap, item)
        heap_sorted = [self.module.heappop(heap) for i in range(size)]
        self.assertEqual(heap_sorted, sorted(data))
",if trial & 1 :,189
"def wait(self, timeout=None):
    if self.returncode is None:
        if timeout is None:
            msecs = _subprocess.INFINITE
        else:
            msecs = max(0, int(timeout * 1000 + 0.5))
        res = _subprocess.WaitForSingleObject(int(self._handle), msecs)
        if res == _subprocess.WAIT_OBJECT_0:
            code = _subprocess.GetExitCodeProcess(self._handle)
            if code == TERMINATE:
                code = -signal.SIGTERM
            self.returncode = code
    return self.returncode
",if res == _subprocess . WAIT_OBJECT_0 :,154
"def _on_change(self):
    changed = False
    self.save()
    for key, value in self.data.items():
        if isinstance(value, bool):
            if value:
                changed = True
                break
        if isinstance(value, int):
            if value != 1:
                changed = True
                break
        elif value is None:
            continue
        elif len(value) != 0:
            changed = True
            break
    self._reset_button.disabled = not changed
",elif value is None :,145
"def isnotsurplus(self, item: T) -> bool:
    if not self.matchers:
        if self.mismatch_description:
            self.mismatch_description.append_text(
                ""not matched: ""
            ).append_description_of(item)
        return False
    return True
",if self . mismatch_description :,80
"def resolve_env_secrets(config, environ):
    """"""Create copy that recursively replaces {""$env"": ""NAME""} with values from environ""""""
    if isinstance(config, dict):
        if list(config.keys()) == [""$env""]:
            return environ.get(list(config.values())[0])
        elif list(config.keys()) == [""$file""]:
            return open(list(config.values())[0]).read()
        else:
            return {
                key: resolve_env_secrets(value, environ)
                for key, value in config.items()
            }
    elif isinstance(config, list):
        return [resolve_env_secrets(value, environ) for value in config]
    else:
        return config
","if list ( config . keys ( ) ) == [ ""$env"" ] :",190
"def __open__(filename, *args, **kwargs):
    if os.path.isfile(filename):
        return __realopen__(filename, *args, **kwargs)
    if not os.path.isabs(filename):
        datafilename = __papplet__.dataPath(filename)
        if os.path.isfile(datafilename):
            return __realopen__(datafilename, *args, **kwargs)
        sketchfilename = __papplet__.sketchPath(filename)
    if os.path.isfile(sketchfilename):
        return __realopen__(sketchfilename, *args, **kwargs)
    # Fail naturally
    return __realopen__(filename, *args, **kwargs)
",if os . path . isfile ( datafilename ) :,172
"def run(self):
    while not self.completed:
        if self.block:
            time.sleep(self.period)
        else:
            self._completed.wait(self.period)
        self.counter += 1
        try:
            self.callback(self.counter)
        except Exception:
            self.stop()
        if self.timeout is not None:
            dt = time.time() - self._start_time
            if dt > self.timeout:
                self.stop()
        if self.counter == self.count:
            self.stop()
",if self . block :,159
"def remove(self, path, config=None, error_on_path=False, defaults=None):
    if not path:
        if error_on_path:
            raise NoSuchSettingsPath()
        return
    if config is not None or defaults is not None:
        if config is None:
            config = self._config
        if defaults is None:
            defaults = dict(self._map.parents)
        chain = HierarchicalChainMap(config, defaults)
    else:
        chain = self._map
    try:
        chain.del_by_path(path)
        self._mark_dirty()
    except KeyError:
        if error_on_path:
            raise NoSuchSettingsPath()
        pass
",if error_on_path :,184
"def structured_dot_grad(sparse_A, dense_B, ga):
    if sparse_A.type.format in (""csc"", ""csr""):
        if sparse_A.type.format == ""csc"":
            sdgcsx = sdg_csc
            CSx = CSC
        else:
            sdgcsx = sdg_csr
            CSx = CSR
        g_A_data = sdgcsx(csm_indices(sparse_A), csm_indptr(sparse_A), dense_B, ga)
        return CSx(
            g_A_data, csm_indices(sparse_A), csm_indptr(sparse_A), csm_shape(sparse_A)
        )
    else:
        raise NotImplementedError()
","if sparse_A . type . format == ""csc"" :",180
"def step_async(self, actions):
    listify = True
    try:
        if len(actions) == self.num_envs:
            listify = False
    except TypeError:
        pass
    if not listify:
        self.actions = actions
    else:
        assert (
            self.num_envs == 1
        ), f""actions {actions} is either not a list or has a wrong size - cannot match to {self.num_envs} environments""
        self.actions = [actions]
",if len ( actions ) == self . num_envs :,130
"def tempFailureRetry(func, *args, **kwargs):
    while True:
        try:
            return func(*args, **kwargs)
        except (os.error, IOError) as ex:
            if ex.errno == errno.EINTR:
                continue
            else:
                raise
",if ex . errno == errno . EINTR :,83
"def test_learning_always_changes_generation(chars, order):
    learner = LStar(lambda s: len(s) == 1 and s[0] in chars)
    for c in order:
        prev = learner.generation
        s = bytes([c])
        if learner.dfa.matches(s) != learner.member(s):
            learner.learn(s)
            assert learner.generation > prev
",if learner . dfa . matches ( s ) != learner . member ( s ) :,108
"def test_costs_5D_noisy_names(signal_bkps_5D_noisy, cost_name):
    signal, bkps = signal_bkps_5D_noisy
    cost = cost_factory(cost_name)
    cost.fit(signal)
    cost.error(0, 100)
    cost.error(100, signal.shape[0])
    cost.error(10, 50)
    cost.sum_of_costs(bkps)
    with pytest.raises(NotEnoughPoints):
        if cost_name == ""cosine"":
            cost.min_size = 4
            cost.error(1, 2)
        else:
            cost.error(1, 2)
","if cost_name == ""cosine"" :",174
"def remove_empty_dirs(dirname):
    logger.debug(""remove_empty_dirs '%s'"" % (dirname))
    try:
        if not isinstance(dirname, str):
            dirname = dirname.encode(""utf-8"")
        os.removedirs(dirname)
        logger.debug(""remove_empty_dirs '%s' done"" % (dirname))
    except OSError as exc:  # Python >2.5
        if exc.errno == errno.ENOTEMPTY:
            logger.debug(""remove_empty_dirs '%s' not empty"" % (dirname))
            pass
        else:
            raise
    except Exception as e:
        logger.exception(e)
        logger.error(""remove_empty_dirs exception: "" + dirname)
        raise e
","if not isinstance ( dirname , str ) :",193
"def get_unique_attribute(self, name: str):
    feat = None
    for f in self.features:
        if self._return_feature(f) and hasattr(f, name):
            if feat is not None:
                raise RuntimeError(""The attribute was not unique."")
            feat = f
    if feat is None:
        raise RuntimeError(""The attribute did not exist"")
    return getattr(feat, name)
","if self . _return_feature ( f ) and hasattr ( f , name ) :",106
"def get_allocated_address(
    self, config: ActorPoolConfig, allocated: allocated_type
) -> str:
    addresses = config.get_external_addresses(label=self.label)
    for addr in addresses:
        occupied = False
        for strategy, _ in allocated.get(addr, dict()).values():
            if strategy == self:
                occupied = True
                break
        if not occupied:
            return addr
    raise NoIdleSlot(
        f""No idle slot for creating actor "" f""with label {self.label}, mark {self.mark}""
    )
",if not occupied :,146
"def __deepcopy__(self, memo):
    cls = self.__class__
    result = cls.__new__(cls)
    memo[id(self)] = result
    for key, value in self.__dict__.items():
        if key in cls.dynamic_methods:
            setattr(result, key, copy.copy(value))
        else:
            setattr(result, key, copy.deepcopy(value, memo))
    return result
",if key in cls . dynamic_methods :,105
"def restore_forward(model):
    for child in model.children():
        # leaf node
        if is_leaf(child) and hasattr(child, ""old_forward""):
            child.forward = child.old_forward
            child.old_forward = None
        else:
            restore_forward(child)
","if is_leaf ( child ) and hasattr ( child , ""old_forward"" ) :",82
"def add(self, obj, allow_duplicates=False):
    if allow_duplicates or obj not in self._constants:
        self._constant_pool.append(obj)
        self._constants[obj] = len(self)
        if obj.__class__ in (Double, Long):
            self._constant_pool.append(None)
","if obj . __class__ in ( Double , Long ) :",83
"def find_file_copyright_notices(fname):
    ret = set()
    f = open(fname)
    lines = f.readlines()
    for l in lines[:80]:  # hmmm, assume copyright to be in first 80 lines
        idx = l.lower().find(""copyright"")
        if idx < 0:
            continue
        copyright = l[idx + 9 :].strip()
        if not copyright:
            continue
        copyright = sanitise(copyright)
        # hmm, do a quick check to see if there's a year,
        # if not, skip it
        if not copyright.find(""200"") >= 0 and not copyright.find(""199"") >= 0:
            continue
        ret.add(copyright)
    return ret
","if not copyright . find ( ""200"" ) >= 0 and not copyright . find ( ""199"" ) >= 0 :",186
"def callback(lexer, match, context):
    text = match.group()
    extra = """"
    if start:
        context.next_indent = len(text)
        if context.next_indent < context.indent:
            while context.next_indent < context.indent:
                context.indent = context.indent_stack.pop()
            if context.next_indent > context.indent:
                extra = text[context.indent :]
                text = text[: context.indent]
    else:
        context.next_indent += len(text)
    if text:
        yield match.start(), TokenClass, text
    if extra:
        yield match.start() + len(text), TokenClass.Error, extra
    context.pos = match.end()
",if context . next_indent > context . indent :,196
"def queries(self):
    if DEV:
        cmd = ShellCommand(""docker"", ""ps"", ""-qf"", ""name=%s"" % self.path.k8s)
        if not cmd.check(f""docker check for {self.path.k8s}""):
            if not cmd.stdout.strip():
                log_cmd = ShellCommand(
                    ""docker"", ""logs"", self.path.k8s, stderr=subprocess.STDOUT
                )
                if log_cmd.check(f""docker logs for {self.path.k8s}""):
                    print(cmd.stdout)
                pytest.exit(f""container failed to start for {self.path.k8s}"")
    return ()
","if log_cmd . check ( f""docker logs for {self.path.k8s}"" ) :",188
"def nodes(self):
    if not self._nodes:
        nodes = self.cluster_group.instances()
        self._nodes = []
        master = self.master_node
        nodeid = 1
        for node in nodes:
            if node.state not in [""pending"", ""running""]:
                continue
            if node.id == master.id:
                self._nodes.insert(0, master)
                continue
            self._nodes.append(Node(node, self.key_location, ""node%.3d"" % nodeid))
            nodeid += 1
    else:
        for node in self._nodes:
            log.debug(""refreshing instance %s"" % node.id)
            node.update()
    return self._nodes
",if node . id == master . id :,198
"def match(cls, agent_name, guid, uri, media=None):
    # Retrieve `Agent` for provided `guid`
    agent = Agents.get(agent_name)
    if agent is None:
        if agent_name not in unsupported_agents:
            # First occurrence of unsupported agent
            log.warn(""Unsupported metadata agent: %s"" % agent_name)
            # Mark unsupported agent as ""seen""
            unsupported_agents[agent_name] = True
            return False
        # Duplicate occurrence of unsupported agent
        log.warn(
            ""Unsupported metadata agent: %s"" % agent_name, extra={""duplicate"": True}
        )
        return False
    # Fill `guid` with details from agent
    return agent.fill(guid, uri, media)
",if agent_name not in unsupported_agents :,199
"def __createRandom(plug):
    node = plug.node()
    parentNode = node.ancestor(Gaffer.Node)
    with Gaffer.UndoScope(node.scriptNode()):
        randomNode = Gaffer.Random()
        parentNode.addChild(randomNode)
        if isinstance(plug, (Gaffer.FloatPlug, Gaffer.IntPlug)):
            plug.setInput(randomNode[""outFloat""])
        elif isinstance(plug, Gaffer.Color3fPlug):
            plug.setInput(randomNode[""outColor""])
    GafferUI.NodeEditor.acquire(randomNode)
","elif isinstance ( plug , Gaffer . Color3fPlug ) :",158
"def post_arrow(self, arr: pa.Table, graph_type: str, opts: str = """"):
    dataset_id = self.dataset_id
    tok = self.token
    sub_path = f""api/v2/upload/datasets/{dataset_id}/{graph_type}/arrow""
    try:
        resp = self.post_arrow_generic(sub_path, tok, arr, opts)
        out = resp.json()
        if not (""success"" in out) or not out[""success""]:
            raise Exception(""No success indicator in server response"")
        return out
    except Exception as e:
        logger.error(""Failed to post arrow to %s"", sub_path, exc_info=True)
        raise e
","if not ( ""success"" in out ) or not out [ ""success"" ] :",179
"def dict_to_XML(tag, dictionary, **kwargs):
    """"""Return XML element converting dicts recursively.""""""
    elem = Element(tag, **kwargs)
    for key, val in dictionary.items():
        if tag == ""layers"":
            child = dict_to_XML(""layer"", val, name=key)
        elif isinstance(val, MutableMapping):
            child = dict_to_XML(key, val)
        else:
            if tag == ""config"":
                child = Element(""variable"", name=key)
            else:
                child = Element(key)
            child.text = str(val)
        elem.append(child)
    return elem
","if tag == ""layers"" :",175
"def apply_incpaths_ml(self):
    inc_lst = self.includes.split()
    lst = self.incpaths_lst
    for dir in inc_lst:
        node = self.path.find_dir(dir)
        if not node:
            error(""node not found: "" + str(dir))
            continue
        if not node in lst:
            lst.append(node)
        self.bld_incpaths_lst.append(node)
",if not node :,121
"def _table_reprfunc(self, row, col, val):
    if self._table.column_names[col].endswith(""Size""):
        if isinstance(val, compat.string_types):
            return ""  %s"" % val
        elif val < 1024 ** 2:
            return ""  %.1f KB"" % (val / 1024.0 ** 1)
        elif val < 1024 ** 3:
            return ""  %.1f MB"" % (val / 1024.0 ** 2)
        else:
            return ""  %.1f GB"" % (val / 1024.0 ** 3)
    if col in (0, """"):
        return str(val)
    else:
        return ""  %s"" % val
",elif val < 1024 ** 3 :,182
"def _cache_mem(curr_out, prev_mem, mem_len, reuse_len=None):
    """"""cache hidden states into memory.""""""
    if mem_len is None or mem_len == 0:
        return None
    else:
        if reuse_len is not None and reuse_len > 0:
            curr_out = curr_out[:reuse_len]
        if prev_mem is None:
            new_mem = curr_out[-mem_len:]
        else:
            new_mem = tf.concat([prev_mem, curr_out], 0)[-mem_len:]
    new_mem.stop_gradient = True
    return new_mem
",if reuse_len is not None and reuse_len > 0 :,165
"def GROUP_CONCAT(builder, distinct, expr, sep=None):
    assert distinct in (None, True, False)
    result = distinct and ""GROUP_CONCAT(DISTINCT "" or ""GROUP_CONCAT("", builder(expr)
    if sep is not None:
        if builder.provider.dialect == ""MySQL"":
            result = result, "" SEPARATOR "", builder(sep)
        else:
            result = result, "", "", builder(sep)
    return result, "")""
","if builder . provider . dialect == ""MySQL"" :",117
"def __init__(self, *args, **kwargs):
    super().__init__(*args, **kwargs)
    self.custom_fields = []
    self.obj_type = ContentType.objects.get_for_model(self.model)
    # Add all applicable CustomFields to the form
    custom_fields = CustomField.objects.filter(content_types=self.obj_type)
    for cf in custom_fields:
        # Annotate non-required custom fields as nullable
        if not cf.required:
            self.nullable_fields.append(cf.name)
        self.fields[cf.name] = cf.to_form_field(
            set_initial=False, enforce_required=False
        )
        # Annotate this as a custom field
        self.custom_fields.append(cf.name)
",if not cf . required :,199
"def is_child_of(self, item_hash, possible_child_hash):
    if self.get_last(item_hash) != self.get_last(possible_child_hash):
        return None
    while True:
        if possible_child_hash == item_hash:
            return True
        if possible_child_hash not in self.items:
            return False
        possible_child_hash = self.items[possible_child_hash].previous_hash
",if possible_child_hash == item_hash :,119
"def validate(self):
    self.assertEqual(len(self.inputs), len(self.outputs))
    for batch_in, batch_out in zip(self.inputs, self.outputs):
        self.assertEqual(len(batch_in), len(batch_out))
        if self.use_parallel_executor and not self.use_double_buffer:
            self.validate_unordered_batch(batch_in, batch_out)
        else:
            for in_data, out_data in zip(batch_in, batch_out):
                self.assertEqual(in_data.shape, out_data.shape)
                if not self.use_parallel_executor:
                    self.assertTrue((in_data == out_data).all())
",if self . use_parallel_executor and not self . use_double_buffer :,189
"def add_cells(self, cells):
    for cell in cells:
        if cell not in self.cell_id_map:
            id = len(self.cell_id_map)
            self.cell_id_map[cell] = id
            self.id_cell_map[id] = cell
",if cell not in self . cell_id_map :,80
"def _verify_out(marker="">>""):
    if shared:
        self.assertIn(""libapp_lib.dylib"", self.client.out)
    else:
        if marker == "">>"":
            self.assertIn(""libapp_lib.a"", self.client.out)
        else:  # Incremental build not the same msg
            self.assertIn(""Built target app_lib"", self.client.out)
    out = str(self.client.out).splitlines()
    for k, v in vals.items():
        self.assertIn(""%s %s: %s"" % (marker, k, v), out)
","if marker == "">>"" :",152
"def Visit_expr(self, node):  # pylint: disable=invalid-name
    # expr ::= xor_expr ('|' xor_expr)*
    for child in node.children:
        self.Visit(child)
        if isinstance(child, pytree.Leaf) and child.value == ""|"":
            _AppendTokenSubtype(child, format_token.Subtype.BINARY_OPERATOR)
","if isinstance ( child , pytree . Leaf ) and child . value == ""|"" :",92
"def fill_members(self):
    if self._get_retrieve():
        after = self.after.id if self.after else None
        data = await self.get_members(self.guild.id, self.retrieve, after)
        if not data:
            # no data, terminate
            return
        if len(data) < 1000:
            self.limit = 0  # terminate loop
        self.after = Object(id=int(data[-1][""user""][""id""]))
        for element in reversed(data):
            await self.members.put(self.create_member(element))
",if len ( data ) < 1000 :,153
"def assert_warns(expected):
    with warnings.catch_warnings(record=True) as w:
        warnings.simplefilter(""always"")
        yield
    # Python 2 does not raise warnings multiple times from the same stack
    # frame.
    if sys.version_info >= (3, 0):
        if not any(isinstance(m.message, expected) for m in w):
            try:
                exc_name = expected.__name__
            except AttributeError:
                exc_name = str(expected)
            raise AssertionError(""%s not triggerred"" % exc_name)
","if not any ( isinstance ( m . message , expected ) for m in w ) :",147
"def __init__(self, measures):
    """"""Constructs a ContingencyMeasures given a NgramAssocMeasures class""""""
    self.__class__.__name__ = ""Contingency"" + measures.__class__.__name__
    for k in dir(measures):
        if k.startswith(""__""):
            continue
        v = getattr(measures, k)
        if not k.startswith(""_""):
            v = self._make_contingency_fn(measures, v)
        setattr(self, k, v)
","if k . startswith ( ""__"" ) :",116
"def _omit_keywords(self, context):
    omitted_kws = 0
    for event, elem in context:
        # Teardowns aren't omitted to allow checking suite teardown status.
        omit = elem.tag == ""kw"" and elem.get(""type"") != ""teardown""
        start = event == ""start""
        if omit and start:
            omitted_kws += 1
        if not omitted_kws:
            yield event, elem
        elif not start:
            elem.clear()
        if omit and not start:
            omitted_kws -= 1
",if omit and start :,144
"def read_block(buffer, i):
    offset = i * BLOCK_LENGTH % config.CAPTURE_BUFFER
    while True:
        if buffer[offset] == BLOCK_MARKER.END:
            return None
        while buffer[offset] == BLOCK_MARKER.WRITE:
            time.sleep(SHORT_SENSOR_SLEEP_TIME)
        buffer[offset] = BLOCK_MARKER.READ
        buffer.seek(offset + 1)
        length = struct.unpack(""=H"", buffer.read(2))[0]
        retval = buffer.read(length)
        if buffer[offset] == BLOCK_MARKER.READ:
            break
    buffer[offset] = BLOCK_MARKER.NOP
    return retval
",if buffer [ offset ] == BLOCK_MARKER . READ :,179
"def _start(self):
    try:
        instance_info = self._get_instance_info()
        if not instance_info.is_running():
            self._multipass_cmd.start(instance_name=self.instance_name)
    except errors.ProviderInfoError as instance_error:
        # Until we have proper multipass error codes to know if this
        # was a communication error we should keep this error tracking
        # and generation here.
        raise errors.ProviderInstanceNotFoundError(
            instance_name=self.instance_name
        ) from instance_error
",if not instance_info . is_running ( ) :,145
"def _river_driver(self):
    if self._cached_river_driver:
        return self._cached_river_driver
    else:
        if app_config.IS_MSSQL:
            self._cached_river_driver = MsSqlDriver(
                self.workflow, self.wokflow_object_class, self.field_name
            )
        else:
            self._cached_river_driver = OrmDriver(
                self.workflow, self.wokflow_object_class, self.field_name
            )
        return self._cached_river_driver
",if app_config . IS_MSSQL :,156
"def __LazyMap__(self, attr):
    try:
        if self._LazyAddAttr_(attr):
            debug_attr_print(
                ""%s.__LazyMap__(%s) added something"" % (self._username_, attr)
            )
            return 1
    except AttributeError:
        return 0
",if self . _LazyAddAttr_ ( attr ) :,80
"def prepare(self, data=None, user=None):
    """"""Prepare activation for execution.""""""
    super(ManagedStartViewActivation, self).prepare.original()
    self.task.owner = user
    management_form_class = self.get_management_form_class()
    self.management_form = management_form_class(data=data, instance=self.task)
    if data:
        if not self.management_form.is_valid():
            raise FlowRuntimeError(
                ""Activation metadata is broken {}"".format(self.management_form.errors)
            )
        self.task = self.management_form.save(commit=False)
",if not self . management_form . is_valid ( ) :,160
"def PreprocessConditionalStatement(self, IfList, ReplacedLine):
    while self:
        if self.__Token:
            x = 1
        elif not IfList:
            if self <= 2:
                continue
            RegionSizeGuid = 3
            if not RegionSizeGuid:
                RegionLayoutLine = 5
                continue
            RegionLayoutLine = self.CurrentLineNumber
    return 1
",if not RegionSizeGuid :,111
"def _get_completion(self, document):
    try:
        completion_header = document.xpath(""//div[@id='complete_day']"")[0]
        completion_message = completion_header.getchildren()[0]
        if ""day_incomplete_message"" in completion_message.classes:
            return False
        elif ""day_complete_message"" in completion_message.classes:
            return True
    except IndexError:
        return False  # Who knows, probably not my diary.
","if ""day_incomplete_message"" in completion_message . classes :",123
"def run(self):
    DISPATCH_SYNC = components.interfaces.nsIEventTarget.DISPATCH_SYNC
    try:
        if self._stopped:
            return
        for match in findlib2.find_all_matches(self.regex, self.text):
            if self._stopped:
                return
            self.target.dispatch(lambda: self.callback(match), DISPATCH_SYNC)
            if self._stopped:
                return
        self.target.dispatch(lambda: self.callback(None), DISPATCH_SYNC)
    finally:
        self.callback = None
        self.target = None
",if self . _stopped :,164
"def to_key(literal_or_identifier):
    """"""returns string representation of this object""""""
    if literal_or_identifier[""type""] == ""Identifier"":
        return literal_or_identifier[""name""]
    elif literal_or_identifier[""type""] == ""Literal"":
        k = literal_or_identifier[""value""]
        if isinstance(k, float):
            return unicode(float_repr(k))
        elif ""regex"" in literal_or_identifier:
            return compose_regex(k)
        elif isinstance(k, bool):
            return ""true"" if k else ""false""
        elif k is None:
            return ""null""
        else:
            return unicode(k)
","elif ""regex"" in literal_or_identifier :",179
"def process_image_pre_creation(sender, instance: Image, **kwargs):
    # FIXME(winkidney): May have issue on determining if it
    #  is created or not
    if instance.pk is not None:
        return
    for plugin in _plugin_instances:
        process_fn = getattr(plugin, ""process_image_pre_creation"", None)
        if process_fn is None:
            continue
        try:
            process_fn(
                django_settings=settings,
                image_instance=instance,
            )
        except Exception:
            logging.exception(
                ""Error occurs while trying to access plugin's pin_pre_save ""
                ""for plugin %s"" % plugin
            )
",if process_fn is None :,197
"def check_screenshots(self):
    # If we arrive here, there have not been any failures yet
    if self.interactive:
        self._commit_screenshots()
    else:
        if self._has_reference_screenshots():
            self._validate_screenshots()
            # Always commit the screenshots here. They can be used for the next test run.
            # If reference screenshots were already present and there was a mismatch, it should
            # have failed above.
            self._commit_screenshots()
        elif self.allow_missing_screenshots:
            warnings.warn(""No committed reference screenshots available. Ignoring."")
        else:
            self.fail(
                ""No committed reference screenshots available. Run interactive first.""
            )
",if self . _has_reference_screenshots ( ) :,190
"def on_task_abort(self, task, config):
    if ""abort"" in config:
        if task.silent_abort:
            return
        log.debug(""sending abort notification"")
        self.send_notification(
            config[""abort""][""title""],
            config[""abort""][""message""],
            config[""abort""][""via""],
            template_renderer=task.render,
        )
",if task . silent_abort :,104
"def block_users(self, user_ids):
    broken_items = []
    self.logger.info(""Going to block %d users."" % len(user_ids))
    for user_id in tqdm(user_ids):
        if not self.block(user_id):
            self.error_delay()
            broken_items = user_ids[user_ids.index(user_id) :]
            break
    self.logger.info(""DONE: Total blocked %d users."" % self.total[""blocks""])
    return broken_items
",if not self . block ( user_id ) :,135
"def find_widget_by_id(self, id, parent=None):
    """"""Recursively searches for widget with specified ID""""""
    if parent == None:
        if id in self:
            return self[id]  # Do things fast if possible
        parent = self[""editor""]
    for c in parent.get_children():
        if hasattr(c, ""get_id""):
            if c.get_id() == id:
                return c
        if isinstance(c, Gtk.Container):
            r = self.find_widget_by_id(id, c)
            if not r is None:
                return r
    return None
",if not r is None :,167
"def addClasses(self, name):
    # Result: void - None
    # In: name: string
    for n in name.split():
        try:
            k, method = n.split(""."")
        except ValueError:
            k = n
            method = None
        self.classes[k] = 1
        if method is not None:
            self.methods.setdefault(k, {})[method] = 1
",if method is not None :,109
"def Read(self, lex_mode):
    while True:
        t = self._Read(lex_mode)
        self.was_line_cont = t.id == Id.Ignored_LineCont
        # TODO: Change to ALL IGNORED types, once you have SPACE_TOK.  This means
        # we don't have to handle them in the VS_1/VS_2/etc. states.
        if t.id != Id.Ignored_LineCont:
            break
    # log('Read() Returning %s', t)
    return t
",if t . id != Id . Ignored_LineCont :,137
"def _dir_guildfile(dir, ctx):
    from guild import guildfile
    try:
        return guildfile.for_dir(dir)
    except guildfile.NoModels:
        if ctx:
            help_suffix = "" or '%s' for help"" % click_util.cmd_help(ctx)
        else:
            help_suffix = """"
        cli.error(
            ""%s does not contain a Guild file (guild.yml)\n""
            ""Try specifying a project path or package name%s.""
            % (cwd_desc(dir), help_suffix)
        )
    except guildfile.GuildfileError as e:
        cli.error(str(e))
",if ctx :,186
"def check_response(self, response):
    """"""Specialized version of check_response().""""""
    for line in response:
        # Skip blank lines:
        if not line.strip():
            continue
        if line.startswith(b""OK""):
            return
        elif line.startswith(b""Benutzer/Passwort Fehler""):
            raise BadLogin(line)
        else:
            raise FailedPost(""Server returned '%s'"" % six.ensure_text(line))
","if line . startswith ( b""OK"" ) :",126
"def ParseResponses(
    self,
    knowledge_base: rdf_client.KnowledgeBase,
    responses: Iterable[rdfvalue.RDFValue],
) -> Iterator[rdf_client.User]:
    for response in responses:
        if not isinstance(response, rdf_client_fs.StatEntry):
            raise TypeError(f""Unexpected response type: `{type(response)}`"")
        # TODO: `st_mode` has to be an `int`, not `StatMode`.
        if stat.S_ISDIR(int(response.st_mode)):
            homedir = response.pathspec.path
            username = os.path.basename(homedir)
            if username not in self._ignore_users:
                yield rdf_client.User(username=username, homedir=homedir)
",if stat . S_ISDIR ( int ( response . st_mode ) ) :,198
"def __call__(self, x, uttid=None):
    if self.utt2spk is not None:
        spk = self.utt2spk[uttid]
    else:
        spk = uttid
    if not self.reverse:
        if self.norm_means:
            x = np.add(x, self.bias[spk])
        if self.norm_vars:
            x = np.multiply(x, self.scale[spk])
    else:
        if self.norm_vars:
            x = np.divide(x, self.scale[spk])
        if self.norm_means:
            x = np.subtract(x, self.bias[spk])
    return x
",if self . norm_means :,189
"def hasFixtures(self, ctx_callback=None):
    context = self.context
    if context is None:
        return False
    if self.implementsAnyFixture(context, ctx_callback=ctx_callback):
        return True
    # My context doesn't have any, but its ancestors might
    factory = self.factory
    if factory:
        ancestors = factory.context.get(self, [])
        for ancestor in ancestors:
            if self.implementsAnyFixture(ancestor, ctx_callback=ctx_callback):
                return True
    return False
","if self . implementsAnyFixture ( ancestor , ctx_callback = ctx_callback ) :",137
"def UpdateControlState(self):
    active = self.demoModules.GetActiveID()
    # Update the radio/restore buttons
    for moduleID in self.radioButtons:
        btn = self.radioButtons[moduleID]
        if moduleID == active:
            btn.SetValue(True)
        else:
            btn.SetValue(False)
        if self.demoModules.Exists(moduleID):
            btn.Enable(True)
            if moduleID == modModified:
                self.btnRestore.Enable(True)
        else:
            btn.Enable(False)
            if moduleID == modModified:
                self.btnRestore.Enable(False)
",if moduleID == modModified :,177
"def ignore_proxy_host(self):
    """"""Check if self.host is in the $no_proxy ignore list.""""""
    if urllib.proxy_bypass(self.host):
        return True
    no_proxy = os.environ.get(""no_proxy"")
    if no_proxy:
        entries = [parse_host_port(x) for x in no_proxy.split("","")]
        for host, port in entries:
            if host.lower() == self.host and port == self.port:
                return True
    return False
",if host . lower ( ) == self . host and port == self . port :,133
"def run(self, _):
    view = self.view
    if not view.settings().get(""terminus_view""):
        return
    terminal = Terminal.from_id(view.id())
    if terminal:
        terminal.close()
        panel_name = terminal.panel_name
        if panel_name:
            window = panel_window(view)
            if window:
                window.destroy_output_panel(panel_name)
        else:
            view.close()
",if panel_name :,128
"def get_docname_for_node(self, node: Node) -> str:
    while node:
        if isinstance(node, nodes.document):
            return self.env.path2doc(node[""source""])
        elif isinstance(node, addnodes.start_of_file):
            return node[""docname""]
        else:
            node = node.parent
    return None  # never reached here. only for type hinting
","if isinstance ( node , nodes . document ) :",110
"def TryMerge(self, d):
    while d.avail() > 0:
        tt = d.getVarInt32()
        if tt == 10:
            self.add_version(d.getPrefixedString())
            continue
        if tt == 0:
            raise ProtocolBuffer.ProtocolBufferDecodeError
        d.skipData(tt)
",if tt == 0 :,90
"def _maybe_female(self, path_elements, female, strict):
    if female:
        if self.has_gender_differences:
            elements = path_elements + [""female""]
            try:
                return self._get_file(elements, "".png"", strict=strict)
            except ValueError:
                if strict:
                    raise
        elif strict:
            raise ValueError(""Pokemon %s has no gender differences"" % self.species_id)
    return self._get_file(path_elements, "".png"", strict=strict)
",if strict :,146
"def OnKeyUp(self, event):
    if self._properties.modifiable:
        if event.GetKeyCode() == wx.WXK_ESCAPE:
            self._cancel_editing()
        elif event.GetKeyCode() == wx.WXK_RETURN:
            self._update_value()
        elif event.GetKeyCode() == wx.WXK_DELETE:
            self.SetValue("""")
    if event.GetKeyCode() != wx.WXK_RETURN:
        # Don't send skip event if enter key is pressed
        # On some platforms this event is sent too late and causes crash
        event.Skip()
",elif event . GetKeyCode ( ) == wx . WXK_DELETE :,145
"def sync_up_to_new_location(self, worker_ip):
    if worker_ip != self.worker_ip:
        logger.debug(""Setting new worker IP to %s"", worker_ip)
        self.set_worker_ip(worker_ip)
        self.reset()
        if not self.sync_up():
            logger.warning(""Sync up to new location skipped. This should not occur."")
    else:
        logger.warning(""Sync attempted to same IP %s."", worker_ip)
",if not self . sync_up ( ) :,126
"def _get_download_link(self, url, download_type=""torrent""):
    links = {
        ""torrent"": """",
        ""magnet"": """",
    }
    try:
        data = self.session.get(url).text
        with bs4_parser(data) as html:
            downloads = html.find(""div"", {""class"": ""download""})
            if downloads:
                for download in downloads.findAll(""a""):
                    link = download[""href""]
                    if link.startswith(""magnet""):
                        links[""magnet""] = link
                    else:
                        links[""torrent""] = urljoin(self.urls[""base_url""], link)
    except Exception:
        pass
    return links[download_type]
",if downloads :,200
"def force_ipv4(self, *args):
    """"""only ipv4 localhost in /etc/hosts""""""
    logg.debug(""checking /etc/hosts for '::1 localhost'"")
    lines = []
    for line in open(self.etc_hosts()):
        if ""::1"" in line:
            newline = re.sub(""\\slocalhost\\s"", "" "", line)
            if line != newline:
                logg.info(""/etc/hosts: '%s' => '%s'"", line.rstrip(), newline.rstrip())
                line = newline
        lines.append(line)
    f = open(self.etc_hosts(), ""w"")
    for line in lines:
        f.write(line)
    f.close()
",if line != newline :,182
"def prepare(self):
    # Maybe the brok is a old daemon one or was already prepared
    # if so, the data is already ok
    if hasattr(self, ""prepared"") and not self.prepared:
        self.data = SafeUnpickler.loads(self.data)
        if hasattr(self, ""instance_id""):
            self.data[""instance_id""] = self.instance_id
    self.prepared = True
","if hasattr ( self , ""instance_id"" ) :",104
"def _test_compute_q0(self):
    # Stub code to search a logq space and figure out logq0 by eyeballing
    # results. This code does not run with the tests. Remove underscore to run.
    sigma = 15
    order = 250
    logqs = np.arange(-290, -270, 1)
    count = 0
    for logq in logqs:
        count += 1
        sys.stdout.write(
            ""\t%0.5g: %0.10g"" % (logq, pate.rdp_gaussian(logq, sigma, order))
        )
        sys.stdout.flush()
        if count % 5 == 0:
            print("""")
",if count % 5 == 0 :,175
"def valid_fieldnames(fieldnames):
    """"""check if fieldnames are valid""""""
    for fieldname in fieldnames:
        if fieldname in canonical_field_names and fieldname == ""source"":
            return True
        elif fieldname in fieldname_map and fieldname_map[fieldname] == ""source"":
            return True
    return False
","if fieldname in canonical_field_names and fieldname == ""source"" :",81
"def ns_provide(self, id_):
    global controllers, layouts
    if id_ == ""_leo_viewrendered"":
        c = self.c
        vr = controllers.get(c.hash()) or ViewRenderedController(c)
        h = c.hash()
        controllers[h] = vr
        if not layouts.get(h):
            layouts[h] = c.db.get(""viewrendered_default_layouts"", (None, None))
        # return ViewRenderedController(self.c)
        return vr
",if not layouts . get ( h ) :,143
"def remove(self, path, config=None, error_on_path=False, defaults=None):
    if not path:
        if error_on_path:
            raise NoSuchSettingsPath()
        return
    if config is not None or defaults is not None:
        if config is None:
            config = self._config
        if defaults is None:
            defaults = dict(self._map.parents)
        chain = HierarchicalChainMap(config, defaults)
    else:
        chain = self._map
    try:
        chain.del_by_path(path)
        self._mark_dirty()
    except KeyError:
        if error_on_path:
            raise NoSuchSettingsPath()
        pass
",if defaults is None :,184
"def _mongo_query_and(self, queries):
    if len(queries) == 1:
        return queries[0]
    query = {}
    for q in queries:
        for k, v in q.items():
            if k not in query:
                query[k] = {}
            if isinstance(v, list):
                # TODO check exists of k in query, may be it should be update
                query[k] = v
            else:
                query[k].update(v)
    return query
","if isinstance ( v , list ) :",141
"def write(self, data):
    self.size -= len(data)
    passon = None
    if self.size > 0:
        self.data.append(data)
    else:
        if self.size:
            data, passon = data[: self.size], data[self.size :]
        else:
            passon = b""""
        if data:
            self.data.append(data)
    return passon
",if self . size :,114
"def updateVar(name, data, mode=None):
    if mode:
        if mode == ""append"":
            core.config.globalVariables[name].append(data)
        elif mode == ""add"":
            core.config.globalVariables[name].add(data)
    else:
        core.config.globalVariables[name] = data
","elif mode == ""add"" :",91
"def vi_pos_back_short(line, index=0, count=1):
    line = vi_list(line)
    try:
        for i in range(count):
            index -= 1
            while vi_is_space(line[index]):
                index -= 1
            in_word = vi_is_word(line[index])
            if in_word:
                while vi_is_word(line[index]):
                    index -= 1
            else:
                while not vi_is_word_or_space(line[index]):
                    index -= 1
        return index + 1
    except IndexError:
        return 0
",if in_word :,179
"def _truncate_to_length(generator, len_map=None):
    for example in generator:
        example = list(example)
        if len_map is not None:
            for key, max_len in len_map.items():
                example_len = example[key].shape
                if example_len > max_len:
                    example[key] = np.resize(example[key], max_len)
        yield tuple(example)
",if len_map is not None :,120
"def decorate(f):
    # call-signature of f is exposed via __wrapped__.
    # we want it to mimic Obj.__init__
    f.__wrapped__ = Obj.__init__
    f._uses_signature = Obj
    # Supplement the docstring of f with information from Obj
    if Obj.__doc__:
        doclines = Obj.__doc__.splitlines()
        if f.__doc__:
            doc = f.__doc__ + ""\n"".join(doclines[1:])
        else:
            doc = ""\n"".join(doclines)
        try:
            f.__doc__ = doc
        except AttributeError:
            # __doc__ is not modifiable for classes in Python < 3.3
            pass
    return f
",if f . __doc__ :,192
"def IncrementErrorCount(self, category):
    """"""Bumps the module's error statistic.""""""
    self.error_count += 1
    if self.counting in (""toplevel"", ""detailed""):
        if self.counting != ""detailed"":
            category = category.split(""/"")[0]
        if category not in self.errors_by_category:
            self.errors_by_category[category] = 0
        self.errors_by_category[category] += 1
",if category not in self . errors_by_category :,115
"def _delete_fields(self, data):
    data = self._del(
        data, [""speaker_ids"", ""track_id"", ""microlocation_id"", ""session_type_id""]
    )
    # convert datetime fields
    for _ in [""start_time_tz"", ""end_time_tz""]:
        if _ in data:
            data[_] = SESSION_POST[_[0:-3]].from_str(data[_])
            data[_[0:-3]] = data.pop(_)
    return data
",if _ in data :,128
"def get_strings_of_set(word, char_set, threshold=20):
    count = 0
    letters = """"
    strings = []
    for char in word:
        if char in char_set:
            letters += char
            count += 1
        else:
            if count > threshold:
                strings.append(letters)
            letters = """"
            count = 0
    if count > threshold:
        strings.append(letters)
    return strings
",if count > threshold :,125
"def _ArgumentListHasDictionaryEntry(self, token):
    """"""Check if the function argument list has a dictionary as an arg.""""""
    if _IsArgumentToFunction(token):
        while token:
            if token.value == ""{"":
                length = token.matching_bracket.total_length - token.total_length
                return length + self.stack[-2].indent > self.column_limit
            if token.ClosesScope():
                break
            if token.OpensScope():
                token = token.matching_bracket
            token = token.next_token
    return False
",if token . ClosesScope ( ) :,153
"def check_apns_certificate(ss):
    mode = ""start""
    for s in ss.split(""\n""):
        if mode == ""start"":
            if ""BEGIN RSA PRIVATE KEY"" in s or ""BEGIN PRIVATE KEY"" in s:
                mode = ""key""
        elif mode == ""key"":
            if ""END RSA PRIVATE KEY"" in s or ""END PRIVATE KEY"" in s:
                mode = ""end""
                break
            elif s.startswith(""Proc-Type"") and ""ENCRYPTED"" in s:
                raise ImproperlyConfigured(
                    ""Encrypted APNS private keys are not supported""
                )
    if mode != ""end"":
        raise ImproperlyConfigured(""The APNS certificate doesn't contain a private key"")
","elif s . startswith ( ""Proc-Type"" ) and ""ENCRYPTED"" in s :",195
"def main(self):
    self.model.clear()
    self.callman.unregister_all()
    active_handle = self.get_active(""Person"")
    if active_handle:
        active = self.dbstate.db.get_person_from_handle(active_handle)
        if active:
            self.callman.register_obj(active)
            self.display_citations(active)
        else:
            self.set_has_data(False)
    else:
        self.set_has_data(False)
",if active :,141
"def _validate(self) -> None:
    # Paren validation and such
    super(Tuple, self)._validate()
    if len(self.elements) == 0:
        if len(self.lpar) == 0:  # assumes len(lpar) == len(rpar), via superclass
            raise CSTValidationError(
                ""A zero-length tuple must be wrapped in parentheses.""
            )
",if len ( self . lpar ) == 0 :,101
"def _session_from_arg(self, session_obj, lock_type=None):
    if not isinstance(session_obj, self.ISession):
        vm = self._machine_from_arg(session_obj)
        lock_type = lock_type or self.LockType.null
        if vm:
            return vm.create_session(lock_type)
        return None
    return session_obj
",if vm :,102
"def _decorator(cls):
    for name, meth in inspect.getmembers(cls, inspect.isroutine):
        if name not in cls.__dict__:
            continue
        if name != ""__init__"":
            if not private and name.startswith(""_""):
                continue
        if name in butnot:
            continue
        setattr(cls, name, decorator(meth))
    return cls
","if name != ""__init__"" :",99
"def pdb(message=""""):
    """"""Fall into pdb.""""""
    import pdb  # Required: we have just defined pdb as a function!
    if app and not app.useIpython:
        # from leo.core.leoQt import QtCore
        # This is more portable.
        try:
            import PyQt5.QtCore as QtCore
        except ImportError:
            try:
                import PyQt4.QtCore as QtCore
            except ImportError:
                QtCore = None
        if QtCore:
            # pylint: disable=no-member
            QtCore.pyqtRemoveInputHook()
    if message:
        print(message)
    pdb.set_trace()
",if QtCore :,183
"def get_s3_bucket_locations(buckets, self_log=False):
    """"""return (bucket_name, prefix) for all s3 logging targets""""""
    for b in buckets:
        if b.get(""Logging""):
            if self_log:
                if b[""Name""] != b[""Logging""][""TargetBucket""]:
                    continue
            yield (b[""Logging""][""TargetBucket""], b[""Logging""][""TargetPrefix""])
        if not self_log and b[""Name""].startswith(""cf-templates-""):
            yield (b[""Name""], """")
",if self_log :,138
"def prepare_fields(self):
    # See clean()
    for k, v in self.fields.items():
        v._required = v.required
        v.required = False
        v.widget.is_required = False
        if isinstance(v, I18nFormField):
            v._required = v.one_required
            v.one_required = False
            v.widget.enabled_locales = self.locales
","if isinstance ( v , I18nFormField ) :",110
"def __pack__(self):
    new_values = []
    for i in xrange(len(self.__unpacked_data_elms__)):
        for key in self.__keys__[i]:
            new_val = getattr(self, key)
            old_val = self.__unpacked_data_elms__[i]
            # In the case of Unions, when the first changed value
            # is picked the loop is exited
            if new_val != old_val:
                break
        new_values.append(new_val)
    return struct.pack(self.__format__, *new_values)
",if new_val != old_val :,153
"def run(self):
    pwd_found = []
    if constant.user_dpapi and constant.user_dpapi.unlocked:
        main_vault_directory = os.path.join(
            constant.profile[""APPDATA""], u"".."", u""Local"", u""Microsoft"", u""Vault""
        )
        if os.path.exists(main_vault_directory):
            for vault_directory in os.listdir(main_vault_directory):
                cred = constant.user_dpapi.decrypt_vault(
                    os.path.join(main_vault_directory, vault_directory)
                )
                if cred:
                    pwd_found.append(cred)
    return pwd_found
",if os . path . exists ( main_vault_directory ) :,197
"def on_revision_plugin_revision_pre_save(**kwargs):
    instance = kwargs[""instance""]
    if kwargs.get(""created"", False):
        update_previous_revision = (
            not instance.previous_revision
            and instance.plugin
            and instance.plugin.current_revision
            and instance.plugin.current_revision != instance
        )
        if update_previous_revision:
            instance.previous_revision = instance.plugin.current_revision
    if not instance.revision_number:
        try:
            previous_revision = instance.plugin.revision_set.latest()
            instance.revision_number = previous_revision.revision_number + 1
        except RevisionPluginRevision.DoesNotExist:
            instance.revision_number = 1
",if update_previous_revision :,194
"def __setattr__(self, name, value):
    super().__setattr__(name, value)
    field = self._fields.get(name)
    if field:
        self.check_field_type(field, value)
        if name in self.__ast_frozen_fields__:
            raise TypeError(f""cannot set immutable {name} on {self!r}"")
",if name in self . __ast_frozen_fields__ :,88
"def _check_for_req_data(data):
    required_args = [""columns""]
    for arg in required_args:
        if arg not in data or (isinstance(data[arg], list) and len(data[arg]) < 1):
            return True, make_json_response(
                status=400,
                success=0,
                errormsg=gettext(""Could not find required parameter ({})."").format(arg),
            )
    return False, """"
","if arg not in data or ( isinstance ( data [ arg ] , list ) and len ( data [ arg ] ) < 1 ) :",123
"def train_dict(self, triples):
    """"""Train a dict lemmatizer given training (word, pos, lemma) triples.""""""
    # accumulate counter
    ctr = Counter()
    ctr.update([(p[0], p[1], p[2]) for p in triples])
    # find the most frequent mappings
    for p, _ in ctr.most_common():
        w, pos, l = p
        if (w, pos) not in self.composite_dict:
            self.composite_dict[(w, pos)] = l
        if w not in self.word_dict:
            self.word_dict[w] = l
    return
","if ( w , pos ) not in self . composite_dict :",158
"def render(type_, obj, context):
    if type_ == ""foreign_key"":
        return None
    if type_ == ""column"":
        if obj.name == ""y"":
            return None
        elif obj.name == ""q"":
            return False
        else:
            return ""col(%s)"" % obj.name
    if type_ == ""type"" and isinstance(obj, MySpecialType):
        context.imports.add(""from mypackage import MySpecialType"")
        return ""MySpecialType()""
    return ""render:%s"" % type_
","elif obj . name == ""q"" :",144
"def test_knows_when_stepping_back_possible(self):
    iterator = bidirectional_iterator.BidirectionalIterator([0, 1, 2, 3])
    commands = [0, 1, 0, 0, 1, 1, 0, 0, 0, 0]
    command_count = 0
    results = []
    for _ in iterator:
        if commands[command_count]:
            iterator.step_back_on_next_iteration()
        results.append(iterator.can_step_back())
        command_count += 1
    assert results == [False, True, False, True, True, True, False, True, True, True]
",if commands [ command_count ] :,157
"def flask_debug_true(context):
    if context.is_module_imported_like(""flask""):
        if context.call_function_name_qual.endswith("".run""):
            if context.check_call_arg_value(""debug"", ""True""):
                return bandit.Issue(
                    severity=bandit.HIGH,
                    confidence=bandit.MEDIUM,
                    text=""A Flask app appears to be run with debug=True, ""
                    ""which exposes the Werkzeug debugger and allows ""
                    ""the execution of arbitrary code."",
                    lineno=context.get_lineno_for_call_arg(""debug""),
                )
","if context . check_call_arg_value ( ""debug"" , ""True"" ) :",181
"def __exit__(self, exc_type, exc_val, exc_tb):
    if self._should_meta_profile:
        end_time = timezone.now()
        exception_raised = exc_type is not None
        if exception_raised:
            Logger.error(
                ""Exception when performing meta profiling, dumping trace below""
            )
            traceback.print_exception(exc_type, exc_val, exc_tb)
        request = getattr(DataCollector().local, ""request"", None)
        if request:
            curr = request.meta_time or 0
            request.meta_time = curr + _time_taken(self.start_time, end_time)
",if request :,176
"def get_job_offer(ja_list):
    ja_joff_map = {}
    offers = frappe.get_all(
        ""Job Offer"",
        filters=[[""job_applicant"", ""IN"", ja_list]],
        fields=[""name"", ""job_applicant"", ""status"", ""offer_date"", ""designation""],
    )
    for offer in offers:
        if offer.job_applicant not in ja_joff_map.keys():
            ja_joff_map[offer.job_applicant] = [offer]
        else:
            ja_joff_map[offer.job_applicant].append(offer)
    return ja_joff_map
",if offer . job_applicant not in ja_joff_map . keys ( ) :,176
"def _get_deepest(self, t):
    if isinstance(t, list):
        if len(t) == 1:
            return t[0]
        else:
            for part in t:
                res = self._get_deepest(part)
                if res:
                    return res
            return None
    return None
",if len ( t ) == 1 :,95
"def test_main(self):
    root = os.path.dirname(mutagen.__path__[0])
    skip = [os.path.join(root, ""docs""), os.path.join(root, ""venv"")]
    for dirpath, dirnames, filenames in os.walk(root):
        if any((dirpath.startswith(s + os.sep) or s == dirpath) for s in skip):
            continue
        for filename in filenames:
            if filename.endswith("".py""):
                path = os.path.join(dirpath, filename)
                self._check_encoding(path)
",if any ( ( dirpath . startswith ( s + os . sep ) or s == dirpath ) for s in skip ) :,146
"def xview(self, mode=None, value=None, units=None):
    if type(value) == str:
        value = float(value)
    if mode is None:
        return self.hsb.get()
    elif mode == ""moveto"":
        frameWidth = self.innerframe.winfo_reqwidth()
        self._startX = value * float(frameWidth)
    else:  # mode == 'scroll'
        clipperWidth = self._clipper.winfo_width()
        if units == ""units"":
            jump = int(clipperWidth * self._jfraction)
        else:
            jump = clipperWidth
        self._startX = self._startX + value * jump
    self.reposition()
","if units == ""units"" :",181
"def test_training_script_with_max_history_set(tmpdir):
    train_dialogue_model(
        DEFAULT_DOMAIN_PATH,
        DEFAULT_STORIES_FILE,
        tmpdir.strpath,
        interpreter=RegexInterpreter(),
        policy_config=""data/test_config/max_hist_config.yml"",
        kwargs={},
    )
    agent = Agent.load(tmpdir.strpath)
    for policy in agent.policy_ensemble.policies:
        if hasattr(policy.featurizer, ""max_history""):
            if type(policy) == FormPolicy:
                assert policy.featurizer.max_history == 2
            else:
                assert policy.featurizer.max_history == 5
","if hasattr ( policy . featurizer , ""max_history"" ) :",191
"def generate_auto_complete(self, base, iterable_var):
    sugg = []
    for entry in iterable_var:
        compare_entry = entry
        compare_base = base
        if self.settings.get(IGNORE_CASE_SETTING):
            compare_entry = compare_entry.lower()
            compare_base = compare_base.lower()
        if self.compare_entries(compare_entry, compare_base):
            if entry not in sugg:
                sugg.append(entry)
    return sugg
",if self . settings . get ( IGNORE_CASE_SETTING ) :,137
"def marker_expr(remaining):
    if remaining and remaining[0] == ""("":
        result, remaining = marker(remaining[1:].lstrip())
        if remaining[0] != "")"":
            raise SyntaxError(""unterminated parenthesis: %s"" % remaining)
        remaining = remaining[1:].lstrip()
    else:
        lhs, remaining = marker_var(remaining)
        while remaining:
            m = MARKER_OP.match(remaining)
            if not m:
                break
            op = m.groups()[0]
            remaining = remaining[m.end() :]
            rhs, remaining = marker_var(remaining)
            lhs = {""op"": op, ""lhs"": lhs, ""rhs"": rhs}
        result = lhs
    return result, remaining
","if remaining [ 0 ] != "")"" :",196
"def __repr__(self):
    """"""Dump the class data in the format of a .netrc file.""""""
    rep = """"
    for host in self.hosts.keys():
        attrs = self.hosts[host]
        rep = rep + ""machine "" + host + ""\n\tlogin "" + repr(attrs[0]) + ""\n""
        if attrs[1]:
            rep = rep + ""account "" + repr(attrs[1])
        rep = rep + ""\tpassword "" + repr(attrs[2]) + ""\n""
    for macro in self.macros.keys():
        rep = rep + ""macdef "" + macro + ""\n""
        for line in self.macros[macro]:
            rep = rep + line
        rep = rep + ""\n""
    return rep
",if attrs [ 1 ] :,192
"def _parse_policies(self, policies_yaml):
    for item in policies_yaml:
        id_ = required_key(item, ""id"")
        controls_ids = required_key(item, ""controls"")
        if not isinstance(controls_ids, list):
            if controls_ids != ""all"":
                msg = ""Policy {id_} contains invalid controls list {controls}."".format(
                    id_=id_, controls=str(controls_ids)
                )
                raise ValueError(msg)
        self.policies[id_] = controls_ids
","if not isinstance ( controls_ids , list ) :",155
"def __set__(self, obj, value):  # noqa
    if (
        value is not None
        and self.field._currency_field.null
        and not isinstance(value, MONEY_CLASSES + (Decimal,))
    ):
        # For nullable fields we need either both NULL amount and currency or both NOT NULL
        raise ValueError(""Missing currency value"")
    if isinstance(value, BaseExpression):
        if isinstance(value, Value):
            value = self.prepare_value(obj, value.value)
        elif not isinstance(value, Func):
            validate_money_expression(obj, value)
            prepare_expression(value)
    else:
        value = self.prepare_value(obj, value)
    obj.__dict__[self.field.name] = value
","if isinstance ( value , Value ) :",193
"def Children(self):
    """"""Returns a list of all of this object's owned (strong) children.""""""
    children = []
    for property, attributes in self._schema.iteritems():
        (is_list, property_type, is_strong) = attributes[0:3]
        if is_strong and property in self._properties:
            if not is_list:
                children.append(self._properties[property])
            else:
                children.extend(self._properties[property])
    return children
",if is_strong and property in self . _properties :,130
"def next_item(self, direction):
    """"""Selects next menu item, based on self._direction""""""
    start, i = -1, 0
    try:
        start = self.items.index(self._selected)
        i = start + direction
    except:
        pass
    while True:
        if i == start:
            # Cannot find valid menu item
            self.select(start)
            break
        if i >= len(self.items):
            i = 0
            continue
        if i < 0:
            i = len(self.items) - 1
            continue
        if self.select(i):
            break
        i += direction
        if start < 0:
            start = 0
",if start < 0 :,194
"def setup_displace(self):
    self.displace_mod = None
    self.displace_strength = 0.020
    for mod in self.obj.modifiers:
        if mod.type == ""DISPLACE"":
            self.displace_mod = mod
            self.displace_strength = mod.strength
    if not self.displace_mod:
        bpy.ops.object.modifier_add(type=""DISPLACE"")
        self.displace_mod = self.obj.modifiers[-1]
        self.displace_mod.show_expanded = False
        self.displace_mod.strength = self.displace_strength
        self.displace_mod.show_render = False
        self.displace_mod.show_viewport = False
","if mod . type == ""DISPLACE"" :",195
"def set_json_body(cls, request_builder):
    old_body = request_builder.info.pop(""data"", {})
    if isinstance(old_body, abc.Mapping):
        body = request_builder.info.setdefault(""json"", {})
        for path in old_body:
            if isinstance(path, tuple):
                cls._sequence_path_resolver(path, old_body[path], body)
            else:
                body[path] = old_body[path]
    else:
        request_builder.info.setdefault(""json"", old_body)
","if isinstance ( path , tuple ) :",147
"def build(opt):
    dpath = os.path.join(opt[""datapath""], ""DBLL"")
    version = None
    if not build_data.built(dpath, version_string=version):
        print(""[building data: "" + dpath + ""]"")
        if build_data.built(dpath):
            # An older version exists, so remove these outdated files.
            build_data.remove_dir(dpath)
        build_data.make_dir(dpath)
        # Download the data.
        for downloadable_file in RESOURCES:
            downloadable_file.download_file(dpath)
        # Mark the data as built.
        build_data.mark_done(dpath, version_string=version)
",if build_data . built ( dpath ) :,181
"def test_prefix_lm(self):
    num_tries = 100
    original = ""This is a long test with lots of words to see if it works ok.""
    dataset = tf.data.Dataset.from_tensor_slices({""text"": [original] * num_tries})
    dataset = prep.prefix_lm(dataset)
    for data in test_utils.dataset_as_text(dataset):
        inputs = data[""inputs""].replace(""prefix: "", """")
        targets = data[""targets""]
        reconstructed = """".join(inputs)
        if inputs:
            reconstructed += "" ""
        reconstructed += """".join(targets)
        self.assertEqual(reconstructed, original)
",if inputs :,162
"def leading_whitespace(self, inputstring):
    """"""Get leading whitespace.""""""
    leading_ws = []
    for i, c in enumerate(inputstring):
        if c in legal_indent_chars:
            leading_ws.append(c)
        else:
            break
        if self.indchar is None:
            self.indchar = c
        elif c != self.indchar:
            self.strict_err_or_warn(""found mixing of tabs and spaces"", inputstring, i)
    return """".join(leading_ws)
",if self . indchar is None :,139
"def __init__(self, text):
    self.mappings = {}
    self.attributes = collections.defaultdict(set)
    for stanza in _ParseTextProperties(text):
        processor_id, single_values, multiple_values = self._ParseStanza(stanza)
        if processor_id is None:  # can be 0
            continue
        if processor_id in self.mappings:
            logging.warn(""Processor id %s seen twice in %s"", processor_id, text)
            continue
        self.mappings[processor_id] = single_values
        for key, value in multiple_values.items():
            self.attributes[key].add(value)
",if processor_id in self . mappings :,172
"def __iter__(self):
    for chunk in self.source:
        if chunk is not None:
            self.wait_counter = 0
            yield chunk
        elif self.wait_counter < self.wait_cntr_max:
            self.wait_counter += 1
        else:
            logger.warning(
                ""Data poller has been receiving no data for {} seconds.\n""
                ""Closing data poller"".format(self.wait_cntr_max * self.poll_period)
            )
            break
        time.sleep(self.poll_period)
",if chunk is not None :,156
"def download(self, prefetch=False):
    while self.running:
        try:
            if prefetch:
                (path, start, end) = self.prefetch_queue.get(
                    True, 1
                )  # 1 second time-out
            else:
                (path, start, end) = self.download_queue.get(
                    True, 1
                )  # 1 second time-out
            self.download_data(path, start, end)
            if prefetch:
                self.prefetch_queue.task_done()
            else:
                self.download_queue.task_done()
        except Queue.Empty:
            pass
",if prefetch :,193
"def process_messages(self, found_files, messages):
    for message in messages:
        if self.config.absolute_paths:
            message.to_absolute_path(self.config.workdir)
        else:
            message.to_relative_path(self.config.workdir)
    if self.config.blending:
        messages = blender.blend(messages)
    filepaths = found_files.iter_module_paths(abspath=False)
    return postfilter.filter_messages(filepaths, self.config.workdir, messages)
",if self . config . absolute_paths :,139
"def set_indentation_params(self, ispythonsource, guess=1):
    if guess and ispythonsource:
        i = self.guess_indent()
        if 2 <= i <= 8:
            self.indentwidth = i
        if self.indentwidth != self.tabwidth:
            self.usetabs = 0
    self.editwin.set_tabwidth(self.tabwidth)
",if 2 <= i <= 8 :,100
"def to_tree(self, tagname=None, value=None, namespace=None):
    namespace = getattr(self, ""namespace"", namespace)
    if value is not None:
        if namespace is not None:
            tagname = ""{%s}%s"" % (namespace, tagname)
        el = Element(tagname)
        el.text = safe_string(value)
        return el
",if namespace is not None :,96
"def execute(self, argv: List) -> bool:
    if not argv:
        print(""ERROR: You must give at least one module to download."")
        return False
    for _arg in argv:
        result = module_server.search_module(_arg)
        CacheUpdater(""hub_download"", _arg).start()
        if result:
            url = result[0][""url""]
            with log.ProgressBar(""Download {}"".format(url)) as bar:
                for file, ds, ts in utils.download_with_progress(url):
                    bar.update(float(ds) / ts)
        else:
            print(""ERROR: Could not find a HubModule named {}"".format(_arg))
    return True
",if result :,185
"def visit_type_type(self, t: TypeType) -> ProperType:
    if isinstance(self.s, TypeType):
        typ = self.meet(t.item, self.s.item)
        if not isinstance(typ, NoneType):
            typ = TypeType.make_normalized(typ, line=t.line)
        return typ
    elif isinstance(self.s, Instance) and self.s.type.fullname == ""builtins.type"":
        return t
    elif isinstance(self.s, CallableType):
        return self.meet(t, self.s)
    else:
        return self.default(self.s)
","if not isinstance ( typ , NoneType ) :",154
"def run(self, paths=[]):
    items = []
    for item in SideBarSelection(paths).getSelectedItems():
        items.append(item.name())
    if len(items) > 0:
        sublime.set_clipboard(""\n"".join(items))
        if len(items) > 1:
            sublime.status_message(""Items copied"")
        else:
            sublime.status_message(""Item copied"")
",if len ( items ) > 1 :,113
"def get_icon(self):
    if self.icon is not None:
        # Load it from an absolute filename
        if os.path.exists(self.icon):
            try:
                return GdkPixbuf.Pixbuf.new_from_file_at_size(self.icon, 24, 24)
            except GObject.GError as ge:
                pass
        # Load it from the current icon theme
        (icon_name, extension) = os.path.splitext(os.path.basename(self.icon))
        theme = Gtk.IconTheme()
        if theme.has_icon(icon_name):
            return theme.load_icon(icon_name, 24, 0)
",if os . path . exists ( self . icon ) :,174
"def setup_logger():
    """"""Set up logger and add stdout handler""""""
    logging.setLoggerClass(IPDLogger)
    logger = logging.getLogger(""icloudpd"")
    has_stdout_handler = False
    for handler in logger.handlers:
        if handler.name == ""stdoutLogger"":
            has_stdout_handler = True
    if not has_stdout_handler:
        formatter = logging.Formatter(
            fmt=""%(asctime)s %(levelname)-8s %(message)s"", datefmt=""%Y-%m-%d %H:%M:%S""
        )
        stdout_handler = logging.StreamHandler(stream=sys.stdout)
        stdout_handler.setFormatter(formatter)
        stdout_handler.name = ""stdoutLogger""
        logger.addHandler(stdout_handler)
    return logger
","if handler . name == ""stdoutLogger"" :",195
"def process_extra_fields(self):
    if self.instance.pk is not None:
        if self.cleaned_data.get(""initialize"", None):
            self.instance.initialize()
        if self.cleaned_data.get(""update"", None) or not self.instance.stores.count():
            self.instance.update_from_templates()
","if self . cleaned_data . get ( ""update"" , None ) or not self . instance . stores . count ( ) :",88
"def testFunctions(self):
    from zim.formats.wiki import match_url, is_url
    for input, input_is_url, tail in self.examples:
        if input_is_url:
            if tail:
                self.assertEqual(match_url(input), input[: -len(tail)])
                self.assertFalse(is_url(input))
            else:
                self.assertEqual(match_url(input), input)
                self.assertTrue(is_url(input))
        else:
            self.assertEqual(match_url(input), None)
            self.assertFalse(is_url(input))
",if tail :,168
"def _SetUser(self, users):
    for user in users.items():
        username = user[0]
        settings = user[1]
        room = settings[""room""][""name""] if ""room"" in settings else None
        file_ = settings[""file""] if ""file"" in settings else None
        if ""event"" in settings:
            if ""joined"" in settings[""event""]:
                self._client.userlist.addUser(username, room, file_)
            elif ""left"" in settings[""event""]:
                self._client.removeUser(username)
        else:
            self._client.userlist.modUser(username, room, file_)
","if ""event"" in settings :",170
"def restoreTerminals(self, state):
    for name in list(self.terminals.keys()):
        if name not in state:
            self.removeTerminal(name)
    for name, opts in state.items():
        if name in self.terminals:
            term = self[name]
            term.setOpts(**opts)
            continue
        try:
            opts = strDict(opts)
            self.addTerminal(name, **opts)
        except:
            printExc(""Error restoring terminal %s (%s):"" % (str(name), str(opts)))
",if name not in state :,150
"def htmlify(path, text):
    fname = os.path.basename(path)
    if any((fnmatch.fnmatchcase(fname, p) for p in _patterns)):
        # Get file_id, skip if not in database
        sql = ""SELECT files.id FROM files WHERE path = ? LIMIT 1""
        row = _conn.execute(sql, (path,)).fetchone()
        if row:
            return ClangHtmlifier(_tree, _conn, path, text, row[0])
    return None
",if row :,127
"def autoformat_filter_conv2d(fsize, in_depth, out_depth):
    if isinstance(fsize, int):
        return [fsize, fsize, in_depth, out_depth]
    elif isinstance(fsize, (tuple, list, tf.TensorShape)):
        if len(fsize) == 2:
            return [fsize[0], fsize[1], in_depth, out_depth]
        else:
            raise Exception(
                ""filter length error: ""
                + str(len(fsize))
                + "", only a length of 2 is supported.""
            )
    else:
        raise Exception(""filter format error: "" + str(type(fsize)))
",if len ( fsize ) == 2 :,172
"def _rle_encode(string):
    new = b""""
    count = 0
    for cur in string:
        if not cur:
            count += 1
        else:
            if count:
                new += b""\0"" + bytes([count])
                count = 0
            new += bytes([cur])
    return new
",if not cur :,92
"def is_clean(self):
    acceptable_statuses = {""external"", ""unversioned""}
    root = self._capture_output(""status"", ""--quiet"")
    for elem in root.findall(""./target/entry""):
        status = elem.find(""./wc-status"")
        if status.get(""item"", None) in acceptable_statuses:
            continue
        log.debug(""Path %s is %s"", elem.get(""path""), status.get(""item""))
        return False
    return True
","if status . get ( ""item"" , None ) in acceptable_statuses :",119
"def process(self, body, message):
    try:
        if not isinstance(body, self._handler.message_type):
            raise TypeError(
                'Received an unexpected type ""%s"" for payload.' % type(body)
            )
        response = self._handler.pre_ack_process(body)
        self._dispatcher.dispatch(self._process_message, response)
    except:
        LOG.exception(""%s failed to process message: %s"", self.__class__.__name__, body)
    finally:
        # At this point we will always ack a message.
        message.ack()
","if not isinstance ( body , self . _handler . message_type ) :",152
"def page_file(self, page):
    try:
        page = self.notebook.get_page(page)
        if hasattr(page, ""source"") and isinstance(page.source, File):
            return page.source
        else:
            return None
    except PageNotFoundError:
        return None
","if hasattr ( page , ""source"" ) and isinstance ( page . source , File ) :",79
"def _optimize(self, solutions):
    best_a = None
    best_silhouette = None
    best_k = None
    for a, silhouette, k in solutions():
        if best_silhouette is None:
            pass
        elif silhouette <= best_silhouette:
            break
        best_silhouette = silhouette
        best_a = a
        best_k = k
    return best_a, best_silhouette, best_k
",if best_silhouette is None :,109
"def _cancel_tasks_for_partitions(self, to_cancel_partitions):
    # type: (Iterable[str]) -> None
    with self._lock:
        _LOGGER.debug(
            ""EventProcessor %r tries to cancel partitions %r"",
            self._id,
            to_cancel_partitions,
        )
        for partition_id in to_cancel_partitions:
            if partition_id in self._consumers:
                self._consumers[partition_id].stop = True
                _LOGGER.info(
                    ""EventProcessor %r has cancelled partition %r"",
                    self._id,
                    partition_id,
                )
",if partition_id in self . _consumers :,184
"def get_intersect_all(self, refine=False):
    result = None
    for source, parts in self._per_source.items():
        if result is None:
            result = parts
        else:
            result.intersection_update(parts)
    if not result:
        return None
    elif len(result) == 1:
        return list(result)[0].item
    else:
        solids = [p.item for p in result]
        solid = solids[0].fuse(solids[1:])
        if refine:
            solid = solid.removeSplitter()
        return solid
",if result is None :,159
"def geli_detach(self, pool, clear=False):
    failed = 0
    for ed in self.middleware.call_sync(
        ""datastore.query"",
        ""storage.encrypteddisk"",
        [(""encrypted_volume"", ""="", pool[""id""])],
    ):
        dev = ed[""encrypted_provider""]
        try:
            self.geli_detach_single(dev)
        except Exception as ee:
            self.logger.warn(str(ee))
            failed += 1
        if clear:
            try:
                self.geli_clear(dev)
            except Exception as e:
                self.logger.warn(""Failed to clear %s: %s"", dev, e)
    return failed
",if clear :,191
"def compute_lengths(batch_sizes):
    tmp_batch_sizes = np.copy(batch_sizes)
    lengths = []
    while True:
        c = np.count_nonzero(tmp_batch_sizes > 0)
        if c == 0:
            break
        lengths.append(c)
        tmp_batch_sizes = np.array([b - 1 for b in tmp_batch_sizes])
    return np.array(lengths)
",if c == 0 :,111
"def _render_raw_list(bytes_items):
    flatten_items = []
    for item in bytes_items:
        if item is None:
            flatten_items.append(b"""")
        elif isinstance(item, bytes):
            flatten_items.append(item)
        elif isinstance(item, int):
            flatten_items.append(str(item).encode())
        elif isinstance(item, list):
            flatten_items.append(_render_raw_list(item))
    return b""\n"".join(flatten_items)
",if item is None :,138
"def update(self, new_config):
    jsonschema.validate(new_config, self.schema)
    config = {}
    for k, v in new_config.items():
        if k in self.schema.get(""secret"", []) and v == SECRET_PLACEHOLDER:
            config[k] = self[k]
        else:
            config[k] = v
    self._config = config
    self.changed()
","if k in self . schema . get ( ""secret"" , [ ] ) and v == SECRET_PLACEHOLDER :",108
"def _encode_numpy(values, uniques=None, encode=False, check_unknown=True):
    # only used in _encode below, see docstring there for details
    if uniques is None:
        if encode:
            uniques, encoded = np.unique(values, return_inverse=True)
            return uniques, encoded
        else:
            # unique sorts
            return np.unique(values)
    if encode:
        if check_unknown:
            diff = _encode_check_unknown(values, uniques)
            if diff:
                raise ValueError(""y contains previously unseen labels: %s"" % str(diff))
        encoded = np.searchsorted(uniques, values)
        return uniques, encoded
    else:
        return uniques
",if check_unknown :,190
"def restore_dtype_and_merge(arr, input_dtype):
    if isinstance(arr, list):
        arr = [restore_dtype_and_merge(arr_i, input_dtype) for arr_i in arr]
        shapes = [arr_i.shape for arr_i in arr]
        if len(set(shapes)) == 1:
            arr = np.array(arr)
    if ia.is_np_array(arr):
        arr = iadt.restore_dtypes_(arr, input_dtype)
    return arr
",if len ( set ( shapes ) ) == 1 :,131
"def proc_minute(d):
    if expanded[0][0] != ""*"":
        diff_min = nearest_diff_method(d.minute, expanded[0], 60)
        if diff_min is not None and diff_min != 0:
            if is_prev:
                d += relativedelta(minutes=diff_min, second=59)
            else:
                d += relativedelta(minutes=diff_min, second=0)
            return True, d
    return False, d
",if is_prev :,128
"def _populate_tree(self, element, d):
    """"""Populates an etree with attributes & elements, given a dict.""""""
    for k, v in d.iteritems():
        if isinstance(v, dict):
            self._populate_dict(element, k, v)
        elif isinstance(v, list):
            self._populate_list(element, k, v)
        elif isinstance(v, bool):
            self._populate_bool(element, k, v)
        elif isinstance(v, basestring):
            self._populate_str(element, k, v)
        elif type(v) in [int, float, long, complex]:
            self._populate_number(element, k, v)
","if isinstance ( v , dict ) :",178
"def __createItemAttribute(self, item, function, preload):
    """"""Create the new widget, add it, and remove the old one""""""
    try:
        self.__stack.addWidget(function(item, preload))
        # Remove the widget
        if self.__stack.count() > 1:
            oldWidget = self.__stack.widget(0)
            self.__stack.removeWidget(oldWidget)
            oldWidget.setParent(QtWidgets.QWidget())
    except Exception as e:
        list(map(logger.warning, cuegui.Utils.exceptionOutput(e)))
",if self . __stack . count ( ) > 1 :,145
"def download_main(
    download, download_playlist, urls, playlist, output_dir, merge, info_only
):
    for url in urls:
        if url.startswith(""https://""):
            url = url[8:]
        if not url.startswith(""http://""):
            url = ""http://"" + url
        if playlist:
            download_playlist(
                url, output_dir=output_dir, merge=merge, info_only=info_only
            )
        else:
            download(url, output_dir=output_dir, merge=merge, info_only=info_only)
","if url . startswith ( ""https://"" ) :",155
"def add_enc_zero(obj, enc_zero):
    if isinstance(obj, np.ndarray):
        return obj + enc_zero
    elif isinstance(obj, Iterable):
        return type(obj)(
            EncryptModeCalculator.add_enc_zero(o, enc_zero)
            if isinstance(o, Iterable)
            else o + enc_zero
            for o in obj
        )
    else:
        return obj + enc_zero
","if isinstance ( o , Iterable )",118
"def ensemble(self, pairs, other_preds):
    """"""Ensemble the dict with statistical model predictions.""""""
    lemmas = []
    assert len(pairs) == len(other_preds)
    for p, pred in zip(pairs, other_preds):
        w, pos = p
        if (w, pos) in self.composite_dict:
            lemma = self.composite_dict[(w, pos)]
        elif w in self.word_dict:
            lemma = self.word_dict[w]
        else:
            lemma = pred
        if lemma is None:
            lemma = w
        lemmas.append(lemma)
    return lemmas
",if lemma is None :,164
"def replace_to_6hex(color):
    """"""Validate and replace 3hex colors to 6hex ones.""""""
    if match(r""^#(?:[0-9a-fA-F]{3}){1,2}$"", color):
        if len(color) == 4:
            color = ""#{0}{0}{1}{1}{2}{2}"".format(color[1], color[2], color[3])
        return color
    else:
        exit(_(""Invalid color {}"").format(color))
",if len ( color ) == 4 :,120
"def computeMachineName(self):
    """"""Return the name of the current machine, i.e, HOSTNAME.""""""
    # This is prepended to leoSettings.leo or myLeoSettings.leo
    # to give the machine-specific setting name.
    # How can this be worth doing??
    try:
        import os
        name = os.getenv(""HOSTNAME"")
        if not name:
            name = os.getenv(""COMPUTERNAME"")
        if not name:
            import socket
            name = socket.gethostname()
    except Exception:
        name = """"
    return name
",if not name :,151
"def _git_dirty_working_directory(q, include_untracked):
    try:
        cmd = [""git"", ""status"", ""--porcelain""]
        if include_untracked:
            cmd += [""--untracked-files=normal""]
        else:
            cmd += [""--untracked-files=no""]
        status = _run_git_cmd(cmd)
        if status is not None:
            q.put(bool(status))
        else:
            q.put(None)
    except (subprocess.CalledProcessError, OSError, FileNotFoundError):
        q.put(None)
",if status is not None :,156
"def runAndWaitWork(server, work):
    work.touch()
    thr = threading.Thread(target=workThread, args=(server, work))
    thr.setDaemon(True)
    thr.start()
    # Wait around for done or timeout
    while True:
        if work.isTimedOut():
            break
        # If the thread is done, lets get out.
        if not thr.isAlive():
            break
        # If our parent, or some thread closes stdin,
        # time to pack up and go.
        if sys.stdin.closed:
            break
        time.sleep(2)
",if sys . stdin . closed :,160
"def read(self, count=True, timeout=None, ignore_non_errors=True, ignore_timeouts=True):
    try:
        return self._read(count, timeout)
    except usb.USBError as e:
        if DEBUG_COMM:
            log.info(
                ""read: e.errno=%s e.strerror=%s e.message=%s repr=%s""
                % (e.errno, e.strerror, e.message, repr(e))
            )
        if ignore_timeouts and is_timeout(e):
            return []
        if ignore_non_errors and is_noerr(e):
            return []
        raise
",if ignore_non_errors and is_noerr ( e ) :,174
"def PrintHeader(self):  # print the header array
    if self.draw == False:
        return
    for val in self.parent.header:
        self.SetPrintFont(val[""Font""])
        header_indent = val[""Indent""] * self.pwidth
        text = val[""Text""]
        htype = val[""Type""]
        if htype == ""Date"":
            addtext = self.GetDate()
        elif htype == ""Date & Time"":
            addtext = self.GetDateTime()
        else:
            addtext = """"
        self.OutTextPageWidth(
            text + addtext, self.pheader_margin, val[""Align""], header_indent, True
        )
","if htype == ""Date"" :",184
"def get_intersect_all(self, refine=False):
    result = None
    for source, parts in self._per_source.items():
        if result is None:
            result = parts
        else:
            result.intersection_update(parts)
    if not result:
        return None
    elif len(result) == 1:
        return list(result)[0].item
    else:
        solids = [p.item for p in result]
        solid = solids[0].fuse(solids[1:])
        if refine:
            solid = solid.removeSplitter()
        return solid
",if refine :,159
"def captured_updateNode(self, context):
    if not self.updating_name_from_pointer:
        font_datablock = self.get_bpy_data_from_name(self.fontname, bpy.data.fonts)
        if font_datablock:
            self.font_pointer = font_datablock
            updateNode(self, context)
",if font_datablock :,91
"def __add__(self, other):
    if isinstance(other, Vector2):
        # Vector + Vector -> Vector
        # Vector + Point -> Point
        # Point + Point -> Vector
        if self.__class__ is other.__class__:
            _class = Vector2
        else:
            _class = Point2
        return _class(self.x + other.x, self.y + other.y)
    else:
        assert hasattr(other, ""__len__"") and len(other) == 2
        return Vector2(self.x + other[0], self.y + other[1])
",if self . __class__ is other . __class__ :,150
"def _flatten_settings_from_form(self, settings, form, form_values):
    """"""Take a nested dict and return a flat dict of setting values.""""""
    setting_values = {}
    for field in form.c:
        if isinstance(field, _ContainerMixin):
            setting_values.update(
                self._flatten_settings_from_form(
                    settings, field, form_values[field._name]
                )
            )
        elif field._name in settings:
            setting_values[field._name] = form_values[field._name]
    return setting_values
","if isinstance ( field , _ContainerMixin ) :",156
"def add_include_dirs(self, args):
    ids = []
    for a in args:
        # FIXME same hack, forcibly unpack from holder.
        if hasattr(a, ""includedirs""):
            a = a.includedirs
        if not isinstance(a, IncludeDirs):
            raise InvalidArguments(
                ""Include directory to be added is not an include directory object.""
            )
        ids.append(a)
    self.include_dirs += ids
","if not isinstance ( a , IncludeDirs ) :",120
"def _clip_array(array, config):
    if ""threshold"" in config.keys():
        threshold = config[""threshold""]
    else:
        abs_array = np.max(np.abs(array))
        if abs_array < 1.0:
            return array
        threshold = np.percentile(np.abs(array), 99.99)
    return np.clip(array, -threshold, threshold)
",if abs_array < 1.0 :,103
"def dfs(v: str) -> Iterator[Set[str]]:
    index[v] = len(stack)
    stack.append(v)
    boundaries.append(index[v])
    for w in edges[v]:
        if w not in index:
            yield from dfs(w)
        elif w not in identified:
            while index[w] < boundaries[-1]:
                boundaries.pop()
    if boundaries[-1] == index[v]:
        boundaries.pop()
        scc = set(stack[index[v] :])
        del stack[index[v] :]
        identified.update(scc)
        yield scc
",if w not in index :,162
"def create_balancer(
    self, name, members, protocol=""http"", port=80, algorithm=DEFAULT_ALGORITHM
):
    balancer = self.ex_create_balancer_nowait(name, members, protocol, port, algorithm)
    timeout = 60 * 20
    waittime = 0
    interval = 2 * 15
    if balancer.id is not None:
        return balancer
    else:
        while waittime < timeout:
            balancers = self.list_balancers()
            for i in balancers:
                if i.name == balancer.name and i.id is not None:
                    return i
            waittime += interval
            time.sleep(interval)
    raise Exception(""Failed to get id"")
",if i . name == balancer . name and i . id is not None :,190
"def handle(self, scope: Scope, receive: Receive, send: Send) -> None:
    if self.methods and scope[""method""] not in self.methods:
        if ""app"" in scope:
            raise HTTPException(status_code=405)
        else:
            response = PlainTextResponse(""Method Not Allowed"", status_code=405)
        await response(scope, receive, send)
    else:
        await self.app(scope, receive, send)
","if ""app"" in scope :",116
"def convert(data):
    result = []
    for d in data:
        # noinspection PyCompatibility
        if isinstance(d, tuple) and len(d) == 2:
            result.append((d[0], None, d[1]))
        elif isinstance(d, basestring):
            result.append(d)
    return result
","elif isinstance ( d , basestring ) :",86
"def register_adapters():
    global adapters_registered
    if adapters_registered is True:
        return
    try:
        import pkg_resources
        packageDir = pkg_resources.resource_filename(""pyamf"", ""adapters"")
    except:
        packageDir = os.path.dirname(__file__)
    for f in glob.glob(os.path.join(packageDir, ""*.py"")):
        mod = os.path.basename(f).split(os.path.extsep, 1)[0]
        if mod == ""__init__"" or not mod.startswith(""_""):
            continue
        try:
            register_adapter(mod[1:].replace(""_"", "".""), PackageImporter(mod))
        except ImportError:
            pass
    adapters_registered = True
","if mod == ""__init__"" or not mod . startswith ( ""_"" ) :",188
"def load_modules(
    to_load, load, attr, modules_dict, excluded_aliases, loading_message=None
):
    if loading_message:
        print(loading_message)
    for name in to_load:
        module = load(name)
        if module is None or not hasattr(module, attr):
            continue
        cls = getattr(module, attr)
        if hasattr(cls, ""initialize"") and not cls.initialize():
            continue
        if hasattr(module, ""aliases""):
            for alias in module.aliases():
                if alias not in excluded_aliases:
                    modules_dict[alias] = module
        else:
            modules_dict[name] = module
    if loading_message:
        print()
",if alias not in excluded_aliases :,195
"def clean_items(event, items, variations):
    for item in items:
        if event != item.event:
            raise ValidationError(_(""One or more items do not belong to this event.""))
        if item.has_variations:
            if not any(var.item == item for var in variations):
                raise ValidationError(
                    _(
                        ""One or more items has variations but none of these are in the variations list.""
                    )
                )
",if event != item . event :,127
"def __get_file_by_num(self, num, file_list, idx=0):
    for element in file_list:
        if idx == num:
            return element
        if element[3] and element[4]:
            i = self.__get_file_by_num(num, element[3], idx + 1)
            if not isinstance(i, int):
                return i
            idx = i
        else:
            idx += 1
    return idx
",if idx == num :,127
"def check(chip, xeddb, chipdb):
    all_inst = []
    undoc = []
    for inst in xeddb.recs:
        if inst.isa_set in chipdb[chip]:
            if inst.undocumented:
                undoc.append(inst)
            else:
                all_inst.append(inst)
    return (all_inst, undoc)
",if inst . isa_set in chipdb [ chip ] :,108
"def get_all_topic_src_files(self):
    """"""Retrieves the file paths of all the topics in directory""""""
    topic_full_paths = []
    topic_names = os.listdir(self.topic_dir)
    for topic_name in topic_names:
        # Do not try to load hidden files.
        if not topic_name.startswith("".""):
            topic_full_path = os.path.join(self.topic_dir, topic_name)
            # Ignore the JSON Index as it is stored with topic files.
            if topic_full_path != self.index_file:
                topic_full_paths.append(topic_full_path)
    return topic_full_paths
","if not topic_name . startswith ( ""."" ) :",174
"def _get_element(dom_msi, tag_name, name=None, id_=None):
    """"""Get a xml element defined on Product.""""""
    product = dom_msi.getElementsByTagName(""Product"")[0]
    elements = product.getElementsByTagName(tag_name)
    for element in elements:
        if name and id_:
            if (
                element.getAttribute(""Name"") == name
                and element.getAttribute(""Id"") == id_
            ):
                return element
        elif id_:
            if element.getAttribute(""Id"") == id_:
                return element
",if name and id_ :,153
"def __init__(self, *models):
    super().__init__()
    self.models = ModuleList(models)
    for m in models:
        if not hasattr(m, ""likelihood""):
            raise ValueError(
                ""IndependentModelList currently only supports models that have a likelihood (e.g. ExactGPs)""
            )
    self.likelihood = LikelihoodList(*[m.likelihood for m in models])
","if not hasattr ( m , ""likelihood"" ) :",101
"def _sniff(filename, oxlitype):
    try:
        with open(filename, ""rb"") as fileobj:
            header = fileobj.read(4)
            if header == b""OXLI"":
                fileobj.read(1)  # skip the version number
                ftype = fileobj.read(1)
                if binascii.hexlify(ftype) == oxlitype:
                    return True
        return False
    except OSError:
        return False
","if header == b""OXLI"" :",126
"def convert_port_bindings(port_bindings):
    result = {}
    for k, v in six.iteritems(port_bindings):
        key = str(k)
        if ""/"" not in key:
            key += ""/tcp""
        if isinstance(v, list):
            result[key] = [_convert_port_binding(binding) for binding in v]
        else:
            result[key] = [_convert_port_binding(v)]
    return result
","if isinstance ( v , list ) :",119
"def input_data(self):
    gen = self.config.generator
    # don't try running the generator if we specify an output file explicitly,
    # otherwise generator may segfault and we end up returning the output file anyway
    if gen and (not self.config[""out""] or not self.config[""in""]):
        if self._generated is None:
            self._run_generator(gen, args=self.config.generator_args)
        if self._generated[0]:
            return self._generated[0]
    # in file is optional
    return (
        self._normalize(self.problem.problem_data[self.config[""in""]])
        if self.config[""in""]
        else b""""
    )
",if self . _generated is None :,175
"def __new__(cls, *tasks, **kwargs):
    # This forces `chain(X, Y, Z)` to work the same way as `X | Y | Z`
    if not kwargs and tasks:
        if len(tasks) != 1 or is_list(tasks[0]):
            tasks = tasks[0] if len(tasks) == 1 else tasks
            return reduce(operator.or_, tasks)
    return super(chain, cls).__new__(cls, *tasks, **kwargs)
",if len ( tasks ) != 1 or is_list ( tasks [ 0 ] ) :,118
"def get_file_sources():
    global _file_sources
    if _file_sources is None:
        from galaxy.files import ConfiguredFileSources
        file_sources = None
        if os.path.exists(""file_sources.json""):
            file_sources_as_dict = None
            with open(""file_sources.json"", ""r"") as f:
                file_sources_as_dict = json.load(f)
            if file_sources_as_dict is not None:
                file_sources = ConfiguredFileSources.from_dict(file_sources_as_dict)
        if file_sources is None:
            ConfiguredFileSources.from_dict([])
        _file_sources = file_sources
    return _file_sources
",if file_sources is None :,196
"def InitializeColours(self):
    """"""Initializes the 16 custom colours in :class:`CustomPanel`.""""""
    curr = self._colourData.GetColour()
    self._colourSelection = -1
    for i in range(16):
        c = self._colourData.GetCustomColour(i)
        if c.IsOk():
            self._customColours[i] = self._colourData.GetCustomColour(i)
        else:
            self._customColours[i] = wx.WHITE
        if c == curr:
            self._colourSelection = i
",if c . IsOk ( ) :,147
"def convert_obj_into_marshallable(self, obj):
    if isinstance(obj, self.marshalable_types):
        return obj
    if isinstance(obj, array.array):
        if obj.typecode == ""c"":
            return obj.tostring()
        if obj.typecode == ""u"":
            return obj.tounicode()
        return obj.tolist()
    return self.class_to_dict(obj)
","if obj . typecode == ""u"" :",113
"def run(self):
    self.run_command(""egg_info"")
    from glob import glob
    for pattern in self.match:
        pattern = self.distribution.get_name() + ""*"" + pattern
        files = glob(os.path.join(self.dist_dir, pattern))
        files = [(os.path.getmtime(f), f) for f in files]
        files.sort()
        files.reverse()
        log.info(""%d file(s) matching %s"", len(files), pattern)
        files = files[self.keep :]
        for (t, f) in files:
            log.info(""Deleting %s"", f)
            if not self.dry_run:
                os.unlink(f)
",if not self . dry_run :,188
"def render_token_list(self, tokens):
    result = []
    vars = []
    for token in tokens:
        if token.token_type == TOKEN_TEXT:
            result.append(token.contents.replace(""%"", ""%%""))
        elif token.token_type == TOKEN_VAR:
            result.append(""%%(%s)s"" % token.contents)
            vars.append(token.contents)
    return """".join(result), vars
",elif token . token_type == TOKEN_VAR :,113
"def _handle_raise(self, values, is_NAs, origins):
    for is_NA, origin in zip(is_NAs, origins):
        if np.any(is_NA):
            msg = (
                ""Missing values detected. If you want rows with missing ""
                ""values to be automatically deleted in a list-wise ""
                ""manner (not recommended), please set dropna=True in ""
                ""the Bambi Model initialization.""
            )
            raise PatsyError(msg, origin)
    return values
",if np . any ( is_NA ) :,145
"def add_node_data(node_array, ntwk):
    node_ntwk = nx.Graph()
    newdata = {}
    for idx, data in ntwk.nodes(data=True):
        if not int(idx) == 0:
            newdata[""value""] = node_array[int(idx) - 1]
            data.update(newdata)
            node_ntwk.add_node(int(idx), **data)
    return node_ntwk
",if not int ( idx ) == 0 :,119
"def safe_parse_date(date_hdr):
    """"""Parse a Date: or Received: header into a unix timestamp.""""""
    try:
        if "";"" in date_hdr:
            date_hdr = date_hdr.split("";"")[-1].strip()
        msg_ts = long(rfc822.mktime_tz(rfc822.parsedate_tz(date_hdr)))
        if (msg_ts > (time.time() + 24 * 3600)) or (msg_ts < 1):
            return None
        else:
            return msg_ts
    except (ValueError, TypeError, OverflowError):
        return None
",if ( msg_ts > ( time . time ( ) + 24 * 3600 ) ) or ( msg_ts < 1 ) :,150
"def _route_db(self, model, **hints):
    chosen_db = None
    for router in self.routers:
        try:
            method = getattr(router, action)
        except AttributeError:
            # If the router doesn't have a method, skip to the next one.
            pass
        else:
            chosen_db = method(model, **hints)
            if chosen_db:
                return chosen_db
    try:
        return hints[""instance""]._state.db or DEFAULT_DB_ALIAS
    except KeyError:
        return DEFAULT_DB_ALIAS
",if chosen_db :,154
"def get_keys(struct, ignore_first_level=False):
    res = []
    if isinstance(struct, dict):
        if not ignore_first_level:
            keys = [x.split(""("")[0] for x in struct.keys()]
            res.extend(keys)
        for key in struct:
            if key in IGNORED_KEYS:
                logging.debug(""Ignored: %s: %s"", key, struct[key])
                continue
            res.extend(get_keys(struct[key], key in IGNORED_FIRST_LEVEL))
    elif isinstance(struct, list):
        for item in struct:
            res.extend(get_keys(item))
    return res
",if key in IGNORED_KEYS :,178
"def launch_app(self, fs_id):
    if fs_id in self.app_infos:
        row = self.get_row_by_fsid(fs_id)
        if not row:
            return
        app_info = self.app_infos[fs_id]
        filepath = os.path.join(row[SAVEDIR_COL], row[SAVENAME_COL])
        gfile = Gio.File.new_for_path(filepath)
        app_info.launch(
            [
                gfile,
            ],
            None,
        )
        self.app_infos.pop(fs_id, None)
",if not row :,166
"def create_skipfile(files_changed, skipfile):
    # File is likely to contain some garbage values at start,
    # only the corresponding json should be parsed.
    json_pattern = re.compile(r""^\{.*\}"")
    for line in files_changed.readlines():
        if re.match(json_pattern, line):
            for filename in json.loads(line):
                if ""/COMMIT_MSG"" in filename:
                    continue
                skipfile.write(""+*/%s\n"" % filename)
    skipfile.write(""-*\n"")
","if re . match ( json_pattern , line ) :",142
"def zscore(self, client, request, N):
    check_input(request, N != 2)
    key = request[1]
    db = client.db
    value = db.get(key)
    if value is None:
        client.reply_bulk(None)
    elif not isinstance(value, self.zset_type):
        client.reply_wrongtype()
    else:
        score = value.score(request[2], None)
        if score is not None:
            score = str(score).encode(""utf-8"")
        client.reply_bulk(score)
",if score is not None :,148
"def _list_cases(suite):
    for test in suite:
        if isinstance(test, unittest.TestSuite):
            _list_cases(test)
        elif isinstance(test, unittest.TestCase):
            if support.match_test(test):
                print(test.id())
",if support . match_test ( test ) :,75
"def Run(self):
    """"""The main run method of the client.""""""
    for thread in self._threads.values():
        thread.start()
    logging.info(START_STRING)
    while True:
        dead_threads = [tn for (tn, t) in self._threads.items() if not t.isAlive()]
        if dead_threads:
            raise FatalError(
                ""These threads are dead: %r. Shutting down..."" % dead_threads
            )
        time.sleep(10)
",if dead_threads :,130
"def _slice_queryset(queryset, order_by, per_page, start):
    page_len = int(per_page) + 1
    if start:
        if order_by.startswith(""-""):
            filter_name = ""%s__lte"" % order_by[1:]
        else:
            filter_name = ""%s__gte"" % order_by
        return queryset.filter(**{filter_name: start})[:page_len]
    return queryset[:page_len]
","if order_by . startswith ( ""-"" ) :",118
"def compute_timer_precision(timer):
    precision = None
    points = 0
    timeout = timeout_timer() + 1.0
    previous = timer()
    while timeout_timer() < timeout or points < 5:
        for _ in XRANGE(10):
            t1 = timer()
            t2 = timer()
            dt = t2 - t1
            if 0 < dt:
                break
        else:
            dt = t2 - previous
            if dt <= 0.0:
                continue
        if precision is not None:
            precision = min(precision, dt)
        else:
            precision = dt
        points += 1
        previous = timer()
    return precision
",if 0 < dt :,189
"def findWorkingDir():
    frozen = getattr(sys, ""frozen"", """")
    if not frozen:
        path = os.path.dirname(__file__)
    elif frozen in (""dll"", ""console_exe"", ""windows_exe"", ""macosx_app""):
        path = os.path.dirname(
            os.path.dirname(os.path.dirname(os.path.dirname(__file__)))
        )
    elif frozen:  # needed for PyInstaller
        if getattr(sys, ""_MEIPASS"", """") is not None:
            path = getattr(sys, ""_MEIPASS"", """")  # --onefile
        else:
            path = os.path.dirname(sys.executable)  # --onedir
    else:
        path = """"
    return path
","if getattr ( sys , ""_MEIPASS"" , """" ) is not None :",192
"def CreateDataType(vmodlName, wsdlName, parent, version, props):
    with _lazyLock:
        dic = [vmodlName, wsdlName, parent, version, props]
        names = vmodlName.split(""."")
        if _allowCapitalizedNames:
            vmodlName = ""."".join(name[0].lower() + name[1:] for name in names)
        _AddToDependencyMap(names)
        typeNs = GetWsdlNamespace(version)
        _dataDefMap[vmodlName] = dic
        _wsdlDefMap[(typeNs, wsdlName)] = dic
        _wsdlTypeMapNSs.add(typeNs)
",if _allowCapitalizedNames :,170
"def ParseResponses(
    self,
    knowledge_base: rdf_client.KnowledgeBase,
    responses: Iterable[rdfvalue.RDFValue],
) -> Iterator[rdf_client.User]:
    for response in responses:
        if not isinstance(response, rdf_client_fs.StatEntry):
            raise TypeError(f""Unexpected response type: `{type(response)}`"")
        # TODO: `st_mode` has to be an `int`, not `StatMode`.
        if stat.S_ISDIR(int(response.st_mode)):
            homedir = response.pathspec.path
            username = os.path.basename(homedir)
            if username not in self._ignore_users:
                yield rdf_client.User(username=username, homedir=homedir)
",if username not in self . _ignore_users :,198
"def process_question(qtxt):
    question = """"
    skip = False
    for letter in qtxt:
        if letter == ""<"":
            skip = True
        if letter == "">"":
            skip = False
        if skip:
            continue
        if letter.isalnum() or letter == "" "":
            if letter == "" "":
                letter = ""_""
            question += letter.lower()
    return question
","if letter . isalnum ( ) or letter == "" "" :",110
"def process_all(self, lines, times=1):
    gap = False
    for _ in range(times):
        for line in lines:
            if gap:
                self.write("""")
            self.process(line)
            if not is_command(line):
                gap = True
    return 0
",if not is_command ( line ) :,86
"def _get(self, domain):
    with self.lock:
        try:
            record = self.cache[domain]
            time_now = time.time()
            if time_now - record[""update""] > self.ttl:
                record = None
        except KeyError:
            record = None
        if not record:
            record = {""r"": ""unknown"", ""dns"": {}, ""g"": 1, ""query_count"": 0}
        # self.cache[domain] = record
        return record
",if not record :,137
"def gen_constant_folding(cw):
    types = [""Int32"", ""Double"", ""BigInteger"", ""Complex""]
    for cur_type in types:
        cw.enter_block(""if (constLeft.Value.GetType() == typeof(%s))"" % (cur_type,))
        cw.enter_block(""switch (_op)"")
        for op in ops:
            gen = getattr(op, ""genConstantFolding"", None)
            if gen is not None:
                gen(cw, cur_type)
        cw.exit_block()
        cw.exit_block()
",if gen is not None :,147
"def unreferenced_dummy(self):
    for g, base in zip(self.evgroups, self.evbases):
        for ind, j in enumerate(g):
            if not self.indexobj[base + ind]:
                debug_print(
                    ""replacing unreferenced %d %s with dummy"" % ((base + ind), g[ind])
                )
                g[ind] = ""dummy""
                self.evnum[base + ind] = ""dummy""
",if not self . indexobj [ base + ind ] :,127
"def handle_signature(self, sig: str, signode: desc_signature) -> Tuple[str, str]:
    for cls in self.__class__.__mro__:
        if cls.__name__ != ""DirectiveAdapter"":
            warnings.warn(
                ""PyDecoratorMixin is deprecated. ""
                ""Please check the implementation of %s"" % cls,
                RemovedInSphinx50Warning,
                stacklevel=2,
            )
            break
    else:
        warnings.warn(
            ""PyDecoratorMixin is deprecated"", RemovedInSphinx50Warning, stacklevel=2
        )
    ret = super().handle_signature(sig, signode)  # type: ignore
    signode.insert(0, addnodes.desc_addname(""@"", ""@""))
    return ret
","if cls . __name__ != ""DirectiveAdapter"" :",199
"def _iter_lines(path=path, response=response, max_next=options.http_max_next):
    path.responses = []
    n = 0
    while response:
        path.responses.append(response)
        yield from response.iter_lines(decode_unicode=True)
        src = response.links.get(""next"", {}).get(""url"", None)
        if not src:
            break
        n += 1
        if n > max_next:
            vd.warning(f""stopping at max {max_next} pages"")
            break
        vd.status(f""fetching next page from {src}"")
        response = requests.get(src, stream=True)
",if not src :,179
"def ordered_indices(self):
    with data_utils.numpy_seed(self.seed, self.epoch):
        # Used to store the order of indices of each dataset to use
        indices = [
            np.random.permutation(len(dataset)) for dataset in self.datasets.values()
        ]
        # Keep track of which samples we've  used for each dataset
        counters = [0 for _ in self.datasets]
        sampled_indices = [
            self._sample(indices, counters) for _ in range(self.total_num_instances)
        ]
        if self.sort_indices:
            sampled_indices.sort(key=lambda i: self.num_tokens(i))
        return np.array(sampled_indices, dtype=np.int64)
",if self . sort_indices :,195
"def _build_columns(self):
    self.columns = [Column() for col in self.keys]
    for row in self:
        for (col_idx, col_val) in enumerate(row):
            col = self.columns[col_idx]
            col.append(col_val)
            if (col_val is not None) and (not is_quantity(col_val)):
                col.is_quantity = False
    for (idx, key_name) in enumerate(self.keys):
        self.columns[idx].name = key_name
    self.x = Column()
    self.ys = []
",if ( col_val is not None ) and ( not is_quantity ( col_val ) ) :,158
"def tearDown(self):
    subprocess_list = self.subprocess_list
    processes = subprocess_list.processes
    self.schedule.reset()
    del self.schedule
    for proc in processes:
        if proc.is_alive():
            terminate_process(proc.pid, kill_children=True, slow_stop=True)
    subprocess_list.cleanup()
    processes = subprocess_list.processes
    if processes:
        for proc in processes:
            if proc.is_alive():
                terminate_process(proc.pid, kill_children=True, slow_stop=False)
        subprocess_list.cleanup()
    processes = subprocess_list.processes
    if processes:
        log.warning(""Processes left running: %s"", processes)
",if proc . is_alive ( ) :,187
"def colorNetwork(cls, network, nodesInNetwork, nodeByID=None):
    for node in nodesInNetwork:
        node.use_custom_color = True
        neededCopies = sum(socket.execution.neededCopies for socket in node.outputs)
        if neededCopies == 0:
            color = (0.7, 0.9, 0.7)
        else:
            color = (1.0, 0.3, 0.3)
        node.color = color
",if neededCopies == 0 :,121
"def _init_warmup_scheduler(self, optimizer, states):
    updates_so_far = states.get(""number_training_updates"", 0)
    if self.warmup_updates > 0 and (
        updates_so_far <= self.warmup_updates or self.hard_reset
    ):
        self.warmup_scheduler = optim.lr_scheduler.LambdaLR(optimizer, self._warmup_lr)
        if states.get(""warmup_scheduler""):
            self.warmup_scheduler.load_state_dict(states[""warmup_scheduler""])
    else:
        self.warmup_scheduler = None
","if states . get ( ""warmup_scheduler"" ) :",144
"def inner(self, *iargs, **ikwargs):
    try:
        return getattr(super(VEXResilienceMixin, self), func)(*iargs, **ikwargs)
    except excs as e:
        for exc, handler in zip(excs, handlers):
            if isinstance(e, exc):
                v = getattr(self, handler)(*iargs, **ikwargs)
                if v is raiseme:
                    raise
                return v
        assert False, ""this should be unreachable if Python is working correctly""
",if v is raiseme :,140
"def unwrap_envelope(self, data, many):
    if many:
        if data[""items""]:
            if isinstance(data, InstrumentedList) or isinstance(data, list):
                self.context[""total""] = len(data)
                return data
            else:
                self.context[""total""] = data[""total""]
        else:
            self.context[""total""] = 0
            data = {""items"": []}
        return data[""items""]
    return data
","if isinstance ( data , InstrumentedList ) or isinstance ( data , list ) :",130
"def __subclasscheck__(self, cls):
    if self.__origin__ is not None:
        if sys._getframe(1).f_globals[""__name__""] not in [""abc"", ""functools""]:
            raise TypeError(
                ""Parameterized generics cannot be used with class "" ""or instance checks""
            )
        return False
    if self is Generic:
        raise TypeError(
            ""Class %r cannot be used with class "" ""or instance checks"" % self
        )
    return super().__subclasscheck__(cls)
","if sys . _getframe ( 1 ) . f_globals [ ""__name__"" ] not in [ ""abc"" , ""functools"" ] :",130
"def __init__(self, pyversions, coverage_service):
    build_matrix = """"
    for version in pyversions:
        build_matrix += ""\n    {},"".format(
            version
            if version.startswith(""pypy"")
            else ""py{}"".format("""".join(version.split(""."")))
        )
    coverage_package = """"
    if coverage_service:
        coverage_package += ""\n    {}"".format(coverage_service.package)
    coverage_package += ""\n""
    super(Tox, self).__init__(
        ""tox.ini"",
        TEMPLATE.format(build_matrix=build_matrix, coverage_package=coverage_package),
    )
","if version . startswith ( ""pypy"" )",172
"def _get_app(self, body=None):
    app = self._app
    if app is None:
        try:
            tasks = self.tasks.tasks  # is a group
        except AttributeError:
            tasks = self.tasks
        if len(tasks):
            app = tasks[0]._app
        if app is None and body is not None:
            app = body._app
    return app if app is not None else current_app
",if app is None and body is not None :,117
"def logic():
    for v in [True, False, None, 0, True, None, None, 1]:
        yield clk.posedge
        xd.next = v
        if v is None:
            yd.next = zd.next = None
        elif v:
            yd.next = zd.next = 11
        else:
            yd.next = zd.next = 0
",if v is None :,104
"def run(self):
    eid = self.start_episode()
    obs = self.env.reset()
    while True:
        if random.random() < self.off_pol_frac:
            action = self.env.action_space.sample()
            self.log_action(eid, obs, action)
        else:
            action = self.get_action(eid, obs)
        obs, reward, done, info = self.env.step(action)
        self.log_returns(eid, reward, info=info)
        if done:
            self.end_episode(eid, obs)
            obs = self.env.reset()
            eid = self.start_episode()
",if random . random ( ) < self . off_pol_frac :,187
"def tearDown(self):
    os.chdir(self.orig_working_dir)
    sys.argv = self.orig_argv
    sys.stdout = self.orig_stdout
    sys.stderr = self.orig_stderr
    for dirname in [""lv_LV"", ""ja_JP""]:
        locale_dir = os.path.join(self.datadir, ""project"", ""i18n"", dirname)
        if os.path.isdir(locale_dir):
            shutil.rmtree(locale_dir)
",if os . path . isdir ( locale_dir ) :,122
"def sentry_set_scope(process_context, entity, project, email=None, url=None):
    # Using GLOBAL_HUB means these tags will persist between threads.
    # Normally there is one hub per thread.
    with sentry_sdk.hub.GLOBAL_HUB.configure_scope() as scope:
        scope.set_tag(""process_context"", process_context)
        scope.set_tag(""entity"", entity)
        scope.set_tag(""project"", project)
        if email:
            scope.user = {""email"": email}
        if url:
            scope.set_tag(""url"", url)
",if email :,157
"def getDataMax(self):
    result = -Double.MAX_VALUE
    nCurves = self.chart.getNCurves()
    for i in range(nCurves):
        c = self.getSystemCurve(i)
        if not c.isVisible():
            continue
        if c.getYAxis() == Y_AXIS:
            nPoints = c.getNPoints()
            for j in range(nPoints):
                result = self.maxIgnoreNaNAndMaxValue(result, c.getPoint(j).getY())
    if result == -Double.MAX_VALUE:
        return Double.NaN
    return result
",if not c . isVisible ( ) :,163
"def handle_starttag(self, tag, attrs):
    if tag == ""link"" and (""rel"", ""icon"") in attrs or (""rel"", ""shortcut icon"") in attrs:
        href = None
        icon_type = None
        for attr, value in attrs:
            if attr == ""href"":
                href = value
            elif attr == ""type"":
                icon_type = value
        if href:
            try:
                mimetype = extension_to_mimetype(href.rpartition(""."")[2])
            except KeyError:
                pass
            else:
                icon_type = mimetype
            if icon_type:
                self.icons.append((href, icon_type))
",if href :,188
"def get_version(version_file=STATIC_VERSION_FILE):
    version_info = get_static_version_info(version_file)
    version = version_info[""version""]
    if version == ""__use_git__"":
        version = get_version_from_git()
        if not version:
            version = get_version_from_git_archive(version_info)
        if not version:
            version = Version(""unknown"", None, None)
        return pep440_format(version)
    else:
        return version
",if not version :,137
"def _Sleep(self, seconds):
    if threading.current_thread() is not self._worker_thread:
        return self._original_sleep(seconds)
    self._time += seconds
    self._budget -= seconds
    while self._budget < 0:
        self._worker_thread_turn.clear()
        self._owner_thread_turn.set()
        self._worker_thread_turn.wait()
        if self._worker_thread_done:
            raise FakeTimeline._WorkerThreadExit()
",if self . _worker_thread_done :,127
"def validate_attributes(self):
    if not (self.has_variants or self.variant_of):
        return
    if not self.variant_based_on:
        self.variant_based_on = ""Item Attribute""
    if self.variant_based_on == ""Item Attribute"":
        attributes = []
        if not self.attributes:
            frappe.throw(_(""Attribute table is mandatory""))
        for d in self.attributes:
            if d.attribute in attributes:
                frappe.throw(
                    _(
                        ""Attribute {0} selected multiple times in Attributes Table""
                    ).format(d.attribute)
                )
            else:
                attributes.append(d.attribute)
",if not self . attributes :,197
"def check_digest_auth(user, passwd):
    """"""Check user authentication using HTTP Digest auth""""""
    if request.headers.get(""Authorization""):
        credentails = parse_authorization_header(request.headers.get(""Authorization""))
        if not credentails:
            return
        response_hash = response(
            credentails,
            passwd,
            dict(
                uri=request.script_root + request.path,
                body=request.data,
                method=request.method,
            ),
        )
        if credentails.get(""response"") == response_hash:
            return True
    return False
","if credentails . get ( ""response"" ) == response_hash :",165
"def _get_index_type(return_index_type, ctx):
    if return_index_type is None:  # pragma: no cover
        if ctx.running_mode == RunningMode.local:
            return_index_type = ""object""
        elif ctx.running_mode == RunningMode.local_cluster:
            return_index_type = ""filename""
        else:
            return_index_type = ""bytes""
    return return_index_type
",elif ctx . running_mode == RunningMode . local_cluster :,116
"def iter_event_handlers(
    self,
    resource: resources_.Resource,
    event: bodies.RawEvent,
) -> Iterator[handlers.ResourceWatchingHandler]:
    warnings.warn(
        ""SimpleRegistry.iter_event_handlers() is deprecated; use ""
        ""ResourceWatchingRegistry.iter_handlers()."",
        DeprecationWarning,
    )
    cause = _create_watching_cause(resource, event)
    for handler in self._handlers:
        if not isinstance(handler, handlers.ResourceWatchingHandler):
            pass
        elif registries.match(handler=handler, cause=cause, ignore_fields=True):
            yield handler
","elif registries . match ( handler = handler , cause = cause , ignore_fields = True ) :",160
"def subprocess_post_check(
    completed_process: subprocess.CompletedProcess, raise_error: bool = True
) -> None:
    if completed_process.returncode:
        if completed_process.stdout is not None:
            print(completed_process.stdout, file=sys.stdout, end="""")
        if completed_process.stderr is not None:
            print(completed_process.stderr, file=sys.stderr, end="""")
        if raise_error:
            raise PipxError(
                f""{' '.join([str(x) for x in completed_process.args])!r} failed""
            )
        else:
            logger.info(f""{' '.join(completed_process.args)!r} failed"")
",if completed_process . stdout is not None :,185
"def __pow__(self, power):
    if power == 1:
        return self
    if power == -1:
        # HACK: break cycle
        from cirq.devices import line_qubit
        decomposed = protocols.decompose_once_with_qubits(
            self, qubits=line_qubit.LineQid.for_gate(self), default=None
        )
        if decomposed is None:
            return NotImplemented
        inverse_decomposed = protocols.inverse(decomposed, None)
        if inverse_decomposed is None:
            return NotImplemented
        return _InverseCompositeGate(self)
    return NotImplemented
",if decomposed is None :,164
"def tearDown(self):
    """"""Close the application after tests""""""
    # set it back to it's old position so not to annoy users :-)
    self.old_pos = self.dlg.rectangle
    # close the application
    self.dlg.menu_select(""File->Exit"")
    try:
        if self.app.UntitledNotepad[""Do&n't Save""].exists():
            self.app.UntitledNotepad[""Do&n't Save""].click()
            self.app.UntitledNotepad.wait_not(""visible"")
    except Exception:
        pass
    finally:
        self.app.kill()
","if self . app . UntitledNotepad [ ""Do&n't Save"" ] . exists ( ) :",160
"def terminate_subprocess(proc, timeout=0.1, log=None):
    if proc.poll() is None:
        if log:
            log.info(""Sending SIGTERM to %r"", proc)
        proc.terminate()
        timeout_time = time.time() + timeout
        while proc.poll() is None and time.time() < timeout_time:
            time.sleep(0.02)
        if proc.poll() is None:
            if log:
                log.info(""Sending SIGKILL to %r"", proc)
            proc.kill()
    return proc.returncode
",if proc . poll ( ) is None :,152
"def validate(self, detection, expectation):
    config = SigmaConfiguration()
    self.basic_rule[""detection""] = detection
    with patch(""yaml.safe_load_all"", return_value=[self.basic_rule]):
        parser = SigmaCollectionParser(""any sigma io"", config, None)
        backend = SQLiteBackend(config, self.table)
        assert len(parser.parsers) == 1
        for p in parser.parsers:
            if isinstance(expectation, str):
                self.assertEqual(expectation, backend.generate(p))
            elif isinstance(expectation, Exception):
                self.assertRaises(type(expectation), backend.generate, p)
","if isinstance ( expectation , str ) :",167
"def makelist(d):
    """"""Convert d into a list if all the keys of d are integers.""""""
    if isinstance(d, dict):
        if all(isint(k) for k in d):
            return [makelist(d[k]) for k in sorted(d, key=int)]
        else:
            return web.storage((k, makelist(v)) for k, v in d.items())
    else:
        return d
",if all ( isint ( k ) for k in d ) :,112
"def __share_local_dir(self, lpath, rpath, fast):
    result = const.ENoError
    for walk in self.__walk_normal_file(lpath):
        (dirpath, dirnames, filenames) = walk
        for filename in filenames:
            rpart = os.path.relpath(dirpath, lpath)
            if rpart == ""."":
                rpart = """"
            subr = self.__share_local_file(
                joinpath(dirpath, filename),
                posixpath.join(rpath, rpart, filename),
                fast,
            )
            if subr != const.ENoError:
                result = subr
    return result
",if subr != const . ENoError :,183
"def _targets(self, sigmaparser):
    # build list of matching target mappings
    targets = set()
    for condfield in self.conditions:
        if condfield in sigmaparser.values:
            rulefieldvalues = sigmaparser.values[condfield]
            for condvalue in self.conditions[condfield]:
                if condvalue in rulefieldvalues:
                    targets.update(self.conditions[condfield][condvalue])
    return targets
",if condfield in sigmaparser . values :,115
"def _wrapped_view(request, *args, **kwargs):
    # based on authority/decorators.py
    user = request.user
    if user.is_authenticated():
        obj = _resolve_lookup(obj_lookup, kwargs)
        perm_obj = _resolve_lookup(perm_obj_lookup, kwargs)
        granted = access.has_perm_or_owns(user, perm, obj, perm_obj, owner_attr)
        if granted or user.has_perm(perm):
            return view_func(request, *args, **kwargs)
    # In all other cases, permission denied
    return HttpResponseForbidden()
",if granted or user . has_perm ( perm ) :,157
"def assert_parts_cleaned(self, earlier_parts, current_parts, expected_parts, hint):
    cleaned_parts = []
    for earlier in earlier_parts:
        earlier_part = earlier[""part""]
        earlier_step = earlier[""step""]
        found = False
        for current in current_parts:
            if earlier_part == current[""part""] and earlier_step == current[""step""]:
                found = True
                break
        if not found:
            cleaned_parts.append(dict(part=earlier_part, step=earlier_step))
    self.assertThat(cleaned_parts, HasLength(len(expected_parts)), hint)
    for expected in expected_parts:
        self.assertThat(cleaned_parts, Contains(expected), hint)
","if earlier_part == current [ ""part"" ] and earlier_step == current [ ""step"" ] :",194
"def show_image(self, wnd_name, img):
    if wnd_name in self.named_windows:
        if self.named_windows[wnd_name] == 0:
            self.named_windows[wnd_name] = 1
            self.on_create_window(wnd_name)
            if wnd_name in self.capture_mouse_windows:
                self.capture_mouse(wnd_name)
        self.on_show_image(wnd_name, img)
    else:
        print(""show_image: named_window "", wnd_name, "" not found."")
",if wnd_name in self . capture_mouse_windows :,159
"def readlines(self, hint=None):
    # Again, allow hint but ignore
    body = self._get_body()
    rest = body[self.position :]
    self.position = len(body)
    result = []
    while 1:
        next = rest.find(""\r\n"")
        if next == -1:
            result.append(rest)
            break
        result.append(rest[: next + 2])
        rest = rest[next + 2 :]
    return result
",if next == - 1 :,125
"def __lt__(self, other):
    olen = len(other)
    for i in range(olen):
        try:
            c = self[i] < other[i]
        except IndexError:
            # self must be shorter
            return True
        if c:
            return c
        elif other[i] < self[i]:
            return False
    return len(self) < olen
",elif other [ i ] < self [ i ] :,108
"def social_user(backend, uid, user=None, *args, **kwargs):
    provider = backend.name
    social = backend.strategy.storage.user.get_social_auth(provider, uid)
    if social:
        if user and social.user != user:
            msg = ""This account is already in use.""
            raise AuthAlreadyAssociated(backend, msg)
        elif not user:
            user = social.user
    return {
        ""social"": social,
        ""user"": user,
        ""is_new"": user is None,
        ""new_association"": social is None,
    }
",elif not user :,170
"def markUVs(self, indices=None):
    if isinstance(indices, tuple):
        indices = indices[0]
    ntexco = len(self.texco)
    if indices is None:
        self.utexc = True
    else:
        if self.utexc is False:
            self.utexc = np.zeros(ntexco, dtype=bool)
        if self.utexc is not True:
            self.utexc[indices] = True
",if self . utexc is not True :,120
"def destination(self, type, name, arglist):
    classname = ""ResFunction""
    listname = ""functions""
    if arglist:
        t, n, m = arglist[0]
        if t == ""Handle"" and m == ""InMode"":
            classname = ""ResMethod""
            listname = ""resmethods""
    return classname, listname
","if t == ""Handle"" and m == ""InMode"" :",90
"def select(self, regions, register):
    self.view.sel().clear()
    to_store = []
    for r in regions:
        self.view.sel().add(r)
        if register:
            to_store.append(self.view.substr(self.view.full_line(r)))
    if register:
        text = """".join(to_store)
        if not text.endswith(""\n""):
            text = text + ""\n""
        state = State(self.view)
        state.registers[register] = [text]
","if not text . endswith ( ""\n"" ) :",142
"def _skip_start(self):
    start, stop = self.start, self.stop
    for chunk in self.app_iter:
        self._pos += len(chunk)
        if self._pos < start:
            continue
        elif self._pos == start:
            return b""""
        else:
            chunk = chunk[start - self._pos :]
            if stop is not None and self._pos > stop:
                chunk = chunk[: stop - self._pos]
                assert len(chunk) == stop - start
            return chunk
    else:
        raise StopIteration()
",elif self . _pos == start :,156
"def start(self):
    self.on_config_change()
    self.start_config_watch()
    try:
        if self.config[""MITMf""][""DNS""][""tcp""].lower() == ""on"":
            self.startTCP()
        else:
            self.startUDP()
    except socket.error as e:
        if ""Address already in use"" in e:
            shutdown(
                ""\n[DNS] Unable to start DNS server on port {}: port already in use"".format(
                    self.config[""MITMf""][""DNS""][""port""]
                )
            )
","if ""Address already in use"" in e :",158
"def ignore(self, other):
    if isinstance(other, Suppress):
        if other not in self.ignoreExprs:
            super(ParseElementEnhance, self).ignore(other)
            if self.expr is not None:
                self.expr.ignore(self.ignoreExprs[-1])
    else:
        super(ParseElementEnhance, self).ignore(other)
        if self.expr is not None:
            self.expr.ignore(self.ignoreExprs[-1])
    return self
",if self . expr is not None :,129
"def test_relative_deploy_path_override():
    s = Site(TEST_SITE_ROOT)
    s.load()
    res = s.content.resource_from_relative_path(
        ""blog/2010/december/merry-christmas.html""
    )
    res.relative_deploy_path = ""blog/2010/december/happy-holidays.html""
    for page in s.content.walk_resources():
        if res.source_file == page.source_file:
            assert page.relative_deploy_path == ""blog/2010/december/happy-holidays.html""
        else:
            assert page.relative_deploy_path == Folder(page.relative_path)
",if res . source_file == page . source_file :,177
"def _parser(cls, buf):
    tlvs = []
    while buf:
        tlv_type = LLDPBasicTLV.get_type(buf)
        tlv = cls._tlv_parsers[tlv_type](buf)
        tlvs.append(tlv)
        offset = LLDP_TLV_SIZE + tlv.len
        buf = buf[offset:]
        if tlv.tlv_type == LLDP_TLV_END:
            break
        assert len(buf) > 0
    lldp_pkt = cls(tlvs)
    assert lldp_pkt._tlvs_len_valid()
    assert lldp_pkt._tlvs_valid()
    return lldp_pkt, None, buf
",if tlv . tlv_type == LLDP_TLV_END :,192
"def _do_pull(self, repo, pull_kwargs, silent, ignore_pull_failures):
    try:
        output = self.client.pull(repo, **pull_kwargs)
        if silent:
            with open(os.devnull, ""w"") as devnull:
                yield from stream_output(output, devnull)
        else:
            yield from stream_output(output, sys.stdout)
    except (StreamOutputError, NotFound) as e:
        if not ignore_pull_failures:
            raise
        else:
            log.error(str(e))
",if not ignore_pull_failures :,151
"def _collect_bytecode(ordered_code):
    bytecode_blocks = []
    stack = [ordered_code]
    while stack:
        code = stack.pop()
        bytecode_blocks.append(code.co_code)
        for const in code.co_consts:
            if isinstance(const, blocks.OrderedCode):
                stack.append(const)
    return bytecode_blocks
","if isinstance ( const , blocks . OrderedCode ) :",99
"def displayhook(value):
    if value is None:
        return
    builtins = modules[""builtins""]
    # Set '_' to None to avoid recursion
    builtins._ = None
    text = repr(value)
    try:
        local_stdout = stdout
    except NameError as e:
        raise RuntimeError(""lost sys.stdout"") from e
    try:
        local_stdout.write(text)
    except UnicodeEncodeError:
        bytes = text.encode(local_stdout.encoding, ""backslashreplace"")
        if hasattr(local_stdout, ""buffer""):
            local_stdout.buffer.write(bytes)
        else:
            text = bytes.decode(local_stdout.encoding, ""strict"")
            local_stdout.write(text)
    local_stdout.write(""\n"")
    builtins._ = value
","if hasattr ( local_stdout , ""buffer"" ) :",200
"def _analyze(self):
    lines = open(self.log_path, ""r"").readlines()
    prev_line = None
    for line in lines:
        if line.startswith(""ERROR:"") and prev_line and prev_line.startswith(""=""):
            self.errors.append(line[len(""ERROR:"") :].strip())
        elif line.startswith(""FAIL:"") and prev_line and prev_line.startswith(""=""):
            self.failures.append(line[len(""FAIL:"") :].strip())
        prev_line = line
","elif line . startswith ( ""FAIL:"" ) and prev_line and prev_line . startswith ( ""="" ) :",128
"def _flush(self):
    if self._data:
        if self._last is not None:
            text = """".join(self._data)
            if self._tail:
                assert self._last.tail is None, ""internal error (tail)""
                self._last.tail = text
            else:
                assert self._last.text is None, ""internal error (text)""
                self._last.text = text
        self._data = []
",if self . _tail :,125
"def write(self, chunk):
    consumer = self._current_consumer
    server_side = consumer.server_side
    if server_side:
        server_side.data_received(chunk)
    else:
        consumer.message += chunk
        assert consumer.in_parser.execute(chunk, len(chunk)) == len(chunk)
        if consumer.in_parser.is_message_complete():
            consumer.finished()
",if consumer . in_parser . is_message_complete ( ) :,114
"def _api_change_cat(name, output, kwargs):
    """"""API: accepts output, value(=nzo_id), value2(=category)""""""
    value = kwargs.get(""value"")
    value2 = kwargs.get(""value2"")
    if value and value2:
        nzo_id = value
        cat = value2
        if cat == ""None"":
            cat = None
        result = sabnzbd.NzbQueue.change_cat(nzo_id, cat)
        return report(output, keyword=""status"", data=bool(result > 0))
    else:
        return report(output, _MSG_NO_VALUE)
","if cat == ""None"" :",164
"def get_allocated_address(
    self, config: ActorPoolConfig, allocated: allocated_type
) -> str:
    addresses = config.get_external_addresses(label=self.label)
    for addr in addresses:
        occupied = False
        for strategy, _ in allocated.get(addr, dict()).values():
            if strategy == self:
                occupied = True
                break
        if not occupied:
            return addr
    raise NoIdleSlot(
        f""No idle slot for creating actor "" f""with label {self.label}, mark {self.mark}""
    )
",if strategy == self :,146
"def schedule_logger(job_id=None, delete=False):
    if not job_id:
        return getLogger(""fate_flow_schedule"")
    else:
        if delete:
            with LoggerFactory.lock:
                try:
                    for key in LoggerFactory.schedule_logger_dict.keys():
                        if job_id in key:
                            del LoggerFactory.schedule_logger_dict[key]
                except:
                    pass
            return True
        key = job_id + ""schedule""
        if key in LoggerFactory.schedule_logger_dict:
            return LoggerFactory.schedule_logger_dict[key]
        return LoggerFactory.get_schedule_logger(job_id)
",if delete :,198
"def quick_load(tool_file, async_load=True):
    try:
        tool = self.load_tool(tool_file, tool_cache_data_dir)
        self.__add_tool(tool, load_panel_dict, elems)
        # Always load the tool into the integrated_panel_dict, or it will not be included in the integrated_tool_panel.xml file.
        key = ""tool_%s"" % str(tool.id)
        integrated_elems[key] = tool
        if async_load:
            self._load_tool_panel()
            self._save_integrated_tool_panel()
        return tool.id
    except Exception:
        log.exception(""Failed to load potential tool %s."", tool_file)
        return None
",if async_load :,195
"def _get_default_ordering(self):
    try:
        ordering = super(DocumentChangeList, self)._get_default_ordering()
    except AttributeError:
        ordering = []
        if self.model_admin.ordering:
            ordering = self.model_admin.ordering
        elif self.lookup_opts.ordering:
            ordering = self.lookup_opts.ordering
    return ordering
",elif self . lookup_opts . ordering :,99
"def names(self, persistent=None):
    u = set()
    result = []
    for s in [
        self.__storage(None),
        self.__storage(self.__category),
    ]:
        for b in s:
            if persistent is not None and b.persistent != persistent:
                continue
            if b.name.startswith(""__""):
                continue
            if b.name not in u:
                result.append(b.name)
                u.add(b.name)
    return result
",if persistent is not None and b . persistent != persistent :,139
"def common_check_get_messages_query(
    self, query_params: Dict[str, object], expected: str
) -> None:
    user_profile = self.example_user(""hamlet"")
    request = POSTRequestMock(query_params, user_profile)
    with queries_captured() as queries:
        get_messages_backend(request, user_profile)
    for query in queries:
        if ""/* get_messages */"" in query[""sql""]:
            sql = str(query[""sql""]).replace("" /* get_messages */"", """")
            self.assertEqual(sql, expected)
            return
    raise AssertionError(""get_messages query not found"")
","if ""/* get_messages */"" in query [ ""sql"" ] :",161
"def _activate_only_current_top_active():
    for i in range(0, len(current_sequence().tracks) - 1):
        if i == current_sequence().get_first_active_track().id:
            current_sequence().tracks[i].active = True
        else:
            current_sequence().tracks[i].active = False
    gui.tline_column.widget.queue_draw()
",if i == current_sequence ( ) . get_first_active_track ( ) . id :,103
"def http_wrapper(self, url, postdata={}):
    try:
        if postdata != {}:
            f = urllib.urlopen(url, postdata)
        else:
            f = urllib.urlopen(url)
        response = f.read()
    except:
        import traceback
        import logging, sys
        cla, exc, tb = sys.exc_info()
        logging.error(url)
        if postdata:
            logging.error(""with post data"")
        else:
            logging.error(""without post data"")
        logging.error(exc.args)
        logging.error(traceback.format_tb(tb))
        response = """"
    return response
",if postdata :,178
"def frequent_thread_switches():
    """"""Make concurrency bugs more likely to manifest.""""""
    interval = None
    if not sys.platform.startswith(""java""):
        if hasattr(sys, ""getswitchinterval""):
            interval = sys.getswitchinterval()
            sys.setswitchinterval(1e-6)
        else:
            interval = sys.getcheckinterval()
            sys.setcheckinterval(1)
    try:
        yield
    finally:
        if not sys.platform.startswith(""java""):
            if hasattr(sys, ""setswitchinterval""):
                sys.setswitchinterval(interval)
            else:
                sys.setcheckinterval(interval)
","if not sys . platform . startswith ( ""java"" ) :",177
"def iter_filters(filters, block_end=False):
    queue = deque(filters)
    while queue:
        f = queue.popleft()
        if f is not None and f.type in (""or"", ""and"", ""not""):
            if block_end:
                queue.appendleft(None)
            for gf in f.filters:
                queue.appendleft(gf)
        yield f
","if f is not None and f . type in ( ""or"" , ""and"" , ""not"" ) :",105
"def smartsplit(code):
    """"""Split `code` at "" symbol, only if it is not escaped.""""""
    strings = []
    pos = 0
    while pos < len(code):
        if code[pos] == '""':
            word = """"  # new word
            pos += 1
            while pos < len(code):
                if code[pos] == '""':
                    break
                if code[pos] == ""\\"":
                    word += ""\\""
                    pos += 1
                word += code[pos]
                pos += 1
            strings.append('""%s""' % word)
        pos += 1
    return strings
","if code [ pos ] == '""' :",174
"def get_folder_content(cls, name):
    """"""Return (folders, files) for the given folder in the root dir.""""""
    folders = set()
    files = set()
    for path in cls.LAYOUT:
        if not path.startswith(name + ""/""):
            continue
        parts = path.split(""/"")
        if len(parts) == 2:
            files.add(parts[1])
        else:
            folders.add(parts[1])
    folders = list(folders)
    folders.sort()
    files = list(files)
    files.sort()
    return (folders, files)
","if not path . startswith ( name + ""/"" ) :",155
"def array_for(self, i):
    if 0 <= i < self._cnt:
        if i >= self.tailoff():
            return self._tail
        node = self._root
        level = self._shift
        while level > 0:
            assert isinstance(node, Node)
            node = node._array[(i >> level) & 0x01F]
            level -= 5
        assert isinstance(node, Node)
        return node._array
    affirm(False, u""Index out of Range"")
",if i >= self . tailoff ( ) :,135
"def __or__(self, other) -> ""MultiVector"":
    r""""""``self | other``, the inner product :math:`M \cdot N`""""""
    other, mv = self._checkOther(other)
    if mv:
        newValue = self.layout.imt_func(self.value, other.value)
    else:
        if isinstance(other, np.ndarray):
            obj = self.__array__()
            return obj | other
        # l * M = M * l = 0 for scalar l
        return self._newMV(dtype=np.result_type(self.value.dtype, other))
    return self._newMV(newValue)
","if isinstance ( other , np . ndarray ) :",163
"def parse_bzr_stats(status):
    stats = RepoStats()
    statustype = ""changed""
    for statusline in status:
        if statusline[:2] == ""  "":
            setattr(stats, statustype, getattr(stats, statustype) + 1)
        elif statusline == ""added:"":
            statustype = ""staged""
        elif statusline == ""unknown:"":
            statustype = ""new""
        else:  # removed, missing, renamed, modified or kind changed
            statustype = ""changed""
    return stats
","elif statusline == ""added:"" :",146
"def write(self, timestamps, actualValues, predictedValues, predictionStep=1):
    assert len(timestamps) == len(actualValues) == len(predictedValues)
    for index in range(len(self.names)):
        timestamp = timestamps[index]
        actual = actualValues[index]
        prediction = predictedValues[index]
        writer = self.outputWriters[index]
        if timestamp is not None:
            outputRow = [timestamp, actual, prediction]
            writer.writerow(outputRow)
            self.lineCounts[index] += 1
",if timestamp is not None :,142
"def clean(self):
    """"""Delete old files in ""tmp"".""""""
    now = time.time()
    for entry in os.listdir(os.path.join(self._path, ""tmp"")):
        path = os.path.join(self._path, ""tmp"", entry)
        if now - os.path.getatime(path) > 129600:  # 60 * 60 * 36
            os.remove(path)
",if now - os . path . getatime ( path ) > 129600 :,101
"def _get_info(self, path):
    info = OrderedDict()
    if not self._is_mac() or self._has_xcode_tools():
        stdout = None
        try:
            stdout, stderr = Popen(
                [self._find_binary(), ""info"", os.path.realpath(path)],
                stdout=PIPE,
                stderr=PIPE,
            ).communicate()
        except OSError:
            pass
        else:
            if stdout:
                for line in stdout.splitlines():
                    line = u(line).split("": "", 1)
                    if len(line) == 2:
                        info[line[0]] = line[1]
    return info
",if stdout :,194
"def add(meta_list, info_list=None):
    if not info_list:
        info_list = meta_list
    if not isinstance(meta_list, (list, tuple)):
        meta_list = (meta_list,)
    if not isinstance(info_list, (list, tuple)):
        info_list = (info_list,)
    for info_f in info_list:
        if info.get(info_f) is not None:
            for meta_f in meta_list:
                metadata[meta_f] = info[info_f]
            break
",if info . get ( info_f ) is not None :,149
"def _compute_log_r(model_trace, guide_trace):
    log_r = MultiFrameTensor()
    stacks = get_plate_stacks(model_trace)
    for name, model_site in model_trace.nodes.items():
        if model_site[""type""] == ""sample"":
            log_r_term = model_site[""log_prob""]
            if not model_site[""is_observed""]:
                log_r_term = log_r_term - guide_trace.nodes[name][""log_prob""]
            log_r.add((stacks[name], log_r_term.detach()))
    return log_r
","if model_site [ ""type"" ] == ""sample"" :",162
"def pickline(file, key, casefold=1):
    try:
        f = open(file, ""r"")
    except IOError:
        return None
    pat = re.escape(key) + "":""
    prog = re.compile(pat, casefold and re.IGNORECASE)
    while 1:
        line = f.readline()
        if not line:
            break
        if prog.match(line):
            text = line[len(key) + 1 :]
            while 1:
                line = f.readline()
                if not line or not line[0].isspace():
                    break
                text = text + line
            return text.strip()
    return None
",if not line or not line [ 0 ] . isspace ( ) :,182
"def build_iterator(data, infinite=True):
    """"""Build the iterator for inputs.""""""
    index = 0
    size = len(data[0])
    while True:
        if index + batch_size > size:
            if infinite:
                index = 0
            else:
                return
        yield data[0][index : index + batch_size], data[1][index : index + batch_size]
        index += batch_size
",if infinite :,116
"def checkall(g, bg, dst_nodes, include_dst_in_src=True):
    for etype in g.etypes:
        ntype = g.to_canonical_etype(etype)[2]
        if dst_nodes is not None and ntype in dst_nodes:
            check(g, bg, ntype, etype, dst_nodes[ntype], include_dst_in_src)
        else:
            check(g, bg, ntype, etype, None, include_dst_in_src)
",if dst_nodes is not None and ntype in dst_nodes :,124
"def minimalBases(classes):
    """"""Reduce a list of base classes to its ordered minimum equivalent""""""
    if not __python3:  # pragma: no cover
        classes = [c for c in classes if c is not ClassType]
    candidates = []
    for m in classes:
        for n in classes:
            if issubclass(n, m) and m is not n:
                break
        else:
            # m has no subclasses in 'classes'
            if m in candidates:
                candidates.remove(m)  # ensure that we're later in the list
            candidates.append(m)
    return candidates
",if m in candidates :,160
"def __keep_songs_enable(self, enabled):
    config.set(""memory"", ""queue_keep_songs"", enabled)
    if enabled:
        self.queue.set_first_column_type(CurrentColumn)
    else:
        for col in self.queue.get_columns():
            # Remove the CurrentColum if it exists
            if isinstance(col, CurrentColumn):
                self.queue.set_first_column_type(None)
                break
","if isinstance ( col , CurrentColumn ) :",121
"def outlineView_heightOfRowByItem_(self, tree, item) -> float:
    default_row_height = self.rowHeight
    if item is self:
        return default_row_height
    heights = [default_row_height]
    for column in self.tableColumns:
        value = getattr(item.attrs[""node""], str(column.identifier))
        if isinstance(value, toga.Widget):
            # if the cell value is a widget, use its height
            heights.append(value._impl.native.intrinsicContentSize().height)
    return max(heights)
","if isinstance ( value , toga . Widget ) :",146
"def condition(self):
    if self.__condition is None:
        if len(self.flat_conditions) == 1:
            # Avoid an extra indirection in the common case of only one condition.
            self.__condition = self.flat_conditions[0]
        elif len(self.flat_conditions) == 0:
            # Possible, if unlikely, due to filter predicate rewriting
            self.__condition = lambda _: True
        else:
            self.__condition = lambda x: all(cond(x) for cond in self.flat_conditions)
    return self.__condition
",if len ( self . flat_conditions ) == 1 :,143
"def _find_delimiter(f, block_size=2 ** 16):
    delimiter = b""\n""
    if f.tell() == 0:
        return 0
    while True:
        b = f.read(block_size)
        if not b:
            return f.tell()
        elif delimiter in b:
            return f.tell() - len(b) + b.index(delimiter) + 1
",elif delimiter in b :,105
"def serialize(self, name=None):
    data = super(SimpleText, self).serialize(name)
    data[""contentType""] = self.contentType
    data[""content""] = self.content
    if self.width:
        if self.width not in [100, 50, 33, 25]:
            raise InvalidWidthException(self.width)
        data[""inputOptions""] = {}
        data[""width""] = self.width
    return data
","if self . width not in [ 100 , 50 , 33 , 25 ] :",108
"def inference(self):
    self.attention_weight_dim = self.input_dims[0][-1]
    if self.keep_dim:
        self.output_dim = copy.deepcopy(self.input_dims[0])
    else:
        self.output_dim = []
        for idx, dim in enumerate(self.input_dims[0]):
            if idx != len(self.input_dims[0]) - 2:
                self.output_dim.append(dim)
    super(
        LinearAttentionConf, self
    ).inference()  # PUT THIS LINE AT THE END OF inference()
",if idx != len ( self . input_dims [ 0 ] ) - 2 :,152
"def __delete_hook(self, rpc):
    try:
        rpc.check_success()
    except apiproxy_errors.Error:
        return None
    result = []
    for status in rpc.response.delete_status_list():
        if status == MemcacheDeleteResponse.DELETED:
            result.append(DELETE_SUCCESSFUL)
        elif status == MemcacheDeleteResponse.NOT_FOUND:
            result.append(DELETE_ITEM_MISSING)
        else:
            result.append(DELETE_NETWORK_FAILURE)
    return result
",elif status == MemcacheDeleteResponse . NOT_FOUND :,139
"def identify_page_at_cursor(self):
    for region in self.view.sel():
        text_on_cursor = None
        pos = region.begin()
        scope_region = self.view.extract_scope(pos)
        if not scope_region.empty():
            text_on_cursor = self.view.substr(scope_region)
            return text_on_cursor.strip(string.punctuation)
    return None
",if not scope_region . empty ( ) :,111
"def from_elem(cls, parent, when_elem):
    """"""Loads the proper when by attributes of elem""""""
    when_value = when_elem.get(""value"", None)
    if when_value is not None:
        return ValueToolOutputActionConditionalWhen(parent, when_elem, when_value)
    else:
        when_value = when_elem.get(""datatype_isinstance"", None)
        if when_value is not None:
            return DatatypeIsInstanceToolOutputActionConditionalWhen(
                parent, when_elem, when_value
            )
    raise TypeError(""When type not implemented"")
",if when_value is not None :,151
"def test_insert_entity_empty_string_rk(
    self, tables_cosmos_account_name, tables_primary_cosmos_account_key
):
    # Arrange
    await self._set_up(tables_cosmos_account_name, tables_primary_cosmos_account_key)
    try:
        entity = {""PartitionKey"": ""pk"", ""RowKey"": """"}
        # Act
        with pytest.raises(HttpResponseError):
            await self.table.create_entity(entity=entity)
            # Assert
        #  assert resp is None
    finally:
        await self._tear_down()
        if self.is_live:
            sleep(SLEEP_DELAY)
",if self . is_live :,179
"def provider_uris(self):
    login_urls = {}
    continue_url = self.request.get(""continue_url"")
    for provider in self.provider_info:
        if continue_url:
            login_url = self.uri_for(
                ""social-login"", provider_name=provider, continue_url=continue_url
            )
        else:
            login_url = self.uri_for(""social-login"", provider_name=provider)
        login_urls[provider] = login_url
    return login_urls
",if continue_url :,140
"def expand_extensions(existing):
    for name in extension_names:
        ext = (
            im(""lizard_ext.lizard"" + name.lower()).LizardExtension()
            if isinstance(name, str)
            else name
        )
        existing.insert(
            len(existing) if not hasattr(ext, ""ordering_index"") else ext.ordering_index,
            ext,
        )
    return existing
","if isinstance ( name , str )",116
"def wrapper(self, *args, **kwargs):
    if not self.request.path.endswith(""/""):
        if self.request.method in (""GET"", ""HEAD""):
            uri = self.request.path + ""/""
            if self.request.query:
                uri += ""?"" + self.request.query
            self.redirect(uri, permanent=True)
            return
        raise HTTPError(404)
    return method(self, *args, **kwargs)
",if self . request . query :,118
"def subword_map_by_joiner(subwords, marker=SubwordMarker.JOINER):
    """"""Return word id for each subword token (annotate by joiner).""""""
    flags = [0] * len(subwords)
    for i, tok in enumerate(subwords):
        if tok.endswith(marker):
            flags[i] = 1
        if tok.startswith(marker):
            assert i >= 1 and flags[i - 1] != 1, ""Sentence `{}` not correct!"".format(
                "" "".join(subwords)
            )
            flags[i - 1] = 1
    marker_acc = list(accumulate([0] + flags[:-1]))
    word_group = [(i - maker_sofar) for i, maker_sofar in enumerate(marker_acc)]
    return word_group
",if tok . endswith ( marker ) :,193
"def next_item(self, direction):
    """"""Selects next menu item, based on self._direction""""""
    start, i = -1, 0
    try:
        start = self.items.index(self._selected)
        i = start + direction
    except:
        pass
    while True:
        if i == start:
            # Cannot find valid menu item
            self.select(start)
            break
        if i >= len(self.items):
            i = 0
            continue
        if i < 0:
            i = len(self.items) - 1
            continue
        if self.select(i):
            break
        i += direction
        if start < 0:
            start = 0
",if self . select ( i ) :,194
"def get_config(cls):
    # FIXME: Replace this as soon as we have a config module
    config = {}
    # Try to get iflytek_yuyin config from config
    profile_path = dingdangpath.config(""profile.yml"")
    if os.path.exists(profile_path):
        with open(profile_path, ""r"") as f:
            profile = yaml.safe_load(f)
            if ""iflytek_yuyin"" in profile:
                if ""vid"" in profile[""iflytek_yuyin""]:
                    config[""vid""] = profile[""iflytek_yuyin""][""vid""]
    return config
","if ""iflytek_yuyin"" in profile :",169
"def get_signed_in_user(test_case):
    playback = not (test_case.is_live or test_case.in_recording)
    if playback:
        return MOCKED_USER_NAME
    else:
        account_info = test_case.cmd(""account show"").get_output_in_json()
        if account_info[""user""][""type""] != ""servicePrincipal"":
            return account_info[""user""][""name""]
    return None
","if account_info [ ""user"" ] [ ""type"" ] != ""servicePrincipal"" :",115
"def rename_project(self, project, new_name):
    """"""Rename project, update the related projects if necessary""""""
    old_name = project.name
    for proj in self.projects:
        relproj = proj.get_related_projects()
        if old_name in relproj:
            relproj[relproj.index(old_name)] = new_name
            proj.set_related_projects(relproj)
    project.rename(new_name)
    self.save()
",if old_name in relproj :,121
"def test_call_extern_c_fn(self):
    global memcmp
    memcmp = cffi_support.ExternCFunction(
        ""memcmp"",
        (""int memcmp ( const uint8_t * ptr1, "" ""const uint8_t * ptr2, size_t num )""),
    )
    @udf(BooleanVal(FunctionContext, StringVal, StringVal))
    def fn(context, a, b):
        if a.is_null != b.is_null:
            return False
        if a is None:
            return True
        if len(a) != b.len:
            return False
        if a.ptr == b.ptr:
            return True
        return memcmp(a.ptr, b.ptr, a.len) == 0
",if a . ptr == b . ptr :,199
"def parse_variable(self):
    begin = self._pos
    while True:
        ch = self.read()
        if ch == ""%"":
            return ScriptVariable(self._text[begin : self._pos - 1])
        elif ch is None:
            self.__raise_eof()
        elif not isidentif(ch) and ch != "":"":
            self.__raise_char(ch)
","if ch == ""%"" :",101
"def h_file(self):
    filename = self.abspath()
    st = os.stat(filename)
    cache = self.ctx.hashes_md5_tstamp
    if filename in cache and cache[filename][0] == st.st_mtime:
        return cache[filename][1]
    if STRONGEST:
        ret = Utils.h_file(filename)
    else:
        if stat.S_ISDIR(st[stat.ST_MODE]):
            raise IOError(""Not a file"")
        ret = Utils.md5(str((st.st_mtime, st.st_size)).encode()).digest()
    cache[filename] = (st.st_mtime, ret)
    return ret
",if stat . S_ISDIR ( st [ stat . ST_MODE ] ) :,172
"def add_widgets(self, *widgets_or_spacings):
    """"""Add widgets/spacing to dialog vertical layout""""""
    layout = self.layout()
    for widget_or_spacing in widgets_or_spacings:
        if isinstance(widget_or_spacing, int):
            layout.addSpacing(widget_or_spacing)
        else:
            layout.addWidget(widget_or_spacing)
","if isinstance ( widget_or_spacing , int ) :",103
"def _str_index(self):
    idx = self[""index""]
    out = []
    if len(idx) == 0:
        return out
    out += ["".. index:: %s"" % idx.get(""default"", """")]
    for section, references in idx.iteritems():
        if section == ""default"":
            continue
        elif section == ""refguide"":
            out += [""   single: %s"" % ("", "".join(references))]
        else:
            out += [""   %s: %s"" % (section, "","".join(references))]
    return out
","if section == ""default"" :",145
"def dictify_CPPDEFINES(env):
    cppdefines = env.get(""CPPDEFINES"", {})
    if cppdefines is None:
        return {}
    if SCons.Util.is_Sequence(cppdefines):
        result = {}
        for c in cppdefines:
            if SCons.Util.is_Sequence(c):
                result[c[0]] = c[1]
            else:
                result[c] = None
        return result
    if not SCons.Util.is_Dict(cppdefines):
        return {cppdefines: None}
    return cppdefines
",if SCons . Util . is_Sequence ( c ) :,155
"def decoder(s):
    r = []
    decode = []
    for c in s:
        if c == ""&"" and not decode:
            decode.append(""&"")
        elif c == ""-"" and decode:
            if len(decode) == 1:
                r.append(""&"")
            else:
                r.append(modified_unbase64("""".join(decode[1:])))
            decode = []
        elif decode:
            decode.append(c)
        else:
            r.append(c)
    if decode:
        r.append(modified_unbase64("""".join(decode[1:])))
    bin_str = """".join(r)
    return (bin_str, len(s))
",elif decode :,188
"def optimize(self, graph: Graph):
    MAX_TEXTURE_SIZE = config.WEBGL_MAX_TEXTURE_SIZE
    flag_changed = False
    for v in traverse.listup_variables(graph):
        if not Placeholder.check_resolved(v.size):
            continue
        height, width = TextureShape.get(v)
        if height <= MAX_TEXTURE_SIZE and width <= MAX_TEXTURE_SIZE:
            continue
        if not v.has_attribute(SplitTarget):
            flag_changed = True
            v.attributes.add(SplitTarget())
    return graph, flag_changed
",if not Placeholder . check_resolved ( v . size ) :,157
"def one_gpr_reg_one_mem_scalable(ii):
    n, r = 0, 0
    for op in _gen_opnds(ii):
        if op_agen(op) or (op_mem(op) and op.oc2 in [""v""]):
            n += 1
        elif op_gprv(op):
            r += 1
        else:
            return False
    return n == 1 and r == 1
",elif op_gprv ( op ) :,113
"def get_genome_dir(gid, galaxy_dir, data):
    """"""Return standard location of genome directories.""""""
    if galaxy_dir:
        refs = genome.get_refs(gid, None, galaxy_dir, data)
        seq_file = tz.get_in([""fasta"", ""base""], refs)
        if seq_file and os.path.exists(seq_file):
            return os.path.dirname(os.path.dirname(seq_file))
    else:
        gdirs = glob.glob(os.path.join(_get_data_dir(), ""genomes"", ""*"", gid))
        if len(gdirs) == 1 and os.path.exists(gdirs[0]):
            return gdirs[0]
",if len ( gdirs ) == 1 and os . path . exists ( gdirs [ 0 ] ) :,190
"def __modules(self):
    raw_output = self.__module_avail_output().decode(""utf-8"")
    for line in StringIO(raw_output):
        line = line and line.strip()
        if not line or line.startswith(""-""):
            continue
        line_modules = line.split()
        for module in line_modules:
            if module.endswith(self.default_indicator):
                module = module[0 : -len(self.default_indicator)].strip()
            module_parts = module.split(""/"")
            module_version = None
            if len(module_parts) == 2:
                module_version = module_parts[1]
            module_name = module_parts[0]
            yield module_name, module_version
",if module . endswith ( self . default_indicator ) :,199
"def save(self):
    updates = self.cinder_obj_get_changes()
    if updates:
        if ""metadata"" in updates:
            metadata = updates.pop(""metadata"", None)
            self.metadata = db.backup_metadata_update(
                self._context, self.id, metadata, True
            )
        updates.pop(""parent"", None)
        db.backup_update(self._context, self.id, updates)
    self.obj_reset_changes()
","if ""metadata"" in updates :",127
"def test_set_tag(association_obj, sagemaker_session):
    tag = {""Key"": ""foo"", ""Value"": ""bar""}
    association_obj.set_tag(tag)
    while True:
        actual_tags = sagemaker_session.sagemaker_client.list_tags(
            ResourceArn=association_obj.source_arn
        )[""Tags""]
        if actual_tags:
            break
        time.sleep(5)
    # When sagemaker-client-config endpoint-url is passed as argument to hit some endpoints,
    # length of actual tags will be greater than 1
    assert len(actual_tags) > 0
    assert actual_tags[0] == tag
",if actual_tags :,175
"def test_error_stream(environ, start_response):
    writer = start_response(""200 OK"", [])
    wsgi_errors = environ[""wsgi.errors""]
    error_msg = None
    for method in [
        ""flush"",
        ""write"",
        ""writelines"",
    ]:
        if not hasattr(wsgi_errors, method):
            error_msg = ""wsgi.errors has no '%s' attr"" % method
        if not error_msg and not callable(getattr(wsgi_errors, method)):
            error_msg = ""wsgi.errors.%s attr is not callable"" % method
        if error_msg:
            break
    return_msg = error_msg or ""success""
    writer(return_msg)
    return []
","if not error_msg and not callable ( getattr ( wsgi_errors , method ) ) :",185
"def current_dict(cursor_offset, line):
    """"""If in dictionary completion, return the dict that should be used""""""
    for m in current_dict_re.finditer(line):
        if m.start(2) <= cursor_offset and m.end(2) >= cursor_offset:
            return LinePart(m.start(1), m.end(1), m.group(1))
    return None
",if m . start ( 2 ) <= cursor_offset and m . end ( 2 ) >= cursor_offset :,99
"def show_file_browser(self):
    """"""Show/hide the file browser.""""""
    if self.show_file_browser_action.isChecked():
        sizes = self.panel.sizes()
        if sizes[0] == 0:
            sizes[0] = sum(sizes) // 4
            self.panel.setSizes(sizes)
        self.file_browser.show()
    else:
        self.file_browser.hide()
",if sizes [ 0 ] == 0 :,112
"def run(self, paths=[]):
    items = []
    for item in SideBarSelection(paths).getSelectedItems():
        items.append(item.nameEncoded())
    if len(items) > 0:
        sublime.set_clipboard(""\n"".join(items))
        if len(items) > 1:
            sublime.status_message(""Items copied"")
        else:
            sublime.status_message(""Item copied"")
",if len ( items ) > 1 :,114
"def prepend(self, value):
    """"""prepend value to nodes""""""
    root, root_text = self._get_root(value)
    for i, tag in enumerate(self):
        if not tag.text:
            tag.text = """"
        if len(root) > 0:
            root[-1].tail = tag.text
            tag.text = root_text
        else:
            tag.text = root_text + tag.text
        if i > 0:
            root = deepcopy(list(root))
        tag[:0] = root
        root = tag[: len(root)]
    return self
",if not tag . text :,160
"def getLabel(self, address=None):
    if address is None:
        address = self.address
    label = address
    if shared.config.has_section(address):
        label = shared.config.get(address, ""label"")
    queryreturn = sqlQuery(""""""select label from addressbook where address=?"""""", address)
    if queryreturn != []:
        for row in queryreturn:
            (label,) = row
    else:
        queryreturn = sqlQuery(
            """"""select label from subscriptions where address=?"""""", address
        )
        if queryreturn != []:
            for row in queryreturn:
                (label,) = row
    return label
",if queryreturn != [ ] :,168
"def _parse(self, engine):
    """"""Parse the layer.""""""
    if isinstance(self.args, dict):
        if ""axis"" in self.args:
            self.axis = engine.evaluate(self.args[""axis""], recursive=True)
            if not isinstance(self.axis, int):
                raise ParsingError('""axis"" must be an integer.')
        if ""momentum"" in self.args:
            self.momentum = engine.evaluate(self.args[""momentum""], recursive=True)
            if not isinstance(self.momentum, (int, float)):
                raise ParsingError('""momentum"" must be numeric.')
","if ""momentum"" in self . args :",157
"def urlquote(*args, **kwargs):
    new_kwargs = dict(kwargs)
    if not PY3:
        new_kwargs = dict(kwargs)
        if ""encoding"" in new_kwargs:
            del new_kwargs[""encoding""]
        if ""errors"" in kwargs:
            del new_kwargs[""errors""]
    return quote(*args, **new_kwargs)
","if ""errors"" in kwargs :",93
"def setNextFormPrevious(self, backup=STARTING_FORM):
    try:
        if self._THISFORM.FORM_NAME == self._FORM_VISIT_LIST[-1]:
            self._FORM_VISIT_LIST.pop()  # Remove the current form. if it is at the end of the list
        if self._THISFORM.FORM_NAME == self.NEXT_ACTIVE_FORM:
            # take no action if it looks as if someone has already set the next form.
            self.setNextForm(
                self._FORM_VISIT_LIST.pop()
            )  # Switch to the previous form if one exists
    except IndexError:
        self.setNextForm(backup)
",if self . _THISFORM . FORM_NAME == self . NEXT_ACTIVE_FORM :,178
"def iter_chars_to_words(self, chars):
    current_word = []
    for char in chars:
        if not self.keep_blank_chars and char[""text""].isspace():
            if current_word:
                yield current_word
                current_word = []
        elif current_word and self.char_begins_new_word(current_word, char):
            yield current_word
            current_word = [char]
        else:
            current_word.append(char)
    if current_word:
        yield current_word
","elif current_word and self . char_begins_new_word ( current_word , char ) :",150
"def get(self):
    """"""return a secret by name""""""
    results = self._get(""secrets"", self.name)
    results[""decoded""] = {}
    results[""exists""] = False
    if results[""returncode""] == 0 and results[""results""][0]:
        results[""exists""] = True
        if self.decode:
            if ""data"" in results[""results""][0]:
                for sname, value in results[""results""][0][""data""].items():
                    results[""decoded""][sname] = base64.b64decode(value)
    if results[""returncode""] != 0 and '""%s"" not found' % self.name in results[""stderr""]:
        results[""returncode""] = 0
    return results
",if self . decode :,173
"def insert_use(self, edit):
    if self.is_first_use():
        for location in [r""^\s*namespace\s+[\w\\]+[;{]"", r""<\?php""]:
            inserted = self.insert_first_use(location, edit)
            if inserted:
                break
    else:
        self.insert_use_among_others(edit)
",if inserted :,99
"def _new_rsa_key(spec):
    if ""name"" not in spec:
        if ""/"" in spec[""key""]:
            (head, tail) = os.path.split(spec[""key""])
            spec[""path""] = head
            spec[""name""] = tail
        else:
            spec[""name""] = spec[""key""]
    return rsa_init(spec)
","if ""/"" in spec [ ""key"" ] :",98
"def mimeData(self, indexes):
    if len(indexes) == 1:
        index = indexes[0]
        model = song = index.data(Qt.UserRole)
        if index.column() == Column.album:
            try:
                model = song.album
            except (ProviderIOError, Exception):
                model = None
        return ModelMimeData(model)
",if index . column ( ) == Column . album :,103
"def get(self, url, **kwargs):
    app, url = self._prepare_call(url, kwargs)
    if app:
        if url.endswith(""ping"") and self._first_ping:
            self._first_ping = False
            return EmptyCapabilitiesResponse()
        elif ""Hello0"" in url and ""1.2.1"" in url and ""v1"" in url:
            return ErrorApiResponse()
        else:
            response = app.get(url, **kwargs)
            return TestingResponse(response)
    else:
        return requests.get(url, **kwargs)
","elif ""Hello0"" in url and ""1.2.1"" in url and ""v1"" in url :",153
"def handle_noargs(self, **options):
    self.style = color_style()
    print(""Running Django's own validation:"")
    self.validate(display_num_errors=True)
    for model in loading.get_models():
        if hasattr(model, ""_create_content_base""):
            self.validate_base_model(model)
        if hasattr(model, ""_feincms_content_models""):
            self.validate_content_type(model)
","if hasattr ( model , ""_feincms_content_models"" ) :",117
"def test_rules_widget(self):
    subreddit = self.reddit.subreddit(pytest.placeholders.test_subreddit)
    widgets = subreddit.widgets
    with self.use_cassette(""TestSubredditWidgets.fetch_widgets""):
        rules = None
        for widget in widgets.sidebar:
            if isinstance(widget, RulesWidget):
                rules = widget
                break
        assert isinstance(rules, RulesWidget)
        assert rules == rules
        assert rules.id == rules
        assert rules.display
        assert len(rules) > 0
        assert subreddit == rules.subreddit
","if isinstance ( widget , RulesWidget ) :",173
"def __init__(self, exception):
    message = str(exception)
    with contextlib.suppress(IndexError):
        underlying_exception = exception.args[0]
        if isinstance(underlying_exception, urllib3.exceptions.MaxRetryError):
            message = (
                ""maximum retries exceeded trying to reach the store.\n""
                ""Check your network connection, and check the store ""
                ""status at {}"".format(_STORE_STATUS_URL)
            )
    super().__init__(message=message)
","if isinstance ( underlying_exception , urllib3 . exceptions . MaxRetryError ) :",130
"def wrapped(self, request):
    try:
        return self._finished
    except AttributeError:
        if self.node_ids:
            if not request.session.shouldfail and not request.session.shouldstop:
                log.debug(
                    ""%s is still going to be used, not terminating it. ""
                    ""Still in use on:\n%s"",
                    self,
                    pprint.pformat(list(self.node_ids)),
                )
                return
        log.debug(""Finish called on %s"", self)
        try:
            return func(request)
        finally:
            self._finished = True
",if not request . session . shouldfail and not request . session . shouldstop :,185
"def get_min_vertical_scroll() -> int:
    # Make sure that the cursor line is not below the bottom.
    # (Calculate how many lines can be shown between the cursor and the .)
    used_height = 0
    prev_lineno = ui_content.cursor_position.y
    for lineno in range(ui_content.cursor_position.y, -1, -1):
        used_height += get_line_height(lineno)
        if used_height > height - scroll_offsets_bottom:
            return prev_lineno
        else:
            prev_lineno = lineno
    return 0
",if used_height > height - scroll_offsets_bottom :,148
"def cookies(self):
    # strip cookie_suffix from all cookies in the request, return result
    cookies = flask.Request.cookies.__get__(self)
    result = {}
    desuffixed = {}
    for key, value in cookies.items():
        if key.endswith(self.cookie_suffix):
            desuffixed[key[: -len(self.cookie_suffix)]] = value
        else:
            result[key] = value
    result.update(desuffixed)
    return result
",if key . endswith ( self . cookie_suffix ) :,123
"def update_vars(state1, state2):
    ops = []
    for name in state1._fields:
        state1_vs = getattr(state1, name)
        if isinstance(state1_vs, list):
            ops += [
                tf.assign(_v1, _v2)
                for _v1, _v2 in zip(state1_vs, getattr(state2, name))
            ]
        else:
            ops += [tf.assign(state1_vs, getattr(state2, name))]
    return tf.group(*ops)
","if isinstance ( state1_vs , list ) :",148
"def manifest(self):
    """"""The current manifest dictionary.""""""
    if self.reload:
        if not self.exists(self.manifest_path):
            return {}
        mtime = self.getmtime(self.manifest_path)
        if self._mtime is None or mtime > self._mtime:
            self._manifest = self.get_manifest()
            self._mtime = mtime
    return self._manifest
",if not self . exists ( self . manifest_path ) :,102
"def csvtitle(self):
    if isinstance(self.name, six.string_types):
        return '""' + self.name + '""' + char[""sep""] * (len(self.nick) - 1)
    else:
        ret = """"
        for i, name in enumerate(self.name):
            ret = ret + '""' + name + '""' + char[""sep""] * (len(self.nick) - 1)
            if i + 1 != len(self.name):
                ret = ret + char[""sep""]
        return ret
",if i + 1 != len ( self . name ) :,135
"def cache_dst(self):
    final_dst = None
    final_linenb = None
    for linenb, assignblk in enumerate(self):
        for dst, src in viewitems(assignblk):
            if dst.is_id(""IRDst""):
                if final_dst is not None:
                    raise ValueError(""Multiple destinations!"")
                final_dst = src
                final_linenb = linenb
    self._dst = final_dst
    self._dst_linenb = final_linenb
    return final_dst
","if dst . is_id ( ""IRDst"" ) :",144
"def _ProcessName(self, name, dependencies):
    """"""Retrieve a module name from a node name.""""""
    module_name, dot, base_name = name.rpartition(""."")
    if dot:
        if module_name:
            if module_name in dependencies:
                dependencies[module_name].add(base_name)
            else:
                dependencies[module_name] = {base_name}
        else:
            # If we have a relative import that did not get qualified (usually due
            # to an empty package_name), don't insert module_name='' into the
            # dependencies; we get a better error message if we filter it out here
            # and fail later on.
            logging.warning(""Empty package name: %s"", name)
",if module_name :,196
"def get_aa_from_codonre(re_aa):
    aas = []
    m = 0
    for i in re_aa:
        if i == ""["":
            m = -1
            aas.append("""")
        elif i == ""]"":
            m = 0
            continue
        elif m == -1:
            aas[-1] = aas[-1] + i
        elif m == 0:
            aas.append(i)
    return aas
",elif m == 0 :,129
"def logic():
    count = intbv(0, min=0, max=MAXVAL + 1)
    while True:
        yield clock.posedge, reset.posedge
        if reset == 1:
            count[:] = 0
        else:
            flag.next = 0
            if count == MAXVAL:
                flag.next = 1
                count[:] = 0
            else:
                count += 1
",if count == MAXVAL :,115
"def _history_define_metric(
    self, hkey: str
) -> Optional[wandb_internal_pb2.MetricRecord]:
    """"""check for hkey match in glob metrics, return defined metric.""""""
    # Dont define metric for internal metrics
    if hkey.startswith(""_""):
        return None
    for k, mglob in six.iteritems(self._metric_globs):
        if k.endswith(""*""):
            if hkey.startswith(k[:-1]):
                m = wandb_internal_pb2.MetricRecord()
                m.CopyFrom(mglob)
                m.ClearField(""glob_name"")
                m.name = hkey
                return m
    return None
",if hkey . startswith ( k [ : - 1 ] ) :,180
"def optimize_models(args, use_cuda, models):
    """"""Optimize ensemble for generation""""""
    for model in models:
        model.make_generation_fast_(
            beamable_mm_beam_size=None if args.no_beamable_mm else args.beam,
            need_attn=args.print_alignment,
        )
        if args.fp16:
            model.half()
        if use_cuda:
            model.cuda()
",if args . fp16 :,122
"def _Dynamic_Rollback(self, transaction, transaction_response):
    txid = transaction.handle()
    self.__local_tx_lock.acquire()
    try:
        if txid not in self.__transactions:
            raise apiproxy_errors.ApplicationError(
                datastore_pb.Error.BAD_REQUEST, ""Transaction %d not found."" % (txid,)
            )
        txdata = self.__transactions[txid]
        assert (
            txdata.thread_id == thread.get_ident()
        ), ""Transactions are single-threaded.""
        del self.__transactions[txid]
    finally:
        self.__local_tx_lock.release()
",if txid not in self . __transactions :,174
"def get_job_dirs(path):
    regex = re.compile(""[1-9][0-9]*-"")
    jobdirs = []
    for d in os.listdir(path):
        # skip directories not matching the job result dir pattern
        if not regex.match(d):
            continue
        d = os.path.join(options.resultsdir, d)
        if os.path.isdir(d) and not os.path.exists(os.path.join(d, PUBLISH_FLAGFILE)):
            jobdirs.append(d)
    return jobdirs
",if not regex . match ( d ) :,141
"def traverse(node, functions=[]):
    if hasattr(node, ""grad_fn""):
        node = node.grad_fn
    if hasattr(node, ""variable""):
        node = graph.nodes_by_id.get(id(node.variable))
        if node:
            node.functions = list(functions)
            del functions[:]
    if hasattr(node, ""next_functions""):
        functions.append(type(node).__name__)
        for f in node.next_functions:
            if f[0]:
                functions.append(type(f[0]).__name__)
                traverse(f[0], functions)
    if hasattr(node, ""saved_tensors""):
        for t in node.saved_tensors:
            traverse(t)
",if f [ 0 ] :,195
"def get_all_snap_points(self, forts):
    points = []
    radius = Constants.MAX_DISTANCE_FORT_IS_REACHABLE
    for i in range(0, len(forts)):
        for j in range(i + 1, len(forts)):
            c1, c2 = self.get_enclosing_circles(forts[i], forts[j], radius)
            if c1 and c2:
                points.append((c1, c2, forts[i], forts[j]))
    return points
",if c1 and c2 :,142
"def doDir(elem):
    for child in elem.childNodes:
        if not isinstance(child, minidom.Element):
            continue
        if child.tagName == ""Directory"":
            doDir(child)
        elif child.tagName == ""Component"":
            for grandchild in child.childNodes:
                if not isinstance(grandchild, minidom.Element):
                    continue
                if grandchild.tagName != ""File"":
                    continue
                files.add(grandchild.getAttribute(""Source"").replace(os.sep, ""/""))
","if not isinstance ( grandchild , minidom . Element ) :",152
"def computeLeadingWhitespaceWidth(s, tab_width):
    w = 0
    for ch in s:
        if ch == "" "":
            w += 1
        elif ch == ""\t"":
            w += abs(tab_width) - (w % abs(tab_width))
        else:
            break
    return w
","elif ch == ""\t"" :",87
"def test_avg_group_by(self):
    ret = (
        await Book.annotate(avg=Avg(""rating""))
        .group_by(""author_id"")
        .values(""author_id"", ""avg"")
    )
    for item in ret:
        author_id = item.get(""author_id"")
        avg = item.get(""avg"")
        if author_id == self.a1.pk:
            self.assertEqual(avg, 4.5)
        elif author_id == self.a2.pk:
            self.assertEqual(avg, 2.0)
",if author_id == self . a1 . pk :,150
"def open_session(self, app, request):
    sid = request.cookies.get(app.session_cookie_name)
    if sid:
        stored_session = self.cls.objects(sid=sid).first()
        if stored_session:
            expiration = stored_session.expiration
            if not expiration.tzinfo:
                expiration = expiration.replace(tzinfo=utc)
            if expiration > datetime.datetime.utcnow().replace(tzinfo=utc):
                return MongoEngineSession(
                    initial=stored_session.data, sid=stored_session.sid
                )
    return MongoEngineSession(sid=str(uuid.uuid4()))
",if stored_session :,174
"def one_line_description(self):
    MAX_LINE_LENGTH = 120
    desc = util.remove_html_tags(self.description or """")
    desc = re.sub(""\s+"", "" "", desc).strip()
    if not desc:
        return _(""No description available"")
    else:
        # Decode the description to avoid gPodder bug 1277
        desc = util.convert_bytes(desc).strip()
        if len(desc) > MAX_LINE_LENGTH:
            return desc[:MAX_LINE_LENGTH] + ""...""
        else:
            return desc
",if len ( desc ) > MAX_LINE_LENGTH :,142
"def setInnerHTML(self, html):
    log.HTMLClassifier.classify(
        log.ThugLogging.url if log.ThugOpts.local else log.last_url, html
    )
    self.tag.clear()
    for node in bs4.BeautifulSoup(html, ""html.parser"").contents:
        self.tag.append(node)
        name = getattr(node, ""name"", None)
        if name is None:
            continue
        handler = getattr(log.DFT, ""handle_%s"" % (name,), None)
        if handler:
            handler(node)
",if handler :,151
"def get_supported_period_type_map(cls):
    if cls.supported_period_map is None:
        cls.supported_period_map = {}
        cls.supported_period_map.update(cls.period_type_map)
        try:
            from dateutil import relativedelta
            if relativedelta is not None:
                cls.supported_period_map.update(cls.optional_period_type_map)
        except Exception:
            pass
    return cls.supported_period_map
",if relativedelta is not None :,131
"def _compare_single_run(self, compares_done):
    try:
        compare_id, redo = self.in_queue.get(
            timeout=float(self.config[""ExpertSettings""][""block_delay""])
        )
    except Empty:
        pass
    else:
        if self._decide_whether_to_process(compare_id, redo, compares_done):
            if redo:
                self.db_interface.delete_old_compare_result(compare_id)
            compares_done.add(compare_id)
            self._process_compare(compare_id)
            if self.callback:
                self.callback()
","if self . _decide_whether_to_process ( compare_id , redo , compares_done ) :",177
"def _get_field_actual(cant_be_number, raw_string, field_names):
    for line in raw_string.splitlines():
        for field_name in field_names:
            field_name = field_name.lower()
            if "":"" in line:
                left, right = line.split("":"", 1)
                left = left.strip().lower()
                right = right.strip()
                if left == field_name and len(right) > 0:
                    if cant_be_number:
                        if not right.isdigit():
                            return right
                    else:
                        return right
    return None
",if not right . isdigit ( ) :,184
"def _p_basicstr_content(s, content=_basicstr_re):
    res = []
    while True:
        res.append(s.expect_re(content).group(0))
        if not s.consume(""\\""):
            break
        if s.consume_re(_newline_esc_re):
            pass
        elif s.consume_re(_short_uni_re) or s.consume_re(_long_uni_re):
            res.append(_chr(int(s.last().group(1), 16)))
        else:
            s.expect_re(_escapes_re)
            res.append(_escapes[s.last().group(0)])
    return """".join(res)
",if s . consume_re ( _newline_esc_re ) :,179
"def removedir(self, path):
    # type: (Text) -> None
    _path = self.validatepath(path)
    if _path == ""/"":
        raise errors.RemoveRootError()
    with ftp_errors(self, path):
        try:
            self.ftp.rmd(_encode(_path, self.ftp.encoding))
        except error_perm as error:
            code, _ = _parse_ftp_error(error)
            if code == ""550"":
                if self.isfile(path):
                    raise errors.DirectoryExpected(path)
                if not self.isempty(path):
                    raise errors.DirectoryNotEmpty(path)
            raise  # pragma: no cover
",if not self . isempty ( path ) :,189
"def _normalize_store_path(self, resource_store):
    if resource_store[""type""] == ""filesystem"":
        if not os.path.isabs(resource_store[""base_directory""]):
            resource_store[""base_directory""] = os.path.join(
                self.root_directory, resource_store[""base_directory""]
            )
    return resource_store
","if not os . path . isabs ( resource_store [ ""base_directory"" ] ) :",96
"def _apply_nested(name, val, nested):
    parts = name.split(""."")
    cur = nested
    for i in range(0, len(parts) - 1):
        cur = cur.setdefault(parts[i], {})
        if not isinstance(cur, dict):
            conflicts_with = ""."".join(parts[0 : i + 1])
            raise ValueError(
                ""%r cannot be nested: conflicts with {%r: %s}""
                % (name, conflicts_with, cur)
            )
    cur[parts[-1]] = val
","if not isinstance ( cur , dict ) :",142
"def build_packages(targeted_packages, distribution_directory, is_dev_build=False):
    # run the build and distribution
    for package_root in targeted_packages:
        service_hierarchy = os.path.join(os.path.basename(package_root))
        if is_dev_build:
            verify_update_package_requirement(package_root)
        print(""Generating Package Using Python {}"".format(sys.version))
        run_check_call(
            [
                sys.executable,
                build_packing_script_location,
                ""--dest"",
                os.path.join(distribution_directory, service_hierarchy),
                package_root,
            ],
            root_dir,
        )
",if is_dev_build :,199
"def resolve_root_node_address(self, root_node):
    if ""["" in root_node:
        name, numbers = root_node.split(""["", maxsplit=1)
        number = numbers.split("","", maxsplit=1)[0]
        if ""-"" in number:
            number = number.split(""-"")[0]
        number = re.sub(""[^0-9]"", """", number)
        root_node = name + number
    return root_node
","if ""-"" in number :",109
"def _map_args(maps: dict, **kwargs):
    # maps: key=old name, value= new name
    output = {}
    for name, val in kwargs.items():
        if name in maps:
            assert isinstance(maps[name], str)
            output.update({maps[name]: val})
        else:
            output.update({name: val})
    for keys in maps.keys():
        if keys not in output.keys():
            pass
    return output
",if keys not in output . keys ( ) :,125
"def next_item(self, direction):
    """"""Selects next menu item, based on self._direction""""""
    start, i = -1, 0
    try:
        start = self.items.index(self._selected)
        i = start + direction
    except:
        pass
    while True:
        if i == start:
            # Cannot find valid menu item
            self.select(start)
            break
        if i >= len(self.items):
            i = 0
            continue
        if i < 0:
            i = len(self.items) - 1
            continue
        if self.select(i):
            break
        i += direction
        if start < 0:
            start = 0
",if i >= len ( self . items ) :,194
"def detect_reentrancy(self, contract):
    for function in contract.functions_and_modifiers_declared:
        if function.is_implemented:
            if self.KEY in function.context:
                continue
            self._explore(function.entry_point, [])
            function.context[self.KEY] = True
",if function . is_implemented :,87
"def load_model(self):
    if not os.path.exists(self.get_filename(absolute=True)):
        if args.train:
            return {}, {}
        error(
            ""Model file with pre-trained convolution layers not found. Download it here..."",
            ""https://github.com/alexjc/neural-enhance/releases/download/v%s/%s""
            % (__version__, self.get_filename()),
        )
    print(""  - Loaded file `{}` with trained model."".format(self.get_filename()))
    return pickle.load(bz2.open(self.get_filename(), ""rb""))
",if args . train :,158
"def get_nonexisting_check_definition_extends(definition, indexed_oval_defs):
    # TODO: handle multiple levels of referrals.
    # OVAL checks that go beyond one level of extend_definition won't be properly identified
    for extdefinition in definition.findall("".//{%s}extend_definition"" % oval_ns):
        # Verify each extend_definition in the definition
        extdefinitionref = extdefinition.get(""definition_ref"")
        # Search the OVAL tree for a definition with the referred ID
        referreddefinition = indexed_oval_defs.get(extdefinitionref)
        if referreddefinition is None:
            # There is no oval satisfying the extend_definition referal
            return extdefinitionref
    return None
",if referreddefinition is None :,177
"def pause(self):
    if self.is_playing:
        self.state = MusicPlayerState.PAUSED
        if self._current_player:
            self._current_player.pause()
        self.emit(""pause"", player=self, entry=self.current_entry)
        return
    elif self.is_paused:
        return
    raise ValueError(""Cannot pause a MusicPlayer in state %s"" % self.state)
",if self . _current_player :,107
"def setNextFormPrevious(self, backup=STARTING_FORM):
    try:
        if self._THISFORM.FORM_NAME == self._FORM_VISIT_LIST[-1]:
            self._FORM_VISIT_LIST.pop()  # Remove the current form. if it is at the end of the list
        if self._THISFORM.FORM_NAME == self.NEXT_ACTIVE_FORM:
            # take no action if it looks as if someone has already set the next form.
            self.setNextForm(
                self._FORM_VISIT_LIST.pop()
            )  # Switch to the previous form if one exists
    except IndexError:
        self.setNextForm(backup)
",if self . _THISFORM . FORM_NAME == self . _FORM_VISIT_LIST [ - 1 ] :,178
"def get_expr_referrers(schema: s_schema.Schema, obj: so.Object) -> Dict[so.Object, str]:
    """"""Return schema referrers with refs in expressions.""""""
    refs = schema.get_referrers_ex(obj)
    result = {}
    for (mcls, fn), referrers in refs.items():
        field = mcls.get_field(fn)
        if issubclass(field.type, (Expression, ExpressionList)):
            result.update({ref: fn for ref in referrers})
    return result
","if issubclass ( field . type , ( Expression , ExpressionList ) ) :",136
"def _fields_to_index(cls):
    fields = []
    for field in cls._meta.sorted_fields:
        if field.primary_key:
            continue
        requires_index = any(
            (field.index, field.unique, isinstance(field, ForeignKeyField))
        )
        if requires_index:
            fields.append(field)
    return fields
",if field . primary_key :,99
"def ident_values(self):
    value = self._ident_values
    if value is False:
        value = None
        # XXX: how will this interact with orig_prefix ?
        #      not exposing attrs for now if orig_prefix is set.
        if not self.orig_prefix:
            wrapped = self.wrapped
            idents = getattr(wrapped, ""ident_values"", None)
            if idents:
                value = [self._wrap_hash(ident) for ident in idents]
            ##else:
            ##    ident = self.ident
            ##    if ident is not None:
            ##        value = [ident]
        self._ident_values = value
    return value
",if idents :,200
"def apply_incpaths_ml(self):
    inc_lst = self.includes.split()
    lst = self.incpaths_lst
    for dir in inc_lst:
        node = self.path.find_dir(dir)
        if not node:
            error(""node not found: "" + str(dir))
            continue
        if not node in lst:
            lst.append(node)
        self.bld_incpaths_lst.append(node)
",if not node in lst :,121
"def application_openFiles_(self, nsapp, filenames):
    # logging.info('[osx] file open')
    # logging.info('[osx] file : %s' % (filenames))
    for filename in filenames:
        logging.info(""[osx] receiving from macOS : %s"", filename)
        if os.path.exists(filename):
            if sabnzbd.filesystem.get_ext(filename) in VALID_ARCHIVES + VALID_NZB_FILES:
                sabnzbd.add_nzbfile(filename, keep=True)
",if sabnzbd . filesystem . get_ext ( filename ) in VALID_ARCHIVES + VALID_NZB_FILES :,136
"def check(self, xp, nout):
    input = xp.asarray(self.x).astype(numpy.float32)
    with warnings.catch_warnings():
        if self.ignore_warning:
            warnings.simplefilter(""ignore"", self.ignore_warning)
        if self.result:
            self.check_positive(xp, self.func, input, self.eps, nout)
        else:
            self.check_negative(xp, self.func, input, self.eps, nout)
",if self . result :,125
"def _set_scheme(url, newscheme):
    scheme = _get_scheme(url)
    newscheme = newscheme or """"
    newseparator = "":"" if newscheme in COLON_SEPARATED_SCHEMES else ""://""
    if scheme == """":  # Protocol relative URL.
        url = ""%s:%s"" % (newscheme, url)
    elif scheme is None and url:  # No scheme.
        url = """".join([newscheme, newseparator, url])
    elif scheme:  # Existing scheme.
        remainder = url[len(scheme) :]
        if remainder.startswith(""://""):
            remainder = remainder[3:]
        elif remainder.startswith("":""):
            remainder = remainder[1:]
        url = """".join([newscheme, newseparator, remainder])
    return url
","if remainder . startswith ( ""://"" ) :",191
"def parquet(tables, data_directory, ignore_missing_dependency, **params):
    try:
        import pyarrow as pa  # noqa: F401
        import pyarrow.parquet as pq  # noqa: F401
    except ImportError:
        msg = ""PyArrow dependency is missing""
        if ignore_missing_dependency:
            logger.warning(""Ignored: %s"", msg)
            return 0
        else:
            raise click.ClickException(msg)
    data_directory = Path(data_directory)
    for table, df in read_tables(tables, data_directory):
        arrow_table = pa.Table.from_pandas(df)
        target_path = data_directory / ""{}.parquet"".format(table)
        pq.write_table(arrow_table, str(target_path))
",if ignore_missing_dependency :,199
"def h2i(self, pkt, s):
    t = ()
    if type(s) is str:
        t = time.strptime(s)
        t = t[:2] + t[2:-3]
    else:
        if not s:
            y, m, d, h, min, sec, rest, rest, rest = time.gmtime(time.time())
            t = (y, m, d, h, min, sec)
        else:
            t = s
    return t
",if not s :,130
"def filter_episodes(self, batch, cross_entropy):
    """"""Filter the episodes for the cross_entropy method""""""
    accumulated_reward = [sum(rewards) for rewards in batch[""rewards""]]
    percentile = cross_entropy * 100
    reward_bound = np.percentile(accumulated_reward, percentile)
    # we save the batch with reward above the bound
    result = {k: [] for k in self.data_keys}
    episode_kept = 0
    for i in range(len(accumulated_reward)):
        if accumulated_reward[i] >= reward_bound:
            for k in self.data_keys:
                result[k].append(batch[k][i])
            episode_kept += 1
    return result
",if accumulated_reward [ i ] >= reward_bound :,181
"def _readenv(var, msg):
    match = _ENV_VAR_PAT.match(var)
    if match and match.groups():
        envvar = match.groups()[0]
        if envvar in os.environ:
            value = os.environ[envvar]
            if six.PY2:
                value = value.decode(""utf8"")
            return value
        else:
            raise InvalidConfigException(
                ""{} - environment variable '{}' not set"".format(msg, var)
            )
    else:
        raise InvalidConfigException(
            ""{} - environment variable name '{}' does not match pattern '{}'"".format(
                msg, var, _ENV_VAR_PAT_STR
            )
        )
",if six . PY2 :,190
"def _allocate_nbd(self):
    if not os.path.exists(""/sys/block/nbd0""):
        self.error = _(""nbd unavailable: module not loaded"")
        return None
    while True:
        if not self._DEVICES:
            # really want to log this info, not raise
            self.error = _(""No free nbd devices"")
            return None
        device = self._DEVICES.pop()
        if not os.path.exists(""/sys/block/%s/pid"" % os.path.basename(device)):
            break
    return device
","if not os . path . exists ( ""/sys/block/%s/pid"" % os . path . basename ( device ) ) :",146
"def _expand_deps_java_generation(self):
    """"""Ensure that all multilingual dependencies such as proto_library generate java code.""""""
    queue = collections.deque(self.deps)
    keys = set()
    while queue:
        k = queue.popleft()
        if k not in keys:
            keys.add(k)
            dep = self.target_database[k]
            if ""generate_java"" in dep.attr:  # Has this attribute
                dep.attr[""generate_java""] = True
                queue.extend(dep.deps)
",if k not in keys :,144
"def load_syntax(syntax):
    context = _create_scheme() or {}
    partition_scanner = PartitionScanner(syntax.get(""partitions"", []))
    scanners = {}
    for part_name, part_scanner in list(syntax.get(""scanner"", {}).items()):
        scanners[part_name] = Scanner(part_scanner)
    formats = []
    for fname, fstyle in list(syntax.get(""formats"", {}).items()):
        if isinstance(fstyle, basestring):
            if fstyle.startswith(""%("") and fstyle.endswith("")s""):
                key = fstyle[2:-2]
                fstyle = context[key]
            else:
                fstyle = fstyle % context
        formats.append((fname, fstyle))
    return partition_scanner, scanners, formats
","if fstyle . startswith ( ""%("" ) and fstyle . endswith ( "")s"" ) :",199
"def rollback(self):
    for operation, values in self.current_transaction_state[::-1]:
        if operation == ""insert"":
            values.remove()
        elif operation == ""update"":
            old_value, new_value = values
            if new_value.full_filename != old_value.full_filename:
                os.unlink(new_value.full_filename)
            old_value.write()
    self._post_xact_cleanup()
","if operation == ""insert"" :",121
"def _buildOffsets(offsetDict, localeData, indexStart):
    o = indexStart
    for key in localeData:
        if ""|"" in key:
            for k in key.split(""|""):
                offsetDict[k] = o
        else:
            offsetDict[key] = o
        o += 1
","if ""|"" in key :",83
"def _check_start_pipeline_execution_errors(
    graphene_info, execution_params, execution_plan
):
    if execution_params.step_keys:
        for step_key in execution_params.step_keys:
            if not execution_plan.has_step(step_key):
                raise UserFacingGraphQLError(
                    graphene_info.schema.type_named(""InvalidStepError"")(
                        invalid_step_key=step_key
                    )
                )
",if not execution_plan . has_step ( step_key ) :,132
"def __setattr__(self, option_name, option_value):
    if option_name in self._options:
        # type checking
        sort = self.OPTIONS[self.arch.name][option_name][0]
        if sort is None or isinstance(option_value, sort):
            self._options[option_name] = option_value
        else:
            raise ValueError(
                'Value for option ""%s"" must be of type %s' % (option_name, sort)
            )
    else:
        super(CFGArchOptions, self).__setattr__(option_name, option_value)
","if sort is None or isinstance ( option_value , sort ) :",155
"def value(self):
    quote = False
    if self.defects:
        quote = True
    else:
        for x in self:
            if x.token_type == ""quoted-string"":
                quote = True
    if quote:
        pre = post = """"
        if self[0].token_type == ""cfws"" or self[0][0].token_type == ""cfws"":
            pre = "" ""
        if self[-1].token_type == ""cfws"" or self[-1][-1].token_type == ""cfws"":
            post = "" ""
        return pre + quote_string(self.display_name) + post
    else:
        return super(DisplayName, self).value
","if x . token_type == ""quoted-string"" :",186
"def __init__(self, patch_files, patch_directories):
    files = []
    files_data = {}
    for filename_data in patch_files:
        if isinstance(filename_data, list):
            filename, data = filename_data
        else:
            filename = filename_data
            data = None
        if not filename.startswith(os.sep):
            filename = ""{0}{1}"".format(FakeState.deploy_dir, filename)
        files.append(filename)
        if data:
            files_data[filename] = data
    self.files = files
    self.files_data = files_data
    self.directories = patch_directories
","if isinstance ( filename_data , list ) :",171
"def _evaluateStack(s):
    op = s.pop()
    if op in ""+-*/@^"":
        op2 = _evaluateStack(s)
        op1 = _evaluateStack(s)
        result = opn[op](op1, op2)
        if debug_flag:
            print(result)
        return result
    else:
        return op
",if debug_flag :,97
"def reconnect_user(self, user_id, host_id, server_id):
    if host_id == settings.local.host_id:
        return
    if server_id and self.server.id != server_id:
        return
    for client in self.clients.find({""user_id"": user_id}):
        self.clients.update_id(
            client[""id""],
            {
                ""ignore_routes"": True,
            },
        )
        if len(client[""id""]) > 32:
            self.instance.disconnect_wg(client[""id""])
        else:
            self.instance_com.client_kill(client[""id""])
","if len ( client [ ""id"" ] ) > 32 :",176
"def _get_library(self, name, args):
    library_database = self._library_manager.get_new_connection_to_library_database()
    try:
        last_updated = library_database.get_library_last_updated(name, args)
        if last_updated:
            if time.time() - last_updated > 10.0:
                self._library_manager.fetch_keywords(
                    name, args, self._libraries_need_refresh_listener
                )
            return library_database.fetch_library_keywords(name, args)
        return self._library_manager.get_and_insert_keywords(name, args)
    finally:
        library_database.close()
",if time . time ( ) - last_updated > 10.0 :,184
"def get_paths(self, path, commit):
    """"""Return a generator of all filepaths under path at commit.""""""
    _check_path_is_repo_relative(path)
    git_path = _get_git_path(path)
    tree = self.gl_repo.git_repo[commit.tree[git_path].id]
    assert tree.type == pygit2.GIT_OBJ_TREE
    for tree_entry in tree:
        tree_entry_path = os.path.join(path, tree_entry.name)
        if tree_entry.type == ""tree"":
            for fp in self.get_paths(tree_entry_path, commit):
                yield fp
        else:
            yield tree_entry_path
","if tree_entry . type == ""tree"" :",185
"def scan_resource_conf(self, conf):
    if ""properties"" in conf:
        if ""attributes"" in conf[""properties""]:
            if ""exp"" in conf[""properties""][""attributes""]:
                if conf[""properties""][""attributes""][""exp""]:
                    return CheckResult.PASSED
    return CheckResult.FAILED
","if ""exp"" in conf [ ""properties"" ] [ ""attributes"" ] :",82
"def _set_parse_context(self, tag, tag_attrs):
    # special case: script or style parse context
    if not self._wb_parse_context:
        if tag == ""style"":
            self._wb_parse_context = ""style""
        elif tag == ""script"":
            if self._allow_js_type(tag_attrs):
                self._wb_parse_context = ""script""
",if self . _allow_js_type ( tag_attrs ) :,106
"def modified(self):
    paths = set()
    dictionary_list = []
    for op_list in self._operations:
        if not isinstance(op_list, list):
            op_list = (op_list,)
        for item in chain(*op_list):
            if item is None:
                continue
            dictionary = item.dictionary
            if dictionary.path in paths:
                continue
            paths.add(dictionary.path)
            dictionary_list.append(dictionary)
    return dictionary_list
",if item is None :,139
"def preorder(root):
    res = []
    if not root:
        return res
    stack = []
    stack.append(root)
    while stack:
        root = stack.pop()
        res.append(root.val)
        if root.right:
            stack.append(root.right)
        if root.left:
            stack.append(root.left)
    return res
",if root . right :,105
"def create(exported_python_target):
    if exported_python_target not in created:
        self.context.log.info(
            ""Creating setup.py project for {}"".format(exported_python_target)
        )
        subject = self.derived_by_original.get(
            exported_python_target, exported_python_target
        )
        setup_dir, dependencies = self.create_setup_py(subject, dist_dir)
        created[exported_python_target] = setup_dir
        if self._recursive:
            for dep in dependencies:
                if is_exported_python_target(dep):
                    create(dep)
",if is_exported_python_target ( dep ) :,172
"def test_array_interface(self, data):
    result = np.array(data)
    np.testing.assert_array_equal(result[0], data[0])
    result = np.array(data, dtype=object)
    expected = np.array(list(data), dtype=object)
    for a1, a2 in zip(result, expected):
        if np.isscalar(a1):
            assert np.isnan(a1) and np.isnan(a2)
        else:
            tm.assert_numpy_array_equal(a2, a1)
",if np . isscalar ( a1 ) :,143
"def valueChanged(plug):
    changed = plug.getInput() is not None
    if not changed and isinstance(plug, Gaffer.ValuePlug):
        if Gaffer.NodeAlgo.hasUserDefault(plug):
            changed = not Gaffer.NodeAlgo.isSetToUserDefault(plug)
        else:
            changed = not plug.isSetToDefault()
    return changed
",if Gaffer . NodeAlgo . hasUserDefault ( plug ) :,101
"def process_tag(hive_name, company, company_key, tag, default_arch):
    with winreg.OpenKeyEx(company_key, tag) as tag_key:
        version = load_version_data(hive_name, company, tag, tag_key)
        if version is not None:  # if failed to get version bail
            major, minor, _ = version
            arch = load_arch_data(hive_name, company, tag, tag_key, default_arch)
            if arch is not None:
                exe_data = load_exe(hive_name, company, company_key, tag)
                if exe_data is not None:
                    exe, args = exe_data
                    return company, major, minor, arch, exe, args
",if version is not None :,199
"def __iter__(self):
    for name, value in self.__class__.__dict__.items():
        if isinstance(value, alias_flag_value):
            continue
        if isinstance(value, flag_value):
            yield (name, self._has_flag(value.flag))
","if isinstance ( value , flag_value ) :",71
"def connect(self):
    self.sock = sockssocket()
    self.sock.setproxy(*proxy_args)
    if type(self.timeout) in (int, float):
        self.sock.settimeout(self.timeout)
    self.sock.connect((self.host, self.port))
    if isinstance(self, compat_http_client.HTTPSConnection):
        if hasattr(self, ""_context""):  # Python > 2.6
            self.sock = self._context.wrap_socket(self.sock, server_hostname=self.host)
        else:
            self.sock = ssl.wrap_socket(self.sock)
","if hasattr ( self , ""_context"" ) :",158
"def frequent_thread_switches():
    """"""Make concurrency bugs more likely to manifest.""""""
    interval = None
    if not sys.platform.startswith(""java""):
        if hasattr(sys, ""getswitchinterval""):
            interval = sys.getswitchinterval()
            sys.setswitchinterval(1e-6)
        else:
            interval = sys.getcheckinterval()
            sys.setcheckinterval(1)
    try:
        yield
    finally:
        if not sys.platform.startswith(""java""):
            if hasattr(sys, ""setswitchinterval""):
                sys.setswitchinterval(interval)
            else:
                sys.setcheckinterval(interval)
","if hasattr ( sys , ""setswitchinterval"" ) :",177
"def vars(self):
    ret = []
    if op.intlist:
        varlist = op.intlist
    else:
        varlist = self.discover
        for name in varlist:
            if name in (""0"", ""1"", ""2"", ""8"", ""CPU0"", ""ERR"", ""LOC"", ""MIS"", ""NMI""):
                varlist.remove(name)
        if not op.full and len(varlist) > 3:
            varlist = varlist[-3:]
    for name in varlist:
        if name in self.discover:
            ret.append(name)
        elif name.lower() in self.intmap:
            ret.append(self.intmap[name.lower()])
    return ret
",elif name . lower ( ) in self . intmap :,191
"def deleteDuplicates(gadgets, callback=None):
    toReturn = []
    inst = set()
    count = 0
    added = False
    len_gadgets = len(gadgets)
    for i, gadget in enumerate(gadgets):
        inst.add(gadget._gadget)
        if len(inst) > count:
            count = len(inst)
            toReturn.append(gadget)
            added = True
        if callback:
            callback(gadget, added, float(i + 1) / (len_gadgets))
            added = False
    return toReturn
",if len ( inst ) > count :,164
"def ident(self):
    value = self._ident
    if value is False:
        value = None
        # XXX: how will this interact with orig_prefix ?
        #      not exposing attrs for now if orig_prefix is set.
        if not self.orig_prefix:
            wrapped = self.wrapped
            ident = getattr(wrapped, ""ident"", None)
            if ident is not None:
                value = self._wrap_hash(ident)
        self._ident = value
    return value
",if ident is not None :,135
"def _flatten_settings_from_form(self, settings, form, form_values):
    """"""Take a nested dict and return a flat dict of setting values.""""""
    setting_values = {}
    for field in form.c:
        if isinstance(field, _ContainerMixin):
            setting_values.update(
                self._flatten_settings_from_form(
                    settings, field, form_values[field._name]
                )
            )
        elif field._name in settings:
            setting_values[field._name] = form_values[field._name]
    return setting_values
",elif field . _name in settings :,156
"def _decorator(cls):
    for name, meth in inspect.getmembers(cls, inspect.isroutine):
        if name not in cls.__dict__:
            continue
        if name != ""__init__"":
            if not private and name.startswith(""_""):
                continue
        if name in butnot:
            continue
        setattr(cls, name, decorator(meth))
    return cls
","if not private and name . startswith ( ""_"" ) :",99
"def _do_cmp(f1, f2):
    bufsize = BUFSIZE
    with open(f1, ""rb"") as fp1, open(f2, ""rb"") as fp2:
        while True:
            b1 = fp1.read(bufsize)
            b2 = fp2.read(bufsize)
            if b1 != b2:
                return False
            if not b1:
                return True
",if b1 != b2 :,118
"def _memoized(*args):
    now = time.time()
    try:
        value, last_update = self.cache[args]
        age = now - last_update
        if self._call_count > self.ctl or age > self.ttl:
            self._call_count = 0
            raise AttributeError
        if self.ctl:
            self._call_count += 1
        return value
    except (KeyError, AttributeError):
        value = func(*args)
        if value:
            self.cache[args] = (value, now)
        return value
    except TypeError:
        return func(*args)
",if self . ctl :,164
"def check(self, hyperlinks: Dict[str, Hyperlink]) -> Generator[CheckResult, None, None]:
    self.invoke_threads()
    total_links = 0
    for hyperlink in hyperlinks.values():
        if self.is_ignored_uri(hyperlink.uri):
            yield CheckResult(
                hyperlink.uri, hyperlink.docname, hyperlink.lineno, ""ignored"", """", 0
            )
        else:
            self.wqueue.put(CheckRequest(CHECK_IMMEDIATELY, hyperlink), False)
            total_links += 1
    done = 0
    while done < total_links:
        yield self.rqueue.get()
        done += 1
    self.shutdown_threads()
",if self . is_ignored_uri ( hyperlink . uri ) :,188
"def remove_subscriber(self, topic, subscriber):
    if subscriber in self.subscribers[topic]:
        if hasattr(subscriber, ""_pyroRelease""):
            subscriber._pyroRelease()
        if hasattr(subscriber, ""_pyroUri""):
            try:
                proxy = self.proxy_cache[subscriber._pyroUri]
                proxy._pyroRelease()
                del self.proxy_cache[subscriber._pyroUri]
            except KeyError:
                pass
        self.subscribers[topic].discard(subscriber)
","if hasattr ( subscriber , ""_pyroRelease"" ) :",139
"def delete_arc(collection, document, origin, target, type):
    directory = collection
    real_dir = real_directory(directory)
    mods = ModificationTracker()
    projectconf = ProjectConfiguration(real_dir)
    document = path_join(real_dir, document)
    with TextAnnotations(document) as ann_obj:
        # bail as quick as possible if read-only
        if ann_obj._read_only:
            raise AnnotationsIsReadOnlyError(ann_obj.get_document())
        _delete_arc_with_ann(origin, target, type, mods, ann_obj, projectconf)
        mods_json = mods.json_response()
        mods_json[""annotations""] = _json_from_ann(ann_obj)
        return mods_json
",if ann_obj . _read_only :,193
"def _select_from(self, parent_path, is_dir, exists, listdir):
    if not is_dir(parent_path):
        return
    with _cached(listdir) as listdir:
        yielded = set()
        try:
            successor_select = self.successor._select_from
            for starting_point in self._iterate_directories(
                parent_path, is_dir, listdir
            ):
                for p in successor_select(starting_point, is_dir, exists, listdir):
                    if p not in yielded:
                        yield p
                        yielded.add(p)
        finally:
            yielded.clear()
",if p not in yielded :,183
"def _fractional_part(self, n, expr, evaluation):
    n_sympy = n.to_sympy()
    if n_sympy.is_constant():
        if n_sympy >= 0:
            positive_integer_part = (
                Expression(""Floor"", n).evaluate(evaluation).to_python()
            )
            result = n - positive_integer_part
        else:
            negative_integer_part = (
                Expression(""Ceiling"", n).evaluate(evaluation).to_python()
            )
            result = n - negative_integer_part
    else:
        return expr
    return from_python(result)
",if n_sympy >= 0 :,169
"def check_bounds(geometry):
    if isinstance(geometry[0], (list, tuple)):
        return list(map(check_bounds, geometry))
    else:
        if geometry[0] > 180 or geometry[0] < -180:
            raise ValueError(
                ""Longitude is out of bounds, check your JSON format or data""
            )
        if geometry[1] > 90 or geometry[1] < -90:
            raise ValueError(
                ""Latitude is out of bounds, check your JSON format or data""
            )
",if geometry [ 1 ] > 90 or geometry [ 1 ] < - 90 :,143
"def get_absolute_path(self, root, path):
    # find the first absolute path that exists
    self.root = self.roots[0]
    for root in self.roots:
        abspath = os.path.abspath(os.path.join(root, path))
        if os.path.exists(abspath):
            self.root = root  # make sure all the other methods in the base class know how to find the file
            break
    return abspath
",if os . path . exists ( abspath ) :,114
"def do_setflow(self, l=""""):
    try:
        if not isinstance(l, str) or not len(l):
            l = str(self.flow_slider.GetValue())
        else:
            l = l.lower()
        flow = int(l)
        if self.p.online:
            self.p.send_now(""M221 S"" + l)
            self.log(_(""Setting print flow factor to %d%%."") % flow)
        else:
            self.logError(_(""Printer is not online.""))
    except Exception as x:
        self.logError(_(""You must enter a flow. (%s)"") % (repr(x),))
","if not isinstance ( l , str ) or not len ( l ) :",170
"def sources():
    for d in os.listdir(base):
        #        if d.startswith('talis'):
        #            continue
        if d.endswith(""old""):
            continue
        if d == ""indcat"":
            continue
        if not os.path.isdir(base + d):
            continue
        yield d
","if d . endswith ( ""old"" ) :",105
"def create_accumulator(self) -> tf_metric_accumulators.TFCompilableMetricsAccumulator:
    configs = zip(self._metric_configs, self._loss_configs)
    padding_options = None
    if self._eval_config is not None:
        model_spec = model_util.get_model_spec(self._eval_config, self._model_name)
        if model_spec is not None and model_spec.HasField(""padding_options""):
            padding_options = model_spec.padding_options
    return tf_metric_accumulators.TFCompilableMetricsAccumulator(
        padding_options,
        [len(m) + len(l) for m, l in configs],
        desired_batch_size=self._desired_batch_size,
    )
","if model_spec is not None and model_spec . HasField ( ""padding_options"" ) :",196
"def parseImpl(self, instring, loc, doActions=True):
    try:
        loc, tokens = self.expr._parse(instring, loc, doActions, callPreParse=False)
    except (ParseException, IndexError):
        if self.defaultValue is not self.__optionalNotMatched:
            if self.expr.resultsName:
                tokens = ParseResults([self.defaultValue])
                tokens[self.expr.resultsName] = self.defaultValue
            else:
                tokens = [self.defaultValue]
        else:
            tokens = []
    return loc, tokens
",if self . defaultValue is not self . __optionalNotMatched :,157
"def handleConnection(self):
    # connection handshake
    try:
        if self.daemon._handshake(self.csock):
            return True
        self.csock.close()
    except:
        ex_t, ex_v, ex_tb = sys.exc_info()
        tb = util.formatTraceback(ex_t, ex_v, ex_tb)
        log.warning(""error during connect/handshake: %s; %s"", ex_v, ""\n"".join(tb))
        self.csock.close()
    return False
",if self . daemon . _handshake ( self . csock ) :,138
"def getProc(su, innerTarget):
    if len(su) == 1:  # have a one element wedge
        proc = (""first"", ""last"")
    else:
        if su.isFirst(innerTarget) and su.isLast(innerTarget):
            proc = (""first"", ""last"")  # same element can be first and last
        elif su.isFirst(innerTarget):
            proc = (""first"",)
        elif su.isLast(innerTarget):
            proc = (""last"",)
        else:
            proc = ()
    return proc
",if su . isFirst ( innerTarget ) and su . isLast ( innerTarget ) :,143
"def get_color_dtype(data, column_names):
    has_color = all(column in data[""points""] for column in column_names)
    if has_color:
        color_data_types = [
            data[""points""][column_name].dtype for column_name in column_names
        ]
        if len(set(color_data_types)) > 1:
            raise TypeError(
                f""Data types of color values are inconsistent: got {color_data_types}""
            )
        color_data_type = color_data_types[0]
    else:
        color_data_type = None
    return color_data_type
",if len ( set ( color_data_types ) ) > 1 :,168
"def close(self):
    children = []
    for children_part, line_offset, last_line_offset_leaf in self.children_groups:
        if line_offset != 0:
            try:
                _update_positions(children_part, line_offset, last_line_offset_leaf)
            except _PositionUpdatingFinished:
                pass
        children += children_part
    self.tree_node.children = children
    # Reset the parents
    for node in children:
        node.parent = self.tree_node
",if line_offset != 0 :,138
"def get_multi(self, keys, index=None):
    with self._lmdb.begin() as txn:
        result = []
        for key in keys:
            packed = txn.get(key.encode())
            if packed is not None:
                result.append((key, cbor.loads(packed)))
    return result
",if packed is not None :,86
"def get_directory_info(prefix, pth, recursive):
    res = []
    directory = os.listdir(pth)
    directory.sort()
    for p in directory:
        if p[0] != ""."":
            subp = os.path.join(pth, p)
            p = os.path.join(prefix, p)
            if recursive and os.path.isdir(subp):
                res.append([p, get_directory_info(prefix, subp, 1)])
            else:
                res.append([p, None])
    return res
",if recursive and os . path . isdir ( subp ) :,148
"def __schedule(self, workflow_scheduler_id, workflow_scheduler):
    invocation_ids = self.__active_invocation_ids(workflow_scheduler_id)
    for invocation_id in invocation_ids:
        log.debug(""Attempting to schedule workflow invocation [%s]"", invocation_id)
        self.__attempt_schedule(invocation_id, workflow_scheduler)
        if not self.monitor_running:
            return
",if not self . monitor_running :,103
"def write(self, data):
    self.size -= len(data)
    passon = None
    if self.size > 0:
        self.data.append(data)
    else:
        if self.size:
            data, passon = data[: self.size], data[self.size :]
        else:
            passon = b""""
        if data:
            self.data.append(data)
    return passon
",if data :,114
"def __getstate__(self):
    try:
        store_func, load_func = self.store_function, self.load_function
        self.store_function, self.load_function = None, None
        # ignore analyses. we re-initialize analyses when restoring from pickling so that we do not lose any newly
        # added analyses classes
        d = dict(
            (k, v)
            for k, v in self.__dict__.items()
            if k
            not in {
                ""analyses"",
            }
        )
        return d
    finally:
        self.store_function, self.load_function = store_func, load_func
",if k,175
"def mouse_down(self, event):
    if event.button == 1:
        if self.scrolling:
            p = event.local
            if self.scroll_up_rect().collidepoint(p):
                self.scroll_up()
                return
            elif self.scroll_down_rect().collidepoint(p):
                self.scroll_down()
                return
    if event.button == 4:
        self.scroll_up()
    if event.button == 5:
        self.scroll_down()
    GridView.mouse_down(self, event)
",if self . scroll_up_rect ( ) . collidepoint ( p ) :,160
"def on_api_command(self, command, data):
    if command == ""select"":
        if not Permissions.PLUGIN_ACTION_COMMAND_PROMPT_INTERACT.can():
            return flask.abort(403, ""Insufficient permissions"")
        if self._prompt is None:
            return flask.abort(409, ""No active prompt"")
        choice = data[""choice""]
        if not isinstance(choice, int) or not self._prompt.validate_choice(choice):
            return flask.abort(
                400, ""{!r} is not a valid value for choice"".format(choice)
            )
        self._answer_prompt(choice)
","if not isinstance ( choice , int ) or not self . _prompt . validate_choice ( choice ) :",164
"def register_predictors(self, model_data_arr):
    for integration in self._get_integrations():
        if integration.check_connection():
            integration.register_predictors(model_data_arr)
        else:
            logger.warning(
                f""There is no connection to {integration.name}. predictor wouldn't be registred.""
            )
",if integration . check_connection ( ) :,98
"def _pack_shears(shearData):
    shears = list()
    vidxs = list()
    for e_idx, entry in enumerate(shearData):
        # Should be 3 entries
        if entry is None:
            shears.extend([float(""nan""), float(""nan"")])
            vidxs.extend([0, 0])
        else:
            vidx1, vidx2, shear1, shear2 = entry
            shears.extend([shear1, shear2])
            vidxs.extend([vidx1, vidx2])
    return (np.asarray(shears, dtype=np.float32), np.asarray(vidxs, dtype=np.uint32))
",if entry is None :,173
"def aiter_cogs(cls) -> AsyncIterator[Tuple[str, str]]:
    yield ""Core"", ""0""
    for _dir in data_manager.cog_data_path().iterdir():
        fpath = _dir / ""settings.json""
        if not fpath.exists():
            continue
        with fpath.open() as f:
            try:
                data = json.load(f)
            except json.JSONDecodeError:
                continue
        if not isinstance(data, dict):
            continue
        cog_name = _dir.stem
        for cog_id, inner in data.items():
            if not isinstance(inner, dict):
                continue
            yield cog_name, cog_id
","if not isinstance ( inner , dict ) :",192
"def subFeaName(m, newNames, state):
    try:
        int(m[3], 16)
    except:
        return m[0]
    name = m[2]
    if name in newNames:
        # print('sub %r => %r' % (m[0], m[1] + newNames[name] + m[4]))
        if name == ""uni0402"":
            print(""sub %r => %r"" % (m[0], m[1] + newNames[name] + m[4]))
        state[""didChange""] = True
        return m[1] + newNames[name] + m[4]
    return m[0]
","if name == ""uni0402"" :",172
"def log_graph(self, model: LightningModule, input_array=None):
    if self._log_graph:
        if input_array is None:
            input_array = model.example_input_array
        if input_array is not None:
            input_array = model._apply_batch_transfer_handler(input_array)
            self.experiment.add_graph(model, input_array)
        else:
            rank_zero_warn(
                ""Could not log computational graph since the""
                "" `model.example_input_array` attribute is not set""
                "" or `input_array` was not given"",
                UserWarning,
            )
",if input_array is not None :,182
"def apply(self, db, person):
    for family_handle in person.get_family_handle_list():
        family = db.get_family_from_handle(family_handle)
        if family:
            for event_ref in family.get_event_ref_list():
                if event_ref:
                    event = db.get_event_from_handle(event_ref.ref)
                    if not event.get_place_handle():
                        return True
                    if not event.get_date_object():
                        return True
    return False
",if not event . get_place_handle ( ) :,159
"def format(m):
    if m > 1000:
        if m % 1000 == 0:
            return (str(int(m / 1000)), ""km"")
        else:
            return (str(round(m / 1000, 1)), ""km"")
    return (str(m), ""m"")
",if m % 1000 == 0 :,75
"def previous(self):
    try:
        idx = _jump_list_index
        next_index = idx + 1
        if next_index > 100:
            next_index = 100
        next_index = min(len(_jump_list) - 1, next_index)
        _jump_list_index = next_index
        return _jump_list[next_index]
    except (IndexError, KeyError) as e:
        return None
",if next_index > 100 :,114
"def _validate_and_set_default_hyperparameters(self):
    """"""Placeholder docstring""""""
    # Check if all the required hyperparameters are set. If there is a default value
    # for one, set it.
    for name, definition in self.hyperparameter_definitions.items():
        if name not in self.hyperparam_dict:
            spec = definition[""spec""]
            if ""DefaultValue"" in spec:
                self.hyperparam_dict[name] = spec[""DefaultValue""]
            elif ""IsRequired"" in spec and spec[""IsRequired""]:
                raise ValueError(""Required hyperparameter: %s is not set"" % name)
","if ""DefaultValue"" in spec :",158
"def _actions_read(self, c):
    self.action_input.handle_read(c)
    if c in [curses.KEY_ENTER, util.KEY_ENTER2]:
        # take action
        if self.action_input.selected_index == 0:  # Cancel
            self.back_to_parent()
        elif self.action_input.selected_index == 1:  # Apply
            self._apply_prefs()
            client.core.get_config().addCallback(self._update_preferences)
        elif self.action_input.selected_index == 2:  # OK
            self._apply_prefs()
            self.back_to_parent()
",elif self . action_input . selected_index == 1 :,174
"def _split_anonymous_function(s):
    # Regex is not sufficient to handle differences between anonymous
    # functions and YAML encoded lists. We perform a sniff test to see
    # if it might be an anonymous function and then confirm by
    # decoding it as YAML and testing the result.
    if s[:1] == ""["" and s[-1:] == ""]"" and "":"" in s:
        try:
            l = yaml_util.decode_yaml(s)
        except Exception:
            return None, s[1:-1]
        else:
            if len(l) == 1 and isinstance(l[0], (six.string_types, int)):
                return None, s[1:-1]
    return None
","if len ( l ) == 1 and isinstance ( l [ 0 ] , ( six . string_types , int ) ) :",177
"def test_source_address(self):
    for addr, is_ipv6 in VALID_SOURCE_ADDRESSES:
        if is_ipv6 and not HAS_IPV6_AND_DNS:
            warnings.warn(""No IPv6 support: skipping."", NoIPv6Warning)
            continue
        pool = HTTPConnectionPool(
            self.host, self.port, source_address=addr, retries=False
        )
        self.addCleanup(pool.close)
        r = pool.request(""GET"", ""/source_address"")
        self.assertEqual(r.data, b(addr[0]))
",if is_ipv6 and not HAS_IPV6_AND_DNS :,150
"def vim_G(self):
    """"""Put the cursor on the last character of the file.""""""
    if self.is_text_wrapper(self.w):
        if self.state == ""visual"":
            self.do(""end-of-buffer-extend-selection"")
        else:
            self.do(""end-of-buffer"")
        self.done()
    else:
        self.quit()
","if self . state == ""visual"" :",103
"def backend_supported(module, manager, **kwargs):
    if CollectionNodeModule.backend_supported(module, manager, **kwargs):
        if ""tid"" not in kwargs:
            return True
        conn = manager.connection(did=kwargs[""did""])
        template_path = ""partitions/sql/{0}/#{0}#{1}#"".format(
            manager.server_type, manager.version
        )
        SQL = render_template(
            ""/"".join([template_path, ""backend_support.sql""]), tid=kwargs[""tid""]
        )
        status, res = conn.execute_scalar(SQL)
        # check if any errors
        if not status:
            return internal_server_error(errormsg=res)
        return res
",if not status :,195
"def _get_regex_config(self, data_asset_name: Optional[str] = None) -> dict:
    regex_config: dict = copy.deepcopy(self._default_regex)
    asset: Optional[Asset] = None
    if data_asset_name:
        asset = self._get_asset(data_asset_name=data_asset_name)
    if asset is not None:
        # Override the defaults
        if asset.pattern:
            regex_config[""pattern""] = asset.pattern
        if asset.group_names:
            regex_config[""group_names""] = asset.group_names
    return regex_config
",if asset . pattern :,159
"def resolve(self, other):
    if other == ANY_TYPE:
        return self
    elif isinstance(other, ComplexType):
        f = self.first.resolve(other.first)
        s = self.second.resolve(other.second)
        if f and s:
            return ComplexType(f, s)
        else:
            return None
    elif self == ANY_TYPE:
        return other
    else:
        return None
",if f and s :,114
"def collect_pages(app):
    new_images = {}
    for full_path, basename in app.builder.images.iteritems():
        base, ext = os.path.splitext(full_path)
        retina_path = base + ""@2x"" + ext
        if retina_path in app.env.images:
            new_images[retina_path] = app.env.images[retina_path][1]
    app.builder.images.update(new_images)
    return []
",if retina_path in app . env . images :,129
"def has_bad_headers(self):
    headers = [self.sender, self.reply_to] + self.recipients
    for header in headers:
        if _has_newline(header):
            return True
    if self.subject:
        if _has_newline(self.subject):
            for linenum, line in enumerate(self.subject.split(""\r\n"")):
                if not line:
                    return True
                if linenum > 0 and line[0] not in ""\t "":
                    return True
                if _has_newline(line):
                    return True
                if len(line.strip()) == 0:
                    return True
    return False
",if not line :,186
"def reader():
    try:
        imgs = mp4_loader(video_path, seg_num, seglen, mode)
        if len(imgs) < 1:
            logger.error(
                ""{} frame length {} less than 1."".format(video_path, len(imgs))
            )
            yield None, None
    except:
        logger.error(""Error when loading {}"".format(mp4_path))
        yield None, None
    imgs_ret = imgs_transform(
        imgs, mode, seg_num, seglen, short_size, target_size, img_mean, img_std
    )
    label_ret = video_path
    yield imgs_ret, label_ret
",if len ( imgs ) < 1 :,176
"def translate_from_sortname(name, sortname):
    """"""'Translate' the artist name by reversing the sortname.""""""
    for c in name:
        ctg = unicodedata.category(c)
        if ctg[0] == ""L"" and unicodedata.name(c).find(""LATIN"") == -1:
            for separator in ("" & "", ""; "", "" and "", "" vs. "", "" with "", "" y ""):
                if separator in sortname:
                    parts = sortname.split(separator)
                    break
            else:
                parts = [sortname]
                separator = """"
            return separator.join(map(_reverse_sortname, parts))
    return name
","if ctg [ 0 ] == ""L"" and unicodedata . name ( c ) . find ( ""LATIN"" ) == - 1 :",181
"def _to_local_path(path):
    """"""Convert local path to SFTP path""""""
    if sys.platform == ""win32"":  # pragma: no cover
        path = os.fsdecode(path)
        if path[:1] == ""/"" and path[2:3] == "":"":
            path = path[1:]
        path = path.replace(""/"", ""\\"")
    return path
","if path [ : 1 ] == ""/"" and path [ 2 : 3 ] == "":"" :",95
"def __call__(self, text: str) -> str:
    for t in self.cleaner_types:
        if t == ""tacotron"":
            text = tacotron_cleaner.cleaners.custom_english_cleaners(text)
        elif t == ""jaconv"":
            text = jaconv.normalize(text)
        elif t == ""vietnamese"":
            if vietnamese_cleaners is None:
                raise RuntimeError(""Please install underthesea"")
            text = vietnamese_cleaners.vietnamese_cleaner(text)
        else:
            raise RuntimeError(f""Not supported: type={t}"")
    return text
","elif t == ""jaconv"" :",174
"def cb_syncthing_system_data(self, daemon, mem, cpu, d_failed, d_total):
    if self.daemon.get_my_id() in self.devices:
        # Update my device display
        device = self.devices[self.daemon.get_my_id()]
        device[""ram""] = sizeof_fmt(mem)
        device[""cpu""] = ""%3.2f%%"" % (cpu)
        if d_total == 0:
            device[""announce""] = _(""disabled"")
        else:
            device[""announce""] = ""%s/%s"" % (d_total - d_failed, d_total)
",if d_total == 0 :,162
"def update_kls(self, sampled_kls):
    for i, kl in enumerate(sampled_kls):
        if kl < self.kl_target / 1.5:
            self.kl_coeff_val[i] *= 0.5
        elif kl > 1.5 * self.kl_target:
            self.kl_coeff_val[i] *= 2.0
    return self.kl_coeff_val
",if kl < self . kl_target / 1.5 :,106
"def DeleteEmptyCols(self):
    cols2delete = []
    for c in range(0, self.GetCols()):
        f = True
        for r in range(0, self.GetRows()):
            if self.FindItemAtPosition((r, c)) is not None:
                f = False
        if f:
            cols2delete.append(c)
    for i in range(0, len(cols2delete)):
        self.ShiftColsLeft(cols2delete[i] + 1)
        cols2delete = [x - 1 for x in cols2delete]
",if f :,150
"def get_session(self):
    if self._session is None:
        session = super(ChildResourceManager, self).get_session()
        if self.resource_type.resource != constants.RESOURCE_ACTIVE_DIRECTORY:
            session = session.get_session_for_resource(self.resource_type.resource)
        self._session = session
    return self._session
",if self . resource_type . resource != constants . RESOURCE_ACTIVE_DIRECTORY :,92
"def _get_master_authorized_networks_config(self, raw_cluster):
    if raw_cluster.get(""masterAuthorizedNetworksConfig""):
        config = raw_cluster.get(""masterAuthorizedNetworksConfig"")
        config[""includes_public_cidr""] = False
        for block in config[""cidrBlocks""]:
            if block[""cidrBlock""] == ""0.0.0.0/0"":
                config[""includes_public_cidr""] = True
        return config
    else:
        return {""enabled"": False, ""cidrBlocks"": [], ""includes_public_cidr"": False}
","if block [ ""cidrBlock"" ] == ""0.0.0.0/0"" :",146
"def scan_folder(folder):
    scanned_files = []
    for root, dirs, files in os.walk(folder):
        dirs[:] = [d for d in dirs if d != ""__pycache__""]
        relative_path = os.path.relpath(root, folder)
        for f in files:
            if f.endswith("".pyc""):
                continue
            relative_name = os.path.normpath(os.path.join(relative_path, f)).replace(
                ""\\"", ""/""
            )
            scanned_files.append(relative_name)
    return sorted(scanned_files)
","if f . endswith ( "".pyc"" ) :",154
"def read_progress(self):
    while True:
        processed_file = self.queue.get()
        self.threading_completed.append(processed_file)
        total_number = len(self.file_list)
        completed_number = len(self.threading_completed)
        # Just for the record, this slows down book searching by about 20%
        if _progress_emitter:  # Skip update in reading mode
            _progress_emitter.update_progress(completed_number * 100 // total_number)
        if total_number == completed_number:
            break
",if total_number == completed_number :,145
"def next_instruction_is_function_or_class(lines):
    """"""Is the first non-empty, non-commented line of the cell either a function or a class?""""""
    parser = StringParser(""python"")
    for i, line in enumerate(lines):
        if parser.is_quoted():
            parser.read_line(line)
            continue
        parser.read_line(line)
        if not line.strip():  # empty line
            if i > 0 and not lines[i - 1].strip():
                return False
            continue
        if line.startswith(""def "") or line.startswith(""class ""):
            return True
        if line.startswith((""#"", ""@"", "" "", "")"")):
            continue
        return False
    return False
",if i > 0 and not lines [ i - 1 ] . strip ( ) :,194
"def __next__(self):
    try:
        data = next(self.iter_loader)
    except StopIteration:
        self._epoch += 1
        if hasattr(self._dataloader.sampler, ""set_epoch""):
            self._dataloader.sampler.set_epoch(self._epoch)
        self.iter_loader = iter(self._dataloader)
        data = next(self.iter_loader)
    return data
","if hasattr ( self . _dataloader . sampler , ""set_epoch"" ) :",104
"def dgl_mp_batchify_fn(data):
    if isinstance(data[0], tuple):
        data = zip(*data)
        return [dgl_mp_batchify_fn(i) for i in data]
    for dt in data:
        if dt is not None:
            if isinstance(dt, dgl.DGLGraph):
                return [d for d in data if isinstance(d, dgl.DGLGraph)]
            elif isinstance(dt, nd.NDArray):
                pad = Pad(axis=(1, 2), num_shards=1, ret_length=False)
                data_list = [dt for dt in data if dt is not None]
                return pad(data_list)
","if isinstance ( dt , dgl . DGLGraph ) :",183
"def f(self, info):
    for k in keys:
        if callable(k):
            for k2 in list(info.keys()):
                if k(k2):
                    info.pop(k2)
        else:
            info.pop(k, None)
",if callable ( k ) :,78
"def create(path, binary=False):
    for i in range(10):
        try:
            os.makedirs(os.path.dirname(path), exist_ok=True)
            if binary:
                return open(path, ""wb"")
            else:
                return open(path, ""w"", encoding=""utf-8"")
            if i > 0:
                log(True, f""Created {path} at attempt {i + 1}"")
        except:
            time.sleep(0.5)
    else:
        raise Error(f""Failed to create {path}"")
",if binary :,157
"def validate_update(self, update_query):
    structure = DotCollapsedDict(self.doc_class.structure)
    for op, fields in update_query.iteritems():
        for field in fields:
            if op != ""$unset"" and op != ""$rename"":
                if field not in structure:
                    raise UpdateQueryError(
                        ""'%s' not found in %s's structure""
                        % (field, self.doc_class.__name__)
                    )
",if field not in structure :,133
"def check_enums_ATLAS_ISAEXT(lines):
    for i, isaext in enumerate(ATLAS_ISAEXT):
        got = lines.pop(0).strip()
        if i == 0:
            expect = ""none: 1""
        else:
            expect = ""{0}: {1}"".format(isaext, 1 << i)
        if got != expect:
            raise RuntimeError(
                ""ATLAS_ISAEXT mismatch at position ""
                + str(i)
                + "": got >>""
                + got
                + ""<<, expected >>""
                + expect
                + ""<<""
            )
",if i == 0 :,180
"def _test_export_session_csv(self, test_session=None):
    with self.app.test_request_context():
        if not test_session:
            test_session = SessionFactory()
        field_data = export_sessions_csv([test_session])
        session_row = field_data[1]
        self.assertEqual(session_row[0], ""example (accepted)"")
        self.assertEqual(session_row[9], ""accepted"")
",if not test_session :,116
"def get_report_to_platform(self, args, scan_reports):
    if self.bc_api_key:
        if args.directory:
            repo_id = self.get_repository(args)
            self.setup_bridgecrew_credentials(
                bc_api_key=self.bc_api_key, repo_id=repo_id
            )
        if self.is_integration_configured():
            self._upload_run(args, scan_reports)
",if args . directory :,126
"def test_fvalue(self):
    if not getattr(self, ""skip_f"", False):
        rtol = getattr(self, ""rtol"", 1e-10)
        assert_allclose(self.res1.fvalue, self.res2.F, rtol=rtol)
        if hasattr(self.res2, ""Fp""):
            # only available with ivreg2
            assert_allclose(self.res1.f_pvalue, self.res2.Fp, rtol=rtol)
    else:
        raise pytest.skip(""TODO: document why this test is skipped"")
","if hasattr ( self . res2 , ""Fp"" ) :",143
"def fix_repeating_arguments(self):
    """"""Fix elements that should accumulate/increment values.""""""
    either = [list(child.children) for child in transform(self).children]
    for case in either:
        for e in [child for child in case if case.count(child) > 1]:
            if type(e) is Argument or type(e) is Option and e.argcount:
                if e.value is None:
                    e.value = []
                elif type(e.value) is not list:
                    e.value = e.value.split()
            if type(e) is Command or type(e) is Option and e.argcount == 0:
                e.value = 0
    return self
",elif type ( e . value ) is not list :,190
"def touch(self):
    if not self.exists():
        try:
            self.parent().touch()
        except ValueError:
            pass
        node = self._fs.touch(self.pathnames, {})
        if not node.isdir:
            raise AssertionError(""Not a folder: %s"" % self.path)
        if self.watcher:
            self.watcher.emit(""created"", self)
",if self . watcher :,107
"def __init__(self, _inf=None, _tzinfos=None):
    if _inf:
        self._tzinfos = _tzinfos
        self._utcoffset, self._dst, self._tzname = _inf
    else:
        _tzinfos = {}
        self._tzinfos = _tzinfos
        self._utcoffset, self._dst, self._tzname = self._transition_info[0]
        _tzinfos[self._transition_info[0]] = self
        for inf in self._transition_info[1:]:
            if not _tzinfos.has_key(inf):
                _tzinfos[inf] = self.__class__(inf, _tzinfos)
",if not _tzinfos . has_key ( inf ) :,173
"def test_sample_output():
    comment = ""SAMPLE OUTPUT""
    skip_files = [""__init__.py""]
    errors = []
    for _file in sorted(MODULE_PATH.iterdir()):
        if _file.suffix == "".py"" and _file.name not in skip_files:
            with _file.open() as f:
                if comment not in f.read():
                    errors.append((comment, _file))
    if errors:
        line = ""Missing sample error(s) detected!\n\n""
        for error in errors:
            line += ""`{}` is not in module `{}`\n"".format(*error)
        print(line[:-1])
        assert False
","if _file . suffix == "".py"" and _file . name not in skip_files :",174
"def http_get(url, target):
    req = requests.get(url, stream=True)
    content_length = req.headers.get(""Content-Length"")
    total = int(content_length) if content_length is not None else None
    progress = tqdm(unit=""B"", total=total)
    with open(target, ""wb"") as target_file:
        for chunk in req.iter_content(chunk_size=1024):
            if chunk:  # filter out keep-alive new chunks
                progress.update(len(chunk))
                target_file.write(chunk)
    progress.close()
",if chunk :,154
"def _elements_to_datasets(self, elements, level=0):
    for element in elements:
        extra_kwds = {""identifier_%d"" % level: element[""name""]}
        if ""elements"" in element:
            for inner_element in self._elements_to_datasets(
                element[""elements""], level=level + 1
            ):
                dataset = extra_kwds.copy()
                dataset.update(inner_element)
                yield dataset
        else:
            dataset = extra_kwds
            extra_kwds.update(element)
            yield extra_kwds
","if ""elements"" in element :",156
"def update_dict(a, b):
    for key, value in b.items():
        if value is None:
            continue
        if key not in a:
            a[key] = value
        elif isinstance(a[key], dict) and isinstance(value, dict):
            update_dict(a[key], value)
        elif isinstance(a[key], list):
            a[key].append(value)
        else:
            a[key] = [a[key], value]
",if key not in a :,131
"def scan(self, targets):
    for target in targets:
        target.print_infos()
        if self.is_interesting(target):
            self.target[""other""].append(target)
            if self.match(target):
                return target
    return None
",if self . match ( target ) :,72
"def printConnections(switches):
    ""Compactly print connected nodes to each switch""
    for sw in switches:
        output(""%s: "" % sw)
        for intf in sw.intfList():
            link = intf.link
            if link:
                intf1, intf2 = link.intf1, link.intf2
                remote = intf1 if intf1.node != sw else intf2
                output(""%s(%s) "" % (remote.node, sw.ports[intf]))
        output(""\n"")
",if link :,147
"def __cut(sentence):
    global emit_P
    prob, pos_list = viterbi(sentence, ""BMES"", start_P, trans_P, emit_P)
    begin, nexti = 0, 0
    # print pos_list, sentence
    for i, char in enumerate(sentence):
        pos = pos_list[i]
        if pos == ""B"":
            begin = i
        elif pos == ""E"":
            yield sentence[begin : i + 1]
            nexti = i + 1
        elif pos == ""S"":
            yield char
            nexti = i + 1
    if nexti < len(sentence):
        yield sentence[nexti:]
","elif pos == ""E"" :",174
"def check_files(self, paths=None):
    """"""Run all checks on the paths.""""""
    if paths is None:
        paths = self.paths
    report = self.options.report
    runner = self.runner
    report.start()
    try:
        for path in paths:
            if os.path.isdir(path):
                self.input_dir(path)
            elif not self.excluded(path):
                runner(path)
    except KeyboardInterrupt:
        print(""... stopped"")
    report.stop()
    return report
",if os . path . isdir ( path ) :,140
"def verts_of_loop(edge_loop):
    verts = []
    for e0, e1 in iter_pairs(edge_loop, False):
        if not verts:
            v0 = e0.shared_vert(e1)
            verts += [e0.other_vert(v0), v0]
        verts += [e1.other_vert(verts[-1])]
    if len(verts) > 1 and verts[0] == verts[-1]:
        return verts[:-1]
    return verts
",if not verts :,134
"def generator(self, data):
    for task in data:
        # Do we scan everything or just /bin/bash instances?
        if not (self._config.SCAN_ALL or str(task.p_comm) == ""bash""):
            continue
        for bucket in task.bash_hash_entries():
            yield (
                0,
                [
                    int(task.p_pid),
                    str(task.p_comm),
                    int(bucket.times_found),
                    str(bucket.key),
                    str(bucket.data.path),
                ],
            )
","if not ( self . _config . SCAN_ALL or str ( task . p_comm ) == ""bash"" ) :",174
"def __get_ratio(self):
    """"""Return splitter ratio of the main splitter.""""""
    c = self.c
    free_layout = c.free_layout
    if free_layout:
        w = free_layout.get_main_splitter()
        if w:
            aList = w.sizes()
            if len(aList) == 2:
                n1, n2 = aList
                # 2017/06/07: guard against division by zero.
                ratio = 0.5 if n1 + n2 == 0 else float(n1) / float(n1 + n2)
                return ratio
    return 0.5
",if len ( aList ) == 2 :,170
"def geterrors(self):
    """"""Get all error messages.""""""
    notes = self.getnotes(origin=""translator"").split(""\n"")
    errordict = {}
    for note in notes:
        if ""(pofilter) "" in note:
            error = note.replace(""(pofilter) "", """")
            errorname, errortext = error.split("": "", 1)
            errordict[errorname] = errortext
    return errordict
","if ""(pofilter) "" in note :",107
"def rename_path(self, path, new_path):
    logger.debug(""rename_path '%s' -> '%s'"" % (path, new_path))
    dirs = self.readdir(path)
    for d in dirs:
        if d in [""."", ""..""]:
            continue
        d_path = """".join([path, ""/"", d])
        d_new_path = """".join([new_path, ""/"", d])
        attr = self.getattr(d_path)
        if stat.S_ISDIR(attr[""st_mode""]):
            self.rename_path(d_path, d_new_path)
        else:
            self.rename_item(d_path, d_new_path)
    self.rename_item(path, new_path, dir=True)
","if stat . S_ISDIR ( attr [ ""st_mode"" ] ) :",196
"def index(self, url_id: int) -> FlaskResponse:  # pylint: disable=no-self-use
    url = db.session.query(models.Url).get(url_id)
    if url and url.url:
        explore_url = ""//superset/explore/?""
        if url.url.startswith(explore_url):
            explore_url += f""r={url_id}""
            return redirect(explore_url[1:])
        return redirect(url.url[1:])
    flash(""URL to nowhere..."", ""danger"")
    return redirect(""/"")
",if url . url . startswith ( explore_url ) :,142
"def testShortCircuit(self):
    """"""Test that creation short-circuits to reuse existing references""""""
    sd = {}
    for s in self.ss:
        sd[s] = 1
    for t in self.ts:
        if hasattr(t, ""x""):
            self.assertTrue(sd.has_key(safeRef(t.x)))
            self.assertTrue(safeRef(t.x) in sd)
        else:
            self.assertTrue(sd.has_key(safeRef(t)))
            self.assertTrue(safeRef(t) in sd)
","if hasattr ( t , ""x"" ) :",146
"def wrapped(request, *args, **kwargs):
    if not request.user.is_authenticated():
        request.session[""_next""] = request.get_full_path()
        if ""organization_slug"" in kwargs:
            redirect_uri = reverse(
                ""sentry-auth-organization"", args=[kwargs[""organization_slug""]]
            )
        else:
            redirect_uri = get_login_url()
        return HttpResponseRedirect(redirect_uri)
    return func(request, *args, **kwargs)
","if ""organization_slug"" in kwargs :",132
"def read_info(reader, dump=None):
    line_number_table_length = reader.read_u2()
    if dump is not None:
        reader.debug(
            ""    "" * dump, ""Line numbers (%s total):"" % line_number_table_length
        )
    line_numbers = []
    for i in range(0, line_number_table_length):
        start_pc = reader.read_u2()
        line_number = reader.read_u2()
        if dump is not None:
            reader.debug(""    "" * (dump + 1), ""%s: %s"" % (start_pc, line_number))
        line_numbers.append((start_pc, line_number))
    return LineNumberTable(line_numbers)
",if dump is not None :,198
"def compute_timer_precision(timer):
    precision = None
    points = 0
    timeout = timeout_timer() + 1.0
    previous = timer()
    while timeout_timer() < timeout or points < 5:
        for _ in XRANGE(10):
            t1 = timer()
            t2 = timer()
            dt = t2 - t1
            if 0 < dt:
                break
        else:
            dt = t2 - previous
            if dt <= 0.0:
                continue
        if precision is not None:
            precision = min(precision, dt)
        else:
            precision = dt
        points += 1
        previous = timer()
    return precision
",if precision is not None :,189
"def get_hi_lineno(self):
    lineno = Node.get_hi_lineno(self)
    if self.expr1 is None:
        pass
    else:
        lineno = self.expr1.get_hi_lineno()
        if self.expr2 is None:
            pass
        else:
            lineno = self.expr2.get_hi_lineno()
            if self.expr3 is None:
                pass
            else:
                lineno = self.expr3.get_hi_lineno()
    return lineno
",if self . expr2 is None :,142
"def validate_cluster_resource_group(cmd, namespace):
    if namespace.cluster_resource_group is not None:
        client = get_mgmt_service_client(
            cmd.cli_ctx, ResourceType.MGMT_RESOURCE_RESOURCES
        )
        if client.resource_groups.check_existence(namespace.cluster_resource_group):
            raise InvalidArgumentValueError(
                ""Invalid --cluster-resource-group '%s': resource group must not exist.""
                % namespace.cluster_resource_group
            )
",if client . resource_groups . check_existence ( namespace . cluster_resource_group ) :,137
"def find_word_bounds(self, text, index, allowed_chars):
    right = left = index
    done = False
    while not done:
        if left == 0:
            done = True
        elif not self.word_boundary_char(text[left - 1]):
            left -= 1
        else:
            done = True
    done = False
    while not done:
        if right == len(text):
            done = True
        elif not self.word_boundary_char(text[right]):
            right += 1
        else:
            done = True
    return left, right
",if left == 0 :,159
"def _check_good_input(self, X, y=None):
    if isinstance(X, dict):
        lengths = [len(X1) for X1 in X.values()]
        if len(set(lengths)) > 1:
            raise ValueError(""Not all values of X are of equal length."")
        x_len = lengths[0]
    else:
        x_len = len(X)
    if y is not None:
        if len(y) != x_len:
            raise ValueError(""X and y are not of equal length."")
    if self.regression and y is not None and y.ndim == 1:
        y = y.reshape(-1, 1)
    return X, y
",if len ( y ) != x_len :,175
"def _get_text_nodes(nodes, html_body):
    text = []
    open_tags = 0
    for node in nodes:
        if isinstance(node, HtmlTag):
            if node.tag_type == OPEN_TAG:
                open_tags += 1
            elif node.tag_type == CLOSE_TAG:
                open_tags -= 1
        elif (
            isinstance(node, HtmlDataFragment)
            and node.is_text_content
            and open_tags == 0
        ):
            text.append(html_body[node.start : node.end])
    return text
",elif node . tag_type == CLOSE_TAG :,165
"def _get_spyne_type(cls_name, k, v):
    try:
        v = NATIVE_MAP.get(v, v)
    except TypeError:
        return
    try:
        subc = issubclass(v, ModelBase) or issubclass(v, SelfReference)
    except:
        subc = False
    if subc:
        if issubclass(v, Array) and len(v._type_info) != 1:
            raise Exception(""Invalid Array definition in %s.%s."" % (cls_name, k))
        elif issubclass(v, Point) and v.Attributes.dim is None:
            raise Exception(""Please specify the number of dimensions"")
        return v
","elif issubclass ( v , Point ) and v . Attributes . dim is None :",171
"def customize(cls, **kwargs):
    """"""return a class with some existing attributes customized""""""
    for name, value in kwargs.iteritems():
        if name in [""cookie"", ""circuit"", ""upstream"", ""downstream"", ""stream""]:
            raise TransportError(
                ""you cannot customize the protected attribute %s"" % name
            )
        if not hasattr(cls, name):
            raise TransportError(""Transport has no attribute %s"" % name)
    NewSubClass = type(""Customized_{}"".format(cls.__name__), (cls,), kwargs)
    return NewSubClass
","if name in [ ""cookie"" , ""circuit"" , ""upstream"" , ""downstream"" , ""stream"" ] :",144
"def test_UNrelativize(self):
    import URIlib
    relative = self.relative + self.full_relativize
    for base, rel, fullpath, common in relative:
        URI = uriparse.UnRelativizeURL(base, rel)
        fullURI = URIlib.URIParser(URI)
        # We need to canonicalize the result from unrelativize
        # compared to the original full path we expect to see.
        if fullpath[-1] in (""/"", ""\\""):
            fullpath = fullpath[:-1]
        self.failUnlessSamePath(
            os.path.normcase(fullURI.path), os.path.normcase(fullpath)
        )
","if fullpath [ - 1 ] in ( ""/"" , ""\\"" ) :",170
"def get_release_info(file_path=RELEASE_FILE):
    RELEASE_TYPE_REGEX = re.compile(r""^[Rr]elease [Tt]ype: (major|minor|patch)$"")
    with open(file_path, ""r"") as f:
        line = f.readline()
        match = RELEASE_TYPE_REGEX.match(line)
        if not match:
            print(
                ""The file RELEASE.md should start with `Release type` ""
                ""and specify one of the following values: major, minor or patch.""
            )
            sys.exit(1)
        type_ = match.group(1)
        changelog = """".join([line for line in f.readlines()]).strip()
    return type_, changelog
",if not match :,190
"def _get_next_history_entry(self):
    if self._history:
        hist_len = len(self._history) - 1
        self.history_index = min(hist_len, self.history_index + 1)
        index = self.history_index
        if self.history_index == hist_len:
            self.history_index += 1
        return self._history[index]
    return """"
",if self . history_index == hist_len :,107
"def star_op(self):
    """"""Put a '*' op, with special cases for *args.""""""
    val = ""*""
    if self.paren_level:
        i = len(self.code_list) - 1
        if self.code_list[i].kind == ""blank"":
            i -= 1
        token = self.code_list[i]
        if token.kind == ""lt"":
            self.op_no_blanks(val)
        elif token.value == "","":
            self.blank()
            self.add_token(""op-no-blanks"", val)
        else:
            self.op(val)
    else:
        self.op(val)
","if token . kind == ""lt"" :",177
"def get_safe_settings():
    ""Returns a dictionary of the settings module, with sensitive settings blurred out.""
    settings_dict = {}
    for k in dir(settings):
        if k.isupper():
            if HIDDEN_SETTINGS.search(k):
                settings_dict[k] = ""********************""
            else:
                settings_dict[k] = getattr(settings, k)
    return settings_dict
",if k . isupper ( ) :,109
"def nextEditable(self):
    """"""Moves focus of the cursor to the next editable window""""""
    if self.currentEditable is None:
        if len(self._editableChildren):
            self._currentEditableRef = self._editableChildren[0]
    else:
        for ref in weakref.getweakrefs(self.currentEditable):
            if ref in self._editableChildren:
                cei = self._editableChildren.index(ref)
                nei = cei + 1
                if nei >= len(self._editableChildren):
                    nei = 0
                self._currentEditableRef = self._editableChildren[nei]
    return self.currentEditable
",if ref in self . _editableChildren :,179
"def _handle_dependents_type(types, type_str, type_name, rel_name, row):
    if types[type_str[0]] is None:
        if type_str[0] == ""i"":
            type_name = ""index""
            rel_name = row[""indname""] + "" ON "" + rel_name
        elif type_str[0] == ""o"":
            type_name = ""operator""
            rel_name = row[""relname""]
    else:
        type_name = types[type_str[0]]
    return type_name, rel_name
","if type_str [ 0 ] == ""i"" :",152
"def streamErrorHandler(self, conn, error):
    name, text = ""error"", error.getData()
    for tag in error.getChildren():
        if tag.getNamespace() == NS_XMPP_STREAMS:
            if tag.getName() == ""text"":
                text = tag.getData()
            else:
                name = tag.getName()
    if name in stream_exceptions.keys():
        exc = stream_exceptions[name]
    else:
        exc = StreamError
    raise exc((name, text))
",if tag . getNamespace ( ) == NS_XMPP_STREAMS :,138
"def _validate_names(self, settings: _SettingsType) -> None:
    """"""Make sure all settings exist.""""""
    unknown = []
    for name in settings:
        if name not in configdata.DATA:
            unknown.append(name)
    if unknown:
        errors = [
            configexc.ConfigErrorDesc(
                ""While loading options"", ""Unknown option {}"".format(e)
            )
            for e in sorted(unknown)
        ]
        raise configexc.ConfigFileErrors(""autoconfig.yml"", errors)
",if name not in configdata . DATA :,139
"def can_haz(self, target, credentials):
    """"""Check whether key-values in target are present in credentials.""""""
    # TODO(termie): handle ANDs, probably by providing a tuple instead of a
    #               string
    for requirement in target:
        key, match = requirement.split("":"", 1)
        check = credentials.get(key)
        if check is None or isinstance(check, basestring):
            check = [check]
        if match in check:
            return True
","if check is None or isinstance ( check , basestring ) :",135
"def _recursive_fx_apply(input: dict, fx):
    for k, v in input.items():
        if isinstance(v, list):
            v = torch.tensor(v)
        if isinstance(v, torch.Tensor):
            v = fx(v.float())
            input[k] = v
        else:
            _recursive_fx_apply(v, fx)
","if isinstance ( v , list ) :",102
"def get(self, url, **kwargs):
    app, url = self._prepare_call(url, kwargs)
    if app:
        if url.endswith(""ping"") and self._first_ping:
            self._first_ping = False
            return EmptyCapabilitiesResponse()
        elif ""Hello0"" in url and ""1.2.1"" in url and ""v1"" in url:
            return ErrorApiResponse()
        else:
            response = app.get(url, **kwargs)
            return TestingResponse(response)
    else:
        return requests.get(url, **kwargs)
","if url . endswith ( ""ping"" ) and self . _first_ping :",153
"def server_thread_fn():
    server_ctx = ssl.create_default_context(ssl.Purpose.CLIENT_AUTH)
    server_ctx.load_cert_chain(""trio-test-1.pem"")
    server = server_ctx.wrap_socket(
        server_sock,
        server_side=True,
        suppress_ragged_eofs=False,
    )
    while True:
        data = server.recv(4096)
        print(""server got:"", data)
        if not data:
            print(""server waiting for client to finish everything"")
            client_done.wait()
            print(""server attempting to send back close-notify"")
            server.unwrap()
            print(""server ok"")
            break
        server.sendall(data)
",if not data :,198
"def find_hostnames(data):
    # sends back an array of hostnames
    hostnames = []
    for i in re.finditer(hostname_regex, data):
        h = string.lower(i.group(1))
        tld = h.split(""."")[-1:][0]
        if tld in tlds:
            hostnames.append(h)
    return hostnames
",if tld in tlds :,91
"def Validate(self, win):
    textCtrl = self.GetWindow()
    text = textCtrl.GetValue().strip()
    sChar = Character.getInstance()
    try:
        if len(text) == 0:
            raise ValueError(_t(""You must supply a name for the Character!""))
        elif text in [x.name for x in sChar.getCharacterList()]:
            raise ValueError(
                _t(""Character name already in use, please choose another."")
            )
        return True
    except ValueError as e:
        pyfalog.error(e)
        wx.MessageBox(""{}"".format(e), _t(""Error""))
        textCtrl.SetFocus()
        return False
",if len ( text ) == 0 :,177
"def get_random_user_agent(agent_list=UA_CACHE):
    if not len(agent_list):
        ua_file = file(UA_FILE)
        for line in ua_file:
            line = line.strip()
            if line:
                agent_list.append(line)
    ua = random.choice(UA_CACHE)
    return ua
",if line :,100
"def _validate_action_like_for_prefixes(self, key):
    for statement in self._statements:
        if key in statement:
            if isinstance(statement[key], string_types):
                self._validate_action_prefix(statement[key])
            else:
                for action in statement[key]:
                    self._validate_action_prefix(action)
",if key in statement :,100
"def predict(self, X):
    if self.regression:
        return self.predict_proba(X)
    else:
        y_pred = np.argmax(self.predict_proba(X), axis=1)
        if self.use_label_encoder:
            y_pred = self.enc_.inverse_transform(y_pred)
        return y_pred
",if self . use_label_encoder :,93
"def _threaded_request_tracker(self, builder):
    while True:
        event_type = self._read_q.get()
        if event_type is False:
            return
        payload = {""body"": b""""}
        request_id = builder.build_record(event_type, payload, """")
        self._write_q.put_nowait(request_id)
",if event_type is False :,96
"def __call__(self, value):
    try:
        super(EmailValidator, self).__call__(value)
    except ValidationError as e:
        # Trivial case failed. Try for possible IDN domain-part
        if value and ""@"" in value:
            parts = value.split(""@"")
            try:
                parts[-1] = parts[-1].encode(""idna"").decode(""ascii"")
            except UnicodeError:
                raise e
            super(EmailValidator, self).__call__(""@"".join(parts))
        else:
            raise
","if value and ""@"" in value :",143
"def PreprocessConditionalStatement(self, IfList, ReplacedLine):
    while self:
        if self.__Token:
            x = 1
        elif not IfList:
            if self <= 2:
                continue
            RegionSizeGuid = 3
            if not RegionSizeGuid:
                RegionLayoutLine = 5
                continue
            RegionLayoutLine = self.CurrentLineNumber
    return 1
",elif not IfList :,111
"def _arg_with_type(self):
    for t in self.d[""Args""]:
        m = re.search(""([A-Za-z0-9_-]+)\s{0,4}(\(.+\))\s{0,4}:"", t)
        if m:
            self.args[m.group(1)] = m.group(2)
    return self.args
",if m :,95
"def get_palette_for_custom_classes(self, class_names, palette=None):
    if self.label_map is not None:
        # return subset of palette
        palette = []
        for old_id, new_id in sorted(self.label_map.items(), key=lambda x: x[1]):
            if new_id != -1:
                palette.append(self.PALETTE[old_id])
        palette = type(self.PALETTE)(palette)
    elif palette is None:
        if self.PALETTE is None:
            palette = np.random.randint(0, 255, size=(len(class_names), 3))
        else:
            palette = self.PALETTE
    return palette
",if self . PALETTE is None :,194
"def Visit_star_expr(self, node):  # pylint: disable=invalid-name
    # star_expr ::= '*' expr
    for child in node.children:
        self.Visit(child)
        if isinstance(child, pytree.Leaf) and child.value == ""*"":
            _AppendTokenSubtype(child, format_token.Subtype.UNARY_OPERATOR)
            _AppendTokenSubtype(child, format_token.Subtype.VARARGS_STAR)
","if isinstance ( child , pytree . Leaf ) and child . value == ""*"" :",110
"def create_if_compatible(cls, typ: Type, *, root: ""RootNode"") -> Optional[""Node""]:
    if cls.compatible_types:
        target_type: Type = typ
        if cls.use_origin:
            target_type = getattr(typ, ""__origin__"", None) or typ
        if cls._issubclass(target_type, cls.compatible_types):
            return cls(typ, root=root)
    return None
",if cls . use_origin :,109
"def grep_full_py_identifiers(tokens):
    global pykeywords
    tokens = list(tokens)
    i = 0
    while i < len(tokens):
        tokentype, token = tokens[i]
        i += 1
        if tokentype != ""id"":
            continue
        while (
            i + 1 < len(tokens)
            and tokens[i] == (""op"", ""."")
            and tokens[i + 1][0] == ""id""
        ):
            token += ""."" + tokens[i + 1][1]
            i += 2
        if token == """":
            continue
        if token in pykeywords:
            continue
        if token[0] in "".0123456789"":
            continue
        yield token
","if token == """" :",194
"def create_config_filepath(cls, visibility=None):
    if cls.is_local(visibility):
        # Local to this directory
        base_path = os.path.join(""."")
        if cls.IS_POLYAXON_DIR:
            # Add it to the current ""./.polyaxon""
            base_path = os.path.join(base_path, "".polyaxon"")
            cls._create_dir(base_path)
    elif cls.CONFIG_PATH:  # Custom path
        pass
    else:  # Handle both global and all cases
        base_path = polyaxon_user_path()
        cls._create_dir(base_path)
",if cls . IS_POLYAXON_DIR :,170
"def test_len(self):
    eq = self.assertEqual
    eq(base64MIME.base64_len(""hello""), len(base64MIME.encode(""hello"", eol="""")))
    for size in range(15):
        if size == 0:
            bsize = 0
        elif size <= 3:
            bsize = 4
        elif size <= 6:
            bsize = 8
        elif size <= 9:
            bsize = 12
        elif size <= 12:
            bsize = 16
        else:
            bsize = 20
        eq(base64MIME.base64_len(""x"" * size), bsize)
",elif size <= 3 :,160
"def as_dict(path="""", version=""latest"", section=""meta-data""):
    result = {}
    dirs = dir(path, version, section)
    if not dirs:
        return None
    for item in dirs:
        if item.endswith(""/""):
            records = as_dict(path + item, version, section)
            if records:
                result[item[:-1]] = records
        elif is_dict.match(item):
            idx, name = is_dict.match(item).groups()
            records = as_dict(path + idx + ""/"", version, section)
            if records:
                result[name] = records
        else:
            result[item] = valueconv(get(path + item, version, section))
    return result
","if item . endswith ( ""/"" ) :",197
"def api_read(self):
    result = {}
    files = [""my.cnf"", ""debian.cnf""]
    directory_list = self.exec_payload(""mysql_config_directory"")[""directory""]
    for _file in files:
        for directory in directory_list:
            mysql_conf = directory + _file
            content = self.shell.read(mysql_conf)
            if content:
                result[mysql_conf] = content
    return result
",if content :,118
"def generate(self, count=100):
    self.pre_generate()
    counter = iter(range(count))
    created = 0
    while True:
        batch = list(islice(counter, self.batch_size))
        if not batch:
            break
        self.do_generate(batch, self.batch_size)
        from_size = created
        created += len(batch)
        print(""Generate %s: %s-%s"" % (self.resource, from_size, created))
    self.after_generate()
",if not batch :,135
"def _normalize_fields(self, document, loader):
    # type: (Dict[Text, Text], Loader) -> None
    # Normalize fields which are prefixed or full URIn to vocabulary terms
    for d in list(document.keys()):
        d2 = loader.expand_url(d, u"""", scoped_id=False, vocab_term=True)
        if d != d2:
            document[d2] = document[d]
            del document[d]
",if d != d2 :,115
"def load_cache(filename, get_key=mangle_key):
    cache = {}
    if not os.path.exists(filename):
        return cache
    f = open(filename, ""rb"")
    l = 0
    for line in f.readlines():
        l += 1
        fields = line.split(b"" "")
        if fields == None or not len(fields) == 2 or fields[0][0:1] != b"":"":
            sys.stderr.write(""Invalid file format in [%s], line %d\n"" % (filename, l))
            continue
        # put key:value in cache, key without ^:
        cache[get_key(fields[0][1:])] = fields[1].split(b""\n"")[0]
    f.close()
    return cache
","if fields == None or not len ( fields ) == 2 or fields [ 0 ] [ 0 : 1 ] != b"":"" :",197
"def __lshift__(self, other):
    if not self.symbolic and type(other) is int:
        return RegisterOffset(
            self._bits, self.reg, self._to_signed(self.offset << other)
        )
    else:
        if self.symbolic:
            return RegisterOffset(self._bits, self.reg, self.offset << other)
        else:
            return RegisterOffset(
                self._bits,
                self.reg,
                ArithmeticExpression(
                    ArithmeticExpression.LShift,
                    (
                        self.offset,
                        other,
                    ),
                ),
            )
",if self . symbolic :,192
"def SaveSettings(self, force=False):
    if self.config is not None:
        frame.ShellFrameMixin.SaveSettings(self)
        if self.autoSaveSettings or force:
            frame.Frame.SaveSettings(self, self.config)
            self.shell.SaveSettings(self.config)
",if self . autoSaveSettings or force :,79
"def _parse_gene(element):
    for genename_element in element:
        if ""type"" in genename_element.attrib:
            ann_key = ""gene_%s_%s"" % (
                genename_element.tag.replace(NS, """"),
                genename_element.attrib[""type""],
            )
            if genename_element.attrib[""type""] == ""primary"":
                self.ParsedSeqRecord.annotations[ann_key] = genename_element.text
            else:
                append_to_annotations(ann_key, genename_element.text)
","if genename_element . attrib [ ""type"" ] == ""primary"" :",157
"def _write_pkg_file(self, file):
    with TemporaryFile(mode=""w+"") as tmpfd:
        _write_pkg_file_orig(self, tmpfd)
        tmpfd.seek(0)
        for line in tmpfd:
            if line.startswith(""Metadata-Version: ""):
                file.write(""Metadata-Version: 2.1\n"")
            elif line.startswith(""Description: ""):
                file.write(
                    ""Description-Content-Type: %s; charset=UTF-8\n""
                    % long_description_content_type
                )
                file.write(line)
            else:
                file.write(line)
","if line . startswith ( ""Metadata-Version: "" ) :",188
"def get(self):
    """"""If a value/an exception is stored, return/raise it. Otherwise until switch() or throw() is called.""""""
    if self._exception is not _NONE:
        if self._exception is None:
            return self.value
        getcurrent().throw(*self._exception)  # pylint:disable=undefined-variable
    else:
        if self.greenlet is not None:
            raise ConcurrentObjectUseError(
                ""This Waiter is already used by %r"" % (self.greenlet,)
            )
        self.greenlet = getcurrent()  # pylint:disable=undefined-variable
        try:
            return self.hub.switch()
        finally:
            self.greenlet = None
",if self . _exception is None :,188
"def connect(self, *args):
    """"""connects to the dropbox. args[0] is the username.""""""
    if len(args) != 1:
        return ""expected one argument!""
    try:
        dbci = get_dropbox_client(args[0], False, None, None)
    except Exception as e:
        return e.message
    else:
        if dbci is None:
            return ""No Dropbox configured for '{u}'."".format(u=args[0])
        else:
            self.client = dbci
        return True
",if dbci is None :,142
"def escape(text, newline=False):
    """"""Escape special html characters.""""""
    if isinstance(text, str):
        if ""&"" in text:
            text = text.replace(""&"", ""&amp;"")
        if "">"" in text:
            text = text.replace("">"", ""&gt;"")
        if ""<"" in text:
            text = text.replace(""<"", ""&lt;"")
        if '""' in text:
            text = text.replace('""', ""&quot;"")
        if ""'"" in text:
            text = text.replace(""'"", ""&quot;"")
        if newline:
            if ""\n"" in text:
                text = text.replace(""\n"", ""<br>"")
    return text
","if ""\n"" in text :",170
"def t(ret):
    with IPDB() as ipdb:
        with ipdb.eventqueue() as evq:
            for msg in evq:
                if msg.get_attr(""IFLA_IFNAME"") == ""test1984"":
                    ret.append(msg)
                    return
","if msg . get_attr ( ""IFLA_IFNAME"" ) == ""test1984"" :",83
"def check_stmt(self, stmt):
    if is_future(stmt):
        for name, asname in stmt.names:
            if name in self.features:
                self.found[name] = 1
            else:
                raise SyntaxError(""future feature %s is not defined"" % name)
        stmt.valid_future = 1
        return 1
    return 0
",if name in self . features :,99
"def process_pypi_option(option, option_str, option_value, parser):
    if option_str.startswith(""--no""):
        setattr(parser.values, option.dest, [])
    else:
        indexes = getattr(parser.values, option.dest, [])
        if _PYPI not in indexes:
            indexes.append(_PYPI)
        setattr(parser.values, option.dest, indexes)
",if _PYPI not in indexes :,102
"def modify_address(self, name, address, domain):
    if not self.get_entries_by_name(name, domain):
        raise exception.NotFound
    infile = open(self.filename, ""r"")
    outfile = tempfile.NamedTemporaryFile(""w"", delete=False)
    for line in infile:
        entry = self.parse_line(line)
        if entry and entry[""name""].lower() == self.qualify(name, domain).lower():
            outfile.write(
                ""%s   %s   %s\n"" % (address, self.qualify(name, domain), entry[""type""])
            )
        else:
            outfile.write(line)
    infile.close()
    outfile.close()
    shutil.move(outfile.name, self.filename)
","if entry and entry [ ""name"" ] . lower ( ) == self . qualify ( name , domain ) . lower ( ) :",197
"def tms_to_quadkey(self, tms, google=False):
    quadKey = """"
    x, y, z = tms
    # this algorithm works with google tiles, rather than tms, so convert
    # to those first.
    if not google:
        y = (2 ** z - 1) - y
    for i in range(z, 0, -1):
        digit = 0
        mask = 1 << (i - 1)
        if (x & mask) != 0:
            digit += 1
        if (y & mask) != 0:
            digit += 2
        quadKey += str(digit)
    return quadKey
",if ( y & mask ) != 0 :,164
"def add_if_unique(self, issuer, use, keys):
    if use in self.issuer_keys[issuer] and self.issuer_keys[issuer][use]:
        for typ, key in keys:
            flag = 1
            for _typ, _key in self.issuer_keys[issuer][use]:
                if _typ == typ and key is _key:
                    flag = 0
                    break
            if flag:
                self.issuer_keys[issuer][use].append((typ, key))
    else:
        self.issuer_keys[issuer][use] = keys
",if flag :,158
"def scan_error(self):
    ""A string describing why the last scan failed, or None if it didn't.""
    self.acquire_lock()
    try:
        if self._scan_error_cache is None:
            try:
                self._load_buf_data_once()
            except NotFoundInDatabase:
                pass
        return self._scan_error_cache
    finally:
        self.release_lock()
",if self . _scan_error_cache is None :,114
"def _query(self):
    if self._mongo_query is None:
        self._mongo_query = self._query_obj.to_query(self._document)
        if self._cls_query:
            if ""_cls"" in self._mongo_query:
                self._mongo_query = {""$and"": [self._cls_query, self._mongo_query]}
            else:
                self._mongo_query.update(self._cls_query)
    return self._mongo_query
",if self . _cls_query :,127
"def CountButtons(self):
    """"""Returns the number of visible buttons in the docked pane.""""""
    n = 0
    if self.HasCaption() or self.HasCaptionLeft():
        if isinstance(wx.GetTopLevelParent(self.window), AuiFloatingFrame):
            return 1
        if self.HasCloseButton():
            n += 1
        if self.HasMaximizeButton():
            n += 1
        if self.HasMinimizeButton():
            n += 1
        if self.HasPinButton():
            n += 1
    return n
",if self . HasPinButton ( ) :,149
"def testBind(self):
    try:
        with socket.socket(socket.PF_CAN, socket.SOCK_DGRAM, socket.CAN_J1939) as s:
            addr = (
                self.interface,
                socket.J1939_NO_NAME,
                socket.J1939_NO_PGN,
                socket.J1939_NO_ADDR,
            )
            s.bind(addr)
            self.assertEqual(s.getsockname(), addr)
    except OSError as e:
        if e.errno == errno.ENODEV:
            self.skipTest(""network interface `%s` does not exist"" % self.interface)
        else:
            raise
",if e . errno == errno . ENODEV :,189
"def createFields(self):
    while self.current_size < self.size:
        pos = self.stream.searchBytes(
            ""\0\0\1"", self.current_size, self.current_size + 1024 * 1024 * 8
        )  # seek forward by at most 1MB
        if pos is not None:
            padsize = pos - self.current_size
            if padsize:
                yield PaddingBytes(self, ""pad[]"", padsize // 8)
        chunk = Chunk(self, ""chunk[]"")
        try:
            # force chunk to be processed, so that CustomFragments are complete
            chunk[""content/data""]
        except:
            pass
        yield chunk
",if padsize :,184
"def index_modulemd_files(repo_path):
    merger = Modulemd.ModuleIndexMerger()
    for fn in sorted(os.listdir(repo_path)):
        if not fn.endswith("".yaml""):
            continue
        yaml_path = os.path.join(repo_path, fn)
        mmd = Modulemd.ModuleIndex()
        mmd.update_from_file(yaml_path, strict=True)
        merger.associate_index(mmd, 0)
    return merger.resolve()
","if not fn . endswith ( "".yaml"" ) :",129
"def set_visible(self, visible=True):
    self._visible = visible
    if self._nswindow is not None:
        if visible:
            # Not really sure why on_resize needs to be here,
            # but it's what pyglet wants.
            self.dispatch_event(""on_resize"", self._width, self._height)
            self.dispatch_event(""on_show"")
            self.dispatch_event(""on_expose"")
            self._nswindow.makeKeyAndOrderFront_(None)
        else:
            self._nswindow.orderOut_(None)
",if visible :,153
"def __repr__(self):
    if self._in_repr:
        return ""<recursion>""
    try:
        self._in_repr = True
        if self.is_computed():
            status = ""computed, ""
            if self.error() is None:
                if self.value() is self:
                    status += ""= self""
                else:
                    status += ""= "" + repr(self.value())
            else:
                status += ""error = "" + repr(self.error())
        else:
            status = ""isn't computed""
        return ""%s (%s)"" % (type(self), status)
    finally:
        self._in_repr = False
",if self . value ( ) is self :,189
"def _individual_get(self, segment, index_type, index, strictdoc):
    if index_type == ""val"":
        for key, value in segment.items():
            if key == index[0]:
                return value
            if hasattr(key, ""text""):
                if key.text == index[0]:
                    return value
        raise Exception(""Invalid state"")
    elif index_type == ""index"":
        return segment[index]
    elif index_type == ""textslice"":
        return segment[index[0] : index[1]]
    elif index_type == ""key"":
        return index[1] if strictdoc else index[0]
    else:
        raise Exception(""Invalid state"")
","if hasattr ( key , ""text"" ) :",186
"def _makeSafeAbsoluteURI(base, rel=None):
    # bail if ACCEPTABLE_URI_SCHEMES is empty
    if not ACCEPTABLE_URI_SCHEMES:
        return _urljoin(base, rel or u"""")
    if not base:
        return rel or u""""
    if not rel:
        try:
            scheme = urlparse.urlparse(base)[0]
        except ValueError:
            return u""""
        if not scheme or scheme in ACCEPTABLE_URI_SCHEMES:
            return base
        return u""""
    uri = _urljoin(base, rel)
    if uri.strip().split("":"", 1)[0] not in ACCEPTABLE_URI_SCHEMES:
        return u""""
    return uri
",if not scheme or scheme in ACCEPTABLE_URI_SCHEMES :,186
"def _write_packet(self, packet):
    # Immediately writes the given packet to the network. The caller must
    # have the write lock acquired before calling this method.
    try:
        for listener in self.early_outgoing_packet_listeners:
            listener.call_packet(packet)
        if self.options.compression_enabled:
            packet.write(self.socket, self.options.compression_threshold)
        else:
            packet.write(self.socket)
        for listener in self.outgoing_packet_listeners:
            listener.call_packet(packet)
    except IgnorePacket:
        pass
",if self . options . compression_enabled :,160
"def rangelist_to_set(rangelist):
    result = set()
    if not rangelist:
        return result
    for x in rangelist.split("",""):
        if re.match(r""^(\d+)$"", x):
            result.add(int(x))
            continue
        m = re.match(r""^(\d+)-(\d+)$"", x)
        if m:
            start = int(m.group(1))
            end = int(m.group(2))
            result.update(set(range(start, end + 1)))
            continue
        msg = ""Cannot understand data input: %s %s"" % (x, rangelist)
        raise ValueError(msg)
    return result
","if re . match ( r""^(\d+)$"" , x ) :",181
"def test_device_property_logfile_isinstance(self):
    mock = MagicMock()
    with patch(builtin_string + "".open"", mock):
        if sys.version > ""3"":
            builtin_file = ""io.TextIOWrapper""
        else:
            builtin_file = builtin_string + "".file""
        with patch(builtin_file, MagicMock):
            handle = open(""filename"", ""r"")
            self.dev.logfile = handle
            self.assertEqual(self.dev.logfile, handle)
","if sys . version > ""3"" :",130
"def _line_ranges(statements, lines):
    """"""Produce a list of ranges for `format_lines`.""""""
    statements = sorted(statements)
    lines = sorted(lines)
    pairs = []
    start = None
    lidx = 0
    for stmt in statements:
        if lidx >= len(lines):
            break
        if stmt == lines[lidx]:
            lidx += 1
            if not start:
                start = stmt
            end = stmt
        elif start:
            pairs.append((start, end))
            start = None
    if start:
        pairs.append((start, end))
    return pairs
",elif start :,167
"def reset_parameters(self):
    initialize = layers.get_initializer(self._hparams.initializer)
    if initialize is not None:
        # Do not re-initialize LayerNorm modules.
        for name, param in self.named_parameters():
            if name.split(""."")[-1] == ""weight"" and ""layer_norm"" not in name:
                initialize(param)
","if name . split ( ""."" ) [ - 1 ] == ""weight"" and ""layer_norm"" not in name :",93
"def billing_invoice_show_validator(namespace):
    from azure.cli.core.azclierror import (
        RequiredArgumentMissingError,
        MutuallyExclusiveArgumentError,
    )
    valid_combs = (
        ""only --account-name, --name / --name / --name, --by-subscription is valid""
    )
    if namespace.account_name is not None:
        if namespace.by_subscription is not None:
            raise MutuallyExclusiveArgumentError(valid_combs)
        if namespace.name is None:
            raise RequiredArgumentMissingError(""--name is also required"")
    if namespace.by_subscription is not None:
        if namespace.name is None:
            raise RequiredArgumentMissingError(""--name is also required"")
",if namespace . name is None :,188
"def DeleteDocuments(self, document_ids, response):
    """"""Deletes documents for the given document_ids.""""""
    for document_id in document_ids:
        if document_id in self._documents:
            document = self._documents[document_id]
            self._inverted_index.RemoveDocument(document)
            del self._documents[document_id]
        delete_status = response.add_status()
        delete_status.set_code(search_service_pb.SearchServiceError.OK)
",if document_id in self . _documents :,125
"def generate_new_element(items, prefix, numeric=False):
    """"""Creates a random string with prefix, that is not in 'items' list.""""""
    while True:
        if numeric:
            candidate = prefix + generate_random_numeric(8)
        else:
            candidate = prefix + generate_random_alphanumeric(8)
        if not candidate in items:
            return candidate
        LOG.debug(""Random collision on %s"" % candidate)
",if numeric :,115
"def generate_text_for_vocab(self, data_dir, tmp_dir):
    for i, sample in enumerate(
        self.generate_samples(data_dir, tmp_dir, problem.DatasetSplit.TRAIN)
    ):
        if self.has_inputs:
            yield sample[""inputs""]
        yield sample[""targets""]
        if self.max_samples_for_vocab and (i + 1) >= self.max_samples_for_vocab:
            break
",if self . max_samples_for_vocab and ( i + 1 ) >= self . max_samples_for_vocab :,118
"def _get_ccp(config=None, config_path=None, saltenv=""base""):
    """""" """"""
    if config_path:
        config = __salt__[""cp.get_file_str""](config_path, saltenv=saltenv)
        if config is False:
            raise SaltException(""{} is not available"".format(config_path))
    if isinstance(config, six.string_types):
        config = config.splitlines()
    ccp = ciscoconfparse.CiscoConfParse(config)
    return ccp
",if config is False :,135
"def rpush(key, *vals, **kwargs):
    ttl = kwargs.get(""ttl"")
    cap = kwargs.get(""cap"")
    if not ttl and not cap:
        _client.rpush(key, *vals)
    else:
        pipe = _client.pipeline()
        pipe.rpush(key, *vals)
        if cap:
            pipe.ltrim(key, 0, cap)
        if ttl:
            pipe.expire(key, ttl)
        pipe.execute()
",if cap :,131
"def check_apns_certificate(ss):
    mode = ""start""
    for s in ss.split(""\n""):
        if mode == ""start"":
            if ""BEGIN RSA PRIVATE KEY"" in s or ""BEGIN PRIVATE KEY"" in s:
                mode = ""key""
        elif mode == ""key"":
            if ""END RSA PRIVATE KEY"" in s or ""END PRIVATE KEY"" in s:
                mode = ""end""
                break
            elif s.startswith(""Proc-Type"") and ""ENCRYPTED"" in s:
                raise ImproperlyConfigured(
                    ""Encrypted APNS private keys are not supported""
                )
    if mode != ""end"":
        raise ImproperlyConfigured(""The APNS certificate doesn't contain a private key"")
","if mode == ""start"" :",195
"def _add_communication_type(apps, schema_editor, communication_type):
    Worker = apps.get_model(""orchestra"", ""Worker"")
    CommunicationPreference = apps.get_model(""orchestra"", ""CommunicationPreference"")
    for worker in Worker.objects.all():
        (
            communication_preference,
            created,
        ) = CommunicationPreference.objects.get_or_create(
            worker=worker, communication_type=communication_type
        )
        # By default set both Slack and Email notifications to True
        if created:
            communication_preference.methods.slack = True
            communication_preference.methods.email = True
        communication_preference.save()
",if created :,183
"def get_postgresql_driver_name():
    # pylint: disable=unused-variable
    try:
        driver = os.getenv(""CODECHECKER_DB_DRIVER"")
        if driver:
            return driver
        try:
            # pylint: disable=W0611
            import psycopg2
            return ""psycopg2""
        except Exception:
            # pylint: disable=W0611
            import pg8000
            return ""pg8000""
    except Exception as ex:
        LOG.error(str(ex))
        LOG.error(""Failed to import psycopg2 or pg8000 module."")
        raise
",if driver :,157
"def env_purge_doc(app: Sphinx, env: BuildEnvironment, docname: str) -> None:
    modules = getattr(env, ""_viewcode_modules"", {})
    for modname, entry in list(modules.items()):
        if entry is False:
            continue
        code, tags, used, refname = entry
        for fullname in list(used):
            if used[fullname] == docname:
                used.pop(fullname)
        if len(used) == 0:
            modules.pop(modname)
",if len ( used ) == 0 :,133
"def do_query(data, q):
    ret = []
    if not q:
        return ret
    qkey = q[0]
    for key, value in iterate(data):
        if len(q) == 1:
            if key == qkey:
                ret.append(value)
            elif is_iterable(value):
                ret.extend(do_query(value, q))
        else:
            if not is_iterable(value):
                continue
            if key == qkey:
                ret.extend(do_query(value, q[1:]))
            else:
                ret.extend(do_query(value, q))
    return ret
",if not is_iterable ( value ) :,185
"def _get_bucket_for_key(self, key: bytes) -> Optional[_DBValueTuple]:
    dbs: Iterable[PartitionDB]
    try:
        partition = self._key_index[key]
        dbs = [PartitionDB(partition, self._dbs[partition])]
    except KeyError:
        dbs = cast(Iterable[PartitionDB], self._dbs.items())
    for partition, db in dbs:
        if db.key_may_exist(key)[0]:
            value = db.get(key)
            if value is not None:
                self._key_index[key] = partition
                return _DBValueTuple(db, value)
    return None
",if value is not None :,177
"def _clean(self):
    logger.info(""Cleaning up..."")
    if self._process is not None:
        if self._process.poll() is None:
            for _ in range(3):
                self._process.terminate()
                time.sleep(0.5)
                if self._process.poll() is not None:
                    break
            else:
                self._process.kill()
                self._process.wait()
                logger.error(""KILLED"")
    if os.path.exists(self._tmp_dir):
        shutil.rmtree(self._tmp_dir)
    self._process = None
    self._ws = None
    logger.info(""Cleanup complete"")
",if self . _process . poll ( ) is not None :,189
"def _calculate_runtimes(states):
    results = {""runtime"": 0.00, ""num_failed_states"": 0, ""num_passed_states"": 0}
    for state, resultset in states.items():
        if isinstance(resultset, dict) and ""duration"" in resultset:
            # Count the pass vs failures
            if resultset[""result""]:
                results[""num_passed_states""] += 1
            else:
                results[""num_failed_states""] += 1
            # Count durations
            results[""runtime""] += resultset[""duration""]
    log.debug(""Parsed state metrics: {}"".format(results))
    return results
","if resultset [ ""result"" ] :",167
"def spaces_after(token, prev, next, min=-1, max=-1, min_desc=None, max_desc=None):
    if next is not None and token.end_mark.line == next.start_mark.line:
        spaces = next.start_mark.pointer - token.end_mark.pointer
        if max != -1 and spaces > max:
            return LintProblem(
                token.start_mark.line + 1, next.start_mark.column, max_desc
            )
        elif min != -1 and spaces < min:
            return LintProblem(
                token.start_mark.line + 1, next.start_mark.column + 1, min_desc
            )
",elif min != - 1 and spaces < min :,184
"def getfileinfo(name):
    finfo = FInfo()
    with io.open(name, ""rb"") as fp:
        # Quick check for textfile
        data = fp.read(512)
        if 0 not in data:
            finfo.Type = ""TEXT""
        fp.seek(0, 2)
        dsize = fp.tell()
    dir, file = os.path.split(name)
    file = file.replace("":"", ""-"", 1)
    return file, finfo, dsize, 0
",if 0 not in data :,124
"def dict_to_XML(tag, dictionary, **kwargs):
    """"""Return XML element converting dicts recursively.""""""
    elem = Element(tag, **kwargs)
    for key, val in dictionary.items():
        if tag == ""layers"":
            child = dict_to_XML(""layer"", val, name=key)
        elif isinstance(val, MutableMapping):
            child = dict_to_XML(key, val)
        else:
            if tag == ""config"":
                child = Element(""variable"", name=key)
            else:
                child = Element(key)
            child.text = str(val)
        elem.append(child)
    return elem
","elif isinstance ( val , MutableMapping ) :",175
"def _read_bytes(self, length):
    buffer = b""""
    while length:
        chunk = self.request.recv(length)
        if chunk == b"""":
            log.debug(""Connection closed"")
            return False
        length -= len(chunk)
        buffer += chunk
    return buffer
","if chunk == b"""" :",79
"def rec_deps(services, container_by_name, cnt, init_service):
    deps = cnt[""_deps""]
    for dep in deps.copy():
        dep_cnts = services.get(dep)
        if not dep_cnts:
            continue
        dep_cnt = container_by_name.get(dep_cnts[0])
        if dep_cnt:
            # TODO: avoid creating loops, A->B->A
            if init_service and init_service in dep_cnt[""_deps""]:
                continue
            new_deps = rec_deps(services, container_by_name, dep_cnt, init_service)
            deps.update(new_deps)
    return deps
",if not dep_cnts :,181
"def fix_repeating_arguments(self):
    """"""Fix elements that should accumulate/increment values.""""""
    either = [list(child.children) for child in transform(self).children]
    for case in either:
        for e in [child for child in case if case.count(child) > 1]:
            if type(e) is Argument or type(e) is Option and e.argcount:
                if e.value is None:
                    e.value = []
                elif type(e.value) is not list:
                    e.value = e.value.split()
            if type(e) is Command or type(e) is Option and e.argcount == 0:
                e.value = 0
    return self
",if type ( e ) is Argument or type ( e ) is Option and e . argcount :,190
"def do_cli(manager, options):
    header = [""Name"", ""Description""]
    table_data = [header]
    for filter_name, filter in get_filters():
        if options.name and not options.name in filter_name:
            continue
        filter_doc = inspect.getdoc(filter) or """"
        table_data.append([filter_name, filter_doc])
    try:
        table = TerminalTable(options.table_type, table_data)
    except TerminalTableError as e:
        console(""ERROR: %s"" % str(e))
    else:
        console(table.output)
",if options . name and not options . name in filter_name :,155
"def _do_cmp(f1, f2):
    bufsize = BUFSIZE
    with open(f1, ""rb"") as fp1, open(f2, ""rb"") as fp2:
        while True:
            b1 = fp1.read(bufsize)
            b2 = fp2.read(bufsize)
            if b1 != b2:
                return False
            if not b1:
                return True
",if not b1 :,118
"def apply(self, db, person):
    families = person.get_parent_family_handle_list()
    if families == []:
        return True
    for family_handle in person.get_parent_family_handle_list():
        family = db.get_family_from_handle(family_handle)
        if family:
            father_handle = family.get_father_handle()
            mother_handle = family.get_mother_handle()
            if not father_handle:
                return True
            if not mother_handle:
                return True
    return False
",if not father_handle :,157
"def caesar_cipher(s, k):
    result = """"
    for char in s:
        n = ord(char)
        if 64 < n < 91:
            n = ((n - 65 + k) % 26) + 65
        if 96 < n < 123:
            n = ((n - 97 + k) % 26) + 97
        result = result + chr(n)
    return result
",if 96 < n < 123 :,104
"def title_by_index(self, trans, index, context):
    d_type = self.get_datatype(trans, context)
    for i, (composite_name, composite_file) in enumerate(d_type.writable_files.items()):
        if i == index:
            rval = composite_name
            if composite_file.description:
                rval = ""{} ({})"".format(rval, composite_file.description)
            if composite_file.optional:
                rval = ""%s [optional]"" % rval
            return rval
    if index < self.get_file_count(trans, context):
        return ""Extra primary file""
    return None
",if composite_file . optional :,167
"def __str__(self):
    t = ""    ""
    if self._name != ""root"":
        r = f""{t * (self._level-1)}{self._name}:\n""
    else:
        r = """"
    level = self._level
    for i, (k, v) in enumerate(self._pointer.items()):
        if isinstance(v, Config):
            r += f""{t * (self._level)}{v}\n""
            self._level += 1
        else:
            r += f""{t * (self._level)}{k}: {v} ({type(v).__name__})\n""
        self._level = level
    return r[:-1]
","if isinstance ( v , Config ) :",176
"def __get_securitygroups(vm_):
    vm_securitygroups = config.get_cloud_config_value(
        ""securitygroups"", vm_, __opts__, search_global=False
    )
    if not vm_securitygroups:
        return []
    securitygroups = list_securitygroups()
    for i in range(len(vm_securitygroups)):
        vm_securitygroups[i] = six.text_type(vm_securitygroups[i])
        if vm_securitygroups[i] not in securitygroups:
            raise SaltCloudNotFound(
                ""The specified securitygroups '{0}' could not be found."".format(
                    vm_securitygroups[i]
                )
            )
    return vm_securitygroups
",if vm_securitygroups [ i ] not in securitygroups :,186
"def assert_walk_snapshot(
    self, field, filespecs_or_globs, paths, ignore_patterns=None, prepare=None
):
    with self.mk_project_tree(ignore_patterns=ignore_patterns) as project_tree:
        scheduler = self.mk_scheduler(
            rules=create_fs_rules(), project_tree=project_tree
        )
        if prepare:
            prepare(project_tree)
        result = self.execute(scheduler, Snapshot, self.specs(filespecs_or_globs))[0]
        self.assertEqual(sorted(getattr(result, field)), sorted(paths))
",if prepare :,152
"def _parse_rowids(self, rowids):
    xploded = []
    rowids = [x.strip() for x in rowids.split("","")]
    for rowid in rowids:
        try:
            if ""-"" in rowid:
                start = int(rowid.split(""-"")[0].strip())
                end = int(rowid.split(""-"")[-1].strip())
                xploded += range(start, end + 1)
            else:
                xploded.append(int(rowid))
        except ValueError:
            continue
    return sorted(list(set(xploded)))
","if ""-"" in rowid :",156
"def ensemble(self, pairs, other_preds):
    """"""Ensemble the dict with statistical model predictions.""""""
    lemmas = []
    assert len(pairs) == len(other_preds)
    for p, pred in zip(pairs, other_preds):
        w, pos = p
        if (w, pos) in self.composite_dict:
            lemma = self.composite_dict[(w, pos)]
        elif w in self.word_dict:
            lemma = self.word_dict[w]
        else:
            lemma = pred
        if lemma is None:
            lemma = w
        lemmas.append(lemma)
    return lemmas
","if ( w , pos ) in self . composite_dict :",164
"def selectionToChunks(self, remove=False, add=False):
    box = self.selectionBox()
    if box:
        if box == self.level.bounds:
            self.selectedChunks = set(self.level.allChunks)
            return
        selectedChunks = self.selectedChunks
        boxedChunks = set(box.chunkPositions)
        if boxedChunks.issubset(selectedChunks):
            remove = True
        if remove and not add:
            selectedChunks.difference_update(boxedChunks)
        else:
            selectedChunks.update(boxedChunks)
    self.selectionTool.selectNone()
",if box == self . level . bounds :,158
"def _ensure_max_size(cls, image, max_size, interpolation):
    if max_size is not None:
        size = max(image.shape[0], image.shape[1])
        if size > max_size:
            resize_factor = max_size / size
            new_height = int(image.shape[0] * resize_factor)
            new_width = int(image.shape[1] * resize_factor)
            image = ia.imresize_single_image(
                image, (new_height, new_width), interpolation=interpolation
            )
    return image
",if size > max_size :,155
"def _1_0_cloud_ips(self, method, url, body, headers):
    if method == ""GET"":
        return self.test_response(httplib.OK, self.fixtures.load(""list_cloud_ips.json""))
    elif method == ""POST"":
        if body:
            body = json.loads(body)
        node = json.loads(self.fixtures.load(""create_cloud_ip.json""))
        if ""reverse_dns"" in body:
            node[""reverse_dns""] = body[""reverse_dns""]
        return self.test_response(httplib.ACCEPTED, json.dumps(node))
",if body :,159
"def get_formatted_stats(self):
    """"""Get percentage or number of rar's done""""""
    if self.cur_setname and self.cur_setname in self.total_volumes:
        # This won't work on obfuscated posts
        if self.total_volumes[self.cur_setname] >= self.cur_volume and self.cur_volume:
            return ""%02d/%02d"" % (self.cur_volume, self.total_volumes[self.cur_setname])
    return self.cur_volume
",if self . total_volumes [ self . cur_setname ] >= self . cur_volume and self . cur_volume :,128
"def wdayset(self, year, month, day):
    # We need to handle cross-year weeks here.
    dset = [None] * (self.yearlen + 7)
    i = datetime.date(year, month, day).toordinal() - self.yearordinal
    start = i
    for j in range(7):
        dset[i] = i
        i += 1
        # if (not (0 <= i < self.yearlen) or
        #    self.wdaymask[i] == self.rrule._wkst):
        # This will cross the year boundary, if necessary.
        if self.wdaymask[i] == self.rrule._wkst:
            break
    return dset, start, i
",if self . wdaymask [ i ] == self . rrule . _wkst :,184
"def do_acquire_read_lock(self, wait=True):
    self.condition.acquire()
    try:
        # see if a synchronous operation is waiting to start
        # or is already running, in which case we wait (or just
        # give up and return)
        if wait:
            while self.current_sync_operation is not None:
                self.condition.wait()
        else:
            if self.current_sync_operation is not None:
                return False
        self.asynch += 1
    finally:
        self.condition.release()
    if not wait:
        return True
",if wait :,162
"def _blend(x, y):  # pylint: disable=invalid-name
    """"""Implements the ""blend"" strategy for `deep_merge`.""""""
    if isinstance(x, (dict, OrderedDict)):
        if not isinstance(y, (dict, OrderedDict)):
            return y
        return _merge(x, y, recursion_func=_blend)
    if isinstance(x, (list, tuple)):
        if not isinstance(y, (list, tuple)):
            return y
        result = [_blend(*i) for i in zip(x, y)]
        if len(x) > len(y):
            result += x[len(y) :]
        elif len(x) < len(y):
            result += y[len(x) :]
        return result
    return y
","if not isinstance ( y , ( dict , OrderedDict ) ) :",194
"def update_forum_nums_topic_post(modeladmin, request, queryset):
    for forum in queryset:
        forum.num_topics = forum.count_nums_topic()
        forum.num_posts = forum.count_nums_post()
        if forum.num_topics:
            forum.last_post = forum.topic_set.order_by(""-last_reply_on"")[0].last_post
        else:
            forum.last_post = """"
        forum.save()
",if forum . num_topics :,123
"def get_docname_for_node(self, node: Node) -> str:
    while node:
        if isinstance(node, nodes.document):
            return self.env.path2doc(node[""source""])
        elif isinstance(node, addnodes.start_of_file):
            return node[""docname""]
        else:
            node = node.parent
    return None  # never reached here. only for type hinting
","elif isinstance ( node , addnodes . start_of_file ) :",110
"def _selected_machines(self, virtual_machines):
    selected_machines = []
    for machine in virtual_machines:
        if self._args.host and self._args.host == machine.name:
            selected_machines.append(machine)
        if self.tags and self._tags_match(machine.tags, self.tags):
            selected_machines.append(machine)
        if self.locations and machine.location in self.locations:
            selected_machines.append(machine)
    return selected_machines
",if self . locations and machine . location in self . locations :,129
"def transform_kwarg(self, name, value, split_single_char_options):
    if len(name) == 1:
        if value is True:
            return [""-%s"" % name]
        elif value not in (False, None):
            if split_single_char_options:
                return [""-%s"" % name, ""%s"" % value]
            else:
                return [""-%s%s"" % (name, value)]
    else:
        if value is True:
            return [""--%s"" % dashify(name)]
        elif value is not False and value is not None:
            return [""--%s=%s"" % (dashify(name), value)]
    return []
","elif value not in ( False , None ) :",183
"def indent(elem, level=0):
    i = ""\n"" + level * ""  ""
    if len(elem):
        if not elem.text or not elem.text.strip():
            elem.text = i + ""  ""
        if not elem.tail or not elem.tail.strip():
            elem.tail = i
        for elem in elem:
            indent(elem, level + 1)
        if not elem.tail or not elem.tail.strip():
            elem.tail = i
    else:
        if level and (not elem.tail or not elem.tail.strip()):
            elem.tail = i
",if level and ( not elem . tail or not elem . tail . strip ( ) ) :,161
"def _run_instances_op(self, op, instance_ids, **kwargs):
    while instance_ids:
        try:
            return self.manager.retry(op, InstanceIds=instance_ids, **kwargs)
        except ClientError as e:
            if e.response[""Error""][""Code""] == ""IncorrectInstanceState"":
                instance_ids.remove(extract_instance_id(e))
            raise
","if e . response [ ""Error"" ] [ ""Code"" ] == ""IncorrectInstanceState"" :",105
"def runTest(self):
    self.poco(text=""wait UI"").click()
    bomb_count = 0
    while True:
        blue_fish = self.poco(""fish_emitter"").child(""blue"")
        yellow_fish = self.poco(""fish_emitter"").child(""yellow"")
        bomb = self.poco(""fish_emitter"").child(""bomb"")
        fish = self.poco.wait_for_any([blue_fish, yellow_fish, bomb])
        if fish is bomb:
            bomb_count += 1
            if bomb_count > 3:
                return
        else:
            fish.click()
        time.sleep(2.5)
",if fish is bomb :,192
"def lineWidth(self, lw=None):
    """"""Set/get width of mesh edges. Same as `lw()`.""""""
    if lw is not None:
        if lw == 0:
            self.GetProperty().EdgeVisibilityOff()
            self.GetProperty().SetRepresentationToSurface()
            return self
        self.GetProperty().EdgeVisibilityOn()
        self.GetProperty().SetLineWidth(lw)
    else:
        return self.GetProperty().GetLineWidth()
    return self
",if lw == 0 :,130
"def _current_date_updater(doc, field_name, value):
    if isinstance(doc, dict):
        if value == {""$type"": ""timestamp""}:
            # TODO(juannyg): get_current_timestamp should also be using helpers utcnow,
            # as it currently using time.time internally
            doc[field_name] = helpers.get_current_timestamp()
        else:
            doc[field_name] = mongomock.utcnow()
","if value == { ""$type"" : ""timestamp"" } :",118
"def fill_members(self):
    if self._get_retrieve():
        after = self.after.id if self.after else None
        data = await self.get_members(self.guild.id, self.retrieve, after)
        if not data:
            # no data, terminate
            return
        if len(data) < 1000:
            self.limit = 0  # terminate loop
        self.after = Object(id=int(data[-1][""user""][""id""]))
        for element in reversed(data):
            await self.members.put(self.create_member(element))
",if not data :,153
"def extract(self, page, start_index=0, end_index=None):
    items = []
    for extractor in self.extractors:
        extracted = extractor.extract(
            page, start_index, end_index, self.template.ignored_regions
        )
        for item in arg_to_iter(extracted):
            if item:
                if isinstance(item, (ItemProcessor, dict)):
                    item[u""_template""] = self.template.id
                items.append(item)
    return items
","if isinstance ( item , ( ItemProcessor , dict ) ) :",141
"def _get_node_type_specific_fields(self, node_id: str, fields_key: str) -> Any:
    fields = self.config[fields_key]
    node_tags = self.provider.node_tags(node_id)
    if TAG_RAY_USER_NODE_TYPE in node_tags:
        node_type = node_tags[TAG_RAY_USER_NODE_TYPE]
        if node_type not in self.available_node_types:
            raise ValueError(f""Unknown node type tag: {node_type}."")
        node_specific_config = self.available_node_types[node_type]
        if fields_key in node_specific_config:
            fields = node_specific_config[fields_key]
    return fields
",if fields_key in node_specific_config :,189
"def _write_all(self, writer):
    """"""Writes messages and insert comments here and there.""""""
    # Note: we make no assumptions about the length of original_messages and original_comments
    for msg, comment in zip_longest(
        self.original_messages, self.original_comments, fillvalue=None
    ):
        # msg and comment might be None
        if comment is not None:
            print(""writing comment: "", comment)
            writer.log_event(comment)  # we already know that this method exists
        if msg is not None:
            print(""writing message: "", msg)
            writer(msg)
",if comment is not None :,157
"def run_tests():
    # type: () -> None
    x = 5
    with switch(x) as case:
        if case(0):
            print(""zero"")
            print(""zero"")
        elif case(1, 2):
            print(""one or two"")
        elif case(3, 4):
            print(""three or four"")
        else:
            print(""default"")
            print(""another"")
",if case ( 0 ) :,114
"def date_to_format(value, target_format):
    """"""Convert date to specified format""""""
    if target_format == str:
        if isinstance(value, datetime.date):
            ret = value.strftime(""%d/%m/%y"")
        elif isinstance(value, datetime.datetime):
            ret = value.strftime(""%d/%m/%y"")
        elif isinstance(value, datetime.time):
            ret = value.strftime(""%H:%M:%S"")
    else:
        ret = value
    return ret
","elif isinstance ( value , datetime . datetime ) :",130
"def database_app(request):
    if request.param == ""postgres_app"":
        if not which(""initdb""):
            pytest.skip(""initdb must be on PATH for postgresql fixture"")
        if not psycopg2:
            pytest.skip(""psycopg2 must be installed for postgresql fixture"")
    if request.param == ""sqlite_rabbitmq_app"":
        if not os.environ.get(""GALAXY_TEST_AMQP_INTERNAL_CONNECTION""):
            pytest.skip(
                ""rabbitmq tests will be skipped if GALAXY_TEST_AMQP_INTERNAL_CONNECTION env var is unset""
            )
    return request.getfixturevalue(request.param)
",if not psycopg2 :,174
"def poll_ms(self, timeout=-1):
    s = bytearray(self.evbuf)
    if timeout >= 0:
        deadline = utime.ticks_add(utime.ticks_ms(), timeout)
    while True:
        n = epoll_wait(self.epfd, s, 1, timeout)
        if not os.check_error(n):
            break
        if timeout >= 0:
            timeout = utime.ticks_diff(deadline, utime.ticks_ms())
            if timeout < 0:
                n = 0
                break
    res = []
    if n > 0:
        vals = struct.unpack(epoll_event, s)
        res.append((vals[1], vals[0]))
    return res
",if timeout >= 0 :,192
"def get_all_active_plugins(self) -> List[BotPlugin]:
    """"""This returns the list of plugins in the callback ordered defined from the config.""""""
    all_plugins = []
    for name in self.plugins_callback_order:
        # None is a placeholder for any plugin not having a defined order
        if name is None:
            all_plugins += [
                plugin
                for name, plugin in self.plugins.items()
                if name not in self.plugins_callback_order and plugin.is_activated
            ]
        else:
            plugin = self.plugins[name]
            if plugin.is_activated:
                all_plugins.append(plugin)
    return all_plugins
",if plugin . is_activated :,186
"def get_expected_sql(self):
    sql_base_path = path.join(path.dirname(path.realpath(__file__)), ""sql"")
    # Iterate the version mapping directories.
    for version_mapping in get_version_mapping_directories(self.server[""type""]):
        if version_mapping[""number""] > self.server_information[""server_version""]:
            continue
        complete_path = path.join(sql_base_path, version_mapping[""name""])
        if not path.exists(complete_path):
            continue
        break
    data_sql = """"
    with open(path.join(complete_path, ""test_sql_output.sql"")) as fp:
        data_sql = fp.read()
    return data_sql
","if version_mapping [ ""number"" ] > self . server_information [ ""server_version"" ] :",185
"def _validate_headers(self, headers):
    if headers is None:
        return headers
    res = {}
    for key, value in headers.items():
        if isinstance(value, (int, float)):
            value = str(value)
        if not isinstance(key, (bytes, str)) or not isinstance(value, (bytes, str)):
            raise ScriptError(
                {
                    ""message"": ""headers must be a table""
                    "" with strings as keys and values.""
                    ""Header: `{!r}:{!r}` is not valid"".format(key, value)
                }
            )
        res[key] = value
    return res
","if not isinstance ( key , ( bytes , str ) ) or not isinstance ( value , ( bytes , str ) ) :",181
"def _get_literal_value(self, pyval):
    if pyval == self.vm.lookup_builtin(""builtins.True""):
        return True
    elif pyval == self.vm.lookup_builtin(""builtins.False""):
        return False
    elif isinstance(pyval, str):
        prefix, value = parser_constants.STRING_RE.match(pyval).groups()[:2]
        value = value[1:-1]  # remove quotation marks
        if ""b"" in prefix and not self.vm.PY2:
            value = compat.bytestring(value)
        elif ""u"" in prefix and self.vm.PY2:
            value = compat.UnicodeType(value)
        return value
    else:
        return pyval
","if ""b"" in prefix and not self . vm . PY2 :",183
"def decode_query_ids(self, trans, conditional):
    if conditional.operator == ""and"":
        self.decode_query_ids(trans, conditional.left)
        self.decode_query_ids(trans, conditional.right)
    else:
        left_base = conditional.left.split(""."")[0]
        if left_base in self.FIELDS:
            field = self.FIELDS[left_base]
            if field.id_decode:
                conditional.right = trans.security.decode_id(conditional.right)
",if left_base in self . FIELDS :,135
"def testLastPhrases(self):
    for day in (11, 12, 13, 14, 15, 16, 17):
        start = datetime.datetime(2012, 11, day, 9, 0, 0)
        (yr, mth, dy, _, _, _, wd, yd, isdst) = start.timetuple()
        n = 4 - wd
        if n >= 0:
            n -= 7
        target = start + datetime.timedelta(days=n)
        self.assertExpectedResult(
            self.cal.parse(""last friday"", start.timetuple()),
            (target.timetuple(), 1),
            dateOnly=True,
        )
",if n >= 0 :,168
"def _convertNbCharsInNbBits(self, nbChars):
    nbMinBit = None
    nbMaxBit = None
    if nbChars is not None:
        if isinstance(nbChars, int):
            nbMinBit = nbChars * 8
            nbMaxBit = nbMinBit
        else:
            if nbChars[0] is not None:
                nbMinBit = nbChars[0] * 8
            if nbChars[1] is not None:
                nbMaxBit = nbChars[1] * 8
    return (nbMinBit, nbMaxBit)
",if nbChars [ 0 ] is not None :,158
"def getpystone():
    # Start calculation
    maxpystone = 0
    # Start with a short run, find the the pystone, and increase runtime until duration took > 0.1 second
    for pyseed in [1000, 2000, 5000, 10000, 20000, 50000, 100000, 200000]:
        duration, pystonefloat = pystones(pyseed)
        maxpystone = max(maxpystone, int(pystonefloat))
        # Stop when pystone() has been running for at least 0.1 second
        if duration > 0.1:
            break
    return maxpystone
",if duration > 0.1 :,144
"def _append_to_io_queue(self, data, stream_name):
    # Make sure ANSI CSI codes and object links are stored as separate events
    # TODO: try to complete previously submitted incomplete code
    parts = re.split(OUTPUT_SPLIT_REGEX, data)
    for part in parts:
        if part:  # split may produce empty string in the beginning or start
            # split the data so that very long lines separated
            for block in re.split(
                ""(.{%d,})"" % (self._get_squeeze_threshold() + 1), part
            ):
                if block:
                    self._queued_io_events.append((block, stream_name))
",if part :,174
"def qtTypeIdent(conn, *args):
    # We're not using the conn object at the moment, but - we will
    # modify the
    # logic to use the server version specific keywords later.
    res = None
    value = None
    for val in args:
        # DataType doesn't have len function then convert it to string
        if not hasattr(val, ""__len__""):
            val = str(val)
        if len(val) == 0:
            continue
        value = val
        if Driver.needsQuoting(val, True):
            value = value.replace('""', '""""')
            value = '""' + value + '""'
        res = ((res and res + ""."") or """") + value
    return res
","if not hasattr ( val , ""__len__"" ) :",181
"def SetVerbose(self, level):
    """"""Sets the verbose level.""""""
    try:
        if type(level) != types.IntType:
            level = int(level)
        if (level >= 0) and (level <= 3):
            self._verbose = level
            return
    except ValueError:
        pass
    self.Error(""Verbose level (%s) must be between 0 and 3 inclusive."" % level)
",if type ( level ) != types . IntType :,105
"def step(self) -> None:
    """"""Performs a single optimization step.""""""
    for group in self.param_groups:
        for p in group[""params""]:
            if p.grad is None:
                continue
            p.add_(p.grad, alpha=(-group[""lr""] * self.num_data))
    return None
",if p . grad is None :,85
"def fill(self, values):
    if lupa.lua_type(values) != ""table"":
        raise ScriptError(
            {
                ""argument"": ""values"",
                ""message"": ""element:fill values is not a table"",
                ""splash_method"": ""fill"",
            }
        )
    # marking all tables as arrays by default
    for key, value in values.items():
        if lupa.lua_type(value) == ""table"":
            _mark_table_as_array(self.lua, value)
    values = self.lua.lua2python(values)
    return self.element.fill(values)
","if lupa . lua_type ( value ) == ""table"" :",177
"def _gen_repr(self, buf):
    print >> buf, ""    def __repr__(self):""
    if self.argnames:
        fmt = COMMA.join([""%s""] * self.nargs)
        if ""("" in self.args:
            fmt = ""(%s)"" % fmt
        vals = [""repr(self.%s)"" % name for name in self.argnames]
        vals = COMMA.join(vals)
        if self.nargs == 1:
            vals = vals + "",""
        print >> buf, '        return ""%s(%s)"" %% (%s)' % (self.name, fmt, vals)
    else:
        print >> buf, '        return ""%s()""' % self.name
","if ""("" in self . args :",189
"def render_observation(self):
    x = self.read_head_position
    label = ""Observation Grid    : ""
    x_str = """"
    for j in range(-1, self.rows + 1):
        if j != -1:
            x_str += "" "" * len(label)
        for i in range(-2, self.input_width + 2):
            if i == x[0] and j == x[1]:
                x_str += colorize(self._get_str_obs((i, j)), ""green"", highlight=True)
            else:
                x_str += self._get_str_obs((i, j))
        x_str += ""\n""
    x_str = label + x_str
    return x_str
",if j != - 1 :,200
"def get_module_comment(self, attrname: str) -> Optional[List[str]]:
    try:
        analyzer = ModuleAnalyzer.for_module(self.modname)
        analyzer.analyze()
        key = ("""", attrname)
        if key in analyzer.attr_docs:
            return list(analyzer.attr_docs[key])
    except PycodeError:
        pass
    return None
",if key in analyzer . attr_docs :,99
"def tms_to_quadkey(self, tms, google=False):
    quadKey = """"
    x, y, z = tms
    # this algorithm works with google tiles, rather than tms, so convert
    # to those first.
    if not google:
        y = (2 ** z - 1) - y
    for i in range(z, 0, -1):
        digit = 0
        mask = 1 << (i - 1)
        if (x & mask) != 0:
            digit += 1
        if (y & mask) != 0:
            digit += 2
        quadKey += str(digit)
    return quadKey
",if ( x & mask ) != 0 :,164
"def test_enumerate(app):
    async with new_stream(app) as stream:
        for i in range(100):
            await stream.channel.deliver(message(key=i, value=i * 4))
        async for i, value in stream.enumerate():
            current_event = stream.current_event
            assert i == current_event.key
            assert value == i * 4
            if i >= 99:
                break
        assert await channel_empty(stream.channel)
",if i >= 99 :,131
"def print_messages(self):
    output_reports = self.config.get_output_report()
    for report in output_reports:
        output_format, output_files = report
        self.summary[""formatter""] = output_format
        formatter = FORMATTERS[output_format](
            self.summary, self.messages, self.config.profile
        )
        if not output_files:
            self.write_to(formatter, sys.stdout)
        for output_file in output_files:
            with open(output_file, ""w+"") as target:
                self.write_to(formatter, target)
",if not output_files :,160
"def eval_metrics(self):
    for task in self.task_list:
        if ""summarize"" in task.name:
            return [
                metrics.Metrics.ACC,
                metrics.Metrics.NEG_LOG_PERPLEXITY,
                metrics.Metrics.ROUGE_2_F,
                metrics.Metrics.ROUGE_L_F,
            ]
    return [
        metrics.Metrics.ACC,
        metrics.Metrics.NEG_LOG_PERPLEXITY,
    ]
","if ""summarize"" in task . name :",137
"def _getBuildRequestForBrdict(self, brdict):
    # Turn a brdict into a BuildRequest into a brdict. This is useful
    # for API like 'nextBuild', which operate on BuildRequest objects.
    breq = self.breqCache.get(brdict[""buildrequestid""])
    if not breq:
        breq = yield BuildRequest.fromBrdict(self.master, brdict)
        if breq:
            self.breqCache[brdict[""buildrequestid""]] = breq
    defer.returnValue(breq)
",if breq :,136
"def _stash_splitter(states):
    keep, split = [], []
    if state_func is not None:
        for s in states:
            ns = state_func(s)
            if isinstance(ns, SimState):
                split.append(ns)
            elif isinstance(ns, (list, tuple, set)):
                split.extend(ns)
            else:
                split.append(s)
    if stash_func is not None:
        split = stash_func(states)
    if to_stash is not stash:
        keep = states
    return keep, split
","if isinstance ( ns , SimState ) :",163
"def sequence_to_text(sequence):
    """"""Converts a sequence of IDs back to a string""""""
    result = """"
    for symbol_id in sequence:
        if symbol_id in _id_to_symbol:
            s = _id_to_symbol[symbol_id]
            # Enclose ARPAbet back in curly braces:
            if len(s) > 1 and s[0] == ""@"":
                s = ""{%s}"" % s[1:]
            result += s
    return result.replace(""}{"", "" "")
",if symbol_id in _id_to_symbol :,137
"def get_code(self, fullname=None):
    fullname = self._fix_name(fullname)
    if self.code is None:
        mod_type = self.etc[2]
        if mod_type == imp.PY_SOURCE:
            source = self.get_source(fullname)
            self.code = compile(source, self.filename, ""exec"")
        elif mod_type == imp.PY_COMPILED:
            self._reopen()
            try:
                self.code = read_code(self.file)
            finally:
                self.file.close()
        elif mod_type == imp.PKG_DIRECTORY:
            self.code = self._get_delegate().get_code()
    return self.code
",if mod_type == imp . PY_SOURCE :,196
"def identwaf(self, findall=False):
    detected = list()
    try:
        self.attackres = self.performCheck(self.centralAttack)
    except RequestBlocked:
        return detected
    for wafvendor in self.checklist:
        self.log.info(""Checking for %s"" % wafvendor)
        if self.wafdetections[wafvendor](self):
            detected.append(wafvendor)
            if not findall:
                break
    self.knowledge[""wafname""] = detected
    return detected
",if not findall :,143
"def SessionId(self):
    """"""Returns the Session ID of the process""""""
    if self.Session.is_valid():
        process_space = self.get_process_address_space()
        if process_space:
            return obj.Object(
                ""_MM_SESSION_SPACE"", offset=self.Session, vm=process_space
            ).SessionId
    return obj.NoneObject(""Cannot find process session"")
",if process_space :,105
"def _convert_java_pattern_to_python(pattern):
    """"""Convert a replacement pattern from the Java-style `$5` to the Python-style `\\5`.""""""
    s = list(pattern)
    i = 0
    while i < len(s) - 1:
        c = s[i]
        if c == ""$"" and s[i + 1] in ""0123456789"":
            s[i] = ""\\""
        elif c == ""\\"" and s[i + 1] == ""$"":
            s[i] = """"
            i += 1
        i += 1
    return pattern[:0].join(s)
","elif c == ""\\"" and s [ i + 1 ] == ""$"" :",152
"def __init__(self, coverage):
    self.coverage = coverage
    self.config = self.coverage.config
    self.source_paths = set()
    if self.config.source:
        for src in self.config.source:
            if os.path.exists(src):
                if not self.config.relative_files:
                    src = files.canonical_filename(src)
                self.source_paths.add(src)
    self.packages = {}
    self.xml_out = None
",if os . path . exists ( src ) :,133
"def populate_vol_format(self):
    rhel6_file_whitelist = [""raw"", ""qcow2"", ""qed""]
    model = self.widget(""vol-format"").get_model()
    model.clear()
    formats = self.vol_class.formats
    if hasattr(self.vol_class, ""create_formats""):
        formats = getattr(self.vol_class, ""create_formats"")
    if self.vol_class == Storage.FileVolume and not self.conn.rhel6_defaults_caps():
        newfmts = []
        for f in rhel6_file_whitelist:
            if f in formats:
                newfmts.append(f)
        formats = newfmts
    for f in formats:
        model.append([f, f])
",if f in formats :,196
"def get_file_sources():
    global _file_sources
    if _file_sources is None:
        from galaxy.files import ConfiguredFileSources
        file_sources = None
        if os.path.exists(""file_sources.json""):
            file_sources_as_dict = None
            with open(""file_sources.json"", ""r"") as f:
                file_sources_as_dict = json.load(f)
            if file_sources_as_dict is not None:
                file_sources = ConfiguredFileSources.from_dict(file_sources_as_dict)
        if file_sources is None:
            ConfiguredFileSources.from_dict([])
        _file_sources = file_sources
    return _file_sources
","if os . path . exists ( ""file_sources.json"" ) :",196
"def _blend(x, y):  # pylint: disable=invalid-name
    """"""Implements the ""blend"" strategy for `deep_merge`.""""""
    if isinstance(x, (dict, OrderedDict)):
        if not isinstance(y, (dict, OrderedDict)):
            return y
        return _merge(x, y, recursion_func=_blend)
    if isinstance(x, (list, tuple)):
        if not isinstance(y, (list, tuple)):
            return y
        result = [_blend(*i) for i in zip(x, y)]
        if len(x) > len(y):
            result += x[len(y) :]
        elif len(x) < len(y):
            result += y[len(x) :]
        return result
    return y
",if len ( x ) > len ( y ) :,194
"def copy_dicts(dct):
    if ""_remote_data"" in dct:
        dsindex = dct[""_remote_data""][""_content""].dsindex
        newdct = dct.copy()
        newdct[""_remote_data""] = {""_content"": dsindex}
        return list(newdct.items())
    elif ""_data"" in dct:
        newdct = dct.copy()
        newdata = copy_dicts(dct[""_data""])
        if newdata:
            newdct[""_data""] = newdata
        return list(newdct.items())
    return None
",if newdata :,139
"def _import_epic_activity(self, project_data, taiga_epic, epic, options):
    offset = 0
    while True:
        activities = self._client.get(
            ""/projects/{}/epics/{}/activity"".format(
                project_data[""id""],
                epic[""id""],
            ),
            {""envelope"": ""true"", ""limit"": 300, ""offset"": offset},
        )
        offset += 300
        for activity in activities[""data""]:
            self._import_activity(taiga_epic, activity, options)
        if len(activities[""data""]) < 300:
            break
","if len ( activities [ ""data"" ] ) < 300 :",173
"def __get__(self, instance, instance_type=None):
    if instance:
        if self.att_name not in instance._obj_cache:
            rel_obj = self.get_obj(instance)
            if rel_obj:
                instance._obj_cache[self.att_name] = rel_obj
        return instance._obj_cache.get(self.att_name)
    return self
",if self . att_name not in instance . _obj_cache :,105
"def download_main(
    download, download_playlist, urls, playlist, output_dir, merge, info_only
):
    for url in urls:
        if url.startswith(""https://""):
            url = url[8:]
        if not url.startswith(""http://""):
            url = ""http://"" + url
        if playlist:
            download_playlist(
                url, output_dir=output_dir, merge=merge, info_only=info_only
            )
        else:
            download(url, output_dir=output_dir, merge=merge, info_only=info_only)
",if playlist :,155
"def _mksubs(self):
    self._subs = {}
    commit_dir = CommitDir(self, "".commit"")
    self._subs["".commit""] = commit_dir
    tag_dir = TagDir(self, "".tag"")
    self._subs["".tag""] = tag_dir
    for (name, sha) in git.list_refs():
        if name.startswith(""refs/heads/""):
            name = name[11:]
            date = git.rev_get_date(sha.encode(""hex""))
            n1 = BranchList(self, name, sha)
            n1.ctime = n1.mtime = date
            self._subs[name] = n1
","if name . startswith ( ""refs/heads/"" ) :",168
"def readAtOffset(self, offset, size, shortok=False):
    ret = b""""
    self.fd.seek(offset)
    while len(ret) != size:
        rlen = size - len(ret)
        x = self.fd.read(rlen)
        if x == b"""":
            if not shortok:
                return None
            return ret
        ret += x
    return ret
","if x == b"""" :",111
"def remove_indent(self):
    """"""Remove one tab-width of blanks from the previous token.""""""
    w = abs(self.tab_width)
    if self.result:
        s = self.result[-1]
        if s.isspace():
            self.result.pop()
            s = s.replace(""\t"", "" "" * w)
            if s.startswith(""\n""):
                s2 = s[1:]
                self.result.append(""\n"" + s2[:-w])
            else:
                self.result.append(s[:-w])
",if s . isspace ( ) :,151
"def flush(self, *args, **kwargs):
    with self._lock:
        self._last_updated = time.time()
        try:
            if kwargs.get(""in_place"", False):
                self._locked_flush_without_tempfile()
            else:
                mailbox.mbox.flush(self, *args, **kwargs)
        except OSError:
            if ""_create_temporary"" in traceback.format_exc():
                self._locked_flush_without_tempfile()
            else:
                raise
        self._last_updated = time.time()
","if ""_create_temporary"" in traceback . format_exc ( ) :",157
"def _collect_manual_intervention_nodes(pipeline_tree):
    for act in pipeline_tree[""activities""].values():
        if act[""type""] == ""SubProcess"":
            _collect_manual_intervention_nodes(act[""pipeline""])
        elif act[""component""][""code""] in MANUAL_INTERVENTION_COMP_CODES:
            manual_intervention_nodes.add(act[""id""])
","if act [ ""type"" ] == ""SubProcess"" :",105
"def banned():
    if request.endpoint == ""views.themes"":
        return
    if authed():
        user = get_current_user_attrs()
        team = get_current_team_attrs()
        if user and user.banned:
            return (
                render_template(
                    ""errors/403.html"", error=""You have been banned from this CTF""
                ),
                403,
            )
        if team and team.banned:
            return (
                render_template(
                    ""errors/403.html"",
                    error=""Your team has been banned from this CTF"",
                ),
                403,
            )
",if user and user . banned :,193
"def remove(self, values):
    if not isinstance(values, (list, tuple, set)):
        values = [values]
    for v in values:
        v = str(v)
        if isinstance(self._definition, dict):
            self._definition.pop(v, None)
        elif self._definition == ""ANY"":
            if v == ""ANY"":
                self._definition = []
        elif v in self._definition:
            self._definition.remove(v)
    if (
        self._value is not None
        and self._value not in self._definition
        and self._not_any()
    ):
        raise ConanException(bad_value_msg(self._name, self._value, self.values_range))
",elif v in self . _definition :,192
"def save(self, learner, file_name):
    """"""Save the model to location specified in file_name.""""""
    with open(file_name, ""wb"") as f:
        if hasattr(learner, ""inference_cache_""):
            # don't store the large inference cache!
            learner.inference_cache_, tmp = (None, learner.inference_cache_)
            pickle.dump(learner, f, -1)
            learner.inference_cache_ = tmp
        else:
            pickle.dump(learner, f, -1)
","if hasattr ( learner , ""inference_cache_"" ) :",137
"def __init__(self, exprs, savelist=False):
    super(ParseExpression, self).__init__(savelist)
    if isinstance(exprs, _generatorType):
        exprs = list(exprs)
    if isinstance(exprs, basestring):
        self.exprs = [ParserElement._literalStringClass(exprs)]
    elif isinstance(exprs, collections.Iterable):
        exprs = list(exprs)
        # if sequence of strings provided, wrap with Literal
        if all(isinstance(expr, basestring) for expr in exprs):
            exprs = map(ParserElement._literalStringClass, exprs)
        self.exprs = list(exprs)
    else:
        try:
            self.exprs = list(exprs)
        except TypeError:
            self.exprs = [exprs]
    self.callPreparse = False
","if all ( isinstance ( expr , basestring ) for expr in exprs ) :",199
"def find(self, back=False):
    flags = 0
    if back:
        flags = QTextDocument.FindBackward
    if self.csBox.isChecked():
        flags = flags | QTextDocument.FindCaseSensitively
    text = self.searchEdit.text()
    if not self.findMain(text, flags):
        if text in self.editBoxes[self.ind].toPlainText():
            cursor = self.editBoxes[self.ind].textCursor()
            if back:
                cursor.movePosition(QTextCursor.End)
            else:
                cursor.movePosition(QTextCursor.Start)
            self.editBoxes[self.ind].setTextCursor(cursor)
            self.findMain(text, flags)
",if back :,195
"def _load_storage(self):
    self._storage = {}
    for row in self(""SELECT object, resource, amount FROM storage""):
        ownerid = int(row[0])
        if ownerid in self._storage:
            self._storage[ownerid].append(row[1:])
        else:
            self._storage[ownerid] = [row[1:]]
",if ownerid in self . _storage :,94
"def parse_chunked(self, unreader):
    (size, rest) = self.parse_chunk_size(unreader)
    while size > 0:
        while size > len(rest):
            size -= len(rest)
            yield rest
            rest = unreader.read()
            if not rest:
                raise NoMoreData()
        yield rest[:size]
        # Remove \r\n after chunk
        rest = rest[size:]
        while len(rest) < 2:
            rest += unreader.read()
        if rest[:2] != b""\r\n"":
            raise ChunkMissingTerminator(rest[:2])
        (size, rest) = self.parse_chunk_size(unreader, data=rest[2:])
",if not rest :,197
"def _augment_batch_(self, batch, random_state, parents, hooks):
    for column in batch.columns:
        if column.name in [""keypoints"", ""bounding_boxes"", ""polygons"", ""line_strings""]:
            for i, cbaoi in enumerate(column.value):
                column.value[i] = cbaoi.clip_out_of_image_()
    return batch
","if column . name in [ ""keypoints"" , ""bounding_boxes"" , ""polygons"" , ""line_strings"" ] :",93
"def to_nim(self):
    if self.is_pointer == 2:
        s = ""cstringArray"" if self.type == ""GLchar"" else ""ptr pointer""
    else:
        s = self.type
        if self.is_pointer == 1:
            default = ""ptr "" + s
            s = self.NIM_POINTER_MAP.get(s, default)
    return s
",if self . is_pointer == 1 :,105
"def find(self, path):
    if os.path.isfile(path) or os.path.islink(path):
        self.num_files = self.num_files + 1
        if self.match_function(path):
            self.files.append(path)
    elif os.path.isdir(path):
        for content in os.listdir(path):
            file = os.path.join(path, content)
            if os.path.isfile(file) or os.path.islink(file):
                self.num_files = self.num_files + 1
                if self.match_function(file):
                    self.files.append(file)
            else:
                self.find(file)
",if os . path . isfile ( file ) or os . path . islink ( file ) :,192
"def remove(self, event):
    try:
        self._events_current_sweep.remove(event)
        if USE_DEBUG:
            assert event.in_sweep == True
            assert event.other.in_sweep == True
            event.in_sweep = False
            event.other.in_sweep = False
        return True
    except KeyError:
        if USE_DEBUG:
            assert event.in_sweep == False
            assert event.other.in_sweep == False
        return False
",if USE_DEBUG :,134
"def update_metadata(self):
    for attrname in dir(self):
        if attrname.startswith(""__""):
            continue
        attrvalue = getattr(self, attrname, None)
        if attrvalue == 0:
            continue
        if attrname == ""salt_version"":
            attrname = ""version""
        if hasattr(self.metadata, ""set_{0}"".format(attrname)):
            getattr(self.metadata, ""set_{0}"".format(attrname))(attrvalue)
        elif hasattr(self.metadata, attrname):
            try:
                setattr(self.metadata, attrname, attrvalue)
            except AttributeError:
                pass
","if attrname == ""salt_version"" :",173
"def _init_auxiliary_head(self, auxiliary_head):
    """"""Initialize ``auxiliary_head``""""""
    if auxiliary_head is not None:
        if isinstance(auxiliary_head, list):
            self.auxiliary_head = nn.ModuleList()
            for head_cfg in auxiliary_head:
                self.auxiliary_head.append(builder.build_head(head_cfg))
        else:
            self.auxiliary_head = builder.build_head(auxiliary_head)
","if isinstance ( auxiliary_head , list ) :",121
"def _str_param_list(self, name):
    out = []
    if self[name]:
        out += self._str_header(name)
        for param in self[name]:
            parts = []
            if param.name:
                parts.append(param.name)
            if param.type:
                parts.append(param.type)
            out += ["" : "".join(parts)]
            if param.desc and """".join(param.desc).strip():
                out += self._str_indent(param.desc)
        out += [""""]
    return out
",if param . type :,157
"def _set_handler(
    self, name, handle=None, obj=None, constructor_args=(), constructor_kwds={}
):
    if handle is None:
        handle = obj is not None
    if handle:
        handler_class = self.handler_classes[name]
        if obj is not None:
            newhandler = handler_class(obj)
        else:
            newhandler = handler_class(*constructor_args, **constructor_kwds)
    else:
        newhandler = None
    self._replace_handler(name, newhandler)
",if obj is not None :,137
"def _extract_subtitles(src):
    subtitles = {}
    for caption in try_get(src, lambda x: x[""captions""], list) or []:
        subtitle_url = url_or_none(caption.get(""uri""))
        if subtitle_url:
            lang = caption.get(""language"", ""deu"")
            subtitles.setdefault(lang, []).append(
                {
                    ""url"": subtitle_url,
                }
            )
    return subtitles
",if subtitle_url :,131
"def get_keys(struct, ignore_first_level=False):
    res = []
    if isinstance(struct, dict):
        if not ignore_first_level:
            keys = [x.split(""("")[0] for x in struct.keys()]
            res.extend(keys)
        for key in struct:
            if key in IGNORED_KEYS:
                logging.debug(""Ignored: %s: %s"", key, struct[key])
                continue
            res.extend(get_keys(struct[key], key in IGNORED_FIRST_LEVEL))
    elif isinstance(struct, list):
        for item in struct:
            res.extend(get_keys(item))
    return res
",if not ignore_first_level :,178
"def create_dir(path):
    curr_path = None
    for p in path:
        if curr_path is None:
            curr_path = os.path.abspath(p)
        else:
            curr_path = os.path.join(curr_path, p)
        if not os.path.exists(curr_path):
            os.mkdir(curr_path)
",if not os . path . exists ( curr_path ) :,100
"def dataToDumpFile(dumpFile, data):
    try:
        dumpFile.write(data)
        dumpFile.flush()
    except IOError as ex:
        if ""No space left"" in getUnicode(ex):
            errMsg = ""no space left on output device""
            logger.error(errMsg)
        elif ""Permission denied"" in getUnicode(ex):
            errMsg = ""permission denied when flushing dump data""
            logger.error(errMsg)
        else:
            errMsg = (
                ""error occurred when writing dump data to file ('%s')"" % getUnicode(ex)
            )
            logger.error(errMsg)
","elif ""Permission denied"" in getUnicode ( ex ) :",176
"def elements(self, top):
    res = []
    # try:
    #     string = ""== %s (%s)"" % (self.name,self.__class__)
    # except AttributeError:
    #     string = ""== (%s)"" % (self.__class__,)
    # print(string)
    for part in self.parts:
        if isinstance(part, Element):
            res.append(name_or_ref(part, top))
        else:
            if isinstance(part, Extension):
                res.append(part.base)
            res.extend(part.elements(top))
    return res
","if isinstance ( part , Element ) :",159
"def _parse_param_value(name, datatype, default):
    if datatype == ""bool"":
        if default.lower() == ""true"":
            return True
        elif default.lower() == ""false"":
            return False
        else:
            _s = ""{}: Invalid default value '{}' for bool parameter {}""
            raise SyntaxError(_s.format(self.name, default, p))
    elif datatype == ""int"":
        if type(default) == int:
            return default
        else:
            return int(default, 0)
    elif datatype == ""real"":
        if type(default) == float:
            return default
        else:
            return float(default)
    else:
        return str(default)
","elif default . lower ( ) == ""false"" :",191
"def dvmethod(c, dx, doAST=False):
    for m in c.get_methods():
        mx = dx.get_method(m)
        ms = DvMethod(mx)
        ms.process(doAST=doAST)
        if doAST:
            assert ms.get_ast() is not None
            assert isinstance(ms.get_ast(), dict)
            assert ""body"" in ms.get_ast()
        else:
            assert ms.get_source() is not None
",if doAST :,132
"def _repr_pretty_(self, p, cycle):
    if cycle:
        return ""{{...}""
    with p.group(2, ""{"", ""}""):
        p.breakable("""")
        for idx, key in enumerate(self._items):
            if idx:
                p.text("","")
                p.breakable()
            value = self._items[key]
            p.pretty(key)
            p.text("": "")
            if isinstance(value, bytes):
                value = trimmed_repr(value)
            p.pretty(value)
        p.breakable("""")
",if idx :,159
"def remove_rating(self, songs, librarian):
    count = len(songs)
    if count > 1 and config.getboolean(""browsers"", ""rating_confirm_multiple""):
        parent = qltk.get_menu_item_top_parent(self)
        dialog = ConfirmRateMultipleDialog(parent, _(""_Remove Rating""), count, None)
        if dialog.run() != Gtk.ResponseType.YES:
            return
    reset = []
    for song in songs:
        if ""~#rating"" in song:
            del song[""~#rating""]
            reset.append(song)
    librarian.changed(reset)
",if dialog . run ( ) != Gtk . ResponseType . YES :,159
"def get_or_create_place(self, place_name):
    ""Return the requested place object tuple-packed with a new indicator.""
    LOG.debug(""get_or_create_place: looking for: %s"", place_name)
    for place_handle in self.db.iter_place_handles():
        place = self.db.get_place_from_handle(place_handle)
        place_title = place_displayer.display(self.db, place)
        if place_title == place_name:
            return (0, place)
    place = Place()
    place.set_title(place_name)
    place.name = PlaceName(value=place_name)
    self.db.add_place(place, self.trans)
    return (1, place)
",if place_title == place_name :,193
"def _skip_trivial(constraint_data):
    if skip_trivial_constraints:
        if isinstance(constraint_data, LinearCanonicalRepn):
            if constraint_data.variables is None:
                return True
        else:
            if constraint_data.body.polynomial_degree() == 0:
                return True
    return False
","if isinstance ( constraint_data , LinearCanonicalRepn ) :",90
"def get_other(self, data, items):
    is_tuple = False
    if type(data) == tuple:
        data = list(data)
        is_tuple = True
    if type(data) == list:
        m_items = items.copy()
        for idx, item in enumerate(items):
            if item < 0:
                m_items[idx] = len(data) - abs(item)
        for i in sorted(set(m_items), reverse=True):
            if i < len(data) and i > -1:
                del data[i]
        if is_tuple:
            return tuple(data)
        else:
            return data
    else:
        return None
",if i < len ( data ) and i > - 1 :,191
"def test_case_insensitivity(self):
    with support.EnvironmentVarGuard() as env:
        env.set(""PYTHONCASEOK"", ""1"")
        if b""PYTHONCASEOK"" not in _bootstrap._os.environ:
            self.skipTest(""os.environ changes not reflected in "" ""_os.environ"")
        loader = self.find_module()
        self.assertTrue(hasattr(loader, ""load_module""))
","if b""PYTHONCASEOK"" not in _bootstrap . _os . environ :",103
"def field_spec(self):
    if self.field_spec_ is None:
        self.lazy_init_lock_.acquire()
        try:
            if self.field_spec_ is None:
                self.field_spec_ = FieldSpec()
        finally:
            self.lazy_init_lock_.release()
    return self.field_spec_
",if self . field_spec_ is None :,95
"def reduce(self, f, init):
    for x in range(self._idx, rt.count(self._w_array)):
        if rt.reduced_QMARK_(init):
            return rt.deref(init)
        init = f.invoke([init, rt.nth(self._w_array, rt.wrap(x))])
    return init
",if rt . reduced_QMARK_ ( init ) :,86
"def _find(event: E) -> None:
    # We first check values after the selected value, then all values.
    values = list(self.values)
    for value in values[self._selected_index + 1 :] + values:
        text = fragment_list_to_text(to_formatted_text(value[1])).lower()
        if text.startswith(event.data.lower()):
            self._selected_index = self.values.index(value)
            return
",if text . startswith ( event . data . lower ( ) ) :,118
"def check_permissions():
    if platform_os() != ""Windows"":
        if getuid() == 0:
            print(localization.lang_check_permissions[""permissions_granted""])
        else:
            print(localization.lang_check_permissions[""permissions_denied""])
            exit()
    else:
        print(localization.lang_check_permissions[""windows_warning""])
        exit()
",if getuid ( ) == 0 :,101
"def _ProcessName(self, name, dependencies):
    """"""Retrieve a module name from a node name.""""""
    module_name, dot, base_name = name.rpartition(""."")
    if dot:
        if module_name:
            if module_name in dependencies:
                dependencies[module_name].add(base_name)
            else:
                dependencies[module_name] = {base_name}
        else:
            # If we have a relative import that did not get qualified (usually due
            # to an empty package_name), don't insert module_name='' into the
            # dependencies; we get a better error message if we filter it out here
            # and fail later on.
            logging.warning(""Empty package name: %s"", name)
",if module_name in dependencies :,196
"def _load_db(self):
    try:
        with open(self.db) as db:
            content = db.read(8)
            db.seek(0)
            if content == (""Salted__""):
                data = StringIO()
                if self.encryptor:
                    self.encryptor.decrypt(db, data)
                else:
                    raise EncryptionError(
                        ""Encrpyted credential storage: {}"".format(self.db)
                    )
                return json.loads(data.getvalue())
            else:
                return json.load(db)
    except:
        return {""creds"": []}
",if self . encryptor :,187
"def _parse(self, stream, context):
    obj = []
    try:
        context_for_subcon = context
        if self.subcon.conflags & self.FLAG_COPY_CONTEXT:
            context_for_subcon = context.__copy__()
        while True:
            subobj = self.subcon._parse(stream, context_for_subcon)
            if self.predicate(subobj, context):
                break
            obj.append(subobj)
    except ConstructError as ex:
        raise ArrayError(""missing terminator"", ex)
    return obj
","if self . predicate ( subobj , context ) :",150
"def is_active_for_user(self, user):
    is_active = super(AbstractUserFlag, self).is_active_for_user(user)
    if is_active:
        return is_active
    user_ids = self._get_user_ids()
    if hasattr(user, ""pk"") and user.pk in user_ids:
        return True
    if hasattr(user, ""groups""):
        group_ids = self._get_group_ids()
        if group_ids:
            user_groups = set(user.groups.all().values_list(""pk"", flat=True))
            if group_ids.intersection(user_groups):
                return True
    return None
",if group_ids . intersection ( user_groups ) :,175
"def lookup_member(self, member_name):
    document_choices = self.choices or []
    for document_choice in document_choices:
        doc_and_subclasses = [document_choice] + document_choice.__subclasses__()
        for doc_type in doc_and_subclasses:
            field = doc_type._fields.get(member_name)
            if field:
                return field
",if field :,101
"def apply(self, db, person):
    families = person.get_parent_family_handle_list()
    if families == []:
        return True
    for family_handle in person.get_parent_family_handle_list():
        family = db.get_family_from_handle(family_handle)
        if family:
            father_handle = family.get_father_handle()
            mother_handle = family.get_mother_handle()
            if not father_handle:
                return True
            if not mother_handle:
                return True
    return False
",if not mother_handle :,157
"def init_weights(self):
    for m in self.modules():
        if isinstance(m, nn.Linear):
            normal_init(m, std=0.01)
        if isinstance(m, nn.Conv3d):
            xavier_init(m, distribution=""uniform"")
        if isinstance(m, nn.BatchNorm3d):
            constant_init(m, 1)
","if isinstance ( m , nn . BatchNorm3d ) :",99
"def _update_learning_params(self):
    model = self.model
    hparams = self.hparams
    fd = self.runner.feed_dict
    step_num = self.step_num
    if hparams.model_type == ""resnet_tf"":
        if step_num < hparams.lrn_step:
            lrn_rate = hparams.mom_lrn
        elif step_num < 30000:
            lrn_rate = hparams.mom_lrn / 10
        elif step_num < 35000:
            lrn_rate = hparams.mom_lrn / 100
        else:
            lrn_rate = hparams.mom_lrn / 1000
        fd[model.lrn_rate] = lrn_rate
",if step_num < hparams . lrn_step :,190
"def token_producer(source):
    token = source.read_uint8()
    while token is not None:
        if is_push_data_token(token):
            yield DataToken(read_data(token, source))
        elif is_small_integer(token):
            yield SmallIntegerToken(read_small_integer(token))
        else:
            yield Token(token)
        token = source.read_uint8()
",elif is_small_integer ( token ) :,113
"def user_info(oicsrv, userdb, sub, client_id="""", user_info_claims=None):
    identity = userdb[sub]
    if user_info_claims:
        result = {}
        for key, restr in user_info_claims[""claims""].items():
            try:
                result[key] = identity[key]
            except KeyError:
                if restr == {""essential"": True}:
                    raise Exception(""Missing property '%s'"" % key)
    else:
        result = identity
    return OpenIDSchema(**result)
","if restr == { ""essential"" : True } :",147
"def _helpSlot(self, *args):
    help_text = ""Filters are applied to packets in both direction.\n\n""
    filter_nb = 0
    for filter in self._filters:
        help_text += ""{}: {}"".format(filter[""name""], filter[""description""])
        filter_nb += 1
        if len(self._filters) != filter_nb:
            help_text += ""\n\n""
    QtWidgets.QMessageBox.information(self, ""Help for filters"", help_text)
",if len ( self . _filters ) != filter_nb :,124
"def find_user_theme(self, name: str) -> Theme:
    """"""Find a theme named as *name* from latex_theme_path.""""""
    for theme_path in self.theme_paths:
        config_path = path.join(theme_path, name, ""theme.conf"")
        if path.isfile(config_path):
            try:
                return UserTheme(name, config_path)
            except ThemeError as exc:
                logger.warning(exc)
    return None
",if path . isfile ( config_path ) :,128
"def decompress(self, value):
    if value:
        if type(value) == PhoneNumber:
            if value.country_code and value.national_number:
                return [
                    ""+%d"" % value.country_code,
                    national_significant_number(value),
                ]
        else:
            return value.split(""."")
    return [None, """"]
",if type ( value ) == PhoneNumber :,111
"def update_prevdoc_status(self, flag):
    for quotation in list(set([d.prevdoc_docname for d in self.get(""items"")])):
        if quotation:
            doc = frappe.get_doc(""Quotation"", quotation)
            if doc.docstatus == 2:
                frappe.throw(_(""Quotation {0} is cancelled"").format(quotation))
            doc.set_status(update=True)
            doc.update_opportunity()
",if quotation :,127
"def map(item):
    if item.deleted:
        return
    exploration = exp_fetchers.get_exploration_from_model(item)
    for state_name, state in exploration.states.items():
        hints_length = len(state.interaction.hints)
        if hints_length > 0:
            exp_and_state_key = ""%s %s"" % (item.id, state_name.encode(""utf-8""))
            yield (python_utils.UNICODE(hints_length), exp_and_state_key)
",if hints_length > 0 :,136
"def _selected_machines(self, virtual_machines):
    selected_machines = []
    for machine in virtual_machines:
        if self._args.host and self._args.host == machine.name:
            selected_machines.append(machine)
        if self.tags and self._tags_match(machine.tags, self.tags):
            selected_machines.append(machine)
        if self.locations and machine.location in self.locations:
            selected_machines.append(machine)
    return selected_machines
","if self . tags and self . _tags_match ( machine . tags , self . tags ) :",129
"def _ripple_trim_compositors_move(self, delta):
    comp_ids = self.multi_data.moved_compositors_destroy_ids
    tracks_compositors = _get_tracks_compositors_list()
    track_moved = self.multi_data.track_affected
    for i in range(1, len(current_sequence().tracks) - 1):
        if not track_moved[i - 1]:
            continue
        track_comps = tracks_compositors[i - 1]
        for comp in track_comps:
            if comp.destroy_id in comp_ids:
                comp.move(delta)
",if comp . destroy_id in comp_ids :,158
"def stream_docker_log(log_stream):
    async for line in log_stream:
        if ""stream"" in line and line[""stream""].strip():
            logger.debug(line[""stream""].strip())
        elif ""status"" in line:
            logger.debug(line[""status""].strip())
        elif ""error"" in line:
            logger.error(line[""error""].strip())
            raise DockerBuildError
","elif ""status"" in line :",108
"def create_keyfile(self, keyfile, size=64, force=False):
    if force or not os.path.exists(keyfile):
        keypath = os.path.dirname(keyfile)
        if not os.path.exists(keypath):
            os.makedirs(keypath)
        subprocess.run(
            [""dd"", ""if=/dev/random"", f""of={keyfile}"", f""bs={size}"", ""count=1""],
            check=True,
            stdout=subprocess.DEVNULL,
            stderr=subprocess.DEVNULL,
        )
",if not os . path . exists ( keypath ) :,138
"def calc(self, arg):
    op = arg[""op""]
    if op == ""C"":
        self.clear()
        return str(self.current)
    num = decimal.Decimal(arg[""num""])
    if self.op:
        if self.op == ""+"":
            self.current += num
        elif self.op == ""-"":
            self.current -= num
        elif self.op == ""*"":
            self.current *= num
        elif self.op == ""/"":
            self.current /= num
        self.op = op
    else:
        self.op = op
        self.current = num
    res = str(self.current)
    if op == ""="":
        self.clear()
    return res
","elif self . op == ""*"" :",187
"def chop(expr, delta=10.0 ** (-10.0)):
    if isinstance(expr, Real):
        if -delta < expr.get_float_value() < delta:
            return Integer(0)
    elif isinstance(expr, Complex) and expr.is_inexact():
        real, imag = expr.real, expr.imag
        if -delta < real.get_float_value() < delta:
            real = Integer(0)
        if -delta < imag.get_float_value() < delta:
            imag = Integer(0)
        return Complex(real, imag)
    elif isinstance(expr, Expression):
        return Expression(chop(expr.head), *[chop(leaf) for leaf in expr.leaves])
    return expr
",if - delta < real . get_float_value ( ) < delta :,186
"def get_file_sources():
    global _file_sources
    if _file_sources is None:
        from galaxy.files import ConfiguredFileSources
        file_sources = None
        if os.path.exists(""file_sources.json""):
            file_sources_as_dict = None
            with open(""file_sources.json"", ""r"") as f:
                file_sources_as_dict = json.load(f)
            if file_sources_as_dict is not None:
                file_sources = ConfiguredFileSources.from_dict(file_sources_as_dict)
        if file_sources is None:
            ConfiguredFileSources.from_dict([])
        _file_sources = file_sources
    return _file_sources
",if file_sources_as_dict is not None :,196
"def _get_sort_map(tags):
    """"""See TAG_TO_SORT""""""
    tts = {}
    for name, tag in tags.items():
        if tag.has_sort:
            if tag.user:
                tts[name] = ""%ssort"" % name
            if tag.internal:
                tts[""~%s"" % name] = ""~%ssort"" % name
    return tts
",if tag . has_sort :,111
"def __init__(self, **kwargs):
    if self.name is None:
        raise RuntimeError(""RenderPrimitive cannot be used directly"")
    self.option_values = {}
    for key, val in kwargs.items():
        if not key in self.options:
            raise ValueError(
                ""primitive `{0}' has no option `{1}'"".format(self.name, key)
            )
        self.option_values[key] = val
    # set up defaults
    for name, (description, default) in self.options.items():
        if not name in self.option_values:
            self.option_values[name] = default
",if not key in self . options :,162
"def modify_bottle_params(self, output_stride=None):
    if output_stride is not None and output_stride % 2 != 0:
        raise Exception(""output stride must to be even number"")
    if output_stride is None:
        return
    else:
        stride = 2
        for i, _cfg in enumerate(self.cfg):
            stride = stride * _cfg[-1]
            if stride > output_stride:
                s = 1
                self.cfg[i][-1] = s
",if stride > output_stride :,134
"def do_query(data, q):
    ret = []
    if not q:
        return ret
    qkey = q[0]
    for key, value in iterate(data):
        if len(q) == 1:
            if key == qkey:
                ret.append(value)
            elif is_iterable(value):
                ret.extend(do_query(value, q))
        else:
            if not is_iterable(value):
                continue
            if key == qkey:
                ret.extend(do_query(value, q[1:]))
            else:
                ret.extend(do_query(value, q))
    return ret
",elif is_iterable ( value ) :,185
"def make_shares(self, plaintext):
    share_arrays = []
    for i, p in enumerate(plaintext):
        share_array = self.make_byte_shares(p)
        for sa in share_array:
            if i == 0:
                share_arrays.append(array.array(""H""))
            current_share_array = sa
            current_share_array.append(sa)
    return share_arrays
",if i == 0 :,112
"def populate(self, item):
    # log.message('populate: %s', item)
    path = self.getItemPath(item)
    # log.message('populate: path=%s', path)
    value = self.getValue(path)
    for name in sorted(value.__dict__.keys()):
        if name[:2] == ""__"" and name[-2:] == ""__"":
            continue
        child = getattr(value, name, None)
        if hasattr(child, ""__dict__""):
            item.addChild(name, True)
        else:
            item.addChild(name, False)
","if name [ : 2 ] == ""__"" and name [ - 2 : ] == ""__"" :",151
"def __repr__(self):
    try:
        if self._semlock._is_mine():
            name = current_process().name
            if threading.current_thread().name != ""MainThread"":
                name += ""|"" + threading.current_thread().name
        elif self._semlock._get_value() == 1:
            name = ""None""
        elif self._semlock._count() > 0:
            name = ""SomeOtherThread""
        else:
            name = ""SomeOtherProcess""
    except Exception:
        name = ""unknown""
    return ""<Lock(owner=%s)>"" % name
","if threading . current_thread ( ) . name != ""MainThread"" :",160
"def buffer(self, lines, scroll_end=True, scroll_if_editing=False):
    ""Add data to be displayed in the buffer.""
    self.values.extend(lines)
    if scroll_end:
        if not self.editing:
            self.start_display_at = len(self.values) - len(self._my_widgets)
        elif scroll_if_editing:
            self.start_display_at = len(self.values) - len(self._my_widgets)
",if not self . editing :,124
"def warehouses(self) -> tuple:
    from ..repositories import WarehouseBaseRepo
    repos = dict()
    for dep in chain(self.dependencies, [self]):
        if dep.repo is None:
            continue
        if not isinstance(dep.repo, WarehouseBaseRepo):
            continue
        for repo in dep.repo.repos:
            if repo.from_config:
                continue
            repos[repo.name] = repo
    return tuple(repos.values())
","if not isinstance ( dep . repo , WarehouseBaseRepo ) :",124
"def _apply_flag_attrs(src_flag, dest_flag):
    # Use a baseline flag def to get default values for empty data.
    baseline_flag = FlagDef("""", {}, None)
    for name in dir(src_flag):
        if name[:1] == ""_"":
            continue
        dest_val = getattr(dest_flag, name, None)
        baseline_val = getattr(baseline_flag, name, None)
        if dest_val == baseline_val:
            setattr(dest_flag, name, getattr(src_flag, name))
","if name [ : 1 ] == ""_"" :",137
"def out(parent, attr, indent=0):
    val = getattr(parent, attr)
    prefix = ""%s%s:"" % ("" "" * indent, attr.replace(""_"", ""-""))
    if val is None:
        cli.out(prefix)
    else:
        if attr == ""choices"":
            val = [flag_util.encode_flag_val(c.value) for c in val]
        cli.out(""%s %s"" % (prefix, flag_util.encode_flag_val(val)))
","if attr == ""choices"" :",125
"def add_cand_to_check(cands):
    for cand in cands:
        x = cand.creator
        if x is None:
            continue
        if x not in fan_out:
            # `len(fan_out)` is in order to avoid comparing `x`
            heapq.heappush(cand_funcs, (-x.rank, len(fan_out), x))
        fan_out[x] += 1
",if x not in fan_out :,111
"def task_tree_lines(task=None):
    if task is None:
        task = current_root_task()
    rendered_children = []
    nurseries = list(task.child_nurseries)
    while nurseries:
        nursery = nurseries.pop()
        nursery_children = _rendered_nursery_children(nursery)
        if rendered_children:
            nested = _render_subtree(""(nested nursery)"", rendered_children)
            nursery_children.append(nested)
        rendered_children = nursery_children
    return _render_subtree(task.name, rendered_children)
",if rendered_children :,172
"def lock_workspace(build_dir):
    _BUILDING_LOCK_FILE = "".blade.building.lock""
    lock_file_fd, ret_code = lock_file(os.path.join(build_dir, _BUILDING_LOCK_FILE))
    if lock_file_fd == -1:
        if ret_code == errno.EAGAIN:
            console.fatal(""There is already an active building in current workspace."")
        else:
            console.fatal(""Lock exception, please try it later."")
    return lock_file_fd
",if ret_code == errno . EAGAIN :,136
"def test_list(self):
    self._create_locations()
    response = self.client.get(self.geojson_boxedlocation_list_url)
    self.assertEqual(response.status_code, 200)
    self.assertEqual(len(response.data[""features""]), 2)
    for feature in response.data[""features""]:
        self.assertIn(""bbox"", feature)
        fid = feature[""id""]
        if fid == 1:
            self.assertEqual(feature[""bbox""], self.bl1.bbox_geometry.extent)
        elif fid == 2:
            self.assertEqual(feature[""bbox""], self.bl2.bbox_geometry.extent)
        else:
            self.fail(""Unexpected id: {0}"".format(fid))
    BoxedLocation.objects.all().delete()
",elif fid == 2 :,196
"def result():
    # ""global"" does not work here...
    R, V = rays, virtual_rays
    if V is not None:
        if normalize:
            V = normalize_rays(V, lattice)
        if check:
            R = PointCollection(V, lattice)
            V = PointCollection(V, lattice)
            d = lattice.dimension()
            if len(V) != d - R.dim() or (R + V).dim() != d:
                raise ValueError(
                    ""virtual rays must be linearly ""
                    ""independent and with other rays span the ambient space.""
                )
    return RationalPolyhedralFan(cones, R, lattice, is_complete, V)
",if len ( V ) != d - R . dim ( ) or ( R + V ) . dim ( ) != d :,194
"def search_host(self, search_string):
    results = []
    for host_entry in self.config_data:
        if host_entry.get(""type"") != ""entry"":
            continue
        if host_entry.get(""host"") == ""*"":
            continue
        searchable_information = host_entry.get(""host"")
        for key, value in six.iteritems(host_entry.get(""options"")):
            if isinstance(value, list):
                value = "" "".join(value)
            if isinstance(value, int):
                value = str(value)
            searchable_information += "" "" + value
        if search_string in searchable_information:
            results.append(host_entry)
    return results
","if isinstance ( value , list ) :",191
"def test_async_iterator(app):
    async with new_stream(app) as stream:
        for i in range(100):
            await stream.channel.deliver(message(key=i, value=i))
        received = 0
        async for value in stream:
            assert value == received
            received += 1
            if received >= 100:
                break
        assert await channel_empty(stream.channel)
",if received >= 100 :,113
"def has_google_credentials():
    global _HAS_GOOGLE_CREDENTIALS
    if _HAS_GOOGLE_CREDENTIALS is None:
        provider = Provider(""google"")
        if provider.get_access_key() is None or provider.get_secret_key() is None:
            _HAS_GOOGLE_CREDENTIALS = False
        else:
            _HAS_GOOGLE_CREDENTIALS = True
    return _HAS_GOOGLE_CREDENTIALS
",if provider . get_access_key ( ) is None or provider . get_secret_key ( ) is None :,102
"def __cmp__(self, other):
    if isinstance(other, date) or isinstance(other, datetime):
        a = self._d.getTime()
        b = other._d.getTime()
        if a < b:
            return -1
        elif a == b:
            return 0
    else:
        raise TypeError(""expected date or datetime object"")
    return 1
",elif a == b :,98
"def validate_weight(self, weight):
    try:
        add_acl_to_obj(self.context[""user_acl""], self.category)
    except AttributeError:
        return weight  # don't validate weight further if category failed
    if weight > self.category.acl.get(""can_pin_threads"", 0):
        if weight == 2:
            raise ValidationError(
                _(
                    ""You don't have permission to pin threads globally ""
                    ""in this category.""
                )
            )
        else:
            raise ValidationError(
                _(""You don't have permission to pin threads in this category."")
            )
    return weight
",if weight == 2 :,177
"def effective(line):
    for b in line:
        if not b.cond:
            return
        else:
            try:
                val = 5
                if val:
                    if b.ignore:
                        b.ignore -= 1
                    else:
                        return (b, True)
            except:
                return (b, False)
    return
",if val :,118
"def wheelEvent(self, event):
    """"""Handle a wheel event.""""""
    if QtCore.Qt.ControlModifier & event.modifiers():
        d = {""c"": self.leo_c}
        if isQt5:
            point = event.angleDelta()
            delta = point.y() or point.x()
        else:
            delta = event.delta()
        if delta < 0:
            zoom_out(d)
        else:
            zoom_in(d)
        event.accept()
        return
    QtWidgets.QTextBrowser.wheelEvent(self, event)
",if delta < 0 :,160
"def test_evname_in_mp_events_testcases():
    ok = True
    for evname in ins.mp_events:
        if evname == ""version"":
            continue
        for i, args in enumerate(ins.mp_events[evname][""test_cases""]):
            if evname != args[0]:
                msg = ""Error, for evname %s the testase #%d does not match evname""
                print(msg % (evname, i))
                ok = False
    if ok:
        print(""test_evname_in_mp_events_testcases: passed"")
",if evname != args [ 0 ] :,156
"def check_database():
    if len(EmailAddress.objects.all()) > 0:
        print(
            ""Are you sure you want to wipe the existing development database and reseed it? (Y/N)""
        )
        if raw_input().lower() == ""y"":
            destroy_database()
        else:
            return False
    else:
        return True
","if raw_input ( ) . lower ( ) == ""y"" :",97
"def _get_requested_databases(self):
    """"""Returns a list of databases requested, not including ignored dbs""""""
    requested_databases = []
    if (self._requested_namespaces is not None) and (self._requested_namespaces != []):
        for requested_namespace in self._requested_namespaces:
            if requested_namespace[0] is ""*"":
                return []
            elif requested_namespace[0] not in IGNORE_DBS:
                requested_databases.append(requested_namespace[0])
    return requested_databases
",elif requested_namespace [ 0 ] not in IGNORE_DBS :,131
"def decorated(self, *args, **kwargs):
    start_time = time.perf_counter()
    stderr = """"
    saved_exception = None
    try:
        yield from fn(self, *args, **kwargs)
    except GitSavvyError as e:
        stderr = e.stderr
        saved_exception = e
    finally:
        end_time = time.perf_counter()
        util.debug.log_git(args, None, ""<SNIP>"", stderr, end_time - start_time)
        if saved_exception:
            raise saved_exception from None
",if saved_exception :,146
"def is_suppressed_warning(
    type: str, subtype: str, suppress_warnings: List[str]
) -> bool:
    """"""Check the warning is suppressed or not.""""""
    if type is None:
        return False
    for warning_type in suppress_warnings:
        if ""."" in warning_type:
            target, subtarget = warning_type.split(""."", 1)
        else:
            target, subtarget = warning_type, None
        if target == type:
            if (
                subtype is None
                or subtarget is None
                or subtarget == subtype
                or subtarget == ""*""
            ):
                return True
    return False
",if target == type :,178
"def talk(self, words):
    if self.writeSentence(words) == 0:
        return
    r = []
    while 1:
        i = self.readSentence()
        if len(i) == 0:
            continue
        reply = i[0]
        attrs = {}
        for w in i[1:]:
            j = w.find(""="", 1)
            if j == -1:
                attrs[w] = """"
            else:
                attrs[w[:j]] = w[j + 1 :]
        r.append((reply, attrs))
        if reply == ""!done"":
            return r
","if reply == ""!done"" :",169
"def encrypt(self, plaintext):
    encrypted = []
    for p in _string_to_bytes(plaintext):
        if len(self._remaining_block) == 0:
            self._remaining_block = self._aes.encrypt(self._last_precipherblock)
            self._last_precipherblock = []
        precipherbyte = self._remaining_block.pop(0)
        self._last_precipherblock.append(precipherbyte)
        cipherbyte = p ^ precipherbyte
        encrypted.append(cipherbyte)
    return _bytes_to_string(encrypted)
",if len ( self . _remaining_block ) == 0 :,146
"def find_symbol(self, r, globally=False):
    query = self.view.substr(self.view.word(r))
    fname = self.view.file_name().replace(""\\"", ""/"")
    locations = self.view.window().lookup_symbol_in_index(query)
    if not locations:
        return
    try:
        if not globally:
            location = [hit[2] for hit in locations if fname.endswith(hit[1])][0]
            return location[0] - 1, location[1] - 1
        else:
            # TODO: There might be many symbols with the same name.
            return locations[0]
    except IndexError:
        return
",if not globally :,172
"def __getslice__(self, i, j):
    try:
        if j == sys.maxint:
            # handle the case where the right bound is unspecified
            j = len(self)
        if i < 0 or j < 0:
            raise dns.exception.FormError
        # If it's not an empty slice, access left and right bounds
        # to make sure they're valid
        if i != j:
            super(WireData, self).__getitem__(i)
            super(WireData, self).__getitem__(j - 1)
        return WireData(super(WireData, self).__getslice__(i, j))
    except IndexError:
        raise dns.exception.FormError
",if j == sys . maxint :,181
"def main():
    r = redis.StrictRedis()
    curr_memory = prev_memory = r.info()[""used_memory""]
    while True:
        if prev_memory != curr_memory:
            print(
                ""Delta Memory : %d, Total Memory : %d""
                % ((curr_memory - prev_memory), curr_memory)
            )
        time.sleep(1)
        prev_memory = curr_memory
        curr_memory = r.info()[""used_memory""]
",if prev_memory != curr_memory :,130
"def _visit(self, func):
    fname = func[0]
    if fname in self._flags:
        if self._flags[fname] == 1:
            logger.critical(""Fatal error! network ins not Dag."")
            import sys
            sys.exit(-1)
        else:
            return
    else:
        if fname not in self._flags:
            self._flags[fname] = 1
        for output in func[3]:
            for f in self._orig:
                for input in f[2]:
                    if output == input:
                        self._visit(f)
    self._flags[fname] = 2
    self._sorted.insert(0, func)
",if fname not in self . _flags :,188
"def urls(self, version=None):
    """"""Returns all URLS that are mapped to this interface""""""
    urls = []
    for _base_url, routes in self.api.http.routes.items():
        for url, methods in routes.items():
            for _method, versions in methods.items():
                for interface_version, interface in versions.items():
                    if interface_version == version and interface == self:
                        if not url in urls:
                            urls.append(
                                (""/v{0}"".format(version) if version else """") + url
                            )
    return urls
",if interface_version == version and interface == self :,169
"def _handle_data(self, text):
    if self._translate:
        if not text.startswith(""gtk-""):
            self._data.append(text)
        else:
            self._translate = False
            self._data = []
            self._comments = []
","if not text . startswith ( ""gtk-"" ) :",74
"def set_dir_modes(self, dirname, mode):
    if not self.is_chmod_supported():
        return
    for dirpath, dirnames, fnames in os.walk(dirname):
        if os.path.islink(dirpath):
            continue
        log.info(""changing mode of %s to %o"", dirpath, mode)
        if not self.dry_run:
            os.chmod(dirpath, mode)
",if os . path . islink ( dirpath ) :,105
"def language(self):
    if self.lang_data:
        lang_data = [s if s != ""None"" else None for s in self.lang_data]
        if lang_data[0]:
            return Language(lang_data[0], country=lang_data[1], script=lang_data[2])
",if lang_data [ 0 ] :,80
"def _addItemToLayout(self, sample, label):
    col = self.layout.columnCount()
    row = self.layout.rowCount()
    if row:
        row -= 1
    nCol = self.columnCount * 2
    # FIRST ROW FULL
    if col == nCol:
        for col in range(0, nCol, 2):
            # FIND RIGHT COLUMN
            if not self.layout.itemAt(row, col):
                break
        if col + 2 == nCol:
            # MAKE NEW ROW
            col = 0
            row += 1
    self.layout.addItem(sample, row, col)
    self.layout.addItem(label, row, col + 1)
","if not self . layout . itemAt ( row , col ) :",185
"def align_comments(tlist):
    tidx, token = tlist.token_next_by(i=sql.Comment)
    while token:
        pidx, prev_ = tlist.token_prev(tidx)
        if isinstance(prev_, sql.TokenList):
            tlist.group_tokens(sql.TokenList, pidx, tidx, extend=True)
            tidx = pidx
        tidx, token = tlist.token_next_by(i=sql.Comment, idx=tidx)
","if isinstance ( prev_ , sql . TokenList ) :",130
"def hook_GetVariable(ql, address, params):
    if params[""VariableName""] in ql.env:
        var = ql.env[params[""VariableName""]]
        read_len = read_int64(ql, params[""DataSize""])
        if params[""Attributes""] != 0:
            write_int64(ql, params[""Attributes""], 0)
        write_int64(ql, params[""DataSize""], len(var))
        if read_len < len(var):
            return EFI_BUFFER_TOO_SMALL
        if params[""Data""] != 0:
            ql.mem.write(params[""Data""], var)
        return EFI_SUCCESS
    return EFI_NOT_FOUND
",if read_len < len ( var ) :,177
"def _PromptMySQL(self, config):
    """"""Prompts the MySQL configuration, retrying if the configuration is invalid.""""""
    while True:
        self._PromptMySQLOnce(config)
        if self._CheckMySQLConnection():
            print(""Successfully connected to MySQL with the given configuration."")
            return
        else:
            print(""Error: Could not connect to MySQL with the given configuration."")
            retry = RetryBoolQuestion(""Do you want to retry MySQL configuration?"", True)
            if not retry:
                raise ConfigInitError()
",if not retry :,136
"def split_long_line_with_indent(line, max_per_line, indent):
    """"""Split the `line` so that it doesn't go over `max_per_line` and adds `indent` to new lines.""""""
    words = line.split("" "")
    lines = []
    current_line = words[0]
    for word in words[1:]:
        if len(f""{current_line} {word}"") > max_per_line:
            lines.append(current_line)
            current_line = "" "" * indent + word
        else:
            current_line = f""{current_line} {word}""
    lines.append(current_line)
    return ""\n"".join(lines)
","if len ( f""{current_line} {word}"" ) > max_per_line :",176
"def gen_cli(docs_dir):
    with open(os.path.join(docs_dir, ""CLI_template.md""), ""r"") as cli_temp_file:
        temp_lines = cli_temp_file.readlines()
    lines = []
    for line in temp_lines:
        matched = re.match(r""{onnx-tf.*}"", line)
        if matched:
            command = matched.string.strip()[1:-1]
            output = subprocess.check_output(command.split("" "")).decode(""UTF-8"")
            lines.append(output)
        else:
            lines.append(line)
    with open(os.path.join(docs_dir, ""CLI.md""), ""w"") as cli_file:
        cli_file.writelines(lines)
",if matched :,200
"def read(self, size=None):
    if size == 0:
        return """"
    data = list()
    while size is None or size > 0:
        line = self.readline(size or -1)
        if not line:
            break
        if size is not None:
            size -= len(line)
        data.append(line)
    return """".join(data)
",if size is not None :,101
"def _get_format_and_pattern(file_path):
    file_path = Path(file_path)
    with file_path.open() as f:
        first_line = f.readline().strip()
        match = re.match(r""format *: *(.+)"", first_line)
        if match is None:
            return ""gztar"", first_line, 1
        return match.group(1), f.readline().strip(), 2
",if match is None :,111
"def remove_old_snapshot(install_dir):
    logging.info(""Removing any old files in {}"".format(install_dir))
    for file in glob.glob(""{}/*"".format(install_dir)):
        try:
            if os.path.isfile(file):
                os.unlink(file)
            elif os.path.isdir(file):
                shutil.rmtree(file)
        except Exception as error:
            logging.error(""Error: {}"".format(error))
            sys.exit(1)
",elif os . path . isdir ( file ) :,133
"def _test_forever(self, tests):
    while True:
        for test_name in tests:
            yield test_name
            if self.bad:
                return
            if self.ns.fail_env_changed and self.environment_changed:
                return
",if self . ns . fail_env_changed and self . environment_changed :,76
"def _swig_extract_dependency_files(self, src):
    dep = []
    for line in open(src):
        if line.startswith(""#include"") or line.startswith(""%include""):
            line = line.split("" "")[1].strip(""""""'""\r\n"""""")
            if not (""<"" in line or line in dep):
                dep.append(line)
    return [i for i in dep if os.path.exists(i)]
","if not ( ""<"" in line or line in dep ) :",111
"def update_service_key(kid, name=None, metadata=None):
    try:
        with db_transaction():
            key = db_for_update(ServiceKey.select().where(ServiceKey.kid == kid)).get()
            if name is not None:
                key.name = name
            if metadata is not None:
                key.metadata.update(metadata)
            key.save()
    except ServiceKey.DoesNotExist:
        raise ServiceKeyDoesNotExist
",if name is not None :,127
"def range(self, dimension, data_range=True, dimension_range=True):
    if self.nodes and dimension in self.nodes.dimensions():
        node_range = self.nodes.range(dimension, data_range, dimension_range)
        if self._edgepaths:
            path_range = self._edgepaths.range(dimension, data_range, dimension_range)
            return max_range([node_range, path_range])
        return node_range
    return super(Graph, self).range(dimension, data_range, dimension_range)
",if self . _edgepaths :,137
"def handler(chan, host, port):
    sock = socket()
    try:
        sock.connect((host, port))
    except Exception as e:
        if verbose == True:
            print(e)
        return
    while True:
        r, w, x = select.select([sock, chan], [], [])
        if sock in r:
            data = sock.recv(1024)
            if len(data) == 0:
                break
            chan.send(data)
        if chan in r:
            data = chan.recv(1024)
            if len(data) == 0:
                break
            sock.send(data)
    chan.close()
    sock.close()
",if len ( data ) == 0 :,190
"def output_layer(self, features, **kwargs):
    """"""Project features to the vocabulary size.""""""
    if self.adaptive_softmax is None:
        # project back to size of vocabulary
        if self.share_input_output_embed:
            return F.linear(features, self.embed_tokens.weight)
        else:
            return F.linear(features, self.embed_out)
    else:
        return features
",if self . share_input_output_embed :,108
"def generate(self, dest, vars):
    util.ensure_dir(dest)
    for relpath, src, template in self._file_templates:
        file_dest = os.path.join(dest, relpath)
        util.ensure_dir(os.path.dirname(file_dest))
        if template is None:
            shutil.copyfile(src, file_dest)
        else:
            _render_template(template, vars, file_dest)
",if template is None :,115
"def _py_matching_callback(self, context, result, sender, device):
    d = HIDDevice.get_device(c_void_p(device))
    if d not in self.devices:
        self.devices.add(d)
        for x in self.matching_observers:
            if hasattr(x, ""device_discovered""):
                x.device_discovered(d)
","if hasattr ( x , ""device_discovered"" ) :",102
"def urlquote(*args, **kwargs):
    new_kwargs = dict(kwargs)
    if not PY3:
        new_kwargs = dict(kwargs)
        if ""encoding"" in new_kwargs:
            del new_kwargs[""encoding""]
        if ""errors"" in kwargs:
            del new_kwargs[""errors""]
    return quote(*args, **new_kwargs)
","if ""encoding"" in new_kwargs :",93
"def Set(self, attr, value):
    hook = getattr(self, ""_set_%s"" % attr, None)
    if hook:
        # If there is a set hook we must use the context manager.
        if self._lock > 0:
            raise ValueError(
                ""Can only update attribute %s using the context manager."" % attr
            )
        if attr not in self._pending_hooks:
            self._pending_hooks.append(attr)
        self._pending_parameters[attr] = value
    else:
        super(Configuration, self).Set(attr, value)
",if self . _lock > 0 :,150
"def on_profiles_loaded(self, profiles):
    cb = self.builder.get_object(""cbProfile"")
    model = cb.get_model()
    model.clear()
    for f in profiles:
        name = f.get_basename()
        if name.endswith("".mod""):
            continue
        if name.endswith("".sccprofile""):
            name = name[0:-11]
        model.append((name, f, None))
    cb.set_active(0)
","if name . endswith ( "".sccprofile"" ) :",122
"def get_eval_task(self, worker_id):
    """"""Return next evaluation (task_id, Task) tuple""""""
    with self._lock:
        if not self._eval_todo:
            return -1, None
        self._task_id += 1
        task = self._eval_todo.pop()
        self._doing[self._task_id] = (worker_id, task, time.time())
        return self._task_id, task
",if not self . _eval_todo :,115
"def queries(self):
    if DEV:
        cmd = ShellCommand(""docker"", ""ps"", ""-qf"", ""name=%s"" % self.path.k8s)
        if not cmd.check(f""docker check for {self.path.k8s}""):
            if not cmd.stdout.strip():
                log_cmd = ShellCommand(
                    ""docker"", ""logs"", self.path.k8s, stderr=subprocess.STDOUT
                )
                if log_cmd.check(f""docker logs for {self.path.k8s}""):
                    print(cmd.stdout)
                pytest.exit(f""container failed to start for {self.path.k8s}"")
    return ()
","if not cmd . check ( f""docker check for {self.path.k8s}"" ) :",188
"def disjoined(data):
    # create marginalized distributions and multiple them together
    data_disjoined = None
    dim = len(data.shape)
    for d in range(dim):
        axes = list(range(dim))
        axes.remove(d)
        data1d = multisum(data, axes)
        shape = [1 for k in range(dim)]
        shape[d] = len(data1d)
        data1d = data1d.reshape(tuple(shape))
        if d == 0:
            data_disjoined = data1d
        else:
            data_disjoined = data_disjoined * data1d
    return data_disjoined
",if d == 0 :,176
"def safe_repr(val):
    try:
        if isinstance(val, dict):
            # We special case dicts to have a sorted repr. This makes testing
            # significantly easier
            val = _obj_with_safe_repr(val)
        ret = repr(val)
        if six.PY2:
            ret = ret.decode(""utf-8"")
    except UnicodeEncodeError:
        ret = red(""a %r that cannot be represented"" % type(val))
    else:
        ret = green(ret)
    return ret
","if isinstance ( val , dict ) :",138
"def wrapper(*args, **kwargs):
    resp = view_func(*args, **kwargs)
    if isinstance(resp, dict):
        ctx_params = request.environ.get(""webrec.template_params"")
        if ctx_params:
            resp.update(ctx_params)
        template = self.jinja_env.jinja_env.get_or_select_template(template_name)
        return template.render(**resp)
    else:
        return resp
",if ctx_params :,117
"def post(self, request, *args, **kwargs):
    contact_id = kwargs.get(""pk"")
    self.object = get_object_or_404(Contact, id=contact_id)
    if (
        self.request.user.role != ""ADMIN""
        and not self.request.user.is_superuser
        and self.request.user != self.object.created_by
    ) or self.object.company != self.request.company:
        raise PermissionDenied
    else:
        if self.object.address_id:
            self.object.address.delete()
        self.object.delete()
        if self.request.is_ajax():
            return JsonResponse({""error"": False})
        return redirect(""contacts:list"")
",if self . request . is_ajax ( ) :,189
"def escape(text, newline=False):
    """"""Escape special html characters.""""""
    if isinstance(text, str):
        if ""&"" in text:
            text = text.replace(""&"", ""&amp;"")
        if "">"" in text:
            text = text.replace("">"", ""&gt;"")
        if ""<"" in text:
            text = text.replace(""<"", ""&lt;"")
        if '""' in text:
            text = text.replace('""', ""&quot;"")
        if ""'"" in text:
            text = text.replace(""'"", ""&quot;"")
        if newline:
            if ""\n"" in text:
                text = text.replace(""\n"", ""<br>"")
    return text
","if ""<"" in text :",170
"def everythingIsUnicode(d):
    """"""Takes a dictionary, recursively verifies that every value is unicode""""""
    for k, v in d.iteritems():
        if isinstance(v, dict) and k != ""headers"":
            if not everythingIsUnicode(v):
                return False
        elif isinstance(v, list):
            for i in v:
                if isinstance(i, dict) and not everythingIsUnicode(i):
                    return False
                elif isinstance(i, _bytes):
                    return False
        elif isinstance(v, _bytes):
            return False
    return True
",if not everythingIsUnicode ( v ) :,158
"def fill(self):
    try:
        while (
            not self.stopping.wait(self.sample_wait)
            and len(self.queue) < self.queue.maxlen
        ):
            self.queue.append(self.parent._read())
            if self.partial and isinstance(self.parent, EventsMixin):
                self.parent._fire_events()
        self.full.set()
        while not self.stopping.wait(self.sample_wait):
            self.queue.append(self.parent._read())
            if isinstance(self.parent, EventsMixin):
                self.parent._fire_events()
    except ReferenceError:
        # Parent is dead; time to die!
        pass
","if self . partial and isinstance ( self . parent , EventsMixin ) :",190
"def _SetListviewTextItems(self, items):
    self.listview.DeleteAllItems()
    index = -1
    for item in items:
        index = self.listview.InsertItem(index + 1, item[0])
        data = item[1]
        if data is None:
            data = """"
        self.listview.SetItemText(index, 1, data)
",if data is None :,99
"def process_request(self, request):
    for old, new in self.names_name:
        request.uri = request.uri.replace(old, new)
        if is_text_payload(request) and request.body:
            body = six.ensure_str(request.body)
            if old in body:
                request.body = body.replace(old, new)
    return request
",if old in body :,103
"def serialize(cls, value, *args, **kwargs):
    if value is None:
        return """"
    value_as_string = six.text_type(value)
    if SHOULD_NOT_USE_LOCALE:
        return value_as_string
    else:
        grouping = kwargs.get(""grouping"", None)
        has_decimal_places = value_as_string.find(""."") != -1
        if not has_decimal_places:
            string_format = ""%d""
        else:
            decimal_places = len(value_as_string.split(""."")[1])
            string_format = ""%.{}f"".format(decimal_places)
        return locale.format(string_format, value, grouping=grouping)
",if not has_decimal_places :,185
"def review_link(request, path_obj):
    try:
        if path_obj.has_suggestions():
            if check_permission(""translate"", request):
                text = _(""Review Suggestions"")
            else:
                text = _(""View Suggestions"")
            return {
                ""href"": dispatch.translate(
                    request, path_obj.pootle_path, matchnames=[""hassuggestion""]
                ),
                ""text"": text,
            }
    except IOError:
        pass
",if path_obj . has_suggestions ( ) :,146
"def _migrate_key(self, key):
    """"""migrate key from old .dat file""""""
    key_path = os.path.join(self.home_path, ""keys.dat"")
    if os.path.exists(key_path):
        try:
            key_data = json.loads(open(key_path, ""rb"").read())
            if key_data.get(key):
                self.add_key(key, key_data.get(key))
        except:
            self.error(f""Corrupt key file. Manual migration of '{key}' required."")
",if key_data . get ( key ) :,148
"def gather_callback_args(self, obj, callbacks):
    session = sa.orm.object_session(obj)
    for callback in callbacks:
        backref = callback.backref
        root_objs = getdotattr(obj, backref) if backref else obj
        if root_objs:
            if not isinstance(root_objs, Iterable):
                root_objs = [root_objs]
            with session.no_autoflush:
                for root_obj in root_objs:
                    if root_obj:
                        args = self.get_callback_args(root_obj, callback)
                        if args:
                            yield args
","if not isinstance ( root_objs , Iterable ) :",182
"def GetDefFile(self, gyp_to_build_path):
    """"""Returns the .def file from sources, if any.  Otherwise returns None.""""""
    spec = self.spec
    if spec[""type""] in (""shared_library"", ""loadable_module"", ""executable""):
        def_files = [s for s in spec.get(""sources"", []) if s.endswith("".def"")]
        if len(def_files) == 1:
            return gyp_to_build_path(def_files[0])
        elif len(def_files) > 1:
            raise Exception(""Multiple .def files"")
    return None
",if len ( def_files ) == 1 :,153
"def _validate_gallery(images):
    for image in images:
        image_path = image.get(""image_path"", """")
        if image_path:
            if not isfile(image_path):
                raise TypeError(f""{image_path!r} is not a valid image path."")
        else:
            raise TypeError(""'image_path' is required."")
        if not len(image.get(""caption"", """")) <= 180:
            raise TypeError(""Caption must be 180 characters or less."")
","if not len ( image . get ( ""caption"" , """" ) ) <= 180 :",130
"def VType(self):
    if ""DW_AT_type"" in self.attributes:
        target = self.types[self.type_id]
        target_type = target.VType()
        if not isinstance(target_type, list):
            target_type = [target_type, None]
        return [""Pointer"", dict(target=target_type[0], target_args=target_type[1])]
    return [""Pointer"", dict(target=""Void"")]
","if not isinstance ( target_type , list ) :",117
"def addInPlace(self, value1, value2):
    for group in value2:
        for key in value2[group]:
            if key not in value1[group]:
                value1[group][key] = value2[group][key]
            else:
                value1[group][key] += value2[group][key]
    return value1
",if key not in value1 [ group ] :,97
"def _mongo_query_and(self, queries):
    if len(queries) == 1:
        return queries[0]
    query = {}
    for q in queries:
        for k, v in q.items():
            if k not in query:
                query[k] = {}
            if isinstance(v, list):
                # TODO check exists of k in query, may be it should be update
                query[k] = v
            else:
                query[k].update(v)
    return query
",if k not in query :,141
"def _handled_eventtype(self, eventtype, handler):
    if eventtype not in known_events:
        log.error('The event ""%s"" is not known', eventtype)
        return False
    if known_events[eventtype].__module__.startswith(""deluge.event""):
        if handler.__self__ is self:
            return True
        log.error(
            ""You cannot register custom notification providers ""
            ""for built-in event types.""
        )
        return False
    return True
",if handler . __self__ is self :,130
"def get_ax_arg(uri):
    if not ax_ns:
        return u""""
    prefix = ""openid."" + ax_ns + "".type.""
    ax_name = None
    for name, values in self.request.arguments.iteritems():
        if values[-1] == uri and name.startswith(prefix):
            part = name[len(prefix) :]
            ax_name = ""openid."" + ax_ns + "".value."" + part
            break
    if not ax_name:
        return u""""
    return self.get_argument(ax_name, u"""")
",if values [ - 1 ] == uri and name . startswith ( prefix ) :,148
"def handle_starttag(self, tag, attrs):
    if tag == ""base"":
        self.base_url = dict(attrs).get(""href"")
    if self.scan_tag(tag):
        for attr, value in attrs:
            if self.scan_attr(attr):
                if self.strip:
                    value = strip_html5_whitespace(value)
                url = self.process_attr(value)
                link = Link(url=url)
                self.links.append(link)
                self.current_link = link
",if self . scan_attr ( attr ) :,151
"def test_long_steadystate_queue_popright(self):
    for size in (0, 1, 2, 100, 1000):
        d = deque(reversed(range(size)))
        append, pop = d.appendleft, d.pop
        for i in range(size, BIG):
            append(i)
            x = pop()
            if x != i - size:
                self.assertEqual(x, i - size)
        self.assertEqual(list(reversed(list(d))), list(range(BIG - size, BIG)))
",if x != i - size :,143
"def _update_read(self):
    """"""Update state when there is read event""""""
    try:
        msg = bytes(self._sock.recv(4096))
        if msg:
            self.on_message(msg)
            return True
        # normal close, remote is closed
        self.close()
    except socket.error as err:
        if err.args[0] in (errno.EAGAIN, errno.EWOULDBLOCK):
            pass
        else:
            self.on_error(err)
    return False
",if msg :,142
"def prepend(self, value):
    """"""prepend value to nodes""""""
    root, root_text = self._get_root(value)
    for i, tag in enumerate(self):
        if not tag.text:
            tag.text = """"
        if len(root) > 0:
            root[-1].tail = tag.text
            tag.text = root_text
        else:
            tag.text = root_text + tag.text
        if i > 0:
            root = deepcopy(list(root))
        tag[:0] = root
        root = tag[: len(root)]
    return self
",if len ( root ) > 0 :,160
"def cmp(self, other):
    v_is_ptr = not isinstance(self, CTypesGenericPrimitive)
    w_is_ptr = isinstance(other, CTypesData) and not isinstance(
        other, CTypesGenericPrimitive
    )
    if v_is_ptr and w_is_ptr:
        return cmpfunc(self._convert_to_address(None), other._convert_to_address(None))
    elif v_is_ptr or w_is_ptr:
        return NotImplemented
    else:
        if isinstance(self, CTypesGenericPrimitive):
            self = self._value
        if isinstance(other, CTypesGenericPrimitive):
            other = other._value
        return cmpfunc(self, other)
","if isinstance ( other , CTypesGenericPrimitive ) :",179
"def get_external_addresses(self, label=None) -> List[str]:
    result = []
    for c in self._conf[""pools""].values():
        if label is not None:
            if label == c[""label""]:
                result.append(c[""external_address""][0])
        else:
            result.append(c[""external_address""][0])
    return result
",if label is not None :,99
"def coerce_text(v):
    if not isinstance(v, basestring_):
        if sys.version_info[0] < 3:
            attr = ""__unicode__""
        else:
            attr = ""__str__""
        if hasattr(v, attr):
            return unicode(v)
        else:
            return bytes(v)
    return v
",if sys . version_info [ 0 ] < 3 :,93
"def check_localhost(self):
    """"""Warn if any socket_host is 'localhost'. See #711.""""""
    for k, v in cherrypy.config.items():
        if k == ""server.socket_host"" and v == ""localhost"":
            warnings.warn(
                ""The use of 'localhost' as a socket host can ""
                ""cause problems on newer systems, since ""
                ""'localhost' can map to either an IPv4 or an ""
                ""IPv6 address. You should use '127.0.0.1' ""
                ""or '[::1]' instead.""
            )
","if k == ""server.socket_host"" and v == ""localhost"" :",160
"def add_songs(self, filenames, library):
    changed = []
    for i in range(len(self)):
        if isinstance(self[i], str) and self._list[i] in filenames:
            song = library[self._list[i]]
            self._list[i] = song
            changed.append(song)
    if changed:
        self._emit_changed(changed, msg=""add"")
    return bool(changed)
","if isinstance ( self [ i ] , str ) and self . _list [ i ] in filenames :",113
"def _expand_deps_java_generation(self):
    """"""Ensure that all multilingual dependencies such as proto_library generate java code.""""""
    queue = collections.deque(self.deps)
    keys = set()
    while queue:
        k = queue.popleft()
        if k not in keys:
            keys.add(k)
            dep = self.target_database[k]
            if ""generate_java"" in dep.attr:  # Has this attribute
                dep.attr[""generate_java""] = True
                queue.extend(dep.deps)
","if ""generate_java"" in dep . attr :",144
"def get(self):
    name = request.args.get(""filename"")
    if name is not None:
        opts = dict()
        opts[""type""] = ""episode""
        result = guessit(name, options=opts)
        res = dict()
        if ""episode"" in result:
            res[""episode""] = result[""episode""]
        else:
            res[""episode""] = 0
        if ""season"" in result:
            res[""season""] = result[""season""]
        else:
            res[""season""] = 0
        if ""subtitle_language"" in result:
            res[""subtitle_language""] = str(result[""subtitle_language""])
        return jsonify(data=res)
    else:
        return """", 400
","if ""episode"" in result :",196
"def _get_error_file(self) -> Optional[str]:
    error_file = None
    min_timestamp = sys.maxsize
    for replicas in self.role_replicas.values():
        for replica in replicas:
            if not os.path.exists(replica.error_file):
                continue
            mtime = os.path.getmtime(replica.error_file)
            if mtime < min_timestamp:
                min_timestamp = mtime
                error_file = replica.error_file
    return error_file
",if not os . path . exists ( replica . error_file ) :,138
"def findChapterNameForPosition(self, p):
    """"""Return the name of a chapter containing p or None if p does not exist.""""""
    cc, c = self, self.c
    if not p or not c.positionExists(p):
        return None
    for name in cc.chaptersDict:
        if name != ""main"":
            theChapter = cc.chaptersDict.get(name)
            if theChapter.positionIsInChapter(p):
                return name
    return ""main""
","if name != ""main"" :",128
"def remove_files(folder, file_extensions):
    for f in os.listdir(folder):
        f_path = os.path.join(folder, f)
        if os.path.isfile(f_path):
            extension = os.path.splitext(f_path)[1]
            if extension in file_extensions:
                os.remove(f_path)
",if extension in file_extensions :,96
"def execute_uncomment(self, event):
    cursor = self._editor.GetCurrentPos()
    line, pos = self._editor.GetCurLine()
    spaces = "" "" * self._tab_size
    comment = ""Comment"" + spaces
    cpos = cursor - len(comment)
    lenline = len(line)
    if lenline > 0:
        idx = 0
        while idx < lenline and line[idx] == "" "":
            idx += 1
        if (line[idx : len(comment) + idx]).lower() == comment.lower():
            self._editor.DeleteRange(cursor - pos + idx, len(comment))
            self._editor.SetCurrentPos(cpos)
            self._editor.SetSelection(cpos, cpos)
            self.store_position()
",if ( line [ idx : len ( comment ) + idx ] ) . lower ( ) == comment . lower ( ) :,197
"def test_batch_kwarg_path_relative_dot_slash_is_modified_and_found_in_a_code_cell(
    critical_suite_with_citations, empty_data_context
):
    obs = SuiteEditNotebookRenderer.from_data_context(empty_data_context).render(
        critical_suite_with_citations, {""path"": ""./foo/data""}
    )
    assert isinstance(obs, dict)
    found_expected = False
    for cell in obs[""cells""]:
        if cell[""cell_type""] == ""code"":
            source_code = cell[""source""]
            if 'batch_kwargs = {""path"": ""../.././foo/data""}' in source_code:
                found_expected = True
                break
    assert found_expected
","if cell [ ""cell_type"" ] == ""code"" :",196
"def _get_file(self):
    if self._file is None:
        self._file = SpooledTemporaryFile(
            max_size=self._storage.max_memory_size,
            suffix="".S3Boto3StorageFile"",
            dir=setting(""FILE_UPLOAD_TEMP_DIR""),
        )
        if ""r"" in self._mode:
            self._is_dirty = False
            self.obj.download_fileobj(self._file)
            self._file.seek(0)
        if self._storage.gzip and self.obj.content_encoding == ""gzip"":
            self._file = GzipFile(mode=self._mode, fileobj=self._file, mtime=0.0)
    return self._file
","if self . _storage . gzip and self . obj . content_encoding == ""gzip"" :",184
"def _parse_filters(f_strs):
    filters = []
    if not f_strs:
        return filters
    for f_str in f_strs:
        if "":"" in f_str:
            fname, fopts = f_str.split("":"", 1)
            filters.append((fname, _parse_options([fopts])))
        else:
            filters.append((f_str, {}))
    return filters
","if "":"" in f_str :",107
"def update_completion(self):
    """"""Update completion model with exist tags""""""
    orig_text = self.widget.text()
    text = "", "".join(orig_text.replace("", "", "","").split("","")[:-1])
    tags = []
    for tag in self.tags_list:
        if "","" in orig_text:
            if orig_text[-1] not in ("","", "" ""):
                tags.append(""%s,%s"" % (text, tag))
            tags.append(""%s, %s"" % (text, tag))
        else:
            tags.append(tag)
    if tags != self.completer_model.stringList():
        self.completer_model.setStringList(tags)
","if "","" in orig_text :",177
"def _get_startup_packages(lib_path: Path, packages) -> Set[str]:
    names = set()
    for path in lib_path.iterdir():
        name = path.name
        if name == ""__pycache__"":
            continue
        if name.endswith("".py""):
            names.add(name.split(""."")[0])
        elif path.is_dir() and ""."" not in name:
            names.add(name)
    if packages:
        packages = {package.lower().replace(""-"", ""_"") for package in packages}
        if len(names & packages) == len(packages):
            return packages
    return names
","if name . endswith ( "".py"" ) :",159
"def get_cloud_credential(self):
    """"""Return the credential which is directly tied to the inventory source type.""""""
    credential = None
    for cred in self.credentials.all():
        if self.source in CLOUD_PROVIDERS:
            if cred.kind == self.source.replace(""ec2"", ""aws""):
                credential = cred
                break
        else:
            # these need to be returned in the API credential field
            if cred.credential_type.kind != ""vault"":
                credential = cred
                break
    return credential
",if self . source in CLOUD_PROVIDERS :,149
"def newickize(clade):
    """"""Convert a node tree to a Newick tree string, recursively.""""""
    label = clade.name or """"
    if label:
        unquoted_label = re.match(token_dict[""unquoted node label""], label)
        if (not unquoted_label) or (unquoted_label.end() < len(label)):
            label = ""'%s'"" % label.replace(""\\"", ""\\\\"").replace(""'"", ""\\'"")
    if clade.is_terminal():  # terminal
        return label + make_info_string(clade, terminal=True)
    else:
        subtrees = (newickize(sub) for sub in clade)
        return ""(%s)%s"" % ("","".join(subtrees), label + make_info_string(clade))
",if ( not unquoted_label ) or ( unquoted_label . end ( ) < len ( label ) ) :,184
"def __iter__(self):
    for name, value in self._vars.store.data.items():
        source = self._sources[name]
        prefix = self._get_prefix(value)
        name = u""{0}{{{1}}}"".format(prefix, name)
        if source == self.ARGUMENT_SOURCE:
            yield ArgumentInfo(name, value)
        else:
            yield VariableInfo(name, value, source)
",if source == self . ARGUMENT_SOURCE :,110
"def filepath_enumerate(paths):
    """"""Enumerate the file paths of all subfiles of the list of paths""""""
    out = []
    for path in paths:
        if os.path.isfile(path):
            out.append(path)
        else:
            for root, dirs, files in os.walk(path):
                for name in files:
                    out.append(os.path.normpath(os.path.join(root, name)))
    return out
",if os . path . isfile ( path ) :,122
"def del_(self, key):
    hash_ = self.hash(key)
    node_ = self._table[hash_]
    pre_node = None
    while node_ is not None:
        if node_.key == key:
            if pre_node is None:
                self._table[hash_] = node_.next
            else:
                pre_node.next = node_.next
            self._len -= 1
        pre_node = node_
        node_ = node_.next
",if node_ . key == key :,129
"def _recurse(self, base_path, rel_source, rel_zip):
    submodules_path = Path(base_path) / ""submodules""
    if not submodules_path.is_dir():
        return
    for submodule in submodules_path.iterdir():
        source_path = submodule / rel_source
        if not source_path.is_dir():
            continue
        output_path = submodule / rel_zip
        self._build_lambdas(source_path, output_path)
        self._recurse(submodule, rel_source, rel_zip)
",if not source_path . is_dir ( ) :,139
"def find_test_functions(collections):
    if not isinstance(collections, list):
        collections = [collections]
    functions = []
    for collection in collections:
        if not isinstance(collection, dict):
            collection = vars(collection)
        keys = collection.keys()
        keys.sort()
        for key in keys:
            value = collection[key]
            if isinstance(value, types.FunctionType) and hasattr(value, ""unittest""):
                functions.append(value)
    return functions
","if isinstance ( value , types . FunctionType ) and hasattr ( value , ""unittest"" ) :",131
"def __init__(
    self,
    classifier,
    layer_name=None,
    transpose=None,
    distance=None,
    copy_weights=True,
):
    super().__init__()
    self.copy_weights = copy_weights
    ### set layer weights ###
    if layer_name is not None:
        self.set_weights(getattr(classifier, layer_name))
    else:
        for x in self.possible_layer_names:
            layer = getattr(classifier, x, None)
            if layer is not None:
                self.set_weights(layer)
                break
    ### set distance measure ###
    self.distance = classifier.distance if distance is None else distance
    self.transpose = transpose
",if layer is not None :,183
"def multi_dev_generator(self):
    for data in self._data_loader():
        if len(self._tail_data) < self._base_number:
            self._tail_data += data
        if len(self._tail_data) == self._base_number:
            yield self._tail_data
            self._tail_data = []
",if len ( self . _tail_data ) == self . _base_number :,91
"def Resolve(self, updater=None):
    if len(self.Conflicts):
        for setting, edge in self.Conflicts:
            answer = self.AskUser(self.Setting, setting)
            if answer == Gtk.ResponseType.YES:
                value = setting.Value.split(""|"")
                value.remove(edge)
                setting.Value = ""|"".join(value)
                if updater:
                    updater.UpdateSetting(setting)
            if answer == Gtk.ResponseType.NO:
                return False
    return True
",if answer == Gtk . ResponseType . NO :,146
"def _post_process_ttl(zone):
    for name in zone:
        for record_type in zone[name]:
            records = zone[name][record_type]
            if isinstance(records, list):
                ttl = min([x[""ttl""] for x in records])
                for record in records:
                    if record[""ttl""] != ttl:
                        logger.warning(
                            ""Using lowest TTL {} for the record set. Ignoring value {}"".format(
                                ttl, record[""ttl""]
                            )
                        )
                    record[""ttl""] = ttl
","if record [ ""ttl"" ] != ttl :",183
"def __init__(self, cmds, env, cleanup=[]):
    self.handle = None
    self.cmds = cmds
    self.env = env
    if cleanup:
        if callable(cleanup):
            cleanup = [cleanup]
        else:
            try:
                cleanup = [c for c in cleanup if callable(c)]
            except:
                cleanup = []
    self.cleanup = cleanup
",if callable ( cleanup ) :,106
"def _parse_data_of_birth(cls, data_of_birth_string):
    if data_of_birth_string:
        format = ""%m/%d/%Y""
        try:
            parsed_date = datetime.datetime.strptime(data_of_birth_string, format)
            return parsed_date
        except ValueError:
            # Facebook sometimes provides a partial date format
            # ie 04/07 (ignore those)
            if data_of_birth_string.count(""/"") != 1:
                raise
","if data_of_birth_string . count ( ""/"" ) != 1 :",138
"def process_lib(vars_, coreval):
    for d in vars_:
        var = d.upper()
        if var == ""QTCORE"":
            continue
        value = env[""LIBPATH_"" + var]
        if value:
            core = env[coreval]
            accu = []
            for lib in value:
                if lib in core:
                    continue
                accu.append(lib)
            env[""LIBPATH_"" + var] = accu
",if lib in core :,133
"def throttle_status(server=None):
    result = AmonStruct()
    result.allow = False
    last_check = server.get(""last_check"")
    server_check_period = server.get(""check_every"", 60)
    if last_check:
        period_since_last_check = unix_utc_now() - last_check
        # Add 15 seconds buffer, for statsd
        period_since_last_check = period_since_last_check + 15
        if period_since_last_check >= server_check_period:
            result.allow = True
    else:
        result.allow = True  # Never checked
    return result
",if period_since_last_check >= server_check_period :,164
"def fetch_scatter_outputs(self, task):
    scatteroutputs = []
    for var in task[""body""]:
        # TODO variable support
        if var.startswith(""call""):
            if ""outputs"" in self.tasks_dictionary[task[""body""][var][""task""]]:
                for output in self.tasks_dictionary[task[""body""][var][""task""]][
                    ""outputs""
                ]:
                    scatteroutputs.append(
                        {""task"": task[""body""][var][""alias""], ""output"": output[0]}
                    )
    return scatteroutputs
","if ""outputs"" in self . tasks_dictionary [ task [ ""body"" ] [ var ] [ ""task"" ] ] :",152
"def _add_constant_node(self, source_node):
    parent_ids = range(len(source_node.in_edges))
    for idx in parent_ids:
        parent_node = self.tf_graph.get_node(source_node.in_edges[idx])
        if parent_node.type == ""Const"":
            self._rename_Const(parent_node)
","if parent_node . type == ""Const"" :",96
"def enableCtrls(self):
    # Check if each ctrl has a requirement or an incompatibility,
    # look it up, and enable/disable if so
    for data in self.storySettingsData:
        name = data[""name""]
        if name in self.ctrls:
            if ""requires"" in data:
                set = self.getSetting(data[""requires""])
                for i in self.ctrls[name]:
                    i.Enable(set not in [""off"", ""false"", ""0""])
","if ""requires"" in data :",133
"def update_realtime(self, stdout="""", stderr="""", delete=False):
    wooey_cache = wooey_settings.WOOEY_REALTIME_CACHE
    if delete == False and wooey_cache is None:
        self.stdout = stdout
        self.stderr = stderr
        self.save()
    elif wooey_cache is not None:
        cache = django_cache[wooey_cache]
        if delete:
            cache.delete(self.get_realtime_key())
        else:
            cache.set(
                self.get_realtime_key(),
                json.dumps({""stdout"": stdout, ""stderr"": stderr}),
            )
",if delete :,177
"def _check_for_batch_clashes(xs):
    """"""Check that batch names do not overlap with sample names.""""""
    names = set([x[""description""] for x in xs])
    dups = set([])
    for x in xs:
        batches = tz.get_in((""metadata"", ""batch""), x)
        if batches:
            if not isinstance(batches, (list, tuple)):
                batches = [batches]
            for batch in batches:
                if batch in names:
                    dups.add(batch)
    if len(dups) > 0:
        raise ValueError(
            ""Batch names must be unique from sample descriptions.\n""
            ""Clashing batch names: %s"" % sorted(list(dups))
        )
",if batch in names :,192
"def toggle(self, event=None):
    if self.absolute:
        if self.save == self.split:
            self.save = 100
        if self.split > 20:
            self.save = self.split
            self.split = 1
        else:
            self.split = self.save
    else:
        if self.save == self.split:
            self.save = 0.3
        if self.split <= self.min or self.split >= self.max:
            self.split = self.save
        elif self.split < 0.5:
            self.split = self.min
        else:
            self.split = self.max
    self.placeChilds()
",if self . split <= self . min or self . split >= self . max :,189
"def can_read(self):
    if hasattr(self.file, ""__iter__""):
        iterator = iter(self.file)
        head = next(iterator, None)
        if head is None:
            self.repaired = []
            return True
        if isinstance(head, str):
            self.repaired = itertools.chain([head], iterator)
            return True
        else:
            # We may have mangled a generator at this point, so just abort
            raise IOSourceError(
                ""Could not open source: %r (mode: %r)""
                % (self.file, self.options[""mode""])
            )
    return False
",if head is None :,176
"def _print_message_content(self, peer, data):
    inheaders = 1
    lines = data.splitlines()
    for line in lines:
        # headers first
        if inheaders and not line:
            peerheader = ""X-Peer: "" + peer[0]
            if not isinstance(data, str):
                # decoded_data=false; make header match other binary output
                peerheader = repr(peerheader.encode(""utf-8""))
            print(peerheader)
            inheaders = 0
        if not isinstance(data, str):
            # Avoid spurious 'str on bytes instance' warning.
            line = repr(line)
        print(line)
","if not isinstance ( data , str ) :",180
"def connect(self):
    # Makes connection with MySQL server
    try:
        if os.path.exists(""/etc/mysql/conf.d/my.cnf""):
            connection = pymysql.connect(read_default_file=""/etc/mysql/conf.d/my.cnf"")
        else:
            connection = pymysql.connect(read_default_file=""~/.my.cnf"")
        return connection
    except ValueError as e:
        Log.debug(self, str(e))
        raise MySQLConnectionError
    except pymysql.err.InternalError as e:
        Log.debug(self, str(e))
        raise MySQLConnectionError
","if os . path . exists ( ""/etc/mysql/conf.d/my.cnf"" ) :",156
"def _copy_package_apps(
    local_bin_dir: Path, app_paths: List[Path], suffix: str = """"
) -> None:
    for src_unresolved in app_paths:
        src = src_unresolved.resolve()
        app = src.name
        dest = Path(local_bin_dir / add_suffix(app, suffix))
        if not dest.parent.is_dir():
            mkdir(dest.parent)
        if dest.exists():
            logger.warning(f""{hazard}  Overwriting file {str(dest)} with {str(src)}"")
            dest.unlink()
        if src.exists():
            shutil.copy(src, dest)
",if src . exists ( ) :,177
"def update(self, x, who=None, metadata=None):
    self._retain_refs(metadata)
    y = self._get_key(x)
    if self.keep == ""last"":
        # remove key if already present so that emitted value
        # will reflect elements' actual relative ordering
        self._buffer.pop(y, None)
        self._metadata_buffer.pop(y, None)
        self._buffer[y] = x
        self._metadata_buffer[y] = metadata
    else:  # self.keep == ""first""
        if y not in self._buffer:
            self._buffer[y] = x
            self._metadata_buffer[y] = metadata
    return self.last
",if y not in self . _buffer :,180
"def resolve_credential_keys(m_keys, keys):
    res = []
    for k in m_keys:
        if k[""c7n:match-type""] == ""credential"":
            c_date = parse_date(k[""last_rotated""])
            for ak in keys:
                if c_date == ak[""CreateDate""]:
                    ak = dict(ak)
                    ak[""c7n:match-type""] = ""access""
                    if ak not in res:
                        res.append(ak)
        elif k not in res:
            res.append(k)
    return res
",if ak not in res :,169
"def _apply_flag_attrs(src_flag, dest_flag):
    # Use a baseline flag def to get default values for empty data.
    baseline_flag = FlagDef("""", {}, None)
    for name in dir(src_flag):
        if name[:1] == ""_"":
            continue
        dest_val = getattr(dest_flag, name, None)
        baseline_val = getattr(baseline_flag, name, None)
        if dest_val == baseline_val:
            setattr(dest_flag, name, getattr(src_flag, name))
",if dest_val == baseline_val :,137
"def _ws_keep_reading(self):
    import websockets.exceptions
    while not self._reader_stopped:
        try:
            data = await self._ws.recv()
            if isinstance(data, str):
                data = data.encode(""UTF-8"")
            if len(data) == 0:
                self._error = ""EOF""
                break
        except websockets.exceptions.ConnectionClosedError:
            # TODO: try to reconnect in case of Ctrl+D
            self._error = ""EOF""
            break
        self.num_bytes_received += len(data)
        self._make_output_available(data, block=False)
","if isinstance ( data , str ) :",180
"def to_dict(self) -> Dict[str, Any]:
    result = {}
    for field_name in self.API_FIELDS:
        if field_name == ""id"":
            result[""stream_id""] = self.id
            continue
        elif field_name == ""date_created"":
            result[""date_created""] = datetime_to_timestamp(self.date_created)
            continue
        result[field_name] = getattr(self, field_name)
    result[""is_announcement_only""] = (
        self.stream_post_policy == Stream.STREAM_POST_POLICY_ADMINS
    )
    return result
","if field_name == ""id"" :",164
"def all_masks(
    cls,
    images,
    run,
    run_key,
    step,
):
    all_mask_groups = []
    for image in images:
        if image._masks:
            mask_group = {}
            for k in image._masks:
                mask = image._masks[k]
                mask_group[k] = mask.to_json(run)
            all_mask_groups.append(mask_group)
        else:
            all_mask_groups.append(None)
    if all_mask_groups and not all(x is None for x in all_mask_groups):
        return all_mask_groups
    else:
        return False
",if image . _masks :,183
"def disconnect_all(listener):
    """"""Disconnect from all signals""""""
    for emitter in listener._signal_data.emitters:
        for signal in emitter._signal_data.listeners:
            emitter._signal_data.listeners[signal] = [
                i
                for i in emitter._signal_data.listeners[signal]
                if getattr(i, ""__self__"", None) != listener
            ]
","if getattr ( i , ""__self__"" , None ) != listener",108
"def wait(self, timeout=None):
    if self.returncode is None:
        if timeout is None:
            msecs = _subprocess.INFINITE
        else:
            msecs = max(0, int(timeout * 1000 + 0.5))
        res = _subprocess.WaitForSingleObject(int(self._handle), msecs)
        if res == _subprocess.WAIT_OBJECT_0:
            code = _subprocess.GetExitCodeProcess(self._handle)
            if code == TERMINATE:
                code = -signal.SIGTERM
            self.returncode = code
    return self.returncode
",if code == TERMINATE :,154
"def set_pbar_fraction(self, frac, progress, stage=None):
    gtk.gdk.threads_enter()
    try:
        self.is_pulsing = False
        self.set_stage_text(stage or _(""Processing...""))
        self.pbar.set_text(progress)
        if frac > 1:
            frac = 1.0
        if frac < 0:
            frac = 0
        self.pbar.set_fraction(frac)
    finally:
        gtk.gdk.threads_leave()
",if frac > 1 :,135
"def get_aa_from_codonre(re_aa):
    aas = []
    m = 0
    for i in re_aa:
        if i == ""["":
            m = -1
            aas.append("""")
        elif i == ""]"":
            m = 0
            continue
        elif m == -1:
            aas[-1] = aas[-1] + i
        elif m == 0:
            aas.append(i)
    return aas
","elif i == ""]"" :",129
"def link(token, base_url):
    """"""Validation for ``link``.""""""
    if get_keyword(token) == ""none"":
        return ""none""
    parsed_url = get_url(token, base_url)
    if parsed_url:
        return parsed_url
    function = parse_function(token)
    if function:
        name, args = function
        prototype = (name, [a.type for a in args])
        args = [getattr(a, ""value"", a) for a in args]
        if prototype == (""attr"", [""ident""]):
            return (""attr()"", args[0])
","if prototype == ( ""attr"" , [ ""ident"" ] ) :",153
"def on_bt_search_clicked(self, widget):
    if self.current_provider is None:
        return
    query = self.en_query.get_text()
    @self.obtain_podcasts_with
    def load_data():
        if self.current_provider.kind == directory.Provider.PROVIDER_SEARCH:
            return self.current_provider.on_search(query)
        elif self.current_provider.kind == directory.Provider.PROVIDER_URL:
            return self.current_provider.on_url(query)
        elif self.current_provider.kind == directory.Provider.PROVIDER_FILE:
            return self.current_provider.on_file(query)
",elif self . current_provider . kind == directory . Provider . PROVIDER_URL :,172
"def test_handle_single(self):
    self.skipTest(
        ""Pops up windows and needs user input.. so disabled.""
        ""Still worth keeping whilst we don't have unit tests ""
        ""for all plugins.""
    )
    # Ignored...
    for id_, plugin in self.plugins.items():
        if self.h.plugin_handle(plugin):
            self.h.plugin_enable(plugin, None)
            self.h.handle(id_, self.lib, self.parent, SONGS)
            self.h.plugin_disable(plugin)
",if self . h . plugin_handle ( plugin ) :,144
"def __repr__(self):
    attrs = []
    for k in self._keydata:
        if k == ""p"":
            attrs.append(""p(%d)"" % (self.size() + 1,))
        elif hasattr(self, k):
            attrs.append(k)
    if self.has_private():
        attrs.append(""private"")
    # PY3K: This is meant to be text, do not change to bytes (data)
    return ""<%s @0x%x %s>"" % (self.__class__.__name__, id(self), "","".join(attrs))
","if k == ""p"" :",142
"def apply(self, node, code, required):
    yield ""try:""
    yield from self.iterIndented(code)
    yield ""    pass""
    yield ""except {}:"".format(self.exceptionString)
    outputVariables = node.getOutputSocketVariables()
    for i, s in enumerate(node.outputs):
        if s.identifier in required:
            if hasattr(s, ""getDefaultValueCode""):
                yield f""    {outputVariables[s.identifier]} = {s.getDefaultValueCode()}""
            else:
                yield f""    {outputVariables[s.identifier]} = self.outputs[{i}].getDefaultValue()""
    yield ""    pass""
",if s . identifier in required :,181
"def __import__(name, globals=None, locals=None, fromlist=(), level=0):
    module = orig___import__(name, globals, locals, fromlist, level)
    if fromlist and module.__name__ in modules:
        if ""*"" in fromlist:
            fromlist = list(fromlist)
            fromlist.remove(""*"")
            fromlist.extend(getattr(module, ""__all__"", []))
        for x in fromlist:
            if isinstance(getattr(module, x, None), types.ModuleType):
                from_name = ""{}.{}"".format(module.__name__, x)
                if from_name in modules:
                    importlib.import_module(from_name)
    return module
","if isinstance ( getattr ( module , x , None ) , types . ModuleType ) :",175
"def _consume_msg(self):
    ws = self._ws
    try:
        while True:
            r = await ws.recv()
            if isinstance(r, bytes):
                r = r.decode(""utf-8"")
            msg = json.loads(r)
            stream = msg.get(""stream"")
            if stream is not None:
                await self._dispatch(stream, msg)
    except websockets.WebSocketException as wse:
        logging.warn(wse)
        await self.close()
        asyncio.ensure_future(self._ensure_ws())
","if isinstance ( r , bytes ) :",158
"def add_source(self, source, name=None):
    """"""Adds a new data source to an existing provider.""""""
    if self.randomize:
        if not source.can_shuffle():
            raise ValueError(
                ""Cannot add a non-shuffleable source to an ""
                ""already shuffled provider.""
            )
    super().add_source(source, name=name)
    if self.randomize is True:
        self._shuffle_len = self.entries
",if not source . can_shuffle ( ) :,121
"def __str__(self):
    buf = [""""]
    if self.fileName:
        buf.append(self.fileName + "":"")
    if self.line != -1:
        if not self.fileName:
            buf.append(""line "")
        buf.append(str(self.line))
        if self.column != -1:
            buf.append("":"" + str(self.column))
        buf.append("":"")
    buf.append("" "")
    return str("""").join(buf)
",if self . column != - 1 :,124
"def has_bad_headers(self):
    headers = [self.sender, self.reply_to] + self.recipients
    for header in headers:
        if _has_newline(header):
            return True
    if self.subject:
        if _has_newline(self.subject):
            for linenum, line in enumerate(self.subject.split(""\r\n"")):
                if not line:
                    return True
                if linenum > 0 and line[0] not in ""\t "":
                    return True
                if _has_newline(line):
                    return True
                if len(line.strip()) == 0:
                    return True
    return False
",if len ( line . strip ( ) ) == 0 :,186
"def scanHexEscape(self, prefix):
    code = 0
    leng = 4 if (prefix == ""u"") else 2
    for i in xrange(leng):
        if self.index < self.length and isHexDigit(self.source[self.index]):
            ch = self.source[self.index]
            self.index += 1
            code = code * 16 + HEX_CONV[ch]
        else:
            return """"
    return unichr(code)
",if self . index < self . length and isHexDigit ( self . source [ self . index ] ) :,119
"def _get_table_info(self, table_name):
    table_addr = self.addr_space.profile.get_symbol(table_name)
    table_size = self._get_table_info_distorm()
    if table_size == 0:
        table_size = self._get_table_info_other(table_addr, table_name)
        if table_size == 0:
            debug.error(""Unable to get system call table size"")
    return [table_addr, table_size]
",if table_size == 0 :,126
"def format_file_path(filepath):
    """"""Formats a path as absolute and with the correct platform separator.""""""
    try:
        is_windows_network_mount = WINDOWS_NETWORK_MOUNT_PATTERN.match(filepath)
        filepath = os.path.realpath(os.path.abspath(filepath))
        filepath = re.sub(BACKSLASH_REPLACE_PATTERN, ""/"", filepath)
        is_windows_drive = WINDOWS_DRIVE_PATTERN.match(filepath)
        if is_windows_drive:
            filepath = filepath.capitalize()
        if is_windows_network_mount:
            # Add back a / to the front, since the previous modifications
            # will have replaced any double slashes with single
            filepath = ""/"" + filepath
    except:
        pass
    return filepath
",if is_windows_drive :,194
"def _match(self, cre, s):
    # Run compiled regular expression match method on 's'.
    # Save result, return success.
    self.mo = cre.match(s)
    if __debug__:
        if self.mo is not None and self.debug >= 5:
            self._mesg(""\tmatched r'%r' => %r"" % (cre.pattern, self.mo.groups()))
    return self.mo is not None
",if self . mo is not None and self . debug >= 5 :,108
"def reload_sanitize_allowlist(self, explicit=True):
    self.sanitize_allowlist = []
    try:
        with open(self.sanitize_allowlist_file) as f:
            for line in f.readlines():
                if not line.startswith(""#""):
                    self.sanitize_allowlist.append(line.strip())
    except OSError:
        if explicit:
            log.warning(
                ""Sanitize log file explicitly specified as '%s' but does not exist, continuing with no tools allowlisted."",
                self.sanitize_allowlist_file,
            )
",if explicit :,149
"def conj(self):
    dtype = self.dtype
    if issubclass(self.dtype.type, np.complexfloating):
        if not self.flags.forc:
            raise RuntimeError(
                ""only contiguous arrays may "" ""be used as arguments to this operation""
            )
        if self.flags.f_contiguous:
            order = ""F""
        else:
            order = ""C""
        result = self._new_like_me(order=order)
        func = elementwise.get_conj_kernel(dtype)
        func.prepared_async_call(
            self._grid, self._block, None, self.gpudata, result.gpudata, self.mem_size
        )
        return result
    else:
        return self
",if self . flags . f_contiguous :,198
"def scan_spec_conf(self, conf):
    if ""metadata"" in conf:
        if ""annotations"" in conf[""metadata""] and conf[""metadata""].get(""annotations""):
            for annotation in conf[""metadata""][""annotations""]:
                for key in annotation:
                    if ""seccomp.security.alpha.kubernetes.io/defaultProfileName"" in key:
                        if (
                            ""docker/default"" in annotation[key]
                            or ""runtime/default"" in annotation[key]
                        ):
                            return CheckResult.PASSED
    return CheckResult.FAILED
","if ""seccomp.security.alpha.kubernetes.io/defaultProfileName"" in key :",167
"def test_error_through_destructor(self):
    # Test that the exception state is not modified by a destructor,
    # even if close() fails.
    rawio = self.CloseFailureIO()
    with support.catch_unraisable_exception() as cm:
        with self.assertRaises(AttributeError):
            self.tp(rawio).xyzzy
        if not IOBASE_EMITS_UNRAISABLE:
            self.assertIsNone(cm.unraisable)
        elif cm.unraisable is not None:
            self.assertEqual(cm.unraisable.exc_type, OSError)
",elif cm . unraisable is not None :,157
"def _dumpf(frame):
    if frame is None:
        return ""<None>""
    else:
        addn = ""(with trace!)""
        if frame.f_trace is None:
            addn = "" **No Trace Set **""
        return ""Frame at %d, file %s, line: %d%s"" % (
            id(frame),
            frame.f_code.co_filename,
            frame.f_lineno,
            addn,
        )
",if frame . f_trace is None :,128
"def containsBadbytes(self, value, bytecount=4):
    for b in self.badbytes:
        tmp = value
        if type(b) == str:
            b = ord(b)
        for i in range(bytecount):
            if (tmp & 0xFF) == b:
                return True
            tmp >>= 8
    return False
",if type ( b ) == str :,95
"def _set_peer_statuses(self):
    """"""Set peer statuses.""""""
    cutoff = time.time() - STALE_SECS
    for peer in self.peers:
        if peer.bad:
            peer.status = PEER_BAD
        elif peer.last_good > cutoff:
            peer.status = PEER_GOOD
        elif peer.last_good:
            peer.status = PEER_STALE
        else:
            peer.status = PEER_NEVER
",if peer . bad :,128
"def afterTest(self, test):
    try:
        # If the browser window is still open, close it now.
        self.driver.quit()
    except AttributeError:
        pass
    except Exception:
        pass
    if self.options.headless:
        if self.headless_active:
            try:
                self.display.stop()
            except AttributeError:
                pass
            except Exception:
                pass
",if self . headless_active :,117
"def _written_variables_in_proxy(self, contract):
    variables = []
    if contract.is_upgradeable:
        variables_name_written_in_proxy = self._variable_written_in_proxy()
        if variables_name_written_in_proxy:
            variables_in_contract = [
                contract.get_state_variable_from_name(v)
                for v in variables_name_written_in_proxy
            ]
            variables_in_contract = [v for v in variables_in_contract if v]
            variables += variables_in_contract
    return list(set(variables))
",if variables_name_written_in_proxy :,162
"def _available_symbols(self, scoperef, expr):
    cplns = []
    found_names = set()
    while scoperef:
        elem = self._elem_from_scoperef(scoperef)
        for child in elem:
            name = child.get(""name"", """")
            if name.startswith(expr):
                if name not in found_names:
                    found_names.add(name)
                    ilk = child.get(""ilk"") or child.tag
                    cplns.append((ilk, name))
        scoperef = self.parent_scoperef_from_scoperef(scoperef)
        if not scoperef:
            break
    return sorted(cplns, key=operator.itemgetter(1))
",if name not in found_names :,196
"def get_resource_public_actions(resource_class):
    resource_class_members = inspect.getmembers(resource_class)
    resource_methods = {}
    for name, member in resource_class_members:
        if not name.startswith(""_""):
            if not name[0].isupper():
                if not name.startswith(""wait_until""):
                    if is_resource_action(member):
                        resource_methods[name] = member
    return resource_methods
",if not name [ 0 ] . isupper ( ) :,122
"def UpdateControlState(self):
    active = self.demoModules.GetActiveID()
    # Update the radio/restore buttons
    for moduleID in self.radioButtons:
        btn = self.radioButtons[moduleID]
        if moduleID == active:
            btn.SetValue(True)
        else:
            btn.SetValue(False)
        if self.demoModules.Exists(moduleID):
            btn.Enable(True)
            if moduleID == modModified:
                self.btnRestore.Enable(True)
        else:
            btn.Enable(False)
            if moduleID == modModified:
                self.btnRestore.Enable(False)
",if moduleID == active :,177
"def test_controlcharacters(self):
    for i in range(128):
        c = chr(i)
        testString = ""string containing %s"" % c
        if i >= 32 or c in ""\r\n\t"":
            # \r, \n and \t are the only legal control chars in XML
            data = plistlib.dumps(testString, fmt=plistlib.FMT_XML)
            if c != ""\r"":
                self.assertEqual(plistlib.loads(data), testString)
        else:
            with self.assertRaises(ValueError):
                plistlib.dumps(testString, fmt=plistlib.FMT_XML)
        plistlib.dumps(testString, fmt=plistlib.FMT_BINARY)
","if c != ""\r"" :",188
"def remove_usernames(self, username: SLT[str]) -> None:
    with self.__lock:
        if self._chat_ids:
            raise RuntimeError(
                f""Can't set {self.username_name} in conjunction with (already set) ""
                f""{self.chat_id_name}s.""
            )
        parsed_username = self._parse_username(username)
        self._usernames -= parsed_username
",if self . _chat_ids :,116
"def get_size(self, shape_info):
    # The size is the data, that have constant size.
    state = np.random.RandomState().get_state()
    size = 0
    for elem in state:
        if isinstance(elem, str):
            size += len(elem)
        elif isinstance(elem, np.ndarray):
            size += elem.size * elem.itemsize
        elif isinstance(elem, int):
            size += np.dtype(""int"").itemsize
        elif isinstance(elem, float):
            size += np.dtype(""float"").itemsize
        else:
            raise NotImplementedError()
    return size
","elif isinstance ( elem , int ) :",159
"def before_step(self, step, feed_dict):
    if step == 0:
        for _type, mem in self.memories.items():
            if ""var"" in mem and ""source"" in mem:
                self.gan.session.run(tf.assign(mem[""var""], mem[""source""]))
","if ""var"" in mem and ""source"" in mem :",79
"def write(self, *bits):
    for bit in bits:
        if not self.bytestream:
            self.bytestream.append(0)
        byte = self.bytestream[self.bytenum]
        if self.bitnum == 8:
            if self.bytenum == len(self.bytestream) - 1:
                byte = 0
                self.bytestream += bytes([byte])
            self.bytenum += 1
            self.bitnum = 0
        mask = 2 ** self.bitnum
        if bit:
            byte |= mask
        else:
            byte &= ~mask
        self.bytestream[self.bytenum] = byte
        self.bitnum += 1
",if self . bitnum == 8 :,186
"def _validate_parameter_range(self, value_hp, parameter_range):
    """"""Placeholder docstring""""""
    for (
        parameter_range_key,
        parameter_range_value,
    ) in parameter_range.__dict__.items():
        if parameter_range_key == ""scaling_type"":
            continue
        # Categorical ranges
        if isinstance(parameter_range_value, list):
            for categorical_value in parameter_range_value:
                value_hp.validate(categorical_value)
        # Continuous, Integer ranges
        else:
            value_hp.validate(parameter_range_value)
","if isinstance ( parameter_range_value , list ) :",159
"def _trackA(self, tracks):
    try:
        track, start, end = self.featureA
        assert track in tracks
        return track
    except TypeError:
        for track in tracks:
            for feature_set in track.get_sets():
                if hasattr(feature_set, ""features""):
                    if self.featureA in feature_set.features.values():
                        return track
        return None
",if self . featureA in feature_set . features . values ( ) :,116
"def walk(directory, path_so_far):
    for name in sorted(os.listdir(directory)):
        if any(fnmatch(name, pattern) for pattern in basename_ignore):
            continue
        path = path_so_far + ""/"" + name if path_so_far else name
        if any(fnmatch(path, pattern) for pattern in path_ignore):
            continue
        full_name = os.path.join(directory, name)
        if os.path.isdir(full_name):
            for file_path in walk(full_name, path):
                yield file_path
        elif os.path.isfile(full_name):
            yield path
",if os . path . isdir ( full_name ) :,172
"def _poll_ipc_requests(self) -> None:
    try:
        if self._ipc_requests.empty():
            return
        while not self._ipc_requests.empty():
            args = self._ipc_requests.get()
            try:
                for filename in args:
                    if os.path.isfile(filename):
                        self.get_editor_notebook().show_file(filename)
            except Exception as e:
                logger.exception(""Problem processing ipc request"", exc_info=e)
        self.become_active_window()
    finally:
        self.after(50, self._poll_ipc_requests)
",if self . _ipc_requests . empty ( ) :,180
"def test_read1(self):
    self.test_write()
    blocks = []
    nread = 0
    with gzip.GzipFile(self.filename, ""r"") as f:
        while True:
            d = f.read1()
            if not d:
                break
            blocks.append(d)
            nread += len(d)
            # Check that position was updated correctly (see issue10791).
            self.assertEqual(f.tell(), nread)
    self.assertEqual(b"""".join(blocks), data1 * 50)
",if not d :,146
"def _target_generator(self):
    if self._internal_target_generator is None:
        if self._net_none:
            return None
        from ....model_zoo.rcnn.rpn.rpn_target import RPNTargetGenerator
        self._internal_target_generator = RPNTargetGenerator(
            num_sample=self._num_sample,
            pos_iou_thresh=self._pos_iou_thresh,
            neg_iou_thresh=self._neg_iou_thresh,
            pos_ratio=self._pos_ratio,
            stds=self._box_norm,
            **self._kwargs
        )
        return self._internal_target_generator
    else:
        return self._internal_target_generator
",if self . _net_none :,191
"def time_left(self):
    """"""Return how many seconds are left until the timeout expires""""""
    if self.is_non_blocking:
        return 0
    elif self.is_infinite:
        return None
    else:
        delta = self.target_time - self.TIME()
        if delta > self.duration:
            # clock jumped, recalculate
            self.target_time = self.TIME() + self.duration
            return self.duration
        else:
            return max(0, delta)
",if delta > self . duration :,132
"def _decorator(cls):
    for name, meth in inspect.getmembers(cls, inspect.isroutine):
        if name not in cls.__dict__:
            continue
        if name != ""__init__"":
            if not private and name.startswith(""_""):
                continue
        if name in butnot:
            continue
        setattr(cls, name, decorator(meth))
    return cls
",if name in butnot :,99
"def load_vocab(vocab_file: str) -> List:
    """"""Loads a vocabulary file into a dictionary.""""""
    vocab = collections.OrderedDict()
    with io.open(vocab_file, ""r"", encoding=""UTF-8"") as file:
        for num, line in enumerate(file):
            items = convert_to_unicode(line.strip()).split(""\t"")
            if len(items) > 2:
                break
            token = items[0]
            index = items[1] if len(items) == 2 else num
            token = token.strip()
            vocab[token] = int(index)
        return vocab
",if len ( items ) > 2 :,164
"def slice_fill(self, slice_):
    ""Fills the slice with zeroes for the dimensions that have single elements and squeeze_dims true""
    if isinstance(self.indexes, int):
        new_slice_ = [0]
        offset = 0
    else:
        new_slice_ = [slice_[0]]
        offset = 1
    for i in range(1, len(self.nums)):
        if self.squeeze_dims[i]:
            new_slice_.append(0)
        elif offset < len(slice_):
            new_slice_.append(slice_[offset])
            offset += 1
    new_slice_ += slice_[offset:]
    return new_slice_
",if self . squeeze_dims [ i ] :,171
"def check_update_function(url, folder, update_setter, version_setter, auto):
    remote_version = urllib.urlopen(url).read()
    if remote_version.isdigit():
        local_version = get_local_timestamp(folder)
        if remote_version > local_version:
            if auto:
                update_setter.set_value(True)
            version_setter.set_value(remote_version)
            return True
        else:
            return False
    else:
        return False
",if auto :,136
"def iter_content(self, chunk_size_bytes):
    while True:
        try:
            data = self._fp.read(chunk_size_bytes)
        except IOError as e:
            raise Fetcher.PermanentError(
                ""Problem reading chunk from {}: {}"".format(self._fp.name, e)
            )
        if not data:
            break
        yield data
",if not data :,105
"def gvariant_args(args: List[Any]) -> str:
    """"""Convert args into gvariant.""""""
    gvariant = """"
    for arg in args:
        if isinstance(arg, bool):
            gvariant += "" {}"".format(str(arg).lower())
        elif isinstance(arg, (int, float)):
            gvariant += f"" {arg}""
        elif isinstance(arg, str):
            gvariant += f' ""{arg}""'
        else:
            gvariant += f"" {arg!s}""
    return gvariant.lstrip()
","elif isinstance ( arg , str ) :",139
"def _element_keywords(cls, backend, elements=None):
    ""Returns a dictionary of element names to allowed keywords""
    if backend not in Store.loaded_backends():
        return {}
    mapping = {}
    backend_options = Store.options(backend)
    elements = elements if elements is not None else backend_options.keys()
    for element in elements:
        if ""."" in element:
            continue
        element = element if isinstance(element, tuple) else (element,)
        element_keywords = []
        options = backend_options[""."".join(element)]
        for group in Options._option_groups:
            element_keywords.extend(options[group].allowed_keywords)
        mapping[element[0]] = element_keywords
    return mapping
","if ""."" in element :",185
"def setup_parameter_node(self, param_node):
    if param_node.bl_idname == ""SvNumberNode"":
        if self.use_prop or self.get_prop_name():
            value = self.sv_get()[0][0]
            print(""V"", value)
            if isinstance(value, int):
                param_node.selected_mode = ""int""
                param_node.int_ = value
            elif isinstance(value, float):
                param_node.selected_mode = ""float""
                param_node.float_ = value
","if isinstance ( value , int ) :",156
"def _get_oshape(indices_shape, depth, axis):
    oshape = []
    true_axis = len(indices_shape) if axis == -1 else axis
    ndim = len(indices_shape) + 1
    indices_index = 0
    for i in range(0, ndim):
        if i == true_axis:
            oshape.append(depth)
        else:
            oshape.append(indices_shape[indices_index])
            indices_index += 1
    return oshape
",if i == true_axis :,123
"def check(self, value):
    value = String.check(self, value)
    if isinstance(value, str):
        value = value.upper()
        for prefix in (self.prefix, self.prefix.split(""_"", 1)[1]):
            # e.g. PANGO_WEIGHT_BOLD --> BOLD but also WEIGHT_BOLD --> BOLD
            if value.startswith(prefix):
                value = value[len(prefix) :]
            value = value.lstrip(""_"")
        if hasattr(self.group, value):
            return getattr(self.group, value)
        else:
            raise ValueError(""No such constant: %s_%s"" % (self.prefix, value))
    else:
        return value
",if value . startswith ( prefix ) :,182
"def shuffle_unison_inplace(list_of_lists, random_state=None):
    if list_of_lists:
        assert all(len(l) == len(list_of_lists[0]) for l in list_of_lists)
        if random_state is not None:
            random_state.permutation(len(list_of_lists[0]))
        else:
            p = np.random.permutation(len(list_of_lists[0]))
        return [l[p] for l in list_of_lists]
    return None
",if random_state is not None :,139
"def _load_module(self):
    spec = self.default_module_spec
    module_identifier = self.module_identifier
    if module_identifier:
        impls = self.get_module_implementation_map()
        if module_identifier not in impls:
            raise ModuleNotFound(
                ""Invalid module identifier %r in %s""
                % (module_identifier, force_ascii(repr(self)))
            )
        spec = impls[module_identifier]
    cls = load(
        spec, context_explanation=""Loading module for %s"" % force_ascii(repr(self))
    )
    options = getattr(self, self.module_options_field, None) or {}
    return cls(self, options)
",if module_identifier not in impls :,185
"def get_data(self, state=None, request=None):
    if self.load_in_memory:
        data, shapes = self._in_memory_get_data(state, request)
    else:
        data, shapes = self._out_of_memory_get_data(state, request)
    for i in range(len(data)):
        if shapes[i] is not None:
            if isinstance(request, numbers.Integral):
                data[i] = data[i].reshape(shapes[i])
            else:
                for j in range(len(data[i])):
                    data[i][j] = data[i][j].reshape(shapes[i][j])
    return tuple(data)
",if shapes [ i ] is not None :,187
"def resolve_credential_keys(m_keys, keys):
    res = []
    for k in m_keys:
        if k[""c7n:match-type""] == ""credential"":
            c_date = parse_date(k[""last_rotated""])
            for ak in keys:
                if c_date == ak[""CreateDate""]:
                    ak = dict(ak)
                    ak[""c7n:match-type""] = ""access""
                    if ak not in res:
                        res.append(ak)
        elif k not in res:
            res.append(k)
    return res
","if c_date == ak [ ""CreateDate"" ] :",169
"def _is_legacy_mode(self, node):
    """"""Checks if the ``ast.Call`` node's keywords signal using legacy mode.""""""
    script_mode = False
    py_version = ""py2""
    for kw in node.keywords:
        if kw.arg == ""script_mode"":
            script_mode = (
                bool(kw.value.value) if isinstance(kw.value, ast.NameConstant) else True
            )
        if kw.arg == ""py_version"":
            py_version = kw.value.s if isinstance(kw.value, ast.Str) else ""py3""
    return not (py_version.startswith(""py3"") or script_mode)
","if kw . arg == ""script_mode"" :",173
"def get_upstream_statuses_events(self, upstream: Set) -> Dict[str, V1Statuses]:
    statuses_by_refs = {u: [] for u in upstream}
    events = self.events or []  # type: List[V1EventTrigger]
    for e in events:
        entity_ref = contexts_refs.get_entity_ref(e.ref)
        if not entity_ref:
            continue
        if entity_ref not in statuses_by_refs:
            continue
        for kind in e.kinds:
            status = V1EventKind.events_statuses_mapping.get(kind)
            if status:
                statuses_by_refs[entity_ref].append(status)
    return statuses_by_refs
",if entity_ref not in statuses_by_refs :,191
"def items(self):
    dict = {}
    for userdir in self.XDG_DIRS.keys():
        prefix = self.get(userdir).strip('""').split(""/"")[0]
        if prefix:
            path = (
                os.getenv(""HOME"")
                + ""/""
                + ""/"".join(self.get(userdir).strip('""').split(""/"")[1:])
            )
        else:
            path = self.get(userdir).strip('""')
        dict[userdir] = path
    return dict.items()
",if prefix :,140
"def clean_objects(string, common_attributes):
    """"""Return object and attribute lists""""""
    string = clean_string(string)
    words = string.split()
    if len(words) > 1:
        prefix_words_are_adj = True
        for att in words[:-1]:
            if att not in common_attributes:
                prefix_words_are_adj = False
        if prefix_words_are_adj:
            return words[-1:], words[:-1]
        else:
            return [string], []
    else:
        return [string], []
",if att not in common_attributes :,148
"def extract_custom(extractor, *args, **kw):
    for match in extractor(*args, **kw):
        msg = match[2]
        if isinstance(msg, tuple) and msg[0] == """":
            unused = (
                ""<unused singular (hash=%s)>"" % md5(msg[1].encode(""utf8"")).hexdigest()
            )
            msg = (unused, msg[1], msg[2])
            match = (match[0], match[1], msg, match[3])
        yield match
","if isinstance ( msg , tuple ) and msg [ 0 ] == """" :",136
"def test_convex_decomposition(self):
    mesh = g.get_mesh(""quadknot.obj"")
    engines = [(""vhacd"", g.trimesh.interfaces.vhacd.exists)]
    for engine, exists in engines:
        if not exists:
            g.log.warning(""skipping convex decomposition engine %s"", engine)
            continue
        g.log.info(""Testing convex decomposition with engine %s"", engine)
        meshes = mesh.convex_decomposition(engine=engine)
        self.assertTrue(len(meshes) > 1)
        for m in meshes:
            self.assertTrue(m.is_watertight)
        g.log.info(""convex decomposition succeeded with %s"", engine)
",if not exists :,183
"def _to_string_infix(self, ostream, idx, verbose):
    if verbose:
        ostream.write("" , "")
    else:
        hasConst = not (
            self._const.__class__ in native_numeric_types and self._const == 0
        )
        if hasConst:
            idx -= 1
        _l = self._coef[id(self._args[idx])]
        _lt = _l.__class__
        if _lt is _NegationExpression or (_lt in native_numeric_types and _l < 0):
            ostream.write("" - "")
        else:
            ostream.write("" + "")
",if _lt is _NegationExpression or ( _lt in native_numeric_types and _l < 0 ) :,169
"def get_other(self, data, items):
    is_tuple = False
    if type(data) == tuple:
        data = list(data)
        is_tuple = True
    if type(data) == list:
        m_items = items.copy()
        for idx, item in enumerate(items):
            if item < 0:
                m_items[idx] = len(data) - abs(item)
        for i in sorted(set(m_items), reverse=True):
            if i < len(data) and i > -1:
                del data[i]
        if is_tuple:
            return tuple(data)
        else:
            return data
    else:
        return None
",if is_tuple :,191
"def process_error(self, data):
    if data.get(""error""):
        if ""denied"" in data[""error""] or ""cancelled"" in data[""error""]:
            raise AuthCanceled(self, data.get(""error_description"", """"))
        raise AuthFailed(self, data.get(""error_description"") or data[""error""])
    elif ""denied"" in data:
        raise AuthCanceled(self, data[""denied""])
","if ""denied"" in data [ ""error"" ] or ""cancelled"" in data [ ""error"" ] :",103
"def tamper(payload, **kwargs):
    junk_chars = ""!#$%&()*~+-_.,:;?@[/|\]^`""
    retval = """"
    for i, char in enumerate(payload, start=1):
        amount = random.randint(10, 15)
        if char == "">"":
            retval += "">""
            for _ in range(amount):
                retval += random.choice(junk_chars)
        elif char == ""<"":
            retval += ""<""
            for _ in range(amount):
                retval += random.choice(junk_chars)
        elif char == "" "":
            for _ in range(amount):
                retval += random.choice(junk_chars)
        else:
            retval += char
    return retval
","elif char == "" "" :",200
"def retry_http_digest_auth(self, req, auth):
    token, challenge = auth.split("" "", 1)
    chal = parse_keqv_list(parse_http_list(challenge))
    auth = self.get_authorization(req, chal)
    if auth:
        auth_val = ""Digest %s"" % auth
        if req.headers.get(self.auth_header, None) == auth_val:
            return None
        req.add_unredirected_header(self.auth_header, auth_val)
        resp = self.parent.open(req)
        return resp
","if req . headers . get ( self . auth_header , None ) == auth_val :",154
"def close(self):
    self.selector.close()
    if self.sock:
        sockname = None
        try:
            sockname = self.sock.getsockname()
        except (socket.error, OSError):
            pass
        self.sock.close()
        if type(sockname) is str:
            # it was a Unix domain socket, remove it from the filesystem
            if os.path.exists(sockname):
                os.remove(sockname)
    self.sock = None
",if os . path . exists ( sockname ) :,128
"def to_nurbs(self, curves):
    result = []
    for i, c in enumerate(curves):
        nurbs = SvNurbsCurve.to_nurbs(c)
        if nurbs is None:
            raise Exception(f""Curve #{i} - {c} - can not be converted to NURBS!"")
        result.append(nurbs)
    return result
",if nurbs is None :,102
"def handle_1_roomid_raffle(self, i):
    if i[1] in [""handle_1_room_TV"", ""handle_1_room_captain""]:
        if await self.notify(""check_if_normal_room"", i[0], -1):
            await self.notify(""post_watching_history"", i[0])
            await self.notify(i[1], i[0], i[2])
    else:
        print(""hhjjkskddrsfvsfdfvdfvvfdvdvdfdfffdfsvh"", i)
","if await self . notify ( ""check_if_normal_room"" , i [ 0 ] , - 1 ) :",144
"def init_ps_var_partition(self):
    ps_vars = {}
    for v in self._non_embed_vars.values():
        if v.name not in self._var_to_ps:
            self._var_to_ps[v.name] = string_to_id(v.name, self._ps_num)
        ps_id = self._var_to_ps[v.name]
        if ps_id not in ps_vars:
            ps_vars[ps_id] = [v]
        else:
            ps_vars[ps_id].append(v)
    self._ps_vars = ps_vars
",if ps_id not in ps_vars :,164
"def get_files(d):
    f = []
    for root, dirs, files in os.walk(d):
        for name in files:
            if ""meta-environment"" in root or ""cross-canadian"" in root:
                continue
            if ""qemux86copy-"" in root or ""qemux86-"" in root:
                continue
            if ""do_build"" not in name and ""do_populate_sdk"" not in name:
                f.append(os.path.join(root, name))
    return f
","if ""do_build"" not in name and ""do_populate_sdk"" not in name :",143
"def setSelectedLabelState(self, p):  # selected, disabled
    c = self.c
    # g.trace(p,c.edit_widget(p))
    if p and c.edit_widget(p):
        if 0:
            g.trace(self.trace_n, c.edit_widget(p), p)
            # g.trace(g.callers(6))
            self.trace_n += 1
        self.setDisabledHeadlineColors(p)
",if 0 :,122
"def filter_tasks(self, task_types=None, task_states=None, task_text=None):
    tasks = self.api.tasks(self.id).get(""tasks"", {})
    if tasks and tasks.get(""task""):
        return [
            Task(self, task)
            for task in tasks.get(""task"", [])
            if (not task_types or task[""type""].lower() in task_types)
            and (not task_states or task[""state""].lower() in task_states)
            and (not task_text or task_text.lower() in str(task).lower())
        ]
    else:
        return []
","if ( not task_types or task [ ""type"" ] . lower ( ) in task_types )",166
"def GenerateVector(self, hits, vector, level):
    """"""Generate possible hit vectors which match the rules.""""""
    for item in hits.get(level, []):
        if vector:
            if item < vector[-1]:
                continue
            if item > self.max_separation + vector[-1]:
                break
        new_vector = vector + [item]
        if level + 1 == len(hits):
            yield new_vector
        elif level + 1 < len(hits):
            for result in self.GenerateVector(hits, new_vector, level + 1):
                yield result
",if vector :,157
"def _transmit_from_storage(self) -> None:
    for blob in self.storage.gets():
        # give a few more seconds for blob lease operation
        # to reduce the chance of race (for perf consideration)
        if blob.lease(self._timeout + 5):
            envelopes = [TelemetryItem(**x) for x in blob.get()]
            result = self._transmit(list(envelopes))
            if result == ExportResult.FAILED_RETRYABLE:
                blob.lease(1)
            else:
                blob.delete()
",if result == ExportResult . FAILED_RETRYABLE :,147
"def load_dictionary(file):
    oui = {}
    with open(file, ""r"") as f:
        for line in f:
            if ""(hex)"" in line:
                data = line.split(""(hex)"")
                key = data[0].replace(""-"", "":"").lower().strip()
                company = data[1].strip()
                oui[key] = company
    return oui
","if ""(hex)"" in line :",108
"def _yield_minibatches_idx(self, rgen, n_batches, data_ary, shuffle=True):
    indices = np.arange(data_ary.shape[0])
    if shuffle:
        indices = rgen.permutation(indices)
    if n_batches > 1:
        remainder = data_ary.shape[0] % n_batches
        if remainder:
            minis = np.array_split(indices[:-remainder], n_batches)
            minis[-1] = np.concatenate((minis[-1], indices[-remainder:]), axis=0)
        else:
            minis = np.array_split(indices, n_batches)
    else:
        minis = (indices,)
    for idx_batch in minis:
        yield idx_batch
",if remainder :,194
"def canonical_custom_headers(self, headers):
    hoi = []
    custom_headers = {}
    for key in headers:
        lk = key.lower()
        if headers[key] is not None:
            if lk.startswith(""x-amz-""):
                custom_headers[lk] = "","".join(v.strip() for v in headers.get_all(key))
    sorted_header_keys = sorted(custom_headers.keys())
    for key in sorted_header_keys:
        hoi.append(""%s:%s"" % (key, custom_headers[key]))
    return ""\n"".join(hoi)
","if lk . startswith ( ""x-amz-"" ) :",158
"def validate(self, data):
    if not data.get(""reason""):
        # If reason is not provided, message is required and can not be
        # null or blank.
        message = data.get(""message"")
        if not message:
            if ""message"" not in data:
                msg = serializers.Field.default_error_messages[""required""]
            elif message is None:
                msg = serializers.Field.default_error_messages[""null""]
            else:
                msg = serializers.CharField.default_error_messages[""blank""]
            raise serializers.ValidationError({""message"": [msg]})
    return data
",elif message is None :,167
"def tearDown(self):
    try:
        os.chdir(self.cwd)
        if self.pythonexe != sys.executable:
            os.remove(self.pythonexe)
        test_support.rmtree(self.parent_dir)
    finally:
        BaseTestCase.tearDown(self)
",if self . pythonexe != sys . executable :,77
"def update(self, value, label):
    if self._disabled:
        return
    try:
        self._progress.value = value
        self._label.value = label
        if not self._displayed:
            self._displayed = True
            display_widget(self._widget)
    except Exception as e:
        self._disabled = True
        logger.exception(e)
        wandb.termwarn(""Unable to render progress bar, see the user log for details"")
",if not self . _displayed :,122
"def GetBinaryOperationBinder(self, op):
    with self._lock:
        if self._binaryOperationBinders.ContainsKey(op):
            return self._binaryOperationBinders[op]
        b = runtime.SymplBinaryOperationBinder(op)
        self._binaryOperationBinders[op] = b
    return b
",if self . _binaryOperationBinders . ContainsKey ( op ) :,83
"def apply(self, l, b, evaluation):
    ""FromDigits[l_, b_]""
    if l.get_head_name() == ""System`List"":
        value = Integer(0)
        for leaf in l.leaves:
            value = Expression(""Plus"", Expression(""Times"", value, b), leaf)
        return value
    elif isinstance(l, String):
        value = FromDigits._parse_string(l.get_string_value(), b)
        if value is None:
            evaluation.message(""FromDigits"", ""nlst"")
        else:
            return value
    else:
        evaluation.message(""FromDigits"", ""nlst"")
",if value is None :,163
"def hsconn_sender(self):
    while not self.stop_event.is_set():
        try:
            # Block, but timeout, so that we can exit the loop gracefully
            request = self.send_queue.get(True, 6.0)
            if self.socket is not None:
                # Socket got closed and set to None in another thread...
                self.socket.sendall(request)
            if self.send_queue is not None:
                self.send_queue.task_done()
        except queue.Empty:
            pass
        except OSError:
            self.stop_event.set()
",if self . send_queue is not None :,168
"def check_expected(result, expected, contains=False):
    if sys.version_info[0] >= 3:
        if isinstance(result, str):
            result = result.encode(""ascii"")
        if isinstance(expected, str):
            expected = expected.encode(""ascii"")
    resultlines = result.splitlines()
    expectedlines = expected.splitlines()
    if len(resultlines) != len(expectedlines):
        return False
    for rline, eline in zip(resultlines, expectedlines):
        if contains:
            if eline not in rline:
                return False
        else:
            if not rline.endswith(eline):
                return False
    return True
",if eline not in rline :,181
"def init_weights(self):
    """"""Initialize model weights.""""""
    for _, m in self.multi_deconv_layers.named_modules():
        if isinstance(m, nn.ConvTranspose2d):
            normal_init(m, std=0.001)
        elif isinstance(m, nn.BatchNorm2d):
            constant_init(m, 1)
    for m in self.multi_final_layers.modules():
        if isinstance(m, nn.Conv2d):
            normal_init(m, std=0.001, bias=0)
","if isinstance ( m , nn . ConvTranspose2d ) :",139
"def filter_rel_attrs(field_name, **rel_attrs):
    clean_dict = {}
    for k, v in rel_attrs.items():
        if k.startswith(field_name + ""__""):
            splitted_key = k.split(""__"")
            key = ""__"".join(splitted_key[1:])
            clean_dict[key] = v
        else:
            clean_dict[k] = v
    return clean_dict
","if k . startswith ( field_name + ""__"" ) :",114
"def cancel(self):
    with self._condition:
        if not self._cancelled and not self._final and self._previous_context_id:
            self._squash(
                state_root=self._previous_state_hash,
                context_ids=[self._previous_context_id],
                persist=False,
                clean_up=True,
            )
        self._cancelled = True
        self._condition.notify_all()
",if not self . _cancelled and not self . _final and self . _previous_context_id :,121
"def _get_level(levels, level_ref):
    if level_ref in levels:
        return levels.index(level_ref)
    if isinstance(level_ref, six.integer_types):
        if level_ref < 0:
            level_ref += len(levels)
        if not (0 <= level_ref < len(levels)):
            raise PatsyError(""specified level %r is out of range"" % (level_ref,))
        return level_ref
    raise PatsyError(""specified level %r not found"" % (level_ref,))
",if level_ref < 0 :,138
"def parse_node(self, node, alias_map=None, conv=None):
    sql, params, unknown = self._parse(node, alias_map, conv)
    if unknown and conv and params:
        params = [conv.db_value(i) for i in params]
    if isinstance(node, Node):
        if node._negated:
            sql = ""NOT %s"" % sql
        if node._alias:
            sql = "" "".join((sql, ""AS"", node._alias))
        if node._ordering:
            sql = "" "".join((sql, node._ordering))
    return sql, params
",if node . _alias :,155
"def parse_object_id(_, values):
    if values:
        for key in values:
            if key.endswith(""_id""):
                val = values[key]
                if len(val) > 10:
                    try:
                        values[key] = utils.ObjectIdSilent(val)
                    except:
                        values[key] = None
","if key . endswith ( ""_id"" ) :",108
"def TryMerge(self, d):
    while d.avail() > 0:
        tt = d.getVarInt32()
        if tt == 10:
            self.set_app_id(d.getPrefixedString())
            continue
        if tt == 16:
            self.set_max_rows(d.getVarInt32())
            continue
        if tt == 0:
            raise ProtocolBuffer.ProtocolBufferDecodeError
        d.skipData(tt)
",if tt == 0 :,124
"def has_invalid_cce(yaml_file, product_yaml=None):
    rule = yaml.open_and_macro_expand(yaml_file, product_yaml)
    if ""identifiers"" in rule and rule[""identifiers""] is not None:
        for i_type, i_value in rule[""identifiers""].items():
            if i_type[0:3] == ""cce"":
                if not checks.is_cce_value_valid(""CCE-"" + str(i_value)):
                    return True
    return False
","if i_type [ 0 : 3 ] == ""cce"" :",134
"def _generate_table(self, fromdesc, todesc, diffs):
    if fromdesc or todesc:
        yield (
            simple_colorize(fromdesc, ""description""),
            simple_colorize(todesc, ""description""),
        )
    for i, line in enumerate(diffs):
        if line is None:
            # mdiff yields None on separator lines; skip the bogus ones
            # generated for the first line
            if i > 0:
                yield (
                    simple_colorize(""---"", ""separator""),
                    simple_colorize(""---"", ""separator""),
                )
        else:
            yield line
",if i > 0 :,170
"def _getPatternTemplate(pattern, key=None):
    if key is None:
        key = pattern
        if ""%"" not in pattern:
            key = pattern.upper()
    template = DD_patternCache.get(key)
    if not template:
        if key in (""EPOCH"", ""{^LN-BEG}EPOCH"", ""^EPOCH""):
            template = DateEpoch(lineBeginOnly=(key != ""EPOCH""))
        elif key in (""TAI64N"", ""{^LN-BEG}TAI64N"", ""^TAI64N""):
            template = DateTai64n(wordBegin=(""start"" if key != ""TAI64N"" else False))
        else:
            template = DatePatternRegex(pattern)
    DD_patternCache.set(key, template)
    return template
","elif key in ( ""TAI64N"" , ""{^LN-BEG}TAI64N"" , ""^TAI64N"" ) :",195
"def ref_max_pooling_2d(x, kernel, stride, ignore_border, pad):
    y = []
    for xx in x.reshape((-1,) + x.shape[-3:]):
        if xx.ndim == 2:
            xx = xx[np.newaxis]
        y += [
            refs.pooling_2d(xx, ""max"", kernel, stride, pad, ignore_border)[np.newaxis]
        ]
    y = np.vstack(y)
    if x.ndim == 2:
        y = np.squeeze(y, 1)
    return y.reshape(x.shape[:-3] + y.shape[1:])
",if xx . ndim == 2 :,160
"def show_topics():
    """"""prints all available miscellaneous help topics.""""""
    print(_stash.text_color(""Miscellaneous Topics:"", ""yellow""))
    for pp in PAGEPATHS:
        if not os.path.isdir(pp):
            continue
        content = os.listdir(pp)
        for pn in content:
            if ""."" in pn:
                name = pn[: pn.index(""."")]
            else:
                name = pn
            print(name)
","if ""."" in pn :",125
"def justify_toggle_auto(self, event=None):
    c = self
    if c.editCommands.autojustify == 0:
        c.editCommands.autojustify = abs(c.config.getInt(""autojustify"") or 0)
        if c.editCommands.autojustify:
            g.es(""Autojustify on, @int autojustify == %s"" % c.editCommands.autojustify)
        else:
            g.es(""Set @int autojustify in @settings"")
    else:
        c.editCommands.autojustify = 0
        g.es(""Autojustify off"")
",if c . editCommands . autojustify :,153
"def render_token_list(self, tokens):
    result = []
    vars = []
    for token in tokens:
        if token.token_type == TOKEN_TEXT:
            result.append(token.contents.replace(""%"", ""%%""))
        elif token.token_type == TOKEN_VAR:
            result.append(""%%(%s)s"" % token.contents)
            vars.append(token.contents)
    return """".join(result), vars
",if token . token_type == TOKEN_TEXT :,113
"def get_target_dimensions(self):
    width, height = self.engine.size
    for operation in self.operations:
        if operation[""type""] == ""crop"":
            width = operation[""right""] - operation[""left""]
            height = operation[""bottom""] - operation[""top""]
        if operation[""type""] == ""resize"":
            width = operation[""width""]
            height = operation[""height""]
    return (width, height)
","if operation [ ""type"" ] == ""resize"" :",112
"def get_eval_matcher(self):
    if isinstance(self.data[""match""], str):
        if self.data[""match""] == ""denied"":
            values = [""explicitDeny"", ""implicitDeny""]
        else:
            values = [""allowed""]
        vf = ValueFilter(
            {""type"": ""value"", ""key"": ""EvalDecision"", ""value"": values, ""op"": ""in""}
        )
    else:
        vf = ValueFilter(self.data[""match""])
    vf.annotate = False
    return vf
","if self . data [ ""match"" ] == ""denied"" :",140
"def test_training(self):
    if not self.model_tester.is_training:
        return
    config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()
    config.return_dict = True
    for model_class in self.all_model_classes:
        if model_class in MODEL_MAPPING.values():
            continue
        model = model_class(config)
        model.to(torch_device)
        model.train()
        inputs = self._prepare_for_class(inputs_dict, model_class, return_labels=True)
        loss = model(**inputs).loss
        loss.backward()
",if model_class in MODEL_MAPPING . values ( ) :,168
"def prehook(self, emu, op, eip):
    if op in self.badops:
        emu.stopEmu()
        raise v_exc.BadOpBytes(op.va)
    if op.mnem in STOS:
        if self.arch == ""i386"":
            reg = emu.getRegister(envi.archs.i386.REG_EDI)
        elif self.arch == ""amd64"":
            reg = emu.getRegister(envi.archs.amd64.REG_RDI)
        if self.vw.isValidPointer(reg) and self.vw.getLocation(reg) is None:
            self.vw.makePointer(reg, follow=True)
","if self . arch == ""i386"" :",186
"def test_len(self):
    eq = self.assertEqual
    eq(base64mime.base64_len(""hello""), len(base64mime.encode(""hello"", eol="""")))
    for size in range(15):
        if size == 0:
            bsize = 0
        elif size <= 3:
            bsize = 4
        elif size <= 6:
            bsize = 8
        elif size <= 9:
            bsize = 12
        elif size <= 12:
            bsize = 16
        else:
            bsize = 20
        eq(base64mime.base64_len(""x"" * size), bsize)
",if size == 0 :,160
"def __new__(cls, dependencies):
    deps = check.list_param(dependencies, ""dependencies"", of_type=DependencyDefinition)
    seen = {}
    for dep in deps:
        key = dep.solid + "":"" + dep.output
        if key in seen:
            raise DagsterInvalidDefinitionError(
                'Duplicate dependencies on solid ""{dep.solid}"" output ""{dep.output}"" '
                ""used in the same MultiDependencyDefinition."".format(dep=dep)
            )
        seen[key] = True
    return super(MultiDependencyDefinition, cls).__new__(cls, deps)
",if key in seen :,149
"def get_explanation(self, spec):
    """"""Expand an explanation.""""""
    if spec:
        try:
            a = self.dns_txt(spec)
            if len(a) == 1:
                return str(self.expand(to_ascii(a[0]), stripdot=False))
        except PermError:
            # RFC4408 6.2/4 syntax errors cause exp= to be ignored
            if self.strict > 1:
                raise  # but report in harsh mode for record checking tools
            pass
    elif self.strict > 1:
        raise PermError(""Empty domain-spec on exp="")
    # RFC4408 6.2/4 empty domain spec is ignored
    # (unless you give precedence to the grammar).
    return None
",if self . strict > 1 :,200
"def build(self):
    if self.args.get(""sle_id""):
        self.process_sle_against_current_voucher()
    else:
        entries_to_fix = self.get_future_entries_to_fix()
        i = 0
        while i < len(entries_to_fix):
            sle = entries_to_fix[i]
            i += 1
            self.process_sle(sle)
            if sle.dependant_sle_voucher_detail_no:
                self.get_dependent_entries_to_fix(entries_to_fix, sle)
    if self.exceptions:
        self.raise_exceptions()
    self.update_bin()
",if sle . dependant_sle_voucher_detail_no :,187
"def ValidateStopLatitude(self, problems):
    if self.stop_lat is not None:
        value = self.stop_lat
        try:
            if not isinstance(value, (float, int)):
                self.stop_lat = util.FloatStringToFloat(value, problems)
        except (ValueError, TypeError):
            problems.InvalidValue(""stop_lat"", value)
            del self.stop_lat
        else:
            if self.stop_lat > 90 or self.stop_lat < -90:
                problems.InvalidValue(""stop_lat"", value)
",if self . stop_lat > 90 or self . stop_lat < - 90 :,153
"def set(self, obj, **kwargs):
    """"""Check for missing event functions and substitute these with""""""
    """"""the ignore method""""""
    ignore = getattr(self, ""ignore"")
    for k, v in kwargs.iteritems():
        setattr(self, k, getattr(obj, v))
        if k in self.combinations:
            for k1 in self.combinations[k]:
                if not hasattr(self, k1):
                    setattr(self, k1, ignore)
",if k in self . combinations :,121
"def split(self, duration, include_remainder=True):
    # Convert seconds to timedelta, if appropriate.
    duration = _seconds_or_timedelta(duration)
    if duration <= timedelta(seconds=0):
        raise ValueError(""cannot call split with a non-positive timedelta"")
    start = self.start
    while start < self.end:
        if start + duration <= self.end:
            yield MayaInterval(start, start + duration)
        elif include_remainder:
            yield MayaInterval(start, self.end)
        start += duration
",elif include_remainder :,137
"def get_first_field(layout, clz):
    for layout_object in layout.fields:
        if issubclass(layout_object.__class__, clz):
            return layout_object
        elif hasattr(layout_object, ""get_field_names""):
            gf = get_first_field(layout_object, clz)
            if gf:
                return gf
","elif hasattr ( layout_object , ""get_field_names"" ) :",94
"def _getPatternTemplate(pattern, key=None):
    if key is None:
        key = pattern
        if ""%"" not in pattern:
            key = pattern.upper()
    template = DD_patternCache.get(key)
    if not template:
        if key in (""EPOCH"", ""{^LN-BEG}EPOCH"", ""^EPOCH""):
            template = DateEpoch(lineBeginOnly=(key != ""EPOCH""))
        elif key in (""TAI64N"", ""{^LN-BEG}TAI64N"", ""^TAI64N""):
            template = DateTai64n(wordBegin=(""start"" if key != ""TAI64N"" else False))
        else:
            template = DatePatternRegex(pattern)
    DD_patternCache.set(key, template)
    return template
","if key in ( ""EPOCH"" , ""{^LN-BEG}EPOCH"" , ""^EPOCH"" ) :",195
"def findOwningViewController(self, object):
    while object:
        if self.isViewController(object):
            description = fb.evaluateExpressionValue(object).GetObjectDescription()
            print(""Found the owning view controller.\n{}"".format(description))
            cmd = 'echo {} | tr -d ""\n"" | pbcopy'.format(object)
            os.system(cmd)
            return
        else:
            object = self.nextResponder(object)
    print(""Could not find an owning view controller"")
",if self . isViewController ( object ) :,141
"def __get_file_by_num(self, num, file_list, idx=0):
    for element in file_list:
        if idx == num:
            return element
        if element[3] and element[4]:
            i = self.__get_file_by_num(num, element[3], idx + 1)
            if not isinstance(i, int):
                return i
            idx = i
        else:
            idx += 1
    return idx
","if not isinstance ( i , int ) :",127
"def promtool(**kwargs):
    key = ""prometheus:promtool""
    try:
        path = pathlib.Path(util.setting(key))
    except TypeError:
        yield checks.Warning(
            ""Missing setting for %s in %s "" % (key, settings.PROMGEN_CONFIG_FILE),
            id=""promgen.W001"",
        )
    else:
        if not os.access(path, os.X_OK):
            yield checks.Warning(""Unable to execute file %s"" % path, id=""promgen.W003"")
","if not os . access ( path , os . X_OK ) :",141
"def parse_config(schema, config):
    schemaparser = ConfigParser()
    schemaparser.readfp(StringIO(schema))
    cfgparser = ConfigParser()
    cfgparser.readfp(StringIO(config))
    result = {}
    for section in cfgparser.sections():
        result_section = {}
        schema = {}
        if section in schemaparser.sections():
            schema = dict(schemaparser.items(section))
        for key, value in cfgparser.items(section):
            converter = converters[schema.get(key, ""string"")]
            result_section[key] = converter(value)
        result[section] = result_section
    return result
",if section in schemaparser . sections ( ) :,165
"def validate_arguments(args):
    if args.num_pss < 1:
        print(""Value error: must have ore than one parameter servers."")
        exit(1)
    if not GPU_IDS:
        num_cpus = multiprocessing.cpu_count()
        if args.cpu_trainers > num_cpus:
            print(
                ""Value error: there are %s available CPUs but you are requiring %s.""
                % (num_cpus, args.cpu_trainers)
            )
            exit(1)
    if not os.path.isfile(args.file):
        print(""Value error: model trainning file does not exist"")
        exit(1)
",if args . cpu_trainers > num_cpus :,177
"def infer_dataset_impl(path):
    if IndexedRawTextDataset.exists(path):
        return ""raw""
    elif IndexedDataset.exists(path):
        with open(index_file_path(path), ""rb"") as f:
            magic = f.read(8)
            if magic == IndexedDataset._HDR_MAGIC:
                return ""cached""
            elif magic == MMapIndexedDataset.Index._HDR_MAGIC[:8]:
                return ""mmap""
            else:
                return None
    elif FastaDataset.exists(path):
        return ""fasta""
    else:
        return None
",elif magic == MMapIndexedDataset . Index . _HDR_MAGIC [ : 8 ] :,167
"def _add_resource_group(obj):
    if isinstance(obj, list):
        for array_item in obj:
            _add_resource_group(array_item)
    elif isinstance(obj, dict):
        try:
            if ""resourcegroup"" not in [x.lower() for x in obj.keys()]:
                if obj[""id""]:
                    obj[""resourceGroup""] = _parse_id(obj[""id""])[""resource-group""]
        except (KeyError, IndexError, TypeError):
            pass
        for item_key in obj:
            if item_key != ""sourceVault"":
                _add_resource_group(obj[item_key])
","if obj [ ""id"" ] :",175
"def reformatBody(self, event=None):
    """"""Reformat all paragraphs in the body.""""""
    c, p = self, self.p
    undoType = ""reformat-body""
    w = c.frame.body.wrapper
    c.undoer.beforeChangeGroup(p, undoType)
    w.setInsertPoint(0)
    while 1:
        progress = w.getInsertPoint()
        c.reformatParagraph(event, undoType=undoType)
        ins = w.getInsertPoint()
        s = w.getAllText()
        w.setInsertPoint(ins)
        if ins <= progress or ins >= len(s):
            break
    c.undoer.afterChangeGroup(p, undoType)
",if ins <= progress or ins >= len ( s ) :,181
"def make_sources(project: RootDependency) -> str:
    content = []
    if project.readme:
        content.append(project.readme.path.name)
        if project.readme.markup != ""rst"":
            content.append(project.readme.to_rst().path.name)
    path = project.package.path
    for fname in (""setup.cfg"", ""setup.py""):
        if (path / fname).exists():
            content.append(fname)
    for package in chain(project.package.packages, project.package.data):
        for fpath in package:
            fpath = fpath.relative_to(project.package.path)
            content.append(""/"".join(fpath.parts))
    return ""\n"".join(content)
","if project . readme . markup != ""rst"" :",193
"def __init__(self, response):
    error = ""{} {}"".format(response.status_code, response.reason)
    extra = []
    try:
        response_json = response.json()
        if ""error_list"" in response_json:
            error = "" "".join(error[""message""] for error in response_json[""error_list""])
            extra = [
                error[""extra""]
                for error in response_json[""error_list""]
                if ""extra"" in error
            ]
    except JSONDecodeError:
        pass
    super().__init__(response=response, error=error, extra=extra)
","if ""error_list"" in response_json :",161
"def handle_event(self, fileno=None, events=None):
    if self._state == RUN:
        if self._it is None:
            self._it = self._process_result(0)  # non-blocking
        try:
            next(self._it)
        except (StopIteration, CoroStop):
            self._it = None
",if self . _it is None :,92
"def find_query(self, needle, haystack):
    try:
        import pinyin
        haystack_py = pinyin.get_initial(haystack, """")
        needle_len = len(needle)
        start = 0
        result = []
        while True:
            found = haystack_py.find(needle, start)
            if found < 0:
                break
            result.append((found, needle_len))
            start = found + needle_len
        return result
    except:
        return None
",if found < 0 :,136
"def decorated_function(*args, **kwargs):
    rv = f(*args, **kwargs)
    if ""Last-Modified"" not in rv.headers:
        try:
            result = date
            if callable(result):
                result = result(rv)
            if not isinstance(result, basestring):
                from werkzeug.http import http_date
                result = http_date(result)
            if result:
                rv.headers[""Last-Modified""] = result
        except Exception:
            logging.getLogger(__name__).exception(
                ""Error while calculating the lastmodified value for response {!r}"".format(
                    rv
                )
            )
    return rv
",if result :,189
"def check_require(require_modules, require_lines):
    for require_module in require_modules:
        st = try_import(require_module)
        if st == 0:
            continue
        elif st == 1:
            print(
                ""installed {}: {}\n"".format(
                    require_module, require_lines[require_module]
                )
            )
        elif st == 2:
            print(
                ""failed installed {}: {}\n"".format(
                    require_module, require_lines[require_module]
                )
            )
",elif st == 1 :,164
"def bundle_directory(self, dirpath):
    """"""Bundle all modules/packages in the given directory.""""""
    dirpath = os.path.abspath(dirpath)
    for nm in os.listdir(dirpath):
        nm = _u(nm)
        if nm.startswith("".""):
            continue
        itempath = os.path.join(dirpath, nm)
        if os.path.isdir(itempath):
            if os.path.exists(os.path.join(itempath, ""__init__.py"")):
                self.bundle_package(itempath)
        elif nm.endswith("".py""):
            self.bundle_module(itempath)
","elif nm . endswith ( "".py"" ) :",160
"def _find_root():
    test_dirs = [""Src"", ""Build"", ""Package"", ""Tests"", ""Util""]
    root = os.getcwd()
    test = all([os.path.exists(os.path.join(root, x)) for x in test_dirs])
    while not test:
        last_root = root
        root = os.path.dirname(root)
        if root == last_root:
            raise Exception(""Root not found"")
        test = all([os.path.exists(os.path.join(root, x)) for x in test_dirs])
    return root
",if root == last_root :,148
"def findMarkForUnitTestNodes(self):
    """"""return the position of *all* non-ignored @mark-for-unit-test nodes.""""""
    c = self.c
    p, result, seen = c.rootPosition(), [], []
    while p:
        if p.v in seen:
            p.moveToNodeAfterTree()
        else:
            seen.append(p.v)
            if g.match_word(p.h, 0, ""@ignore""):
                p.moveToNodeAfterTree()
            elif p.h.startswith(""@mark-for-unit-tests""):
                result.append(p.copy())
                p.moveToNodeAfterTree()
            else:
                p.moveToThreadNext()
    return result
","elif p . h . startswith ( ""@mark-for-unit-tests"" ) :",200
"def startTagFrameset(self, token):
    self.parser.parseError(""unexpected-start-tag"", {""name"": ""frameset""})
    if len(self.tree.openElements) == 1 or self.tree.openElements[1].name != ""body"":
        assert self.parser.innerHTML
    elif not self.parser.framesetOK:
        pass
    else:
        if self.tree.openElements[1].parent:
            self.tree.openElements[1].parent.removeChild(self.tree.openElements[1])
        while self.tree.openElements[-1].name != ""html"":
            self.tree.openElements.pop()
        self.tree.insertElement(token)
        self.parser.phase = self.parser.phases[""inFrameset""]
",if self . tree . openElements [ 1 ] . parent :,193
"def try_split(self, split_text: List[str]):
    ret = []
    for i in split_text:
        if len(i) == 0:
            continue
        val = int(i, 2)
        if val > 255 or val < 0:
            return None
        ret.append(val)
    if len(ret) != 0:
        ret = bytes(ret)
        logger.debug(f""binary successful, returning {ret.__repr__()}"")
        return ret
",if len ( i ) == 0 :,127
"def generator(self, data):
    for sock in data:
        if not self._config.PHYSICAL_OFFSET:
            offset = sock.obj_offset
        else:
            offset = sock.obj_vm.vtop(sock.obj_offset)
        yield (
            0,
            [
                Address(offset),
                int(sock.Pid),
                int(sock.LocalPort),
                int(sock.Protocol),
                str(protos.protos.get(sock.Protocol.v(), ""-"")),
                str(sock.LocalIpAddress),
                str(sock.CreateTime),
            ],
        )
",if not self . _config . PHYSICAL_OFFSET :,181
"def __init__(self, num_bits=4, always_apply=False, p=0.5):
    super(Posterize, self).__init__(always_apply, p)
    if isinstance(num_bits, (list, tuple)):
        if len(num_bits) == 3:
            self.num_bits = [to_tuple(i, 0) for i in num_bits]
        else:
            self.num_bits = to_tuple(num_bits, 0)
    else:
        self.num_bits = to_tuple(num_bits, num_bits)
",if len ( num_bits ) == 3 :,146
"def tearDown(self):
    """"""Just in case yn00 creates some junk files, do a clean-up.""""""
    del_files = [self.out_file, ""2YN.dN"", ""2YN.dS"", ""2YN.t"", ""rst"", ""rst1"", ""rub""]
    for filename in del_files:
        if os.path.exists(filename):
            os.remove(filename)
    if os.path.exists(self.working_dir):
        for filename in os.listdir(self.working_dir):
            filepath = os.path.join(self.working_dir, filename)
            os.remove(filepath)
        os.rmdir(self.working_dir)
",if os . path . exists ( filename ) :,179
"def reverse_search_history(self, searchfor, startpos=None):
    if startpos is None:
        startpos = self.history_cursor
    if _ignore_leading_spaces:
        res = [
            (idx, line.lstrip())
            for idx, line in enumerate(self.history[startpos:0:-1])
            if line.lstrip().startswith(searchfor.lstrip())
        ]
    else:
        res = [
            (idx, line)
            for idx, line in enumerate(self.history[startpos:0:-1])
            if line.startswith(searchfor)
        ]
    if res:
        self.history_cursor -= res[0][0]
        return res[0][1].get_line_text()
    return """"
",if line . lstrip ( ) . startswith ( searchfor . lstrip ( ) ),198
"def ComboBoxDroppedHeightTest(windows):
    ""Check if each combobox height is the same as the reference""
    bugs = []
    for win in windows:
        if not win.ref:
            continue
        if win.Class() != ""ComboBox"" or win.ref.Class() != ""ComboBox"":
            continue
        if win.DroppedRect().height() != win.ref.DroppedRect().height():
            bugs.append(
                (
                    [
                        win,
                    ],
                    {},
                    testname,
                    0,
                )
            )
    return bugs
","if win . Class ( ) != ""ComboBox"" or win . ref . Class ( ) != ""ComboBox"" :",181
"def get_changed(self):
    if self._is_expression():
        result = self._get_node_text(self.ast)
        if result == self.source:
            return None
        return result
    else:
        collector = codeanalyze.ChangeCollector(self.source)
        last_end = -1
        for match in self.matches:
            start, end = match.get_region()
            if start < last_end:
                if not self._is_expression():
                    continue
            last_end = end
            replacement = self._get_matched_text(match)
            collector.add_change(start, end, replacement)
        return collector.get_changed()
",if start < last_end :,189
"def unpickle_from_file(file_path, gzip=False):
    """"""Unpickle obj from file_path with gzipping.""""""
    with tf.io.gfile.GFile(file_path, ""rb"") as f:
        if not gzip:
            obj = pickle.load(f)
        else:
            with gzip_lib.GzipFile(fileobj=f, compresslevel=2) as gzipf:
                obj = pickle.load(gzipf)
    return obj
",if not gzip :,116
"def get_user_context(request, escape=False):
    if isinstance(request, HttpRequest):
        user = getattr(request, ""user"", None)
        result = {""ip_address"": request.META[""REMOTE_ADDR""]}
        if user and user.is_authenticated():
            result.update(
                {
                    ""email"": user.email,
                    ""id"": user.id,
                }
            )
            if user.name:
                result[""name""] = user.name
    else:
        result = {}
    return mark_safe(json.dumps(result))
",if user . name :,163
"def get_item_address(self, item):
    """"""Get an item's address as a collection of names""""""
    result = []
    while True:
        name = self.tree_ctrl.GetItemPyData(item)
        if name is None:
            break
        else:
            result.insert(0, name)
            item = self.tree_ctrl.GetItemParent(item)
    return result
",if name is None :,104
"def closest_unseen(self, row1, col1, filter=None):
    # find the closest unseen from this row/col
    min_dist = maxint
    closest_unseen = None
    for row in range(self.height):
        for col in range(self.width):
            if filter is None or (row, col) not in filter:
                if self.map[row][col] == UNSEEN:
                    dist = self.distance(row1, col1, row, col)
                    if dist < min_dist:
                        min_dist = dist
                        closest_unseen = (row, col)
    return closest_unseen
",if self . map [ row ] [ col ] == UNSEEN :,174
"def log_graph(self, model: LightningModule, input_array=None):
    if self._log_graph:
        if input_array is None:
            input_array = model.example_input_array
        if input_array is not None:
            input_array = model._apply_batch_transfer_handler(input_array)
            self.experiment.add_graph(model, input_array)
        else:
            rank_zero_warn(
                ""Could not log computational graph since the""
                "" `model.example_input_array` attribute is not set""
                "" or `input_array` was not given"",
                UserWarning,
            )
",if input_array is None :,182
"def get_scene_exceptions_by_season(self, season=-1):
    scene_exceptions = []
    for scene_exception in self.scene_exceptions:
        if not len(scene_exception) == 2:
            continue
        scene_name, scene_season = scene_exception.split(""|"")
        if season == scene_season:
            scene_exceptions.append(scene_name)
    return scene_exceptions
",if season == scene_season :,121
"def _clean_temp_files():
    for pattern in _temp_files:
        for path in glob.glob(pattern):
            if os.path.islink(path) or os.path.isfile(path):
                os.remove(path)
            else:
                shutil.rmtree(path)
",if os . path . islink ( path ) or os . path . isfile ( path ) :,81
"def wait_for_completion(self, job_id, offset, max_results, start_time, timeout):
    """"""Wait for job completion and return the first page.""""""
    while True:
        result = self.get_query_results(
            job_id=job_id, page_token=None, start_index=offset, max_results=max_results
        )
        if result[""jobComplete""]:
            return result
        if (time.time() - start_time) > timeout:
            raise Exception(
                ""Timeout: the query doesn't finish within %d seconds."" % timeout
            )
        time.sleep(1)
","if result [ ""jobComplete"" ] :",165
"def get_data(self, element, ranges, style):
    if element.kdims:
        groups = element.groupby(element.kdims).items()
    else:
        groups = [(element.label, element)]
    plots = []
    axis = ""x"" if self.invert_axes else ""y""
    for key, group in groups:
        if element.kdims:
            label = "","".join([d.pprint_value(v) for d, v in zip(element.kdims, key)])
        else:
            label = key
        data = {axis: group.dimension_values(group.vdims[0]), ""name"": label}
        plots.append(data)
    return plots
",if element . kdims :,176
"def get_files(self, dirname):
    if not self._data.has_key(dirname):
        self._create(dirname)
    else:
        new_time = self._changed(dirname)
        if new_time:
            self._update(dirname, new_time)
            dcLog.debug(""==> "" + ""\t\n"".join(self._data[dirname][""flist""]))
    return self._data[dirname][""flist""]
",if new_time :,112
"def __init__(self, dir):
    self.module_names = set()
    for name in os.listdir(dir):
        if name.endswith("".py""):
            self.module_names.add(name[:-3])
        elif ""."" not in name:
            self.module_names.add(name)
","if name . endswith ( "".py"" ) :",79
"def logic():
    for i in range(100):
        yield clock.posedge, reset.negedge
        if reset == ACTIVE_LOW:
            count.next = 0
        else:
            if enable:
                count.next = (count + 1) % n
    raise StopSimulation
",if reset == ACTIVE_LOW :,81
"def sortkeypicker(keynames):
    negate = set()
    for i, k in enumerate(keynames):
        if k[:1] == ""-"":
            keynames[i] = k[1:]
            negate.add(k[1:])
    def getit(adict):
        composite = [adict[k] for k in keynames]
        for i, (k, v) in enumerate(zip(keynames, composite)):
            if k in negate:
                composite[i] = -v
        return composite
    return getit
",if k in negate :,140
"def show_image(self, wnd_name, img):
    if wnd_name in self.named_windows:
        if self.named_windows[wnd_name] == 0:
            self.named_windows[wnd_name] = 1
            self.on_create_window(wnd_name)
            if wnd_name in self.capture_mouse_windows:
                self.capture_mouse(wnd_name)
        self.on_show_image(wnd_name, img)
    else:
        print(""show_image: named_window "", wnd_name, "" not found."")
",if self . named_windows [ wnd_name ] == 0 :,159
"def check_action_permitted(self):
    if (
        self._action == ""sts:GetCallerIdentity""
    ):  # always allowed, even if there's an explicit Deny for it
        return True
    policies = self._access_key.collect_policies()
    permitted = False
    for policy in policies:
        iam_policy = IAMPolicy(policy)
        permission_result = iam_policy.is_action_permitted(self._action)
        if permission_result == PermissionResult.DENIED:
            self._raise_access_denied()
        elif permission_result == PermissionResult.PERMITTED:
            permitted = True
    if not permitted:
        self._raise_access_denied()
",elif permission_result == PermissionResult . PERMITTED :,184
"def _limit_value(key, value, config):
    if config[key].get(""upper_limit""):
        limit = config[key][""upper_limit""]
        # auto handle datetime
        if isinstance(value, datetime) and isinstance(limit, timedelta):
            if config[key][""inverse""] is True:
                if (datetime.now() - limit) > value:
                    value = datetime.now() - limit
            else:
                if (datetime.now() + limit) < value:
                    value = datetime.now() + limit
        elif value > limit:
            value = limit
    return value
",if ( datetime . now ( ) - limit ) > value :,164
"def replace_dataset_ids(path, key, value):
    """"""Exchanges dataset_ids (HDA, LDA, HDCA, not Dataset) in input_values with dataset ids used in job.""""""
    current_case = input_values
    if key == ""id"":
        for i, p in enumerate(path):
            if isinstance(current_case, (list, dict)):
                current_case = current_case[p]
        if src == current_case.get(""src""):
            return key, translate_values.get(current_case[""id""], value)
    return key, value
","if src == current_case . get ( ""src"" ) :",148
"def load_ext(name, funcs):
    ExtModule = namedtuple(""ExtModule"", funcs)
    ext_list = []
    lib_root = os.path.dirname(os.path.dirname(os.path.realpath(__file__)))
    for fun in funcs:
        if fun in [""nms"", ""softnms""]:
            ext_list.append(extension.load(fun, name, lib_dir=lib_root).op)
        else:
            ext_list.append(extension.load(fun, name, lib_dir=lib_root).op_)
    return ExtModule(*ext_list)
","if fun in [ ""nms"" , ""softnms"" ] :",146
"def execute_action(self):
    selected_actions = self.model_action.get_selected_results_with_index()
    if selected_actions and self.args_for_action:
        for name, _, act_idx in selected_actions:
            try:
                action = self.actions[act_idx]
                if action:
                    action.act([arg for arg, _, _ in self.args_for_action], self)
            except Exception as e:
                debug.log(""execute_action"", e)
",if action :,140
"def __getattr__(self, attr):
    proxy = self.__proxy
    if proxy and hasattr(proxy, attr):
        return getattr(proxy, attr)
    attrmap = self.__attrmap
    if attr in attrmap:
        source = attrmap[attr]
        if callable(source):
            value = source()
        else:
            value = _import_object(source)
        setattr(self, attr, value)
        self.__log.debug(""loaded lazy attr %r: %r"", attr, value)
        return value
    raise AttributeError(""'module' object has no attribute '%s'"" % (attr,))
",if callable ( source ) :,154
"def forward(self, x):
    # BxT -> BxCxT
    x = x.unsqueeze(1)
    for conv in self.conv_layers:
        residual = x
        x = conv(x)
        if self.skip_connections and x.size(1) == residual.size(1):
            tsz = x.size(2)
            r_tsz = residual.size(2)
            residual = residual[..., :: r_tsz // tsz][..., :tsz]
            x = (x + residual) * self.residual_scale
    if self.log_compression:
        x = x.abs()
        x = x + 1
        x = x.log()
    return x
",if self . skip_connections and x . size ( 1 ) == residual . size ( 1 ) :,186
"def __Prefix_Step2a(self, token):
    for prefix in self.__prefix_step2a:
        if token.startswith(prefix) and len(token) > 5:
            token = token[len(prefix) :]
            self.prefix_step2a_success = True
            break
    return token
",if token . startswith ( prefix ) and len ( token ) > 5 :,81
"def is_valid(sample):
    if sample is None:
        return False
    if isinstance(sample, tuple):
        for s in sample:
            if s is None:
                return False
            elif isinstance(s, np.ndarray) and s.size == 0:
                return False
            elif isinstance(s, collections.abc.Sequence) and len(s) == 0:
                return False
    return True
","elif isinstance ( s , collections . abc . Sequence ) and len ( s ) == 0 :",114
"def get_all_comments(self, gallery_id, post_no, comment_cnt):
    comment_page_cnt = (comment_cnt - 1) // self.options.comments_per_page + 1
    comments = []
    headers = {""X-Requested-With"": ""XMLHttpRequest""}
    data = {""ci_t"": self._session.cookies[""ci_c""], ""id"": gallery_id, ""no"": post_no}
    for i in range(comment_page_cnt):
        data[""comment_page""] = i + 1
        response = self.request_comment(headers, data)
        batch = self.parse_comments(response.text)
        if not batch:
            break
        comments = batch + comments
    return comments
",if not batch :,187
"def run_on_module(self):
    try:
        self.module_base.disable(self.opts.module_spec)
    except dnf.exceptions.MarkingErrors as e:
        if self.base.conf.strict:
            if e.no_match_group_specs or e.error_group_specs:
                raise e
            if (
                e.module_depsolv_errors
                and e.module_depsolv_errors[1]
                != libdnf.module.ModulePackageContainer.ModuleErrorType_ERROR_IN_DEFAULTS
            ):
                raise e
        logger.error(str(e))
",if self . base . conf . strict :,174
"def find_field_notnull_differ(self, meta, table_description, table_name):
    if not self.can_detect_notnull_differ:
        return
    for field in all_local_fields(meta):
        attname = field.db_column or field.attname
        if (table_name, attname) in self.new_db_fields:
            continue
        null = self.get_field_db_nullable(field, table_name)
        if field.null != null:
            action = field.null and ""DROP"" or ""SET""
            self.add_difference(""notnull-differ"", table_name, attname, action)
","if ( table_name , attname ) in self . new_db_fields :",167
"def _change_moving_module(self, changes, dest):
    if not self.source.is_folder():
        pymodule = self.pycore.resource_to_pyobject(self.source)
        source = self.import_tools.relatives_to_absolutes(pymodule)
        pymodule = self.tools.new_pymodule(pymodule, source)
        source = self._change_occurrences_in_module(dest, pymodule)
        source = self.tools.new_source(pymodule, source)
        if source != self.source.read():
            changes.add_change(ChangeContents(self.source, source))
",if source != self . source . read ( ) :,155
"def get(quality_name):
    """"""Returns a quality object based on canonical quality name.""""""
    found_components = {}
    for part in quality_name.lower().split():
        component = _registry.get(part)
        if not component:
            raise ValueError(""`%s` is not a valid quality string"" % part)
        if component.type in found_components:
            raise ValueError(
                ""`%s` cannot be defined twice in a quality"" % component.type
            )
        found_components[component.type] = component
    if not found_components:
        raise ValueError(""No quality specified"")
    result = Quality()
    for type, component in found_components.items():
        setattr(result, type, component)
    return result
",if not component :,191
"def _unselected(self):
    selected = self._selected
    k = 0
    z = selected[k]
    k += 1
    for i in range(self._n):
        if i == z:
            if k < len(selected):
                z = selected[k]
                k += 1
            else:
                z = -1
        else:
            yield i
",if k < len ( selected ) :,107
"def render_headers(self) -> bytes:
    if not hasattr(self, ""_headers""):
        parts = [
            b""Content-Disposition: form-data; "",
            format_form_param(""name"", self.name),
        ]
        if self.filename:
            filename = format_form_param(""filename"", self.filename)
            parts.extend([b""; "", filename])
        if self.content_type is not None:
            content_type = self.content_type.encode()
            parts.extend([b""\r\nContent-Type: "", content_type])
        parts.append(b""\r\n\r\n"")
        self._headers = b"""".join(parts)
    return self._headers
",if self . filename :,189
"def app_middleware(next, root, info, **kwargs):
    app_auth_header = ""HTTP_AUTHORIZATION""
    prefix = ""bearer""
    request = info.context
    if request.path == API_PATH:
        if not hasattr(request, ""app""):
            request.app = None
            auth = request.META.get(app_auth_header, """").split()
            if len(auth) == 2:
                auth_prefix, auth_token = auth
                if auth_prefix.lower() == prefix:
                    request.app = SimpleLazyObject(lambda: get_app(auth_token))
    return next(root, info, **kwargs)
",if len ( auth ) == 2 :,171
"def _shortest_hypernym_paths(self, simulate_root):
    if self.offset == ""00000000"":
        return {self: 0}
    queue = deque([(self, 0)])
    path = {}
    while queue:
        s, depth = queue.popleft()
        if s in path:
            continue
        path[s] = depth
        depth += 1
        queue.extend((hyp, depth) for hyp in s._hypernyms())
    if simulate_root:
        root = Synset(self._wordnet_corpus_reader, None, self.pos(), ""00000000"", """")
        path[root] = max(path.values()) + 1
    return path
",if s in path :,163
"def _populate_class_variables():
    lookup = {}
    reverse_lookup = {}
    characters_for_re = []
    for codepoint, name in list(codepoint2name.items()):
        character = chr(codepoint)
        if codepoint != 34:
            # There's no point in turning the quotation mark into
            # &quot;, unless it happens within an attribute value, which
            # is handled elsewhere.
            characters_for_re.append(character)
            lookup[character] = name
        # But we do want to turn &quot; into the quotation mark.
        reverse_lookup[name] = character
    re_definition = ""[%s]"" % """".join(characters_for_re)
    return lookup, reverse_lookup, re.compile(re_definition)
",if codepoint != 34 :,193
"def prepare_data_status(self, view: sublime.View, data: Dict[str, Any]) -> Any:
    """"""Prepare the returned data for status""""""
    if (
        data[""success""]
        and ""No docstring"" not in data[""doc""]
        and data[""doc""] != ""list\n""
    ):
        self.signature = data[""doc""]
        if self._signature_excluded(self.signature):
            return
        try:
            self.signature = self.signature.splitlines()[2]
        except KeyError:
            return
        return self._show_status(view)
",if self . _signature_excluded ( self . signature ) :,155
"def _setup_once_tables(cls):
    if cls.run_define_tables == ""once"":
        cls.define_tables(cls.metadata)
        if cls.run_create_tables == ""once"":
            cls.metadata.create_all(cls.bind)
        cls.tables.update(cls.metadata.tables)
","if cls . run_create_tables == ""once"" :",84
"def _send_recursive(self, files):
    for base in files:
        if not os.path.isdir(base):
            # filename mixed into the bunch
            self._send_files([base])
            continue
        last_dir = asbytes(base)
        for root, dirs, fls in os.walk(base):
            self._chdir(last_dir, asbytes(root))
            self._send_files([os.path.join(root, f) for f in fls])
            last_dir = asbytes(root)
        # back out of the directory
        for i in range(len(os.path.split(last_dir))):
            self._send_popd()
",if not os . path . isdir ( base ) :,183
"def __init__(self, *args, **kwargs):
    super().__init__(*args, **kwargs)
    # Automatically register models if required.
    if not is_registered(self.model):
        inline_fields = ()
        for inline in self.inlines:
            inline_model, follow_field = self._reversion_introspect_inline_admin(inline)
            if inline_model:
                self._reversion_autoregister(inline_model, ())
            if follow_field:
                inline_fields += (follow_field,)
        self._reversion_autoregister(self.model, inline_fields)
",if follow_field :,161
"def dispatch_hook(key, hooks, hook_data, **kwargs):
    """"""Dispatches a hook dictionary on a given piece of data.""""""
    hooks = hooks or dict()
    hooks = hooks.get(key)
    if hooks:
        if hasattr(hooks, ""__call__""):
            hooks = [hooks]
        for hook in hooks:
            _hook_data = hook(hook_data, **kwargs)
            if _hook_data is not None:
                hook_data = _hook_data
    return hook_data
",if _hook_data is not None :,133
"def __call__(self, image, crop=True):
    if isinstance(image, PTensor):
        return self.crop_to_output(
            numpy_to_paddle(self(paddle_to_numpy(image), crop=False))
        )
    else:
        warp = cv.warpAffine(
            image,
            self.transform_matrix,
            image.shape[1::-1],
            borderMode=cv.BORDER_REPLICATE,
        )
        if crop:
            return self.crop_to_output(warp)
        else:
            return warp
",if crop :,157
"def _analyze(self):
    lines = open(self.log_path, ""r"").readlines()
    prev_line = None
    for line in lines:
        if line.startswith(""ERROR:"") and prev_line and prev_line.startswith(""=""):
            self.errors.append(line[len(""ERROR:"") :].strip())
        elif line.startswith(""FAIL:"") and prev_line and prev_line.startswith(""=""):
            self.failures.append(line[len(""FAIL:"") :].strip())
        prev_line = line
","if line . startswith ( ""ERROR:"" ) and prev_line and prev_line . startswith ( ""="" ) :",128
"def end(self, name):
    self.soup.endData()
    completed_tag = self.soup.tagStack[-1]
    namespace, name = self._getNsTag(name)
    nsprefix = None
    if namespace is not None:
        for inverted_nsmap in reversed(self.nsmaps):
            if inverted_nsmap is not None and namespace in inverted_nsmap:
                nsprefix = inverted_nsmap[namespace]
                break
    self.soup.handle_endtag(name, nsprefix)
    if len(self.nsmaps) > 1:
        # This tag, or one of its parents, introduced a namespace
        # mapping, so pop it off the stack.
        self.nsmaps.pop()
",if inverted_nsmap is not None and namespace in inverted_nsmap :,184
"def _bind_parameters(operation, parameters):
    # inspired by MySQL Python Connector (conversion.py)
    string_parameters = {}
    for (name, value) in parameters.iteritems():
        if value is None:
            string_parameters[name] = ""NULL""
        elif isinstance(value, basestring):
            string_parameters[name] = ""'"" + _escape(value) + ""'""
        else:
            string_parameters[name] = str(value)
    return operation % string_parameters
","elif isinstance ( value , basestring ) :",126
"def plugin_on_song_ended(self, song, skipped):
    if song is not None:
        rating = song(""~#rating"")
        invrating = 1.0 - rating
        delta = min(rating, invrating) / 2.0
        if skipped:
            rating -= delta
        else:
            rating += delta
        song[""~#rating""] = rating
",if skipped :,97
"def on_activated_async(self, view):
    if settings[""modified_lines_only""]:
        self.freeze_last_version(view)
    if settings[""enabled""]:
        match_trailing_spaces(view)
        # continuously watch view for changes to the visible region
        if not view.id() in active_views:
            # track
            active_views[view.id()] = view.visible_region()
            self.update_on_region_change(view)
",if not view . id ( ) in active_views :,123
"def _notin_text(term, text, verbose=False):
    index = text.find(term)
    head = text[:index]
    tail = text[index + len(term) :]
    correct_text = head + tail
    diff = _diff_text(correct_text, text, verbose)
    newdiff = [u(""%s is contained here:"") % py.io.saferepr(term, maxsize=42)]
    for line in diff:
        if line.startswith(u(""Skipping"")):
            continue
        if line.startswith(u(""- "")):
            continue
        if line.startswith(u(""+ "")):
            newdiff.append(u(""  "") + line[2:])
        else:
            newdiff.append(line)
    return newdiff
","if line . startswith ( u ( ""Skipping"" ) ) :",192
"def delete_all(path):
    ppath = os.getcwd()
    os.chdir(path)
    for fn in glob.glob(""*""):
        fn_full = os.path.join(path, fn)
        if os.path.isdir(fn):
            delete_all(fn_full)
        elif fn.endswith("".png""):
            os.remove(fn_full)
        elif fn.endswith("".md""):
            os.remove(fn_full)
        elif DELETE_ALL_OLD:
            os.remove(fn_full)
    os.chdir(ppath)
    os.rmdir(path)
","elif fn . endswith ( "".md"" ) :",158
"def reward(self):
    """"""Returns a tuple of sum of raw and processed rewards.""""""
    raw_rewards, processed_rewards = 0, 0
    for ts in self.time_steps:
        # NOTE: raw_reward and processed_reward are None for the first time-step.
        if ts.raw_reward is not None:
            raw_rewards += ts.raw_reward
        if ts.processed_reward is not None:
            processed_rewards += ts.processed_reward
    return raw_rewards, processed_rewards
",if ts . processed_reward is not None :,134
"def formatmonthname(self, theyear, themonth, withyear=True):
    with TimeEncoding(self.locale) as encoding:
        s = month_name[themonth]
        if encoding is not None:
            s = s.decode(encoding)
        if withyear:
            s = ""%s %s"" % (s, theyear)
        return '<tr><th colspan=""7"" class=""month"">%s</th></tr>' % s
",if encoding is not None :,115
"def check_digest_auth(user, passwd):
    """"""Check user authentication using HTTP Digest auth""""""
    if request.headers.get(""Authorization""):
        credentails = parse_authorization_header(request.headers.get(""Authorization""))
        if not credentails:
            return
        response_hash = response(
            credentails,
            passwd,
            dict(
                uri=request.script_root + request.path,
                body=request.data,
                method=request.method,
            ),
        )
        if credentails.get(""response"") == response_hash:
            return True
    return False
",if not credentails :,165
"def wrapped(self, request):
    try:
        return self._finished
    except AttributeError:
        if self.node_ids:
            if not request.session.shouldfail and not request.session.shouldstop:
                log.debug(
                    ""%s is still going to be used, not terminating it. ""
                    ""Still in use on:\n%s"",
                    self,
                    pprint.pformat(list(self.node_ids)),
                )
                return
        log.debug(""Finish called on %s"", self)
        try:
            return func(request)
        finally:
            self._finished = True
",if self . node_ids :,185
"def run_tests():
    # type: () -> None
    x = 5
    with switch(x) as case:
        if case(0):
            print(""zero"")
            print(""zero"")
        elif case(1, 2):
            print(""one or two"")
        elif case(3, 4):
            print(""three or four"")
        else:
            print(""default"")
            print(""another"")
","elif case ( 3 , 4 ) :",114
"def task_done(self):
    with self._cond:
        if not self._unfinished_tasks.acquire(False):
            raise ValueError(""task_done() called too many times"")
        if self._unfinished_tasks._semlock._is_zero():
            self._cond.notify_all()
",if not self . _unfinished_tasks . acquire ( False ) :,75
"def _set_uid(self, val):
    if val is not None:
        if pwd is None:
            self.bus.log(""pwd module not available; ignoring uid."", level=30)
            val = None
        elif isinstance(val, text_or_bytes):
            val = pwd.getpwnam(val)[2]
    self._uid = val
",if pwd is None :,92
"def process_tag(hive_name, company, company_key, tag, default_arch):
    with winreg.OpenKeyEx(company_key, tag) as tag_key:
        version = load_version_data(hive_name, company, tag, tag_key)
        if version is not None:  # if failed to get version bail
            major, minor, _ = version
            arch = load_arch_data(hive_name, company, tag, tag_key, default_arch)
            if arch is not None:
                exe_data = load_exe(hive_name, company, company_key, tag)
                if exe_data is not None:
                    exe, args = exe_data
                    return company, major, minor, arch, exe, args
",if exe_data is not None :,199
"def run(algs):
    for alg in algs:
        vcs = alg.get(""variantcaller"")
        if vcs:
            if isinstance(vcs, dict):
                vcs = reduce(operator.add, vcs.values())
            if not isinstance(vcs, (list, tuple)):
                vcs = [vcs]
            return any(vc.startswith(prefix) for vc in vcs if vc)
","if not isinstance ( vcs , ( list , tuple ) ) :",117
"def wrapper(self, *args, **kwargs):
    if not self.request.path.endswith(""/""):
        if self.request.method in (""GET"", ""HEAD""):
            uri = self.request.path + ""/""
            if self.request.query:
                uri += ""?"" + self.request.query
            self.redirect(uri, permanent=True)
            return
        raise HTTPError(404)
    return method(self, *args, **kwargs)
","if self . request . method in ( ""GET"" , ""HEAD"" ) :",118
"def check_response(self, response):
    """"""Specialized version of check_response().""""""
    for line in response:
        # Skip blank lines:
        if not line.strip():
            continue
        if line.startswith(b""OK""):
            return
        elif line.startswith(b""Benutzer/Passwort Fehler""):
            raise BadLogin(line)
        else:
            raise FailedPost(""Server returned '%s'"" % six.ensure_text(line))
",if not line . strip ( ) :,126
"def Walk(self, hMenu=None):
    if not hMenu:
        hMenu = self.handle
    n = user32.GetMenuItemCount(hMenu)
    mi = MENUITEMINFO()
    for i in range(n):
        mi.fMask = 2  #  MIIM_ID
        user32.GetMenuItemInfoA(hMenu, i, 1, byref(mi))
        handle = user32.GetSubMenu(hMenu, i)
        if handle:
            yield handle, self.ListItems(handle)
            for i in self.Walk(handle):
                yield i
",if handle :,157
"def setSelection(self, labels):
    input = self.__validateInput(labels)
    if len(input) == 0 and not self.__allowEmptySelection:
        return
    if self.__allowMultipleSelection:
        self.__selectedLabels[:] = input
        self.__selectionChanged()
    else:
        if len(input) > 1:
            raise RuntimeError(
                ""Parameter must be single item or a list with one element.""
            )
        else:
            self.__selectedLabels[:] = input
            self.__selectionChanged()
    # Remove all selected labels that are not in the menu, emit signals if necessary and update the button.
    self.__validateState()
",if len ( input ) > 1 :,168
"def _parse(self, engine):
    """"""Parse the layer.""""""
    if isinstance(self.args, dict):
        if ""axis"" in self.args:
            self.axis = engine.evaluate(self.args[""axis""], recursive=True)
            if not isinstance(self.axis, int):
                raise ParsingError('""axis"" must be an integer.')
        if ""momentum"" in self.args:
            self.momentum = engine.evaluate(self.args[""momentum""], recursive=True)
            if not isinstance(self.momentum, (int, float)):
                raise ParsingError('""momentum"" must be numeric.')
","if not isinstance ( self . momentum , ( int , float ) ) :",157
"def get_order(self, aBuf):
    if not aBuf:
        return -1, 1
    # find out current char's byte length
    first_char = wrap_ord(aBuf[0])
    if (0x81 <= first_char <= 0x9F) or (0xE0 <= first_char <= 0xFC):
        charLen = 2
    else:
        charLen = 1
    # return its order if it is hiragana
    if len(aBuf) > 1:
        second_char = wrap_ord(aBuf[1])
        if (first_char == 202) and (0x9F <= second_char <= 0xF1):
            return second_char - 0x9F, charLen
    return -1, charLen
",if ( first_char == 202 ) and ( 0x9F <= second_char <= 0xF1 ) :,194
"def saveSpecial(self, **kwargs):
    for kw in SPECIAL_BOOL_LIST + SPECIAL_VALUE_LIST + SPECIAL_LIST_LIST:
        item = config.get_config(""misc"", kw)
        value = kwargs.get(kw)
        msg = item.set(value)
        if msg:
            return badParameterResponse(msg)
    config.save_config()
    raise Raiser(self.__root)
",if msg :,105
"def sanitize_event_keys(kwargs, valid_keys):
    # Sanity check: Don't honor keys that we don't recognize.
    for key in list(kwargs.keys()):
        if key not in valid_keys:
            kwargs.pop(key)
    # Truncate certain values over 1k
    for key in [""play"", ""role"", ""task"", ""playbook""]:
        if isinstance(kwargs.get(""event_data"", {}).get(key), str):
            if len(kwargs[""event_data""][key]) > 1024:
                kwargs[""event_data""][key] = Truncator(kwargs[""event_data""][key]).chars(
                    1024
                )
",if key not in valid_keys :,168
"def toggleFactorReload(self, value=None):
    self.serviceFittingOptions[""useGlobalForceReload""] = (
        value
        if value is not None
        else not self.serviceFittingOptions[""useGlobalForceReload""]
    )
    fitIDs = set()
    for fit in set(self._loadedFits):
        if fit is None:
            continue
        if fit.calculated:
            fit.factorReload = self.serviceFittingOptions[""useGlobalForceReload""]
            fit.clearFactorReloadDependentData()
            fitIDs.add(fit.ID)
    return fitIDs
",if fit is None :,149
"def closest_unseen(self, row1, col1, filter=None):
    # find the closest unseen from this row/col
    min_dist = maxint
    closest_unseen = None
    for row in range(self.height):
        for col in range(self.width):
            if filter is None or (row, col) not in filter:
                if self.map[row][col] == UNSEEN:
                    dist = self.distance(row1, col1, row, col)
                    if dist < min_dist:
                        min_dist = dist
                        closest_unseen = (row, col)
    return closest_unseen
","if filter is None or ( row , col ) not in filter :",174
"def getAlphaClone(lookfor, eager=None):
    if isinstance(lookfor, int):
        if eager is None:
            item = get_gamedata_session().query(AlphaClone).get(lookfor)
        else:
            item = (
                get_gamedata_session()
                .query(AlphaClone)
                .options(*processEager(eager))
                .filter(AlphaClone.ID == lookfor)
                .first()
            )
    else:
        raise TypeError(""Need integer as argument"")
    return item
",if eager is None :,150
"def _rle_encode(string):
    new = b""""
    count = 0
    for cur in string:
        if not cur:
            count += 1
        else:
            if count:
                new += b""\0"" + bytes([count])
                count = 0
            new += bytes([cur])
    return new
",if count :,92
"def result_iterator():
    try:
        for future in fs:
            if timeout is None:
                yield future.result()
            else:
                yield future.result(end_time - time.time())
    finally:
        for future in fs:
            future.cancel()
",if timeout is None :,81
"def _individual_get(self, segment, index_type, index, strictdoc):
    if index_type == ""val"":
        for key, value in segment.items():
            if key == index[0]:
                return value
            if hasattr(key, ""text""):
                if key.text == index[0]:
                    return value
        raise Exception(""Invalid state"")
    elif index_type == ""index"":
        return segment[index]
    elif index_type == ""textslice"":
        return segment[index[0] : index[1]]
    elif index_type == ""key"":
        return index[1] if strictdoc else index[0]
    else:
        raise Exception(""Invalid state"")
",if key == index [ 0 ] :,186
"def _reset_sequences(self, db_name):
    conn = connections[db_name]
    if conn.features.supports_sequence_reset:
        sql_list = conn.ops.sequence_reset_by_name_sql(
            no_style(), conn.introspection.sequence_list()
        )
        if sql_list:
            try:
                cursor = conn.cursor()
                for sql in sql_list:
                    cursor.execute(sql)
            except Exception:
                transaction.rollback_unless_managed(using=db_name)
                raise
            transaction.commit_unless_managed(using=db_name)
",if sql_list :,177
"def translate_to_statements(self, statements, conditional_write_vars):
    lines = []
    for stmt in statements:
        if stmt.op == "":="" and not stmt.var in self.variables:
            self.temporary_vars.add((stmt.var, stmt.dtype))
        line = self.translate_statement(stmt)
        if stmt.var in conditional_write_vars:
            subs = {}
            condvar = conditional_write_vars[stmt.var]
            lines.append(""if %s:"" % condvar)
            lines.append(indent(line))
        else:
            lines.append(line)
    return lines
","if stmt . op == "":="" and not stmt . var in self . variables :",168
"def _bytecode_filenames(self, py_filenames):
    bytecode_files = []
    for py_file in py_filenames:
        # Since build_py handles package data installation, the
        # list of outputs can contain more than just .py files.
        # Make sure we only report bytecode for the .py files.
        ext = os.path.splitext(os.path.normcase(py_file))[1]
        if ext != PYTHON_SOURCE_EXTENSION:
            continue
        if self.compile:
            bytecode_files.append(py_file + ""c"")
        if self.optimize > 0:
            bytecode_files.append(py_file + ""o"")
    return bytecode_files
",if self . compile :,175
"def logic():
    for i in range(100):
        yield clock.posedge, reset.negedge
        if reset == ACTIVE_LOW:
            count.next = 0
        else:
            if enable:
                count.next = (count + 1) % n
    raise StopSimulation
",if enable :,81
"def _is_subnet_of(a, b):
    try:
        # Always false if one is v4 and the other is v6.
        if a._version != b._version:
            raise TypeError(""%s and %s are not of the same version"" % (a, b))
        return (
            b.network_address <= a.network_address
            and b.broadcast_address >= a.broadcast_address
        )
    except AttributeError:
        raise TypeError(
            ""Unable to test subnet containment "" ""between %s and %s"" % (a, b)
        )
",if a . _version != b . _version :,153
"def _filter_paths(basename, path, is_dir, exclude):
    """""".gitignore style file filtering.""""""
    for item in exclude:
        # Items ending in '/' apply only to directories.
        if item.endswith(""/"") and not is_dir:
            continue
        # Items starting with '/' apply to the whole path.
        # In any other cases just the basename is used.
        match = path if item.startswith(""/"") else basename
        if fnmatch.fnmatch(match, item.strip(""/"")):
            return True
    return False
","if fnmatch . fnmatch ( match , item . strip ( ""/"" ) ) :",130
"def __recv_null(self):
    """"""Receive a null byte.""""""
    while 1:
        c = self.sock.recv(1)
        if c == """":
            self.close()
            raise EOFError(""Socket Closed"")
        if c == ""\0"":
            return
","if c == ""\0"" :",74
"def onMessage(self, payload, isBinary):
    if isBinary:
        self.result = ""Expected text message with payload, but got binary.""
    else:
        if len(payload) != self.DATALEN:
            self.result = (
                ""Expected text message with payload of length %d, but got %d.""
                % (self.DATALEN, len(payload))
            )
        else:
            ## FIXME : check actual content
            ##
            self.behavior = Case.OK
            self.result = ""Received text message of length %d."" % len(payload)
    self.p.createWirelog = True
    self.p.sendClose(self.p.CLOSE_STATUS_CODE_NORMAL)
",if len ( payload ) != self . DATALEN :,191
"def rename_path(self, path, new_path):
    logger.debug(""rename_path '%s' -> '%s'"" % (path, new_path))
    dirs = self.readdir(path)
    for d in dirs:
        if d in [""."", ""..""]:
            continue
        d_path = """".join([path, ""/"", d])
        d_new_path = """".join([new_path, ""/"", d])
        attr = self.getattr(d_path)
        if stat.S_ISDIR(attr[""st_mode""]):
            self.rename_path(d_path, d_new_path)
        else:
            self.rename_item(d_path, d_new_path)
    self.rename_item(path, new_path, dir=True)
","if d in [ ""."" , "".."" ] :",196
"def dir_box_click(self, double):
    if double:
        name = self.list_box.get_selected_name()
        path = os.path.join(self.directory, name)
        suffix = os.path.splitext(name)[1]
        if suffix not in self.suffixes and os.path.isdir(path):
            self.directory = path
        else:
            self.double_click_file(name)
    self.update()
",if suffix not in self . suffixes and os . path . isdir ( path ) :,119
"def __getattr__(self, key):
    try:
        value = self.__parent.contents[key]
    except KeyError:
        pass
    else:
        if value is not None:
            if isinstance(value, _ModuleMarker):
                return value.mod_ns
            else:
                assert isinstance(value, _MultipleClassMarker)
                return value.attempt_get(self.__parent.path, key)
    raise AttributeError(
        ""Module %r has no mapped classes ""
        ""registered under the name %r"" % (self.__parent.name, key)
    )
","if isinstance ( value , _ModuleMarker ) :",154
"def poll_thread():
    time.sleep(0.5)
    if process.wait() and process_state:
        time.sleep(0.25)
        if not check_global_interrupt():
            stdout, stderr = process._communicate(None)
            logger.error(
                ""Web server process exited unexpectedly"",
                ""app"",
                stdout=stdout,
                stderr=stderr,
            )
            time.sleep(1)
            restart_server(1)
",if not check_global_interrupt ( ) :,135
"def apply_dateparser_timezone(utc_datetime, offset_or_timezone_abb):
    for name, info in _tz_offsets:
        if info[""regex""].search("" %s"" % offset_or_timezone_abb):
            tz = StaticTzInfo(name, info[""offset""])
            return utc_datetime.astimezone(tz)
","if info [ ""regex"" ] . search ( "" %s"" % offset_or_timezone_abb ) :",87
"def _load_wordlist(filename):
    if filename is None:
        return {}
    path = None
    for dir in (CONFIG_DIR, ASSETS_DIR):
        path = os.path.realpath(os.path.join(dir, filename))
        if os.path.exists(path):
            break
    words = {}
    with open(path, encoding=""utf-8"") as f:
        pairs = [word.strip().rsplit("" "", 1) for word in f]
        pairs.sort(reverse=True, key=lambda x: int(x[1]))
        words = {p[0]: int(p[1]) for p in pairs}
    return words
",if os . path . exists ( path ) :,168
"def terminate_processes_matching_names(match_strings, kill=False):
    """"""Terminates processes matching particular names (case sensitive).""""""
    if isinstance(match_strings, str):
        match_strings = [match_strings]
    for process in psutil.process_iter():
        try:
            process_info = process.as_dict(attrs=[""name"", ""pid""])
            process_name = process_info[""name""]
        except (psutil.AccessDenied, psutil.NoSuchProcess, OSError):
            continue
        if any(x == process_name for x in match_strings):
            terminate_process(process_info[""pid""], kill)
",if any ( x == process_name for x in match_strings ) :,159
"def has_scheme(self, inp):
    if ""://"" in inp:
        return True
    else:
        authority = inp.replace(""/"", ""#"").replace(""?"", ""#"").split(""#"")[0]
        if "":"" in authority:
            _, host_or_port = authority.split("":"", 1)
            # Assert it's not a port number
            if re.match(r""^\d+$"", host_or_port):
                return False
        else:
            return False
    return True
","if "":"" in authority :",126
"def close(self):
    with BrowserContext._BROWSER_LOCK:
        BrowserContext._BROWSER_REFCNT -= 1
        if BrowserContext._BROWSER_REFCNT == 0:
            logger.info(""Destroying browser main loop"")
            BrowserContext._BROWSER_LOOP.destroy()
            BrowserContext._BROWSER_LOOP = None
",if BrowserContext . _BROWSER_REFCNT == 0 :,77
"def _mock_get_merge_ticks(self, order_book_id_list, trading_date, last_dt=None):
    for tick in self._ticks:
        if tick.order_book_id not in order_book_id_list:
            continue
        if (
            self.env.data_proxy.get_future_trading_date(tick.datetime).date()
            != trading_date.date()
        ):
            continue
        if last_dt and tick.datetime <= last_dt:
            continue
        yield tick
",if tick . order_book_id not in order_book_id_list :,146
"def messageSourceStamps(self, source_stamps):
    text = """"
    for ss in source_stamps:
        source = """"
        if ss[""branch""]:
            source += ""[branch %s] "" % ss[""branch""]
        if ss[""revision""]:
            source += str(ss[""revision""])
        else:
            source += ""HEAD""
        if ss[""patch""] is not None:
            source += "" (plus patch)""
        discriminator = """"
        if ss[""codebase""]:
            discriminator = "" '%s'"" % ss[""codebase""]
        text += ""Build Source Stamp%s: %s\n"" % (discriminator, source)
    return text
","if ss [ ""branch"" ] :",176
"def test_open_read_bytes(self, sftp):
    """"""Test reading bytes from a file""""""
    f = None
    try:
        self._create_file(""file"", ""xxx"")
        f = yield from sftp.open(""file"", ""rb"")
        self.assertEqual((yield from f.read()), b""xxx"")
    finally:
        if f:  # pragma: no branch
            yield from f.close()
        remove(""file"")
",if f :,115
"def handler(chan, host, port):
    sock = socket()
    try:
        sock.connect((host, port))
    except Exception as e:
        if verbose == True:
            print(e)
        return
    while True:
        r, w, x = select.select([sock, chan], [], [])
        if sock in r:
            data = sock.recv(1024)
            if len(data) == 0:
                break
            chan.send(data)
        if chan in r:
            data = chan.recv(1024)
            if len(data) == 0:
                break
            sock.send(data)
    chan.close()
    sock.close()
",if chan in r :,190
"def detect(get_page):
    retval = False
    for vector in WAF_ATTACK_VECTORS:
        page, headers, code = get_page(get=vector)
        retval = re.search(r""url\('/ks-waf-error\.png'\)"", page, re.I) is not None
        if retval:
            break
    return retval
",if retval :,94
"def __init__(self, raw):
    ticker_ticks = {}
    for tick in raw[""results""]:
        if ticker_ticks.get(tick[""T""]):
            ticker_ticks[tick[""T""]].append(tick)
        else:
            ticker_ticks[tick[""T""]] = [tick]
    super().__init__(
        {ticker: Aggsv2({""results"": ticks}) for ticker, ticks in ticker_ticks.items()}
    )
","if ticker_ticks . get ( tick [ ""T"" ] ) :",114
"def _makefiles(self, f):
    if isinstance(f, dict):
        for k, v in list(f.items()):
            if isinstance(v, list):
                self.makedir(dirname=k, content=v)
            elif isinstance(v, str):
                self.make_file(filename=k, content=v)
            else:  # pragma: nocover
                raise ValueError(""Unexpected:"", k, v)
    elif isinstance(f, str):
        self._make_empty_file(f)
    elif isinstance(f, list):
        self.make_list(f)
    else:  # pragma: nocover
        raise ValueError(""Unknown type:"", f)
","if isinstance ( v , list ) :",182
"def migrate_command_storage(apps, schema_editor):
    model = apps.get_model(""terminal"", ""CommandStorage"")
    init_storage_data(model)
    setting = get_setting(apps, schema_editor, ""TERMINAL_COMMAND_STORAGE"")
    if not setting:
        return
    values = get_storage_data(setting)
    for name, meta in values.items():
        tp = meta.pop(""TYPE"")
        if not tp or name in [""default"", ""null""]:
            continue
        model.objects.create(name=name, type=tp, meta=meta)
","if not tp or name in [ ""default"" , ""null"" ] :",146
"def build_vertices(self, ulines):
    vertex_idx = 0
    vertices = collections.OrderedDict()
    for line in ulines:
        for vt in line:
            if vt.replacement is not None:
                continue
            new_vertex = (vt.u, vt.v, 0.0)
            if new_vertex in vertices:
                continue
            vt.index = vertex_idx
            vertex_idx += 1
            vertices[new_vertex] = 1
    return vertex_idx, list(vertices.keys())
",if vt . replacement is not None :,145
"def get_quarantine_count(self):
    """"""get obj/container/account quarantine counts""""""
    qcounts = {""objects"": 0, ""containers"": 0, ""accounts"": 0}
    qdir = ""quarantined""
    for device in os.listdir(self.devices):
        for qtype in qcounts:
            qtgt = os.path.join(self.devices, device, qdir, qtype)
            if os.path.exists(qtgt):
                linkcount = os.lstat(qtgt).st_nlink
                if linkcount > 2:
                    qcounts[qtype] += linkcount - 2
    return qcounts
",if linkcount > 2 :,171
"def _format_arg(self, name, trait_spec, value):
    if name == ""mask_file"":
        return """"
    if name == ""op_string"":
        if ""-k %s"" in self.inputs.op_string:
            if isdefined(self.inputs.mask_file):
                return self.inputs.op_string % self.inputs.mask_file
            else:
                raise ValueError(""-k %s option in op_string requires mask_file"")
    return super(ImageStats, self)._format_arg(name, trait_spec, value)
","if ""-k %s"" in self . inputs . op_string :",146
"def _update_theme_style(self, *args):
    self.line_color_normal = self.theme_cls.divider_color
    if not any([self.error, self._text_len_error]):
        if not self.focus:
            self._current_hint_text_color = self.theme_cls.disabled_hint_text_color
            self._current_right_lbl_color = self.theme_cls.disabled_hint_text_color
            if self.helper_text_mode == ""persistent"":
                self._current_error_color = self.theme_cls.disabled_hint_text_color
","if self . helper_text_mode == ""persistent"" :",158
"def createFields(self):
    for item in self.format:
        if isinstance(item[-1], dict):
            yield item[0](self, *item[1:-1], **item[-1])
        else:
            yield item[0](self, *item[1:])
","if isinstance ( item [ - 1 ] , dict ) :",72
"def execute(self, statement, arguments=None):
    while True:
        try:
            if arguments:
                self.cursor.execute(statement, arguments)
            else:
                self.cursor.execute(statement)
        except sqlite3.OperationalError as ex:
            if ""locked"" not in getSafeExString(ex):
                raise
        else:
            break
    if statement.lstrip().upper().startswith(""SELECT""):
        return self.cursor.fetchall()
",if arguments :,130
"def set_income_account_for_fixed_assets(self):
    disposal_account = depreciation_cost_center = None
    for d in self.get(""items""):
        if d.is_fixed_asset:
            if not disposal_account:
                (
                    disposal_account,
                    depreciation_cost_center,
                ) = get_disposal_account_and_cost_center(self.company)
            d.income_account = disposal_account
            if not d.cost_center:
                d.cost_center = depreciation_cost_center
",if d . is_fixed_asset :,170
"def _convertNbCharsInNbBits(self, nbChars):
    nbMinBit = None
    nbMaxBit = None
    if nbChars is not None:
        if isinstance(nbChars, int):
            nbMinBit = nbChars * 8
            nbMaxBit = nbMinBit
        else:
            if nbChars[0] is not None:
                nbMinBit = nbChars[0] * 8
            if nbChars[1] is not None:
                nbMaxBit = nbChars[1] * 8
    return (nbMinBit, nbMaxBit)
",if nbChars [ 1 ] is not None :,158
"def _get_service_full_name(self, name, help_command_table):
    if help_command_table and name not in self._NON_SERVICE_COMMANDS:
        if name in self._HIGH_LEVEL_SERVICE_FULL_NAMES:
            return self._HIGH_LEVEL_SERVICE_FULL_NAMES[name]
        service = help_command_table.get(name)
        if service:
            return service.service_model.metadata[""serviceFullName""]
",if name in self . _HIGH_LEVEL_SERVICE_FULL_NAMES :,116
"def print_addresses(self):
    p = 3
    tmp_str = ""[""
    if self.get_len() >= 7:  # at least one complete IP address
        while 1:
            if p + 1 == self.get_ptr():
                tmp_str += ""#""
            tmp_str += self.get_ip_address(p)
            p += 4
            if p >= self.get_len():
                break
            else:
                tmp_str += "", ""
    tmp_str += ""] ""
    if self.get_ptr() % 4:  # ptr field should be a multiple of 4
        tmp_str += ""nonsense ptr field: %d "" % self.get_ptr()
    return tmp_str
",if p + 1 == self . get_ptr ( ) :,191
"def run(self):
    for _ in range(self.n):
        error = True
        try:
            self.collection.insert_one({""test"": ""insert""})
            error = False
        except:
            if not self.expect_exception:
                raise
        if self.expect_exception:
            assert error
",if self . expect_exception :,91
"def create_composite_mounter_by_args(args):
    """"""Creates a CompositeMounter by the images in given args.""""""
    logging.info(""Mount images..."")
    mounter = composite_mounter.CompositeMounter()
    for partition in composite_mounter.SUPPORTED_PARTITIONS:
        image_source = vars(args)[partition]
        if image_source:
            logging.info(""  %s=%s"", partition, image_source)
            mounter.add_by_mount_target(partition, image_source)
    if mounter.is_empty():
        raise RuntimeError(""Must give at least one image source."")
    return mounter
",if image_source :,162
"def _get_containing_class(self, pyname):
    if isinstance(pyname, pynames.DefinedName):
        scope = pyname.get_object().get_scope()
        parent = scope.parent
        if parent is not None and parent.get_kind() == ""Class"":
            return parent.pyobject
","if parent is not None and parent . get_kind ( ) == ""Class"" :",80
"def test_chunkcoding(self):
    tstring_lines = []
    for b in self.tstring:
        lines = b.split(b""\n"")
        last = lines.pop()
        assert last == b""""
        lines = [line + b""\n"" for line in lines]
        tstring_lines.append(lines)
    for native, utf8 in zip(*tstring_lines):
        u = self.decode(native)[0]
        self.assertEqual(u, utf8.decode(""utf-8""))
        if self.roundtriptest:
            self.assertEqual(native, self.encode(u)[0])
",if self . roundtriptest :,161
"def set_default_variants(apps, schema_editor):
    Product = apps.get_model(""product"", ""Product"")
    for product in Product.objects.iterator():
        first_variant = product.variants.first()
        if first_variant:
            product.default_variant = first_variant
            product.save(update_fields=[""default_variant"", ""updated_at""])
",if first_variant :,95
"def json(self):
    try:
        if self.is_json():
            raw_data = self.raw_data()
            if not isinstance(raw_data, text_type):
                raw_data = raw_data.decode(""utf-8"")
            return json.loads(raw_data)
    except ValueError:
        pass
","if not isinstance ( raw_data , text_type ) :",91
"def clear_react(self, message: discord.Message, emoji: MutableMapping = None) -> None:
    try:
        await message.clear_reactions()
    except discord.Forbidden:
        if not emoji:
            return
        with contextlib.suppress(discord.HTTPException):
            async for key in AsyncIter(emoji.values(), delay=0.2):
                await message.remove_reaction(key, self.bot.user)
    except discord.HTTPException:
        return
",if not emoji :,134
"def check(self, value):
    value = String.check(self, value)
    if isinstance(value, str):
        value = value.upper()
        for prefix in (self.prefix, self.prefix.split(""_"", 1)[1]):
            # e.g. PANGO_WEIGHT_BOLD --> BOLD but also WEIGHT_BOLD --> BOLD
            if value.startswith(prefix):
                value = value[len(prefix) :]
            value = value.lstrip(""_"")
        if hasattr(self.group, value):
            return getattr(self.group, value)
        else:
            raise ValueError(""No such constant: %s_%s"" % (self.prefix, value))
    else:
        return value
","if hasattr ( self . group , value ) :",182
"def value(self):
    quote = False
    if self.defects:
        quote = True
    else:
        for x in self:
            if x.token_type == ""quoted-string"":
                quote = True
    if quote:
        pre = post = """"
        if self[0].token_type == ""cfws"" or self[0][0].token_type == ""cfws"":
            pre = "" ""
        if self[-1].token_type == ""cfws"" or self[-1][-1].token_type == ""cfws"":
            post = "" ""
        return pre + quote_string(self.display_name) + post
    else:
        return super(DisplayName, self).value
","if self [ - 1 ] . token_type == ""cfws"" or self [ - 1 ] [ - 1 ] . token_type == ""cfws"" :",186
"def get_drive(self, root_path="""", volume_guid_path=""""):
    for drive in self.drives:
        if root_path:
            config_root_path = drive.get(""root_path"")
            if config_root_path and root_path == config_root_path:
                return drive
        elif volume_guid_path:
            config_volume_guid_path = drive.get(""volume_guid_path"")
            if config_volume_guid_path and config_volume_guid_path == volume_guid_path:
                return drive
",elif volume_guid_path :,148
"def parse_edges(self, pcb):
    edges = []
    drawings = list(pcb.GetDrawings())
    bbox = None
    for m in pcb.GetModules():
        for g in m.GraphicalItems():
            drawings.append(g)
    for d in drawings:
        if d.GetLayer() == pcbnew.Edge_Cuts:
            parsed_drawing = self.parse_drawing(d)
            if parsed_drawing:
                edges.append(parsed_drawing)
                if bbox is None:
                    bbox = d.GetBoundingBox()
                else:
                    bbox.Merge(d.GetBoundingBox())
    if bbox:
        bbox.Normalize()
    return edges, bbox
",if parsed_drawing :,197
"def to_key(literal_or_identifier):
    """"""returns string representation of this object""""""
    if literal_or_identifier[""type""] == ""Identifier"":
        return literal_or_identifier[""name""]
    elif literal_or_identifier[""type""] == ""Literal"":
        k = literal_or_identifier[""value""]
        if isinstance(k, float):
            return unicode(float_repr(k))
        elif ""regex"" in literal_or_identifier:
            return compose_regex(k)
        elif isinstance(k, bool):
            return ""true"" if k else ""false""
        elif k is None:
            return ""null""
        else:
            return unicode(k)
",elif k is None :,179
"def find_multiple_stats(stats, name, _found=None, _on_found=None):
    if _found is None:
        _found = []
    for child_stats in stats:
        if child_stats.name == name:
            _found.append(child_stats)
            if callable(_on_found):
                _on_found(_found)
        find_multiple_stats(child_stats, name, _found)
    return _found
",if callable ( _on_found ) :,119
"def _run_generated_code(
    self,
    code,
    globs,
    locs,
    fails_under_py3k=True,
):
    import warnings
    from zope.interface._compat import PYTHON3
    with warnings.catch_warnings(record=True) as log:
        warnings.resetwarnings()
        if not PYTHON3:
            exec(code, globs, locs)
            self.assertEqual(len(log), 0)  # no longer warn
            return True
        else:
            try:
                exec(code, globs, locs)
            except TypeError:
                return False
            else:
                if fails_under_py3k:
                    self.fail(""Didn't raise TypeError"")
",if not PYTHON3 :,195
"def _get_node(self, node_id):
    self.non_terminated_nodes({})  # Side effect: updates cache
    with self.lock:
        if node_id in self.cached_nodes:
            return self.cached_nodes[node_id]
        instance = (
            self.compute.instances()
            .get(
                project=self.provider_config[""project_id""],
                zone=self.provider_config[""availability_zone""],
                instance=node_id,
            )
            .execute()
        )
        return instance
",if node_id in self . cached_nodes :,156
"def skip_to_close_match(self):
    nestedCount = 1
    while 1:
        tok = self.tokenizer.get_next_token()
        ttype = tok[""style""]
        if ttype == SCE_PL_UNUSED:
            return
        elif self.classifier.is_index_op(tok):
            tval = tok[""text""]
            if self.opHash.has_key(tval):
                if self.opHash[tval][1] == 1:
                    nestedCount += 1
                else:
                    nestedCount -= 1
                    if nestedCount <= 0:
                        break
",if self . opHash . has_key ( tval ) :,176
"def _create_or_get_helper(self, infer_mode: Optional[bool] = None, **kwargs) -> Helper:
    # Prefer creating a new helper when at least one kwarg is specified.
    prefer_new = len(kwargs) > 0
    kwargs.update(infer_mode=infer_mode)
    is_training = not infer_mode if infer_mode is not None else self.training
    helper = self._train_helper if is_training else self._infer_helper
    if prefer_new or helper is None:
        helper = self.create_helper(**kwargs)
        if is_training and self._train_helper is None:
            self._train_helper = helper
        elif not is_training and self._infer_helper is None:
            self._infer_helper = helper
    return helper
",elif not is_training and self . _infer_helper is None :,195
"def get_ldset(self, ldsets):
    ldset = None
    if self._properties[""ldset_name""] == """":
        nldset = len(ldsets)
        if nldset == 0:
            msg = _(""Logical Disk Set could not be found."")
            raise exception.NotFound(msg)
        else:
            ldset = None
    else:
        if self._properties[""ldset_name""] not in ldsets:
            msg = (
                _(""Logical Disk Set `%s` could not be found."")
                % self._properties[""ldset_name""]
            )
            raise exception.NotFound(msg)
        ldset = ldsets[self._properties[""ldset_name""]]
    return ldset
","if self . _properties [ ""ldset_name"" ] not in ldsets :",195
"def calc_fractal_serial(q, maxiter):
    # calculate z using pure python on a numpy array
    # note that, unlike the other two implementations,
    # the number of iterations per point is NOT constant
    z = np.zeros(q.shape, complex)
    output = np.resize(
        np.array(
            0,
        ),
        q.shape,
    )
    for i in range(len(q)):
        for iter in range(maxiter):
            z[i] = z[i] * z[i] + q[i]
            if abs(z[i]) > 2.0:
                output[i] = iter
                break
    return output
",if abs ( z [ i ] ) > 2.0 :,180
"def _verifySubs(self):
    for inst in self.subs:
        if not isinstance(inst, (_Block, _Instantiator, Cosimulation)):
            raise BlockError(_error.ArgType % (self.name,))
        if isinstance(inst, (_Block, _Instantiator)):
            if not inst.modctxt:
                raise BlockError(_error.InstanceError % (self.name, inst.callername))
","if isinstance ( inst , ( _Block , _Instantiator ) ) :",110
"def walks_generator():
    if filelist is not None:
        bucket = []
        for filename in filelist:
            with io.open(filename) as inf:
                for line in inf:
                    walk = [int(x) for x in line.strip(""\n"").split("" "")]
                    bucket.append(walk)
                    if len(bucket) == batch_size:
                        yield bucket
                        bucket = []
        if len(bucket):
            yield bucket
    else:
        for _ in range(epoch):
            for nodes in graph.node_batch_iter(batch_size):
                walks = graph.random_walk(nodes, walk_len)
                yield walks
",if len ( bucket ) == batch_size :,198
"def _traverse(op):
    if op in visited:
        return
    visited.add(op)
    if tag.is_injective(op.tag):
        if op not in s.outputs:
            s[op].compute_inline()
        for tensor in op.input_tensors:
            if isinstance(tensor.op, tvm.te.ComputeOp):
                _traverse(tensor.op)
    callback(op)
","if isinstance ( tensor . op , tvm . te . ComputeOp ) :",112
"def unwatch_run(self, run_id, handler):
    with self._dict_lock:
        if run_id in self._run_id_dict:
            self._handlers_dict[run_id] = [
                (start_cursor, callback)
                for (start_cursor, callback) in self._handlers_dict[run_id]
                if callback != handler
            ]
        if not self._handlers_dict[run_id]:
            del self._handlers_dict[run_id]
            run_id_dict = self._run_id_dict
            del run_id_dict[run_id]
            self._run_id_dict = run_id_dict
",if run_id in self . _run_id_dict :,185
"def _PromptMySQL(self, config):
    """"""Prompts the MySQL configuration, retrying if the configuration is invalid.""""""
    while True:
        self._PromptMySQLOnce(config)
        if self._CheckMySQLConnection():
            print(""Successfully connected to MySQL with the given configuration."")
            return
        else:
            print(""Error: Could not connect to MySQL with the given configuration."")
            retry = RetryBoolQuestion(""Do you want to retry MySQL configuration?"", True)
            if not retry:
                raise ConfigInitError()
",if self . _CheckMySQLConnection ( ) :,136
"def get_courses_without_topic(topic):
    data = []
    for entry in frappe.db.get_all(""Course""):
        course = frappe.get_doc(""Course"", entry.name)
        topics = [t.topic for t in course.topics]
        if not topics or topic not in topics:
            data.append(course.name)
    return data
",if not topics or topic not in topics :,103
"def _error_handler(action, **keywords):
    if keywords:
        file_type = keywords.get(""file_type"", None)
        if file_type:
            raise exceptions.FileTypeNotSupported(
                constants.FILE_TYPE_NOT_SUPPORTED_FMT % (file_type, action)
            )
        else:
            if ""on_demand"" in keywords:
                keywords.pop(""on_demand"")
            msg = ""Please check if there were typos in ""
            msg += ""function parameters: %s. Otherwise ""
            msg += ""unrecognized parameters were given.""
            raise exceptions.UnknownParameters(msg % keywords)
    else:
        raise exceptions.UnknownParameters(""No parameters found!"")
","if ""on_demand"" in keywords :",186
"def select(self, regions, register):
    self.view.sel().clear()
    to_store = []
    for r in regions:
        self.view.sel().add(r)
        if register:
            to_store.append(self.view.substr(self.view.full_line(r)))
    if register:
        text = """".join(to_store)
        if not text.endswith(""\n""):
            text = text + ""\n""
        state = State(self.view)
        state.registers[register] = [text]
",if register :,142
"def has_actor(self, message: HasActorMessage) -> ResultMessage:
    actor_ref = message.actor_ref
    # lookup allocated
    for address, item in self._allocated_actors.items():
        ref = create_actor_ref(address, actor_ref.uid)
        if ref in item:
            return ResultMessage(message.message_id, True, protocol=message.protocol)
    return ResultMessage(message.message_id, False, protocol=message.protocol)
",if ref in item :,113
"def toggleMetaButton(self, event):
    """"""Process clicks on toggle buttons""""""
    clickedBtn = event.EventObject
    if wx.GetMouseState().GetModifiers() == wx.MOD_CONTROL:
        activeBtns = [btn for btn in self.metaButtons if btn.GetValue()]
        if activeBtns:
            clickedBtn.setUserSelection(clickedBtn.GetValue())
            self.itemView.filterItemStore()
        else:
            # Do 'nothing' if we're trying to turn last active button off
            # Keep button in the same state
            clickedBtn.setUserSelection(True)
    else:
        for btn in self.metaButtons:
            btn.setUserSelection(btn == clickedBtn)
        self.itemView.filterItemStore()
",if activeBtns :,198
"def __init__(self, hub=None):  # pylint: disable=unused-argument
    if resolver._resolver is None:
        _resolver = resolver._resolver = _DualResolver()
        if config.resolver_nameservers:
            _resolver.network_resolver.nameservers[:] = config.resolver_nameservers
        if config.resolver_timeout:
            _resolver.network_resolver.lifetime = config.resolver_timeout
    # Different hubs in different threads could be sharing the same
    # resolver.
    assert isinstance(resolver._resolver, _DualResolver)
    self._resolver = resolver._resolver
",if config . resolver_timeout :,147
"def sub_paragraph(self, li):
    """"""Search for checkbox in sub-paragraph.""""""
    found = False
    if len(li):
        first = list(li)[0]
        if first.tag == ""p"" and first.text is not None:
            m = RE_CHECKBOX.match(first.text)
            if m is not None:
                first.text = self.markdown.htmlStash.store(
                    get_checkbox(m.group(""state"")), safe=True
                ) + m.group(""line"")
                found = True
    return found
","if first . tag == ""p"" and first . text is not None :",152
"def _check_mswin_locale(locale):
    msloc = None
    try:
        msloc = _LOCALE_NAMES[locale[:5]][:2]
        locale = locale[:5]
    except KeyError:
        try:
            msloc = _LOCALE_NAMES[locale[:2]][:2]
            locale = locale[:2]
        except KeyError:
            # US English is the outlier, all other English locales want
            # real English:
            if locale[:2] == (""en"") and locale[:5] != ""en_US"":
                return (""en_GB"", ""1252"")
            return (None, None)
    return (locale, msloc)
","if locale [ : 2 ] == ( ""en"" ) and locale [ : 5 ] != ""en_US"" :",176
"def setLabel(self, s, protect=False):
    """"""Set the label of the minibuffer.""""""
    c, k, w = self.c, self, self.w
    if w:
        # Support for the curses gui.
        if hasattr(g.app.gui, ""set_minibuffer_label""):
            g.app.gui.set_minibuffer_label(c, s)
        w.setAllText(s)
        n = len(s)
        w.setSelectionRange(n, n, insert=n)
        if protect:
            k.mb_prefix = s
","if hasattr ( g . app . gui , ""set_minibuffer_label"" ) :",151
"def getProc(su, innerTarget):
    if len(su) == 1:  # have a one element wedge
        proc = (""first"", ""last"")
    else:
        if su.isFirst(innerTarget) and su.isLast(innerTarget):
            proc = (""first"", ""last"")  # same element can be first and last
        elif su.isFirst(innerTarget):
            proc = (""first"",)
        elif su.isLast(innerTarget):
            proc = (""last"",)
        else:
            proc = ()
    return proc
",elif su . isFirst ( innerTarget ) :,143
"def await_test_end(self):
    iterations = 0
    while True:
        if iterations > 100:
            self.log.debug(""Await: iteration limit reached"")
            return
        status = self.master.get_status()
        if status.get(""status"") == ""ENDED"":
            return
        iterations += 1
        time.sleep(1.0)
","if status . get ( ""status"" ) == ""ENDED"" :",100
"def _handle_autocomplete_request_for_text(text):
    if not hasattr(text, ""autocompleter""):
        if isinstance(text, (CodeViewText, ShellText)) and text.is_python_text():
            if isinstance(text, CodeViewText):
                text.autocompleter = Completer(text)
            elif isinstance(text, ShellText):
                text.autocompleter = ShellCompleter(text)
            text.bind(""<1>"", text.autocompleter.on_text_click)
        else:
            return
    text.autocompleter.handle_autocomplete_request()
","if isinstance ( text , CodeViewText ) :",151
"def validate_party_details(self):
    if self.party:
        if not frappe.db.exists(self.party_type, self.party):
            frappe.throw(_(""Invalid {0}: {1}"").format(self.party_type, self.party))
        if self.party_account and self.party_type in (""Customer"", ""Supplier""):
            self.validate_account_type(
                self.party_account, [erpnext.get_party_account_type(self.party_type)]
            )
","if not frappe . db . exists ( self . party_type , self . party ) :",140
"def format(self, formatstr):
    pieces = []
    for i, piece in enumerate(re_formatchars.split(force_text(formatstr))):
        if i % 2:
            pieces.append(force_text(getattr(self, piece)()))
        elif piece:
            pieces.append(re_escaped.sub(r""\1"", piece))
    return """".join(pieces)
",if i % 2 :,99
"def _convert_java_pattern_to_python(pattern):
    """"""Convert a replacement pattern from the Java-style `$5` to the Python-style `\\5`.""""""
    s = list(pattern)
    i = 0
    while i < len(s) - 1:
        c = s[i]
        if c == ""$"" and s[i + 1] in ""0123456789"":
            s[i] = ""\\""
        elif c == ""\\"" and s[i + 1] == ""$"":
            s[i] = """"
            i += 1
        i += 1
    return pattern[:0].join(s)
","if c == ""$"" and s [ i + 1 ] in ""0123456789"" :",152
"def download(self, url, filename, **kwargs):
    try:
        r = self.get(url, timeout=10, stream=True, **kwargs)
        if r.status_code >= 400:
            return False
        with open(filename, ""wb"") as f:
            for chunk in r.iter_content(chunk_size=1024):
                if chunk:
                    f.write(chunk)
        helpers.chmod_as_parent(filename)
    except Exception as e:
        sickrage.app.log.debug(
            ""Failed to download file from {} - ERROR: {}"".format(url, e)
        )
        if os.path.exists(filename):
            os.remove(filename)
        return False
    return True
",if r . status_code >= 400 :,200
"def run(self, paths=[]):
    items = []
    for item in SideBarSelection(paths).getSelectedFilesWithExtension(""js""):
        items.append(
            '<script type=""text/javascript"" src=""'
            + item.pathAbsoluteFromProjectEncoded()
            + '""></script>'
        )
    if len(items) > 0:
        sublime.set_clipboard(""\n"".join(items))
        if len(items) > 1:
            sublime.status_message(""Items copied"")
        else:
            sublime.status_message(""Item copied"")
",if len ( items ) > 1 :,153
"def work(self):
    while True:
        timeout = self.timeout
        if idle.is_set():
            timeout = self.idle_timeout
        log.debug(""Wait for {}"".format(timeout))
        fetch.wait(timeout)
        if shutting_down.is_set():
            log.info(""Stop fetch worker"")
            break
        self.fetch()
",if shutting_down . is_set ( ) :,99
"def check_apns_certificate(ss):
    mode = ""start""
    for s in ss.split(""\n""):
        if mode == ""start"":
            if ""BEGIN RSA PRIVATE KEY"" in s or ""BEGIN PRIVATE KEY"" in s:
                mode = ""key""
        elif mode == ""key"":
            if ""END RSA PRIVATE KEY"" in s or ""END PRIVATE KEY"" in s:
                mode = ""end""
                break
            elif s.startswith(""Proc-Type"") and ""ENCRYPTED"" in s:
                raise ImproperlyConfigured(
                    ""Encrypted APNS private keys are not supported""
                )
    if mode != ""end"":
        raise ImproperlyConfigured(""The APNS certificate doesn't contain a private key"")
","if ""BEGIN RSA PRIVATE KEY"" in s or ""BEGIN PRIVATE KEY"" in s :",195
"def compare_lists(self, l1, l2, key):
    l2_lookup = {o.get(key): o for o in l2}
    for obj1 in l1:
        obj2 = l2_lookup.get(obj1.get(key))
        for k in obj1:
            if k not in ""id"" and obj1.get(k):
                self.assertEqual(obj1.get(k), obj2.get(k))
","if k not in ""id"" and obj1 . get ( k ) :",117
"def before_get_object(self, view_kwargs):
    if view_kwargs.get(""id"") is not None:
        try:
            user_favourite_event = find_user_favourite_event_by_id(
                event_id=view_kwargs[""id""]
            )
        except NoResultFound:
            raise ObjectNotFound(
                {""source"": ""/data/relationships/event""}, ""Object: not found""
            )
        else:
            if user_favourite_event is not None:
                view_kwargs[""id""] = user_favourite_event.id
            else:
                view_kwargs[""id""] = None
",if user_favourite_event is not None :,182
"def close(self):
    super().close()
    if not sys.is_finalizing():
        for sig in list(self._signal_handlers):
            self.remove_signal_handler(sig)
    else:
        if self._signal_handlers:
            warnings.warn(
                f""Closing the loop {self!r} ""
                f""on interpreter shutdown ""
                f""stage, skipping signal handlers removal"",
                ResourceWarning,
                source=self,
            )
            self._signal_handlers.clear()
",if self . _signal_handlers :,148
"def install_script(self, script, install_options=None):
    try:
        fname = utils.do_script(
            script,
            python_exe=osp.join(self.target, ""python.exe""),
            architecture=self.architecture,
            verbose=self.verbose,
            install_options=install_options,
        )
    except RuntimeError:
        if not self.verbose:
            print(""Failed!"")
            raise
",if not self . verbose :,123
"def GetRouterForUser(self, username):
    """"""Returns a router corresponding to a given username.""""""
    for index, router in enumerate(self.routers):
        router_id = str(index)
        if self.auth_manager.CheckPermissions(username, router_id):
            logging.debug(
                ""Matched router %s to user %s"", router.__class__.__name__, username
            )
            return router
    logging.debug(
        ""No router ACL rule match for user %s. Using default "" ""router %s"",
        username,
        self.default_router.__class__.__name__,
    )
    return self.default_router
","if self . auth_manager . CheckPermissions ( username , router_id ) :",168
"def charset(self):
    """"""The charset from the content type.""""""
    header = self.environ.get(""CONTENT_TYPE"")
    if header:
        ct, options = parse_options_header(header)
        charset = options.get(""charset"")
        if charset:
            if is_known_charset(charset):
                return charset
            return self.unknown_charset(charset)
    return self.default_charset
",if charset :,108
"def isFinished(self):
    # returns true if episode timesteps has reached episode length and resets the task
    if self.count > self.epiLen:
        self.res()
        return True
    else:
        if self.count == 1:
            self.pertGlasPos(0)
        if self.count == self.epiLen / 2 + 1:
            self.env.reset()
            self.pertGlasPos(1)
        self.count += 1
        return False
",if self . count == self . epiLen / 2 + 1 :,132
"def mtimes_of_files(dirnames: List[str], suffix: str) -> Iterator[float]:
    for dirname in dirnames:
        for root, dirs, files in os.walk(dirname):
            for sfile in files:
                if sfile.endswith(suffix):
                    try:
                        yield path.getmtime(path.join(root, sfile))
                    except OSError:
                        pass
",if sfile . endswith ( suffix ) :,113
"def get_all_hashes(self):
    event_hashes = []
    sample_hashes = []
    for a in self.event.attributes:
        h = None
        if a.type in (""md5"", ""sha1"", ""sha256""):
            h = a.value
            event_hashes.append(h)
        elif a.type in (""filename|md5"", ""filename|sha1"", ""filename|sha256""):
            h = a.value.split(""|"")[1]
            event_hashes.append(h)
        elif a.type == ""malware-sample"":
            h = a.value.split(""|"")[1]
            sample_hashes.append(h)
    return event_hashes, sample_hashes
","elif a . type == ""malware-sample"" :",184
"def _validate(self, event):
    if self.type is None:
        return
    new = self.value
    if not isinstance(new, self.type) and new is not None:
        if event:
            self.value = event.old
        types = repr(self.type) if isinstance(self.type, tuple) else self.type.__name__
        raise ValueError(
            ""LiteralInput expected %s type but value %s ""
            ""is of type %s."" % (types, new, type(new).__name__)
        )
",if event :,140
"def update_dict(a, b):
    for key, value in b.items():
        if value is None:
            continue
        if key not in a:
            a[key] = value
        elif isinstance(a[key], dict) and isinstance(value, dict):
            update_dict(a[key], value)
        elif isinstance(a[key], list):
            a[key].append(value)
        else:
            a[key] = [a[key], value]
",if value is None :,131
"def on_pre_save(self, view):
    extOrClause = ""|"".join(s.get(""format_on_save_extensions""))
    extRegex = ""\\.("" + extOrClause + "")$""
    if s.get(""format_on_save"") and re.search(extRegex, view.file_name()):
        # only auto-format on save if there are no ""lint errors""
        # here are some named regions from sublimelint see https://github.com/lunixbochs/sublimelint/tree/st3
        lints_regions = [""lint-keyword-underline"", ""lint-keyword-outline""]
        for linter in lints_regions:
            if len(view.get_regions(linter)):
                return
        view.run_command(""js_format"")
",if len ( view . get_regions ( linter ) ) :,198
"def readMemory(self, va, size):
    for mva, mmaxva, mmap, mbytes in self._map_defs:
        if mva <= va < mmaxva:
            mva, msize, mperms, mfname = mmap
            if not mperms & MM_READ:
                raise envi.SegmentationViolation(va)
            offset = va - mva
            return mbytes[offset : offset + size]
    raise envi.SegmentationViolation(va)
",if not mperms & MM_READ :,127
"def assertFilepathsEqual(self, p1, p2):
    if sys.platform == ""win32"":
        if isinstance(p1, (list, tuple)):
            p1 = [normcase(normpath(x)) for x in p1]
            p2 = [normcase(normpath(x)) for x in p2]
        else:
            assert isinstance(p1, (str, unicode))
            p1 = normcase(normpath(p1))
            p2 = normcase(normpath(p2))
    self.assertEqual(p1, p2)
","if isinstance ( p1 , ( list , tuple ) ) :",140
"def add_directory_csv_files(dir_path, paths=None):
    if not paths:
        paths = []
    for p in listdir(dir_path):
        path = join(dir_path, p)
        if isdir(path):
            # call recursively for each dir
            paths = add_directory_csv_files(path, paths)
        elif isfile(path) and path.endswith("".csv""):
            # add every file to the list
            paths.append(path)
    return paths
","elif isfile ( path ) and path . endswith ( "".csv"" ) :",130
"def _verifySubs(self):
    for inst in self.subs:
        if not isinstance(inst, (_Block, _Instantiator, Cosimulation)):
            raise BlockError(_error.ArgType % (self.name,))
        if isinstance(inst, (_Block, _Instantiator)):
            if not inst.modctxt:
                raise BlockError(_error.InstanceError % (self.name, inst.callername))
","if not isinstance ( inst , ( _Block , _Instantiator , Cosimulation ) ) :",110
"def __annotations_bytes(self):
    if self.annotations:
        a = []
        for k, v in self.annotations.items():
            if len(k) != 4:
                raise errors.ProtocolError(""annotation key must be of length 4"")
            if sys.version_info >= (3, 0):
                k = k.encode(""ASCII"")
            a.append(struct.pack(""!4sH"", k, len(v)))
            a.append(v)
        return b"""".join(a)
    return b""""
","if sys . version_info >= ( 3 , 0 ) :",143
"def session(self, profile: str = ""default"", region: str = None) -> boto3.Session:
    region = self._get_region(region, profile)
    try:
        session = self._cache_lookup(
            self._session_cache,
            [profile, region],
            self._boto3.Session,
            [],
            {""region_name"": region, ""profile_name"": profile},
        )
    except ProfileNotFound:
        if profile != ""default"":
            raise
        session = self._boto3.Session(region_name=region)
        self._cache_set(self._session_cache, [profile, region], session)
    return session
","if profile != ""default"" :",174
"def spans_score(gold_spans, system_spans):
    correct, gi, si = 0, 0, 0
    while gi < len(gold_spans) and si < len(system_spans):
        if system_spans[si].start < gold_spans[gi].start:
            si += 1
        elif gold_spans[gi].start < system_spans[si].start:
            gi += 1
        else:
            correct += gold_spans[gi].end == system_spans[si].end
            si += 1
            gi += 1
    return Score(len(gold_spans), len(system_spans), correct)
",elif gold_spans [ gi ] . start < system_spans [ si ] . start :,160
"def to_api(tag, raw_value):
    try:
        api_tag, converter = _QL_TO_SC[tag] if tag else (""q"", None)
    except KeyError:
        if tag not in SUPPORTED:
            raise self.error(
                ""Unsupported '%s' tag. Try: %s"" % (tag, "", "".join(SUPPORTED))
            )
        return None, None
    else:
        value = str(converter(raw_value) if converter else raw_value)
        return api_tag, value
",if tag not in SUPPORTED :,137
"def unpack(self, buf):
    dpkt.Packet.unpack(self, buf)
    buf = buf[self.__hdr_len__ :]
    # single-byte IE
    if self.type & 0x80:
        self.len = 0
        self.data = b""""
    # multi-byte IE
    else:
        # special PER-encoded UUIE
        if self.type == USER_TO_USER:
            self.len = struct.unpack("">H"", buf[:2])[0]
            buf = buf[2:]
        # normal TLV-like IE
        else:
            self.len = struct.unpack(""B"", buf[:1])[0]
            buf = buf[1:]
        self.data = buf[: self.len]
",if self . type == USER_TO_USER :,195
"def on_bt_search_clicked(self, widget):
    if self.current_provider is None:
        return
    query = self.en_query.get_text()
    @self.obtain_podcasts_with
    def load_data():
        if self.current_provider.kind == directory.Provider.PROVIDER_SEARCH:
            return self.current_provider.on_search(query)
        elif self.current_provider.kind == directory.Provider.PROVIDER_URL:
            return self.current_provider.on_url(query)
        elif self.current_provider.kind == directory.Provider.PROVIDER_FILE:
            return self.current_provider.on_file(query)
",if self . current_provider . kind == directory . Provider . PROVIDER_SEARCH :,172
"def _text(bitlist):
    out = """"
    for typ, text in bitlist:
        if not typ:
            out += text
        elif typ == ""em"":
            out += ""\\fI%s\\fR"" % text
        elif typ in [""strong"", ""code""]:
            out += ""\\fB%s\\fR"" % text
        else:
            raise ValueError(""unexpected tag %r inside text"" % (typ,))
    out = out.strip()
    out = re.sub(re.compile(r""^\s+"", re.M), """", out)
    return out
","elif typ == ""em"" :",150
"def process(self, buckets):
    with self.executor_factory(max_workers=3) as w:
        futures = {}
        results = []
        for b in buckets:
            futures[w.submit(self.process_bucket, b)] = b
        for f in as_completed(futures):
            if f.exception():
                b = futures[f]
                self.log.error(
                    ""error modifying bucket:%s\n%s"", b[""Name""], f.exception()
                )
            results += filter(None, [f.result()])
        return results
",if f . exception ( ) :,160
"def check_settings(self):
    if self.settings_dict[""TIME_ZONE""] is not None:
        if not settings.USE_TZ:
            raise ImproperlyConfigured(
                ""Connection '%s' cannot set TIME_ZONE because USE_TZ is ""
                ""False."" % self.alias
            )
        elif self.features.supports_timezones:
            raise ImproperlyConfigured(
                ""Connection '%s' cannot set TIME_ZONE because its engine ""
                ""handles time zones conversions natively."" % self.alias
            )
",if not settings . USE_TZ :,140
"def process_webhook_prop(namespace):
    if not isinstance(namespace.webhook_properties, list):
        return
    result = {}
    for each in namespace.webhook_properties:
        if each:
            if ""="" in each:
                key, value = each.split(""="", 1)
            else:
                key, value = each, """"
            result[key] = value
    namespace.webhook_properties = result
","if ""="" in each :",111
"def _expand_query_values(original_query_list):
    query_list = []
    for key, value in original_query_list:
        if isinstance(value, basestring):
            query_list.append((key, value))
        else:
            key_fmt = key + ""[%s]""
            value_list = _to_kv_list(value)
            query_list.extend((key_fmt % k, v) for k, v in value_list)
    return query_list
","if isinstance ( value , basestring ) :",127
"def tags():
    """"""Return a dictionary of all tags in the form {hash: [tag_names, ...]}.""""""
    tags = {}
    for (n, c) in list_refs():
        if n.startswith(""refs/tags/""):
            name = n[10:]
            if not c in tags:
                tags[c] = []
            tags[c].append(name)  # more than one tag can point at 'c'
    return tags
",if not c in tags :,116
"def test_colorspiral(self):
    """"""Set of 625 colours, with jitter, using get_colors().""""""
    boxedge = 20
    boxes_per_row = 25
    rows = 0
    for i, c in enumerate(get_colors(625)):
        self.c.setFillColor(c)
        x1 = boxedge * (i % boxes_per_row)
        y1 = rows * boxedge
        self.c.rect(x1, y1, boxedge, boxedge, fill=1, stroke=0)
        if not (i + 1) % boxes_per_row:
            rows += 1
    self.finish()
",if not ( i + 1 ) % boxes_per_row :,164
"def oldest_pending_update_in_days():
    """"""Return the datestamp of the oldest pending update""""""
    pendingupdatespath = os.path.join(
        prefs.pref(""ManagedInstallDir""), ""UpdateNotificationTracking.plist""
    )
    try:
        pending_updates = FoundationPlist.readPlist(pendingupdatespath)
    except FoundationPlist.NSPropertyListSerializationException:
        return 0
    oldest_date = now = NSDate.date()
    for category in pending_updates:
        for name in pending_updates[category]:
            this_date = pending_updates[category][name]
            if this_date < oldest_date:
                oldest_date = this_date
    return now.timeIntervalSinceDate_(oldest_date) / (24 * 60 * 60)
",if this_date < oldest_date :,199
"def _try_read_gpg(path):
    path = os.path.expanduser(path)
    cmd = _gpg_cmd() + [path]
    log.debug(""gpg cmd: %s"", cmd)
    try:
        p = subprocess.Popen(
            cmd, env=os.environ, stdout=subprocess.PIPE, stderr=subprocess.PIPE
        )
    except OSError as e:
        log.error(""cannot decode %s with command '%s' (%s)"", path, "" "".join(cmd), e)
    else:
        out, err = p.communicate()
        if p.returncode != 0:
            log.error(err.decode(errors=""replace"").strip())
            return None
        return out.decode(errors=""replace"")
",if p . returncode != 0 :,190
"def sort_nested_dictionary_lists(d):
    for k, v in d.items():
        if isinstance(v, list):
            for i in range(0, len(v)):
                if isinstance(v[i], dict):
                    v[i] = await sort_nested_dictionary_lists(v[i])
                d[k] = sorted(v)
        if isinstance(v, dict):
            d[k] = await sort_nested_dictionary_lists(v)
    return d
","if isinstance ( v , list ) :",134
"def _the_callback(widget, event_id):
    point = widget.GetCenter()
    index = widget.WIDGET_INDEX
    if hasattr(callback, ""__call__""):
        if num > 1:
            args = [point, index]
        else:
            args = [point]
        if pass_widget:
            args.append(widget)
        try_callback(callback, *args)
    return
",if num > 1 :,109
"def _add_cs(master_cs, sub_cs, prefix, delimiter=""."", parent_hp=None):
    new_parameters = []
    for hp in sub_cs.get_hyperparameters():
        new_parameter = copy.deepcopy(hp)
        # Allow for an empty top-level parameter
        if new_parameter.name == """":
            new_parameter.name = prefix
        elif not prefix == """":
            new_parameter.name = ""{}{}{}"".format(prefix, SPLITTER, new_parameter.name)
        new_parameters.append(new_parameter)
    for hp in new_parameters:
        _add_hp(master_cs, hp)
","if new_parameter . name == """" :",162
"def tearDown(self):
    """"""Shutdown the server.""""""
    try:
        if self.server:
            self.server.stop()
        if self.sl_hdlr:
            self.root_logger.removeHandler(self.sl_hdlr)
            self.sl_hdlr.close()
    finally:
        BaseTest.tearDown(self)
",if self . server :,92
"def app_uninstall_all(self, excludes=[], verbose=False):
    """"""Uninstall all apps""""""
    our_apps = [""com.github.uiautomator"", ""com.github.uiautomator.test""]
    output, _ = self.shell([""pm"", ""list"", ""packages"", ""-3""])
    pkgs = re.findall(r""package:([^\s]+)"", output)
    pkgs = set(pkgs).difference(our_apps + excludes)
    pkgs = list(pkgs)
    for pkg_name in pkgs:
        if verbose:
            print(""uninstalling"", pkg_name, "" "", end="""", flush=True)
        ok = self.app_uninstall(pkg_name)
        if verbose:
            print(""OK"" if ok else ""FAIL"")
    return pkgs
",if verbose :,188
"def httpapi(self, arg, opts):
    sc = HttpAPIStatsCollector()
    headers = [""#Item"", ""Value""]
    table = []
    for k, v in sc.get().getStats().items():
        if isinstance(v, dict):
            v = json.dumps(v)
        row = []
        row.append(""#%s"" % k)
        if k[-3:] == ""_at"":
            row.append(formatDateTime(v))
        else:
            row.append(v)
        table.append(row)
    self.protocol.sendData(
        tabulate(table, headers, tablefmt=""plain"", numalign=""left"").encode(""ascii"")
    )
","if k [ - 3 : ] == ""_at"" :",178
"def Get_Gene(self, id):
    """"""Retreive the gene name (GN).""""""
    entry = self.Get(id)
    if not entry:
        return None
    GN = """"
    for line in string.split(entry, ""\n""):
        if line[0:5] == ""GN   "":
            GN = string.strip(line[5:])
            if GN[-1] == ""."":
                GN = GN[0:-1]
            return GN
        if line[0:2] == ""//"":
            break
    return GN
","if line [ 0 : 5 ] == ""GN   "" :",150
"def replace_dir_vars(path, d):
    """"""Replace common directory paths with appropriate variable references (e.g. /etc becomes ${sysconfdir})""""""
    dirvars = {}
    # Sort by length so we get the variables we're interested in first
    for var in sorted(list(d.keys()), key=len):
        if var.endswith(""dir"") and var.lower() == var:
            value = d.getVar(var)
            if value.startswith(""/"") and not ""\n"" in value and value not in dirvars:
                dirvars[value] = var
    for dirpath in sorted(list(dirvars.keys()), reverse=True):
        path = path.replace(dirpath, ""${%s}"" % dirvars[dirpath])
    return path
","if value . startswith ( ""/"" ) and not ""\n"" in value and value not in dirvars :",184
"def _scrub_generated_timestamps(self, target_workdir):
    """"""Remove the first line of comment from each file if it contains a timestamp.""""""
    for root, _, filenames in safe_walk(target_workdir):
        for filename in filenames:
            source = os.path.join(root, filename)
            with open(source, ""r"") as f:
                lines = f.readlines()
            if len(lines) < 1:
                return
            with open(source, ""w"") as f:
                if not self._COMMENT_WITH_TIMESTAMP_RE.match(lines[0]):
                    f.write(lines[0])
                for line in lines[1:]:
                    f.write(line)
",if not self . _COMMENT_WITH_TIMESTAMP_RE . match ( lines [ 0 ] ) :,196
"def get_all_active_plugins(self) -> List[BotPlugin]:
    """"""This returns the list of plugins in the callback ordered defined from the config.""""""
    all_plugins = []
    for name in self.plugins_callback_order:
        # None is a placeholder for any plugin not having a defined order
        if name is None:
            all_plugins += [
                plugin
                for name, plugin in self.plugins.items()
                if name not in self.plugins_callback_order and plugin.is_activated
            ]
        else:
            plugin = self.plugins[name]
            if plugin.is_activated:
                all_plugins.append(plugin)
    return all_plugins
",if name is None :,186
"def test_query_level(self):
    ""Tests querying at a level other than max""
    # level 2
    l2 = set()
    for p in self.tile_paths:
        l2.add(p[0:2])
    for path in iterate_base4(2):
        if path in l2:
            self.assertTrue(self.tree.query_path(path))
        else:
            self.assertFalse(self.tree.query_path(path))
    # level 1:
    self.assertTrue(self.tree.query_path((0,)))
    self.assertTrue(self.tree.query_path((1,)))
    self.assertTrue(self.tree.query_path((2,)))
    self.assertFalse(self.tree.query_path((3,)))
",if path in l2 :,191
"def program_exists(name):
    paths = (os.getenv(""PATH"") or os.defpath).split(os.pathsep)
    for p in paths:
        fn = ""%s/%s"" % (p, name)
        if os.path.exists(fn):
            return not os.path.isdir(fn) and os.access(fn, os.X_OK)
",if os . path . exists ( fn ) :,93
"def decoration_helper(self, patched, args, keywargs):
    extra_args = []
    with contextlib.ExitStack() as exit_stack:
        for patching in patched.patchings:
            arg = exit_stack.enter_context(patching)
            if patching.attribute_name is not None:
                keywargs.update(arg)
            elif patching.new is DEFAULT:
                extra_args.append(arg)
        args += tuple(extra_args)
        yield (args, keywargs)
",if patching . attribute_name is not None :,134
"def update_neighbor(neigh_ip_address, changes):
    rets = []
    for k, v in changes.items():
        if k == neighbors.MULTI_EXIT_DISC:
            rets.append(_update_med(neigh_ip_address, v))
        if k == neighbors.ENABLED:
            rets.append(update_neighbor_enabled(neigh_ip_address, v))
        if k == neighbors.CONNECT_MODE:
            rets.append(_update_connect_mode(neigh_ip_address, v))
    return all(rets)
",if k == neighbors . ENABLED :,138
"def calcUniqueStates(self):
    # Here we show which colors can be relied on to map to an
    # internal state.  The current position will be at the first
    # character in the buffer styled that color, so this might not
    # work in all cases.
    self.uniqueStates = {}
    for k in self.holdUniqueStates.keys():
        v = self.holdUniqueStates[k]
        if len(v.keys()) == 1:
            self.uniqueStates[k] = v.keys()[0]
            log.debug(""Map style [%s] to state [%s]"", k, v.keys()[0])
        log.debug(""Style [%s] maps to states [%s]"", k, "", "".join(v.keys()))
    self.holdUniqueStates = None
",if len ( v . keys ( ) ) == 1 :,191
"def init_logger():
    configured_loggers = [log_config.get(""root"", {})] + [
        logger for logger in log_config.get(""loggers"", {}).values()
    ]
    used_handlers = {
        handler for log in configured_loggers for handler in log.get(""handlers"", [])
    }
    for handler_id, handler in list(log_config[""handlers""].items()):
        if handler_id not in used_handlers:
            del log_config[""handlers""][handler_id]
        elif ""filename"" in handler.keys():
            filename = handler[""filename""]
            logfile_path = Path(filename).expanduser().resolve()
            handler[""filename""] = str(logfile_path)
    logging.config.dictConfig(log_config)
",if handler_id not in used_handlers :,192
"def _selected_machines(self, virtual_machines):
    selected_machines = []
    for machine in virtual_machines:
        if self._args.host and self._args.host == machine.name:
            selected_machines.append(machine)
        if self.tags and self._tags_match(machine.tags, self.tags):
            selected_machines.append(machine)
        if self.locations and machine.location in self.locations:
            selected_machines.append(machine)
    return selected_machines
",if self . _args . host and self . _args . host == machine . name :,129
"def init(self):
    r = self.get_redis()
    if r:
        key = ""pocsuite_target""
        info_msg = ""[PLUGIN] try fetch targets from redis...""
        logger.info(info_msg)
        targets = r.get(key)
        count = 0
        if targets:
            for target in targets:
                if self.add_target(target):
                    count += 1
        info_msg = ""[PLUGIN] get {0} target(s) from redis"".format(count)
        logger.info(info_msg)
",if self . add_target ( target ) :,151
"def tearDown(self):
    suffix = str(os.getgid())
    cli = monitoring_v3.MetricServiceClient()
    for md in cli.list_metric_descriptors(""projects/{}"".format(PROJECT)):
        if ""OpenCensus"" in md.name and suffix in md.name:
            try:
                cli.delete_metric_descriptor(md.name)
            except Exception:
                pass
","if ""OpenCensus"" in md . name and suffix in md . name :",106
"def InitializeColours(self):
    """"""Initializes the 16 custom colours in :class:`CustomPanel`.""""""
    curr = self._colourData.GetColour()
    self._colourSelection = -1
    for i in range(16):
        c = self._colourData.GetCustomColour(i)
        if c.IsOk():
            self._customColours[i] = self._colourData.GetCustomColour(i)
        else:
            self._customColours[i] = wx.WHITE
        if c == curr:
            self._colourSelection = i
",if c == curr :,147
"def __getitem__(self, index):
    if self._check():
        if isinstance(index, int):
            if index < 0 or index >= len(self.features):
                raise IndexError(index)
            if self.features[index] is None:
                feature = self.device.feature_request(FEATURE.FEATURE_SET, 0x10, index)
                if feature:
                    (feature,) = _unpack(""!H"", feature[:2])
                    self.features[index] = FEATURE[feature]
            return self.features[index]
        elif isinstance(index, slice):
            indices = index.indices(len(self.features))
            return [self.__getitem__(i) for i in range(*indices)]
",if feature :,195
"def _get_data_from_buffer(obj):
    try:
        view = memoryview(obj)
    except TypeError:
        # try to use legacy buffer protocol if 2.7, otherwise re-raise
        if PY2:
            view = memoryview(buffer(obj))
            warnings.warn(
                ""using old buffer interface to unpack %s; ""
                ""this leads to unpacking errors if slicing is used and ""
                ""will be removed in a future version"" % type(obj),
                RuntimeWarning,
                stacklevel=3,
            )
        else:
            raise
    if view.itemsize != 1:
        raise ValueError(""cannot unpack from multi-byte object"")
    return view
",if PY2 :,188
"def import_modules(modules, safe=True):
    """"""Safely import a list of *modules*""""""
    all = []
    for mname in modules:
        if mname.endswith("".*""):
            to_load = expand_star(mname)
        else:
            to_load = [mname]
        for module in to_load:
            try:
                all.append(import_module(module))
            except ImportError:
                if not safe:
                    raise
    return all
",if not safe :,139
"def pack(types, *args):
    if len(types) != len(args):
        raise Exception(""number of arguments does not match format string"")
    port = StringIO()
    for (type, value) in zip(types, args):
        if type == ""V"":
            write_vuint(port, value)
        elif type == ""v"":
            write_vint(port, value)
        elif type == ""s"":
            write_bvec(port, value)
        else:
            raise Exception('unknown xpack format string item ""' + type + '""')
    return port.getvalue()
","elif type == ""v"" :",153
"def create_local_app_folder(local_app_path):
    if exists(local_app_path):
        raise ValueError(""There is already a '%s' folder! Aborting!"" % local_app_path)
    for folder in subfolders(local_app_path):
        if not exists(folder):
            os.mkdir(folder)
            init_path = join(folder, ""__init__.py"")
            if not exists(init_path):
                create_file(init_path)
",if not exists ( folder ) :,126
"def _get_node_type_specific_fields(self, node_id: str, fields_key: str) -> Any:
    fields = self.config[fields_key]
    node_tags = self.provider.node_tags(node_id)
    if TAG_RAY_USER_NODE_TYPE in node_tags:
        node_type = node_tags[TAG_RAY_USER_NODE_TYPE]
        if node_type not in self.available_node_types:
            raise ValueError(f""Unknown node type tag: {node_type}."")
        node_specific_config = self.available_node_types[node_type]
        if fields_key in node_specific_config:
            fields = node_specific_config[fields_key]
    return fields
",if node_type not in self . available_node_types :,189
"def _maybe_fix_sequence_in_union(
    aliases: List[Alias], typecst: cst.SubscriptElement
) -> cst.SubscriptElement:
    slc = typecst.slice
    if isinstance(slc, cst.Index):
        val = slc.value
        if isinstance(val, cst.Subscript):
            return cst.ensure_type(
                typecst.deep_replace(val, _get_clean_type_from_subscript(aliases, val)),
                cst.SubscriptElement,
            )
    return typecst
","if isinstance ( val , cst . Subscript ) :",144
"def cancel_download(self, downloads):
    # Make sure we're always dealing with a list
    if isinstance(downloads, Download):
        downloads = [downloads]
    for download in downloads:
        if download == self.__current_download:
            self.cancel_current_download()
        else:
            self.__paused = True
            new_queue = queue.Queue()
            while not self.__queue.empty():
                queued_download = self.__queue.get()
                if download == queued_download:
                    download.cancel()
                else:
                    new_queue.put(queued_download)
            self.__queue = new_queue
            self.__paused = False
",if download == self . __current_download :,188
"def migrate_account_metadata(account_id):
    from inbox.models.session import session_scope
    from inbox.models import Account
    with session_scope(versioned=False) as db_session:
        account = db_session.query(Account).get(account_id)
        if account.discriminator == ""easaccount"":
            create_categories_for_easfoldersyncstatuses(account, db_session)
        else:
            create_categories_for_folders(account, db_session)
        if account.discriminator == ""gmailaccount"":
            set_labels_for_imapuids(account, db_session)
        db_session.commit()
","if account . discriminator == ""gmailaccount"" :",168
"def __init__(self, fmt=None, *args):
    if not isinstance(fmt, BaseException):
        Error.__init__(self, fmt, *args)
    else:
        e = fmt
        cls = e.__class__
        fmt = ""%s.%s: %s"" % (cls.__module__, cls.__name__, e)
        tb = sys.exc_info()[2]
        if tb:
            fmt += ""\n""
            fmt += """".join(traceback.format_tb(tb))
        Error.__init__(self, fmt)
",if tb :,138
"def setLabel(self, label):
    if label is None:
        if self.label is not None:
            self.label.scene().removeItem(self.label)
            self.label = None
    else:
        if self.label is None:
            self.label = TextItem()
            self.label.setParentItem(self)
        self.label.setText(label)
        self._updateLabel()
",if self . label is not None :,112
"def serve_until_stopped(self) -> None:
    while True:
        rd, wr, ex = select.select([self.socket.fileno()], [], [], self.timeout)
        if rd:
            self.handle_request()
        if self.event is not None and self.event.is_set():
            break
",if rd :,83
"def generateCompressedFile(inputfile, outputfile, formatstring):
    try:
        if formatstring == ""w:xz"":
            in_file = open(inputfile, ""rb"")
            in_data = in_file.read()
            out_file = open(inputfile + "".xz"", ""wb"")
            out_file.write(xz.compress(in_data))
            in_file.close()
            out_file.close()
        else:
            tarout = tarfile.open(outputfile, formatstring)
            tarout.add(inputfile, arcname=os.path.basename(inputfile))
            tarout.close()
    except Exception as e:
        print(e)
        return False
    return True
","if formatstring == ""w:xz"" :",191
"def _datastore_get_handler(signal, sender, keys, **kwargs):
    txn = current_transaction()
    if txn:
        for key in keys:
            if key in txn._protected_keys:
                raise PreventedReadError(
                    ""Attempted to read key (%s:%s) inside a transaction ""
                    ""where it was marked protected"" % (key.kind(), key.id_or_name())
                )
        txn._fetched_keys.update(set(keys))
",if key in txn . _protected_keys :,130
"def TryMerge(self, d):
    while d.avail() > 0:
        tt = d.getVarInt32()
        if tt == 10:
            self.set_access_token(d.getPrefixedString())
            continue
        if tt == 16:
            self.set_expiration_time(d.getVarInt64())
            continue
        if tt == 0:
            raise ProtocolBuffer.ProtocolBufferDecodeError
        d.skipData(tt)
",if tt == 10 :,124
"def write_vuint(port, x):
    if x < 0:
        raise Exception(""vuints must not be negative"")
    elif x == 0:
        port.write(""\0"")
    else:
        while x:
            seven_bits = x & 0x7F
            x >>= 7
            if x:
                port.write(chr(0x80 | seven_bits))
            else:
                port.write(chr(seven_bits))
",if x :,129
"def _expand_srcs(self):
    """"""Expand src to [(src, full_path)]""""""
    result = []
    for src in self.srcs:
        full_path = self._source_file_path(src)
        if not os.path.exists(full_path):
            # Assume generated
            full_path = self._target_file_path(src)
        result.append((src, full_path))
    return result
",if not os . path . exists ( full_path ) :,113
"def pytest_collection_modifyitems(items):
    for item in items:
        if item.nodeid.startswith(""tests/ops""):
            if ""stage"" not in item.keywords:
                item.add_marker(pytest.mark.stage(""unit""))
            if ""init"" not in item.keywords:
                item.add_marker(pytest.mark.init(rng_seed=123))
","if ""init"" not in item . keywords :",102
"def set_shape(self, shape):
    """"""Sets a shape.""""""
    if self._shape is not None:
        logger.warning('Modifying the shape of Placeholder ""%s"".', self.name)
    if not isinstance(shape, (list, tuple)):
        shape = (shape,)
    shape = tuple(x if x != ""None"" else None for x in shape)
    for x in shape:
        if not isinstance(x, (int, type(None))):
            raise ParsingError(
                'All entries in ""shape"" must be integers, or in special '
                ""cases None. Shape is: {}"".format(shape)
            )
    self._shape = shape
","if not isinstance ( x , ( int , type ( None ) ) ) :",169
"def _get_field_actual(cant_be_number, raw_string, field_names):
    for line in raw_string.splitlines():
        for field_name in field_names:
            field_name = field_name.lower()
            if "":"" in line:
                left, right = line.split("":"", 1)
                left = left.strip().lower()
                right = right.strip()
                if left == field_name and len(right) > 0:
                    if cant_be_number:
                        if not right.isdigit():
                            return right
                    else:
                        return right
    return None
",if left == field_name and len ( right ) > 0 :,184
"def validate_attributes(self):
    for attribute in self.get_all_attributes():
        value = getattr(self, attribute.code, None)
        if value is None:
            if attribute.required:
                raise ValidationError(
                    _(""%(attr)s attribute cannot be blank"") % {""attr"": attribute.code}
                )
        else:
            try:
                attribute.validate_value(value)
            except ValidationError as e:
                raise ValidationError(
                    _(""%(attr)s attribute %(err)s"") % {""attr"": attribute.code, ""err"": e}
                )
",if attribute . required :,168
"def append(self, s):
    buf = self.buf
    if buf is None:
        strbuf = self.strbuf
        if len(strbuf) + len(s) < STRBUF_LIMIT:
            self.strbuf = strbuf + s
            return
        buf = self._create_buffer()
    buf.append(s)
    # use buf.__len__ rather than len(buf) FBO of not getting
    # OverflowError on Python 2
    sz = buf.__len__()
    if not self.overflowed:
        if sz >= self.overflow:
            self._set_large_buffer()
",if len ( strbuf ) + len ( s ) < STRBUF_LIMIT :,154
"def billing_invoice_show_validator(namespace):
    from azure.cli.core.azclierror import (
        RequiredArgumentMissingError,
        MutuallyExclusiveArgumentError,
    )
    valid_combs = (
        ""only --account-name, --name / --name / --name, --by-subscription is valid""
    )
    if namespace.account_name is not None:
        if namespace.by_subscription is not None:
            raise MutuallyExclusiveArgumentError(valid_combs)
        if namespace.name is None:
            raise RequiredArgumentMissingError(""--name is also required"")
    if namespace.by_subscription is not None:
        if namespace.name is None:
            raise RequiredArgumentMissingError(""--name is also required"")
",if namespace . by_subscription is not None :,188
"def Handle(self, args, context=None):
    for client_id in args.client_ids:
        cid = str(client_id)
        data_store.REL_DB.RemoveClientLabels(cid, context.username, args.labels)
        labels_to_remove = set(args.labels)
        existing_labels = data_store.REL_DB.ReadClientLabels(cid)
        for label in existing_labels:
            labels_to_remove.discard(label.name)
        if labels_to_remove:
            idx = client_index.ClientIndex()
            idx.RemoveClientLabels(cid, labels_to_remove)
",if labels_to_remove :,164
"def delete_snapshot(self, snapshot):
    snap_name = self._get_snap_name(snapshot[""id""])
    LOG.debug(""Deleting snapshot (%s)"", snapshot[""id""])
    self.client_login()
    try:
        self.client.delete_snapshot(snap_name, self.backend_type)
    except exception.DotHillRequestError as ex:
        # if the volume wasn't found, ignore the error
        if ""The volume was not found on this system."" in ex.args:
            return
        LOG.exception(""Deleting snapshot %s failed"", snapshot[""id""])
        raise exception.Invalid(ex)
    finally:
        self.client_logout()
","if ""The volume was not found on this system."" in ex . args :",165
"def jobs(self):
    # How many jobs have we done?
    total_processed = 0
    for jobEntity in self.jobItems.query_entities():
        # Process the items in the page
        yield AzureJob.fromEntity(jobEntity)
        total_processed += 1
        if total_processed % 1000 == 0:
            # Produce some feedback for the user, because this can take
            # a long time on, for example, Azure
            logger.debug(""Processed %d total jobs"" % total_processed)
    logger.debug(""Processed %d total jobs"" % total_processed)
",if total_processed % 1000 == 0 :,153
"def run(self):
    while not self.completed:
        if self.block:
            time.sleep(self.period)
        else:
            self._completed.wait(self.period)
        self.counter += 1
        try:
            self.callback(self.counter)
        except Exception:
            self.stop()
        if self.timeout is not None:
            dt = time.time() - self._start_time
            if dt > self.timeout:
                self.stop()
        if self.counter == self.count:
            self.stop()
",if self . counter == self . count :,159
"def get_instance(cls, pool_size=None):
    if cls._instance is not None:
        return cls._instance
    # Lazy init
    with cls._SINGLETON_LOCK:
        if cls._instance is None:
            cls._instance = cls(
                ARCTIC_ASYNC_NWORKERS if pool_size is None else pool_size
            )
    return cls._instance
",if cls . _instance is None :,102
"def set_state(self, state):
    if self._inhibit_play:
        # PLAYING, PAUSED change the state for after buffering is finished,
        # everything else aborts buffering
        if state not in (Gst.State.PLAYING, Gst.State.PAUSED):
            # abort
            self.__set_inhibit_play(False)
            self.bin.set_state(state)
            return
        self._wanted_state = state
    else:
        self.bin.set_state(state)
","if state not in ( Gst . State . PLAYING , Gst . State . PAUSED ) :",136
"def seen_add(options):
    seen_name = options.add_value
    if is_imdb_url(seen_name):
        console(""IMDB url detected, try to parse ID"")
        imdb_id = extract_id(seen_name)
        if imdb_id:
            seen_name = imdb_id
        else:
            console(""Could not parse IMDB ID"")
    db.add(seen_name, ""cli_add"", {""cli_add"": seen_name})
    console(""Added %s as seen. This will affect all tasks."" % seen_name)
",if imdb_id :,144
"def test_204_invalid_content_length(self):
    # 204 status with non-zero content length is malformed
    with ExpectLog(gen_log, "".*Response with code 204 should not have body""):
        response = self.fetch(""/?error=1"")
        if not self.http1:
            self.skipTest(""requires HTTP/1.x"")
        if self.http_client.configured_class != SimpleAsyncHTTPClient:
            self.skipTest(""curl client accepts invalid headers"")
        self.assertEqual(response.code, 599)
",if self . http_client . configured_class != SimpleAsyncHTTPClient :,136
"def set_related_perm(_mapper: Mapper, _connection: Connection, target: Slice) -> None:
    src_class = target.cls_model
    id_ = target.datasource_id
    if id_:
        ds = db.session.query(src_class).filter_by(id=int(id_)).first()
        if ds:
            target.perm = ds.perm
            target.schema_perm = ds.schema_perm
",if ds :,110
"def on_modified_async(self, view):
    if self.is_command_line(view):
        if view.size() > 6 and view.substr(sublime.Region(0, 6)).lower() == ""search"":
            view.run_command(""text_pastry_selection_preview"")
","if view . size ( ) > 6 and view . substr ( sublime . Region ( 0 , 6 ) ) . lower ( ) == ""search"" :",76
"def _improve_answer_span(
    doc_tokens, input_start, input_end, tokenizer, orig_answer_text
):
    """"""Returns tokenized answer spans that better match the annotated answer.""""""
    tok_answer_text = "" "".join(tokenizer.tokenize(orig_answer_text))
    for new_start in range(input_start, input_end + 1):
        for new_end in range(input_end, new_start - 1, -1):
            text_span = "" "".join(doc_tokens[new_start : (new_end + 1)])
            if text_span == tok_answer_text:
                return new_start, new_end
    return input_start, input_end
",if text_span == tok_answer_text :,174
"def TryMerge(self, d):
    while d.avail() > 0:
        tt = d.getVarInt32()
        if tt == 10:
            self.set_url(d.getPrefixedString())
            continue
        if tt == 18:
            self.set_app_version_id(d.getPrefixedString())
            continue
        if tt == 26:
            self.set_method(d.getPrefixedString())
            continue
        if tt == 34:
            self.set_queue(d.getPrefixedString())
            continue
        if tt == 0:
            raise ProtocolBuffer.ProtocolBufferDecodeError
        d.skipData(tt)
",if tt == 10 :,184
"def _add_resource_group(obj):
    if isinstance(obj, list):
        for array_item in obj:
            _add_resource_group(array_item)
    elif isinstance(obj, dict):
        try:
            if ""resourcegroup"" not in [x.lower() for x in obj.keys()]:
                if obj[""id""]:
                    obj[""resourceGroup""] = _parse_id(obj[""id""])[""resource-group""]
        except (KeyError, IndexError, TypeError):
            pass
        for item_key in obj:
            if item_key != ""sourceVault"":
                _add_resource_group(obj[item_key])
","if ""resourcegroup"" not in [ x . lower ( ) for x in obj . keys ( ) ] :",175
"def build(opt):
    dpath = os.path.join(opt[""datapath""], DECODE)
    version = DECODE_VERSION
    if not build_data.built(dpath, version_string=version):
        print(""[building data: "" + dpath + ""]"")
        if build_data.built(dpath):
            # An older version exists, so remove these outdated files.
            build_data.remove_dir(dpath)
        build_data.make_dir(dpath)
        # Download the data.
        for downloadable_file in RESOURCES:
            downloadable_file.download_file(dpath)
        # Mark the data as built.
        build_data.mark_done(dpath, version_string=version)
",if build_data . built ( dpath ) :,183
"def toterminal(self, tw):
    # the entries might have different styles
    last_style = None
    for i, entry in enumerate(self.reprentries):
        if entry.style == ""long"":
            tw.line("""")
        entry.toterminal(tw)
        if i < len(self.reprentries) - 1:
            next_entry = self.reprentries[i + 1]
            if (
                entry.style == ""long""
                or entry.style == ""short""
                and next_entry.style == ""long""
            ):
                tw.sep(self.entrysep)
    if self.extraline:
        tw.line(self.extraline)
","if entry . style == ""long"" :",198
"def reposition_division(f1):
    lines = f1.splitlines()
    if lines[2] == division:
        lines.pop(2)
    found = 0
    for i, line in enumerate(lines):
        if line.startswith('""""""'):
            found += 1
            if found == 2:
                if division in ""\n"".join(lines):
                    break  # already in the right place
                lines.insert(i + 1, """")
                lines.insert(i + 2, division)
                break
    return ""\n"".join(lines)
","if line . startswith ( '""""""' ) :",153
"def run_on_module(self):
    try:
        self.module_base.disable(self.opts.module_spec)
    except dnf.exceptions.MarkingErrors as e:
        if self.base.conf.strict:
            if e.no_match_group_specs or e.error_group_specs:
                raise e
            if (
                e.module_depsolv_errors
                and e.module_depsolv_errors[1]
                != libdnf.module.ModulePackageContainer.ModuleErrorType_ERROR_IN_DEFAULTS
            ):
                raise e
        logger.error(str(e))
",if e . no_match_group_specs or e . error_group_specs :,174
"def test_len(self):
    eq = self.assertEqual
    eq(base64mime.base64_len(""hello""), len(base64mime.encode(""hello"", eol="""")))
    for size in range(15):
        if size == 0:
            bsize = 0
        elif size <= 3:
            bsize = 4
        elif size <= 6:
            bsize = 8
        elif size <= 9:
            bsize = 12
        elif size <= 12:
            bsize = 16
        else:
            bsize = 20
        eq(base64mime.base64_len(""x"" * size), bsize)
",elif size <= 9 :,160
"def is_valid(self):
    """"""Determines whether file is valid for this reader""""""
    blocklist = self.open()
    valid = True
    for line in blocklist:
        line = decode_bytes(line)
        if not self.is_ignored(line):
            try:
                (start, end) = self.parse(line)
                if not re.match(r""^(\d{1,3}\.){4}$"", start + ""."") or not re.match(
                    r""^(\d{1,3}\.){4}$"", end + "".""
                ):
                    valid = False
            except Exception:
                valid = False
            break
    blocklist.close()
    return valid
",if not self . is_ignored ( line ) :,188
"def next(self):
    while self.index < len(self.data):
        uid = self._read_next_word()
        dont_care = self._read_next_word()
        entry = self._read_next_string()
        total_size = int(4 + 4 + len(entry))
        count = int(total_size / self.SIZE)
        if count == 0:
            mod = self.SIZE - total_size
        else:
            mod = self.SIZE - int(total_size - (count * self.SIZE))
        if mod > 0:
            remainder = self._read_next_block(mod)
        yield (uid, entry)
",if mod > 0 :,175
"def _str_param_list(self, name):
    out = []
    if self[name]:
        out += self._str_header(name)
        for param in self[name]:
            parts = []
            if param.name:
                parts.append(param.name)
            if param.type:
                parts.append(param.type)
            out += ["" : "".join(parts)]
            if param.desc and """".join(param.desc).strip():
                out += self._str_indent(param.desc)
        out += [""""]
    return out
","if param . desc and """" . join ( param . desc ) . strip ( ) :",157
"def assert_backend(self, expected_translated, language=""cs""):
    """"""Check that backend has correct data.""""""
    translation = self.get_translation(language)
    translation.commit_pending(""test"", None)
    store = translation.component.file_format_cls(translation.get_filename(), None)
    messages = set()
    translated = 0
    for unit in store.content_units:
        id_hash = unit.id_hash
        self.assertFalse(id_hash in messages, ""Duplicate string in in backend file!"")
        if unit.is_translated():
            translated += 1
    self.assertEqual(
        translated,
        expected_translated,
        ""Did not found expected number of translations ({} != {})."".format(
            translated, expected_translated
        ),
    )
",if unit . is_translated ( ) :,195
"def status(self, name, error=""No matching script logs found""):
    with self.script_lock:
        if self.script_running and self.script_running[1] == name:
            return self.script_running[1:]
        elif self.script_last and self.script_last[1] == name:
            return self.script_last[1:]
        else:
            raise ValueError(error)
",if self . script_running and self . script_running [ 1 ] == name :,107
"def dict_no_value_from_proto_list(obj_list):
    d = dict()
    for item in obj_list:
        possible_dict = json.loads(item.value_json)
        if not isinstance(possible_dict, dict) or ""value"" not in possible_dict:
            # (tss) TODO: This is protecting against legacy 'wandb_version' field.
            # Should investigate why the config payload even has 'wandb_version'.
            logger.warning(""key '{}' has no 'value' attribute"".format(item.key))
            continue
        d[item.key] = possible_dict[""value""]
    return d
","if not isinstance ( possible_dict , dict ) or ""value"" not in possible_dict :",167
"def visit(self, node):
    """"""dispatcher on node's class/bases name.""""""
    cls = node.__class__
    try:
        visitmethod = self.cache[cls]
    except KeyError:
        for subclass in cls.__mro__:
            visitmethod = getattr(self, subclass.__name__, None)
            if visitmethod is not None:
                break
        else:
            visitmethod = self.__object
        self.cache[cls] = visitmethod
    visitmethod(node)
",if visitmethod is not None :,127
"def _get_adapter(
    mcls,
    reversed_mro: Tuple[type, ...],
    collection: Dict[Any, Dict[type, Adapter]],
    kwargs: Dict[str, Any],
) -> Optional[Adapter]:
    registry_key = mcls.get_registry_key(kwargs)
    adapters = collection.get(registry_key)
    if adapters is None:
        return None
    result = None
    seen: Set[Adapter] = set()
    for base in reversed_mro:
        for adaptee, adapter in adapters.items():
            found = mcls._match_adapter(base, adaptee, adapter)
            if found and found not in seen:
                result = found
                seen.add(found)
    return result
",if found and found not in seen :,190
"def test_pt_BR_rg(self):
    for _ in range(100):
        to_test = self.fake.rg()
        if ""X"" in to_test:
            assert re.search(r""^\d{8}X"", to_test)
        else:
            assert re.search(r""^\d{9}$"", to_test)
","if ""X"" in to_test :",91
"def get_user_extra_data_by_client_id(self, client_id, username):
    extra_data = {}
    current_client = self.clients.get(client_id, None)
    if current_client:
        for readable_field in current_client.get_readable_fields():
            attribute = list(
                filter(
                    lambda f: f[""Name""] == readable_field,
                    self.users.get(username).attributes,
                )
            )
            if len(attribute) > 0:
                extra_data.update({attribute[0][""Name""]: attribute[0][""Value""]})
    return extra_data
",if len ( attribute ) > 0 :,176
"def augment(self, resources):
    super().augment(resources)
    for r in resources:
        md = r.get(""SAMLMetadataDocument"")
        if not md:
            continue
        root = sso_metadata(md)
        r[""IDPSSODescriptor""] = root[""IDPSSODescriptor""]
    return resources
",if not md :,82
"def __init__(self, mode=0, decode=None):
    self.regex = self.REGEX[mode]
    self.decode = decode
    if decode:
        self.header = _(
            ""### This log has been decoded with automatic search pattern\n""
            ""### If some paths are not decoded you can manually decode them with:\n""
        )
        self.header += ""### 'backintime --quiet ""
        if int(decode.config.currentProfile()) > 1:
            self.header += '--profile ""%s"" ' % decode.config.profileName()
        self.header += ""--decode <path>'\n\n""
    else:
        self.header = """"
",if int ( decode . config . currentProfile ( ) ) > 1 :,173
"def _get_dynamic_attr(self, attname, obj, default=None):
    try:
        attr = getattr(self, attname)
    except AttributeError:
        return default
    if callable(attr):
        # Check co_argcount rather than try/excepting the function and
        # catching the TypeError, because something inside the function
        # may raise the TypeError. This technique is more accurate.
        try:
            code = six.get_function_code(attr)
        except AttributeError:
            code = six.get_function_code(attr.__call__)
        if code.co_argcount == 2:  # one argument is 'self'
            return attr(obj)
        else:
            return attr()
    return attr
",if code . co_argcount == 2 :,190
"def grep_full_py_identifiers(tokens):
    global pykeywords
    tokens = list(tokens)
    i = 0
    while i < len(tokens):
        tokentype, token = tokens[i]
        i += 1
        if tokentype != ""id"":
            continue
        while (
            i + 1 < len(tokens)
            and tokens[i] == (""op"", ""."")
            and tokens[i + 1][0] == ""id""
        ):
            token += ""."" + tokens[i + 1][1]
            i += 2
        if token == """":
            continue
        if token in pykeywords:
            continue
        if token[0] in "".0123456789"":
            continue
        yield token
","if tokentype != ""id"" :",194
"def _add_disk_config(self, context, images):
    for image in images:
        metadata = image[""metadata""]
        if INTERNAL_DISK_CONFIG in metadata:
            raw_value = metadata[INTERNAL_DISK_CONFIG]
            value = utils.bool_from_str(raw_value)
            image[API_DISK_CONFIG] = disk_config_to_api(value)
",if INTERNAL_DISK_CONFIG in metadata :,101
"def test_edgeql_expr_valid_setop_07(self):
    expected_error_msg = ""cannot be applied to operands""
    # IF ELSE with every scalar as the condition
    for val in get_test_values():
        query = f""""""SELECT 1 IF {val} ELSE 2;""""""
        if val == ""<bool>True"":
            await self.assert_query_result(query, [1])
        else:
            # every other combination must produce an error
            with self.assertRaisesRegex(
                edgedb.QueryError, expected_error_msg, msg=query
            ):
                async with self.con.transaction():
                    await self.con.execute(query)
","if val == ""<bool>True"" :",179
"def get_all_url_infos() -> Dict[str, UrlInfo]:
    """"""Returns dict associating URL to UrlInfo.""""""
    url_infos = {}
    for path in _checksum_paths().values():
        dataset_url_infos = load_url_infos(path)
        for url, url_info in dataset_url_infos.items():
            if url_infos.get(url, url_info) != url_info:
                raise AssertionError(
                    ""URL {} is registered with 2+ distinct size/checksum tuples. ""
                    ""{} vs {}"".format(url, url_info, url_infos[url])
                )
        url_infos.update(dataset_url_infos)
    return url_infos
","if url_infos . get ( url , url_info ) != url_info :",185
"def global_fixes():
    """"""Yield multiple (code, function) tuples.""""""
    for function in list(globals().values()):
        if inspect.isfunction(function):
            arguments = _get_parameters(function)
            if arguments[:1] != [""source""]:
                continue
            code = extract_code_from_function(function)
            if code:
                yield (code, function)
",if inspect . isfunction ( function ) :,106
"def createSocket(self):
    skt = Port.createSocket(self)
    if self.listenMultiple:
        skt.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)
        if hasattr(socket, ""SO_REUSEPORT""):
            skt.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEPORT, 1)
    return skt
","if hasattr ( socket , ""SO_REUSEPORT"" ) :",90
"def _asStringList(self, sep=""""):
    out = []
    for item in self._toklist:
        if out and sep:
            out.append(sep)
        if isinstance(item, ParseResults):
            out += item._asStringList()
        else:
            out.append(str(item))
    return out
","if isinstance ( item , ParseResults ) :",88
"def parse_c_comments(lexer, tok, ntok):
    if tok != ""/"" or ntok != ""*"":
        return False
    quotes = lexer.quotes
    lexer.quotes = """"
    while True:
        tok = lexer.get_token()
        ntok = lexer.get_token()
        if tok == ""*"" and ntok == ""/"":
            lexer.quotes = quotes
            break
        else:
            lexer.push_token(ntok)
    return True
","if tok == ""*"" and ntok == ""/"" :",121
"def doWorkForFindAll(self, v, target, partialMatch):
    sibling = self
    while sibling:
        c1 = partialMatch and sibling.equalsTreePartial(target)
        if c1:
            v.append(sibling)
        else:
            c2 = not partialMatch and sibling.equalsTree(target)
            if c2:
                v.append(sibling)
        ### regardless of match or not, check any children for matches
        if sibling.getFirstChild():
            sibling.getFirstChild().doWorkForFindAll(v, target, partialMatch)
        sibling = sibling.getNextSibling()
",if c2 :,163
"def __view_beside(self, onsideof, **kwargs):
    bounds = self.info[""bounds""]
    min_dist, found = -1, None
    for ui in UiObject(self.session, Selector(**kwargs)):
        dist = onsideof(bounds, ui.info[""bounds""])
        if dist >= 0 and (min_dist < 0 or dist < min_dist):
            min_dist, found = dist, ui
    return found
",if dist >= 0 and ( min_dist < 0 or dist < min_dist ) :,112
"def __eq__(self, other):
    if isinstance(other, numeric_range):
        empty_self = not bool(self)
        empty_other = not bool(other)
        if empty_self or empty_other:
            return empty_self and empty_other  # True if both empty
        else:
            return (
                self._start == other._start
                and self._step == other._step
                and self._get_by_index(-1) == other._get_by_index(-1)
            )
    else:
        return False
",if empty_self or empty_other :,151
"def _buffered_generator(self, size):
    buf = []
    c_size = 0
    push = buf.append
    while 1:
        try:
            while c_size < size:
                c = next(self._gen)
                push(c)
                if c:
                    c_size += 1
        except StopIteration:
            if not c_size:
                return
        yield concat(buf)
        del buf[:]
        c_size = 0
",if not c_size :,137
"def connect(self):
    with self._conn_lock:
        if self.deferred:
            raise Exception(
                ""Error, database not properly initialized "" ""before opening connection""
            )
        with self.exception_wrapper():
            self.__local.conn = self._connect(self.database, **self.connect_kwargs)
            self.__local.closed = False
            self.initialize_connection(self.__local.conn)
",if self . deferred :,115
"def _merge_substs(self, subst, new_substs):
    subst = subst.copy()
    for new_subst in new_substs:
        for name, var in new_subst.items():
            if name not in subst:
                subst[name] = var
            elif subst[name] is not var:
                subst[name].PasteVariable(var)
    return subst
",if name not in subst :,109
"def remove(self, tag):
    """"""Removes a tag recursively from all containers.""""""
    new_contents = []
    self.content_size = 0
    for element in self.contents:
        if element.name != tag:
            new_contents.append(element)
            if isinstance(element, Container):
                element.remove(tag)
            self.content_size += element.size()
    self.contents = new_contents
","if isinstance ( element , Container ) :",111
"def _create_object(self, obj_body):
    props = obj_body[SYMBOL_PROPERTIES]
    for prop_name, prop_value in props.items():
        if isinstance(prop_value, dict) and prop_value:
            # get the first key as the convert function
            func_name = list(prop_value.keys())[0]
            if func_name.startswith(""_""):
                func = getattr(self, func_name)
                props[prop_name] = func(prop_value[func_name])
    if SYMBOL_TYPE in obj_body and obj_body[SYMBOL_TYPE] in self.fake_func_mapping:
        return self.fake_func_mapping[obj_body[SYMBOL_TYPE]](**props)
    else:
        return props
","if func_name . startswith ( ""_"" ) :",199
"def visit_try_stmt(self, o: ""mypy.nodes.TryStmt"") -> str:
    a = [o.body]  # type: List[Any]
    for i in range(len(o.vars)):
        a.append(o.types[i])
        if o.vars[i]:
            a.append(o.vars[i])
        a.append(o.handlers[i])
    if o.else_body:
        a.append((""Else"", o.else_body.body))
    if o.finally_body:
        a.append((""Finally"", o.finally_body.body))
    return self.dump(a, o)
",if o . vars [ i ] :,166
"def everythingIsUnicode(d):
    """"""Takes a dictionary, recursively verifies that every value is unicode""""""
    for k, v in d.iteritems():
        if isinstance(v, dict) and k != ""headers"":
            if not everythingIsUnicode(v):
                return False
        elif isinstance(v, list):
            for i in v:
                if isinstance(i, dict) and not everythingIsUnicode(i):
                    return False
                elif isinstance(i, _bytes):
                    return False
        elif isinstance(v, _bytes):
            return False
    return True
","elif isinstance ( i , _bytes ) :",158
"def msg_ser(inst, sformat, lev=0):
    if sformat in [""urlencoded"", ""json""]:
        if isinstance(inst, Message):
            res = inst.serialize(sformat, lev)
        else:
            res = inst
    elif sformat == ""dict"":
        if isinstance(inst, Message):
            res = inst.serialize(sformat, lev)
        elif isinstance(inst, dict):
            res = inst
        elif isinstance(inst, str):  # Iff ID Token
            res = inst
        else:
            raise MessageException(""Wrong type: %s"" % type(inst))
    else:
        raise PyoidcError(""Unknown sformat"", inst)
    return res
","elif isinstance ( inst , dict ) :",182
"def start_container_if_stopped(self, container, attach_logs=False, quiet=False):
    if not container.is_running:
        if not quiet:
            log.info(""Starting %s"" % container.name)
        if attach_logs:
            container.attach_log_stream()
        return self.start_container(container)
",if not quiet :,90
"def layer_op(self, input_image, mask=None):
    if not isinstance(input_image, dict):
        self._set_full_border(input_image)
        input_image = np.pad(input_image, self.full_border, mode=self.mode)
        return input_image, mask
    for name, image in input_image.items():
        self._set_full_border(image)
        if name not in self.image_name:
            tf.logging.warning(
                ""could not pad, dict name %s not in %s"", name, self.image_name
            )
            continue
        input_image[name] = np.pad(image, self.full_border, mode=self.mode)
    return input_image, mask
",if name not in self . image_name :,200
"def __Suffix_Noun_Step2b(self, token):
    for suffix in self.__suffix_noun_step2b:
        if token.endswith(suffix) and len(token) >= 5:
            token = token[:-2]
            self.suffix_noun_step2b_success = True
            break
    return token
",if token . endswith ( suffix ) and len ( token ) >= 5 :,86
"def replace_header_items(ps, replacments):
    match = read_while(ps, header_item_or_end_re.match, lambda match: match is None)
    while not ps.current_line.startswith(""*/""):
        match = header_item_re.match(ps.current_line)
        if match is not None:
            key = match.groupdict()[""key""]
            if key in replacments:
                ps.current_line = match.expand(
                    ""\g<key>\g<space>%s\n"" % replacments[key]
                )
        ps.read_line()
",if match is not None :,163
"def __projectBookmark(widget, location):
    script = None
    while widget is not None:
        if hasattr(widget, ""scriptNode""):
            script = widget.scriptNode()
            if isinstance(script, Gaffer.ScriptNode):
                break
        widget = widget.parent()
    if script is not None:
        p = script.context().substitute(location)
        if not os.path.exists(p):
            try:
                os.makedirs(p)
            except OSError:
                pass
        return p
    else:
        return os.getcwd()
","if hasattr ( widget , ""scriptNode"" ) :",159
"def events_to_str(event_field, all_events):
    result = []
    for (flag, string) in all_events:
        c_flag = flag
        if event_field & c_flag:
            result.append(string)
            event_field = event_field & (~c_flag)
        if not event_field:
            break
    if event_field:
        result.append(hex(event_field))
    return ""|"".join(result)
",if not event_field :,123
"def get_s3_bucket_locations(buckets, self_log=False):
    """"""return (bucket_name, prefix) for all s3 logging targets""""""
    for b in buckets:
        if b.get(""Logging""):
            if self_log:
                if b[""Name""] != b[""Logging""][""TargetBucket""]:
                    continue
            yield (b[""Logging""][""TargetBucket""], b[""Logging""][""TargetPrefix""])
        if not self_log and b[""Name""].startswith(""cf-templates-""):
            yield (b[""Name""], """")
","if not self_log and b [ ""Name"" ] . startswith ( ""cf-templates-"" ) :",138
"def extract_file(tgz, tarinfo, dst_path, buffer_size=10 << 20, log_function=None):
    """"""Extracts 'tarinfo' from 'tgz' and writes to 'dst_path'.""""""
    src = tgz.extractfile(tarinfo)
    if src is None:
        return
    dst = tf.compat.v1.gfile.GFile(dst_path, ""wb"")
    while 1:
        buf = src.read(buffer_size)
        if not buf:
            break
        dst.write(buf)
        if log_function is not None:
            log_function(len(buf))
    dst.close()
    src.close()
",if log_function is not None :,165
"def make_index_fields(rec):
    fields = {}
    for k, v in rec.iteritems():
        if k in (""lccn"", ""oclc"", ""isbn""):
            fields[k] = v
            continue
        if k == ""full_title"":
            fields[""title""] = [read_short_title(v)]
    return fields
","if k == ""full_title"" :",93
"def disconnect_application(self):
    if not self.is_app_running(self.APP_BACKDROP):
        self.socket.send(commands.CloseCommand(destination_id=False))
        start_time = time.time()
        while not self.is_app_running(None):
            try:
                self.socket.send_and_wait(commands.StatusCommand())
            except cast_socket.ConnectionTerminatedException:
                break
            current_time = time.time()
            if current_time - start_time > self.timeout:
                raise TimeoutException()
            time.sleep(self.WAIT_INTERVAL)
    else:
        logger.debug(""Closing not necessary. Backdrop is running ..."")
",if current_time - start_time > self . timeout :,190
"def matches(self, cursor_offset, line, **kwargs):
    cs = lineparts.current_string(cursor_offset, line)
    if cs is None:
        return None
    matches = set()
    username = cs.word.split(os.path.sep, 1)[0]
    user_dir = os.path.expanduser(username)
    for filename in self.safe_glob(os.path.expanduser(cs.word)):
        if os.path.isdir(filename):
            filename += os.path.sep
        if cs.word.startswith(""~""):
            filename = username + filename[len(user_dir) :]
        matches.add(filename)
    return matches
","if cs . word . startswith ( ""~"" ) :",169
"def eventFilter(self, obj, event):
    if event.type() == QEvent.MouseButtonPress:
        button = event.button()
        if button == Qt.BackButton:
            self._app.browser.back()
            return True
        elif button == Qt.ForwardButton:
            self._app.browser.forward()
            return True
    return False
",if button == Qt . BackButton :,96
"def reset_parameters(self):
    for m in self.modules():
        if isinstance(m, nn.Embedding):
            continue
        elif isinstance(m, nn.LayerNorm):
            nn.init.constant_(m.weight, 0.1)
            nn.init.constant_(m.bias, 0)
        else:
            for p in m.parameters():
                nn.init.normal_(p, 0, 0.1)
","elif isinstance ( m , nn . LayerNorm ) :",115
"def get_scalding_core(self):
    lib_dir = os.path.join(self.scalding_home, ""lib"")
    for j in os.listdir(lib_dir):
        if j.startswith(""scalding-core-""):
            p = os.path.join(lib_dir, j)
            logger.debug(""Found scalding-core: %s"", p)
            return p
    raise luigi.contrib.hadoop.HadoopJobError(""Could not find scalding-core."")
","if j . startswith ( ""scalding-core-"" ) :",126
"def save(self):
    """"""Saves a new set of golden output frames to disk.""""""
    for pixels, (relative_to_assets, filename) in zip(
        self.iter_render(), self._iter_paths()
    ):
        full_directory_path = os.path.join(self._ASSETS_DIR, relative_to_assets)
        if not os.path.exists(full_directory_path):
            os.makedirs(full_directory_path)
        path = os.path.join(full_directory_path, filename)
        _save_pixels(pixels, path)
",if not os . path . exists ( full_directory_path ) :,147
"def _fix_var_naming(operators, names, mod=""input""):
    new_names = []
    map = {}
    for op in operators:
        if mod == ""input"":
            iter = op.inputs
        else:
            iter = op.outputs
        for i in iter:
            for name in names:
                if i.raw_name == name and name not in map:
                    map[i.raw_name] = i.full_name
        if len(map) == len(names):
            break
    for name in names:
        new_names.append(map[name])
    return new_names
","if mod == ""input"" :",168
"def Tokenize(s):
    # type: (str) -> Iterator[Token]
    for item in TOKEN_RE.findall(s):
        # The type checker can't know the true type of item!
        item = cast(TupleStr4, item)
        if item[0]:
            typ = ""number""
            val = item[0]
        elif item[1]:
            typ = ""name""
            val = item[1]
        elif item[2]:
            typ = item[2]
            val = item[2]
        elif item[3]:
            typ = item[3]
            val = item[3]
        yield Token(typ, val)
",elif item [ 1 ] :,181
"def init_errorhandler():
    # http error handling
    for ex in default_exceptions:
        if ex < 500:
            app.register_error_handler(ex, error_http)
        elif ex == 500:
            app.register_error_handler(ex, internal_error)
    if services.ldap:
        # Only way of catching the LDAPException upon logging in with LDAP server down
        @app.errorhandler(services.ldap.LDAPException)
        def handle_exception(e):
            log.debug(""LDAP server not accessible while trying to login to opds feed"")
            return error_http(FailedDependency())
",elif ex == 500 :,168
"def decode(self, ids):
    ids = pad_decr(ids)
    tokens = []
    for int_id in ids:
        if int_id < len(self._vocab_list):
            tokens.append(self._vocab_list[int_id])
        else:
            tokens.append(self._oov_token)
    return self._decode_token_separator.join(tokens)
",if int_id < len ( self . _vocab_list ) :,101
"def remove_contest(contest_id):
    with SessionGen() as session:
        contest = session.query(Contest).filter(Contest.id == contest_id).first()
        if not contest:
            print(""No contest with id %s found."" % contest_id)
            return False
        contest_name = contest.name
        if not ask(contest):
            print(""Not removing contest `%s'."" % contest_name)
            return False
        session.delete(contest)
        session.commit()
        print(""Contest `%s' removed."" % contest_name)
    return True
",if not ask ( contest ) :,169
"def get_hi_lineno(self):
    lineno = Node.get_hi_lineno(self)
    if self.expr1 is None:
        pass
    else:
        lineno = self.expr1.get_hi_lineno()
        if self.expr2 is None:
            pass
        else:
            lineno = self.expr2.get_hi_lineno()
            if self.expr3 is None:
                pass
            else:
                lineno = self.expr3.get_hi_lineno()
    return lineno
",if self . expr3 is None :,142
"def _send_internal(self, bytes_):
    # buffering
    if self.pendings:
        self.pendings += bytes_
        bytes_ = self.pendings
    try:
        # reconnect if possible
        self._reconnect()
        # send message
        self.socket.sendall(bytes_)
        # send finished
        self.pendings = None
    except Exception:  # pylint: disable=broad-except
        # close socket
        self._close()
        # clear buffer if it exceeds max bufer size
        if self.pendings and (len(self.pendings) > self.bufmax):
            # TODO: add callback handler here
            self.pendings = None
        else:
            self.pendings = bytes_
",if self . pendings and ( len ( self . pendings ) > self . bufmax ) :,194
"def _unpack(self, fmt, byt):
    d = unpack(self._header[""byteorder""] + fmt, byt)[0]
    if fmt[-1] in self.MISSING_VALUES:
        nmin, nmax = self.MISSING_VALUES[fmt[-1]]
        if d < nmin or d > nmax:
            if self._missing_values:
                return StataMissingValue(nmax, d)
            else:
                return None
    return d
",if self . _missing_values :,121
"def tuple_iter(self):
    for x in range(
        self.center.x - self.max_radius, self.center.x + self.max_radius + 1
    ):
        for y in range(
            self.center.y - self.max_radius, self.center.y + self.max_radius + 1
        ):
            if self.min_radius <= self.center.distance((x, y)) <= self.max_radius:
                yield (x, y)
","if self . min_radius <= self . center . distance ( ( x , y ) ) <= self . max_radius :",126
"def _parse_gene(element):
    for genename_element in element:
        if ""type"" in genename_element.attrib:
            ann_key = ""gene_%s_%s"" % (
                genename_element.tag.replace(NS, """"),
                genename_element.attrib[""type""],
            )
            if genename_element.attrib[""type""] == ""primary"":
                self.ParsedSeqRecord.annotations[ann_key] = genename_element.text
            else:
                append_to_annotations(ann_key, genename_element.text)
","if ""type"" in genename_element . attrib :",157
"def invalidateDependentSlices(self, iFirstCurve):
    # only user defined curve can have slice dependency relationships
    if self.isSystemCurveIndex(iFirstCurve):
        return
    nCurves = self.getNCurves()
    for i in range(iFirstCurve, nCurves):
        c = self.getSystemCurve(i)
        if isinstance(c.getSymbol().getSymbolType(), SymbolType.PieSliceSymbolType):
            c.invalidate()
        elif i == iFirstCurve:
            # if first curve isn't a slice,
            break
            # there are no dependent slices
",elif i == iFirstCurve :,154
"def gen_app_versions(self):
    for app_config in apps.get_app_configs():
        name = app_config.verbose_name
        app = app_config.module
        version = self.get_app_version(app)
        if version:
            yield app.__name__, name, version
",if version :,80
"def verify_relative_valid_path(root, path):
    if len(path) < 1:
        raise PackagerError(""Empty chown path"")
    checkpath = root
    parts = path.split(os.sep)
    for part in parts:
        if part in (""."", ""..""):
            raise PackagerError("". and .. is not allowed in chown path"")
        checkpath = os.path.join(checkpath, part)
        relpath = checkpath[len(root) + 1 :]
        if not os.path.exists(checkpath):
            raise PackagerError(f""chown path {relpath} does not exist"")
        if os.path.islink(checkpath):
            raise PackagerError(f""chown path {relpath} is a soft link"")
",if not os . path . exists ( checkpath ) :,191
"def create_or_update_tag_at_scope(cmd, resource_id=None, tags=None, tag_name=None):
    rcf = _resource_client_factory(cmd.cli_ctx)
    if resource_id is not None:
        if not tags:
            raise IncorrectUsageError(""Tags could not be empty."")
        Tags = cmd.get_models(""Tags"")
        tag_obj = Tags(tags=tags)
        return rcf.tags.create_or_update_at_scope(scope=resource_id, properties=tag_obj)
    return rcf.tags.create_or_update(tag_name=tag_name)
",if not tags :,160
"def generate_auto_complete(self, base, iterable_var):
    sugg = []
    for entry in iterable_var:
        compare_entry = entry
        compare_base = base
        if self.settings.get(IGNORE_CASE_SETTING):
            compare_entry = compare_entry.lower()
            compare_base = compare_base.lower()
        if self.compare_entries(compare_entry, compare_base):
            if entry not in sugg:
                sugg.append(entry)
    return sugg
","if self . compare_entries ( compare_entry , compare_base ) :",137
"def createFields(self):
    yield String(self, ""dict_start"", 2)
    while not self.eof:
        addr = self.absolute_address + self.current_size
        if self.stream.readBytes(addr, 2) != "">>"":
            for field in parsePDFType(self):
                yield field
        else:
            break
    yield String(self, ""dict_end"", 2)
","if self . stream . readBytes ( addr , 2 ) != "">>"" :",107
"def Visit_and_test(self, node):  # pylint: disable=invalid-name
    # and_test ::= not_test ('and' not_test)*
    for child in node.children:
        self.Visit(child)
        if isinstance(child, pytree.Leaf) and child.value == ""and"":
            _AppendTokenSubtype(child, format_token.Subtype.BINARY_OPERATOR)
","if isinstance ( child , pytree . Leaf ) and child . value == ""and"" :",99
"def getfiledata(directories):
    columns = None
    data = []
    counter = 1
    for directory in directories:
        for f in os.listdir(directory):
            if not os.path.isfile(os.path.join(directory, f)):
                continue
            counter += 1
            st = os.stat(os.path.join(directory, f))
            if columns is None:
                columns = [""rowid"", ""name"", ""directory""] + [
                    x for x in dir(st) if x.startswith(""st_"")
                ]
            data.append([counter, f, directory] + [getattr(st, x) for x in columns[3:]])
    return columns, data
",if columns is None :,185
"def copy_attributes(info_add, obj, name_fmt, attributes, formatter=None):
    for attr in attributes:
        value = getattr(obj, attr, None)
        if value is None:
            continue
        name = name_fmt % attr
        if formatter is not None:
            value = formatter(attr, value)
        info_add(name, value)
",if formatter is not None :,97
"def main(args):
    ap = argparse.ArgumentParser()
    ap.add_argument(""job_ids"", nargs=""+"", type=int, help=""ID of a running job"")
    ns = ap.parse_args(args)
    _stash = globals()[""_stash""]
    """""":type : StaSh""""""
    for job_id in ns.job_ids:
        if job_id in _stash.runtime.worker_registry:
            print(""killing job {} ..."".format(job_id))
            worker = _stash.runtime.worker_registry.get_worker(job_id)
            worker.kill()
            time.sleep(1)
        else:
            print(""error: no such job with id: {}"".format(job_id))
            break
",if job_id in _stash . runtime . worker_registry :,195
"def _check_choice(self):
    if self.type == ""choice"":
        if self.choices is None:
            raise OptionError(""must supply a list of choices for type 'choice'"", self)
        elif type(self.choices) not in (types.TupleType, types.ListType):
            raise OptionError(
                ""choices must be a list of strings ('%s' supplied)""
                % str(type(self.choices)).split(""'"")[1],
                self,
            )
    elif self.choices is not None:
        raise OptionError(""must not supply choices for type %r"" % self.type, self)
","elif type ( self . choices ) not in ( types . TupleType , types . ListType ) :",162
"def add_file(pipe, srcpath, tgtpath):
    with open(srcpath, ""rb"") as handle:
        if os.access(srcpath, os.X_OK):
            write(pipe, enc(""M 100755 inline %s\n"" % tgtpath))
        else:
            write(pipe, enc(""M 100644 inline %s\n"" % tgtpath))
        data = handle.read()
        write(pipe, enc(""data %d\n"" % len(data)))
        write(pipe, enc(data))
        write(pipe, enc(""\n""))
","if os . access ( srcpath , os . X_OK ) :",148
"def cdf(self, x):
    if x == numpy.inf:
        return 1.0
    else:  # Inefficient sum.
        if x != int(x):
            raise RuntimeError(""Invalid value."")
        c = 0.0
        for i in xrange(x + 1):
            c += self.probability(i)
        return c
",if x != int ( x ) :,91
"def convert_to_strings(self, out, seq_len):
    results = []
    for b, batch in enumerate(out):
        utterances = []
        for p, utt in enumerate(batch):
            size = seq_len[b][p]
            if size > 0:
                transcript = """".join(
                    map(lambda x: self.int_to_char[x.item()], utt[0:size])
                )
            else:
                transcript = """"
            utterances.append(transcript)
        results.append(utterances)
    return results
",if size > 0 :,158
"def get_date_range(self):
    if not hasattr(self, ""start"") or not hasattr(self, ""end""):
        args = (self.today.year, self.today.month)
        form = self.get_form()
        if form.is_valid():
            args = (int(form.cleaned_data[""year""]), int(form.cleaned_data[""month""]))
        self.start = self.get_start(*args)
        self.end = self.get_end(*args)
    return self.start, self.end
",if form . is_valid ( ) :,136
"def save_stats(self):
    LOGGER.info(""Saving task-level statistics."")
    has_headers = os.path.isfile(paths.TABLE_COUNT_PATH)
    with open(paths.TABLE_COUNT_PATH, ""a"") as csvfile:
        headers = [""start_time"", ""database_name"", ""number_tables""]
        writer = csv.DictWriter(
            csvfile, delimiter="","", lineterminator=""\n"", fieldnames=headers
        )
        if not has_headers:
            writer.writeheader()
        writer.writerow(
            {
                ""start_time"": self.start_time,
                ""database_name"": self.database_name,
                ""number_tables"": self.count,
            }
        )
",if not has_headers :,199
"def _CheckCanaryCommand(self):
    if OpenStackVirtualMachine.command_works:  # fast path
        return
    with self._lock:
        if OpenStackVirtualMachine.command_works:
            return
        logging.info(""Testing OpenStack CLI command is installed and working"")
        cmd = os_utils.OpenStackCLICommand(self, ""image"", ""list"")
        stdout, stderr, _ = cmd.Issue()
        if stderr:
            raise errors.Config.InvalidValue(
                ""OpenStack CLI test command failed. Please make sure the OpenStack ""
                ""CLI client is installed and properly configured""
            )
        OpenStackVirtualMachine.command_works = True
",if OpenStackVirtualMachine . command_works :,176
"def test_windows_hidden(self):
    if not sys.platform == ""win32"":
        self.skipTest(""sys.platform is not windows"")
        return
    # FILE_ATTRIBUTE_HIDDEN = 2 (0x2) from GetFileAttributes documentation.
    hidden_mask = 2
    with tempfile.NamedTemporaryFile() as f:
        # Hide the file using
        success = ctypes.windll.kernel32.SetFileAttributesW(f.name, hidden_mask)
        if not success:
            self.skipTest(""unable to set file attributes"")
        self.assertTrue(hidden.is_hidden(f.name))
",if not success :,147
"def recv_some(p, t=0.1, e=1, tr=5, stderr=0):
    if tr < 1:
        tr = 1
    x = time.time() + t
    y = []
    r = """"
    if stderr:
        pr = p.recv_err
    else:
        pr = p.recv
    while time.time() < x or r:
        r = pr()
        if r is None:
            break
        elif r:
            y.append(r)
        else:
            time.sleep(max((x - time.time()) / tr, 0))
    return b"""".join(y)
",elif r :,169
"def _is_xml(accepts):
    if accepts.startswith(b""application/""):
        has_xml = accepts.find(b""xml"")
        if has_xml > 0:
            semicolon = accepts.find(b"";"")
            if semicolon < 0 or has_xml < semicolon:
                return True
    return False
",if has_xml > 0 :,86
"def times(self, value: int):
    if value is None:
        self._times = None
    else:
        try:
            candidate = int(value)
        except ValueError:
            # pylint: disable:raise-missing-from
            raise BarException(f""cannot set repeat times to: {value!r}"")
        if candidate < 0:
            raise BarException(
                f""cannot set repeat times to a value less than zero: {value}""
            )
        if self.direction == ""start"":
            raise BarException(""cannot set repeat times on a start Repeat"")
        self._times = candidate
","if self . direction == ""start"" :",163
"def __call__(self, *args, **kwargs):
    if not NET_INITTED:
        return self.raw(*args, **kwargs)
    for stack in traceback.walk_stack(None):
        if ""self"" in stack[0].f_locals:
            layer = stack[0].f_locals[""self""]
            if layer in layer_names:
                log.pytorch_layer_name = layer_names[layer]
                print(layer_names[layer])
                break
    out = self.obj(self.raw, *args, **kwargs)
    # if isinstance(out,Variable):
    #     out=[out]
    return out
",if layer in layer_names :,173
"def do_begin(self, byte):
    if byte.isspace():
        return
    if byte != ""<"":
        if self.beExtremelyLenient:
            self._leadingBodyData = byte
            return ""bodydata""
        self._parseError(""First char of document [{!r}] wasn't <"".format(byte))
    return ""tagstart""
",if self . beExtremelyLenient :,91
"def pretty(self, n, comment=True):
    if isinstance(n, (str, bytes, list, tuple, dict)):
        r = repr(n)
        if not comment:  # then it can be inside a comment!
            r = r.replace(""*/"", r""\x2a/"")
        return r
    if not isinstance(n, six.integer_types):
        return n
    if isinstance(n, constants.Constant):
        if comment:
            return ""%s /* %s */"" % (n, self.pretty(int(n)))
        else:
            return ""%s (%s)"" % (n, self.pretty(int(n)))
    elif abs(n) < 10:
        return str(n)
    else:
        return hex(n)
",if not comment :,194
"def test_training_script_with_max_history_set(tmpdir):
    train_dialogue_model(
        DEFAULT_DOMAIN_PATH,
        DEFAULT_STORIES_FILE,
        tmpdir.strpath,
        interpreter=RegexInterpreter(),
        policy_config=""data/test_config/max_hist_config.yml"",
        kwargs={},
    )
    agent = Agent.load(tmpdir.strpath)
    for policy in agent.policy_ensemble.policies:
        if hasattr(policy.featurizer, ""max_history""):
            if type(policy) == FormPolicy:
                assert policy.featurizer.max_history == 2
            else:
                assert policy.featurizer.max_history == 5
",if type ( policy ) == FormPolicy :,191
"def cli_uninstall_distro():
    distro_list = install_distro_list()
    if distro_list is not None:
        for index, _distro_dir in enumerate(distro_list):
            log(str(index) + ""  --->>  "" + _distro_dir)
        user_input = read_input_uninstall()
        if user_input is not False:
            for index, _distro_dir in enumerate(distro_list):
                if index == user_input:
                    config.uninstall_distro_dir_name = _distro_dir
                    unin_distro()
    else:
        log(""No distro installed on "" + config.usb_disk)
",if user_input is not False :,189
"def set_random_avatar(user):
    galleries = get_available_galleries(include_default=True)
    if not galleries:
        raise RuntimeError(""no avatar galleries are set"")
    avatars_list = []
    for gallery in galleries:
        if gallery[""name""] == DEFAULT_GALLERY:
            avatars_list = gallery[""images""]
            break
        else:
            avatars_list += gallery[""images""]
    random_avatar = random.choice(avatars_list)
    store.store_new_avatar(user, Image.open(random_avatar.image))
","if gallery [ ""name"" ] == DEFAULT_GALLERY :",169
"def make_query(self, key, filters):
    meta = self.get_meta(key)
    q = {meta.facet_key: self.normalize_key(meta.path)}
    if filters:
        if filters.get(""has_fulltext"") == ""true"":
            q[""has_fulltext""] = ""true""
        if filters.get(""publish_year""):
            q[""publish_year""] = filters[""publish_year""]
    return q
","if filters . get ( ""publish_year"" ) :",113
"def test_named_parameters_and_constraints(self):
    likelihood = gpytorch.likelihoods.GaussianLikelihood()
    model = ExactGPModel(None, None, likelihood)
    for name, _param, constraint in model.named_parameters_and_constraints():
        if name == ""likelihood.noise_covar.raw_noise"":
            self.assertIsInstance(constraint, gpytorch.constraints.GreaterThan)
        elif name == ""mean_module.constant"":
            self.assertIsNone(constraint)
        elif name == ""covar_module.raw_outputscale"":
            self.assertIsInstance(constraint, gpytorch.constraints.Positive)
        elif name == ""covar_module.base_kernel.raw_lengthscale"":
            self.assertIsInstance(constraint, gpytorch.constraints.Positive)
","elif name == ""mean_module.constant"" :",192
"def _test_pooling(input_shape, **kwargs):
    _test_pooling_iteration(input_shape, **kwargs)
    if is_gpu_available():
        if len(input_shape) == 4:
            input_shape = [input_shape[ii] for ii in (0, 3, 1, 2)]
            kwargs[""data_format""] = ""NCHW""
            _test_pooling_iteration(input_shape, **kwargs)
",if len ( input_shape ) == 4 :,111
"def init(self):
    r = self.get_redis()
    if r:
        key = ""pocsuite_target""
        info_msg = ""[PLUGIN] try fetch targets from redis...""
        logger.info(info_msg)
        targets = r.get(key)
        count = 0
        if targets:
            for target in targets:
                if self.add_target(target):
                    count += 1
        info_msg = ""[PLUGIN] get {0} target(s) from redis"".format(count)
        logger.info(info_msg)
",if targets :,151
"def reload_json_api_settings(*args, **kwargs):
    django_setting = kwargs[""setting""]
    setting = django_setting.replace(JSON_API_SETTINGS_PREFIX, """")
    value = kwargs[""value""]
    if setting in DEFAULTS.keys():
        if value is not None:
            setattr(json_api_settings, setting, value)
        elif hasattr(json_api_settings, setting):
            delattr(json_api_settings, setting)
","elif hasattr ( json_api_settings , setting ) :",115
"def update_metadata(self):
    for attrname in dir(self):
        if attrname.startswith(""__""):
            continue
        attrvalue = getattr(self, attrname, None)
        if attrvalue == 0:
            continue
        if attrname == ""salt_version"":
            attrname = ""version""
        if hasattr(self.metadata, ""set_{0}"".format(attrname)):
            getattr(self.metadata, ""set_{0}"".format(attrname))(attrvalue)
        elif hasattr(self.metadata, attrname):
            try:
                setattr(self.metadata, attrname, attrvalue)
            except AttributeError:
                pass
","elif hasattr ( self . metadata , attrname ) :",173
"def test_02_looking_at_listdir_path_(name):
    for dline in listdir.json():
        if dline[""path""] == f""{path}/{name}"":
            assert dline[""type""] in (""DIRECTORY"", ""FILE""), listdir.text
            assert dline[""uid""] == 0, listdir.text
            assert dline[""gid""] == 0, listdir.text
            assert dline[""name""] == name, listdir.text
            break
    else:
        raise AssertionError(f""/{path}/{name} not found"")
","if dline [ ""path"" ] == f""{path}/{name}"" :",136
"def DeletePlugin():
    oid = request.form.get(""oid"", """")
    if oid:
        result = Mongo.coll[""Plugin""].find_one_and_delete(
            {""_id"": ObjectId(oid)}, remove=True
        )
        if not result[""filename""].find(""."") > -1:
            result[""filename""] = result[""filename""] + "".py""
        if os.path.exists(file_path + result[""filename""]):
            os.remove(file_path + result[""filename""])
            return ""success""
    return ""fail""
","if not result [ ""filename"" ] . find ( ""."" ) > - 1 :",144
"def iterparent(self, node):
    """"""Iterator wrapper to get allowed parent and child all at once.""""""
    # We do not allow the marker inside a header as that
    # would causes an enless loop of placing a new TOC
    # inside previously generated TOC.
    for child in node:
        if not self.header_rgx.match(child.tag) and child.tag not in [""pre"", ""code""]:
            yield node, child
            yield from self.iterparent(child)
","if not self . header_rgx . match ( child . tag ) and child . tag not in [ ""pre"" , ""code"" ] :",120
"def _get_matched_layout(command):
    # don't use command.split_script here because a layout mismatch will likely
    # result in a non-splitable script as per shlex
    cmd = command.script.split("" "")
    for source_layout in source_layouts:
        is_all_match = True
        for cmd_part in cmd:
            if not all([ch in source_layout or ch in ""-_"" for ch in cmd_part]):
                is_all_match = False
                break
        if is_all_match:
            return source_layout
",if is_all_match :,147
"def _update_tileable_and_chunk_shape(self, tileable_graph, chunk_result, failed_ops):
    for n in tileable_graph:
        if n.op in failed_ops:
            continue
        tiled_n = get_tiled(n)
        if has_unknown_shape(tiled_n):
            if any(c.key not in chunk_result for c in tiled_n.chunks):
                # some of the chunks has been fused
                continue
            new_nsplits = self.get_tileable_nsplits(n, chunk_result=chunk_result)
            for node in (n, tiled_n):
                node._update_shape(tuple(sum(nsplit) for nsplit in new_nsplits))
            tiled_n._nsplits = new_nsplits
",if any ( c . key not in chunk_result for c in tiled_n . chunks ) :,200
"def _get_items(self, name, target=1):
    all_items = self.get_items(name)
    items = [o for o in all_items if not o.disabled]
    if len(items) < target:
        if len(all_items) < target:
            raise ItemNotFoundError(""insufficient items with name %r"" % name)
        else:
            raise AttributeError(""insufficient non-disabled items with name %s"" % name)
    on = []
    off = []
    for o in items:
        if o.selected:
            on.append(o)
        else:
            off.append(o)
    return on, off
",if o . selected :,169
"def parse_flow_sequence_entry_mapping_value(self):
    if self.check_token(ValueToken):
        token = self.get_token()
        if not self.check_token(FlowEntryToken, FlowSequenceEndToken):
            self.states.append(self.parse_flow_sequence_entry_mapping_end)
            return self.parse_flow_node()
        else:
            self.state = self.parse_flow_sequence_entry_mapping_end
            return self.process_empty_scalar(token.end_mark)
    else:
        self.state = self.parse_flow_sequence_entry_mapping_end
        token = self.peek_token()
        return self.process_empty_scalar(token.start_mark)
","if not self . check_token ( FlowEntryToken , FlowSequenceEndToken ) :",194
"def serialize_config(self, session, key, tid, language):
    cache_key = gen_cache_key(key, tid, language)
    cache_obj = None
    if cache_key not in self.cache:
        if key == ""node"":
            cache_obj = db_admin_serialize_node(session, tid, language)
        elif key == ""notification"":
            cache_obj = db_get_notification(session, tid, language)
        self.cache[cache_key] = cache_obj
    return self.cache[cache_key]
","if key == ""node"" :",139
"def get_lldp_neighbors(self):
    commands = [""show lldp neighbors""]
    output = self.device.run_commands(commands)[0][""lldpNeighbors""]
    lldp = {}
    for n in output:
        if n[""port""] not in lldp.keys():
            lldp[n[""port""]] = []
        lldp[n[""port""]].append(
            {""hostname"": n[""neighborDevice""], ""port"": n[""neighborPort""]}
        )
    return lldp
","if n [ ""port"" ] not in lldp . keys ( ) :",126
"def handle(self):
    from poetry.utils.env import EnvManager
    manager = EnvManager(self.poetry)
    current_env = manager.get()
    for venv in manager.list():
        name = venv.path.name
        if self.option(""full-path""):
            name = str(venv.path)
        if venv == current_env:
            self.line(""<info>{} (Activated)</info>"".format(name))
            continue
        self.line(name)
",if venv == current_env :,129
"def resolve_env_secrets(config, environ):
    """"""Create copy that recursively replaces {""$env"": ""NAME""} with values from environ""""""
    if isinstance(config, dict):
        if list(config.keys()) == [""$env""]:
            return environ.get(list(config.values())[0])
        elif list(config.keys()) == [""$file""]:
            return open(list(config.values())[0]).read()
        else:
            return {
                key: resolve_env_secrets(value, environ)
                for key, value in config.items()
            }
    elif isinstance(config, list):
        return [resolve_env_secrets(value, environ) for value in config]
    else:
        return config
","elif list ( config . keys ( ) ) == [ ""$file"" ] :",190
"def _is_valid_16bit_as_path(cls, buf):
    two_byte_as_size = struct.calcsize(""!H"")
    while buf:
        (type_, num_as) = struct.unpack_from(
            cls._SEG_HDR_PACK_STR, six.binary_type(buf)
        )
        if type_ is not cls._AS_SET and type_ is not cls._AS_SEQUENCE:
            return False
        buf = buf[struct.calcsize(cls._SEG_HDR_PACK_STR) :]
        if len(buf) < num_as * two_byte_as_size:
            return False
        buf = buf[num_as * two_byte_as_size :]
    return True
",if len ( buf ) < num_as * two_byte_as_size :,190
"def reparentChildren(self, newParent):
    if newParent.childNodes:
        newParent.childNodes[-1]._element.tail += self._element.text
    else:
        if not newParent._element.text:
            newParent._element.text = """"
        if self._element.text is not None:
            newParent._element.text += self._element.text
    self._element.text = """"
    base.Node.reparentChildren(self, newParent)
",if not newParent . _element . text :,121
"def get_operation_ast(document_ast, operation_name=None):
    operation = None
    for definition in document_ast.definitions:
        if isinstance(definition, ast.OperationDefinition):
            if not operation_name:
                # If no operation name is provided, only return an Operation if it is the only one present in the
                # document. This means that if we've encountered a second operation as we were iterating over the
                # definitions in the document, there are more than one Operation defined, and we should return None.
                if operation:
                    return None
                operation = definition
            elif definition.name and definition.name.value == operation_name:
                return definition
    return operation
",elif definition . name and definition . name . value == operation_name :,186
"def reprSmart(vw, item):
    ptype = type(item)
    if ptype is int:
        if -1024 < item < 1024:
            return str(item)
        elif vw.isValidPointer(item):
            return vw.reprPointer(item)
        else:
            return hex(item)
    elif ptype in (list, tuple):
        return reprComplex(vw, item)  # recurse
    elif ptype is dict:
        return ""{%s}"" % "","".join(
            [""%s:%s"" % (reprSmart(vw, k), reprSmart(vw, v)) for k, v in item.items()]
        )
    else:
        return repr(item)
",if - 1024 < item < 1024 :,183
"def cleanDataCmd(cmd):
    newcmd = ""AbracadabrA ** <?php ""
    if cmd[:6] != ""php://"":
        if reverseConn not in cmd:
            cmds = cmd.split(""&"")
            for c in cmds:
                if len(c) > 0:
                    newcmd += ""system('%s');"" % c
        else:
            b64cmd = base64.b64encode(cmd)
            newcmd += ""system(base64_decode('%s'));"" % b64cmd
    else:
        newcmd += cmd[6:]
    newcmd += ""?> **""
    return newcmd
",if reverseConn not in cmd :,170
"def render_tasks(self) -> List:
    results = []
    for task in self.tasks.values():
        job_entry = self.jobs.get(task.job_id)
        if job_entry:
            if not self.should_render_job(job_entry):
                continue
        files = self.get_file_counts([task])
        entry = (
            task.job_id,
            task.task_id,
            task.state,
            task.type.name,
            task.target,
            files,
            task.pool,
            task.end_time,
        )
        results.append(entry)
    return results
",if job_entry :,186
"def __call__(self, environ, start_response):
    for key in ""REQUEST_URL"", ""REQUEST_URI"", ""UNENCODED_URL"":
        if key not in environ:
            continue
        request_uri = unquote(environ[key])
        script_name = unquote(environ.get(""SCRIPT_NAME"", """"))
        if request_uri.startswith(script_name):
            environ[""PATH_INFO""] = request_uri[len(script_name) :].split(""?"", 1)[0]
            break
    return self.app(environ, start_response)
",if request_uri . startswith ( script_name ) :,140
"def _add_role_information(self, function_dict, role_id):
    # Make it easier to build rules based on policies attached to execution roles
    function_dict[""role_arn""] = role_id
    role_name = role_id.split(""/"")[-1]
    function_dict[
        ""execution_role""
    ] = await self.facade.awslambda.get_role_with_managed_policies(role_name)
    if function_dict.get(""execution_role""):
        statements = []
        for policy in function_dict[""execution_role""].get(""policies""):
            if ""Document"" in policy and ""Statement"" in policy[""Document""]:
                statements += policy[""Document""][""Statement""]
        function_dict[""execution_role""][""policy_statements""] = statements
","if ""Document"" in policy and ""Statement"" in policy [ ""Document"" ] :",196
"def TryMerge(self, d):
    while d.avail() > 0:
        tt = d.getVarInt32()
        if tt == 8:
            self.set_ts(d.getVarInt64())
            continue
        if tt == 0:
            raise ProtocolBuffer.ProtocolBufferDecodeError
        d.skipData(tt)
",if tt == 0 :,90
"def format_counts(results, json_output=False, human_readable=False):
    if json_output:
        for result in results:
            yield json.dumps(result)
    else:
        for result in results:
            space_consumed = result.get(""spaceConsumed"")
            if human_readable:
                space_consumed = _sizeof_fmt(int(result.get(""spaceConsumed"")))
            yield ""%12s %12s %18s %s"" % (
                result.get(""directoryCount""),
                result.get(""fileCount""),
                space_consumed,
                result.get(""path""),
            )
",if human_readable :,175
"def parse_edges(self, pcb):
    edges = []
    drawings = list(pcb.GetDrawings())
    bbox = None
    for m in pcb.GetModules():
        for g in m.GraphicalItems():
            drawings.append(g)
    for d in drawings:
        if d.GetLayer() == pcbnew.Edge_Cuts:
            parsed_drawing = self.parse_drawing(d)
            if parsed_drawing:
                edges.append(parsed_drawing)
                if bbox is None:
                    bbox = d.GetBoundingBox()
                else:
                    bbox.Merge(d.GetBoundingBox())
    if bbox:
        bbox.Normalize()
    return edges, bbox
",if bbox is None :,197
"def __getitem__(self, k) -> ""SimMemView"":
    if isinstance(k, slice):
        if k.step is not None:
            raise ValueError(""Slices with strides are not supported"")
        elif k.start is None:
            raise ValueError(""Must specify start index"")
        elif k.stop is not None:
            raise ValueError(""Slices with stop index are not supported"")
        else:
            addr = k.start
    elif self._type is not None and self._type._can_refine_int:
        return self._type._refine(self, k)
    else:
        addr = k
    return self._deeper(addr=addr)
",elif k . start is None :,169
"def _parse(self, stream, context):
    obj = []
    try:
        if self.subcon.conflags & self.FLAG_COPY_CONTEXT:
            while True:
                subobj = self.subcon._parse(stream, context.__copy__())
                obj.append(subobj)
                if self.predicate(subobj, context):
                    break
        else:
            while True:
                subobj = self.subcon._parse(stream, context)
                obj.append(subobj)
                if self.predicate(subobj, context):
                    break
    except ConstructError as ex:
        raise ArrayError(""missing terminator"", ex)
    return obj
","if self . predicate ( subobj , context ) :",191
"def before_run(self, run_context):
    if ""featurizer"" in self.model_portion and (
        self.need_to_refresh or self.refresh_base_model
    ):
        if self.model_portion == ""whole_featurizer"":
            self.refresh_base_model = True
        self.init_fn(
            None, run_context.session, self.model_portion, self.refresh_base_model
        )
        self.need_to_refresh = False
        self.refresh_base_model = False
","if self . model_portion == ""whole_featurizer"" :",141
"def run(self):
    while True:
        task = self.requestQueue.get()
        if task is None:
            # The ""None"" value is used as a sentinel by
            # ThreadPool.cleanup().  This indicates that there
            # are no more tasks, so we should quit.
            break
        try:
            if self.interrupted():
                raise SCons.Errors.BuildError(task.targets[0], errstr=interrupt_msg)
            task.execute()
        except:
            task.exception_set()
            ok = False
        else:
            ok = True
        self.resultsQueue.put((task, ok))
",if self . interrupted ( ) :,178
"def get_overdue_evergreen_documents(*, db_session) -> List[Optional[Document]]:
    """"""Returns all documents that have need had a recent evergreen notification.""""""
    documents = (
        db_session.query(Document).filter(Document.evergreen == True)
    ).all()  # noqa
    overdue_documents = []
    now = datetime.utcnow()
    for d in documents:
        next_reminder = d.evergreen_last_reminder_at + timedelta(
            days=d.evergreen_reminder_interval
        )
        if now > next_reminder:
            overdue_documents.append(d)
    return overdue_documents
",if now > next_reminder :,163
"def create_local_app_folder(local_app_path):
    if exists(local_app_path):
        raise ValueError(""There is already a '%s' folder! Aborting!"" % local_app_path)
    for folder in subfolders(local_app_path):
        if not exists(folder):
            os.mkdir(folder)
            init_path = join(folder, ""__init__.py"")
            if not exists(init_path):
                create_file(init_path)
",if not exists ( init_path ) :,126
"def generate():
    for leaf in u.leaves:
        if isinstance(leaf, Integer):
            val = leaf.get_int_value()
            if val in (0, 1):
                yield val
            else:
                raise _NoBoolVector
        elif isinstance(leaf, Symbol):
            if leaf == SymbolTrue:
                yield 1
            elif leaf == SymbolFalse:
                yield 0
            else:
                raise _NoBoolVector
        else:
            raise _NoBoolVector
","if isinstance ( leaf , Integer ) :",138
"def replace(self, old, new):
    v_m = self.var_map
    size = v_m[self.size]
    if not (size.is_const() or size.is_ident()):
        size.replace(old, new)
    else:
        if new.is_ident():
            v_m[new.value()] = new
            self.size = new.value()
        else:
            v_m[old] = new
",if new . is_ident ( ) :,119
"def method_for_doctype(doctype):
    method = ""xhtml""
    if doctype:
        if doctype.startswith(""html""):
            method = ""html""
        elif doctype.startswith(""xhtml""):
            method = ""xhtml""
        elif doctype.startswith(""svg""):
            method = ""xml""
        else:
            method = ""xhtml""
    return method
","elif doctype . startswith ( ""xhtml"" ) :",101
"def delete(self, trans, **kwd):
    idnum = kwd[self.tagged_item_id]
    item = self._get_item_from_id(trans, idnum, check_writable=True)
    if item is not None:
        ex_obj = self.get_item_extended_metadata_obj(trans, item)
        if ex_obj is not None:
            self.unset_item_extended_metadata_obj(trans, item)
            self.delete_extended_metadata(trans, ex_obj)
",if ex_obj is not None :,131
"def check_testv(self, testv):
    test_good = True
    f = open(self.home, ""rb+"")
    for (offset, length, operator, specimen) in testv:
        data = self._read_share_data(f, offset, length)
        if not testv_compare(data, operator, specimen):
            test_good = False
            break
    f.close()
    return test_good
","if not testv_compare ( data , operator , specimen ) :",113
"def get_history_user(self, instance):
    """"""Get the modifying user from instance or middleware.""""""
    try:
        return instance._history_user
    except AttributeError:
        request = None
        try:
            if self.thread.request.user.is_authenticated:
                request = self.thread.request
        except AttributeError:
            pass
    return self.get_user(instance=instance, request=request)
",if self . thread . request . user . is_authenticated :,110
"def _check(self, name, size=None, *extra):
    func = getattr(imageop, name)
    for height in VALUES:
        for width in VALUES:
            strlen = abs(width * height)
            if size:
                strlen *= size
            if strlen < MAX_LEN:
                data = ""A"" * strlen
            else:
                data = AAAAA
            if size:
                arguments = (data, size, width, height) + extra
            else:
                arguments = (data, width, height) + extra
            try:
                func(*arguments)
            except (ValueError, imageop.error):
                pass
",if size :,188
"def __setattr__(self, name, value):
    if name == ""path"":
        if value and value != """":
            if value[0] != ""/"":
                raise ValueError(
                    'The page path should always start with a slash (""/"").'
                )
    elif name == ""load_time"":
        if value and not isinstance(value, int):
            raise ValueError(
                ""Page load time must be specified in integer milliseconds.""
            )
    object.__setattr__(self, name, value)
","if value and not isinstance ( value , int ) :",136
"def __repr__(self):
    if self._in_repr:
        return ""<recursion>""
    try:
        self._in_repr = True
        if self.is_computed():
            status = ""computed, ""
            if self.error() is None:
                if self.value() is self:
                    status += ""= self""
                else:
                    status += ""= "" + repr(self.value())
            else:
                status += ""error = "" + repr(self.error())
        else:
            status = ""isn't computed""
        return ""%s (%s)"" % (type(self), status)
    finally:
        self._in_repr = False
",if self . error ( ) is None :,189
"def _exclude_node(self, name):
    if ""exclude_nodes"" in self.node_filters:
        if name in self.node_filters[""exclude_nodes""]:
            self.loggit.info('Excluding node ""{0}"" due to node_filters'.format(name))
            return True
    return False
","if name in self . node_filters [ ""exclude_nodes"" ] :",79
"def enumerate_projects():
    """"""List projects in _DEFAULT_APP_DIR.""""""
    src_path = os.path.join(_DEFAULT_APP_DIR, ""src"")
    projects = {}
    for project in os.listdir(src_path):
        projects[project] = []
        project_path = os.path.join(src_path, project)
        for file in os.listdir(project_path):
            if file.endswith("".gwt.xml""):
                projects[project].append(file[:-8])
    return projects
","if file . endswith ( "".gwt.xml"" ) :",133
"def zip_readline_read_test(self, f, compression):
    self.make_test_archive(f, compression)
    # Read the ZIP archive
    with zipfile.ZipFile(f, ""r"") as zipfp, zipfp.open(TESTFN) as zipopen:
        data = b""""
        while True:
            read = zipopen.readline()
            if not read:
                break
            data += read
            read = zipopen.read(100)
            if not read:
                break
            data += read
    self.assertEqual(data, self.data)
",if not read :,155
"def f(view, s):
    if mode == modes.NORMAL:
        return sublime.Region(0)
    elif mode == modes.VISUAL:
        if s.a < s.b:
            return sublime.Region(s.a + 1, 0)
        else:
            return sublime.Region(s.a, 0)
    elif mode == modes.INTERNAL_NORMAL:
        return sublime.Region(view.full_line(s.b).b, 0)
    elif mode == modes.VISUAL_LINE:
        if s.a < s.b:
            return sublime.Region(0, s.b)
        else:
            return sublime.Region(0, s.a)
    return s
",if s . a < s . b :,192
"def response(self):
    try:
        response = requests.get(str(self))
        rjson = response.json()
        if not isinstance(rjson, dict):
            raise Exception(response.text)
        return rjson
    except Exception as e:
        raise ResponseFanartError(str(e))
","if not isinstance ( rjson , dict ) :",82
"def __get_type(self, cexpr):
    """"""Returns one of the following types: 'R' - read value, 'W' - write value, 'A' - function argument""""""
    child = cexpr
    for p in reversed(self.parents):
        assert p, ""Failed to get type at "" + helper.to_hex(self.__function_address)
        if p.cexpr.op == idaapi.cot_call:
            return ""Arg""
        if not p.is_expr():
            return ""R""
        if p.cexpr.op == idaapi.cot_asg:
            if p.cexpr.x == child:
                return ""W""
            return ""R""
        child = p.cexpr
",if p . cexpr . x == child :,192
"def _extract_lemma(self, parse: Parse) -> str:
    special_feats = [x for x in self.SPECIAL_FEATURES if x in parse.tag]
    if len(special_feats) == 0:
        return parse.normal_form
    # here we process surnames and patronyms since PyMorphy lemmatizes them incorrectly
    for other in parse.lexeme:
        tag = other.tag
        if any(x not in tag for x in special_feats):
            continue
        if (
            tag.case == ""nomn""
            and tag.gender == parse.tag.gender
            and tag.number == ""sing""
        ):
            return other.word
    return parse.normal_form
",if any ( x not in tag for x in special_feats ) :,185
"def evaluateWord(self, argument):
    wildcard_count = argument[0].count(""*"")
    if wildcard_count > 0:
        if wildcard_count == 1 and argument[0].startswith(""*""):
            return self.GetWordWildcard(argument[0][1:], method=""endswith"")
        if wildcard_count == 1 and argument[0].endswith(""*""):
            return self.GetWordWildcard(argument[0][:-1], method=""startswith"")
        else:
            _regex = argument[0].replace(""*"", "".+"")
            matched = False
            for w in self.words:
                matched = bool(re.search(_regex, w))
                if matched:
                    break
            return matched
    return self.GetWord(argument[0])
","if wildcard_count == 1 and argument [ 0 ] . endswith ( ""*"" ) :",194
"def getAllEntries(self):
    entries = []
    for bucket in self.buckets:
        last = None
        for entry in bucket.entries:
            if last is not None:
                last.size = entry.virtualOffset - last.virtualOffset
            last = entry
            entries.append(entry)
        if len(entries) != 0:
            entries[-1].size = bucket.endOffset - entries[-1].virtualOffset
    return entries
",if len ( entries ) != 0 :,119
"def clean(self):
    if self._ctx:
        if hasattr(libcrypto, ""EVP_CIPHER_CTX_cleanup""):
            libcrypto.EVP_CIPHER_CTX_cleanup(self._ctx)
        else:
            libcrypto.EVP_CIPHER_CTX_reset(self._ctx)
        libcrypto.EVP_CIPHER_CTX_free(self._ctx)
","if hasattr ( libcrypto , ""EVP_CIPHER_CTX_cleanup"" ) :",96
"def _addTab(self, name, label, idx=None):
    label = getLanguageString(label)
    tab = Tab(self, name, label)
    tab.idx = self._makeTab(tab, idx)
    if idx != None:
        # Update index list when inserting tabs at arbitrary positions
        newIdxList = {}
        for tIdx, t in list(self._tabs_by_idx.items()):
            if int(tIdx) >= idx:
                t.idx += 1
            newIdxList[t.idx] = t
        self._tabs_by_idx = newIdxList
    self._tabs_by_idx[tab.idx] = tab
    self._tabs_by_name[tab.name] = tab
    return tab
",if int ( tIdx ) >= idx :,189
"def set(self, _key, _new_login=True):
    with self.lock:
        user = self.users.get(current_user.id, None)
        if user is None:
            self.users[current_user.id] = dict(session_count=1, key=_key)
        else:
            if _new_login:
                user[""session_count""] += 1
            user[""key""] = _key
",if user is None :,116
"def stop(self):
    # Try to shut the connection down, but if we get any sort of
    # errors, go ahead and ignore them.. as we're shutting down anyway
    try:
        self.rpcserver.stop()
        if self.backend_rpcserver:
            self.backend_rpcserver.stop()
        if self.cluster_rpcserver:
            self.cluster_rpcserver.stop()
    except Exception:
        pass
    if self.coordination:
        try:
            coordination.COORDINATOR.stop()
        except Exception:
            pass
    super(Service, self).stop(graceful=True)
",if self . backend_rpcserver :,171
"def __genmenuOnlyAllocated(menu):
    for submenu in menu.Submenus:
        __genmenuOnlyAllocated(submenu)
    if menu.OnlyUnallocated == True:
        tmp[""cache""].addMenuEntries(menu.AppDirs)
        menuentries = []
        for rule in menu.Rules:
            menuentries = rule.do(
                tmp[""cache""].getMenuEntries(menu.AppDirs), rule.Type, 2
            )
        for menuentry in menuentries:
            if menuentry.Add == True:
                menuentry.Parents.append(menu)
                #   menuentry.Add = False
                #   menuentry.Allocated = True
                menu.MenuEntries.append(menuentry)
",if menuentry . Add == True :,198
"def __init__(self, **options):
    self.func_name_highlighting = get_bool_opt(options, ""func_name_highlighting"", True)
    self.disabled_modules = get_list_opt(options, ""disabled_modules"", [])
    self._functions = set()
    if self.func_name_highlighting:
        from pygments.lexers._lua_builtins import MODULES
        for mod, func in iteritems(MODULES):
            if mod not in self.disabled_modules:
                self._functions.update(func)
    RegexLexer.__init__(self, **options)
",if mod not in self . disabled_modules :,150
"def recv_some(p, t=0.1, e=1, tr=5, stderr=0):
    if tr < 1:
        tr = 1
    x = time.time() + t
    y = []
    r = """"
    if stderr:
        pr = p.recv_err
    else:
        pr = p.recv
    while time.time() < x or r:
        r = pr()
        if r is None:
            break
        elif r:
            y.append(r)
        else:
            time.sleep(max((x - time.time()) / tr, 0))
    return """".join(y)
",if r is None :,168
"def get_menu_items(node):
    aList = []
    for child in node.children:
        for tag in (""@menu"", ""@item""):
            if child.h.startswith(tag):
                name = child.h[len(tag) + 1 :].strip()
                if tag == ""@menu"":
                    aList.append((""%s %s"" % (tag, name), get_menu_items(child), None))
                else:
                    b = g.splitLines("""".join(child.b))
                    aList.append((tag, name, b[0] if b else """"))
                break
    return aList
","if tag == ""@menu"" :",173
"def import_suffix_generator(a_block, datatype=False):
    if datatype is False:
        for name, suffix in iteritems(a_block.component_map(Suffix)):
            if suffix.import_enabled() is True:
                yield name, suffix
    else:
        for name, suffix in iteritems(a_block.component_map(Suffix)):
            if (suffix.import_enabled() is True) and (
                suffix.get_datatype() is datatype
            ):
                yield name, suffix
",if suffix . import_enabled ( ) is True :,136
"def verify_relative_valid_path(root, path):
    if len(path) < 1:
        raise PackagerError(""Empty chown path"")
    checkpath = root
    parts = path.split(os.sep)
    for part in parts:
        if part in (""."", ""..""):
            raise PackagerError("". and .. is not allowed in chown path"")
        checkpath = os.path.join(checkpath, part)
        relpath = checkpath[len(root) + 1 :]
        if not os.path.exists(checkpath):
            raise PackagerError(f""chown path {relpath} does not exist"")
        if os.path.islink(checkpath):
            raise PackagerError(f""chown path {relpath} is a soft link"")
",if os . path . islink ( checkpath ) :,191
"def load_syntax(syntax):
    context = _create_scheme() or {}
    partition_scanner = PartitionScanner(syntax.get(""partitions"", []))
    scanners = {}
    for part_name, part_scanner in list(syntax.get(""scanner"", {}).items()):
        scanners[part_name] = Scanner(part_scanner)
    formats = []
    for fname, fstyle in list(syntax.get(""formats"", {}).items()):
        if isinstance(fstyle, basestring):
            if fstyle.startswith(""%("") and fstyle.endswith("")s""):
                key = fstyle[2:-2]
                fstyle = context[key]
            else:
                fstyle = fstyle % context
        formats.append((fname, fstyle))
    return partition_scanner, scanners, formats
","if isinstance ( fstyle , basestring ) :",199
"def should_keep_alive(commit_msg):
    result = False
    ci = get_current_ci() or """"
    for line in commit_msg.splitlines():
        parts = line.strip(""# "").split("":"", 1)
        (key, val) = parts if len(parts) > 1 else (parts[0], """")
        if key == ""CI_KEEP_ALIVE"":
            ci_names = val.replace("","", "" "").lower().split() if val else []
            if len(ci_names) == 0 or ci.lower() in ci_names:
                result = True
    return result
","if key == ""CI_KEEP_ALIVE"" :",150
"def get_note_title_file(note):
    mo = note_title_re.match(note.get(""content"", """"))
    if mo:
        fn = mo.groups()[0]
        fn = fn.replace("" "", ""_"")
        fn = fn.replace(""/"", ""_"")
        if not fn:
            return """"
        if isinstance(fn, str):
            fn = unicode(fn, ""utf-8"")
        else:
            fn = unicode(fn)
        if note_markdown(note):
            fn += "".mkdn""
        else:
            fn += "".txt""
        return fn
    else:
        return """"
","if isinstance ( fn , str ) :",169
"def post(self, orgname, teamname):
    if _syncing_setup_allowed(orgname):
        try:
            team = model.team.get_organization_team(orgname, teamname)
        except model.InvalidTeamException:
            raise NotFound()
        config = request.get_json()
        # Ensure that the specified config points to a valid group.
        status, err = authentication.check_group_lookup_args(config)
        if not status:
            raise InvalidRequest(""Could not sync to group: %s"" % err)
        # Set the team's syncing config.
        model.team.set_team_syncing(team, authentication.federated_service, config)
        return team_view(orgname, team)
    raise Unauthorized()
",if not status :,199
"def _marshalData(self):
    if self._cache == None:
        d = self._data
        s = """"
        s = time.strftime(""%H:%M:%S"", (0, 0, 0) + d + (0, 0, -1))
        f = d[2] - int(d[2])
        if f != 0:
            s += (""%g"" % f)[1:]
        s += ""Z""
        self._cache = s
    return self._cache
",if f != 0 :,126
"def _get_level(levels, level_ref):
    if level_ref in levels:
        return levels.index(level_ref)
    if isinstance(level_ref, six.integer_types):
        if level_ref < 0:
            level_ref += len(levels)
        if not (0 <= level_ref < len(levels)):
            raise PatsyError(""specified level %r is out of range"" % (level_ref,))
        return level_ref
    raise PatsyError(""specified level %r not found"" % (level_ref,))
",if not ( 0 <= level_ref < len ( levels ) ) :,138
"def iterfieldselect(source, field, where, complement, missing):
    it = iter(source)
    hdr = next(it)
    yield tuple(hdr)
    indices = asindices(hdr, field)
    getv = operator.itemgetter(*indices)
    for row in it:
        try:
            v = getv(row)
        except IndexError:
            v = missing
        if bool(where(v)) != complement:  # XOR
            yield tuple(row)
",if bool ( where ( v ) ) != complement :,122
"def _test_wait_read_invalid_switch(self, sleep):
    sock1, sock2 = socket.socketpair()
    try:
        p = gevent.spawn(
            util.wrap_errors(
                AssertionError, socket.wait_read
            ),  # pylint:disable=no-member
            sock1.fileno(),
        )
        gevent.get_hub().loop.run_callback(switch_None, p)
        if sleep is not None:
            gevent.sleep(sleep)
        result = p.get()
        assert isinstance(result, AssertionError), result
        assert ""Invalid switch"" in str(result), repr(str(result))
    finally:
        sock1.close()
        sock2.close()
",if sleep is not None :,190
"def train(config, args):
    gan = setup_gan(config, inputs, args)
    test_batches = []
    for i in range(args.steps):
        gan.step()
        if i % args.sample_every == 0 and i > 0:
            correct_prediction = 0
            total = 0
            for (x, y) in gan.inputs.testdata():
                prediction = gan.generator(x)
                correct_prediction += (
                    torch.argmax(prediction, 1) == torch.argmax(y, 1)
                ).sum()
                total += y.shape[0]
            accuracy = (float(correct_prediction) / total) * 100
            print(""accuracy: "", accuracy)
    return sum_metrics
",if i % args . sample_every == 0 and i > 0 :,200
"def process_response(self, request, response, spider):
    if not response.body:
        return response
    for fmt, func in six.iteritems(self._formats):
        new_response = func(response)
        if new_response:
            logger.debug(
                ""Decompressed response with format: %(responsefmt)s"",
                {""responsefmt"": fmt},
                extra={""spider"": spider},
            )
            return new_response
    return response
",if new_response :,126
"def detect_ssl_option(self):
    for option in self.ssl_options():
        if scan_argv(self.argv, option) is not None:
            for other_option in self.ssl_options():
                if option != other_option:
                    if scan_argv(self.argv, other_option) is not None:
                        raise ConfigurationError(
                            ""Cannot give both %s and %s"" % (option, other_option)
                        )
            return option
","if scan_argv ( self . argv , option ) is not None :",140
"def load(cls, storefile, template_store):
    # Did we get file or filename?
    if not hasattr(storefile, ""read""):
        storefile = open(storefile, ""rb"")
    # Adjust store to have translations
    store = cls.convertfile(storefile, template_store)
    for unit in store.units:
        if unit.isheader():
            continue
        # HTML does this properly on loading, others need it
        if cls.needs_target_sync:
            unit.target = unit.source
            unit.rich_target = unit.rich_source
    return store
",if unit . isheader ( ) :,152
"def _pre_get_table(self, _ctx, table_name):
    vsctl_table = self._get_table(table_name)
    schema_helper = self.schema_helper
    schema_helper.register_table(vsctl_table.table_name)
    for row_id in vsctl_table.row_ids:
        if row_id.table:
            schema_helper.register_table(row_id.table)
        if row_id.name_column:
            schema_helper.register_columns(row_id.table, [row_id.name_column])
        if row_id.uuid_column:
            schema_helper.register_columns(row_id.table, [row_id.uuid_column])
    return vsctl_table
",if row_id . table :,194
"def __init__(self, pin=None, pull_up=False):
    super(InputDevice, self).__init__(pin)
    try:
        self.pin.function = ""input""
        pull = ""up"" if pull_up else ""down""
        if self.pin.pull != pull:
            self.pin.pull = pull
    except:
        self.close()
        raise
    self._active_state = False if pull_up else True
    self._inactive_state = True if pull_up else False
",if self . pin . pull != pull :,130
"def _increment_operations_count(self, operation, executed):
    with self._lock:
        if executed:
            self._executed_operations += 1
            self._executed[operation.job_type] += 1
        else:
            self._skipped[operation.job_type] += 1
",if executed :,76
"def emit(self, type, info=None):
    # Overload emit() to send events to the proxy object at the other end
    ev = super().emit(type, info)
    if self._has_proxy is True and self._session.status > 0:
        # implicit: and self._disposed is False:
        if type in self.__proxy_properties__:
            self._session.send_command(""INVOKE"", self._id, ""_emit_at_proxy"", [ev])
        elif type in self.__event_types_at_proxy:
            self._session.send_command(""INVOKE"", self._id, ""_emit_at_proxy"", [ev])
",if type in self . __proxy_properties__ :,161
"def validate_pull_secret(namespace):
    if namespace.pull_secret is None:
        # TODO: add aka.ms link here
        warning = (
            ""No --pull-secret provided: cluster will not include samples or operators from ""
            + ""Red Hat or from certified partners.""
        )
        logger.warning(warning)
    else:
        try:
            if not isinstance(json.loads(namespace.pull_secret), dict):
                raise Exception()
        except:
            raise InvalidArgumentValueError(""Invalid --pull-secret."")
","if not isinstance ( json . loads ( namespace . pull_secret ) , dict ) :",145
"def pack(types, *args):
    if len(types) != len(args):
        raise Exception(""number of arguments does not match format string"")
    port = StringIO()
    for (type, value) in zip(types, args):
        if type == ""V"":
            write_vuint(port, value)
        elif type == ""v"":
            write_vint(port, value)
        elif type == ""s"":
            write_bvec(port, value)
        else:
            raise Exception('unknown xpack format string item ""' + type + '""')
    return port.getvalue()
","elif type == ""s"" :",153
"def data(self):
    if self._data is not None:
        return self._data
    else:
        if os.path.exists(self.path):
            with open(self.path, ""rb"") as jsonfile:
                data = jsonfile.read().decode(""utf8"")
                data = json.loads(data)
                self._data = data
                return self._data
        else:
            return dict()
",if os . path . exists ( self . path ) :,120
"def interact(self):
    self.output.write(""\n"")
    while True:
        try:
            request = self.getline(""help> "")
            if not request:
                break
        except (KeyboardInterrupt, EOFError):
            break
        request = strip(request)
        # Make sure significant trailing quotation marks of literals don't
        # get deleted while cleaning input
        if (
            len(request) > 2
            and request[0] == request[-1] in (""'"", '""')
            and request[0] not in request[1:-1]
        ):
            request = request[1:-1]
        if lower(request) in (""q"", ""quit""):
            break
        self.help(request)
",if not request :,195
"def api_attachment_metadata(self):
    resp = []
    for part in self.parts:
        if not part.is_attachment:
            continue
        k = {
            ""content_type"": part.block.content_type,
            ""size"": part.block.size,
            ""filename"": part.block.filename,
            ""id"": part.block.public_id,
        }
        content_id = part.content_id
        if content_id:
            if content_id[0] == ""<"" and content_id[-1] == "">"":
                content_id = content_id[1:-1]
            k[""content_id""] = content_id
        resp.append(k)
    return resp
",if not part . is_attachment :,194
"def _notin_text(term, text, verbose=False):
    index = text.find(term)
    head = text[:index]
    tail = text[index + len(term) :]
    correct_text = head + tail
    diff = _diff_text(correct_text, text, verbose)
    newdiff = [u(""%s is contained here:"") % py.io.saferepr(term, maxsize=42)]
    for line in diff:
        if line.startswith(u(""Skipping"")):
            continue
        if line.startswith(u(""- "")):
            continue
        if line.startswith(u(""+ "")):
            newdiff.append(u(""  "") + line[2:])
        else:
            newdiff.append(line)
    return newdiff
","if line . startswith ( u ( ""- "" ) ) :",192
"def get_api(user, url):
    global API_CACHE
    if API_CACHE is None or API_CACHE.get(url) is None:
        API_CACHE_LOCK.acquire()
        try:
            if API_CACHE is None:
                API_CACHE = {}
            if API_CACHE.get(url) is None:
                API_CACHE[url] = ImpalaDaemonApi(url)
        finally:
            API_CACHE_LOCK.release()
    api = API_CACHE[url]
    api.set_user(user)
    return api
",if API_CACHE is None :,148
"def __str__(self, prefix="""", printElemNumber=0):
    res = """"
    if self.has_index_name_:
        res += prefix + (""index_name: %s\n"" % self.DebugFormatString(self.index_name_))
    cnt = 0
    for e in self.prefix_value_:
        elm = """"
        if printElemNumber:
            elm = ""(%d)"" % cnt
        res += prefix + (""prefix_value%s: %s\n"" % (elm, self.DebugFormatString(e)))
        cnt += 1
    if self.has_value_prefix_:
        res += prefix + (
            ""value_prefix: %s\n"" % self.DebugFormatBool(self.value_prefix_)
        )
    return res
",if printElemNumber :,195
"def add_group(x, nl, in_group, mw):
    if len(x) == 0:
        return x
    if len(x) > 1 and not in_group:
        if supports_group(x, nl):
            return [""[[""] + x + [""]]""]
        mw.warn(
            ""Equation will multiplex and may produce inaccurate results (see manual)""
        )
    return [""[""] + x + [""]""]
","if supports_group ( x , nl ) :",114
"def unfulfilled_items(self):
    unfulfilled_items = 0
    for order_item in self.items.all():
        if not order_item.canceled:
            aggr = order_item.deliver_item.aggregate(delivered=Sum(""quantity""))
            unfulfilled_items += order_item.quantity - (aggr[""delivered""] or 0)
    return unfulfilled_items
",if not order_item . canceled :,94
"def _get_pattern(self, pattern_id):
    """"""Get pattern item by id.""""""
    for key in (Tag.PATTERNS1, Tag.PATTERNS2, Tag.PATTERNS3):
        if key in self.tagged_blocks:
            data = self.tagged_blocks.get_data(key)
            for pattern in data:
                if pattern.pattern_id == pattern_id:
                    return pattern
    return None
",if key in self . tagged_blocks :,110
"def query_lister(domain, query="""", max_items=None, attr_names=None):
    more_results = True
    num_results = 0
    next_token = None
    while more_results:
        rs = domain.connection.query_with_attributes(
            domain, query, attr_names, next_token=next_token
        )
        for item in rs:
            if max_items:
                if num_results == max_items:
                    raise StopIteration
            yield item
            num_results += 1
        next_token = rs.next_token
        more_results = next_token != None
",if num_results == max_items :,166
"def find_deprecated_settings(source):  # pragma: no cover
    from celery.utils import deprecated
    for name, opt in flatten(NAMESPACES):
        if (opt.deprecate_by or opt.remove_by) and getattr(source, name, None):
            deprecated.warn(
                description=""The {0!r} setting"".format(name),
                deprecation=opt.deprecate_by,
                removal=opt.remove_by,
                alternative=""Use the {0.alt} instead"".format(opt),
            )
    return source
","if ( opt . deprecate_by or opt . remove_by ) and getattr ( source , name , None ) :",150
"def tearDown(self):
    """"""Shutdown the server.""""""
    try:
        if self.server:
            self.server.stop(2.0)
        if self.sl_hdlr:
            self.root_logger.removeHandler(self.sl_hdlr)
            self.sl_hdlr.close()
    finally:
        BaseTest.tearDown(self)
",if self . server :,96
"def broadcast_events(self, events):
    LOGGER.debug(""Broadcasting events: %s"", events)
    with self._subscribers_cv:
        # Copy the subscribers
        subscribers = {conn: sub.copy() for conn, sub in self._subscribers.items()}
    if subscribers:
        for connection_id, subscriber in subscribers.items():
            if subscriber.is_listening():
                subscriber_events = [
                    event for event in events if subscriber.is_subscribed(event)
                ]
                event_list = EventList(events=subscriber_events)
                self._send(connection_id, event_list.SerializeToString())
",if subscriber . is_listening ( ) :,173
"def _get_info(self, path):
    info = OrderedDict()
    if not self._is_mac() or self._has_xcode_tools():
        stdout = None
        try:
            stdout, stderr = Popen(
                [self._find_binary(), ""info"", os.path.realpath(path)],
                stdout=PIPE,
                stderr=PIPE,
            ).communicate()
        except OSError:
            pass
        else:
            if stdout:
                for line in stdout.splitlines():
                    line = u(line).split("": "", 1)
                    if len(line) == 2:
                        info[line[0]] = line[1]
    return info
",if len ( line ) == 2 :,194
"def test_call_extern_c_fn(self):
    global memcmp
    memcmp = cffi_support.ExternCFunction(
        ""memcmp"",
        (""int memcmp ( const uint8_t * ptr1, "" ""const uint8_t * ptr2, size_t num )""),
    )
    @udf(BooleanVal(FunctionContext, StringVal, StringVal))
    def fn(context, a, b):
        if a.is_null != b.is_null:
            return False
        if a is None:
            return True
        if len(a) != b.len:
            return False
        if a.ptr == b.ptr:
            return True
        return memcmp(a.ptr, b.ptr, a.len) == 0
",if a is None :,199
"def _flatten(*args):
    ahs = set()
    if len(args) > 0:
        for item in args:
            if type(item) is ActionHandle:
                ahs.add(item)
            elif type(item) in (list, tuple, dict, set):
                for ah in item:
                    if type(ah) is not ActionHandle:  # pragma:nocover
                        raise ActionManagerError(""Bad argument type %s"" % str(ah))
                    ahs.add(ah)
            else:  # pragma:nocover
                raise ActionManagerError(""Bad argument type %s"" % str(item))
    return ahs
",if type ( ah ) is not ActionHandle :,183
"def startElement(self, name, attrs, connection):
    if name == ""Parameter"":
        if self._current_param:
            self[self._current_param.name] = self._current_param
        self._current_param = Parameter(self)
        return self._current_param
",if self . _current_param :,73
"def _find_class_in_descendants(self, search_key):
    for cls in self.primitive_classes:
        cls_key = (cls.__name__, cls.__module__)
        self.class_cache[cls_key] = cls
        if cls_key == search_key:
            return cls
",if cls_key == search_key :,77
"def doWorkForFindAll(self, v, target, partialMatch):
    sibling = self
    while sibling:
        c1 = partialMatch and sibling.equalsTreePartial(target)
        if c1:
            v.append(sibling)
        else:
            c2 = not partialMatch and sibling.equalsTree(target)
            if c2:
                v.append(sibling)
        ### regardless of match or not, check any children for matches
        if sibling.getFirstChild():
            sibling.getFirstChild().doWorkForFindAll(v, target, partialMatch)
        sibling = sibling.getNextSibling()
",if sibling . getFirstChild ( ) :,163
"def forward(self, inputs: paddle.Tensor):
    outputs = []
    blocks = self.block(inputs)
    route = None
    for i, block in enumerate(blocks):
        if i > 0:
            block = paddle.concat([route, block], axis=1)
        route, tip = self.yolo_blocks[i](block)
        block_out = self.block_outputs[i](tip)
        outputs.append(block_out)
        if i < 2:
            route = self.route_blocks_2[i](route)
            route = self.upsample(route)
    return outputs
",if i < 2 :,163
"def _filter_paths(basename, path, is_dir, exclude):
    """""".gitignore style file filtering.""""""
    for item in exclude:
        # Items ending in '/' apply only to directories.
        if item.endswith(""/"") and not is_dir:
            continue
        # Items starting with '/' apply to the whole path.
        # In any other cases just the basename is used.
        match = path if item.startswith(""/"") else basename
        if fnmatch.fnmatch(match, item.strip(""/"")):
            return True
    return False
","if item . endswith ( ""/"" ) and not is_dir :",130
"def reposition_division(f1):
    lines = f1.splitlines()
    if lines[2] == division:
        lines.pop(2)
    found = 0
    for i, line in enumerate(lines):
        if line.startswith('""""""'):
            found += 1
            if found == 2:
                if division in ""\n"".join(lines):
                    break  # already in the right place
                lines.insert(i + 1, """")
                lines.insert(i + 2, division)
                break
    return ""\n"".join(lines)
","if division in ""\n"" . join ( lines ) :",153
"def buildImage(opt):
    dpath = os.path.join(opt[""datapath""], ""COCO-IMG-2015"")
    version = ""1""
    if not build_data.built(dpath, version_string=version):
        print(""[building image data: "" + dpath + ""]"")
        if build_data.built(dpath):
            # An older version exists, so remove these outdated files.
            build_data.remove_dir(dpath)
        build_data.make_dir(dpath)
        # Download the data.
        for downloadable_file in RESOURCES[:1]:
            downloadable_file.download_file(dpath)
        # Mark the data as built.
        build_data.mark_done(dpath, version_string=version)
",if build_data . built ( dpath ) :,191
"def colorformat(text):
    if text[0:1] == ""#"":
        col = text[1:]
        if len(col) == 6:
            return col
        elif len(col) == 3:
            return col[0] * 2 + col[1] * 2 + col[2] * 2
    elif text == """":
        return """"
    assert False, ""wrong color format %r"" % text
",if len ( col ) == 6 :,105
"def tree_print(tree):
    for key in tree:
        print(key, end="" "")  # end=' ' prevents a newline character
        tree_element = tree[key]  # multiple lookups is expensive, even amortized O(1)!
        for subElem in tree_element:
            print("" -> "", subElem, end="" "")
            if type(subElem) != str:  # OP wants indenting after digits
                print(""\n "")  # newline and a space to match indenting
        print()  # forces a newline
",if type ( subElem ) != str :,138
"def is_dse_cluster(path):
    try:
        with open(os.path.join(path, ""CURRENT""), ""r"") as f:
            name = f.readline().strip()
            cluster_path = os.path.join(path, name)
            filename = os.path.join(cluster_path, ""cluster.conf"")
            with open(filename, ""r"") as f:
                data = yaml.load(f)
            if ""dse_dir"" in data:
                return True
    except IOError:
        return False
","if ""dse_dir"" in data :",148
"def delete_old_target_output_files(classpath_prefix):
    """"""Delete existing output files or symlinks for target.""""""
    directory, basename = os.path.split(classpath_prefix)
    pattern = re.compile(
        r""^{basename}(([0-9]+)(\.jar)?|classpath\.txt)$"".format(
            basename=re.escape(basename)
        )
    )
    files = [filename for filename in os.listdir(directory) if pattern.match(filename)]
    for rel_path in files:
        path = os.path.join(directory, rel_path)
        if os.path.islink(path) or os.path.isfile(path):
            safe_delete(path)
",if os . path . islink ( path ) or os . path . isfile ( path ) :,175
"def test_files(self):
    # get names of files to test
    dist_dir = os.path.join(os.path.dirname(__file__), os.pardir, os.pardir)
    names = []
    for d in self.test_directories:
        test_dir = os.path.join(dist_dir, d)
        for n in os.listdir(test_dir):
            if n.endswith("".py"") and not n.startswith(""bad""):
                names.append(os.path.join(test_dir, n))
    for filename in names:
        if test_support.verbose:
            print(""Testing %s"" % filename)
        source = read_pyfile(filename)
        self.check_roundtrip(source)
",if test_support . verbose :,186
"def __str__(self):
    if self.HasError():
        return self.ErrorAsStr()
    else:
        # Format is: {action} ""{target}"" ({filename}:{lineno})
        string = self._action
        if self._target is not None:
            string += ' ""{target}""'.format(target=self._target)
        if self._filename is not None:
            path = self._filename
            if self._lineno is not None:
                path += "":{lineno}"".format(lineno=self._lineno)
            string += "" ({path})"".format(path=path)
        return string
",if self . _filename is not None :,156
"def extra_action_out(self, input_dict, state_batches, model, action_dist):
    with self._no_grad_context():
        if extra_action_out_fn:
            stats_dict = extra_action_out_fn(
                self, input_dict, state_batches, model, action_dist
            )
        else:
            stats_dict = parent_cls.extra_action_out(
                self, input_dict, state_batches, model, action_dist
            )
        return self._convert_to_non_torch_type(stats_dict)
",if extra_action_out_fn :,156
"def _retract_bindings(fstruct, inv_bindings, fs_class, visited):
    # Visit each node only once:
    if id(fstruct) in visited:
        return
    visited.add(id(fstruct))
    if _is_mapping(fstruct):
        items = fstruct.items()
    elif _is_sequence(fstruct):
        items = enumerate(fstruct)
    else:
        raise ValueError(""Expected mapping or sequence"")
    for (fname, fval) in items:
        if isinstance(fval, fs_class):
            if id(fval) in inv_bindings:
                fstruct[fname] = inv_bindings[id(fval)]
            _retract_bindings(fval, inv_bindings, fs_class, visited)
",if id ( fval ) in inv_bindings :,181
"def warehouses(self) -> tuple:
    from ..repositories import WarehouseBaseRepo
    repos = dict()
    for dep in chain(self.dependencies, [self]):
        if dep.repo is None:
            continue
        if not isinstance(dep.repo, WarehouseBaseRepo):
            continue
        for repo in dep.repo.repos:
            if repo.from_config:
                continue
            repos[repo.name] = repo
    return tuple(repos.values())
",if dep . repo is None :,124
"def detype(self):
    if self._detyped is not None:
        return self._detyped
    ctx = {}
    for key, val in self._d.items():
        if not isinstance(key, str):
            key = str(key)
        detyper = self.get_detyper(key)
        if detyper is None:
            # cannot be detyped
            continue
        deval = detyper(val)
        if deval is None:
            # cannot be detyped
            continue
        ctx[key] = deval
    self._detyped = ctx
    return ctx
",if detyper is None :,163
"def populate_obj(self, obj, name):
    field = getattr(obj, name, None)
    if field is not None:
        # If field should be deleted, clean it up
        if self._should_delete:
            field.delete()
            return
        if isinstance(self.data, FileStorage) and not is_empty(self.data.stream):
            if not field.grid_id:
                func = field.put
            else:
                func = field.replace
            func(
                self.data.stream,
                filename=self.data.filename,
                content_type=self.data.content_type,
            )
",if self . _should_delete :,182
"def _load(container):
    if isinstance(container, str):
        # If container is a filename.
        if all(c in string.printable for c in container) and os.path.exists(container):
            with open(container, ""rb"") as f:
                return pickle.load(f)
        # If container is a pickle string.
        else:
            return pickle.loads(container)
    # If container is an open file
    elif isinstance(container, IOBase):
        return pickle.load(container)
    # What else could it be?
    else:
        l.error(""Cannot unpickle container of type %s"", type(container))
        return None
",if all ( c in string . printable for c in container ) and os . path . exists ( container ) :,171
"def append_row(self, row):
    self.allocate_future_payments(row)
    self.set_invoice_details(row)
    self.set_party_details(row)
    self.set_ageing(row)
    if self.filters.get(""group_by_party""):
        self.update_sub_total_row(row, row.party)
        if self.previous_party and (self.previous_party != row.party):
            self.append_subtotal_row(self.previous_party)
        self.previous_party = row.party
    self.data.append(row)
",if self . previous_party and ( self . previous_party != row . party ) :,152
"def gg1():
    while 1:
        tt = 3
        while tt > 0:
            trace.append(tt)
            val = yield
            if val is not None:
                tt = 10  # <= uncomment this line
                trace.append(""breaking early..."")
                break
            tt -= 1
        trace.append(""try!"")
",if val is not None :,101
"def migrate_common_facts(facts):
    """"""Migrate facts from various roles into common""""""
    params = {""node"": (""portal_net""), ""master"": (""portal_net"")}
    if ""common"" not in facts:
        facts[""common""] = {}
    # pylint: disable=consider-iterating-dictionary
    for role in params.keys():
        if role in facts:
            for param in params[role]:
                if param in facts[role]:
                    facts[""common""][param] = facts[role].pop(param)
    return facts
",if param in facts [ role ] :,137
"def get_measurements(self, pipeline, object_name, category):
    if self.get_categories(pipeline, object_name) == [category]:
        results = []
        if self.do_corr_and_slope:
            if object_name == ""Image"":
                results += [""Correlation"", ""Slope""]
            else:
                results += [""Correlation""]
        if self.do_overlap:
            results += [""Overlap"", ""K""]
        if self.do_manders:
            results += [""Manders""]
        if self.do_rwc:
            results += [""RWC""]
        if self.do_costes:
            results += [""Costes""]
        return results
    return []
",if self . do_corr_and_slope :,195
"def access_modes(self):
    """"""access_modes property""""""
    if self._access_modes is None:
        self._access_modes = self.get_access_modes()
        if not isinstance(self._access_modes, list):
            self._access_modes = list(self._access_modes)
    return self._access_modes
","if not isinstance ( self . _access_modes , list ) :",85
"def unwrap_envelope(self, data, many):
    if many:
        if data[""items""]:
            if isinstance(data, InstrumentedList) or isinstance(data, list):
                self.context[""total""] = len(data)
                return data
            else:
                self.context[""total""] = data[""total""]
        else:
            self.context[""total""] = 0
            data = {""items"": []}
        return data[""items""]
    return data
","if data [ ""items"" ] :",130
"def to_string(self, fmt=""{:.4f}""):
    result_str = """"
    for key in self.measures:
        result = self.m_dict[key][0]()
        result_str += (
            "","".join(fmt.format(x) for x in result)
            if isinstance(result, tuple)
            else fmt.format(result)
        )
        result_str += "",""
    return result_str[:-1]  # trim the last comma
","if isinstance ( result , tuple )",121
"def on_torrent_created(self, result):
    if not result:
        return
    self.dialog_widget.btn_create.setEnabled(True)
    self.dialog_widget.edit_channel_create_torrent_progress_label.setText(
        ""Created torrent""
    )
    if ""torrent"" in result:
        self.create_torrent_notification.emit({""msg"": ""Torrent successfully created""})
        if self.dialog_widget.add_to_channel_checkbox.isChecked():
            self.add_torrent_to_channel(result[""torrent""])
        self.close_dialog()
",if self . dialog_widget . add_to_channel_checkbox . isChecked ( ) :,151
"def save(self):
    for var_name in self.default_config:
        if getattr(self, var_name, None) == self.default_config[var_name]:
            if var_name in self.file_config:
                del self.file_config[var_name]
        else:
            self.file_config[var_name] = getattr(self, var_name)
    with open(self.config_path, ""w"") as f:
        f.write(json.dumps(self.file_config, indent=2))
","if getattr ( self , var_name , None ) == self . default_config [ var_name ] :",141
"def get_class_parameters(kwarg):
    ret = {""attrs"": []}
    for key in (""rsc"", ""fsc"", ""usc""):
        if key in kwarg:
            ret[""attrs""].append(
                [
                    ""TCA_HFSC_%s"" % key.upper(),
                    {
                        ""m1"": get_rate(kwarg[key].get(""m1"", 0)),
                        ""d"": get_time(kwarg[key].get(""d"", 0)),
                        ""m2"": get_rate(kwarg[key].get(""m2"", 0)),
                    },
                ]
            )
    return ret
",if key in kwarg :,184
"def forward(self, x):
    f_x = x
    if self.exp:
        f_x = self.exp_swish(self.exp_bn(self.exp(f_x)))
    f_x = self.dwise_swish(self.dwise_bn(self.dwise(f_x)))
    f_x = self.se(f_x)
    f_x = self.lin_proj_bn(self.lin_proj(f_x))
    if self.has_skip:
        if self.training and effnet_cfg.EN.DC_RATIO > 0.0:
            f_x = drop_connect(f_x, effnet_cfg.EN.DC_RATIO)
        f_x = x + f_x
    return f_x
",if self . training and effnet_cfg . EN . DC_RATIO > 0.0 :,193
"def cli_uninstall_distro():
    distro_list = install_distro_list()
    if distro_list is not None:
        for index, _distro_dir in enumerate(distro_list):
            log(str(index) + ""  --->>  "" + _distro_dir)
        user_input = read_input_uninstall()
        if user_input is not False:
            for index, _distro_dir in enumerate(distro_list):
                if index == user_input:
                    config.uninstall_distro_dir_name = _distro_dir
                    unin_distro()
    else:
        log(""No distro installed on "" + config.usb_disk)
",if index == user_input :,189
"def IMPORTFROM(self, node):
    if node.module == ""__future__"":
        if not self.futuresAllowed:
            self.report(messages.LateFutureImport, node, [n.name for n in node.names])
    else:
        self.futuresAllowed = False
    for alias in node.names:
        if alias.name == ""*"":
            self.scope.importStarred = True
            self.report(messages.ImportStarUsed, node, node.module)
            continue
        name = alias.asname or alias.name
        importation = Importation(name, node)
        if node.module == ""__future__"":
            importation.used = (self.scope, node)
        self.addBinding(node, importation)
",if not self . futuresAllowed :,190
"def _split_and_load(batch, ctx_list):
    """"""Split data to 1 batch each device.""""""
    new_batch = []
    for _, data in enumerate(batch):
        if isinstance(data, (list, tuple)):
            new_data = [x.as_in_context(ctx) for x, ctx in zip(data, ctx_list)]
        else:
            new_data = [data.as_in_context(ctx_list[0])]
        new_batch.append(new_data)
    return new_batch
","if isinstance ( data , ( list , tuple ) ) :",135
"def wait_success(self, timeout=60 * 10):
    for i in range(timeout // 10):
        time.sleep(10)
        status = self.query_job()
        print(""job {} status is {}"".format(self.job_id, status))
        if status and status == StatusSet.SUCCESS:
            return True
        if status and status in [
            StatusSet.CANCELED,
            StatusSet.TIMEOUT,
            StatusSet.FAILED,
        ]:
            return False
    return False
",if status and status == StatusSet . SUCCESS :,134
"def copy_tree(self, src_dir, dst_dir, skip_variables=False):
    for src_root, _, files in os.walk(src_dir):
        if src_root != src_dir:
            rel_root = os.path.relpath(src_root, src_dir)
        else:
            rel_root = """"
        if skip_variables and rel_root.startswith(""variables""):
            continue
        dst_root = os.path.join(dst_dir, rel_root)
        if not os.path.exists(dst_root):
            os.makedirs(dst_root)
        for f in files:
            shutil.copy(os.path.join(src_root, f), os.path.join(dst_root, f))
",if src_root != src_dir :,197
"def _make_padded_shapes(self, dataset, decoders):
    padded_shapes = dataset.output_shapes
    for i, hparams_i in enumerate(self._hparams.datasets):
        if not _is_text_data(hparams_i[""data_type""]):
            continue
        if not hparams_i[""pad_to_max_seq_length""]:
            continue
        text_and_id_shapes = MonoTextData._make_padded_text_and_id_shapes(
            dataset, hparams_i, decoders[i], self.text_name(i), self.text_id_name(i)
        )
        padded_shapes.update(text_and_id_shapes)
    return padded_shapes
","if not _is_text_data ( hparams_i [ ""data_type"" ] ) :",179
"def format_errors(messages):
    errors = {}
    for k, v in messages.items():
        key = camelize(k, uppercase_first_letter=False)
        if isinstance(v, dict):
            errors[key] = format_errors(v)
        elif isinstance(v, list):
            errors[key] = v[0]
    return errors
","if isinstance ( v , dict ) :",94
"def generic_visit(self, node, parents=None):
    parents = (parents or []) + [node]
    for field, value in iter_fields(node):
        if isinstance(value, list):
            for item in value:
                if isinstance(item, AST):
                    self.visit(item, parents)
        elif isinstance(value, AST):
            self.visit(value, parents)
","if isinstance ( value , list ) :",106
"def get_override_css(self):
    """"""handls allow_css_overrides setting.""""""
    if self.settings.get(""allow_css_overrides""):
        filename = self.view.file_name()
        filetypes = self.settings.get(""markdown_filetypes"")
        if filename and filetypes:
            for filetype in filetypes:
                if filename.endswith(filetype):
                    css_filename = filename.rpartition(filetype)[0] + "".css""
                    if os.path.isfile(css_filename):
                        return u""<style>%s</style>"" % load_utf8(css_filename)
    return """"
",if filename . endswith ( filetype ) :,165
"def clean(self):
    super().clean()
    # If the Cluster is assigned to a Site, all Devices must be assigned to that Site.
    if self.cluster.site is not None:
        for device in self.cleaned_data.get(""devices"", []):
            if device.site != self.cluster.site:
                raise ValidationError(
                    {
                        ""devices"": ""{} belongs to a different site ({}) than the cluster ({})"".format(
                            device, device.site, self.cluster.site
                        )
                    }
                )
",if device . site != self . cluster . site :,156
"def _setProcessPriority(process, nice_val, disable_gc):
    org_nice_val = Computer._process_original_nice_value
    try:
        process.nice(nice_val)
        Computer.in_high_priority_mode = nice_val != org_nice_val
        if disable_gc:
            gc.disable()
        else:
            gc.enable()
        return True
    except psutil.AccessDenied:
        print2err(
            ""WARNING: Could not set process {} priority ""
            ""to {}"".format(process.pid, nice_val)
        )
        return False
",if disable_gc :,161
"def _setResultsName(self, name, listAllMatches=False):
    if __diag__.warn_multiple_tokens_in_named_alternation:
        if any(isinstance(e, And) for e in self.exprs):
            warnings.warn(
                ""{}: setting results name {!r} on {} expression ""
                ""may only return a single token for an And alternative, ""
                ""in future will return the full list of tokens"".format(
                    ""warn_multiple_tokens_in_named_alternation"",
                    name,
                    type(self).__name__,
                ),
                stacklevel=3,
            )
    return super()._setResultsName(name, listAllMatches)
","if any ( isinstance ( e , And ) for e in self . exprs ) :",193
"def make_sources(project: RootDependency) -> str:
    content = []
    if project.readme:
        content.append(project.readme.path.name)
        if project.readme.markup != ""rst"":
            content.append(project.readme.to_rst().path.name)
    path = project.package.path
    for fname in (""setup.cfg"", ""setup.py""):
        if (path / fname).exists():
            content.append(fname)
    for package in chain(project.package.packages, project.package.data):
        for fpath in package:
            fpath = fpath.relative_to(project.package.path)
            content.append(""/"".join(fpath.parts))
    return ""\n"".join(content)
",if ( path / fname ) . exists ( ) :,193
"def findControlPointsInMesh(glyph, va, subsegments):
    controlPointIndices = np.zeros((len(va), 1))
    index = 0
    for i, c in enumerate(subsegments):
        segmentCount = len(glyph.contours[i].segments) - 1
        for j, s in enumerate(c):
            if j < segmentCount:
                if glyph.contours[i].segments[j].type == ""line"":
                    controlPointIndices[index] = 1
            index += s[1]
    return controlPointIndices
","if glyph . contours [ i ] . segments [ j ] . type == ""line"" :",143
"def MergeFrom(self, other):
    if self.message_class is not None:
        if other.Parse(self.message_class):
            self.message.MergeFrom(other.message)
    elif other.message_class is not None:
        if not self.Parse(other.message_class):
            self.message = other.message_class()
            self.message_class = other.message_class
        self.message.MergeFrom(other.message)
    else:
        self.message += other.message
",if not self . Parse ( other . message_class ) :,134
"def remove_old_snapshot(install_dir):
    logging.info(""Removing any old files in {}"".format(install_dir))
    for file in glob.glob(""{}/*"".format(install_dir)):
        try:
            if os.path.isfile(file):
                os.unlink(file)
            elif os.path.isdir(file):
                shutil.rmtree(file)
        except Exception as error:
            logging.error(""Error: {}"".format(error))
            sys.exit(1)
",if os . path . isfile ( file ) :,133
"def writexml(
    self,
    stream,
    indent="""",
    addindent="""",
    newl="""",
    strip=0,
    nsprefixes={},
    namespace="""",
):
    w = _streamWriteWrapper(stream)
    if self.raw:
        val = self.nodeValue
        if not isinstance(val, str):
            val = str(self.nodeValue)
    else:
        v = self.nodeValue
        if not isinstance(v, str):
            v = str(v)
        if strip:
            v = "" "".join(v.split())
        val = escape(v)
    w(val)
",if strip :,164
"def validate_attributes(self):
    for attribute in self.get_all_attributes():
        value = getattr(self, attribute.code, None)
        if value is None:
            if attribute.required:
                raise ValidationError(
                    _(""%(attr)s attribute cannot be blank"") % {""attr"": attribute.code}
                )
        else:
            try:
                attribute.validate_value(value)
            except ValidationError as e:
                raise ValidationError(
                    _(""%(attr)s attribute %(err)s"") % {""attr"": attribute.code, ""err"": e}
                )
",if value is None :,168
"def PyJsHoisted_BinaryExpression_(node, parent, this, arguments, var=var):
    var = Scope(
        {u""node"": node, u""this"": this, u""arguments"": arguments, u""parent"": parent}, var
    )
    var.registers([u""node"", u""parent""])
    if PyJsStrictEq(var.get(u""node"").get(u""operator""), Js(u""in"")):
        if var.get(u""t"").callprop(u""isVariableDeclarator"", var.get(u""parent"")):
            return var.get(u""true"")
        if var.get(u""t"").callprop(u""isFor"", var.get(u""parent"")):
            return var.get(u""true"")
    return Js(False)
","if var . get ( u""t"" ) . callprop ( u""isVariableDeclarator"" , var . get ( u""parent"" ) ) :",196
"def distinct(expr, *on):
    fields = frozenset(expr.fields)
    _on = []
    append = _on.append
    for n in on:
        if isinstance(n, Field):
            if n._child.isidentical(expr):
                n = n._name
            else:
                raise ValueError(""{0} is not a field of {1}"".format(n, expr))
        if not isinstance(n, _strtypes):
            raise TypeError(""on must be a name or field, not: {0}"".format(n))
        elif n not in fields:
            raise ValueError(""{0} is not a field of {1}"".format(n, expr))
        append(n)
    return Distinct(expr, tuple(_on))
","if not isinstance ( n , _strtypes ) :",192
"def encode(self, msg):
    """"""Encodes the message to the stream encoding.""""""
    stream = self.stream
    rv = msg + ""\n""
    if (PY2 and is_unicode(rv)) or not (
        PY2 or is_unicode(rv) or _is_text_stream(stream)
    ):
        enc = self.encoding
        if enc is None:
            enc = getattr(stream, ""encoding"", None) or ""utf-8""
        rv = rv.encode(enc, ""replace"")
    return rv
",if enc is None :,131
"def color_convert(self, to_color_space, preserve_alpha=True):
    if to_color_space == self.color_space and preserve_alpha:
        return self
    else:
        pixels = pixels_as_float(self.pixels)
        converted = convert_color(
            pixels, self.color_space, to_color_space, preserve_alpha
        )
        if converted is None:
            return None
        return Image(converted, to_color_space)
",if converted is None :,125
"def seek(self, pos):
    if self.closed:
        raise IOError(""Cannot seek on a closed file"")
    for n, idx in enumerate(self._indexes[::-1]):
        if idx.offset <= pos:
            if idx != self._curidx:
                self._idxiter = iter(self._indexes[-(n + 1) :])
                self._nextidx()
            break
    else:
        raise Exception(""Cannot seek to pos"")
    self._curfile.seek(pos - self._curidx.offset)
",if idx != self . _curidx :,135
"def load_from_json(self, node_data: dict, import_version: float):
    if import_version <= 0.08:
        self.image_pointer = unpack_pointer_property_name(
            bpy.data.images, node_data, ""image_name""
        )
        if not self.image_pointer:
            proposed_name = node_data.get(""image_name"")
            self.info(f""image data not found in current {proposed_name}"")
",if not self . image_pointer :,126
"def __init__(self, execution_context, aggregate_operators):
    super(_QueryExecutionAggregateEndpointComponent, self).__init__(execution_context)
    self._local_aggregators = []
    self._results = None
    self._result_index = 0
    for operator in aggregate_operators:
        if operator == ""Average"":
            self._local_aggregators.append(_AverageAggregator())
        elif operator == ""Count"":
            self._local_aggregators.append(_CountAggregator())
        elif operator == ""Max"":
            self._local_aggregators.append(_MaxAggregator())
        elif operator == ""Min"":
            self._local_aggregators.append(_MinAggregator())
        elif operator == ""Sum"":
            self._local_aggregators.append(_SumAggregator())
","elif operator == ""Count"" :",192
"def attrgetter(item):
    items = [None] * len(attribute)
    for i, attribute_part in enumerate(attribute):
        item_i = item
        for part in attribute_part:
            item_i = environment.getitem(item_i, part)
        if postprocess is not None:
            item_i = postprocess(item_i)
        items[i] = item_i
    return items
",if postprocess is not None :,105
"def work(self):
    while True:
        timeout = self.timeout
        if idle.is_set():
            timeout = self.idle_timeout
        log.debug(""Wait for {}"".format(timeout))
        fetch.wait(timeout)
        if shutting_down.is_set():
            log.info(""Stop fetch worker"")
            break
        self.fetch()
",if idle . is_set ( ) :,99
"def testCoreInterfaceIntInputData():
    result_testing = False
    for _ in range(10):
        hsyncnet_instance = hsyncnet(
            [[1], [2], [3], [20], [21], [22]], 2, initial_type.EQUIPARTITION, ccore=True
        )
        analyser = hsyncnet_instance.process()
        if len(analyser.allocate_clusters(0.1)) == 2:
            result_testing = True
            break
    assert result_testing
",if len ( analyser . allocate_clusters ( 0.1 ) ) == 2 :,132
"def _gen():
    buf = []
    iterable = dataset()
    try:
        while len(buf) < buffer_size:
            buf.append(next(iterable))
        while 1:
            i = random.randint(0, buffer_size - 1)
            n = next(iterable)
            yield buf[i]
            buf[i] = n
    except StopIteration:
        if len(buf):
            random.shuffle(buf)
            for i in buf:
                yield i
",if len ( buf ) :,137
"def debug_tree(tree):
    l = []
    for elt in tree:
        if isinstance(elt, (int, long)):
            l.append(_names.get(elt, elt))
        elif isinstance(elt, str):
            l.append(elt)
        else:
            l.append(debug_tree(elt))
    return l
","elif isinstance ( elt , str ) :",92
"def reverse_code(apps: StateApps, schema_editor: DatabaseSchemaEditor) -> None:
    PreregistrationUser = apps.get_model(""zerver"", ""PreregistrationUser"")
    for user in PreregistrationUser.objects.all():
        if user.invited_as == 2:  # PreregistrationUser.INVITE_AS['REALM_ADMIN']
            user.invited_as_admin = True
        else:  # PreregistrationUser.INVITE_AS['MEMBER']
            user.invited_as_admin = False
        user.save(update_fields=[""invited_as_admin""])
",if user . invited_as == 2 :,151
"def _fastqc_data_section(self, section_name):
    out = []
    in_section = False
    data_file = os.path.join(self._dir, ""fastqc_data.txt"")
    if os.path.exists(data_file):
        with open(data_file) as in_handle:
            for line in in_handle:
                if line.startswith("">>%s"" % section_name):
                    in_section = True
                elif in_section:
                    if line.startswith("">>END""):
                        break
                    out.append(line.rstrip(""\r\n""))
    return out
","if line . startswith ( "">>END"" ) :",173
"def determine_block_hints(self, text):
    hints = """"
    if text:
        if text[0] in "" \n\x85\u2028\u2029"":
            hints += str(self.best_indent)
        if text[-1] not in ""\n\x85\u2028\u2029"":
            hints += ""-""
        elif len(text) == 1 or text[-2] in ""\n\x85\u2028\u2029"":
            hints += ""+""
    return hints
","if text [ - 1 ] not in ""\n\x85\u2028\u2029"" :",132
"def database_app(request):
    if request.param == ""postgres_app"":
        if not which(""initdb""):
            pytest.skip(""initdb must be on PATH for postgresql fixture"")
        if not psycopg2:
            pytest.skip(""psycopg2 must be installed for postgresql fixture"")
    if request.param == ""sqlite_rabbitmq_app"":
        if not os.environ.get(""GALAXY_TEST_AMQP_INTERNAL_CONNECTION""):
            pytest.skip(
                ""rabbitmq tests will be skipped if GALAXY_TEST_AMQP_INTERNAL_CONNECTION env var is unset""
            )
    return request.getfixturevalue(request.param)
","if not os . environ . get ( ""GALAXY_TEST_AMQP_INTERNAL_CONNECTION"" ) :",174
"def do_rollout(agent, env, num_steps, render=False):
    total_rew = 0
    ob = env.reset()
    for t in range(num_steps):
        a = agent.act(ob)
        (ob, reward, done, _info) = env.step(a)
        total_rew += reward
        if render and t % 3 == 0:
            env.render()
        if done:
            break
    return total_rew, t + 1
",if render and t % 3 == 0 :,126
"def _handle_subrepos(self, ctx, dirty_trees):
    substate = util.parse_hgsubstate(ctx["".hgsubstate""].data().splitlines())
    sub = util.OrderedDict()
    if "".hgsub"" in ctx:
        sub = util.parse_hgsub(ctx["".hgsub""].data().splitlines())
    for path, sha in substate.iteritems():
        # Ignore non-Git repositories keeping state in .hgsubstate.
        if path in sub and not sub[path].startswith(""[git]""):
            continue
        d = os.path.dirname(path)
        dirty_trees.add(d)
        tree = self._dirs.setdefault(d, dulobjs.Tree())
        tree.add(os.path.basename(path), dulobjs.S_IFGITLINK, sha)
","if path in sub and not sub [ path ] . startswith ( ""[git]"" ) :",197
"def get_property_file_image_choices(self, pipeline):
    columns = pipeline.get_measurement_columns()
    image_names = []
    for column in columns:
        object_name, feature, coltype = column[:3]
        choice = feature[(len(C_FILE_NAME) + 1) :]
        if object_name == ""Image"" and (feature.startswith(C_FILE_NAME)):
            image_names.append(choice)
    return image_names
","if object_name == ""Image"" and ( feature . startswith ( C_FILE_NAME ) ) :",118
"def check_all_decorator_order():
    """"""Check that in all test files, the slow decorator is always last.""""""
    errors = []
    for fname in os.listdir(PATH_TO_TESTS):
        if fname.endswith("".py""):
            filename = os.path.join(PATH_TO_TESTS, fname)
            new_errors = check_decorator_order(filename)
            errors += [f""- {filename}, line {i}"" for i in new_errors]
    if len(errors) > 0:
        msg = ""\n"".join(errors)
        raise ValueError(
            f""The parameterized decorator (and its variants) should always be first, but this is not the case in the following files:\n{msg}""
        )
","if fname . endswith ( "".py"" ) :",182
"def on_edit_button_clicked(self, event=None, a=None, col=None):
    tree, tree_id = self.treeView.get_selection().get_selected()
    watchdir_id = str(self.store.get_value(tree_id, 0))
    if watchdir_id:
        if col and col.get_title() == _(""Active""):
            if self.watchdirs[watchdir_id][""enabled""]:
                client.autoadd.disable_watchdir(watchdir_id)
            else:
                client.autoadd.enable_watchdir(watchdir_id)
        else:
            self.opts_dialog.show(self.watchdirs[watchdir_id], watchdir_id)
","if self . watchdirs [ watchdir_id ] [ ""enabled"" ] :",187
"def get_conv_output_size(input_size, kernel_size, stride, padding, dilation):
    ndim = len(input_size)
    output_size = []
    for i in range(ndim):
        size = (
            input_size[i] + 2 * padding[i] - dilation[i] * (kernel_size[i] - 1) - 1
        ) // stride[i] + 1
        if kernel_size[i] == -1:
            output_size.append(1)
        else:
            output_size.append(size)
    return output_size
",if kernel_size [ i ] == - 1 :,151
"def from_location(cls, location, basename, metadata=None, **kw):
    project_name, version, py_version, platform = [None] * 4
    basename, ext = os.path.splitext(basename)
    if ext.lower() in ("".egg"", "".egg-info""):
        match = EGG_NAME(basename)
        if match:
            project_name, version, py_version, platform = match.group(
                ""name"", ""ver"", ""pyver"", ""plat""
            )
    return cls(
        location,
        metadata,
        project_name=project_name,
        version=version,
        py_version=py_version,
        platform=platform,
        **kw
    )
",if match :,187
"def __new__(metacls, typename, bases, namespace):
    annotations = namespace.get(""__annotations__"", {})
    for t in annotations.values():
        if getattr(t, ""__origin__"", """") is Union:
            for ut in t.__args__:
                _assert_tensorizer_type(ut)
        else:
            _assert_tensorizer_type(t)
    return super().__new__(metacls, typename, bases, namespace)
","if getattr ( t , ""__origin__"" , """" ) is Union :",110
"def decode_content(self):
    """"""Return the best possible representation of the response body.""""""
    ct = self.headers.get(""content-type"")
    if ct:
        ct, options = parse_options_header(ct)
        charset = options.get(""charset"")
        if ct in JSON_CONTENT_TYPES:
            return self.json(charset)
        elif ct.startswith(""text/""):
            return self.text(charset)
        elif ct == FORM_URL_ENCODED:
            return parse_qsl(self.content.decode(charset), keep_blank_values=True)
    return self.content
",if ct in JSON_CONTENT_TYPES :,156
"def get_full_path(path):
    if ""://"" not in path:
        path = os.path.join(self.AUTO_COLL_TEMPL, path, """")
        if abs_path:
            path = os.path.join(abs_path, path)
    return path
",if abs_path :,71
"def __getitem__(self, name_or_path):
    if isinstance(name_or_path, integer_types):
        return list.__getitem__(self, name_or_path)
    elif isinstance(name_or_path, tuple):
        try:
            val = self
            for fid in name_or_path:
                if not isinstance(val, FeatStruct):
                    raise KeyError  # path contains base value
                val = val[fid]
            return val
        except (KeyError, IndexError):
            raise KeyError(name_or_path)
    else:
        raise TypeError(self._INDEX_ERROR % name_or_path)
","if not isinstance ( val , FeatStruct ) :",170
"def scan(scope):
    for s in scope.children:
        if s.start_pos <= position <= s.end_pos:
            if isinstance(s, (tree.Scope, tree.Flow)):
                return scan(s) or s
            elif s.type in (""suite"", ""decorated""):
                return scan(s)
    return None
","elif s . type in ( ""suite"" , ""decorated"" ) :",92
"def _get_key(self):
    if not self.key:
        self._channel.send(u""pake"", self.msg1)
        pake_msg = self._channel.get(u""pake"")
        self.key = self.sp.finish(pake_msg)
        self.verifier = self.derive_key(u""wormhole:verifier"")
        if not self._send_confirm:
            return
        confkey = self.derive_key(u""wormhole:confirmation"")
        nonce = os.urandom(CONFMSG_NONCE_LENGTH)
        confmsg = make_confmsg(confkey, nonce)
        self._channel.send(u""_confirm"", confmsg)
",if not self . _send_confirm :,181
"def executeScript(self, script):
    if len(script) > 0:
        commands = []
        for l in script:
            extracted = self.extract_command(l)
            if extracted:
                commands.append(extracted)
        for command in commands:
            cmd, argv = command
            self.dispatch_command(cmd, argv)
",if extracted :,98
"def create_path(n, fullname, meta):
    if meta:
        meta.create_path(fullname)
    else:
        # These fallbacks are important -- meta could be null if, for
        # example, save created a ""fake"" item, i.e. a new strip/graft
        # path element, etc.  You can find cases like that by
        # searching for ""Metadata()"".
        unlink(fullname)
        if stat.S_ISDIR(n.mode):
            mkdirp(fullname)
        elif stat.S_ISLNK(n.mode):
            os.symlink(n.readlink(), fullname)
",elif stat . S_ISLNK ( n . mode ) :,158
"def get_cycle(self):
    if self.has_cycle():
        cross_node = self.path[-1]
        if self.path.count(cross_node) > 1:
            return self.path[self.path.index(cross_node) :]
        else:
            return self.path
    return []
",if self . path . count ( cross_node ) > 1 :,84
"def _select_block(str_in, start_tag, end_tag):
    """"""Select first block delimited by start_tag and end_tag""""""
    start_pos = str_in.find(start_tag)
    if start_pos < 0:
        raise ValueError(""start_tag not found"")
    depth = 0
    for pos in range(start_pos, len(str_in)):
        if str_in[pos] == start_tag:
            depth += 1
        elif str_in[pos] == end_tag:
            depth -= 1
        if depth == 0:
            break
    sel = str_in[start_pos + 1 : pos]
    return sel
",if str_in [ pos ] == start_tag :,171
"def device(self):
    """"""Device on which the data array of this variable reside.""""""
    # lazy initialization for performance
    if self._device is None:
        if self._data[0] is None:
            self._device = backend.CpuDevice()
        else:
            self._device = backend.get_device_from_array(self._data[0])
    return self._device
",if self . _data [ 0 ] is None :,98
"def function_out(*args, **kwargs):
    try:
        return function_in(*args, **kwargs)
    except dbus.exceptions.DBusException as e:
        if e.get_dbus_name() == DBUS_UNKNOWN_METHOD:
            raise ItemNotFoundException(""Item does not exist!"")
        if e.get_dbus_name() == DBUS_NO_SUCH_OBJECT:
            raise ItemNotFoundException(e.get_dbus_message())
        if e.get_dbus_name() in (DBUS_NO_REPLY, DBUS_NOT_SUPPORTED):
            raise SecretServiceNotAvailableException(e.get_dbus_message())
        raise
",if e . get_dbus_name ( ) == DBUS_NO_SUCH_OBJECT :,162
"def run(self):
    """"""Continual loop evaluating when_statements""""""
    while len(self.library) > 0:
        for name, expression in self.library.items():
            if expression.remove_me == True:
                del self.library[name]
            else:
                expression.evaluate()
        sleep(0.01)
    return
",if expression . remove_me == True :,96
"def tamper(payload, **kwargs):
    junk_chars = ""!#$%&()*~+-_.,:;?@[/|\]^`""
    retval = """"
    for i, char in enumerate(payload, start=1):
        amount = random.randint(10, 15)
        if char == "">"":
            retval += "">""
            for _ in range(amount):
                retval += random.choice(junk_chars)
        elif char == ""<"":
            retval += ""<""
            for _ in range(amount):
                retval += random.choice(junk_chars)
        elif char == "" "":
            for _ in range(amount):
                retval += random.choice(junk_chars)
        else:
            retval += char
    return retval
","if char == "">"" :",200
"def _source_target_path(source, source_path, source_location):
    target_path_attr = source.target_path or source.resdef.target_path
    if source.preserve_path:
        if target_path_attr:
            log.warning(
                ""target-path '%s' specified with preserve-path - ignoring"",
                target_path_attr,
            )
        return os.path.relpath(os.path.dirname(source_path), source_location)
    else:
        return target_path_attr or source.resdef.target_path or """"
",if target_path_attr :,152
"def _load_user_from_header(self, header):
    if self._header_callback:
        user = self._header_callback(header)
        if user is not None:
            app = current_app._get_current_object()
            user_loaded_from_header.send(app, user=user)
            return user
    return None
",if user is not None :,92
"def setup(cls):
    ""Check dependencies and warn about firewalling""
    pathCheck(""brctl"", moduleName=""bridge-utils"")
    # Disable Linux bridge firewalling so that traffic can flow!
    for table in ""arp"", ""ip"", ""ip6"":
        cmd = ""sysctl net.bridge.bridge-nf-call-%stables"" % table
        out = quietRun(cmd).strip()
        if out.endswith(""1""):
            warn(""Warning: Linux bridge may not work with"", out, ""\n"")
","if out . endswith ( ""1"" ) :",128
"def _browse_your_music(web_client, variant):
    if not web_client.logged_in:
        return []
    if variant in (""tracks"", ""albums""):
        items = flatten(
            [
                page.get(""items"", [])
                for page in web_client.get_all(
                    f""me/{variant}"",
                    params={""market"": ""from_token"", ""limit"": 50},
                )
                if page
            ]
        )
        if variant == ""tracks"":
            return list(translator.web_to_track_refs(items))
        else:
            return list(translator.web_to_album_refs(items))
    else:
        return []
","if variant == ""tracks"" :",198
"def reset_styling(self):
    for edge in self.fsm_graph.edges_iter():
        style_attr = self.fsm_graph.style_attributes.get(""edge"", {}).get(""default"")
        edge.attr.update(style_attr)
    for node in self.fsm_graph.nodes_iter():
        if ""point"" not in node.attr[""shape""]:
            style_attr = self.fsm_graph.style_attributes.get(""node"", {}).get(""inactive"")
            node.attr.update(style_attr)
    for sub_graph in self.fsm_graph.subgraphs_iter():
        style_attr = self.fsm_graph.style_attributes.get(""graph"", {}).get(""default"")
        sub_graph.graph_attr.update(style_attr)
","if ""point"" not in node . attr [ ""shape"" ] :",200
"def set_message_type_visibility(self, message_type: MessageType):
    try:
        rows = {
            i
            for i, msg in enumerate(self.proto_analyzer.messages)
            if msg.message_type == message_type
        }
        if message_type.show:
            self.ui.tblViewProtocol.show_rows(rows)
        else:
            self.ui.tblViewProtocol.hide_rows(rows)
    except Exception as e:
        logger.exception(e)
",if msg . message_type == message_type,138
"def POP(cpu, *regs):
    for reg in regs:
        val = cpu.stack_pop(cpu.address_bit_size // 8)
        if reg.reg in (""PC"", ""R15""):
            cpu._set_mode_by_val(val)
            val = val & ~0x1
        reg.write(val)
","if reg . reg in ( ""PC"" , ""R15"" ) :",89
"def processMovie(self, atom):
    for field in atom:
        if ""track"" in field:
            self.processTrack(field[""track""])
        if ""movie_hdr"" in field:
            self.processMovieHeader(field[""movie_hdr""])
","if ""movie_hdr"" in field :",69
"def check_update_function(url, folder, update_setter, version_setter, auto):
    remote_version = urllib.urlopen(url).read()
    if remote_version.isdigit():
        local_version = get_local_timestamp(folder)
        if remote_version > local_version:
            if auto:
                update_setter.set_value(True)
            version_setter.set_value(remote_version)
            return True
        else:
            return False
    else:
        return False
",if remote_version > local_version :,136
"def init(self, view, items=None):
    selections = []
    if view.sel():
        for region in view.sel():
            selections.append(view.substr(region))
    values = []
    for idx, index in enumerate(map(int, items)):
        if idx >= len(selections):
            break
        i = index - 1
        if i >= 0 and i < len(selections):
            values.append(selections[i])
        else:
            values.append(None)
    # fill up
    for idx, value in enumerate(selections):
        if len(values) + 1 < idx:
            values.append(value)
    self.stack = values
",if i >= 0 and i < len ( selections ) :,178
"def find_int_identifiers(directory):
    results = find_rules(directory, has_int_identifier)
    print(""Number of rules with integer identifiers: %d"" % len(results))
    for result in results:
        rule_path = result[0]
        product_yaml_path = result[1]
        product_yaml = None
        if product_yaml_path is not None:
            product_yaml = yaml.open_raw(product_yaml_path)
        fix_file(rule_path, product_yaml, fix_int_identifier)
",if product_yaml_path is not None :,138
"def condition(self):
    if self.__condition is None:
        if len(self.flat_conditions) == 1:
            # Avoid an extra indirection in the common case of only one condition.
            self.__condition = self.flat_conditions[0]
        elif len(self.flat_conditions) == 0:
            # Possible, if unlikely, due to filter predicate rewriting
            self.__condition = lambda _: True
        else:
            self.__condition = lambda x: all(cond(x) for cond in self.flat_conditions)
    return self.__condition
",elif len ( self . flat_conditions ) == 0 :,143
"def get_scene_exceptions_by_season(self, season=-1):
    scene_exceptions = []
    for scene_exception in self.scene_exceptions:
        if not len(scene_exception) == 2:
            continue
        scene_name, scene_season = scene_exception.split(""|"")
        if season == scene_season:
            scene_exceptions.append(scene_name)
    return scene_exceptions
",if not len ( scene_exception ) == 2 :,121
"def init(self, view, items=None):
    selections = []
    if view.sel():
        for region in view.sel():
            selections.append(view.substr(region))
    values = []
    for idx, index in enumerate(map(int, items)):
        if idx >= len(selections):
            break
        i = index - 1
        if i >= 0 and i < len(selections):
            values.append(selections[i])
        else:
            values.append(None)
    # fill up
    for idx, value in enumerate(selections):
        if len(values) + 1 < idx:
            values.append(value)
    self.stack = values
",if idx >= len ( selections ) :,178
"def to_tool_path(self, path_or_uri_like, **kwds):
    if ""://"" not in path_or_uri_like:
        path = path_or_uri_like
    else:
        uri_like = path_or_uri_like
        if "":"" not in path_or_uri_like:
            raise Exception(""Invalid URI passed to get_tool_source"")
        scheme, rest = uri_like.split("":"", 2)
        if scheme not in self.resolver_classes:
            raise Exception(
                ""Unknown tool scheme [{}] for URI [{}]"".format(scheme, uri_like)
            )
        path = self.resolver_classes[scheme]().get_tool_source_path(uri_like)
    return path
","if "":"" not in path_or_uri_like :",188
"def mainWindow():
    global MW
    if not MW:
        for i in qApp.topLevelWidgets():
            if i.objectName() == ""MainWindow"":
                MW = i
                return MW
        return None
    else:
        return MW
","if i . objectName ( ) == ""MainWindow"" :",78
"def async_get_service(hass, config, discovery_info=None):
    # pylint: disable=unused-argument
    """"""Get the demo notification service.""""""
    for account, account_dict in hass.data[DATA_ALEXAMEDIA][""accounts""].items():
        for key, _ in account_dict[""devices""][""media_player""].items():
            if key not in account_dict[""entities""][""media_player""]:
                _LOGGER.debug(
                    ""%s: Media player %s not loaded yet; delaying load"",
                    hide_email(account),
                    hide_serial(key),
                )
                return False
    return AlexaNotificationService(hass)
","if key not in account_dict [ ""entities"" ] [ ""media_player"" ] :",182
"def _migrate_bool(self, name: str, true_value: str, false_value: str) -> None:
    if name not in self._settings:
        return
    values = self._settings[name]
    if not isinstance(values, dict):
        return
    for scope, val in values.items():
        if isinstance(val, bool):
            new_value = true_value if val else false_value
            self._settings[name][scope] = new_value
            self.changed.emit()
","if isinstance ( val , bool ) :",130
"def send(self, data, flags=0):
    self._checkClosed()
    if self._sslobj:
        if flags != 0:
            raise ValueError(
                ""non-zero flags not allowed in calls to send() on %s"" % self.__class__
            )
        return self._sslobj.write(data)
    else:
        return socket.send(self, data, flags)
",if flags != 0 :,106
"def rec_deps(services, container_by_name, cnt, init_service):
    deps = cnt[""_deps""]
    for dep in deps.copy():
        dep_cnts = services.get(dep)
        if not dep_cnts:
            continue
        dep_cnt = container_by_name.get(dep_cnts[0])
        if dep_cnt:
            # TODO: avoid creating loops, A->B->A
            if init_service and init_service in dep_cnt[""_deps""]:
                continue
            new_deps = rec_deps(services, container_by_name, dep_cnt, init_service)
            deps.update(new_deps)
    return deps
",if dep_cnt :,181
"def as_dict(path="""", version=""latest"", section=""meta-data""):
    result = {}
    dirs = dir(path, version, section)
    if not dirs:
        return None
    for item in dirs:
        if item.endswith(""/""):
            records = as_dict(path + item, version, section)
            if records:
                result[item[:-1]] = records
        elif is_dict.match(item):
            idx, name = is_dict.match(item).groups()
            records = as_dict(path + idx + ""/"", version, section)
            if records:
                result[name] = records
        else:
            result[item] = valueconv(get(path + item, version, section))
    return result
",elif is_dict . match ( item ) :,197
"def PrintColGroup(col_names, schema):
    """"""Print HTML colgroup element, used for JavaScript sorting.""""""
    print(""  <colgroup>"")
    for i, col in enumerate(col_names):
        if col.endswith(""_HREF""):
            continue
        # CSS class is used for sorting
        if schema.IsNumeric(col):
            css_class = ""number""
        else:
            css_class = ""case-insensitive""
        # NOTE: id is a comment only; not used
        print('    <col id=""{}"" type=""{}"" />'.format(col, css_class))
    print(""  </colgroup>"")
",if schema . IsNumeric ( col ) :,161
"def check_region(self, region):
    for other in self.regions:
        if other is region:
            continue
        if (other.start < region.start < other.end) or (
            other.start < region.end < other.end
        ):
            raise Exception(""%r overlaps with %r"" % (region, other))
",if other is region :,89
"def _write_value(self, rng, value, scalar):
    if rng.api and value:
        # it is assumed by this stage that value is a list of lists
        if scalar:
            value = value[0][0]
        else:
            rng = rng.resize(len(value), len(value[0]))
        rng.raw_value = value
",if scalar :,94
"def TryMerge(self, d):
    while d.avail() > 0:
        tt = d.getVarInt32()
        if tt == 10:
            length = d.getVarInt32()
            tmp = ProtocolBuffer.Decoder(d.buffer(), d.pos(), d.pos() + length)
            d.skip(length)
            self.mutable_cost().TryMerge(tmp)
            continue
        if tt == 24:
            self.add_version(d.getVarInt64())
            continue
        if tt == 0:
            raise ProtocolBuffer.ProtocolBufferDecodeError
        d.skipData(tt)
",if tt == 0 :,167
"def generate_sv_faces(dcel_mesh, point_index, only_select=False, del_flag=None):
    # This part of function creates faces in SV format
    # It ignores  boundless super face
    sv_faces = []
    for i, face in enumerate(dcel_mesh.faces):
        if face.inners and face.outer:
            ""Face ({}) has inner components! Sverchok cant show polygons with holes."".format(
                i
            )
        if not face.outer or del_flag in face.flags:
            continue
        if only_select and not face.select:
            continue
        sv_faces.append([point_index[hedge.origin] for hedge in face.outer.loop_hedges])
    return sv_faces
",if face . inners and face . outer :,198
"def _get_x_for_y(self, xValue, x, y):
    # print(""searching ""+x+"" with the value ""+str(xValue)+"" and want to give back ""+y)
    x_value = str(xValue)
    for anime in self.xmlMap.findall(""anime""):
        try:
            if anime.get(x, False) == x_value:
                return int(anime.get(y, 0))
        except ValueError as e:
            continue
    return 0
","if anime . get ( x , False ) == x_value :",131
"def dir_copy(src_dir, dest_dir, merge_if_exists=True):
    try:
        if not os.path.exists(dest_dir):
            shutil.copytree(src_dir, dest_dir)
        elif merge_if_exists:
            merge_dir(src_dir, dest_dir)
    except OSError as e:
        # If source is not a directory, copy with shutil.copy
        if e.errno == errno.ENOTDIR:
            shutil.copy(src_dir, dest_dir)
        else:
            logging.error(""Could not copy %s to %s"", src_dir, dest_dir)
",elif merge_if_exists :,166
"def mapping(self):
    m = {}
    if getGdriveCredentialsFile() is not None:
        m[""gdrive""] = """"
    unknown = 0
    for f in self.scan:
        bits = f.split(""#"", 2)
        if len(bits) == 1:
            label = os.path.basename(f)
        else:
            label = bits[1]
        if not label or len(label) == 0 or label == """":
            label = ""L"" + str(unknown)
            unknown += 1
        m[label] = bits[0]
    return m
","if not label or len ( label ) == 0 or label == """" :",153
"def get_tag_values(self, event):
    http = event.interfaces.get(""sentry.interfaces.Http"")
    if not http:
        return []
    if not http.headers:
        return []
    headers = http.headers
    # XXX: transitional support for workers
    if isinstance(headers, dict):
        headers = headers.items()
    output = []
    for key, value in headers:
        if key != ""User-Agent"":
            continue
        ua = Parse(value)
        if not ua:
            continue
        result = self.get_tag_from_ua(ua)
        if result:
            output.append(result)
    return output
",if not ua :,176
"def __iter__(self):
    it = DiskHashMerger.__iter__(self)
    direct_upstreams = self.direct_upstreams
    for k, groups in it:
        t = list([[] for _ in range(self.size)])
        for i, g in enumerate(groups):
            if g:
                if i in direct_upstreams:
                    t[i] = g
                else:
                    g.sort(key=itemgetter(0))
                    g1 = []
                    for _, vs in g:
                        g1.extend(vs)
                    t[i] = g1
        yield k, tuple(t)
",if i in direct_upstreams :,185
"def process_question(qtxt):
    question = """"
    skip = False
    for letter in qtxt:
        if letter == ""<"":
            skip = True
        if letter == "">"":
            skip = False
        if skip:
            continue
        if letter.isalnum() or letter == "" "":
            if letter == "" "":
                letter = ""_""
            question += letter.lower()
    return question
","if letter == ""<"" :",110
"def _module_repr_from_spec(spec):
    """"""Return the repr to use for the module.""""""
    # We mostly replicate _module_repr() using the spec attributes.
    name = ""?"" if spec.name is None else spec.name
    if spec.origin is None:
        if spec.loader is None:
            return ""<module {!r}>"".format(name)
        else:
            return ""<module {!r} ({!r})>"".format(name, spec.loader)
    else:
        if spec.has_location:
            return ""<module {!r} from {!r}>"".format(name, spec.origin)
        else:
            return ""<module {!r} ({})>"".format(spec.name, spec.origin)
",if spec . has_location :,180
"def test_row(self, row):
    for idx, test in self.patterns.items():
        try:
            value = row[idx]
        except IndexError:
            value = """"
        result = test(value)
        if self.any_match:
            if result:
                return not self.inverse  # True
        else:
            if not result:
                return self.inverse  # False
    if self.any_match:
        return self.inverse  # False
    else:
        return not self.inverse  # True
",if not result :,149
"def frequent_thread_switches():
    """"""Make concurrency bugs more likely to manifest.""""""
    interval = None
    if not sys.platform.startswith(""java""):
        if hasattr(sys, ""getswitchinterval""):
            interval = sys.getswitchinterval()
            sys.setswitchinterval(1e-6)
        else:
            interval = sys.getcheckinterval()
            sys.setcheckinterval(1)
    try:
        yield
    finally:
        if not sys.platform.startswith(""java""):
            if hasattr(sys, ""setswitchinterval""):
                sys.setswitchinterval(interval)
            else:
                sys.setcheckinterval(interval)
","if hasattr ( sys , ""getswitchinterval"" ) :",177
"def record_expected_exportable_production(self, ticks):
    """"""Record the amount of production that should be transferred to other islands.""""""
    for (quota_holder, resource_id), amount in self._low_priority_requests.items():
        if quota_holder not in self._settlement_manager_id:
            self._settlement_manager_id[quota_holder] = WorldObject.get_object_by_id(
                int(quota_holder[1:].split("","")[0])
            ).settlement_manager.worldid
        self.trade_storage[self._settlement_manager_id[quota_holder]][resource_id] += (
            ticks * amount
        )
",if quota_holder not in self . _settlement_manager_id :,168
"def _method_events_callback(self, values):
    try:
        previous_echoed = (
            values[""child_result_list""][-1].decode().split(""\n"")[-2].strip()
        )
        if previous_echoed.endswith(""foo1""):
            return ""echo foo2\n""
        elif previous_echoed.endswith(""foo2""):
            return ""echo foo3\n""
        elif previous_echoed.endswith(""foo3""):
            return ""exit\n""
        else:
            raise Exception(""Unexpected output {0!r}"".format(previous_echoed))
    except IndexError:
        return ""echo foo1\n""
","if previous_echoed . endswith ( ""foo1"" ) :",172
"def describe_cluster_snapshots(self, cluster_identifier=None, snapshot_identifier=None):
    if cluster_identifier:
        cluster_snapshots = []
        for snapshot in self.snapshots.values():
            if snapshot.cluster.cluster_identifier == cluster_identifier:
                cluster_snapshots.append(snapshot)
        if cluster_snapshots:
            return cluster_snapshots
    if snapshot_identifier:
        if snapshot_identifier in self.snapshots:
            return [self.snapshots[snapshot_identifier]]
        raise ClusterSnapshotNotFoundError(snapshot_identifier)
    return self.snapshots.values()
",if snapshot . cluster . cluster_identifier == cluster_identifier :,149
"def get_snippet_edit_handler(model):
    if model not in SNIPPET_EDIT_HANDLERS:
        if hasattr(model, ""edit_handler""):
            # use the edit handler specified on the page class
            edit_handler = model.edit_handler
        else:
            panels = extract_panel_definitions_from_model_class(model)
            edit_handler = ObjectList(panels)
        SNIPPET_EDIT_HANDLERS[model] = edit_handler.bind_to(model=model)
    return SNIPPET_EDIT_HANDLERS[model]
","if hasattr ( model , ""edit_handler"" ) :",137
"def start():
    if os.environ.get(""RUN_MAIN"") != ""true"":
        try:
            exit_code = restart_with_reloader()
            if exit_code < 0:
                os.kill(os.getpid(), -exit_code)
            else:
                sys.exit(exit_code)
        except KeyboardInterrupt:
            pass
",if exit_code < 0 :,100
"def discover(self, *objlist):
    ret = []
    for l in self.splitlines():
        if len(l) < 5:
            continue
        if l[0] == ""Filename"":
            continue
        try:
            int(l[2])
            int(l[3])
        except:
            continue
        #           ret.append(improve(l[0]))
        ret.append(l[0])
    ret.sort()
    for item in objlist:
        ret.append(item)
    return ret
","if l [ 0 ] == ""Filename"" :",154
"def ipfs_publish(self, lib):
    with tempfile.NamedTemporaryFile() as tmp:
        self.ipfs_added_albums(lib, tmp.name)
        try:
            if self.config[""nocopy""]:
                cmd = ""ipfs add --nocopy -q "".split()
            else:
                cmd = ""ipfs add -q "".split()
            cmd.append(tmp.name)
            output = util.command_output(cmd)
        except (OSError, subprocess.CalledProcessError) as err:
            msg = ""Failed to publish library. Error: {0}"".format(err)
            self._log.error(msg)
            return False
        self._log.info(""hash of library: {0}"", output)
","if self . config [ ""nocopy"" ] :",188
"def spends(self):
    # Return spends indexed by hashX
    spends = defaultdict(list)
    utxos = self.mempool_utxos()
    for tx_hash, tx in self.txs.items():
        for n, input in enumerate(tx.inputs):
            if input.is_generation():
                continue
            prevout = (input.prev_hash, input.prev_idx)
            if prevout in utxos:
                hashX, value = utxos.pop(prevout)
            else:
                hashX, value = self.db_utxos[prevout]
            spends[hashX].append(prevout)
    return spends
",if prevout in utxos :,188
"def terminate(self):
    if self.returncode is None:
        try:
            os.kill(self.pid, TERM_SIGNAL)
        except OSError as exc:
            if getattr(exc, ""errno"", None) != errno.ESRCH:
                if self.wait(timeout=0.1) is None:
                    raise
",if self . wait ( timeout = 0.1 ) is None :,92
"def _getVolumeScalar(self):
    if self._volumeScalar is not None:
        return self._volumeScalar
    # use default
    elif self._value in dynamicStrToScalar:
        return dynamicStrToScalar[self._value]
    else:
        thisDynamic = self._value
        # ignore leading s like in sf
        if ""s"" in thisDynamic:
            thisDynamic = thisDynamic[1:]
        # ignore closing z like in fz
        if thisDynamic[-1] == ""z"":
            thisDynamic = thisDynamic[:-1]
        if thisDynamic in dynamicStrToScalar:
            return dynamicStrToScalar[thisDynamic]
        else:
            return dynamicStrToScalar[None]
","if ""s"" in thisDynamic :",183
"def init_values(self):
    config = self._raw_config
    for valname, value in self.overrides.iteritems():
        if ""."" in valname:
            realvalname, key = valname.split(""."", 1)
            config.setdefault(realvalname, {})[key] = value
        else:
            config[valname] = value
    for name in config:
        if name in self.values:
            self.__dict__[name] = config[name]
    del self._raw_config
",if name in self . values :,131
"def modified(self):
    paths = set()
    dictionary_list = []
    for op_list in self._operations:
        if not isinstance(op_list, list):
            op_list = (op_list,)
        for item in chain(*op_list):
            if item is None:
                continue
            dictionary = item.dictionary
            if dictionary.path in paths:
                continue
            paths.add(dictionary.path)
            dictionary_list.append(dictionary)
    return dictionary_list
",if dictionary . path in paths :,139
"def __getitem__(self, key, _get_mode=False):
    if not _get_mode:
        if isinstance(key, (int, long)):
            return self._list[key]
        elif isinstance(key, slice):
            return self.__class__(self._list[key])
    ikey = key.lower()
    for k, v in self._list:
        if k.lower() == ikey:
            return v
    # micro optimization: if we are in get mode we will catch that
    # exception one stack level down so we can raise a standard
    # key error instead of our special one.
    if _get_mode:
        raise KeyError()
    raise BadRequestKeyError(key)
","if isinstance ( key , ( int , long ) ) :",176
"def _get_items(self, name, target=1):
    all_items = self.get_items(name)
    items = [o for o in all_items if not o.disabled]
    if len(items) < target:
        if len(all_items) < target:
            raise ItemNotFoundError(""insufficient items with name %r"" % name)
        else:
            raise AttributeError(""insufficient non-disabled items with name %s"" % name)
    on = []
    off = []
    for o in items:
        if o.selected:
            on.append(o)
        else:
            off.append(o)
    return on, off
",if len ( all_items ) < target :,169
"def get_genome_dir(gid, galaxy_dir, data):
    """"""Return standard location of genome directories.""""""
    if galaxy_dir:
        refs = genome.get_refs(gid, None, galaxy_dir, data)
        seq_file = tz.get_in([""fasta"", ""base""], refs)
        if seq_file and os.path.exists(seq_file):
            return os.path.dirname(os.path.dirname(seq_file))
    else:
        gdirs = glob.glob(os.path.join(_get_data_dir(), ""genomes"", ""*"", gid))
        if len(gdirs) == 1 and os.path.exists(gdirs[0]):
            return gdirs[0]
",if seq_file and os . path . exists ( seq_file ) :,190
"def _PrintFuncs(self, names):
    # type: (List[str]) -> int
    status = 0
    for name in names:
        if name in self.funcs:
            print(name)
            # TODO: Could print LST for -f, or render LST.  Bash does this.  'trap'
            # could use that too.
        else:
            status = 1
    return status
",if name in self . funcs :,110
"def package_files(self):
    seen_package_directories = ()
    directories = self.distribution.package_dir or {}
    empty_directory_exists = """" in directories
    packages = self.distribution.packages or []
    for package in packages:
        if package in directories:
            package_directory = directories[package]
        elif empty_directory_exists:
            package_directory = os.path.join(directories[""""], package)
        else:
            package_directory = package
        if not package_directory.startswith(seen_package_directories):
            seen_package_directories += (package_directory + ""."",)
            yield package_directory
",if not package_directory . startswith ( seen_package_directories ) :,164
"def apply_conf_file(fn, conf_filename):
    for env in LSF_CONF_ENV:
        conf_file = get_conf_file(conf_filename, env)
        if conf_file:
            with open(conf_file) as conf_handle:
                value = fn(conf_handle)
            if value:
                return value
    return None
",if conf_file :,101
"def on_text(self, text):
    if text != self.chosen_text:
        self.fail_test('Expected ""{}"", received ""{}""'.format(self.chosen_text, text))
    else:
        self.checks_passed += 1
        if self.checks_passed >= self.number_of_checks:
            self.pass_test()
        else:
            self._select_next_text()
",if self . checks_passed >= self . number_of_checks :,103
"def test_field_attr_existence(self):
    for name, item in ast.__dict__.items():
        if self._is_ast_node(name, item):
            if name == ""Index"":
                # Index(value) just returns value now.
                # The argument is required.
                continue
            x = item()
            if isinstance(x, ast.AST):
                self.assertEqual(type(x._fields), tuple)
","if name == ""Index"" :",122
"def apply(self, response):
    updated_headers = self.update_headers(response)
    if updated_headers:
        response.headers.update(updated_headers)
        warning_header_value = self.warning(response)
        if warning_header_value is not None:
            response.headers.update({""Warning"": warning_header_value})
    return response
",if warning_header_value is not None :,92
"def validate(self):
    self.assertEqual(len(self.inputs), len(self.outputs))
    for batch_in, batch_out in zip(self.inputs, self.outputs):
        self.assertEqual(len(batch_in), len(batch_out))
        if self.use_parallel_executor and not self.use_double_buffer:
            self.validate_unordered_batch(batch_in, batch_out)
        else:
            for in_data, out_data in zip(batch_in, batch_out):
                self.assertEqual(in_data.shape, out_data.shape)
                if not self.use_parallel_executor:
                    self.assertTrue((in_data == out_data).all())
",if not self . use_parallel_executor :,189
"def finalize(self):
    if self._started:
        if not self._finalized:
            self._queue.put(None)
            self._queue.join()
            self._consumer.join()
        self._started = False
    self._finalized = True
",if not self . _finalized :,70
"def _get_ilo_version(self):
    try:
        self._get_ilo2('<?xml version=""1.0""?><RIBCL VERSION=""2.0""></RIBCL>')
    except ResponseError as e:
        if hasattr(e, ""code""):
            if e.code == 405:
                return 3
            if e.code == 501:
                return 1
        raise
    return 2
","if hasattr ( e , ""code"" ) :",113
"def _check_data(self, source, expected_bytes, expected_duration):
    received_bytes = 0
    received_seconds = 0.0
    bytes_to_read = 1024
    while True:
        data = source.get_audio_data(bytes_to_read)
        if data is None:
            break
        received_bytes += data.length
        received_seconds += data.duration
        self.assertEqual(data.length, len(data.data))
    self.assertAlmostEqual(expected_duration, received_seconds, places=1)
    self.assertAlmostEqual(expected_bytes, received_bytes, delta=5)
",if data is None :,154
"def __randomize_interval_task(self):
    for job in self.aps_scheduler.get_jobs():
        if isinstance(job.trigger, IntervalTrigger):
            self.aps_scheduler.modify_job(
                job.id,
                next_run_time=datetime.now()
                + timedelta(
                    seconds=randrange(
                        job.trigger.interval.total_seconds() * 0.75,
                        job.trigger.interval.total_seconds(),
                    )
                ),
            )
","if isinstance ( job . trigger , IntervalTrigger ) :",153
"def find_approximant(x):
    c = 1e-4
    it = sympy.ntheory.continued_fraction_convergents(
        sympy.ntheory.continued_fraction_iterator(x)
    )
    for i in it:
        p, q = i.as_numer_denom()
        tol = c / q ** 2
        if abs(i - x) <= tol:
            return i
        if tol < machine_epsilon:
            break
    return x
",if abs ( i - x ) <= tol :,122
"def fix_newlines(lines):
    """"""Convert newlines to unix.""""""
    for i, line in enumerate(lines):
        if line.endswith(""\r\n""):
            lines[i] = line[:-2] + ""\n""
        elif line.endswith(""\r""):
            lines[i] = line[:-1] + ""\n""
","elif line . endswith ( ""\r"" ) :",83
"def payment_control_render(self, request: HttpRequest, payment: OrderPayment):
    template = get_template(""pretixplugins/paypal/control.html"")
    sale_id = None
    for trans in payment.info_data.get(""transactions"", []):
        for res in trans.get(""related_resources"", []):
            if ""sale"" in res and ""id"" in res[""sale""]:
                sale_id = res[""sale""][""id""]
    ctx = {
        ""request"": request,
        ""event"": self.event,
        ""settings"": self.settings,
        ""payment_info"": payment.info_data,
        ""order"": payment.order,
        ""sale_id"": sale_id,
    }
    return template.render(ctx)
","if ""sale"" in res and ""id"" in res [ ""sale"" ] :",192
"def for_name(self, name):
    try:
        name_resources = self._resources[name]
    except KeyError:
        raise LookupError(name)
    else:
        for res in name_resources:
            try:
                inst = res.inst()
            except Exception as e:
                if log.getEffectiveLevel() <= logging.DEBUG:
                    log.exception(""error initializing %s"", res)
                else:
                    log.error(""error initializing %s: %s"", res, e)
            else:
                yield inst
",if log . getEffectiveLevel ( ) <= logging . DEBUG :,154
"def describe(self, done=False):
    description = ShellCommand.describe(self, done)
    if done:
        if not description:
            description = [""compile""]
        description.append(""%d projects"" % self.getStatistic(""projects"", 0))
        description.append(""%d files"" % self.getStatistic(""files"", 0))
        warnings = self.getStatistic(""warnings"", 0)
        if warnings > 0:
            description.append(""%d warnings"" % warnings)
        errors = self.getStatistic(""errors"", 0)
        if errors > 0:
            description.append(""%d errors"" % errors)
    return description
",if not description :,164
"def parse_list(tl):
    ls = []
    nm = []
    while True:
        term, nmt, tl = parse_term(tl)
        ls.append(term)
        if nmt is not None:
            nm.append(nmt)
        if tl[0] != "","":
            break
        tl = tl[1:]
    return ls, nm, tl
",if nmt is not None :,101
"def infer_dataset_impl(path):
    if IndexedRawTextDataset.exists(path):
        return ""raw""
    elif IndexedDataset.exists(path):
        with open(index_file_path(path), ""rb"") as f:
            magic = f.read(8)
            if magic == IndexedDataset._HDR_MAGIC:
                return ""cached""
            elif magic == MMapIndexedDataset.Index._HDR_MAGIC[:8]:
                return ""mmap""
            else:
                return None
    elif FastaDataset.exists(path):
        return ""fasta""
    else:
        return None
",if magic == IndexedDataset . _HDR_MAGIC :,167
"def _get(self):
    fut = item = None
    with self._mutex:
        # Critical section never blocks.
        if not self._queue or self._getters:
            fut = Future()
            fut.add_done_callback(
                lambda f: self._get_complete() if not f.cancelled() else None
            )
            self._getters.append(fut)
        else:
            item = self._get_item()
            self._get_complete()
    return item, fut
",if not self . _queue or self . _getters :,142
"def validate(self):
    dates = []
    for d in self.get(""leave_block_list_dates""):
        # date is not repeated
        if d.block_date in dates:
            frappe.msgprint(
                _(""Date is repeated"") + "":"" + d.block_date, raise_exception=1
            )
        dates.append(d.block_date)
",if d . block_date in dates :,101
"def on_choose_watch_dir_clicked(self):
    if self.window().watchfolder_enabled_checkbox.isChecked():
        previous_watch_dir = self.window().watchfolder_location_input.text() or """"
        watch_dir = QFileDialog.getExistingDirectory(
            self.window(),
            ""Please select the watch folder"",
            previous_watch_dir,
            QFileDialog.ShowDirsOnly,
        )
        if not watch_dir:
            return
        self.window().watchfolder_location_input.setText(watch_dir)
",if not watch_dir :,147
"def log_generator(self, limit=6000, **kwargs):
    # Generator for show_log_panel
    skip = 0
    while True:
        logs = self.log(limit=limit, skip=skip, **kwargs)
        if not logs:
            break
        for entry in logs:
            yield entry
        if len(logs) < limit:
            break
        skip = skip + limit
",if len ( logs ) < limit :,106
"def _setUpClass(cls):
    global solver
    import pyomo.environ
    from pyomo.solvers.tests.io.writer_test_cases import testCases
    for test_case in testCases:
        if ((test_case.name, test_case.io) in solver) and (test_case.available):
            solver[(test_case.name, test_case.io)] = True
","if ( ( test_case . name , test_case . io ) in solver ) and ( test_case . available ) :",101
"def _get_file_data(self, normpath, normrev):
    data = self.client.cat(normpath, normrev)
    if has_expanded_svn_keywords(data):
        # Find out if this file has any keyword expansion set.
        # If it does, collapse these keywords. This is because SVN
        # will return the file expanded to us, which would break patching.
        keywords = self.client.propget(""svn:keywords"", normpath, normrev, recurse=True)
        if normpath in keywords:
            data = collapse_svn_keywords(data, force_bytes(keywords[normpath]))
    return data
",if normpath in keywords :,152
"def add_controller_list(path):
    if not os.path.exists(os.path.join(path, ""__init__.py"")):
        bb.fatal(""Controllers directory %s exists but is missing __init__.py"" % path)
    files = sorted(
        [f for f in os.listdir(path) if f.endswith("".py"") and not f.startswith(""_"")]
    )
    for f in files:
        module = ""oeqa.controllers."" + f[:-3]
        if module not in controllerslist:
            controllerslist.append(module)
        else:
            bb.warn(
                ""Duplicate controller module found for %s, only one added. Layers should create unique controller module names""
                % module
            )
",if module not in controllerslist :,197
"def on_session2(event):
    new_xmpp.get_roster()
    new_xmpp.send_presence()
    logging.info(roster[0])
    data = roster[0][""roster""][""items""]
    logging.info(data)
    for jid, item in data.items():
        if item[""subscription""] != ""none"":
            new_xmpp.send_presence(ptype=""subscribe"", pto=jid)
        new_xmpp.update_roster(jid, name=item[""name""], groups=item[""groups""])
    new_xmpp.disconnect()
","if item [ ""subscription"" ] != ""none"" :",145
"def _parse_class_simplified(symbol):
    results = {}
    name = symbol.name + ""(""
    name += "", "".join([analyzer.expand_attribute(base) for base in symbol.bases])
    name += "")""
    for sym in symbol.body:
        if isinstance(sym, ast.FunctionDef):
            result = _parse_function_simplified(sym, symbol.name)
            results.update(result)
        elif isinstance(sym, ast.ClassDef):
            result = _parse_class_simplified(sym)
            results.update(result)
    lineno = symbol.lineno
    for decorator in symbol.decorator_list:
        lineno += 1
    results[lineno] = (name, ""c"")
    return results
","if isinstance ( sym , ast . FunctionDef ) :",181
"def check_args(args):
    """"""Checks that the args are coherent.""""""
    check_args_has_attributes(args)
    if args.v:
        non_version_attrs = [v for k, v in args.__dict__.items() if k != ""v""]
        print(""non_version_attrs"", non_version_attrs)
        if len([v for v in non_version_attrs if v is not None]) != 0:
            fail(""Cannot show the version number with another command."")
        return
    if args.i is None:
        fail(""Cannot draw ER diagram of no database."")
    if args.o is None:
        fail(""Cannot draw ER diagram with no output file."")
",if len ( [ v for v in non_version_attrs if v is not None ] ) != 0 :,176
"def handle(self, *args, **options):
    if not settings.ST_BASE_DIR.endswith(""spirit""):
        raise CommandError(
            ""settings.ST_BASE_DIR is not the spirit root folder, are you overriding it?""
        )
    for root, dirs, files in os.walk(settings.ST_BASE_DIR):
        if ""locale"" not in dirs:
            continue
        with utils.pushd(root):
            call_command(
                ""makemessages"", stdout=self.stdout, stderr=self.stderr, **options
            )
    self.stdout.write(""ok"")
","if ""locale"" not in dirs :",160
"def scan(scope):
    for s in scope.children:
        if s.start_pos <= position <= s.end_pos:
            if isinstance(s, (tree.Scope, tree.Flow)):
                return scan(s) or s
            elif s.type in (""suite"", ""decorated""):
                return scan(s)
    return None
","if isinstance ( s , ( tree . Scope , tree . Flow ) ) :",92
"def run_sync(self):
    count = 0
    while count < self.args.num_messages:
        batch = self.receiver.fetch_next(max_batch_size=self.args.num_messages - count)
        if self.args.peeklock:
            for msg in batch:
                msg.complete()
        count += len(batch)
",if self . args . peeklock :,93
"def __getitem__(self, item):
    if self._datas is not None:
        ret = []
        for data in self._datas:
            if isinstance(data, np.ndarray):
                ret.append(data[self._offset])
            else:
                ret.append(data.iloc[self._offset])
        self._offset += 1
        return ret
    else:
        return self._get_data(item)
","if isinstance ( data , np . ndarray ) :",115
"def removedir(self, path):
    # type: (Text) -> None
    _path = self.validatepath(path)
    if _path == ""/"":
        raise errors.RemoveRootError()
    with ftp_errors(self, path):
        try:
            self.ftp.rmd(_encode(_path, self.ftp.encoding))
        except error_perm as error:
            code, _ = _parse_ftp_error(error)
            if code == ""550"":
                if self.isfile(path):
                    raise errors.DirectoryExpected(path)
                if not self.isempty(path):
                    raise errors.DirectoryNotEmpty(path)
            raise  # pragma: no cover
",if self . isfile ( path ) :,189
"def replaces_in_file(file, replacement_list):
    rs = [(re.compile(regexp), repl) for (regexp, repl) in replacement_list]
    file_tmp = file + ""."" + str(os.getpid()) + "".tmp""
    with open(file, ""r"") as f:
        with open(file_tmp, ""w"") as f_tmp:
            for line in f:
                for r, replace in rs:
                    match = r.search(line)
                    if match:
                        line = replace + ""\n""
                f_tmp.write(line)
    shutil.move(file_tmp, file)
",if match :,172
"def _get_path_check_mem(self, i, size):
    if size > 0:
        if env.meminfo.rss + size > env.meminfo.mem_limit_soft:
            p = self._get_path(i, -1)
        else:
            p = self._get_path(i, size)
            if p.startswith(""/dev/shm""):
                env.meminfo.add(size)
    else:
        p = self._get_path(i, size)
    return p
",if env . meminfo . rss + size > env . meminfo . mem_limit_soft :,136
"def find_widget_by_id(self, id, parent=None):
    """"""Recursively searches for widget with specified ID""""""
    if parent == None:
        if id in self:
            return self[id]  # Do things fast if possible
        parent = self[""editor""]
    for c in parent.get_children():
        if hasattr(c, ""get_id""):
            if c.get_id() == id:
                return c
        if isinstance(c, Gtk.Container):
            r = self.find_widget_by_id(id, c)
            if not r is None:
                return r
    return None
","if hasattr ( c , ""get_id"" ) :",167
"def _deserialize(cls, io):
    flags = VideoFlags()
    flags.byte = U8.read(io)
    if flags.bit.type == VIDEO_FRAME_TYPE_COMMAND_FRAME:
        data = VideoCommandFrame.deserialize(io)
    else:
        if flags.bit.codec == VIDEO_CODEC_ID_AVC:
            data = AVCVideoData.deserialize(io)
        else:
            data = io.read()
    return cls(flags.bit.type, flags.bit.codec, data)
",if flags . bit . codec == VIDEO_CODEC_ID_AVC :,134
"def asciiLogData(data, maxlen=64, replace=False):
    ellipses = "" ...""
    try:
        if len(data) > maxlen - len(ellipses):
            dd = data[:maxlen] + ellipses
        else:
            dd = data
        return dd.decode(""utf8"", errors=""replace"" if replace else ""strict"")
    except:
        return ""0x"" + binLogData(data, maxlen)
",if len ( data ) > maxlen - len ( ellipses ) :,112
"def _check_units(self, new_unit_system):
    # If no unit system has been specified for me yet, adopt the incoming
    # system
    if self.unit_system is None:
        self.unit_system = new_unit_system
    else:
        # Otherwise, make sure they match
        if self.unit_system != new_unit_system:
            raise ValueError(
                ""Unit system mismatch %d v. %d"" % (self.unit_system, new_unit_system)
            )
",if self . unit_system != new_unit_system :,133
"def command(filenames, dirnames, fix):
    for filename in gather_files(dirnames, filenames):
        visitor = process_file(filename)
        if visitor.needs_fix():
            print(""%s: %s"" % (filename, visitor.get_stats()))
            if fix:
                print(""Fixing: %s"" % filename)
                fix_file(filename)
",if visitor . needs_fix ( ) :,100
"def assign_attributes_to_variants(variant_attributes):
    for value in variant_attributes:
        pk = value[""pk""]
        defaults = value[""fields""]
        defaults[""variant_id""] = defaults.pop(""variant"")
        defaults[""assignment_id""] = defaults.pop(""assignment"")
        assigned_values = defaults.pop(""values"")
        assoc, created = AssignedVariantAttribute.objects.update_or_create(
            pk=pk, defaults=defaults
        )
        if created:
            assoc.values.set(AttributeValue.objects.filter(pk__in=assigned_values))
",if created :,148
"def _info(self, userlist):
    for strng in userlist:
        group_matched = False
        for env in self.base.comps.environments_by_pattern(strng):
            self.output.display_groups_in_environment(env)
            group_matched = True
        for group in self.base.comps.groups_by_pattern(strng):
            self.output.display_pkgs_in_groups(group)
            group_matched = True
        if not group_matched:
            logger.error(_(""Warning: Group %s does not exist.""), strng)
    return 0, []
",if not group_matched :,159
"def parse_implements_interfaces(parser):
    types = []
    if parser.token.value == ""implements"":
        advance(parser)
        while True:
            types.append(parse_named_type(parser))
            if not peek(parser, TokenKind.NAME):
                break
    return types
","if not peek ( parser , TokenKind . NAME ) :",81
"def generate():
    for leaf in u.leaves:
        if isinstance(leaf, Integer):
            val = leaf.get_int_value()
            if val in (0, 1):
                yield val
            else:
                raise _NoBoolVector
        elif isinstance(leaf, Symbol):
            if leaf == SymbolTrue:
                yield 1
            elif leaf == SymbolFalse:
                yield 0
            else:
                raise _NoBoolVector
        else:
            raise _NoBoolVector
",elif leaf == SymbolFalse :,138
"def update_gstin(context):
    dirty = False
    for key, value in iteritems(frappe.form_dict):
        if key != ""party"":
            address_name = frappe.get_value(""Address"", key)
            if address_name:
                address = frappe.get_doc(""Address"", address_name)
                address.gstin = value.upper()
                address.save(ignore_permissions=True)
                dirty = True
    if dirty:
        frappe.db.commit()
        context.updated = True
",if address_name :,151
"def everythingIsUnicode(d):
    """"""Takes a dictionary, recursively verifies that every value is unicode""""""
    for k, v in d.iteritems():
        if isinstance(v, dict) and k != ""headers"":
            if not everythingIsUnicode(v):
                return False
        elif isinstance(v, list):
            for i in v:
                if isinstance(i, dict) and not everythingIsUnicode(i):
                    return False
                elif isinstance(i, _bytes):
                    return False
        elif isinstance(v, _bytes):
            return False
    return True
","if isinstance ( v , dict ) and k != ""headers"" :",158
"def check_graph(graph):  # pragma: no cover
    for c in graph:
        if isinstance(c.op, Fuse):
            raise RuntimeError(""cannot have fuse"")
        for inp in c.inputs:
            if isinstance(inp.op, Fuse):
                raise RuntimeError(""cannot have fuse"")
","if isinstance ( c . op , Fuse ) :",79
"def __getattr__(self, key):
    try:
        value = self.__parent.contents[key]
    except KeyError:
        pass
    else:
        if value is not None:
            if isinstance(value, _ModuleMarker):
                return value.mod_ns
            else:
                assert isinstance(value, _MultipleClassMarker)
                return value.attempt_get(self.__parent.path, key)
    raise AttributeError(
        ""Module %r has no mapped classes ""
        ""registered under the name %r"" % (self.__parent.name, key)
    )
",if value is not None :,154
"def filter_ports(self, dpid, in_port, nw_id, allow_nw_id_external=None):
    assert nw_id != self.nw_id_unknown
    ret = []
    for port in self.get_ports(dpid):
        nw_id_ = port.network_id
        if port.port_no == in_port:
            continue
        if nw_id_ == nw_id:
            ret.append(port.port_no)
        elif allow_nw_id_external is not None and nw_id_ == allow_nw_id_external:
            ret.append(port.port_no)
    return ret
",if nw_id_ == nw_id :,167
"def _parse(self, contents):
    entries = []
    for line in contents.splitlines():
        if not len(line.strip()):
            entries.append((""blank"", [line]))
            continue
        (head, tail) = chop_comment(line.strip(), ""#"")
        if not len(head):
            entries.append((""all_comment"", [line]))
            continue
        entries.append((""option"", [head.split(None), tail]))
    return entries
",if not len ( head ) :,121
"def _get_documented_completions(self, table, startswith=None):
    names = []
    for key, command in table.items():
        if getattr(command, ""_UNDOCUMENTED"", False):
            # Don't tab complete undocumented commands/params
            continue
        if startswith is not None and not key.startswith(startswith):
            continue
        if getattr(command, ""positional_arg"", False):
            continue
        names.append(key)
    return names
",if startswith is not None and not key . startswith ( startswith ) :,118
"def _convert_example(example, use_bfloat16):
    """"""Cast int64 into int32 and float32 to bfloat16 if use_bfloat16.""""""
    for key in list(example.keys()):
        val = example[key]
        if tf.keras.backend.is_sparse(val):
            val = tf.sparse.to_dense(val)
        if val.dtype == tf.int64:
            val = tf.cast(val, tf.int32)
        if use_bfloat16 and val.dtype == tf.float32:
            val = tf.cast(val, tf.bfloat16)
        example[key] = val
",if tf . keras . backend . is_sparse ( val ) :,166
"def _get_lang_zone(self, lang):
    if lang not in self._lang_zone_from_lang:
        if self.mgr.is_multilang(lang):
            self._lang_zone_from_lang[lang] = MultiLangZone(self.mgr, lang)
        else:
            self._lang_zone_from_lang[lang] = LangZone(self.mgr, lang)
    return self._lang_zone_from_lang[lang]
",if self . mgr . is_multilang ( lang ) :,119
"def dispatch(self, request, *args, **kwargs):
    try:
        return super(Handler, self).dispatch(request, *args, **kwargs)
    except Http404 as e:
        if settings.FEINCMS_CMS_404_PAGE:
            try:
                request.original_path_info = request.path_info
                request.path_info = settings.FEINCMS_CMS_404_PAGE
                response = super(Handler, self).dispatch(request, *args, **kwargs)
                response.status_code = 404
                return response
            except Http404:
                raise e
        else:
            raise
",if settings . FEINCMS_CMS_404_PAGE :,179
"def _maybe_update_dropout(self, step):
    for i in range(len(self.dropout_steps)):
        if step > 1 and step == self.dropout_steps[i] + 1:
            self.model.update_dropout(self.dropout[i])
            logger.info(""Updated dropout to %f from step %d"" % (self.dropout[i], step))
",if step > 1 and step == self . dropout_steps [ i ] + 1 :,95
"def bulk_move(*args, **kwargs):
    for arg in args:
        if arg[""src_path""] == arg[""dest_path""]:
            raise PopupException(_(""Source path and destination path cannot be same""))
        request.fs.rename(
            urllib.unquote(arg[""src_path""]), urllib.unquote(arg[""dest_path""])
        )
","if arg [ ""src_path"" ] == arg [ ""dest_path"" ] :",90
"def asisWrite(self, root):
    at, c = self, self.c
    try:
        c.endEditing()
        c.init_error_dialogs()
        fileName = at.initWriteIvars(root, root.atAsisFileNodeName())
        if not at.precheck(fileName, root):
            at.addToOrphanList(root)
            return
        at.openOutputStream()
        for p in root.self_and_subtree(copy=False):
            at.writeAsisNode(p)
        contents = at.closeOutputStream()
        at.replaceFile(contents, at.encoding, fileName, root)
    except Exception:
        at.writeException(fileName, root)
","if not at . precheck ( fileName , root ) :",187
"def next_event(it):
    """"""read an event from an eventstream""""""
    while True:
        try:
            line = next(it)
        except StopIteration:
            return
        if line.startswith(""data:""):
            return json.loads(line.split("":"", 1)[1])
","if line . startswith ( ""data:"" ) :",77
"def process_formdata(self, valuelist):
    if valuelist:
        if valuelist[0] == ""__None"":
            self.data = None
        else:
            if self.queryset is None:
                self.data = None
                return
            try:
                obj = self.queryset.get(pk=valuelist[0])
                self.data = obj
            except DoesNotExist:
                self.data = None
",if self . queryset is None :,122
"def _setResultsName(self, name, listAllMatches=False):
    if __diag__.warn_multiple_tokens_in_named_alternation:
        if any(isinstance(e, And) for e in self.exprs):
            warnings.warn(
                ""{}: setting results name {!r} on {} expression ""
                ""will return a list of all parsed tokens in an And alternative, ""
                ""in prior versions only the first token was returned"".format(
                    ""warn_multiple_tokens_in_named_alternation"",
                    name,
                    type(self).__name__,
                ),
                stacklevel=3,
            )
    return super()._setResultsName(name, listAllMatches)
","if any ( isinstance ( e , And ) for e in self . exprs ) :",195
"def add(request):
    form_type = ""servers""
    if request.method == ""POST"":
        form = BookMarkForm(request.POST)
        if form.is_valid():
            form_type = form.save()
            messages.add_message(request, messages.INFO, ""Bookmark created"")
        else:
            messages.add_message(request, messages.INFO, form.errors)
        if form_type == ""server"":
            url = reverse(""servers"")
        else:
            url = reverse(""metrics"")
        return redirect(url)
    else:
        return redirect(reverse(""servers""))
","if form_type == ""server"" :",164
"def __init__(self, post_id, artist, page, tzInfo=None, dateFormat=None):
    self.imageUrls = list()
    self.imageResizedUrls = list()
    self.imageId = int(post_id)
    self._tzInfo = tzInfo
    self.dateFormat = dateFormat
    if page is not None:
        post_json = demjson.decode(page)
        if artist is None:
            artist_id = post_json[""data""][""item""][""user""][""id""]
            self.artist = SketchArtist(artist_id, page, tzInfo, dateFormat)
        else:
            self.artist = artist
        self.parse_post(post_json[""data""][""item""])
",if artist is None :,182
"def _create_batch_iterator(
    self,
    mark_as_delete: Callable[[Any], None],
    to_key: Callable[[Any], Any],
    to_value: Callable[[Any], Any],
    batch: Iterable[EventT],
) -> Iterable[Tuple[Any, Any]]:
    for event in batch:
        key = to_key(event.key)
        # to delete keys in the table we set the raw value to None
        if event.message.value is None:
            mark_as_delete(key)
            continue
        yield key, to_value(event.value)
",if event . message . value is None :,150
"def test_lc_numeric_nl_langinfo(self):
    # Test nl_langinfo against known values
    tested = False
    for loc in candidate_locales:
        try:
            setlocale(LC_NUMERIC, loc)
            setlocale(LC_CTYPE, loc)
        except Error:
            continue
        for li, lc in ((RADIXCHAR, ""decimal_point""), (THOUSEP, ""thousands_sep"")):
            if self.numeric_tester(""nl_langinfo"", nl_langinfo(li), lc, loc):
                tested = True
    if not tested:
        self.skipTest(""no suitable locales"")
","if self . numeric_tester ( ""nl_langinfo"" , nl_langinfo ( li ) , lc , loc ) :",163
"def _level_up_logging(self):
    for handler in self.log.handlers:
        if issubclass(handler.__class__, logging.FileHandler):
            if handler.level != logging.DEBUG:
                handler.setLevel(logging.DEBUG)
                self.log.debug(""Leveled up log file verbosity"")
","if issubclass ( handler . __class__ , logging . FileHandler ) :",80
"def _show_axes_changed(self):
    marker = self.marker
    if (self._vtk_control is not None) and (marker is not None):
        if not self.show_axes:
            marker.interactor = None
            marker.enabled = False
        else:
            marker.interactor = self.interactor
            marker.enabled = True
        self.render()
",if not self . show_axes :,103
"def handle_keypress(self, rawKey, modifiers, key, *args):
    if self.recordKeyboard and self.__delayPassed():
        if not self.insideKeys:
            self.insideKeys = True
            self.targetParent.start_key_sequence()
        modifierCount = len(modifiers)
        if (
            modifierCount > 1
            or (modifierCount == 1 and Key.SHIFT not in modifiers)
            or (Key.SHIFT in modifiers and len(rawKey) > 1)
        ):
            self.targetParent.append_hotkey(rawKey, modifiers)
        elif key not in MODIFIERS:
            self.targetParent.append_key(key)
",if not self . insideKeys :,178
"def transform(self, data):
    with timer(""transform %s"" % self.name, logging.DEBUG):
        if self.operator in {""lat"", ""latitude""}:
            return self.series(data).apply(GeoIP.get_latitude)
        elif self.operator in {""lon"", ""longitude""}:
            return self.series(data).apply(GeoIP.get_longitude)
        elif self.operator in {""acc"", ""accuracy""}:
            return self.series(data).apply(GeoIP.get_accuracy)
        raise NameError(""Unknown GeoIP operator [lat, lon, acc]: %s"" % self.operator)
","elif self . operator in { ""lon"" , ""longitude"" } :",161
"def _get_sidebar_selected(self):
    sidebar_selected = None
    if self.businessline_id:
        sidebar_selected = ""bl_%s"" % self.businessline_id
        if self.service_id:
            sidebar_selected += ""_s_%s"" % self.service_id
            if self.environment_id:
                sidebar_selected += ""_env_%s"" % self.environment_id
    return sidebar_selected
",if self . service_id :,113
"def _run_response_middleware(self, request, response, request_name=None):
    named_middleware = self.named_response_middleware.get(request_name, deque())
    applicable_middleware = self.response_middleware + named_middleware
    if applicable_middleware:
        for middleware in applicable_middleware:
            _response = middleware(request, response)
            if isawaitable(_response):
                _response = await _response
            if _response:
                response = _response
                break
    return response
",if isawaitable ( _response ) :,136
"def populate_obj(self, obj, name):
    field = getattr(obj, name, None)
    if field is not None:
        # If field should be deleted, clean it up
        if self._should_delete:
            field.delete()
            return
        if isinstance(self.data, FileStorage) and not is_empty(self.data.stream):
            if not field.grid_id:
                func = field.put
            else:
                func = field.replace
            func(
                self.data.stream,
                filename=self.data.filename,
                content_type=self.data.content_type,
            )
","if isinstance ( self . data , FileStorage ) and not is_empty ( self . data . stream ) :",182
"def _import_hash(self, operator):
    # Import required modules into local namespace so that pipelines
    # may be evaluated directly
    for key in sorted(operator.import_hash.keys()):
        module_list = "", "".join(sorted(operator.import_hash[key]))
        if key.startswith(""tpot.""):
            exec(""from {} import {}"".format(key[4:], module_list))
        else:
            exec(""from {} import {}"".format(key, module_list))
        for var in operator.import_hash[key]:
            self.operators_context[var] = eval(var)
","if key . startswith ( ""tpot."" ) :",152
"def remove_files(folder, file_extensions):
    for f in os.listdir(folder):
        f_path = os.path.join(folder, f)
        if os.path.isfile(f_path):
            extension = os.path.splitext(f_path)[1]
            if extension in file_extensions:
                os.remove(f_path)
",if os . path . isfile ( f_path ) :,96
"def clearBuffer(self):
    if self.shouldLose == -1:
        return
    if self.producer:
        self.producer.resumeProducing()
    if self.buffer:
        if self.logFile:
            self.logFile.write(""loopback receiving %s\n"" % repr(self.buffer))
        buffer = self.buffer
        self.buffer = b""""
        self.target.dataReceived(buffer)
    if self.shouldLose == 1:
        self.shouldLose = -1
        self.target.connectionLost(failure.Failure(main.CONNECTION_DONE))
",if self . logFile :,156
"def write(self, data):
    if mock_target._mirror_on_stderr:
        if self._write_line:
            sys.stderr.write(fn + "": "")
        if bytes:
            sys.stderr.write(data.decode(""utf8""))
        else:
            sys.stderr.write(data)
        if (data[-1]) == ""\n"":
            self._write_line = True
        else:
            self._write_line = False
    super(Buffer, self).write(data)
","if ( data [ - 1 ] ) == ""\n"" :",137
"def stop(self):
    self.queue_com.state_lock.acquire()
    try:
        if self.queue_com.state == RUNNING and self.stop_task():
            self.queue_com.state = STOPPED
            self.remove()
            return True
        return False
    finally:
        self.queue_com.state_lock.release()
",if self . queue_com . state == RUNNING and self . stop_task ( ) :,97
"def _handle_special_args(self, pyobjects):
    if len(pyobjects) == len(self.arguments.args):
        if self.arguments.vararg:
            pyobjects.append(rope.base.builtins.get_list())
        if self.arguments.kwarg:
            pyobjects.append(rope.base.builtins.get_dict())
",if self . arguments . kwarg :,91
"def go_to_last_edit_location(self):
    if self.last_edit_cursor_pos is not None:
        filename, position = self.last_edit_cursor_pos
        if not osp.isfile(filename):
            self.last_edit_cursor_pos = None
            return
        else:
            self.load(filename)
            editor = self.get_current_editor()
            if position < editor.document().characterCount():
                editor.set_cursor_position(position)
",if position < editor . document ( ) . characterCount ( ) :,135
"def _create_sentence_objects(self):
    """"""Returns a list of Sentence objects from the raw text.""""""
    sentence_objects = []
    sent_tokenizer = SentenceTokenizer(locale=self.language.code)
    seq = Sequence(self.raw)
    seq = sent_tokenizer.transform(seq)
    for start_index, end_index in zip(seq.idx[:-1], seq.idx[1:]):
        # Sentences share the same models as their parent blob
        sent = seq.text[start_index:end_index].strip()
        if not sent:
            continue
        s = Sentence(sent, start_index=start_index, end_index=end_index)
        s.detected_languages = self.detected_languages
        sentence_objects.append(s)
    return sentence_objects
",if not sent :,196
"def to_json_schema(self, parent=None):
    schema = {}
    if not parent:
        schema[""title""] = self.title
        if self.description:
            schema[""description""] = self.description
        if self.has_default:
            schema[""default""] = self.default
        schema[""_required_""] = self.required
    if self.null:
        schema[""type""] = [""string"", ""null""]
    else:
        schema[""type""] = ""string""
    if self.enum is not None:
        schema[""enum""] = self.enum
    return schema
",if self . description :,150
"def rmdir(dirname):
    if dirname[-1] == os.sep:
        dirname = dirname[:-1]
    if os.path.islink(dirname):
        return  # do not clear link - we can get out of dir
    for f in os.listdir(dirname):
        if f in (""."", ""..""):
            continue
        path = dirname + os.sep + f
        if os.path.isdir(path):
            rmdir(path)
        else:
            os.unlink(path)
    os.rmdir(dirname)
","if f in ( ""."" , "".."" ) :",137
"def convert_whole_dir(path=Path(""marian_ckpt/"")):
    for subdir in tqdm(list(path.ls())):
        dest_dir = f""marian_converted/{subdir.name}""
        if (dest_dir / ""pytorch_model.bin"").exists():
            continue
        convert(source_dir, dest_dir)
","if ( dest_dir / ""pytorch_model.bin"" ) . exists ( ) :",85
"def colorformat(text):
    if text[0:1] == ""#"":
        col = text[1:]
        if len(col) == 6:
            return col
        elif len(col) == 3:
            return col[0] * 2 + col[1] * 2 + col[2] * 2
    elif text == """":
        return """"
    assert False, ""wrong color format %r"" % text
",elif len ( col ) == 3 :,105
"def _init_rel_seek(self):
    ""Sets the file object's position to the relative location set above.""
    rs, fo = self._rel_seek, self._file_obj
    if rs == 0.0:
        fo.seek(0, os.SEEK_SET)
    else:
        fo.seek(0, os.SEEK_END)
        size = fo.tell()
        if rs == 1.0:
            self._cur_pos = size
        else:
            target = int(size * rs)
            fo.seek(target, os.SEEK_SET)
            self._align_to_newline()
            self._cur_pos = fo.tell()
",if rs == 1.0 :,176
"def parse_command_line(self, argv=None):
    """"""Parse the command line""""""
    if self.config:
        parser = argparse.ArgumentParser(add_help=False)
        self.settings[""config""].add_argument(parser)
        opts, _ = parser.parse_known_args(argv)
        if opts.config is not None:
            self.set(""config"", opts.config)
        self.params.update(self.import_from_module())
    parser = self.parser()
    opts = parser.parse_args(argv)
    for k, v in opts.__dict__.items():
        if v is None:
            continue
        self.set(k.lower(), v)
",if v is None :,177
"def process(self, resources, event=None):
    client = local_session(self.manager.session_factory).client(
        ""shield"", region_name=""us-east-1""
    )
    protections = get_type_protections(client, self.manager.get_model())
    protected_resources = {p[""ResourceArn""] for p in protections}
    state = self.data.get(""state"", False)
    results = []
    for arn, r in zip(self.manager.get_arns(resources), resources):
        r[""c7n:ShieldProtected""] = shielded = arn in protected_resources
        if shielded and state:
            results.append(r)
        elif not shielded and not state:
            results.append(r)
    return results
",if shielded and state :,199
"def removeTrailingWs(self, aList):
    i = 0
    while i < len(aList):
        if self.is_ws(aList[i]):
            j = i
            i = self.skip_ws(aList, i)
            assert j < i
            if i >= len(aList) or aList[i] == ""\n"":
                # print ""removing trailing ws:"", `i-j`
                del aList[j:i]
                i = j
        else:
            i += 1
",if self . is_ws ( aList [ i ] ) :,147
"def predict(request: Request):
    form = await request.form()
    files, entry = convert_input(form)
    try:
        if (entry.keys() & input_features) != input_features:
            return JSONResponse(ALL_FEATURES_PRESENT_ERROR, status_code=400)
        try:
            resp = model.predict(data_dict=[entry]).to_dict(""records"")[0]
            return JSONResponse(resp)
        except Exception as e:
            logger.error(""Error: {}"".format(str(e)))
            return JSONResponse(COULD_NOT_RUN_INFERENCE_ERROR, status_code=500)
    finally:
        for f in files:
            os.remove(f.name)
",if ( entry . keys ( ) & input_features ) != input_features :,192
"def reset(self):
    logger.debug(""Arctic.reset()"")
    with self._lock:
        if self.__conn is not None:
            self.__conn.close()
            self.__conn = None
        for _, l in self._library_cache.items():
            if hasattr(l, ""_reset"") and callable(l._reset):
                logger.debug(""Library reset() %s"" % l)
                l._reset()  # the existence of _reset() is not guaranteed/enforced, it also triggers re-auth
",if self . __conn is not None :,137
"def read(self):
    if op.isfile(self.fileName):
        with textfile_open(self.fileName, ""rt"") as fid:
            items = json.load(fid)
            # TODO: catch JSON exception...
            if items is None:
                items = dict()
    else:
        items = dict()
    self._items.clear()
    self._items.update(items)
    self._haveReadData = True
",if items is None :,115
"def get_django_comment(text: str, i: int) -> str:
    end = i + 4
    unclosed_end = 0
    while end <= len(text):
        if text[end - 2 : end] == ""#}"":
            return text[i:end]
        if not unclosed_end and text[end] == ""<"":
            unclosed_end = end
        end += 1
    raise TokenizationException(""Unclosed comment"", text[i:unclosed_end])
","if not unclosed_end and text [ end ] == ""<"" :",126
"def _wrap_forwarded(self, key, value):
    if isinstance(value, SourceCode) and value.late_binding:
        # get cached return value if present
        value_ = self._late_binding_returnvalues.get(key, KeyError)
        if value_ is KeyError:
            # evaluate the late-bound function
            value_ = self._eval_late_binding(value)
            schema = self.late_bind_schemas.get(key)
            if schema is not None:
                value_ = schema.validate(value_)
            # cache result of late bound func
            self._late_binding_returnvalues[key] = value_
        return value_
    else:
        return value
",if value_ is KeyError :,187
"def connect(*args, **ckwargs):
    if ""give_content_type"" in kwargs:
        if len(args) >= 7 and ""content_type"" in args[6]:
            kwargs[""give_content_type""](args[6][""content-type""])
        else:
            kwargs[""give_content_type""]("""")
    if ""give_connect"" in kwargs:
        kwargs[""give_connect""](*args, **ckwargs)
    status = code_iter.next()
    etag = etag_iter.next()
    timestamp = timestamps_iter.next()
    if status == -1:
        raise HTTPException()
    return FakeConn(status, etag, body=kwargs.get(""body"", """"), timestamp=timestamp)
","if len ( args ) >= 7 and ""content_type"" in args [ 6 ] :",178
"def _reset(self):
    self._handle_connect()
    if self.rewarder_session:
        if self._sample_env_ids:
            env_id = random.choice(self._sample_env_ids)
            logger.info(""Randomly sampled env_id={}"".format(env_id))
        else:
            env_id = None
        self.rewarder_session.reset(env_id=env_id)
    else:
        logger.info(
            ""No rewarder session exists, so cannot send a reset via the rewarder channel""
        )
    self._reset_mask()
    return [None] * self.n
",if self . _sample_env_ids :,171
"def _create_architecture_list(architectures, current_arch):
    if not architectures:
        return [_Architecture(build_on=[current_arch])]
    build_architectures: List[str] = []
    architecture_list: List[_Architecture] = []
    for item in architectures:
        if isinstance(item, str):
            build_architectures.append(item)
        if isinstance(item, dict):
            architecture_list.append(
                _Architecture(build_on=item.get(""build-on""), run_on=item.get(""run-on""))
            )
    if build_architectures:
        architecture_list.append(_Architecture(build_on=build_architectures))
    return architecture_list
","if isinstance ( item , dict ) :",196
"def inspect(self, pokemon):
    # Make sure it was not caught!
    for caught_pokemon in self.cache:
        same_latitude = ""{0:.4f}"".format(pokemon[""latitude""]) == ""{0:.4f}"".format(
            caught_pokemon[""latitude""]
        )
        same_longitude = ""{0:.4f}"".format(pokemon[""longitude""]) == ""{0:.4f}"".format(
            caught_pokemon[""longitude""]
        )
        if same_latitude and same_longitude:
            return
    if len(self.cache) >= 200:
        self.cache.pop(0)
    self.cache.append(pokemon)
",if same_latitude and same_longitude :,185
"def parley(self):
    for x in [0, 1]:
        a = self.agents[x].act()
        if a is not None:
            if ""[DONE]"" in a[""text""]:
                self.agents[x - 1].observe(
                    {""id"": ""World"", ""text"": ""The other agent has ended the chat.""}
                )
                self.episodeDone = True
            else:
                self.agents[x - 1].observe(a)
",if a is not None :,134
"def _prepare_subset(
    full_data: torch.Tensor,
    full_targets: torch.Tensor,
    num_samples: int,
    digits: Sequence,
):
    classes = {d: 0 for d in digits}
    indexes = []
    for idx, target in enumerate(full_targets):
        label = target.item()
        if classes.get(label, float(""inf"")) >= num_samples:
            continue
        indexes.append(idx)
        classes[label] += 1
        if all(classes[k] >= num_samples for k in classes):
            break
    data = full_data[indexes]
    targets = full_targets[indexes]
    return data, targets
",if all ( classes [ k ] >= num_samples for k in classes ) :,174
"def get_work_root(self, flags):
    _flags = flags.copy()
    _flags[""is_toplevel""] = True
    target = self._get_target(_flags)
    if target:
        _flags[""target""] = target.name
        tool = self.get_tool(_flags)
        if tool:
            return target.name + ""-"" + tool
        else:
            raise SyntaxError(
                ""Failed to determine work root. Could not resolve tool for target ""
                + target.name
            )
    else:
        raise SyntaxError(""Failed to determine work root. Could not resolve target"")
",if tool :,158
"def run_command(self, data):
    """"""Run editor commands.""""""
    parts = data.split("" "")
    cmd = parts[0].lower()
    if cmd in self.operations.keys():
        return self.run_operation(cmd)
    args = "" "".join(parts[1:])
    self.logger.debug(""Looking for command '{0}'"".format(cmd))
    if cmd in self.modules.modules.keys():
        self.logger.debug(""Trying to run command '{0}'"".format(cmd))
        self.get_editor().store_action_state(cmd)
        if not self.run_module(cmd, args):
            return False
    else:
        self.set_status(""Command '{0}' not found."".format(cmd))
        return False
    return True
","if not self . run_module ( cmd , args ) :",193
"def get_main_chain_layers(self):
    """"""Return a list of layer IDs in the main chain.""""""
    main_chain = self.get_main_chain()
    ret = []
    for u in main_chain:
        for v, layer_id in self.adj_list[u]:
            if v in main_chain and u in main_chain:
                ret.append(layer_id)
    return ret
",if v in main_chain and u in main_chain :,106
"def hash(self, context):
    with context:
        if not self[""fileName""].getValue() or self[""in""].source() == self[""in""]:
            return IECore.MurmurHash()
        h = GafferDispatch.TaskNode.hash(self, context)
        h.append(self[""fileName""].hash())
        h.append(self[""in""].hash())
        h.append(self.__parameterHandler.hash())
        return h
","if not self [ ""fileName"" ] . getValue ( ) or self [ ""in"" ] . source ( ) == self [ ""in"" ] :",116
"def consume_buf():
    ty = state[""ty""] - 1
    for i in xrange(state[""buf""].shape[1] // N):
        tx = x // N + i
        src = state[""buf""][:, i * N : (i + 1) * N, :]
        if src[:, :, 3].any():
            with self.tile_request(tx, ty, readonly=False) as dst:
                mypaintlib.tile_convert_rgba8_to_rgba16(src, dst, self.EOTF)
    if state[""progress""]:
        try:
            state[""progress""].completed(ty - ty0)
        except Exception:
            logger.exception(""Progress.completed() failed"")
            state[""progress""] = None
","if src [ : , : , 3 ] . any ( ) :",188
"def check_permissions(self, obj):
    request = self.context.get(""request"")
    for Perm in permissions:
        perm = Perm()
        if not perm.has_permission(request, self):
            return False
        if not perm.has_object_permission(request, self, obj):
            return False
    return True
","if not perm . has_object_permission ( request , self , obj ) :",88
"def _post_order(op):
    if isinstance(op, tvm.tir.Allocate):
        lift_stmt[-1].append(op)
        return op.body
    if isinstance(op, tvm.tir.AttrStmt):
        if op.attr_key == ""storage_scope"":
            lift_stmt[-1].append(op)
            return op.body
        if op.attr_key == ""virtual_thread"":
            return _merge_block(lift_stmt.pop() + [op], op.body)
        return op
    if isinstance(op, tvm.tir.For):
        return _merge_block(lift_stmt.pop() + [op], op.body)
    raise RuntimeError(""not reached"")
","if op . attr_key == ""virtual_thread"" :",188
"def task_done(self):
    with self._cond:
        if not self._unfinished_tasks.acquire(False):
            raise ValueError(""task_done() called too many times"")
        if self._unfinished_tasks._semlock._is_zero():
            self._cond.notify_all()
",if self . _unfinished_tasks . _semlock . _is_zero ( ) :,75
"def get_json(self):
    if not hasattr(self, ""_json""):
        self._json = None
        if self.request.headers.get(""Content-Type"", """").startswith(""application/json""):
            self._json = json.loads(self.request.body)
    return self._json
","if self . request . headers . get ( ""Content-Type"" , """" ) . startswith ( ""application/json"" ) :",74
"def userfullname():
    """"""Get the user's full name.""""""
    global _userfullname
    if not _userfullname:
        uid = os.getuid()
        entry = pwd_from_uid(uid)
        if entry:
            _userfullname = entry[4].split("","")[0] or entry[0]
        if not _userfullname:
            _userfullname = ""user%d"" % uid
    return _userfullname
",if not _userfullname :,108
"def test_scatter(self):
    for rank in range(self.world_size):
        tensor = []
        if self.rank == rank:
            tensor = [torch.tensor(i) for i in range(self.world_size)]
        result = comm.get().scatter(tensor, rank, size=())
        self.assertTrue(torch.is_tensor(result))
        self.assertEqual(result.item(), self.rank)
",if self . rank == rank :,109
"def decompile(decompiler):
    for pos, next_pos, opname, arg in decompiler.instructions:
        if pos in decompiler.targets:
            decompiler.process_target(pos)
        method = getattr(decompiler, opname, None)
        if method is None:
            throw(DecompileError(""Unsupported operation: %s"" % opname))
        decompiler.pos = pos
        decompiler.next_pos = next_pos
        x = method(*arg)
        if x is not None:
            decompiler.stack.append(x)
",if x is not None :,143
"def print_scenario_ran(self, scenario):
    if scenario.passed:
        self.wrt(""OK"")
    elif scenario.failed:
        reason = self.scenarios_and_its_fails[scenario]
        if isinstance(reason.exception, AssertionError):
            self.wrt(""FAILED"")
        else:
            self.wrt(""ERROR"")
    self.wrt(""\n"")
","if isinstance ( reason . exception , AssertionError ) :",102
"def detect_ssl_option(self):
    for option in self.ssl_options():
        if scan_argv(self.argv, option) is not None:
            for other_option in self.ssl_options():
                if option != other_option:
                    if scan_argv(self.argv, other_option) is not None:
                        raise ConfigurationError(
                            ""Cannot give both %s and %s"" % (option, other_option)
                        )
            return option
","if scan_argv ( self . argv , other_option ) is not None :",140
"def print_po_snippet(en_loc_old_lists, context):
    for m, localized, old in zip(*en_loc_old_lists):
        if m == """":
            continue
        if m == localized:
            localized = old
        print(
            ""#: {file}:{line}\n""
            'msgid ""{context}{en_month}""\n'
            'msgstr ""{localized_month}""\n'.format(
                context=context,
                file=filename,
                line=print_po_snippet.line,
                en_month=m,
                localized_month=localized,
            )
        )
        print_po_snippet.line += 1
",if m == localized :,190
"def set_status(self, dict_new):
    for i, value in dict_new.items():
        self.dict_bili[i] = value
        if i == ""cookie"":
            self.dict_bili[""pcheaders""][""cookie""] = value
            self.dict_bili[""appheaders""][""cookie""] = value
","if i == ""cookie"" :",85
"def makeSomeFiles(pathobj, dirdict):
    pathdict = {}
    for (key, value) in dirdict.items():
        child = pathobj.child(key)
        if isinstance(value, bytes):
            pathdict[key] = child
            child.setContent(value)
        elif isinstance(value, dict):
            child.createDirectory()
            pathdict[key] = makeSomeFiles(child, value)
        else:
            raise ValueError(""only strings and dicts allowed as values"")
    return pathdict
","if isinstance ( value , bytes ) :",138
"def _truncate_to_length(generator, len_map=None):
    for example in generator:
        example = list(example)
        if len_map is not None:
            for key, max_len in len_map.items():
                example_len = example[key].shape
                if example_len > max_len:
                    example[key] = np.resize(example[key], max_len)
        yield tuple(example)
",if example_len > max_len :,120
"def check(self, **kw):
    if not kw:
        return exists(self.strpath)
    if len(kw) == 1:
        if ""dir"" in kw:
            return not kw[""dir""] ^ isdir(self.strpath)
        if ""file"" in kw:
            return not kw[""file""] ^ isfile(self.strpath)
    return super(LocalPath, self).check(**kw)
","if ""file"" in kw :",106
"def next_instruction_is_function_or_class(lines):
    """"""Is the first non-empty, non-commented line of the cell either a function or a class?""""""
    parser = StringParser(""python"")
    for i, line in enumerate(lines):
        if parser.is_quoted():
            parser.read_line(line)
            continue
        parser.read_line(line)
        if not line.strip():  # empty line
            if i > 0 and not lines[i - 1].strip():
                return False
            continue
        if line.startswith(""def "") or line.startswith(""class ""):
            return True
        if line.startswith((""#"", ""@"", "" "", "")"")):
            continue
        return False
    return False
","if line . startswith ( ( ""#"" , ""@"" , "" "" , "")"" ) ) :",194
"def askCheckReadFile(self, localFile, remoteFile):
    if not kb.bruteMode:
        message = ""do you want confirmation that the remote file '%s' "" % remoteFile
        message += ""has been successfully downloaded from the back-end ""
        message += ""DBMS file system? [Y/n] ""
        if readInput(message, default=""Y"", boolean=True):
            return self._checkFileLength(localFile, remoteFile, True)
    return None
","if readInput ( message , default = ""Y"" , boolean = True ) :",118
"def process_tag(hive_name, company, company_key, tag, default_arch):
    with winreg.OpenKeyEx(company_key, tag) as tag_key:
        version = load_version_data(hive_name, company, tag, tag_key)
        if version is not None:  # if failed to get version bail
            major, minor, _ = version
            arch = load_arch_data(hive_name, company, tag, tag_key, default_arch)
            if arch is not None:
                exe_data = load_exe(hive_name, company, company_key, tag)
                if exe_data is not None:
                    exe, args = exe_data
                    return company, major, minor, arch, exe, args
",if arch is not None :,199
"def _get_matching_bracket(self, s, pos):
    if s[pos] != ""{"":
        return None
    end = len(s)
    depth = 1
    pos += 1
    while pos != end:
        c = s[pos]
        if c == ""{"":
            depth += 1
        elif c == ""}"":
            depth -= 1
        if depth == 0:
            break
        pos += 1
    if pos < end and s[pos] == ""}"":
        return pos
    return None
","elif c == ""}"" :",132
"def pred(field, value, item):
    for suffix, p in _BUILTIN_PREDS.iteritems():
        if field.endswith(suffix):
            f = field[: field.index(suffix)]
            if not hasattr(item, f) or getattr(item, f) is None:
                return False
            return p(getattr(item, f), value)
    if not hasattr(item, field) or getattr(item, field) is None:
        return False
    if isinstance(value, type(lambda x: x)):
        return value(getattr(item, field))
    return getattr(item, field) == value
","if not hasattr ( item , f ) or getattr ( item , f ) is None :",155
"def init_weights(self):
    """"""Initialize model weights.""""""
    for _, m in self.multi_deconv_layers.named_modules():
        if isinstance(m, nn.ConvTranspose2d):
            normal_init(m, std=0.001)
        elif isinstance(m, nn.BatchNorm2d):
            constant_init(m, 1)
    for m in self.multi_final_layers.modules():
        if isinstance(m, nn.Conv2d):
            normal_init(m, std=0.001, bias=0)
","if isinstance ( m , nn . Conv2d ) :",139
"def test_byteswap(self):
    if self.typecode == ""u"":
        example = ""\U00100100""
    else:
        example = self.example
    a = array.array(self.typecode, example)
    self.assertRaises(TypeError, a.byteswap, 42)
    if a.itemsize in (1, 2, 4, 8):
        b = array.array(self.typecode, example)
        b.byteswap()
        if a.itemsize == 1:
            self.assertEqual(a, b)
        else:
            self.assertNotEqual(a, b)
        b.byteswap()
        self.assertEqual(a, b)
",if a . itemsize == 1 :,171
"def _remove_blocks_from_variables(variables):
    new_variables = []
    for name, variable in variables:
        if variable.is_block():
            new_variables.extend(variable.locals)
            new_variables.append((name, variable.result))
        else:
            new_variables.append((name, variable))
    return new_variables
",if variable . is_block ( ) :,94
"def scope(self):
    if self.scope_ is None:
        self.lazy_init_lock_.acquire()
        try:
            if self.scope_ is None:
                self.scope_ = Scope()
        finally:
            self.lazy_init_lock_.release()
    return self.scope_
",if self . scope_ is None :,84
"def translate():
    assert Lex.next() is AttributeList
    reader.read()  # Discard attribute list from reader.
    attrs = {}
    d = AttributeList.match.groupdict()
    for k, v in d.items():
        if v is not None:
            if k == ""attrlist"":
                v = subs_attrs(v)
                if v:
                    parse_attributes(v, attrs)
            else:
                AttributeList.attrs[k] = v
    AttributeList.subs(attrs)
    AttributeList.attrs.update(attrs)
",if v :,150
"def parse(self, response):
    try:
        content = response.content.decode(""utf-8"", ""ignore"")
        content = json.loads(content, strict=False)
    except:
        self.logger.error(""Fail to parse the response in json format"")
        return
    for item in content[""data""]:
        if ""objURL"" in item:
            img_url = self._decode_url(item[""objURL""])
        elif ""hoverURL"" in item:
            img_url = item[""hoverURL""]
        else:
            continue
        yield dict(file_url=img_url)
","if ""objURL"" in item :",158
"def canonicalize_instruction_name(instr):
    name = instr.insn_name().upper()
    # XXX bypass a capstone bug that incorrectly labels some insns as mov
    if name == ""MOV"":
        if instr.mnemonic.startswith(""lsr""):
            return ""LSR""
        elif instr.mnemonic.startswith(""lsl""):
            return ""LSL""
        elif instr.mnemonic.startswith(""asr""):
            return ""ASR""
    return OP_NAME_MAP.get(name, name)
","elif instr . mnemonic . startswith ( ""asr"" ) :",135
"def _clean_regions(items, region):
    """"""Intersect region with target file if it exists""""""
    variant_regions = bedutils.population_variant_regions(items, merged=True)
    with utils.tmpfile() as tx_out_file:
        target = subset_variant_regions(variant_regions, region, tx_out_file, items)
        if target:
            if isinstance(target, six.string_types) and os.path.isfile(target):
                target = _load_regions(target)
            else:
                target = [target]
            return target
",if target :,151
"def reader_leaves(self):
    self.mutex.acquire()
    try:
        self.active_readers -= 1
        if self.active_readers == 0 and self.waiting_writers != 0:
            self.active_writers += 1
            self.waiting_writers -= 1
            self.can_write.release()
    finally:
        self.mutex.release()
",if self . active_readers == 0 and self . waiting_writers != 0 :,101
"def _bpe_to_words(sentence, delimiter=""@@""):
    """"""Convert a sequence of bpe words into sentence.""""""
    words = []
    word = """"
    delimiter_len = len(delimiter)
    for subwords in sentence:
        if len(subwords) >= delimiter_len and subwords[-delimiter_len:] == delimiter:
            word += subwords[:-delimiter_len]
        else:
            word += subwords
            words.append(word)
            word = """"
    return words
",if len ( subwords ) >= delimiter_len and subwords [ - delimiter_len : ] == delimiter :,121
"def _make_var_names(exog):
    if hasattr(exog, ""name""):
        var_names = exog.name
    elif hasattr(exog, ""columns""):
        var_names = exog.columns
    else:
        raise ValueError(""exog is not a Series or DataFrame or is unnamed."")
    try:
        var_names = "" "".join(var_names)
    except TypeError:  # cannot have names that are numbers, pandas default
        from statsmodels.base.data import _make_exog_names
        if exog.ndim == 1:
            var_names = ""x1""
        else:
            var_names = "" "".join(_make_exog_names(exog))
    return var_names
",if exog . ndim == 1 :,177
"def __start_element_handler(self, name, attrs):
    if name == ""mime-type"":
        if self.type:
            for extension in self.extensions:
                self[extension] = self.type
        self.type = attrs[""type""].lower()
        self.extensions = []
    elif name == ""glob"":
        pattern = attrs[""pattern""]
        if pattern.startswith(""*.""):
            self.extensions.append(pattern[1:].lower())
",if self . type :,120
"def nodes(self, id=None, name=None):
    for node_dict in self.node_ls(id=id, name=name):
        node_id = node_dict[""ID""]
        node = DockerNode(self, node_id, inspect=node_dict)
        if self._node_prefix and not node.name.startswith(self._node_prefix):
            continue
        yield node
",if self . _node_prefix and not node . name . startswith ( self . _node_prefix ) :,101
"def fix_repeating_arguments(self):
    """"""Fix elements that should accumulate/increment values.""""""
    either = [list(child.children) for child in transform(self).children]
    for case in either:
        for e in [child for child in case if case.count(child) > 1]:
            if type(e) is Argument or type(e) is Option and e.argcount:
                if e.value is None:
                    e.value = []
                elif type(e.value) is not list:
                    e.value = e.value.split()
            if type(e) is Command or type(e) is Option and e.argcount == 0:
                e.value = 0
    return self
",if type ( e ) is Command or type ( e ) is Option and e . argcount == 0 :,190
"def vi_search(self, rng):
    for i in rng:
        line_history = self._history.history[i]
        pos = line_history.get_line_text().find(self._vi_search_text)
        if pos >= 0:
            self._history.history_cursor = i
            self.l_buffer.line_buffer = list(line_history.line_buffer)
            self.l_buffer.point = pos
            self.vi_undo_restart()
            return True
    self._bell()
    return False
",if pos >= 0 :,144
"def visitIf(self, node, scope):
    for test, body in node.tests:
        if isinstance(test, ast.Const):
            if type(test.value) in self._const_types:
                if not test.value:
                    continue
        self.visit(test, scope)
        self.visit(body, scope)
    if node.else_:
        self.visit(node.else_, scope)
","if isinstance ( test , ast . Const ) :",112
"def collect(self):
    for nickname in self.squid_hosts.keys():
        squid_host = self.squid_hosts[nickname]
        fulldata = self._getData(squid_host[""host""], squid_host[""port""])
        if fulldata is not None:
            fulldata = fulldata.splitlines()
            for data in fulldata:
                matches = self.stat_pattern.match(data)
                if matches:
                    self.publish_counter(
                        ""%s.%s"" % (nickname, matches.group(1)), float(matches.group(2))
                    )
",if fulldata is not None :,166
"def convert(x, base, exponents):
    out = []
    for e in exponents:
        d = int(x / (base ** e))
        x -= d * (base ** e)
        out.append(digits[d])
        if x == 0 and e < 0:
            break
    return out
",if x == 0 and e < 0 :,80
"def print_doc(manager, options):
    plugin_name = options.doc
    plugin = plugins.get(plugin_name, None)
    if plugin:
        if not plugin.instance.__doc__:
            console(""Plugin %s does not have documentation"" % plugin_name)
        else:
            console("""")
            console(trim(plugin.instance.__doc__))
            console("""")
    else:
        console(""Could not find plugin %s"" % plugin_name)
",if not plugin . instance . __doc__ :,120
"def _set_attrs(self, attrs):
    for attr in self.ATTRS:
        if attr in attrs:
            setattr(self, attr, attrs[attr])
            del attrs[attr]
        else:
            if attr == ""default"":
                setattr(self, attr, NO_DEFAULT)
            else:
                setattr(self, attr, None)
    if attrs:
        attrs = sorted(attrs.keys())
        raise OptionError(""invalid keyword arguments: %s"" % "", "".join(attrs), self)
","if attr == ""default"" :",139
"def _get_set_scope(
    ir_set: irast.Set, scope_tree: irast.ScopeTreeNode
) -> irast.ScopeTreeNode:
    if ir_set.path_scope_id:
        new_scope = scope_tree.root.find_by_unique_id(ir_set.path_scope_id)
        if new_scope is None:
            raise errors.InternalServerError(
                f""dangling scope pointer to node with uid""
                f"":{ir_set.path_scope_id} in {ir_set!r}""
            )
    else:
        new_scope = scope_tree
    return new_scope
",if new_scope is None :,172
"def test_leave_one_out(self):
    correct = 0
    k = 3
    model = kNN.train(xs, ys, k)
    predictions = [1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1]
    for i in range(len(predictions)):
        model = kNN.train(xs[:i] + xs[i + 1 :], ys[:i] + ys[i + 1 :], k)
        prediction = kNN.classify(model, xs[i])
        self.assertEqual(prediction, predictions[i])
        if prediction == ys[i]:
            correct += 1
    self.assertEqual(correct, 13)
",if prediction == ys [ i ] :,177
"def import_files(self, files):
    """"""Import a list of MORE (.csv) files.""""""
    c = self.c
    if files:
        changed = False
        self.tab_width = c.getTabWidth(c.p)
        for fileName in files:
            g.setGlobalOpenDir(fileName)
            p = self.import_file(fileName)
            if p:
                p.contract()
                p.setDirty()
                c.setChanged(True)
                changed = True
        if changed:
            c.redraw(p)
",if p :,160
"def getPageTemplate(payload, place):
    retVal = (kb.originalPage, kb.errorIsNone)
    if payload and place:
        if (payload, place) not in kb.pageTemplates:
            page, _, _ = Request.queryPage(payload, place, content=True, raise404=False)
            kb.pageTemplates[(payload, place)] = (page, kb.lastParserStatus is None)
        retVal = kb.pageTemplates[(payload, place)]
    return retVal
","if ( payload , place ) not in kb . pageTemplates :",125
"def _skip_trivial(constraint_data):
    if skip_trivial_constraints:
        if isinstance(constraint_data, LinearCanonicalRepn):
            if constraint_data.variables is None:
                return True
        else:
            if constraint_data.body.polynomial_degree() == 0:
                return True
    return False
",if constraint_data . body . polynomial_degree ( ) == 0 :,90
"def get_unique_attribute(self, name: str):
    feat = None
    for f in self.features:
        if self._return_feature(f) and hasattr(f, name):
            if feat is not None:
                raise RuntimeError(""The attribute was not unique."")
            feat = f
    if feat is None:
        raise RuntimeError(""The attribute did not exist"")
    return getattr(feat, name)
",if feat is not None :,106
"def hideEvent(self, event):
    """"""Reimplement Qt method""""""
    if not self.light:
        for plugin in self.widgetlist:
            if plugin.isAncestorOf(self.last_focused_widget):
                plugin.visibility_changed(True)
    QMainWindow.hideEvent(self, event)
",if plugin . isAncestorOf ( self . last_focused_widget ) :,87
"def move_stdout_to_stderr(self):
    to_remove = []
    to_add = []
    for consumer_level, consumer in self.consumers:
        if consumer == sys.stdout:
            to_remove.append((consumer_level, consumer))
            to_add.append((consumer_level, sys.stderr))
    for item in to_remove:
        self.consumers.remove(item)
    self.consumers.extend(to_add)
",if consumer == sys . stdout :,124
"def create(exported_python_target):
    if exported_python_target not in created:
        self.context.log.info(
            ""Creating setup.py project for {}"".format(exported_python_target)
        )
        subject = self.derived_by_original.get(
            exported_python_target, exported_python_target
        )
        setup_dir, dependencies = self.create_setup_py(subject, dist_dir)
        created[exported_python_target] = setup_dir
        if self._recursive:
            for dep in dependencies:
                if is_exported_python_target(dep):
                    create(dep)
",if self . _recursive :,172
"def __add__(self, other):
    other = ArithmeticExpression.try_unpack_const(other)
    if not self.symbolic and type(other) is int:
        return SpOffset(self._bits, self._to_signed(self.offset + other))
    else:
        if self.symbolic:
            return SpOffset(self._bits, self.offset + other)
        else:
            return SpOffset(
                self._bits,
                ArithmeticExpression(
                    ArithmeticExpression.Add,
                    (
                        self.offset,
                        other,
                    ),
                ),
            )
",if self . symbolic :,180
"def check_connection(conn):
    tables = [
        r[0]
        for r in conn.execute(
            ""select name from sqlite_master where type='table'""
        ).fetchall()
    ]
    for table in tables:
        try:
            conn.execute(
                f""PRAGMA table_info({escape_sqlite(table)});"",
            )
        except sqlite3.OperationalError as e:
            if e.args[0] == ""no such module: VirtualSpatialIndex"":
                raise SpatialiteConnectionProblem(e)
            else:
                raise ConnectionProblem(e)
","if e . args [ 0 ] == ""no such module: VirtualSpatialIndex"" :",161
"def _get_github_client(self) -> ""Github"":
    from github import Github
    if self.access_token_secret is not None:
        # If access token secret specified, load it
        access_token = Secret(self.access_token_secret).get()
    else:
        # Otherwise, fallback to loading from local secret or environment variable
        access_token = prefect.context.get(""secrets"", {}).get(""GITHUB_ACCESS_TOKEN"")
        if access_token is None:
            access_token = os.getenv(""GITHUB_ACCESS_TOKEN"")
    return Github(access_token)
",if access_token is None :,149
"def make_tab(lists):
    if hasattr(lists, ""tolist""):
        lists = lists.tolist()
    ut = []
    for rad in lists:
        if type(rad) in [list, tuple]:
            ut.append(""\t"".join([""%s"" % x for x in rad]))
        else:
            ut.append(""%s"" % rad)
    return ""\n"".join(ut)
","if type ( rad ) in [ list , tuple ] :",102
"def _ensure_ffi_initialized(cls):
    with cls._init_lock:
        if not cls._lib_loaded:
            cls.lib = build_conditional_library(lib, CONDITIONAL_NAMES)
            cls._lib_loaded = True
            # initialize the SSL library
            cls.lib.SSL_library_init()
            # adds all ciphers/digests for EVP
            cls.lib.OpenSSL_add_all_algorithms()
            # loads error strings for libcrypto and libssl functions
            cls.lib.SSL_load_error_strings()
            cls._register_osrandom_engine()
",if not cls . _lib_loaded :,162
"def writer_leaves(self):
    self.mutex.acquire()
    try:
        self.active_writers -= 1
        if self.waiting_writers != 0:
            self.active_writers += 1
            self.waiting_writers -= 1
            self.can_write.release()
        elif self.waiting_readers != 0:
            t = self.waiting_readers
            self.waiting_readers = 0
            self.active_readers += t
            while t > 0:
                self.can_read.release()
                t -= 1
    finally:
        self.mutex.release()
",elif self . waiting_readers != 0 :,171
"def _spans(self, operands):
    spans = {}
    k = 0
    j = 0
    for mode in (self.FLOAT, self.MPMATH):
        for i, operand in enumerate(operands[k:]):
            if operand[0] > mode:
                break
            j = i + k + 1
        if k == 0 and j == 1:  # only init state? then ignore.
            j = 0
        spans[mode] = slice(k, j)
        k = j
    spans[self.SYMBOLIC] = slice(k, len(operands))
    return spans
",if k == 0 and j == 1 :,152
"def _report_error(self, completion_routine, response=None, message=None):
    if response:
        # Only include the text in case of error.
        if not response.ok:
            status = location.Status(response.status_code, response.text)
        else:
            status = location.Status(response.status_code)
    else:
        status = location.Status(500, message)
    if response is None or not response.ok:
        if completion_routine:
            return completion_routine(status)
        raise IOError(response.text)
    else:
        if completion_routine:
            completion_routine(status)
    return location.Status(200, response.content)
",if not response . ok :,182
"def readinto(self, buf):
    if self.current_frame:
        n = self.current_frame.readinto(buf)
        if n == 0 and len(buf) != 0:
            self.current_frame = None
            n = len(buf)
            buf[:] = self.file_read(n)
            return n
        if n < len(buf):
            raise UnpicklingError(""pickle exhausted before end of frame"")
        return n
    else:
        n = len(buf)
        buf[:] = self.file_read(n)
        return n
",if n < len ( buf ) :,154
"def __getitem__(self, name, set=set, getattr=getattr, id=id):
    visited = set()
    mydict = self.basedict
    while 1:
        value = mydict[name]
        if value is not None:
            return value
        myid = id(mydict)
        assert myid not in visited
        visited.add(myid)
        mydict = mydict.Parent
        if mydict is None:
            return
",if value is not None :,120
"def _handle_Mul(self, expr):
    arg0, arg1 = expr.args
    expr_0 = self._expr(arg0)
    if expr_0 is None:
        return None
    expr_1 = self._expr(arg1)
    if expr_1 is None:
        return None
    try:
        if isinstance(expr_0, int) and isinstance(expr_1, int):
            # self.tyenv is not used
            mask = (1 << expr.result_size(self.tyenv)) - 1
            return (expr_0 * expr_1) & mask
        else:
            return expr_0 * expr_1
    except TypeError as e:
        self.l.warning(e)
        return None
","if isinstance ( expr_0 , int ) and isinstance ( expr_1 , int ) :",191
"def end_request(self, request_id):
    """"""Removes the information associated with given request_id.""""""
    with self._lock:
        del self._request_wsgi_environ[request_id]
        del self._request_id_to_server_configuration[request_id]
        if request_id in self._request_id_to_instance:
            del self._request_id_to_instance[request_id]
",if request_id in self . _request_id_to_instance :,105
"def generate():
    if self._gzipped:
        decoder = zlib.decompressobj(16 + zlib.MAX_WBITS)
    while True:
        chunk = self.raw.read(chunk_size)
        if not chunk:
            break
        if self._gzipped:
            chunk = decoder.decompress(chunk)
        yield chunk
",if self . _gzipped :,87
"def handle(self):
    from poetry.utils.env import EnvManager
    manager = EnvManager(self.poetry)
    current_env = manager.get()
    for venv in manager.list():
        name = venv.path.name
        if self.option(""full-path""):
            name = str(venv.path)
        if venv == current_env:
            self.line(""<info>{} (Activated)</info>"".format(name))
            continue
        self.line(name)
","if self . option ( ""full-path"" ) :",129
"def addAggregators(sheet, cols, aggrnames):
    ""Add each aggregator in list of *aggrnames* to each of *cols*.""
    for aggrname in aggrnames:
        aggrs = vd.aggregators.get(aggrname)
        aggrs = aggrs if isinstance(aggrs, list) else [aggrs]
        for aggr in aggrs:
            for c in cols:
                if not hasattr(c, ""aggregators""):
                    c.aggregators = []
                if aggr and aggr not in c.aggregators:
                    c.aggregators += [aggr]
","if not hasattr ( c , ""aggregators"" ) :",149
"def on_pre_output_coercion(
    directive_args: Dict[str, Any],
    next_directive: Callable,
    value: Any,
    ctx: Optional[Any],
    info: ""ResolveInfo"",
):
    value = await next_directive(value, ctx, info)
    if value is None:
        return value
    try:
        py_enum = _ENUM_MAP[directive_args[""name""]]
        if isinstance(value, list):
            return [None if item is None else py_enum(item).name for item in value]
        return py_enum(value).name
    except Exception:
        pass
    return value
","if isinstance ( value , list ) :",163
"def cut(sentence):
    sentence = strdecode(sentence)
    blocks = re_han.split(sentence)
    for blk in blocks:
        if re_han.match(blk):
            for word in __cut(blk):
                if word not in Force_Split_Words:
                    yield word
                else:
                    for c in word:
                        yield c
        else:
            tmp = re_skip.split(blk)
            for x in tmp:
                if x:
                    yield x
",if re_han . match ( blk ) :,156
"def refresh_archive_action(self):
    archive_name = self.selected_archive_name()
    if archive_name is not None:
        params = BorgInfoArchiveThread.prepare(self.profile(), archive_name)
        if params[""ok""]:
            thread = BorgInfoArchiveThread(params[""cmd""], params, parent=self.app)
            thread.updated.connect(self._set_status)
            thread.result.connect(self.refresh_archive_result)
            self._toggle_all_buttons(False)
            thread.start()
","if params [ ""ok"" ] :",143
"def get_resource_public_actions(resource_class):
    resource_class_members = inspect.getmembers(resource_class)
    resource_methods = {}
    for name, member in resource_class_members:
        if not name.startswith(""_""):
            if not name[0].isupper():
                if not name.startswith(""wait_until""):
                    if is_resource_action(member):
                        resource_methods[name] = member
    return resource_methods
",if is_resource_action ( member ) :,122
"def _get_compressor(compress_type, compresslevel=None):
    if compress_type == ZIP_DEFLATED:
        if compresslevel is not None:
            return zlib.compressobj(compresslevel, zlib.DEFLATED, -15)
        return zlib.compressobj(zlib.Z_DEFAULT_COMPRESSION, zlib.DEFLATED, -15)
    elif compress_type == ZIP_BZIP2:
        if compresslevel is not None:
            return bz2.BZ2Compressor(compresslevel)
        return bz2.BZ2Compressor()
    # compresslevel is ignored for ZIP_LZMA
    elif compress_type == ZIP_LZMA:
        return LZMACompressor()
    else:
        return None
",if compresslevel is not None :,169
"def parse_header(plyfile, ext):
    # Variables
    line = []
    properties = []
    num_points = None
    while b""end_header"" not in line and line != b"""":
        line = plyfile.readline()
        if b""element"" in line:
            line = line.split()
            num_points = int(line[2])
        elif b""property"" in line:
            line = line.split()
            properties.append((line[2].decode(), ext + ply_dtypes[line[1]]))
    return num_points, properties
","elif b""property"" in line :",149
"def download_release_artifacts(self, version):
    try:
        os.mkdir(self.artifacts_dir)
    except FileExistsError:
        pass
    for job_name in self.build_ids:
        build_number = self.build_ids.get(job_name)
        build_status = self._get_build_status(job_name, build_number)
        if build_status == ""built"":
            self._download_job_artifact(job_name, build_number, version)
        else:
            print(""Build for {} is not fininished"".format(job_name))
            print(""\tRun 'build' action to check status of {}"".format(job_name))
","if build_status == ""built"" :",175
"def update_metadata(self):
    for attrname in dir(self):
        if attrname.startswith(""__""):
            continue
        attrvalue = getattr(self, attrname, None)
        if attrvalue == 0:
            continue
        if attrname == ""salt_version"":
            attrname = ""version""
        if hasattr(self.metadata, ""set_{0}"".format(attrname)):
            getattr(self.metadata, ""set_{0}"".format(attrname))(attrvalue)
        elif hasattr(self.metadata, attrname):
            try:
                setattr(self.metadata, attrname, attrvalue)
            except AttributeError:
                pass
","if attrname . startswith ( ""__"" ) :",173
"def check_heuristic_in_sql():
    heurs = set()
    excluded = [""Equal assembly or pseudo-code"", ""All or most attributes""]
    for heur in HEURISTICS:
        name = heur[""name""]
        if name in excluded:
            continue
        sql = heur[""sql""]
        if sql.lower().find(name.lower()) == -1:
            print((""SQL command not correctly associated to %s"" % repr(name)))
            print(sql)
            assert sql.find(name) != -1
        heurs.add(name)
    print(""Heuristics:"")
    import pprint
    pprint.pprint(heurs)
",if sql . lower ( ) . find ( name . lower ( ) ) == - 1 :,171
"def gettext(rv):
    for child in rv.childNodes:
        if child.nodeType == child.TEXT_NODE:
            yield child.nodeValue
        if child.nodeType == child.ELEMENT_NODE:
            for item in gettext(child):
                yield item
",if child . nodeType == child . TEXT_NODE :,73
"def update(self):
    """"""Update properties over dbus.""""""
    self._check_dbus()
    _LOGGER.info(""Updating service information"")
    self._services.clear()
    try:
        systemd_units = await self.sys_dbus.systemd.list_units()
        for service_data in systemd_units[0]:
            if not service_data[0].endswith("".service"") or service_data[2] != ""loaded"":
                continue
            self._services.add(ServiceInfo.read_from(service_data))
    except (HassioError, IndexError):
        _LOGGER.warning(""Can't update host service information!"")
","if not service_data [ 0 ] . endswith ( "".service"" ) or service_data [ 2 ] != ""loaded"" :",161
"def filtercomments(source):
    """"""NOT USED: strips trailing comments and put them at the top.""""""
    trailing_comments = []
    comment = True
    while comment:
        if re.search(r""^\s*\/\*"", source):
            comment = source[0, source.index(""*/"") + 2]
        elif re.search(r""^\s*\/\/"", source):
            comment = re.search(r""^\s*\/\/"", source).group(0)
        else:
            comment = None
        if comment:
            source = re.sub(r""^\s+"", """", source[len(comment) :])
            trailing_comments.append(comment)
    return ""\n"".join(trailing_comments) + source
","if re . search ( r""^\s*\/\*"" , source ) :",179
"def _getSourceStamp_sync(self, ssid):
    if ssid in self.sourcestamps:
        ssdict = self.sourcestamps[ssid].copy()
        ssdict[""ssid""] = ssid
        patchid = ssdict[""patchid""]
        if patchid:
            ssdict.update(self.patches[patchid])
            ssdict[""patchid""] = patchid
        else:
            ssdict[""patch_body""] = None
            ssdict[""patch_level""] = None
            ssdict[""patch_subdir""] = None
            ssdict[""patch_author""] = None
            ssdict[""patch_comment""] = None
        return ssdict
    else:
        return None
",if patchid :,184
"def parseImpl(self, instring, loc, doActions=True):
    try:
        loc, tokens = self.expr._parse(instring, loc, doActions, callPreParse=False)
    except (ParseException, IndexError):
        if self.defaultValue is not self.__optionalNotMatched:
            if self.expr.resultsName:
                tokens = ParseResults([self.defaultValue])
                tokens[self.expr.resultsName] = self.defaultValue
            else:
                tokens = [self.defaultValue]
        else:
            tokens = []
    return loc, tokens
",if self . expr . resultsName :,157
"def _find_exceptions():
    for _name, obj in iteritems(globals()):
        try:
            is_http_exception = issubclass(obj, HTTPException)
        except TypeError:
            is_http_exception = False
        if not is_http_exception or obj.code is None:
            continue
        __all__.append(obj.__name__)
        old_obj = default_exceptions.get(obj.code, None)
        if old_obj is not None and issubclass(obj, old_obj):
            continue
        default_exceptions[obj.code] = obj
","if old_obj is not None and issubclass ( obj , old_obj ) :",148
"def generator(self, data):
    for (proc_as, key_buf_ptr) in data:
        key_buf = proc_as.read(key_buf_ptr, 24)
        if not key_buf:
            continue
        key = """".join(""%02X"" % ord(k) for k in key_buf)
        yield (
            0,
            [
                str(key),
            ],
        )
",if not key_buf :,117
"def calculateEnableMargins(self):
    self.cnc.resetEnableMargins()
    for block in self.blocks:
        if block.enable:
            CNC.vars[""xmin""] = min(CNC.vars[""xmin""], block.xmin)
            CNC.vars[""ymin""] = min(CNC.vars[""ymin""], block.ymin)
            CNC.vars[""zmin""] = min(CNC.vars[""zmin""], block.zmin)
            CNC.vars[""xmax""] = max(CNC.vars[""xmax""], block.xmax)
            CNC.vars[""ymax""] = max(CNC.vars[""ymax""], block.ymax)
            CNC.vars[""zmax""] = max(CNC.vars[""zmax""], block.zmax)
",if block . enable :,191
"def __init__(self, client, job_id, callback=None):
    self.client = client
    self.job_id = job_id
    # If a job event has been received already then we must set an Event
    # to wait for this job to finish.
    # Otherwise we create a new stub for the job with the Event for when
    # the job event arrives to use existing event.
    with client._jobs_lock:
        job = client._jobs.get(job_id)
        self.event = None
        if job:
            self.event = job.get(""__ready"")
        if self.event is None:
            self.event = job[""__ready""] = Event()
        job[""__callback""] = callback
",if job :,180
"def asset(*paths):
    for path in paths:
        fspath = www_root + ""/assets/"" + path
        etag = """"
        try:
            if env.cache_static:
                etag = asset_etag(fspath)
            else:
                os.stat(fspath)
        except FileNotFoundError as e:
            if path == paths[-1]:
                if not os.path.exists(fspath + "".spt""):
                    tell_sentry(e, {})
            else:
                continue
        except Exception as e:
            tell_sentry(e, {})
        return asset_url + path + (etag and ""?etag="" + etag)
","if not os . path . exists ( fspath + "".spt"" ) :",182
"def set_conf():
    """"""Collapse all object_trail config into cherrypy.request.config.""""""
    base = cherrypy.config.copy()
    # Note that we merge the config from each node
    # even if that node was None.
    for name, obj, conf, segleft in object_trail:
        base.update(conf)
        if ""tools.staticdir.dir"" in conf:
            base[""tools.staticdir.section""] = ""/"" + ""/"".join(
                fullpath[0 : fullpath_len - segleft]
            )
    return base
","if ""tools.staticdir.dir"" in conf :",145
"def __init__(self):
    self.setLayers(None, None)
    self.interface = None
    self.event_callbacks = {}
    self.__stack = None
    self.lock = threading.Lock()
    members = inspect.getmembers(self, predicate=inspect.ismethod)
    for m in members:
        if hasattr(m[1], ""event_callback""):
            fname = m[0]
            fn = m[1]
            self.event_callbacks[fn.event_callback] = getattr(self, fname)
","if hasattr ( m [ 1 ] , ""event_callback"" ) :",133
"def multi_dev_generator(self):
    for data in self._data_loader():
        if len(self._tail_data) < self._base_number:
            self._tail_data += data
        if len(self._tail_data) == self._base_number:
            yield self._tail_data
            self._tail_data = []
",if len ( self . _tail_data ) < self . _base_number :,91
"def replace_field_to_value(layout, cb):
    for i, lo in enumerate(layout.fields):
        if isinstance(lo, Field) or issubclass(lo.__class__, Field):
            layout.fields[i] = ShowField(
                cb, *lo.fields, attrs=lo.attrs, wrapper_class=lo.wrapper_class
            )
        elif isinstance(lo, basestring):
            layout.fields[i] = ShowField(cb, lo)
        elif hasattr(lo, ""get_field_names""):
            replace_field_to_value(lo, cb)
","elif hasattr ( lo , ""get_field_names"" ) :",151
"def function_out(*args, **kwargs):
    try:
        return function_in(*args, **kwargs)
    except dbus.exceptions.DBusException as e:
        if e.get_dbus_name() == DBUS_UNKNOWN_METHOD:
            raise ItemNotFoundException(""Item does not exist!"")
        if e.get_dbus_name() == DBUS_NO_SUCH_OBJECT:
            raise ItemNotFoundException(e.get_dbus_message())
        if e.get_dbus_name() in (DBUS_NO_REPLY, DBUS_NOT_SUPPORTED):
            raise SecretServiceNotAvailableException(e.get_dbus_message())
        raise
","if e . get_dbus_name ( ) in ( DBUS_NO_REPLY , DBUS_NOT_SUPPORTED ) :",162
"def results_iter(self):
    if self.connection.ops.oracle:
        from django.db.models.fields import DateTimeField
        fields = [DateTimeField()]
    else:
        needs_string_cast = self.connection.features.needs_datetime_string_cast
    offset = len(self.query.extra_select)
    for rows in self.execute_sql(MULTI):
        for row in rows:
            date = row[offset]
            if self.connection.ops.oracle:
                date = self.resolve_columns(row, fields)[offset]
            elif needs_string_cast:
                date = typecast_timestamp(str(date))
            yield date
",elif needs_string_cast :,182
"def handle_label(self, path, **options):
    verbosity = int(options.get(""verbosity"", 1))
    result = finders.find(path, all=options[""all""])
    path = smart_unicode(path)
    if result:
        if not isinstance(result, (list, tuple)):
            result = [result]
        output = u""\n  "".join(
            (smart_unicode(os.path.realpath(path)) for path in result)
        )
        self.stdout.write(smart_str(u""Found '%s' here:\n  %s\n"" % (path, output)))
    else:
        if verbosity >= 1:
            self.stderr.write(smart_str(""No matching file found for '%s'.\n"" % path))
",if verbosity >= 1 :,193
"def name(self):
    """"""Get the enumeration name of this storage class.""""""
    if self._name_map is None:
        self._name_map = {}
        for key, value in list(StorageClass.__dict__.items()):
            if isinstance(value, StorageClass):
                self._name_map[value] = key
    return self._name_map[self]
","if isinstance ( value , StorageClass ) :",94
"def index(self, value):
    if self._growing:
        if self._start <= value < self._stop:
            q, r = divmod(value - self._start, self._step)
            if r == self._zero:
                return int(q)
    else:
        if self._start >= value > self._stop:
            q, r = divmod(self._start - value, -self._step)
            if r == self._zero:
                return int(q)
    raise ValueError(""{} is not in numeric range"".format(value))
",if self . _start >= value > self . _stop :,146
"def extract_cookie(cookie_header, cookie_name):
    inx = cookie_header.find(cookie_name)
    if inx >= 0:
        end_inx = cookie_header.find("";"", inx)
        if end_inx > 0:
            value = cookie_header[inx:end_inx]
        else:
            value = cookie_header[inx:]
        return value
    return """"
",if end_inx > 0 :,104
"def get_size(self, shape_info):
    # The size is the data, that have constant size.
    state = np.random.RandomState().get_state()
    size = 0
    for elem in state:
        if isinstance(elem, str):
            size += len(elem)
        elif isinstance(elem, np.ndarray):
            size += elem.size * elem.itemsize
        elif isinstance(elem, int):
            size += np.dtype(""int"").itemsize
        elif isinstance(elem, float):
            size += np.dtype(""float"").itemsize
        else:
            raise NotImplementedError()
    return size
","elif isinstance ( elem , np . ndarray ) :",159
"def createFields(self):
    size = self.size / 8
    if size > 2:
        if size > 4:
            yield UInt8(self, ""cs"", ""10ms units, values from 0 to 199"")
        yield Bits(self, ""2sec"", 5, ""seconds/2"")
        yield Bits(self, ""min"", 6, ""minutes"")
        yield Bits(self, ""hour"", 5, ""hours"")
    yield Bits(self, ""day"", 5, ""(1-31)"")
    yield Bits(self, ""month"", 4, ""(1-12)"")
    yield Bits(self, ""year"", 7, ""(0 = 1980, 127 = 2107)"")
",if size > 4 :,169
"def detect(get_page):
    retval = False
    for vector in WAF_ATTACK_VECTORS:
        page, headers, code = get_page(get=vector)
        retval = (
            re.search(
                r""incap_ses|visid_incap"", headers.get(HTTP_HEADER.SET_COOKIE, """"), re.I
            )
            is not None
        )
        retval |= re.search(r""Incapsula"", headers.get(""X-CDN"", """"), re.I) is not None
        if retval:
            break
    return retval
",if retval :,157
"def _get_order_information(self, node_id, timeout=1200, check_interval=5):
    mask = {
        ""billingItem"": """",
        ""powerState"": """",
        ""operatingSystem"": {""passwords"": """"},
        ""provisionDate"": """",
    }
    for i in range(0, timeout, check_interval):
        res = self.connection.request(
            ""SoftLayer_Virtual_Guest"", ""getObject"", id=node_id, object_mask=mask
        ).object
        if res.get(""provisionDate"", None):
            return res
        time.sleep(check_interval)
    raise SoftLayerException(""Timeout on getting node details"")
","if res . get ( ""provisionDate"" , None ) :",170
"def _process_param_change(self, msg):
    msg = super(Select, self)._process_param_change(msg)
    labels, values = self.labels, self.values
    if ""value"" in msg:
        msg[""value""] = [
            labels[indexOf(v, values)] for v in msg[""value""] if isIn(v, values)
        ]
    if ""options"" in msg:
        msg[""options""] = labels
        if any(not isIn(v, values) for v in self.value):
            self.value = [v for v in self.value if isIn(v, values)]
    return msg
","if any ( not isIn ( v , values ) for v in self . value ) :",161
"def get_object_from_name(self, name, check_symlinks=True):
    if not name:
        return None
    name = name.rstrip(""\\"")
    for a, o in self.objects.items():
        if not o.name:
            continue
        if o.name.lower() == name.lower():
            return o
    if check_symlinks:
        m = [sl[1] for sl in self.symlinks if name.lower() == sl[0].lower()]
        if m:
            name = m[0]
        return self.get_object_from_name(name, False)
",if m :,156
"def run(self):
    for k, v in iteritems(self.objs):
        if k.startswith(""_""):
            continue
        if v[""_class""] == ""User"":
            if v[""email""] == """":
                v[""email""] = None
            if v[""ip""] == ""0.0.0.0"":
                v[""ip""] = None
    return self.objs
","if k . startswith ( ""_"" ) :",102
"def _providers(self, descriptor):
    res = []
    for _md in self.metadata.values():
        for ent_id, ent_desc in _md.items():
            if descriptor in ent_desc:
                if ent_id in res:
                    # print(""duplicated entity_id: %s"" % res)
                    pass
                else:
                    res.append(ent_id)
    return res
",if ent_id in res :,118
"def test_add_participant(self):
    async with self.chat_client:
        await self._create_thread()
        async with self.chat_thread_client:
            share_history_time = datetime.utcnow()
            share_history_time = share_history_time.replace(tzinfo=TZ_UTC)
            new_participant = ChatThreadParticipant(
                user=self.new_user,
                display_name=""name"",
                share_history_time=share_history_time,
            )
            await self.chat_thread_client.add_participant(new_participant)
        if not self.is_playback():
            await self.chat_client.delete_chat_thread(self.thread_id)
",if not self . is_playback ( ) :,197
"def url(regex, view, kwargs=None, name=None, prefix=""""):
    if isinstance(view, (list, tuple)):
        # For include(...) processing.
        urlconf_module, app_name, namespace = view
        return RegexURLResolver(
            regex, urlconf_module, kwargs, app_name=app_name, namespace=namespace
        )
    else:
        if isinstance(view, basestring):
            if not view:
                raise ImproperlyConfigured(
                    ""Empty URL pattern view name not permitted (for pattern %r)"" % regex
                )
            if prefix:
                view = prefix + ""."" + view
        return RegexURLPattern(regex, view, kwargs, name)
",if prefix :,183
"def tx():
    # Sync receiver ready to avoid loss of first packets
    while not sub_ready.ready():
        pub.send(b""test BEGIN"")
        eventlet.sleep(0.005)
    for i in range(1, 101):
        msg = ""test {0}"".format(i).encode()
        if i != 50:
            pub.send(msg)
        else:
            pub.send(b""test LAST"")
            sub_last.wait()
        # XXX: putting a real delay of 1ms here fixes sporadic failures on Travis
        # just yield eventlet.sleep(0) doesn't cut it
        eventlet.sleep(0.001)
    pub.send(b""done DONE"")
",if i != 50 :,185
"def remove_tmp_snapshot_file(self, files):
    for filepath in files:
        path = Path(filepath)
        if path.is_dir() and path.exists():
            shutil.rmtree(path)
        elif path.is_file() and path.exists():
            path.unlink()
",elif path . is_file ( ) and path . exists ( ) :,78
"def f(view, s):
    if mode == modes.INTERNAL_NORMAL:
        if count == 1:
            if view.line(s.b).size() > 0:
                eol = view.line(s.b).b
                return R(s.b, eol)
            return s
    return s
",if view . line ( s . b ) . size ( ) > 0 :,85
"def get_ids(self, **kwargs):
    id = []
    if ""id"" in kwargs:
        id = kwargs[""id""]
        # Coerce ids to list
        if not isinstance(id, list):
            id = id.split("","")
        # Ensure ids are integers
        try:
            id = list(map(int, id))
        except Exception:
            decorators.error(""Invalid id"")
    return id
","if not isinstance ( id , list ) :",111
"def param_value(self):
    # This is part of the ""handle quoted extended parameters"" hack.
    for token in self:
        if token.token_type == ""value"":
            return token.stripped_value
        if token.token_type == ""quoted-string"":
            for token in token:
                if token.token_type == ""bare-quoted-string"":
                    for token in token:
                        if token.token_type == ""value"":
                            return token.stripped_value
    return """"
","if token . token_type == ""value"" :",143
"def get_all_start_methods(self):
    if sys.platform == ""win32"":
        return [""spawn""]
    else:
        methods = [""spawn"", ""fork""] if sys.platform == ""darwin"" else [""fork"", ""spawn""]
        if reduction.HAVE_SEND_HANDLE:
            methods.append(""forkserver"")
        return methods
",if reduction . HAVE_SEND_HANDLE :,88
"def _process_watch(self, watched_event):
    logger.debug(""process_watch: %r"", watched_event)
    with handle_exception(self._tree._error_listeners):
        if watched_event.type == EventType.CREATED:
            assert self._parent is None, ""unexpected CREATED on non-root""
            self.on_created()
        elif watched_event.type == EventType.DELETED:
            self.on_deleted()
        elif watched_event.type == EventType.CHANGED:
            self._refresh_data()
        elif watched_event.type == EventType.CHILD:
            self._refresh_children()
",if watched_event . type == EventType . CREATED :,172
"def assert_open(self, sock, *rest):
    if isinstance(sock, fd_types):
        self.__assert_fd_open(sock)
    else:
        fileno = sock.fileno()
        assert isinstance(fileno, fd_types), fileno
        sockname = sock.getsockname()
        assert isinstance(sockname, tuple), sockname
        if not WIN:
            self.__assert_fd_open(fileno)
        else:
            self._assert_sock_open(sock)
    if rest:
        self.assert_open(rest[0], *rest[1:])
",if not WIN :,148
"def detype(self):
    """"""De-types the instance, allowing it to be exported to the environment.""""""
    style = self.style
    if self._detyped is None:
        self._detyped = "":"".join(
            [
                key
                + ""=""
                + "";"".join(
                    [
                        LsColors.target_value
                        if key in self._targets
                        else ansi_color_name_to_escape_code(v, cmap=style)
                        for v in val
                    ]
                )
                for key, val in sorted(self._d.items())
            ]
        )
    return self._detyped
",if key in self . _targets,198
"def gather_metrics(dry_run=False):
    today = datetime.date.today()
    first = today.replace(day=1)
    last_month = first - datetime.timedelta(days=1)
    filename = ""form_types_{}.csv"".format(last_month.strftime(""%Y-%m""))
    with connection.cursor() as cursor:
        cursor.execute(REGISTRATION_METRICS_SQL)
        if dry_run:
            for row in cursor.fetchall():
                logger.info(encode_row(row))
        else:
            write_raw_data(cursor=cursor, filename=filename)
",if dry_run :,154
"def cat(tensors, dim=0):
    assert isinstance(tensors, list), ""input to cat must be a list""
    if len(tensors) == 1:
        return tensors[0]
    from .autograd_cryptensor import AutogradCrypTensor
    if any(isinstance(t, AutogradCrypTensor) for t in tensors):
        if not isinstance(tensors[0], AutogradCrypTensor):
            tensors[0] = AutogradCrypTensor(tensors[0], requires_grad=False)
        return tensors[0].cat(*tensors[1:], dim=dim)
    else:
        return get_default_backend().cat(tensors, dim=dim)
","if not isinstance ( tensors [ 0 ] , AutogradCrypTensor ) :",166
"def is_installed(self, dlc_title="""") -> bool:
    installed = False
    if dlc_title:
        dlc_version = self.get_dlc_info(""version"", dlc_title)
        installed = True if dlc_version else False
        # Start: Code for compatibility with minigalaxy 1.0
        if not installed:
            status = self.legacy_get_dlc_status(dlc_title)
            installed = True if status in [""installed"", ""updatable""] else False
        # End: Code for compatibility with minigalaxy 1.0
    else:
        if self.install_dir and os.path.exists(self.install_dir):
            installed = True
    return installed
",if self . install_dir and os . path . exists ( self . install_dir ) :,178
"def on_copy(self):
    source_objects = self.__getSelection()
    for source in source_objects:
        if isinstance(source, model.Phrase):
            new_obj = model.Phrase("""", """")
        else:
            new_obj = model.Script("""", """")
        new_obj.copy(source)
        self.cutCopiedItems.append(new_obj)
","if isinstance ( source , model . Phrase ) :",100
"def FetchFn(type_name):
    """"""Fetches all hunt results of a given type.""""""
    offset = 0
    while True:
        results = data_store.REL_DB.ReadHuntResults(
            hunt_id, offset=offset, count=self._RESULTS_PAGE_SIZE, with_type=type_name
        )
        if not results:
            break
        for r in results:
            msg = r.AsLegacyGrrMessage()
            msg.source_urn = source_urn
            yield msg
        offset += self._RESULTS_PAGE_SIZE
",if not results :,149
"def get_blob_type_declaration_sql(self, column):
    length = column.get(""length"")
    if length:
        if length <= self.LENGTH_LIMIT_TINYBLOB:
            return ""TINYBLOB""
        if length <= self.LENGTH_LIMIT_BLOB:
            return ""BLOB""
        if length <= self.LENGTH_LIMIT_MEDIUMBLOB:
            return ""MEDIUMBLOB""
    return ""LONGBLOB""
",if length <= self . LENGTH_LIMIT_TINYBLOB :,115
"def decode(cls, data):
    while data:
        (
            length,
            atype,
        ) = unpack(cls.Header.PACK, data[: cls.Header.LEN])
        if len(data) < length:
            raise AttributesError(""Buffer underrun %d < %d"" % (len(data), length))
        payload = data[cls.Header.LEN : length]
        yield atype, payload
        data = data[int((length + 3) / 4) * 4 :]
",if len ( data ) < length :,128
"def test_join_diffs(db, series_of_diffs, expected):
    diffs = []
    for changes in series_of_diffs:
        tracker = DBDiffTracker()
        for key, val in changes.items():
            if val is None:
                del tracker[key]
            else:
                tracker[key] = val
        diffs.append(tracker.diff())
    DBDiff.join(diffs).apply_to(db)
    assert db == expected
",if val is None :,123
"def ant_map(m):
    tmp = ""rows %s\ncols %s\n"" % (len(m), len(m[0]))
    players = {}
    for row in m:
        tmp += ""m ""
        for col in row:
            if col == LAND:
                tmp += "".""
            elif col == BARRIER:
                tmp += ""%""
            elif col == FOOD:
                tmp += ""*""
            elif col == UNSEEN:
                tmp += ""?""
            else:
                players[col] = True
                tmp += chr(col + 97)
        tmp += ""\n""
    tmp = (""players %s\n"" % len(players)) + tmp
    return tmp
",elif col == UNSEEN :,199
"def _report_error(self, completion_routine, response=None, message=None):
    if response:
        # Only include the text in case of error.
        if not response.ok:
            status = location.Status(response.status_code, response.text)
        else:
            status = location.Status(response.status_code)
    else:
        status = location.Status(500, message)
    if response is None or not response.ok:
        if completion_routine:
            return completion_routine(status)
        raise IOError(response.text)
    else:
        if completion_routine:
            completion_routine(status)
    return location.Status(200, response.content)
",if completion_routine :,182
"def _generate_examples(self, src_path=None, tgt_path=None, replace_unk=None):
    """"""Yields examples.""""""
    with tf.io.gfile.GFile(src_path) as f_d, tf.io.gfile.GFile(tgt_path) as f_s:
        for i, (doc_text, sum_text) in enumerate(zip(f_d, f_s)):
            if replace_unk:
                yield i, {
                    _DOCUMENT: doc_text.strip().replace(""<unk>"", ""UNK""),
                    _SUMMARY: sum_text.strip().replace(""<unk>"", ""UNK""),
                }
            else:
                yield i, {_DOCUMENT: doc_text.strip(), _SUMMARY: sum_text.strip()}
",if replace_unk :,198
"def escape(text, newline=False):
    """"""Escape special html characters.""""""
    if isinstance(text, str):
        if ""&"" in text:
            text = text.replace(""&"", ""&amp;"")
        if "">"" in text:
            text = text.replace("">"", ""&gt;"")
        if ""<"" in text:
            text = text.replace(""<"", ""&lt;"")
        if '""' in text:
            text = text.replace('""', ""&quot;"")
        if ""'"" in text:
            text = text.replace(""'"", ""&quot;"")
        if newline:
            if ""\n"" in text:
                text = text.replace(""\n"", ""<br>"")
    return text
",if newline :,170
"def _handle_url_click(self, event):
    url = _extract_click_text(self.info_text, event, ""url"")
    if url is not None:
        if url.startswith(""http:"") or url.startswith(""https:""):
            import webbrowser
            webbrowser.open(url)
        elif os.path.sep in url:
            os.makedirs(url, exist_ok=True)
            open_path_in_system_file_manager(url)
        else:
            self._start_show_package_info(url)
","if url . startswith ( ""http:"" ) or url . startswith ( ""https:"" ) :",144
"def SConsignFile(self, name="".sconsign"", dbm_module=None):
    if name is not None:
        name = self.subst(name)
        if not os.path.isabs(name):
            name = os.path.join(str(self.fs.SConstruct_dir), name)
    if name:
        name = os.path.normpath(name)
        sconsign_dir = os.path.dirname(name)
        if sconsign_dir and not os.path.exists(sconsign_dir):
            self.Execute(SCons.Defaults.Mkdir(sconsign_dir))
    SCons.SConsign.File(name, dbm_module)
",if not os . path . isabs ( name ) :,178
"def on_train_start(self, trainer: Trainer, pl_module: LightningModule) -> None:
    super().on_train_start(trainer, pl_module)
    submodule_dict = dict(pl_module.named_modules())
    self._hook_handles = []
    for name in self._get_submodule_names(pl_module):
        if name not in submodule_dict:
            rank_zero_warn(
                f""{name} is not a valid identifier for a submodule in {pl_module.__class__.__name__},""
                "" skipping this key.""
            )
            continue
        handle = self._register_hook(name, submodule_dict[name])
        self._hook_handles.append(handle)
",if name not in submodule_dict :,184
"def validate_configuration(self, configuration: Optional[ExpectationConfiguration]):
    super().validate_configuration(configuration)
    if configuration is None:
        configuration = self.configuration
    try:
        assert ""value_set"" in configuration.kwargs, ""value_set is required""
        assert isinstance(
            configuration.kwargs[""value_set""], (list, set, dict)
        ), ""value_set must be a list or a set""
        if isinstance(configuration.kwargs[""value_set""], dict):
            assert (
                ""$PARAMETER"" in configuration.kwargs[""value_set""]
            ), 'Evaluation Parameter dict for value_set kwarg must have ""$PARAMETER"" key.'
    except AssertionError as e:
        raise InvalidExpectationConfigurationError(str(e))
    return True
","if isinstance ( configuration . kwargs [ ""value_set"" ] , dict ) :",191
"def check_refcounts(expected, timeout=10):
    start = time.time()
    while True:
        try:
            _check_refcounts(expected)
            break
        except AssertionError as e:
            if time.time() - start > timeout:
                raise e
            else:
                time.sleep(0.1)
",if time . time ( ) - start > timeout :,96
"def pickline(file, key, casefold=1):
    try:
        f = open(file, ""r"")
    except IOError:
        return None
    pat = re.escape(key) + "":""
    prog = re.compile(pat, casefold and re.IGNORECASE)
    while 1:
        line = f.readline()
        if not line:
            break
        if prog.match(line):
            text = line[len(key) + 1 :]
            while 1:
                line = f.readline()
                if not line or not line[0].isspace():
                    break
                text = text + line
            return text.strip()
    return None
",if not line :,182
"def _is_perf_file(file_path):
    f = get_file(file_path)
    for line in f:
        if line[0] == ""#"":
            continue
        r = event_regexp.search(line)
        if r:
            f.close()
            return True
        f.close()
        return False
","if line [ 0 ] == ""#"" :",92
"def link_pantsrefs(soups, precomputed):
    """"""Transorm soups: <a pantsref=""foo""> becomes <a href=""../foo_page.html#foo"">""""""
    for (page, soup) in soups.items():
        for a in soup.find_all(""a""):
            if not a.has_attr(""pantsref""):
                continue
            pantsref = a[""pantsref""]
            if pantsref not in precomputed.pantsref:
                raise TaskError(
                    f'Page {page} has pantsref ""{pantsref}"" and I cannot find pantsmark for it'
                )
            a[""href""] = rel_href(page, precomputed.pantsref[pantsref])
","if not a . has_attr ( ""pantsref"" ) :",194
"def __init__(self, querylist=None):
    self.query_id = -1
    if querylist is None:
        self.querylist = []
    else:
        self.querylist = querylist
        for query in self.querylist:
            if self.query_id == -1:
                self.query_id = query.query_id
            else:
                if self.query_id != query.query_id:
                    raise ValueError(""query in list must be same query_id"")
",if self . query_id == - 1 :,137
"def _draw_number(
    screen, x_offset, y_offset, number, token=Token.Clock, transparent=False
):
    ""Write number at position.""
    fg = Char("" "", token)
    bg = Char("" "", Token)
    for y, row in enumerate(_numbers[number]):
        screen_row = screen.data_buffer[y + y_offset]
        for x, n in enumerate(row):
            if n == ""#"":
                screen_row[x + x_offset] = fg
            elif not transparent:
                screen_row[x + x_offset] = bg
","if n == ""#"" :",154
"def init(self):
    self.sock.setblocking(True)
    if self.parser is None:
        # wrap the socket if needed
        if self.cfg.is_ssl:
            self.sock = ssl.wrap_socket(
                self.sock, server_side=True, **self.cfg.ssl_options
            )
        # initialize the parser
        self.parser = http.RequestParser(self.cfg, self.sock)
",if self . cfg . is_ssl :,116
"def intersect_face(pt):
    # todo: rewrite! inefficient!
    nonlocal vis_faces2D
    for f, vs in vis_faces2D:
        v0 = vs[0]
        for v1, v2 in iter_pairs(vs[1:], False):
            if intersect_point_tri_2d(pt, v0, v1, v2):
                return f
    return None
","if intersect_point_tri_2d ( pt , v0 , v1 , v2 ) :",104
"def IMPORTFROM(self, node):
    if node.module == ""__future__"":
        if not self.futuresAllowed:
            self.report(messages.LateFutureImport, node, [n.name for n in node.names])
    else:
        self.futuresAllowed = False
    for alias in node.names:
        if alias.name == ""*"":
            self.scope.importStarred = True
            self.report(messages.ImportStarUsed, node, node.module)
            continue
        name = alias.asname or alias.name
        importation = Importation(name, node)
        if node.module == ""__future__"":
            importation.used = (self.scope, node)
        self.addBinding(node, importation)
","if alias . name == ""*"" :",190
"def PyObject_Bytes(obj):
    if type(obj) == bytes:
        return obj
    if hasattr(obj, ""__bytes__""):
        res = obj.__bytes__()
        if not isinstance(res, bytes):
            raise TypeError(
                ""__bytes__ returned non-bytes (type %s)"" % type(res).__name__
            )
    return PyBytes_FromObject(obj)
","if not isinstance ( res , bytes ) :",99
"def on_bt_search_clicked(self, widget):
    if self.current_provider is None:
        return
    query = self.en_query.get_text()
    @self.obtain_podcasts_with
    def load_data():
        if self.current_provider.kind == directory.Provider.PROVIDER_SEARCH:
            return self.current_provider.on_search(query)
        elif self.current_provider.kind == directory.Provider.PROVIDER_URL:
            return self.current_provider.on_url(query)
        elif self.current_provider.kind == directory.Provider.PROVIDER_FILE:
            return self.current_provider.on_file(query)
",elif self . current_provider . kind == directory . Provider . PROVIDER_FILE :,172
"def remove(self, name):
    for s in [self.__storage(self.__category), self.__storage(None)]:
        for i, b in enumerate(s):
            if b.name == name:
                del s[i]
                if b.persistent:
                    self.__save()
                return
    raise KeyError(name)
",if b . name == name :,94
"def _wrapper(data, axis=None, keepdims=False):
    if not keepdims:
        return func(data, axis=axis)
    else:
        if axis is not None:
            axis = axis if isinstance(axis, int) else axis[0]
            out_shape = list(data.shape)
            out_shape[axis] = 1
        else:
            out_shape = [1 for _ in range(len(data.shape))]
        return func(data, axis=axis).reshape(out_shape)
",if axis is not None :,135
"def authn_info(self):
    res = []
    for astat in self.assertion.authn_statement:
        context = astat.authn_context
        try:
            authn_instant = astat.authn_instant
        except AttributeError:
            authn_instant = """"
        if context:
            try:
                aclass = context.authn_context_class_ref.text
            except AttributeError:
                aclass = """"
            try:
                authn_auth = [a.text for a in context.authenticating_authority]
            except AttributeError:
                authn_auth = []
            res.append((aclass, authn_auth, authn_instant))
    return res
",if context :,199
"def _persist_metadata(self, dirname, filename):
    metadata_path = ""{0}/{1}.json"".format(dirname, filename)
    if self.media_metadata or self.comments or self.include_location:
        if self.posts:
            if self.latest:
                self.merge_json({""GraphImages"": self.posts}, metadata_path)
            else:
                self.save_json({""GraphImages"": self.posts}, metadata_path)
        if self.stories:
            if self.latest:
                self.merge_json({""GraphStories"": self.stories}, metadata_path)
            else:
                self.save_json({""GraphStories"": self.stories}, metadata_path)
",if self . latest :,190
"def update_record_image_detail(input_image_record, updated_image_detail, session=None):
    if not session:
        session = db.Session
    image_record = {}
    image_record.update(input_image_record)
    image_record.pop(""created_at"", None)
    image_record.pop(""last_updated"", None)
    if image_record[""image_type""] == ""docker"":
        for tag_record in updated_image_detail:
            if tag_record not in image_record[""image_detail""]:
                image_record[""image_detail""].append(tag_record)
                return update_record(image_record, session=session)
    return image_record
","if tag_record not in image_record [ ""image_detail"" ] :",179
"def backup(self):
    for ds in [(""activedirectory"", ""AD""), (""ldap"", ""LDAP""), (""nis"", ""NIS"")]:
        if (self.middleware.call_sync(f""{ds[0]}.config""))[""enable""]:
            try:
                ds_cache = self.middleware.call_sync(""cache.get"", f""{ds[1]}_cache"")
                with open(f""/var/db/system/.{ds[1]}_cache_backup"", ""wb"") as f:
                    pickle.dump(ds_cache, f)
            except KeyError:
                self.logger.debug(""No cache exists for directory service [%s]."", ds[0])
","if ( self . middleware . call_sync ( f""{ds[0]}.config"" ) ) [ ""enable"" ] :",175
"def parse_setup_cfg(self):
    # type: () -> Dict[STRING_TYPE, Any]
    if self.setup_cfg is not None and self.setup_cfg.exists():
        contents = self.setup_cfg.read_text()
        base_dir = self.setup_cfg.absolute().parent.as_posix()
        try:
            parsed = setuptools_parse_setup_cfg(self.setup_cfg.as_posix())
        except Exception:
            if six.PY2:
                contents = self.setup_cfg.read_bytes()
            parsed = parse_setup_cfg(contents, base_dir)
        if not parsed:
            return {}
        return parsed
    return {}
",if not parsed :,183
"def parts():
    for l in lists.leaves:
        head_name = l.get_head_name()
        if head_name == ""System`List"":
            yield l.leaves
        elif head_name != ""System`Missing"":
            raise MessageException(""Catenate"", ""invrp"", l)
","if head_name == ""System`List"" :",78
"def _get_callback_and_order(self, hook):
    if callable(hook):
        return hook, None
    elif isinstance(hook, tuple) and len(hook) == 2:
        callback, order = hook
        # test that callback is a callable
        if not callable(callback):
            raise ValueError(""Hook callback is not a callable"")
        # test that number is an int
        try:
            int(order)
        except ValueError:
            raise ValueError(""Hook order is not a number"")
        return callback, order
    else:
        raise ValueError(
            ""Invalid hook definition, neither a callable nor a 2-tuple (callback, order): {!r}"".format(
                hook
            )
        )
",if not callable ( callback ) :,189
"def _resize_masks(self, results):
    """"""Resize masks with ``results['scale']``""""""
    for key in results.get(""mask_fields"", []):
        if results[key] is None:
            continue
        if self.keep_ratio:
            results[key] = results[key].rescale(results[""scale""])
        else:
            results[key] = results[key].resize(results[""img_shape""][:2])
",if self . keep_ratio :,110
"def getDataMax(self):
    result = -Double.MAX_VALUE
    nCurves = self.chart.getNCurves()
    for i in range(nCurves):
        c = self.getSystemCurve(i)
        if not c.isVisible():
            continue
        if c.getYAxis() == Y_AXIS:
            nPoints = c.getNPoints()
            for j in range(nPoints):
                result = self.maxIgnoreNaNAndMaxValue(result, c.getPoint(j).getY())
    if result == -Double.MAX_VALUE:
        return Double.NaN
    return result
",if c . getYAxis ( ) == Y_AXIS :,163
"def _check_token(self):
    if settings.app.sso_client_cache and self.server_auth_token:
        doc = self.sso_client_cache_collection.find_one(
            {
                ""user_id"": self.user.id,
                ""server_id"": self.server.id,
                ""device_id"": self.device_id,
                ""device_name"": self.device_name,
                ""auth_token"": self.server_auth_token,
            }
        )
        if doc:
            self.has_token = True
",if doc :,162
"def parse_header(plyfile, ext):
    # Variables
    line = []
    properties = []
    num_points = None
    while b""end_header"" not in line and line != b"""":
        line = plyfile.readline()
        if b""element"" in line:
            line = line.split()
            num_points = int(line[2])
        elif b""property"" in line:
            line = line.split()
            properties.append((line[2].decode(), ext + ply_dtypes[line[1]]))
    return num_points, properties
","if b""element"" in line :",149
"def __codeanalysis_settings_changed(self, current_finfo):
    if self.data:
        run_pyflakes, run_pep8 = self.pyflakes_enabled, self.pep8_enabled
        for finfo in self.data:
            self.__update_editor_margins(finfo.editor)
            finfo.cleanup_analysis_results()
            if (run_pyflakes or run_pep8) and current_finfo is not None:
                if current_finfo is not finfo:
                    finfo.run_code_analysis(run_pyflakes, run_pep8)
",if ( run_pyflakes or run_pep8 ) and current_finfo is not None :,148
"def __modules(self):
    raw_output = self.__module_avail_output().decode(""utf-8"")
    for line in StringIO(raw_output):
        line = line and line.strip()
        if not line or line.startswith(""-""):
            continue
        line_modules = line.split()
        for module in line_modules:
            if module.endswith(self.default_indicator):
                module = module[0 : -len(self.default_indicator)].strip()
            module_parts = module.split(""/"")
            module_version = None
            if len(module_parts) == 2:
                module_version = module_parts[1]
            module_name = module_parts[0]
            yield module_name, module_version
","if not line or line . startswith ( ""-"" ) :",199
"def _set_trailing_size(self, size):
    if self.is_free():
        next_chunk = self.next_chunk()
        if next_chunk is not None:
            self.state.memory.store(next_chunk.base, size, self.state.arch.bytes)
",if next_chunk is not None :,74
"def _execute_for_all_tables(self, app, bind, operation, skip_tables=False):
    app = self.get_app(app)
    if bind == ""__all__"":
        binds = [None] + list(app.config.get(""SQLALCHEMY_BINDS"") or ())
    elif isinstance(bind, string_types) or bind is None:
        binds = [bind]
    else:
        binds = bind
    for bind in binds:
        extra = {}
        if not skip_tables:
            tables = self.get_tables_for_bind(bind)
            extra[""tables""] = tables
        op = getattr(self.Model.metadata, operation)
        op(bind=self.get_engine(app, bind), **extra)
",if not skip_tables :,191
"def getFileName():
    extension = "".json""
    file = ""%s-stats"" % self.clusterName
    counter = 0
    while True:
        suffix = str(counter).zfill(3) + extension
        fullName = os.path.join(self.statsPath, file + suffix)
        if not os.path.exists(fullName):
            return fullName
        counter += 1
",if not os . path . exists ( fullName ) :,98
"def logic():
    # direction
    if goRight == ACTIVE:
        dir.next = DirType.RIGHT
        run.next = True
    elif goLeft == ACTIVE:
        dir.next = DirType.LEFT
        run.next = True
    # stop
    if stop == ACTIVE:
        run.next = False
    # counter action
    if run:
        if dir == DirType.LEFT:
            q.next[4:1] = q[3:]
            q.next[0] = not q[3]
        else:
            q.next[3:] = q[4:1]
            q.next[3] = not q[0]
",if dir == DirType . LEFT :,176
"def test_broadcast(self):
    """"""Test example broadcast functionality.""""""
    self.create_lang_connection(""1000000000"", ""en"")
    self.create_lang_connection(""1000000001"", ""en"")
    self.create_lang_connection(""1000000002"", ""en"")
    self.create_lang_connection(""1000000003"", ""es"")
    self.create_lang_connection(""1000000004"", ""es"")
    app.lang_broadcast()
    self.assertEqual(2, len(self.outbound))
    for message in self.outbound:
        if message.text == ""hello"":
            self.assertEqual(3, len(message.connections))
        elif message.text == ""hola"":
            self.assertEqual(2, len(message.connections))
","elif message . text == ""hola"" :",187
"def get_ovf_env(dirname):
    env_names = (""ovf-env.xml"", ""ovf_env.xml"", ""OVF_ENV.XML"", ""OVF-ENV.XML"")
    for fname in env_names:
        full_fn = os.path.join(dirname, fname)
        if os.path.isfile(full_fn):
            try:
                contents = util.load_file(full_fn)
                return (fname, contents)
            except Exception:
                util.logexc(LOG, ""Failed loading ovf file %s"", full_fn)
    return (None, False)
",if os . path . isfile ( full_fn ) :,164
"def _calc_offsets_children(self, offset, is_last):
    if self.elems:
        elem_last = self.elems[-1]
        for elem in self.elems:
            offset = elem._calc_offsets(offset, (elem is elem_last))
        offset += _BLOCK_SENTINEL_LENGTH
    elif not self.props or self.id in _ELEMS_ID_ALWAYS_BLOCK_SENTINEL:
        if not is_last:
            offset += _BLOCK_SENTINEL_LENGTH
    return offset
",if not is_last :,134
"def publish_state(cls, payload, state):
    try:
        if isinstance(payload, LiveActionDB):
            if state == action_constants.LIVEACTION_STATUS_REQUESTED:
                cls.process(payload)
            else:
                worker.get_worker().process(payload)
    except Exception:
        traceback.print_exc()
        print(payload)
","if isinstance ( payload , LiveActionDB ) :",99
"def log_predictive_density(self, x_test, y_test, Y_metadata=None):
    if isinstance(x_test, list):
        x_test, y_test, ind = util.multioutput.build_XY(x_test, y_test)
        if Y_metadata is None:
            Y_metadata = {""output_index"": ind, ""trials"": np.ones(ind.shape)}
    return super(MultioutputGP, self).log_predictive_density(x_test, y_test, Y_metadata)
",if Y_metadata is None :,131
"def minimalBases(classes):
    """"""Reduce a list of base classes to its ordered minimum equivalent""""""
    if not __python3:  # pragma: no cover
        classes = [c for c in classes if c is not ClassType]
    candidates = []
    for m in classes:
        for n in classes:
            if issubclass(n, m) and m is not n:
                break
        else:
            # m has no subclasses in 'classes'
            if m in candidates:
                candidates.remove(m)  # ensure that we're later in the list
            candidates.append(m)
    return candidates
","if issubclass ( n , m ) and m is not n :",160
"def apply(self, operations, rotations=None, **kwargs):
    rotations = rotations or []
    # apply the circuit operations
    for i, operation in enumerate(operations):
        if i > 0 and isinstance(operation, (QubitStateVector, BasisState)):
            raise DeviceError(
                ""Operation {} cannot be used after other Operations have already been applied ""
                ""on a {} device."".format(operation.name, self.short_name)
            )
    for operation in operations:
        self._apply_operation(operation)
    # store the pre-rotated state
    self._pre_rotated_state = self._state
    # apply the circuit rotations
    for operation in rotations:
        self._apply_operation(operation)
","if i > 0 and isinstance ( operation , ( QubitStateVector , BasisState ) ) :",186
"def __str__(self):
    txt = str(self._called)
    if self.call_gas or self.call_value:
        gas = f""gas: {self.call_gas}"" if self.call_gas else """"
        value = f""value: {self.call_value}"" if self.call_value else """"
        salt = f""salt: {self.call_salt}"" if self.call_salt else """"
        if gas or value or salt:
            options = [gas, value, salt]
            txt += ""{"" + "","".join([o for o in options if o != """"]) + ""}""
    return txt + ""("" + "","".join([str(a) for a in self._arguments]) + "")""
",if gas or value or salt :,183
"def pop(self):
    """"""Pop a nonterminal.  (Internal)""""""
    popdfa, popstate, popnode = self.stack.pop()
    newnode = self.convert(self.grammar, popnode)
    if newnode is not None:
        if self.stack:
            dfa, state, node = self.stack[-1]
            node.children.append(newnode)
        else:
            self.rootnode = newnode
",if self . stack :,112
"def pollpacket(self, wait):
    self._stage0()
    if len(self.buffer) < self.bufneed:
        r, w, x = select.select([self.sock.fileno()], [], [], wait)
        if len(r) == 0:
            return None
        try:
            s = self.sock.recv(BUFSIZE)
        except socket.error:
            raise EOFError
        if len(s) == 0:
            raise EOFError
        self.buffer += s
        self._stage0()
    return self._stage1()
",if len ( r ) == 0 :,148
"def increaseToolReach(self):
    if self.draggingFace is not None:
        d = (1, -1)[self.draggingFace & 1]
        if self.draggingFace >> 1 != 1:  # xxxxx y
            d = -d
        self.draggingY += d
        x, y, z = self.editor.mainViewport.cameraPosition
        pos = [x, y, z]
        pos[self.draggingFace >> 1] += d
        self.editor.mainViewport.cameraPosition = tuple(pos)
    else:
        self.cloneCameraDistance = self.editor._incrementReach(self.cloneCameraDistance)
    return True
",if self . draggingFace >> 1 != 1 :,172
"def selectionToChunks(self, remove=False, add=False):
    box = self.selectionBox()
    if box:
        if box == self.level.bounds:
            self.selectedChunks = set(self.level.allChunks)
            return
        selectedChunks = self.selectedChunks
        boxedChunks = set(box.chunkPositions)
        if boxedChunks.issubset(selectedChunks):
            remove = True
        if remove and not add:
            selectedChunks.difference_update(boxedChunks)
        else:
            selectedChunks.update(boxedChunks)
    self.selectionTool.selectNone()
",if remove and not add :,158
"def __init__(self, *args, **kwargs):
    super(ProjectForm, self).__init__(*args, **kwargs)
    if self.instance.id:
        if Store.objects.filter(translation_project__project=self.instance).count():
            self.fields[""localfiletype""].widget.attrs[""disabled""] = True
            self.fields[""localfiletype""].required = False
        if (
            self.instance.treestyle != ""auto""
            and self.instance.translationproject_set.count()
            and self.instance.treestyle == self.instance._detect_treestyle()
        ):
            self.fields[""treestyle""].widget.attrs[""disabled""] = True
            self.fields[""treestyle""].required = False
",if Store . objects . filter ( translation_project__project = self . instance ) . count ( ) :,193
"def _infer_return_type(*args):
    """"""Look at the type of all args and divine their implied return type.""""""
    return_type = None
    for arg in args:
        if arg is None:
            continue
        if isinstance(arg, bytes):
            if return_type is str:
                raise TypeError(""Can't mix bytes and non-bytes in "" ""path components."")
            return_type = bytes
        else:
            if return_type is bytes:
                raise TypeError(""Can't mix bytes and non-bytes in "" ""path components."")
            return_type = str
    if return_type is None:
        return str  # tempfile APIs return a str by default.
    return return_type
",if return_type is str :,186
"def deleteDuplicates(gadgets, callback=None):
    toReturn = []
    inst = set()
    count = 0
    added = False
    len_gadgets = len(gadgets)
    for i, gadget in enumerate(gadgets):
        inst.add(gadget._gadget)
        if len(inst) > count:
            count = len(inst)
            toReturn.append(gadget)
            added = True
        if callback:
            callback(gadget, added, float(i + 1) / (len_gadgets))
            added = False
    return toReturn
",if callback :,164
"def send_all(self, data: bytes):
    with self._conflict_detector:
        if self._handle_holder.closed:
            raise _core.ClosedResourceError(""this pipe is already closed"")
        if not data:
            await _core.checkpoint()
            return
        try:
            written = await _core.write_overlapped(self._handle_holder.handle, data)
        except BrokenPipeError as ex:
            raise _core.BrokenResourceError from ex
        # By my reading of MSDN, this assert is guaranteed to pass so long
        # as the pipe isn't in nonblocking mode, but... let's just
        # double-check.
        assert written == len(data)
",if self . _handle_holder . closed :,181
"def setup_parameter_node(self, param_node):
    if param_node.bl_idname == ""SvNumberNode"":
        if self.use_prop or self.get_prop_name():
            value = self.sv_get()[0][0]
            print(""V"", value)
            if isinstance(value, int):
                param_node.selected_mode = ""int""
                param_node.int_ = value
            elif isinstance(value, float):
                param_node.selected_mode = ""float""
                param_node.float_ = value
",if self . use_prop or self . get_prop_name ( ) :,156
"def collect_active_inst_idx_list(inst_beams, word_prob, inst_idx_to_position_map):
    active_inst_idx_list = []
    for inst_idx, inst_position in inst_idx_to_position_map.items():
        is_inst_complete = inst_beams[inst_idx].advance(word_prob[inst_position])
        if not is_inst_complete:
            active_inst_idx_list += [inst_idx]
    return active_inst_idx_list
",if not is_inst_complete :,129
"def compare_member_req_resp_without_key(self, request, response):
    for user_response in resp_json(response)[""data""]:
        for user_request in request:
            if user_request[""user_id""] == user_response[""user_id""]:
                assert user_request[""role""] == user_response[""role""]
","if user_request [ ""user_id"" ] == user_response [ ""user_id"" ] :",86
"def __init__(self, dir):
    self.module_names = set()
    for name in os.listdir(dir):
        if name.endswith("".py""):
            self.module_names.add(name[:-3])
        elif ""."" not in name:
            self.module_names.add(name)
","elif ""."" not in name :",79
"def _read_filter(self, data):
    if data:
        if self.expected_inner_sha256:
            self.inner_sha.update(data)
        if self.expected_inner_md5sum:
            self.inner_md5.update(data)
    return data
",if self . expected_inner_md5sum :,76
"def _p_basicstr_content(s, content=_basicstr_re):
    res = []
    while True:
        res.append(s.expect_re(content).group(0))
        if not s.consume(""\\""):
            break
        if s.consume_re(_newline_esc_re):
            pass
        elif s.consume_re(_short_uni_re) or s.consume_re(_long_uni_re):
            res.append(_chr(int(s.last().group(1), 16)))
        else:
            s.expect_re(_escapes_re)
            res.append(_escapes[s.last().group(0)])
    return """".join(res)
","if not s . consume ( ""\\"" ) :",179
"def process_response(self, request, response):
    if (
        response.status_code == 404
        and request.path_info.endswith(""/"")
        and not is_valid_path(request.path_info)
        and is_valid_path(request.path_info[:-1])
    ):
        # Use request.path because we munged app/locale in path_info.
        newurl = request.path[:-1]
        if request.GET:
            with safe_query_string(request):
                newurl += ""?"" + request.META.get(""QUERY_STRING"", """")
        return HttpResponsePermanentRedirect(newurl)
    else:
        return response
",if request . GET :,171
"def convertDict(obj):
    obj = dict(obj)
    for k, v in obj.items():
        del obj[k]
        if not (isinstance(k, str) or isinstance(k, unicode)):
            k = dumps(k)
            # Keep track of which keys need to be decoded when loading.
            if Types.KEYS not in obj:
                obj[Types.KEYS] = []
            obj[Types.KEYS].append(k)
        obj[k] = convertObjects(v)
    return obj
","if not ( isinstance ( k , str ) or isinstance ( k , unicode ) ) :",137
"def __repr__(self):
    if self._in_repr:
        return ""<recursion>""
    try:
        self._in_repr = True
        if self.is_computed():
            status = ""computed, ""
            if self.error() is None:
                if self.value() is self:
                    status += ""= self""
                else:
                    status += ""= "" + repr(self.value())
            else:
                status += ""error = "" + repr(self.error())
        else:
            status = ""isn't computed""
        return ""%s (%s)"" % (type(self), status)
    finally:
        self._in_repr = False
",if self . is_computed ( ) :,189
"def allocate_network(ipv=""ipv4""):
    global dtcd_uuid
    global network_pool
    global allocations
    network = None
    try:
        cx = httplib.HTTPConnection(""localhost:7623"")
        cx.request(""POST"", ""/v1/network/%s/"" % ipv, body=dtcd_uuid)
        resp = cx.getresponse()
        if resp.status == 200:
            network = netaddr.IPNetwork(resp.read().decode(""utf-8""))
        cx.close()
    except Exception:
        pass
    if network is None:
        network = network_pool[ipv].pop()
        allocations[network] = True
    return network
",if resp . status == 200 :,170
"def change_args_to_dict(string):
    if string is None:
        return None
    ans = []
    strings = string.split(""\n"")
    ind = 1
    start = 0
    while ind <= len(strings):
        if ind < len(strings) and strings[ind].startswith("" ""):
            ind += 1
        else:
            if start < ind:
                ans.append(""\n"".join(strings[start:ind]))
            start = ind
            ind += 1
    d = {}
    for line in ans:
        if "":"" in line and len(line) > 0:
            lines = line.split("":"")
            d[lines[0]] = lines[1].strip()
    return d
","if "":"" in line and len ( line ) > 0 :",188
"def kill_members(members, sig, hosts=nodes):
    for member in sorted(members):
        try:
            if ha_tools_debug:
                print(""killing %s"" % member)
            proc = hosts[member][""proc""]
            # Not sure if cygwin makes sense here...
            if sys.platform in (""win32"", ""cygwin""):
                os.kill(proc.pid, signal.CTRL_C_EVENT)
            else:
                os.kill(proc.pid, sig)
        except OSError:
            if ha_tools_debug:
                print(""%s already dead?"" % member)
",if ha_tools_debug :,172
"def check(self):
    for path in self.paths:
        response = self.http_request(
            method=""GET"",
            path=path,
        )
        if response is None:
            continue
        if any(
            map(
                lambda x: x in response.text,
                [
                    ""report.db.server.name"",
                    ""report.db.server.sa.pass"",
                    ""report.db.server.user.pass"",
                ],
            )
        ):
            self.valid = path
            return True  # target is vulnerable
    return False  # target not vulnerable
",if response is None :,184
"def get_to_download_runs_ids(session, headers):
    last_date = 0
    result = []
    while 1:
        r = session.get(RUN_DATA_API.format(last_date=last_date), headers=headers)
        if r.ok:
            run_logs = r.json()[""data""][""records""]
            result.extend([i[""logs""][0][""stats""][""id""] for i in run_logs])
            last_date = r.json()[""data""][""lastTimestamp""]
            since_time = datetime.utcfromtimestamp(last_date / 1000)
            print(f""pares keep ids data since {since_time}"")
            time.sleep(1)  # spider rule
            if not last_date:
                break
    return result
",if not last_date :,199
"def button_press_cb(self, tdw, event):
    self._update_zone_and_cursors(tdw, event.x, event.y)
    if self._zone in (_EditZone.CREATE_FRAME, _EditZone.REMOVE_FRAME):
        button = event.button
        if button == 1 and event.type == Gdk.EventType.BUTTON_PRESS:
            self._click_info = (button, self._zone)
            return False
    return super(FrameEditMode, self).button_press_cb(tdw, event)
",if button == 1 and event . type == Gdk . EventType . BUTTON_PRESS :,135
"def first_timestep():
    assignment = self.has_previous.assign(
        value=tf_util.constant(value=True, dtype=""bool""), read_value=False
    )
    with tf.control_dependencies(control_inputs=(assignment,)):
        if self.concatenate:
            current = x
        else:
            current = tf.expand_dims(input=x, axis=(self.axis + 1))
        multiples = tuple(
            self.length if dims == self.axis + 1 else 1
            for dims in range(self.output_spec().rank + 1)
        )
        return tf.tile(input=current, multiples=multiples)
",if self . concatenate :,167
"def main() -> None:
    onefuzz = Onefuzz()
    jobs = onefuzz.jobs.list()
    for job in jobs:
        print(
            ""job:"",
            str(job.job_id)[:8],
            "":"".join([job.config.project, job.config.name, job.config.build]),
        )
        for task in onefuzz.tasks.list(job_id=job.job_id):
            if task.state in [""stopped"", ""stopping""]:
                continue
            print(
                ""    "",
                str(task.task_id)[:8],
                task.config.task.type,
                task.config.task.target_exe,
            )
","if task . state in [ ""stopped"" , ""stopping"" ] :",200
"def update_stack(self, full_name, template_url, parameters, tags):
    """"""Updates an existing stack in CloudFormation.""""""
    try:
        logger.info(""Attempting to update stack %s."", full_name)
        self.conn.cloudformation.update_stack(
            full_name,
            template_url=template_url,
            parameters=parameters,
            tags=tags,
            capabilities=[""CAPABILITY_IAM""],
        )
        return SUBMITTED
    except BotoServerError as e:
        if ""No updates are to be performed."" in e.message:
            logger.info(""Stack %s did not change, not updating."", full_name)
            return SKIPPED
        raise
","if ""No updates are to be performed."" in e . message :",183
"def header_tag_files(env, files, legal_header, script_files=False):
    """"""Apply the legal_header to the list of files""""""
    try:
        import apply_legal_header
    except:
        xbc.cdie(""XED ERROR: mfile.py could not find scripts directory"")
    for g in files:
        print(""G: "", g)
        for f in mbuild.glob(g):
            print(""F: "", f)
            if script_files:
                apply_legal_header.apply_header_to_data_file(legal_header, f)
            else:
                apply_legal_header.apply_header_to_source_file(legal_header, f)
",if script_files :,186
"def cleanDataCmd(cmd):
    newcmd = ""AbracadabrA ** <?php ""
    if cmd[:6] != ""php://"":
        if reverseConn not in cmd:
            cmds = cmd.split(""&"")
            for c in cmds:
                if len(c) > 0:
                    newcmd += ""system('%s');"" % c
        else:
            b64cmd = base64.b64encode(cmd)
            newcmd += ""system(base64_decode('%s'));"" % b64cmd
    else:
        newcmd += cmd[6:]
    newcmd += ""?> **""
    return newcmd
",if len ( c ) > 0 :,170
"def test_form(self):
    n_qubits = 6
    random_operator = get_fermion_operator(random_interaction_operator(n_qubits))
    chemist_operator = chemist_ordered(random_operator)
    for term, _ in chemist_operator.terms.items():
        if len(term) == 2 or not len(term):
            pass
        else:
            self.assertTrue(term[0][1])
            self.assertTrue(term[2][1])
            self.assertFalse(term[1][1])
            self.assertFalse(term[3][1])
            self.assertTrue(term[0][0] > term[2][0])
            self.assertTrue(term[1][0] > term[3][0])
",if len ( term ) == 2 or not len ( term ) :,199
"def do(server, handler, config, modargs):
    data = []
    clients = server.get_clients(handler.default_filter)
    if not clients:
        return
    for client in clients:
        tags = config.tags(client.node())
        if modargs.remove:
            tags.remove(*modargs.remove)
        if modargs.add:
            tags.add(*modargs.add)
        data.append({""ID"": client.node(), ""TAGS"": tags})
    config.save(project=modargs.write_project, user=modargs.write_user)
    handler.display(Table(data))
",if modargs . remove :,160
"def validate(self):
    if self.data.get(""state"") == ""enabled"":
        if ""bucket"" not in self.data:
            raise PolicyValidationError(
                (
                    ""redshift logging enablement requires `bucket` ""
                    ""and `prefix` specification on %s"" % (self.manager.data,)
                )
            )
    return self
","if ""bucket"" not in self . data :",104
"def renumber(self, x1, y1, x2, y2, dx, dy):
    out = []
    for part in re.split(""(\w+)"", self.formula):
        m = re.match(""^([A-Z]+)([1-9][0-9]*)$"", part)
        if m is not None:
            sx, sy = m.groups()
            x = colname2num(sx)
            y = int(sy)
            if x1 <= x <= x2 and y1 <= y <= y2:
                part = cellname(x + dx, y + dy)
        out.append(part)
    return FormulaCell("""".join(out), self.fmt, self.alignment)
",if m is not None :,179
"def update_sysconfig_file(fn, adjustments, allow_empty=False):
    if not adjustments:
        return
    (exists, contents) = read_sysconfig_file(fn)
    updated_am = 0
    for (k, v) in adjustments.items():
        if v is None:
            continue
        v = str(v)
        if len(v) == 0 and not allow_empty:
            continue
        contents[k] = v
        updated_am += 1
    if updated_am:
        lines = [
            str(contents),
        ]
        if not exists:
            lines.insert(0, util.make_header())
        util.write_file(fn, ""\n"".join(lines) + ""\n"", 0o644)
",if not exists :,198
"def getElement(self, aboutUri, namespace, name):
    for desc in self.rdfRoot.getElementsByTagNameNS(RDF_NAMESPACE, ""Description""):
        if desc.getAttributeNS(RDF_NAMESPACE, ""about"") == aboutUri:
            attr = desc.getAttributeNodeNS(namespace, name)
            if attr != None:
                yield attr
            for element in desc.getElementsByTagNameNS(namespace, name):
                yield element
",if attr != None :,113
"def get_store_name_from_connection_string(connection_string):
    if is_valid_connection_string(connection_string):
        segments = dict(seg.split(""="", 1) for seg in connection_string.split("";""))
        endpoint = segments.get(""Endpoint"")
        if endpoint:
            return endpoint.split(""//"")[1].split(""."")[0]
    return None
",if endpoint :,93
"def insertLoopTemplate(self, layout):
    col = layout.column(align=True)
    for socket in self.activeNode.outputs:
        if not socket.hide and isList(socket.bl_idname):
            props = col.operator(
                ""an.insert_loop_for_iterator"",
                text=""Loop through {}"".format(repr(socket.getDisplayedName())),
                icon=""MOD_ARRAY"",
            )
            props.nodeIdentifier = self.activeNode.identifier
            props.socketIndex = socket.getIndex()
",if not socket . hide and isList ( socket . bl_idname ) :,145
"def do_task(self, task):
    self.running_task += 1
    result = yield gen.Task(self.fetcher.fetch, task)
    type, task, response = result.args
    self.processor.on_task(task, response)
    # do with message
    while not self.processor.inqueue.empty():
        _task, _response = self.processor.inqueue.get()
        self.processor.on_task(_task, _response)
    # do with results
    while not self.processor.result_queue.empty():
        _task, _result = self.processor.result_queue.get()
        if self.result_worker:
            self.result_worker.on_result(_task, _result)
    self.running_task -= 1
",if self . result_worker :,191
"def _parse_config_result(data):
    command_list = "" ; "".join([x.strip() for x in data[0]])
    config_result = data[1]
    if isinstance(config_result, list):
        result = """"
        if isinstance(config_result[0], dict):
            for key in config_result[0]:
                result += config_result[0][key]
            config_result = result
        else:
            config_result = config_result[0]
    return [command_list, config_result]
","if isinstance ( config_result [ 0 ] , dict ) :",142
"def load_api_handler(self, mod_name):
    for name, hdl in API_HANDLERS:
        name = name.lower()
        if mod_name and name == mod_name.lower():
            handler = self.mods.get(name)
            if not handler:
                handler = hdl(self.emu)
                self.mods.update({name: handler})
            return handler
    return None
",if mod_name and name == mod_name . lower ( ) :,113
"def heal(self):
    if not self.doctors:
        return
    proc_ids = self._get_process_ids()
    for proc_id in proc_ids:
        # get proc every time for latest state
        proc = PipelineProcess.objects.get(id=proc_id)
        if not proc.is_alive or proc.is_frozen:
            continue
        for dr in self.doctors:
            if dr.confirm(proc):
                dr.cure(proc)
                break
",if dr . confirm ( proc ) :,138
"def __new__(cls, *args, **kwargs):
    if len(args) == 1:
        if len(kwargs):
            raise ValueError(
                ""You can either use {} with one positional argument or with keyword arguments, not both."".format(
                    cls.__name__
                )
            )
        if not args[0]:
            return super().__new__(cls)
        if isinstance(args[0], cls):
            return cls
    return super().__new__(cls, *args, **kwargs)
",if not args [ 0 ] :,137
"def __lt__(self, other):
    # 0: clock 1: timestamp 3: process id
    try:
        A, B = self[0], other[0]
        # uses logical clock value first
        if A and B:  # use logical clock if available
            if A == B:  # equal clocks use lower process id
                return self[2] < other[2]
            return A < B
        return self[1] < other[1]  # ... or use timestamp
    except IndexError:
        return NotImplemented
",if A and B :,135
"def _get_client(rp_mapping, resource_provider):
    for key, value in rp_mapping.items():
        if str.lower(key) == str.lower(resource_provider):
            if isinstance(value, dict):
                return GeneralPrivateEndpointClient(
                    key,
                    value[""api_version""],
                    value[""support_list_or_not""],
                    value[""resource_get_api_version""],
                )
            return value()
    raise CLIError(
        ""Resource type must be one of {}"".format("", "".join(rp_mapping.keys()))
    )
","if isinstance ( value , dict ) :",165
"def test_progressbar_format_pos(runner, pos, length):
    with _create_progress(length, length_known=length != 0, pos=pos) as progress:
        result = progress.format_pos()
        if progress.length_known:
            assert result == f""{pos}/{length}""
        else:
            assert result == str(pos)
",if progress . length_known :,91
"def optimize(self, graph: Graph):
    MAX_TEXTURE_SIZE = config.WEBGL_MAX_TEXTURE_SIZE
    flag_changed = False
    for v in traverse.listup_variables(graph):
        if not Placeholder.check_resolved(v.size):
            continue
        height, width = TextureShape.get(v)
        if height <= MAX_TEXTURE_SIZE and width <= MAX_TEXTURE_SIZE:
            continue
        if not v.has_attribute(SplitTarget):
            flag_changed = True
            v.attributes.add(SplitTarget())
    return graph, flag_changed
",if not v . has_attribute ( SplitTarget ) :,157
"def ant_map(m):
    tmp = ""rows %s\ncols %s\n"" % (len(m), len(m[0]))
    players = {}
    for row in m:
        tmp += ""m ""
        for col in row:
            if col == LAND:
                tmp += "".""
            elif col == BARRIER:
                tmp += ""%""
            elif col == FOOD:
                tmp += ""*""
            elif col == UNSEEN:
                tmp += ""?""
            else:
                players[col] = True
                tmp += chr(col + 97)
        tmp += ""\n""
    tmp = (""players %s\n"" % len(players)) + tmp
    return tmp
",if col == LAND :,199
"def reset(self):
    logger.debug(""Arctic.reset()"")
    with self._lock:
        if self.__conn is not None:
            self.__conn.close()
            self.__conn = None
        for _, l in self._library_cache.items():
            if hasattr(l, ""_reset"") and callable(l._reset):
                logger.debug(""Library reset() %s"" % l)
                l._reset()  # the existence of _reset() is not guaranteed/enforced, it also triggers re-auth
","if hasattr ( l , ""_reset"" ) and callable ( l . _reset ) :",137
"def add_cand_to_check(cands):
    for cand in cands:
        x = cand.creator
        if x is None:
            continue
        if x not in fan_out:
            # `len(fan_out)` is in order to avoid comparing `x`
            heapq.heappush(cand_funcs, (-x.rank, len(fan_out), x))
        fan_out[x] += 1
",if x is None :,111
"def on_task_modify(self, task, config):
    for entry in task.entries:
        if ""torrent"" in entry:
            size = entry[""torrent""].size / 1024 / 1024
            log.debug(""%s size: %s MB"" % (entry[""title""], size))
            entry[""content_size""] = size
","if ""torrent"" in entry :",83
"def get_measurements(self, pipeline, object_name, category):
    if self.get_categories(pipeline, object_name) == [category]:
        results = []
        if self.do_corr_and_slope:
            if object_name == ""Image"":
                results += [""Correlation"", ""Slope""]
            else:
                results += [""Correlation""]
        if self.do_overlap:
            results += [""Overlap"", ""K""]
        if self.do_manders:
            results += [""Manders""]
        if self.do_rwc:
            results += [""RWC""]
        if self.do_costes:
            results += [""Costes""]
        return results
    return []
",if self . do_costes :,195
"def create_root(cls, site=None, title=""Root"", request=None, **kwargs):
    if not site:
        site = Site.objects.get_current()
    root_nodes = cls.objects.root_nodes().filter(site=site)
    if not root_nodes:
        article = Article()
        revision = ArticleRevision(title=title, **kwargs)
        if request:
            revision.set_from_request(request)
        article.add_revision(revision, save=True)
        article.save()
        root = cls.objects.create(site=site, article=article)
        article.add_object_relation(root)
    else:
        root = root_nodes[0]
    return root
",if request :,185
"def get(self, key):
    filename = self._get_filename(key)
    try:
        with open(filename, ""rb"") as f:
            pickle_time = pickle.load(f)
            if pickle_time == 0 or pickle_time >= time():
                return pickle.load(f)
            else:
                os.remove(filename)
                return None
    except (IOError, OSError, pickle.PickleError):
        return None
",if pickle_time == 0 or pickle_time >= time ( ) :,122
"def build_message(self, options, target):
    message = multipart.MIMEMultipart()
    for name, value in list(options.items()):
        if name == ""EMAIL_BODY"":
            self.add_body(message, value)
        elif name == ""EMAIL_ATTACHMENT"":
            self.add_attachment(message, value)
        else:  # From, To, Subject, etc.
            self.set_option(message, name, value, target)
    return message
","if name == ""EMAIL_BODY"" :",126
"def updateVar(name, data, mode=None):
    if mode:
        if mode == ""append"":
            core.config.globalVariables[name].append(data)
        elif mode == ""add"":
            core.config.globalVariables[name].add(data)
    else:
        core.config.globalVariables[name] = data
","if mode == ""append"" :",91
"def insert_errors(
    el,
    errors,
    form_id=None,
    form_index=None,
    error_class=""error"",
    error_creator=default_error_creator,
):
    el = _find_form(el, form_id=form_id, form_index=form_index)
    for name, error in errors.items():
        if error is None:
            continue
        for error_el, message in _find_elements_for_name(el, name, error):
            assert isinstance(message, (basestring, type(None), ElementBase)), (
                ""Bad message: %r"" % message
            )
            _insert_error(error_el, message, error_class, error_creator)
",if error is None :,190
"def read(self, item, recursive=False, sort=False):
    item = _normalize_path(item)
    if item in self._store:
        if item in self._expire_time and self._expire_time[item] < datetime.now():
            del self._store[item]
            raise KeyError(item)
        return PathResult(item, value=self._store[item])
    else:
        return self._read_dir(item, recursive=recursive, sort=sort)
",if item in self . _expire_time and self . _expire_time [ item ] < datetime . now ( ) :,121
"def _stash_splitter(states):
    keep, split = [], []
    if state_func is not None:
        for s in states:
            ns = state_func(s)
            if isinstance(ns, SimState):
                split.append(ns)
            elif isinstance(ns, (list, tuple, set)):
                split.extend(ns)
            else:
                split.append(s)
    if stash_func is not None:
        split = stash_func(states)
    if to_stash is not stash:
        keep = states
    return keep, split
","elif isinstance ( ns , ( list , tuple , set ) ) :",163
"def run(self):
    while self.runflag:
        if time() - self.last > 5 and self.qsize() > 0:
            with self.lock:
                tasks = list(self.queue)
                self.queue.clear()
            while len(tasks) > 0:
                pathname, remotepath = tasks.pop(0)
                self.bcloud_app.upload_page.add_bg_task(pathname, remotepath)
            self.last = time()
        else:
            sleep(1)
",if time ( ) - self . last > 5 and self . qsize ( ) > 0 :,142
"def _append_patch(self, patch_dir, patch_files):
    for patch in patch_files:
        if type(patch) is dict:
            tmp = patch
            patch = {}
            for key in tmp.keys():
                patch[os.path.join(patch_dir, key)] = tmp[key]
            self.patches.append(patch)
        else:
            self.patches.append(os.path.join(patch_dir, patch))
",if type ( patch ) is dict :,125
"def __remote_port(self):
    port = 22
    if self.git_has_remote:
        m = re.match(r""^(.*?)?@([^/:]*):?([0-9]+)?"", self.git_remote.url)
        if m:
            if m.group(3):
                port = m.group(3)
    return int(port)
",if m . group ( 3 ) :,94
"def _create_or_get_helper(self, infer_mode: Optional[bool] = None, **kwargs) -> Helper:
    # Prefer creating a new helper when at least one kwarg is specified.
    prefer_new = len(kwargs) > 0
    kwargs.update(infer_mode=infer_mode)
    is_training = not infer_mode if infer_mode is not None else self.training
    helper = self._train_helper if is_training else self._infer_helper
    if prefer_new or helper is None:
        helper = self.create_helper(**kwargs)
        if is_training and self._train_helper is None:
            self._train_helper = helper
        elif not is_training and self._infer_helper is None:
            self._infer_helper = helper
    return helper
",if is_training and self . _train_helper is None :,195
"def flushChangeClassifications(self, schedulerid, less_than=None):
    if less_than is not None:
        classifications = self.classifications.setdefault(schedulerid, {})
        for changeid in list(classifications):
            if changeid < less_than:
                del classifications[changeid]
    else:
        self.classifications[schedulerid] = {}
    return defer.succeed(None)
",if changeid < less_than :,107
"def pid_from_name(name):
    processes = []
    for pid in os.listdir(""/proc""):
        try:
            pid = int(pid)
            pname, cmdline = SunProcess._name_args(pid)
            if name in pname:
                return pid
            if name in cmdline.split("" "", 1)[0]:
                return pid
        except:
            pass
    raise ProcessException(""No process with such name: %s"" % name)
","if name in cmdline . split ( "" "" , 1 ) [ 0 ] :",126
"def spew():
    seenUID = False
    start()
    for part in query:
        if part.type == ""uid"":
            seenUID = True
        if part.type == ""body"":
            yield self.spew_body(part, id, msg, write, flush)
        else:
            f = getattr(self, ""spew_"" + part.type)
            yield f(id, msg, write, flush)
        if part is not query[-1]:
            space()
    if uid and not seenUID:
        space()
        yield self.spew_uid(id, msg, write, flush)
    finish()
    flush()
","if part . type == ""uid"" :",174
"def rx():
    while True:
        rx_i = rep.recv()
        if rx_i == b""1000"":
            rep.send(b""done"")
            break
        rep.send(b""i"")
","if rx_i == b""1000"" :",60
"def test_search_incorrect_base_exception_1(self):
    self.connection_1c.bind()
    try:
        result = self.connection_1c.search(
            ""o=nonexistant"", ""(cn=*)"", search_scope=SUBTREE, attributes=[""cn"", ""sn""]
        )
        if not self.connection_1c.strategy.sync:
            _, result = self.connection_1c.get_response(result)
        self.fail(""exception not raised"")
    except LDAPNoSuchObjectResult:
        pass
",if not self . connection_1c . strategy . sync :,138
"def value_from_datadict(self, data, files, prefix):
    count = int(data[""%s-count"" % prefix])
    values_with_indexes = []
    for i in range(0, count):
        if data[""%s-%d-deleted"" % (prefix, i)]:
            continue
        values_with_indexes.append(
            (
                int(data[""%s-%d-order"" % (prefix, i)]),
                self.child_block.value_from_datadict(
                    data, files, ""%s-%d-value"" % (prefix, i)
                ),
            )
        )
    values_with_indexes.sort()
    return [v for (i, v) in values_with_indexes]
","if data [ ""%s-%d-deleted"" % ( prefix , i ) ] :",194
"def _ensure_header_written(self, datasize):
    if not self._headerwritten:
        if not self._nchannels:
            raise Error(""# channels not specified"")
        if not self._sampwidth:
            raise Error(""sample width not specified"")
        if not self._framerate:
            raise Error(""sampling rate not specified"")
        self._write_header(datasize)
",if not self . _framerate :,99
"def wait_til_ready(cls):
    while True:
        now = time.time()
        next_iteration = now // 1.0 + 1
        if cls.connector.ready:
            break
        else:
            await cls._clock.run_til(next_iteration)
        await asyncio.sleep(1.0)
",if cls . connector . ready :,89
"def lookup_actions(self, resp):
    actions = {}
    for action, conditions in self.actions.items():
        for condition, opts in conditions:
            for key, val in condition:
                if key[-1] == ""!"":
                    if resp.match(key[:-1], val):
                        break
                else:
                    if not resp.match(key, val):
                        break
            else:
                actions[action] = opts
    return actions
","if key [ - 1 ] == ""!"" :",138
"def close(self, wait=True, abort=False):
    """"""Close the socket connection.""""""
    if not self.closed and not self.closing:
        self.closing = True
        self.server._trigger_event(""disconnect"", self.sid, run_async=False)
        if not abort:
            self.send(packet.Packet(packet.CLOSE))
        self.closed = True
        self.queue.put(None)
        if wait:
            self.queue.join()
",if wait :,125
"def model_parse(self):
    for name, submodel in self.model.named_modules():
        for op_type in SUPPORTED_OP_TYPE:
            if isinstance(submodel, op_type):
                self.target_layer[name] = submodel
                self.already_pruned[name] = 0
","if isinstance ( submodel , op_type ) :",83
"def pack_identifier(self):
    """"""Return a combined identifier for the whole pack if this has more than one episode.""""""
    # Currently only supports ep mode
    if self.id_type == ""ep"":
        if self.episodes > 1:
            return ""S%02dE%02d-E%02d"" % (
                self.season,
                self.episode,
                self.episode + self.episodes - 1,
            )
        else:
            return self.identifier
    else:
        return self.identifier
",if self . episodes > 1 :,143
"def on_data(res):
    if terminate.is_set():
        return
    if args.strings and not args.no_content:
        if type(res) == tuple:
            f, v = res
            if type(f) == unicode:
                f = f.encode(""utf-8"")
            if type(v) == unicode:
                v = v.encode(""utf-8"")
            self.success(""{}: {}"".format(f, v))
        elif not args.content_only:
            self.success(res)
    else:
        self.success(res)
",if type ( v ) == unicode :,158
"def _enable_contours_changed(self, value):
    """"""Turns on and off the contours.""""""
    if self.module_manager is None:
        return
    if value:
        self.actor.inputs = [self.contour]
        if self.contour.filled_contours:
            self.actor.mapper.scalar_mode = ""use_cell_data""
    else:
        self.actor.inputs = [self.grid_plane]
        self.actor.mapper.scalar_mode = ""default""
    self.render()
",if self . contour . filled_contours :,139
"def _apply_abs_paths(data, script_dir):
    for flag_data in data.values():
        if not isinstance(flag_data, dict):
            continue
        default = flag_data.get(""default"")
        if (
            not default
            or not isinstance(default, six.string_types)
            or os.path.sep not in default
        ):
            continue
        abs_path = os.path.join(script_dir, default)
        if os.path.exists(abs_path):
            flag_data[""default""] = abs_path
","if not isinstance ( flag_data , dict ) :",153
"def button_release(self, mapper):
    self.pressed = False
    if self.waiting_task and self.active is None and not self.action:
        # In HoldModifier, button released before timeout
        mapper.cancel_task(self.waiting_task)
        self.waiting_task = None
        if self.normalaction:
            self.normalaction.button_press(mapper)
            mapper.schedule(0.02, self.normalaction.button_release)
    elif self.active:
        # Released held button
        self.active.button_release(mapper)
        self.active = None
",if self . normalaction :,160
"def goToPrevMarkedHeadline(self, event=None):
    """"""Select the next marked node.""""""
    c = self
    p = c.p
    if not p:
        return
    p.moveToThreadBack()
    wrapped = False
    while 1:
        if p and p.isMarked():
            break
        elif p:
            p.moveToThreadBack()
        elif wrapped:
            break
        else:
            wrapped = True
            p = c.rootPosition()
    if not p:
        g.blue(""done"")
    c.treeSelectHelper(p)  # Sets focus.
",elif p :,164
"def status(self, name, error=""No matching script logs found""):
    with self.script_lock:
        if self.script_running and self.script_running[1] == name:
            return self.script_running[1:]
        elif self.script_last and self.script_last[1] == name:
            return self.script_last[1:]
        else:
            raise ValueError(error)
",elif self . script_last and self . script_last [ 1 ] == name :,107
"def _stderr_supports_color():
    try:
        if hasattr(sys.stderr, ""isatty"") and sys.stderr.isatty():
            if curses:
                curses.setupterm()
                if curses.tigetnum(""colors"") > 0:
                    return True
            elif colorama:
                if sys.stderr is getattr(
                    colorama.initialise, ""wrapped_stderr"", object()
                ):
                    return True
    except Exception:
        # Very broad exception handling because it's always better to
        # fall back to non-colored logs than to break at startup.
        pass
    return False
",if curses :,170
"def main():
    configFilename = ""twitterbot.ini""
    if sys.argv[1:]:
        configFilename = sys.argv[1]
    try:
        if not os.path.exists(configFilename):
            raise Exception()
        load_config(configFilename)
    except Exception as e:
        print(""Error while loading ini file %s"" % (configFilename), file=sys.stderr)
        print(e, file=sys.stderr)
        print(__doc__, file=sys.stderr)
        sys.exit(1)
    bot = TwitterBot(configFilename)
    return bot.run()
",if not os . path . exists ( configFilename ) :,156
"def safe_to_kill(request):
    if os.path.exists(DRAIN_FILE):
        with open(DRAIN_FILE) as f:
            dt = datetime.datetime.fromtimestamp(float(f.read()))
            delta = datetime.datetime.now() - dt
            if delta.seconds > 2:
                return Response(status_int=200)
            else:
                return Response(status_int=400)
    else:
        return Response(status_int=400)
",if delta . seconds > 2 :,131
"def get_class_name(item):
    class_name, module_name = None, None
    for parent in reversed(item.listchain()):
        if isinstance(parent, pytest.Class):
            class_name = parent.name
        elif isinstance(parent, pytest.Module):
            module_name = parent.module.__name__
            break
    # heuristic:
    # - better to group gpu and task tests, since tests from those modules
    #   are likely to share caching more
    # - split up the rest by class name because slow tests tend to be in
    #   the same module
    if class_name and "".tasks."" not in module_name:
        return ""{}.{}"".format(module_name, class_name)
    else:
        return module_name
","if isinstance ( parent , pytest . Class ) :",190
"def getAllFitsLite():
    fits = eos.db.getFitListLite()
    shipMap = {f.shipID: None for f in fits}
    for shipID in shipMap:
        ship = eos.db.getItem(shipID)
        if ship is not None:
            shipMap[shipID] = (ship.name, ship.getShortName())
    fitsToPurge = set()
    for fit in fits:
        try:
            fit.shipName, fit.shipNameShort = shipMap[fit.shipID]
        except (KeyError, TypeError):
            fitsToPurge.add(fit)
    for fit in fitsToPurge:
        fits.remove(fit)
    return fits
",if ship is not None :,185
"def _process(self, event_data):
    self.machine.callbacks(self.machine.prepare_event, event_data)
    _LOGGER.debug(
        ""%sExecuted machine preparation callbacks before conditions."", self.machine.name
    )
    try:
        for trans in self.transitions[event_data.state.name]:
            event_data.transition = trans
            if trans.execute(event_data):
                event_data.result = True
                break
    except Exception as err:
        event_data.error = err
        raise
    finally:
        self.machine.callbacks(self.machine.finalize_event, event_data)
        _LOGGER.debug(""%sExecuted machine finalize callbacks"", self.machine.name)
    return event_data.result
",if trans . execute ( event_data ) :,200
"def fetch_comments(self, force=False, limit=None):
    comments = []
    if (force is True) or (self.badges[""comments""] > 0):
        query_params = {""filter"": ""commentCard,copyCommentCard""}
        if limit is not None:
            query_params[""limit""] = limit
        comments = self.client.fetch_json(
            ""/cards/"" + self.id + ""/actions"", query_params=query_params
        )
        return sorted(comments, key=lambda comment: comment[""date""])
    return comments
",if limit is not None :,140
"def get_changed(self):
    if self._is_expression():
        result = self._get_node_text(self.ast)
        if result == self.source:
            return None
        return result
    else:
        collector = codeanalyze.ChangeCollector(self.source)
        last_end = -1
        for match in self.matches:
            start, end = match.get_region()
            if start < last_end:
                if not self._is_expression():
                    continue
            last_end = end
            replacement = self._get_matched_text(match)
            collector.add_change(start, end, replacement)
        return collector.get_changed()
",if not self . _is_expression ( ) :,189
"def _replace_home(x):
    if xp.ON_WINDOWS:
        home = (
            builtins.__xonsh__.env[""HOMEDRIVE""] + builtins.__xonsh__.env[""HOMEPATH""][0]
        )
        if x.startswith(home):
            x = x.replace(home, ""~"", 1)
        if builtins.__xonsh__.env.get(""FORCE_POSIX_PATHS""):
            x = x.replace(os.sep, os.altsep)
        return x
    else:
        home = builtins.__xonsh__.env[""HOME""]
        if x.startswith(home):
            x = x.replace(home, ""~"", 1)
        return x
",if x . startswith ( home ) :,176
"def project_review(plans):
    for plan in plans:
        print(""Inspecting {} plan"".format(plan))
        branches = get_branches_from_plan(plan)
        for branch in branches:
            build_results = get_results_from_branch(branch)
            for build in build_results:
                build_key = build.get(""buildResultKey"") or None
                print(""Inspecting build - {}"".format(build_key))
                if build_key:
                    for status in STATUS_CLEANED_RESULTS:
                        remove_build_result(build_key=build_key, status=status)
",if build_key :,169
"def _check_for_batch_clashes(xs):
    """"""Check that batch names do not overlap with sample names.""""""
    names = set([x[""description""] for x in xs])
    dups = set([])
    for x in xs:
        batches = tz.get_in((""metadata"", ""batch""), x)
        if batches:
            if not isinstance(batches, (list, tuple)):
                batches = [batches]
            for batch in batches:
                if batch in names:
                    dups.add(batch)
    if len(dups) > 0:
        raise ValueError(
            ""Batch names must be unique from sample descriptions.\n""
            ""Clashing batch names: %s"" % sorted(list(dups))
        )
","if not isinstance ( batches , ( list , tuple ) ) :",192
"def _check_signal(self):
    """"""Checks if a signal was received and issues a message.""""""
    proc_signal = getattr(self.proc, ""signal"", None)
    if proc_signal is None:
        return
    sig, core = proc_signal
    sig_str = SIGNAL_MESSAGES.get(sig)
    if sig_str:
        if core:
            sig_str += "" (core dumped)""
        print(sig_str, file=sys.stderr)
        if self.errors is not None:
            self.errors += sig_str + ""\n""
",if self . errors is not None :,146
"def loadLabelFile(self, labelpath):
    labeldict = {}
    if not os.path.exists(labelpath):
        f = open(labelpath, ""w"", encoding=""utf-8"")
    else:
        with open(labelpath, ""r"", encoding=""utf-8"") as f:
            data = f.readlines()
            for each in data:
                file, label = each.split(""\t"")
                if label:
                    label = label.replace(""false"", ""False"")
                    label = label.replace(""true"", ""True"")
                    labeldict[file] = eval(label)
                else:
                    labeldict[file] = []
    return labeldict
",if label :,191
"def exists_col_to_many(self, select_columns: List[str]) -> bool:
    for column in select_columns:
        if is_column_dotted(column):
            root_relation = get_column_root_relation(column)
            if self.is_relation_many_to_many(
                root_relation
            ) or self.is_relation_one_to_many(root_relation):
                return True
    return False
",if is_column_dotted ( column ) :,120
"def check_sequence_matches(seq, template):
    i = 0
    for pattern in template:
        if not isinstance(pattern, set):
            pattern = {pattern}
        got = set(seq[i : i + len(pattern)])
        assert got == pattern
        i += len(got)
","if not isinstance ( pattern , set ) :",77
"def load_modules(
    to_load, load, attr, modules_dict, excluded_aliases, loading_message=None
):
    if loading_message:
        print(loading_message)
    for name in to_load:
        module = load(name)
        if module is None or not hasattr(module, attr):
            continue
        cls = getattr(module, attr)
        if hasattr(cls, ""initialize"") and not cls.initialize():
            continue
        if hasattr(module, ""aliases""):
            for alias in module.aliases():
                if alias not in excluded_aliases:
                    modules_dict[alias] = module
        else:
            modules_dict[name] = module
    if loading_message:
        print()
","if hasattr ( module , ""aliases"" ) :",195
"def result():
    # ""global"" does not work here...
    R, V = rays, virtual_rays
    if V is not None:
        if normalize:
            V = normalize_rays(V, lattice)
        if check:
            R = PointCollection(V, lattice)
            V = PointCollection(V, lattice)
            d = lattice.dimension()
            if len(V) != d - R.dim() or (R + V).dim() != d:
                raise ValueError(
                    ""virtual rays must be linearly ""
                    ""independent and with other rays span the ambient space.""
                )
    return RationalPolyhedralFan(cones, R, lattice, is_complete, V)
",if normalize :,194
"def communicate(self, _input=None, _timeout=None) -> Tuple[bytes, bytes]:
    if parse_args().print_commands:
        if self.args != get_sudo_refresh_command():
            print_stderr(
                color_line(""=> "", 14) + "" "".join(str(arg) for arg in self.args)
            )
    stdout, stderr = super().communicate(_input, _timeout)
    self.stdout_text = stdout.decode(""utf-8"") if stdout else None
    self.stderr_text = stderr.decode(""utf-8"") if stderr else None
    return stdout, stderr
",if self . args != get_sudo_refresh_command ( ) :,154
"def convert(data):
    result = []
    for d in data:
        # noinspection PyCompatibility
        if isinstance(d, tuple) and len(d) == 2:
            result.append((d[0], None, d[1]))
        elif isinstance(d, basestring):
            result.append(d)
    return result
","if isinstance ( d , tuple ) and len ( d ) == 2 :",86
"def validate(self, value):
    try:
        value = [
            datetime.datetime.strptime(range, ""%Y-%m-%d %H:%M:%S"")
            for range in value.split("" to "")
        ]
        if (len(value) == 2) and (value[0] <= value[1]):
            return True
        else:
            return False
    except ValueError:
        return False
",if ( len ( value ) == 2 ) and ( value [ 0 ] <= value [ 1 ] ) :,110
"def rmdir(dirname):
    if dirname[-1] == os.sep:
        dirname = dirname[:-1]
    if os.path.islink(dirname):
        return  # do not clear link - we can get out of dir
    for f in os.listdir(dirname):
        if f in (""."", ""..""):
            continue
        path = dirname + os.sep + f
        if os.path.isdir(path):
            rmdir(path)
        else:
            os.unlink(path)
    os.rmdir(dirname)
",if os . path . isdir ( path ) :,137
"def onCompletion(self, text):
    res = []
    for l in text.split(""\n""):
        if not l:
            continue
        l = l.split("":"")
        if len(l) != 2:
            continue
        res.append([l[0].strip(), l[1].strip()])
    self.panel.setSlides(res)
",if len ( l ) != 2 :,93
"def pytest_collection_modifyitems(items):
    for item in items:
        if item.nodeid.startswith(""tests/infer""):
            if ""stage"" not in item.keywords:
                item.add_marker(pytest.mark.stage(""unit""))
            if ""init"" not in item.keywords:
                item.add_marker(pytest.mark.init(rng_seed=123))
","if item . nodeid . startswith ( ""tests/infer"" ) :",102
"def build_message(self, options, target):
    message = multipart.MIMEMultipart()
    for name, value in list(options.items()):
        if name == ""EMAIL_BODY"":
            self.add_body(message, value)
        elif name == ""EMAIL_ATTACHMENT"":
            self.add_attachment(message, value)
        else:  # From, To, Subject, etc.
            self.set_option(message, name, value, target)
    return message
","elif name == ""EMAIL_ATTACHMENT"" :",126
"def extend_with_zeroes(b):
    try:
        for x in b:
            x = to_constant(x)
            if isinstance(x, int):
                yield (x)
            else:
                yield (0)
        for _ in range(32):
            yield (0)
    except Exception as e:
        return
","if isinstance ( x , int ) :",99
"def _start_cluster(*, cleanup_atexit=True):
    global _default_cluster
    if _default_cluster is None:
        cluster_addr = os.environ.get(""EDGEDB_TEST_CLUSTER_ADDR"")
        if cluster_addr:
            conn_spec = json.loads(cluster_addr)
            _default_cluster = edgedb_cluster.RunningCluster(**conn_spec)
        else:
            data_dir = os.environ.get(""EDGEDB_TEST_DATA_DIR"")
            _default_cluster = _init_cluster(
                data_dir=data_dir, cleanup_atexit=cleanup_atexit
            )
    return _default_cluster
",if cluster_addr :,175
"def preprocess_raw_enwik9(input_filename, output_filename):
    with open(input_filename, ""r"") as f1:
        with open(output_filename, ""w"") as f2:
            while True:
                line = f1.readline()
                if not line:
                    break
                line = list(enwik9_norm_transform([line]))[0]
                if line != "" "" and line != """":
                    if line[0] == "" "":
                        line = line[1:]
                    f2.writelines(line + ""\n"")
",if not line :,164
"def is_entirely_italic(line):
    style = subs.styles.get(line.style, SSAStyle.DEFAULT_STYLE)
    for fragment, sty in parse_tags(line.text, style, subs.styles):
        fragment = fragment.replace(r""\h"", "" "")
        fragment = fragment.replace(r""\n"", ""\n"")
        fragment = fragment.replace(r""\N"", ""\n"")
        if not sty.italic and fragment and not fragment.isspace():
            return False
    return True
",if not sty . italic and fragment and not fragment . isspace ( ) :,128
"def __get_all_nodes(self):
    nodes = []
    next_level = [self.__tree.get_root()]
    while len(next_level) != 0:
        cur_level = next_level
        nodes += next_level
        next_level = []
        for cur_node in cur_level:
            children = cur_node.get_children()
            if children is not None:
                next_level += children
    return nodes
",if children is not None :,119
"def _openvpn_stdout(self):
    while True:
        line = self.process.stdout.readline()
        if not line:
            if self.process.poll() is not None or self.is_interrupted():
                return
            time.sleep(0.05)
            continue
        yield
        try:
            self.server.output.push_output(line)
        except:
            logger.exception(
                ""Failed to push vpn output"",
                ""server"",
                server_id=self.server.id,
            )
        yield
",if self . process . poll ( ) is not None or self . is_interrupted ( ) :,163
"def payment_received_handler(event):
    if isinstance(event.message.action, types.MessageActionPaymentSentMe):
        payment: types.MessageActionPaymentSentMe = event.message.action
        # do something after payment was received
        if payment.payload.decode(""UTF-8"") == ""product A"":
            await bot.send_message(
                event.message.from_id, ""Thank you for buying product A!""
            )
        elif payment.payload.decode(""UTF-8"") == ""product B"":
            await bot.send_message(
                event.message.from_id, ""Thank you for buying product B!""
            )
        raise events.StopPropagation
","elif payment . payload . decode ( ""UTF-8"" ) == ""product B"" :",181
"def spaces_after(token, prev, next, min=-1, max=-1, min_desc=None, max_desc=None):
    if next is not None and token.end_mark.line == next.start_mark.line:
        spaces = next.start_mark.pointer - token.end_mark.pointer
        if max != -1 and spaces > max:
            return LintProblem(
                token.start_mark.line + 1, next.start_mark.column, max_desc
            )
        elif min != -1 and spaces < min:
            return LintProblem(
                token.start_mark.line + 1, next.start_mark.column + 1, min_desc
            )
",if max != - 1 and spaces > max :,184
"def seek_to_block(self, pos):
    baseofs = 0
    ofs = 0
    for b in self.blocks:
        if ofs + b.uncompressed_size > pos:
            self.current_block = b
            break
        baseofs += b.compressed_size
        ofs += b.uncompressed_size
    else:
        self.current_block = None
        self.current_stream = BytesIO(b"""")
        return
    self.current_block_start = ofs
    self.stream.seek(self.basepos + baseofs)
    buf = BytesIO(self.stream.read(self.current_block.compressed_size))
    self.current_stream = self.current_block.decompress(buf)
",if ofs + b . uncompressed_size > pos :,190
"def rewrite_hunks(hunks):
    # type: (List[Hunk]) -> Iterator[Hunk]
    # Assumes `hunks` are sorted, and from the same file
    deltas = (hunk.b_length - hunk.a_length for hunk in hunks)
    offsets = accumulate(deltas, initial=0)
    for hunk, offset in zip(hunks, offsets):
        new_b = hunk.a_start + offset
        if hunk_of_additions_only(hunk):
            new_b += 1
        elif hunk_of_removals_only(hunk):
            new_b -= 1
        yield hunk._replace(b_start=new_b)
",elif hunk_of_removals_only ( hunk ) :,185
"def do_query(data, q):
    ret = []
    if not q:
        return ret
    qkey = q[0]
    for key, value in iterate(data):
        if len(q) == 1:
            if key == qkey:
                ret.append(value)
            elif is_iterable(value):
                ret.extend(do_query(value, q))
        else:
            if not is_iterable(value):
                continue
            if key == qkey:
                ret.extend(do_query(value, q[1:]))
            else:
                ret.extend(do_query(value, q))
    return ret
",if len ( q ) == 1 :,185
"def get_url(token, base_url):
    """"""Parse an <url> token.""""""
    if token.type == ""url"":
        return _get_url_tuple(token.value, base_url)
    elif token.type == ""function"":
        if token.name == ""attr"":
            return check_attr_function(token, ""url"")
        elif token.name == ""url"" and len(token.arguments) in (1, 2):
            # Ignore url modifiers
            # See https://drafts.csswg.org/css-values-3/#urls
            return _get_url_tuple(token.arguments[0].value, base_url)
","elif token . name == ""url"" and len ( token . arguments ) in ( 1 , 2 ) :",166
"def read(self, count):
    if self.closed:
        return self.upstream.read(count)
    try:
        while len(self.upstream) < count:
            if self.buf_in or self._poll_read(10):
                with self.buf_in:
                    self.transport.downstream_recv(self.buf_in)
            else:
                break
        return self.upstream.read(count)
    except:
        logger.debug(traceback.format_exc())
",if self . buf_in or self . _poll_read ( 10 ) :,140
"def get_timestamp_for_block(
    self, block_hash: HexBytes, max_tries: Optional[int] = 10
) -> int:
    counter = 0
    block: AttributeDict = None
    if block_hash in self._block_cache.keys():
        block = self._block_cache.get(block_hash)
    else:
        while block is None:
            if counter == max_tries:
                raise ValueError(f""Block hash {block_hash.hex()} does not exist."")
            counter += 1
            block = self._block_cache.get(block_hash)
            await asyncio.sleep(0.5)
    return block.get(""timestamp"")
",if counter == max_tries :,172
"def reader():
    batch_out = []
    for video_name in self.video_list:
        video_idx = self.video_list.index(video_name)
        video_feat = self.load_file(video_name)
        batch_out.append((video_feat, video_idx))
        if len(batch_out) == self.batch_size:
            yield batch_out
            batch_out = []
",if len ( batch_out ) == self . batch_size :,111
"def cleanup():
    gscript.message(_(""Erasing temporary files...""))
    for temp_map, maptype in temp_maps:
        if gscript.find_file(temp_map, element=maptype)[""name""]:
            gscript.run_command(
                ""g.remove"", flags=""f"", type=maptype, name=temp_map, quiet=True
            )
","if gscript . find_file ( temp_map , element = maptype ) [ ""name"" ] :",94
"def run(self):
    while True:
        try:
            with DelayedKeyboardInterrupt():
                raw_inputs = self._parent_task_queue.get()
                if self._has_stop_signal(raw_inputs):
                    self._rq.put(raw_inputs, block=True)
                    break
                if self._flow_type == BATCH:
                    self._rq.put(raw_inputs, block=True)
                elif self._flow_type == REALTIME:
                    try:
                        self._rq.put(raw_inputs, block=False)
                    except:
                        pass
        except KeyboardInterrupt:
            continue
",if self . _has_stop_signal ( raw_inputs ) :,199
"def handle_sent(self, elt):
    sent = []
    for child in elt:
        if child.tag in (""mw"", ""hi"", ""corr"", ""trunc""):
            sent += [self.handle_word(w) for w in child]
        elif child.tag in (""w"", ""c""):
            sent.append(self.handle_word(child))
        elif child.tag not in self.tags_to_ignore:
            raise ValueError(""Unexpected element %s"" % child.tag)
    return BNCSentence(elt.attrib[""n""], sent)
","elif child . tag in ( ""w"" , ""c"" ) :",141
"def bind_subscribers_to_graphql_type(self, graphql_type):
    for field, subscriber in self._subscribers.items():
        if field not in graphql_type.fields:
            raise ValueError(""Field %s is not defined on type %s"" % (field, self.name))
        graphql_type.fields[field].subscribe = subscriber
",if field not in graphql_type . fields :,84
"def _get_from_json(self, *, name, version):
    url = urljoin(self.url, posixpath.join(name, str(version), ""json""))
    async with aiohttp_session(auth=self.auth) as session:
        async with session.get(url) as response:
            if response.status == 404:
                raise PackageNotFoundError(package=name, url=url)
            response.raise_for_status()
            response = await response.json()
    dist = response[""info""][""requires_dist""] or []
    if dist:
        return dist
    # If no requires_dist then package metadata can be broken.
    # Let's check distribution files.
    return await self._get_from_files(response[""urls""])
",if response . status == 404 :,186
"def is_active(self):
    if not self.pk:
        log_level = get_setting(""LOG_MISSING_SWITCHES"")
        if log_level:
            logger.log(log_level, ""Switch %s not found"", self.name)
        if get_setting(""CREATE_MISSING_SWITCHES""):
            switch, _created = Switch.objects.get_or_create(
                name=self.name, defaults={""active"": get_setting(""SWITCH_DEFAULT"")}
            )
            cache = get_cache()
            cache.set(self._cache_key(self.name), switch)
        return get_setting(""SWITCH_DEFAULT"")
    return self.active
","if get_setting ( ""CREATE_MISSING_SWITCHES"" ) :",179
"def add_requirements(self, requirements):
    if self._legacy:
        self._legacy.add_requirements(requirements)
    else:
        run_requires = self._data.setdefault(""run_requires"", [])
        always = None
        for entry in run_requires:
            if ""environment"" not in entry and ""extra"" not in entry:
                always = entry
                break
        if always is None:
            always = {""requires"": requirements}
            run_requires.insert(0, always)
        else:
            rset = set(always[""requires""]) | set(requirements)
            always[""requires""] = sorted(rset)
","if ""environment"" not in entry and ""extra"" not in entry :",171
"def display_failures_for_single_test(result: TestResult) -> None:
    """"""Display a failure for a single method / endpoint.""""""
    display_subsection(result)
    checks = _get_unique_failures(result.checks)
    for idx, check in enumerate(checks, 1):
        message: Optional[str]
        if check.message:
            message = f""{idx}. {check.message}""
        else:
            message = None
        example = cast(Case, check.example)  # filtered in `_get_unique_failures`
        display_example(example, check.name, message, result.seed)
        # Display every time except the last check
        if idx != len(checks):
            click.echo(""\n"")
",if idx != len ( checks ) :,188
"def __call__(self, frame: FrameType, event: str, arg: Any) -> ""CallTracer"":
    code = frame.f_code
    if (
        event not in SUPPORTED_EVENTS
        or code.co_name == ""trace_types""
        or self.should_trace
        and not self.should_trace(code)
    ):
        return self
    try:
        if event == EVENT_CALL:
            self.handle_call(frame)
        elif event == EVENT_RETURN:
            self.handle_return(frame, arg)
        else:
            logger.error(""Cannot handle event %s"", event)
    except Exception:
        logger.exception(""Failed collecting trace"")
    return self
",elif event == EVENT_RETURN :,185
"def get_maps(test):
    pages = set()
    for addr in test[""pre""][""memory""].keys():
        pages.add(addr >> 12)
    for addr in test[""pos""][""memory""].keys():
        pages.add(addr >> 12)
    maps = []
    for p in sorted(pages):
        if len(maps) > 0 and maps[-1][0] + maps[-1][1] == p << 12:
            maps[-1] = (maps[-1][0], maps[-1][1] + 0x1000)
        else:
            maps.append((p << 12, 0x1000))
    return maps
",if len ( maps ) > 0 and maps [ - 1 ] [ 0 ] + maps [ - 1 ] [ 1 ] == p << 12 :,157
"def process_rotate_aes_key(self):
    if hasattr(self.options, ""rotate_aes_key"") and isinstance(
        self.options.rotate_aes_key, six.string_types
    ):
        if self.options.rotate_aes_key.lower() == ""true"":
            self.options.rotate_aes_key = True
        elif self.options.rotate_aes_key.lower() == ""false"":
            self.options.rotate_aes_key = False
","if self . options . rotate_aes_key . lower ( ) == ""true"" :",122
"def apply_figure(self, figure):
    super(legend_text_legend, self).apply_figure(figure)
    properties = self.properties.copy()
    with suppress(KeyError):
        del properties[""margin""]
    with suppress(KeyError):
        texts = figure._themeable[""legend_text_legend""]
        for text in texts:
            if not hasattr(text, ""_x""):  # textarea
                text = text._text
            text.set(**properties)
","if not hasattr ( text , ""_x"" ) :",121
"def tearDown(self):
    for i in range(len(self.tree) - 1, -1, -1):
        s = os.path.join(self.root, self.tree[i])
        if not ""."" in s:
            os.rmdir(s)
        else:
            os.remove(s)
    os.rmdir(self.root)
","if not ""."" in s :",93
"def _get_id(self, type, id):
    fields = id.split("":"")
    if len(fields) >= 3:
        if type != fields[-2]:
            logger.warning(
                ""Expected id of type %s but found type %s %s"", type, fields[-2], id
            )
        return fields[-1]
    fields = id.split(""/"")
    if len(fields) >= 3:
        itype = fields[-2]
        if type != itype:
            logger.warning(
                ""Expected id of type %s but found type %s %s"", type, itype, id
            )
        return fields[-1].split(""?"")[0]
    return id
",if type != itype :,178
"def candidates() -> Generator[""Symbol"", None, None]:
    s = self
    if Symbol.debug_lookup:
        Symbol.debug_print(""searching in self:"")
        print(s.to_string(Symbol.debug_indent + 1), end="""")
    while True:
        if matchSelf:
            yield s
        if recurseInAnon:
            yield from s.children_recurse_anon
        else:
            yield from s._children
        if s.siblingAbove is None:
            break
        s = s.siblingAbove
        if Symbol.debug_lookup:
            Symbol.debug_print(""searching in sibling:"")
            print(s.to_string(Symbol.debug_indent + 1), end="""")
",if matchSelf :,190
"def records(account_id):
    """"""Fetch locks data""""""
    s = boto3.Session()
    table = s.resource(""dynamodb"").Table(""Sphere11.Dev.ResourceLocks"")
    results = table.scan()
    for r in results[""Items""]:
        if ""LockDate"" in r:
            r[""LockDate""] = datetime.fromtimestamp(r[""LockDate""])
        if ""RevisionDate"" in r:
            r[""RevisionDate""] = datetime.fromtimestamp(r[""RevisionDate""])
    print(tabulate.tabulate(results[""Items""], headers=""keys"", tablefmt=""fancy_grid""))
","if ""RevisionDate"" in r :",149
"def _handle_errors(errors):
    """"""Log out and possibly reraise errors during import.""""""
    if not errors:
        return
    log_all = True  # pylint: disable=unused-variable
    err_msg = ""T2T: skipped importing {num_missing} data_generators modules.""
    print(err_msg.format(num_missing=len(errors)))
    for module, err in errors:
        err_str = str(err)
        if log_all:
            print(""Did not import module: %s; Cause: %s"" % (module, err_str))
        if not _is_import_err_msg(err_str, module):
            print(""From module %s"" % module)
            raise err
","if not _is_import_err_msg ( err_str , module ) :",184
"def find_needle(self, tree, focused=None):
    if isinstance(tree, list):
        for el in tree:
            res = self.find_needle(el, focused)
            if res:
                return res
    elif isinstance(tree, dict):
        nodes = tree.get(""nodes"", []) + tree.get(""floating_nodes"", [])
        if focused:
            for node in nodes:
                if node[""id""] == focused[""id""]:
                    return tree
        elif tree[""focused""]:
            return tree
        return self.find_needle(nodes, focused)
    return {}
",if res :,169
"def available_datasets(self):
    """"""Automatically determine datasets provided by this file""""""
    res = self.resolution
    coordinates = [""pixel_longitude"", ""pixel_latitude""]
    for var_name, val in self.file_content.items():
        if isinstance(val, netCDF4.Variable):
            ds_info = {
                ""file_type"": self.filetype_info[""file_type""],
                ""resolution"": res,
            }
            if not self.is_geo:
                ds_info[""coordinates""] = coordinates
            yield DatasetID(name=var_name, resolution=res), ds_info
",if not self . is_geo :,165
"def get_subkeys(self, key):
    # TODO: once we revamp the registry emulation,
    # make this better
    parent_path = key.get_path()
    subkeys = []
    for k in self.keys:
        test_path = k.get_path()
        if test_path.lower().startswith(parent_path.lower()):
            sub = test_path[len(parent_path) :]
            if sub.startswith(""\\""):
                sub = sub[1:]
            end_slash = sub.find(""\\"")
            if end_slash >= 0:
                sub = sub[:end_slash]
            if not sub:
                continue
            subkeys.append(sub)
    return subkeys
",if test_path . lower ( ) . startswith ( parent_path . lower ( ) ) :,192
"def default(self, o):
    try:
        if type(o) == datetime.datetime:
            return str(o)
        else:
            # remove unwanted attributes from the provider object during conversion to json
            if hasattr(o, ""profile""):
                del o.profile
            if hasattr(o, ""credentials""):
                del o.credentials
            if hasattr(o, ""metadata_path""):
                del o.metadata_path
            if hasattr(o, ""services_config""):
                del o.services_config
            return vars(o)
    except Exception as e:
        return str(o)
","if hasattr ( o , ""credentials"" ) :",172
"def submit(self, fn, *args, **kwargs):
    with self._shutdown_lock:
        if self._shutdown:
            raise RuntimeError(""cannot schedule new futures after shutdown"")
        f = _base.Future()
        w = _WorkItem(f, fn, args, kwargs)
        self._work_queue.put(w)
        self._adjust_thread_count()
        return f
",if self . _shutdown :,101
"def __viewerKeyPress(viewer, event):
    view = viewer.view()
    if not isinstance(view, GafferSceneUI.SceneView):
        return False
    if event == __editSourceKeyPress:
        selectedPath = __sceneViewSelectedPath(view)
        if selectedPath is not None:
            __editSourceNode(view.getContext(), view[""in""], selectedPath)
        return True
    elif event == __editTweaksKeyPress:
        selectedPath = __sceneViewSelectedPath(view)
        if selectedPath is not None:
            __editTweaksNode(view.getContext(), view[""in""], selectedPath)
        return True
",if selectedPath is not None :,175
"def _split_to_option_groups_and_paths(self, args):
    opt_groups = []
    current = []
    for arg in args:
        if arg.replace(""-"", """") == """" and len(arg) >= 3:
            opts = self._arg_parser.parse_args(current)[0]
            opt_groups.append(opts)
            current = []
        else:
            current.append(arg)
    if opt_groups:
        return opt_groups, current
    raise ValueError(""Nothing to split"")
","if arg . replace ( ""-"" , """" ) == """" and len ( arg ) >= 3 :",136
"def _on_change(self):
    changed = False
    self.save()
    for key, value in self.data.items():
        if isinstance(value, bool):
            if value:
                changed = True
                break
        if isinstance(value, int):
            if value != 1:
                changed = True
                break
        elif value is None:
            continue
        elif len(value) != 0:
            changed = True
            break
    self._reset_button.disabled = not changed
",if value :,145
"def wait_for_child(pid, timeout=1.0):
    deadline = mitogen.core.now() + timeout
    while timeout < mitogen.core.now():
        try:
            target_pid, status = os.waitpid(pid, os.WNOHANG)
            if target_pid == pid:
                return
        except OSError:
            e = sys.exc_info()[1]
            if e.args[0] == errno.ECHILD:
                return
        time.sleep(0.05)
    assert False, ""wait_for_child() timed out""
",if e . args [ 0 ] == errno . ECHILD :,156
"def _get_os_version_lsb_release():
    try:
        output = subprocess.check_output(""lsb_release -sri"", shell=True)
        lines = output.strip().split()
        name, version = lines
        if version.lower() == ""rolling"":
            version = """"
        return name, version
    except:
        return _get_os_version_uname()
","if version . lower ( ) == ""rolling"" :",101
"def _check_snapshot_status_healthy(self, snapshot_uuid):
    status = """"
    try:
        while True:
            status, locked = self._get_snapshot_status(snapshot_uuid)
            if not locked:
                break
            eventlet.sleep(2)
    except Exception:
        with excutils.save_and_reraise_exception():
            LOG.exception(""Failed to get snapshot status. [%s]"", snapshot_uuid)
    LOG.debug(
        ""Lun [%(snapshot)s], status [%(status)s]."",
        {""snapshot"": snapshot_uuid, ""status"": status},
    )
    return status == ""Healthy""
",if not locked :,172
"def CountButtons(self):
    """"""Returns the number of visible buttons in the docked pane.""""""
    n = 0
    if self.HasCaption() or self.HasCaptionLeft():
        if isinstance(wx.GetTopLevelParent(self.window), AuiFloatingFrame):
            return 1
        if self.HasCloseButton():
            n += 1
        if self.HasMaximizeButton():
            n += 1
        if self.HasMinimizeButton():
            n += 1
        if self.HasPinButton():
            n += 1
    return n
",if self . HasCloseButton ( ) :,149
"def _url_encode_impl(obj, charset, encode_keys, sort, key):
    from .datastructures import iter_multi_items
    iterable = iter_multi_items(obj)
    if sort:
        iterable = sorted(iterable, key=key)
    for key, value in iterable:
        if value is None:
            continue
        if not isinstance(key, bytes):
            key = text_type(key).encode(charset)
        if not isinstance(value, bytes):
            value = text_type(value).encode(charset)
        yield _fast_url_quote_plus(key) + ""="" + _fast_url_quote_plus(value)
",if value is None :,168
"def get_response(self, exc_fmt=None):
    self.callback = None
    if __debug__:
        self.parent._log(3, ""%s:%s.ready.wait"" % (self.name, self.tag))
    self.ready.wait()
    if self.aborted is not None:
        typ, val = self.aborted
        if exc_fmt is None:
            exc_fmt = ""%s - %%s"" % typ
        raise typ(exc_fmt % str(val))
    return self.response
",if exc_fmt is None :,131
"def extract_items(self):
    responses = self.fetch()
    items = []
    for response in responses:
        page_key = response.meta.get(""page_key"") or response.url
        item = {""key"": page_key, ""items"": None, ""templates"": None}
        extracted_items = [
            dict(i) for i in self.spider.parse(response) if not isinstance(i, Request)
        ]
        if extracted_items:
            item[""items""] = extracted_items
            item[""templates""] = [
                i[""_template""] for i in extracted_items if i.get(""_template"")
            ]
            items.append(item)
    return items
",if extracted_items :,182
"def fit_one(self, x):
    for i, xi in x.items():
        if self.with_centering:
            self.median[i].update(xi)
        if self.with_scaling:
            self.iqr[i].update(xi)
    return self
",if self . with_scaling :,75
"def find_word_bounds(self, text, index, allowed_chars):
    right = left = index
    done = False
    while not done:
        if left == 0:
            done = True
        elif not self.word_boundary_char(text[left - 1]):
            left -= 1
        else:
            done = True
    done = False
    while not done:
        if right == len(text):
            done = True
        elif not self.word_boundary_char(text[right]):
            right += 1
        else:
            done = True
    return left, right
",elif not self . word_boundary_char ( text [ left - 1 ] ) :,159
"def _validate_duplicate_detection_history_time_window(namespace):
    if namespace.duplicate_detection_history_time_window:
        if iso8601pattern.match(namespace.duplicate_detection_history_time_window):
            pass
        elif timedeltapattern.match(namespace.duplicate_detection_history_time_window):
            pass
        else:
            raise CLIError(
                ""--duplicate-detection-history-time-window Value Error : {0} value is not in ISO 8601 timespan / duration format. e.g. PT10M for duration of 10 min or 00:10:00 for duration of 10 min"".format(
                    namespace.duplicate_detection_history_time_window
                )
            )
",elif timedeltapattern . match ( namespace . duplicate_detection_history_time_window ) :,187
"def get_subkeys(self, key):
    # TODO: once we revamp the registry emulation,
    # make this better
    parent_path = key.get_path()
    subkeys = []
    for k in self.keys:
        test_path = k.get_path()
        if test_path.lower().startswith(parent_path.lower()):
            sub = test_path[len(parent_path) :]
            if sub.startswith(""\\""):
                sub = sub[1:]
            end_slash = sub.find(""\\"")
            if end_slash >= 0:
                sub = sub[:end_slash]
            if not sub:
                continue
            subkeys.append(sub)
    return subkeys
",if not sub :,192
"def generator(self, data):
    if self._config.SILENT:
        silent_vars = self._get_silent_vars()
    for task in data:
        for var, val in task.environment_variables():
            if self._config.SILENT:
                if var in silent_vars:
                    continue
            yield (
                0,
                [
                    int(task.UniqueProcessId),
                    str(task.ImageFileName),
                    Address(task.Peb.ProcessParameters.Environment),
                    str(var),
                    str(val),
                ],
            )
",if self . _config . SILENT :,182
"def start_requests(self):
    if self.fail_before_yield:
        1 / 0
    for s in range(100):
        qargs = {""total"": 10, ""seed"": s}
        url = self.mockserver.url(""/follow?%s"") % urlencode(qargs, doseq=1)
        yield Request(url, meta={""seed"": s})
        if self.fail_yielding:
            2 / 0
    assert self.seedsseen, ""All start requests consumed before any download happened""
",if self . fail_yielding :,127
"def populateGridlines(self):
    cTicks = self.getSystemCurve(self.ticksId)
    cGridlines = self.getSystemCurve(self.gridlinesId)
    cGridlines.clearPoints()
    nTicks = cTicks.getNPoints()
    for iTick in range(nTicks):
        if self.hasGridlines and (iTick % self.ticksPerGridline) == 0:
            p = cTicks.getPoint(iTick)
            cGridlines.addPoint(p.getX(), p.getY())
",if self . hasGridlines and ( iTick % self . ticksPerGridline ) == 0 :,139
"def handle_before_events(request, event_list):
    if not event_list:
        return """"
    if not hasattr(event_list, ""__iter__""):
        project = event_list.project
        event_list = [event_list]
    else:
        projects = set(e.project for e in event_list)
        if len(projects) == 1:
            project = projects.pop()
        else:
            project = None
    for plugin in plugins.for_project(project):
        safe_execute(plugin.before_events, request, event_list)
    return """"
",if len ( projects ) == 1 :,152
"def handle_parse_result(self, ctx, opts, args):
    if self.name in opts:
        if self.mutually_exclusive.intersection(opts):
            self._raise_exclusive_error()
        if self.multiple and len(set(opts[self.name])) > 1:
            self._raise_exclusive_error()
    return super(MutuallyExclusiveOption, self).handle_parse_result(ctx, opts, args)
",if self . mutually_exclusive . intersection ( opts ) :,108
"def current_word(cursor_offset, line):
    """"""the object.attribute.attribute just before or under the cursor""""""
    pos = cursor_offset
    start = pos
    end = pos
    word = None
    for m in current_word_re.finditer(line):
        if m.start(1) < pos and m.end(1) >= pos:
            start = m.start(1)
            end = m.end(1)
            word = m.group(1)
    if word is None:
        return None
    return LinePart(start, end, word)
",if m . start ( 1 ) < pos and m . end ( 1 ) >= pos :,147
"def query_to_script_path(path, query):
    if path != ""*"":
        script = os.path.join(path, query.split("" "")[0])
        if not os.path.exists(script):
            raise IOError(""Script '{}' not found in script directory"".format(query))
        return os.path.join(path, query).split("" "")
    return query
",if not os . path . exists ( script ) :,93
"def expand(self, pbegin):
    # TODO(b/151921205): we have to do an identity map for unmodified
    # PCollections below because otherwise we get an error from beam.
    identity_map = ""Identity"" >> beam.Map(lambda x: x)
    if self._dataset_key.is_flattened_dataset_key():
        if self._flat_pcollection:
            return self._flat_pcollection | identity_map
        else:
            return list(
                self._pcollection_dict.values()
            ) | ""FlattenAnalysisInputs"" >> beam.Flatten(pipeline=pbegin.pipeline)
    else:
        return self._pcollection_dict[self._dataset_key] | identity_map
",if self . _flat_pcollection :,183
"def processCoords(coords):
    newcoords = deque()
    for (x, y, z) in coords:
        for _dir, offsets in faceDirections:
            if _dir == FaceYIncreasing:
                continue
            dx, dy, dz = offsets
            p = (x + dx, y + dy, z + dz)
            if p not in box:
                continue
            nx, ny, nz = p
            if level.blockAt(nx, ny, nz) == 0:
                level.setBlockAt(nx, ny, nz, waterID)
                newcoords.append(p)
    return newcoords
","if level . blockAt ( nx , ny , nz ) == 0 :",173
"def delete_byfilter(userId, remove=True, session=None, **dbfilter):
    if not session:
        session = db.Session
    ret = False
    results = session.query(ObjectStorageMetadata).filter_by(**dbfilter)
    if results:
        for result in results:
            if remove:
                session.delete(result)
            else:
                result.update(
                    {
                        ""record_state_key"": ""to_delete"",
                        ""record_state_val"": str(time.time()),
                    }
                )
            ret = True
    return ret
",if remove :,176
"def fields(self, fields):
    fields_xml = """"
    for field in fields:
        field_dict = DEFAULT_FIELD.copy()
        field_dict.update(field)
        if self.unique_key_field == field[""name""]:
            field_dict[""required""] = ""true""
        fields_xml += FIELD_XML_TEMPLATE % field_dict + ""\n""
    self.xml = force_unicode(
        force_unicode(self.xml).replace(
            u""<!-- REPLACE FIELDS -->"", force_unicode(fields_xml)
        )
    )
","if self . unique_key_field == field [ ""name"" ] :",146
"def get_all_users(self, access_token, timeout=None):
    if timeout is None:
        timeout = DEFAULT_TIMEOUT
    headers = self.retrieve_header(access_token)
    try:
        response = await self.standard_request(
            ""get"", ""/walkoff/api/users"", timeout=DEFAULT_TIMEOUT, headers=headers
        )
        if response.status == 200:
            resp = await response.json()
            return resp, ""Success""
        else:
            return ""Invalid Credentials""
    except asyncio.CancelledError:
        return False, ""TimedOut""
",if response . status == 200 :,156
"def set_val():
    idx = 0
    for idx in range(0, len(model)):
        row = model[idx]
        if value and row[0] == value:
            break
        if idx == len(os_widget.get_model()) - 1:
            idx = -1
    os_widget.set_active(idx)
    if idx == -1:
        os_widget.set_active(0)
    if idx >= 0:
        return row[1]
    if self.show_all_os:
        return None
",if idx == len ( os_widget . get_model ( ) ) - 1 :,142
"def translate_module_name(module: str, relative: int) -> Tuple[str, int]:
    for pkg in VENDOR_PACKAGES:
        for alt in ""six.moves"", ""six"":
            substr = ""{}.{}"".format(pkg, alt)
            if module.endswith(""."" + substr) or (module == substr and relative):
                return alt, 0
            if ""."" + substr + ""."" in module:
                return alt + ""."" + module.partition(""."" + substr + ""."")[2], 0
    return module, relative
","if ""."" + substr + ""."" in module :",132
"def escape(m):
    all, tail = m.group(0, 1)
    assert all.startswith(""\\"")
    esc = simple_escapes.get(tail)
    if esc is not None:
        return esc
    if tail.startswith(""x""):
        hexes = tail[1:]
        if len(hexes) < 2:
            raise ValueError(""invalid hex string escape ('\\%s')"" % tail)
        try:
            i = int(hexes, 16)
        except ValueError:
            raise ValueError(""invalid hex string escape ('\\%s')"" % tail)
    else:
        try:
            i = int(tail, 8)
        except ValueError:
            raise ValueError(""invalid octal string escape ('\\%s')"" % tail)
    return chr(i)
",if len ( hexes ) < 2 :,198
"def __get_k8s_container_name(self, job_wrapper):
    # These must follow a specific regex for Kubernetes.
    raw_id = job_wrapper.job_destination.id
    if isinstance(raw_id, str):
        cleaned_id = re.sub(""[^-a-z0-9]"", ""-"", raw_id)
        if cleaned_id.startswith(""-"") or cleaned_id.endswith(""-""):
            cleaned_id = ""x%sx"" % cleaned_id
        return cleaned_id
    return ""job-container""
","if cleaned_id . startswith ( ""-"" ) or cleaned_id . endswith ( ""-"" ) :",131
"def _power_exact(y, xc, yc, xe):
    yc, ye = y.int, y.exp
    while yc % 10 == 0:
        yc //= 10
        ye += 1
    if xc == 1:
        xe *= yc
        while xe % 10 == 0:
            xe //= 10
            ye += 1
        if ye < 0:
            return None
        exponent = xe * 10 ** ye
        if y and xe:
            xc = exponent
        else:
            xc = 0
        return 5
",if y and xe :,144
"def lpush(key, *vals, **kwargs):
    ttl = kwargs.get(""ttl"")
    cap = kwargs.get(""cap"")
    if not ttl and not cap:
        _client.lpush(key, *vals)
    else:
        pipe = _client.pipeline()
        pipe.lpush(key, *vals)
        if cap:
            pipe.ltrim(key, 0, cap)
        if ttl:
            pipe.expire(key, ttl)
        pipe.execute()
",if cap :,131
"def render_headers(self) -> bytes:
    if not hasattr(self, ""_headers""):
        parts = [
            b""Content-Disposition: form-data; "",
            format_form_param(""name"", self.name),
        ]
        if self.filename:
            filename = format_form_param(""filename"", self.filename)
            parts.extend([b""; "", filename])
        if self.content_type is not None:
            content_type = self.content_type.encode()
            parts.extend([b""\r\nContent-Type: "", content_type])
        parts.append(b""\r\n\r\n"")
        self._headers = b"""".join(parts)
    return self._headers
",if self . content_type is not None :,189
"def validate_custom_field_data(field_type: int, field_data: ProfileFieldData) -> None:
    try:
        if field_type == CustomProfileField.CHOICE:
            # Choice type field must have at least have one choice
            if len(field_data) < 1:
                raise JsonableError(_(""Field must have at least one choice.""))
            validate_choice_field_data(field_data)
        elif field_type == CustomProfileField.EXTERNAL_ACCOUNT:
            validate_external_account_field_data(field_data)
    except ValidationError as error:
        raise JsonableError(error.message)
",if field_type == CustomProfileField . CHOICE :,160
"def get_data(self, path):
    """"""Gross hack to contort loader to deal w/ load_*()'s bad API.""""""
    if self.file and path == self.path:
        if not self.file.closed:
            file = self.file
        else:
            self.file = file = open(self.path, ""r"")
        with file:
            # Technically should be returning bytes, but
            # SourceLoader.get_code() just passed what is returned to
            # compile() which can handle str. And converting to bytes would
            # require figuring out the encoding to decode to and
            # tokenize.detect_encoding() only accepts bytes.
            return file.read()
    else:
        return super().get_data(path)
",if not self . file . closed :,195
"def handle_read(self):
    """"""Called when there is data waiting to be read.""""""
    try:
        chunk = self.recv(self.ac_in_buffer_size)
    except RetryError:
        pass
    except socket.error:
        self.handle_error()
    else:
        self.tot_bytes_received += len(chunk)
        if not chunk:
            self.transfer_finished = True
            # self.close()  # <-- asyncore.recv() already do that...
            return
        if self._data_wrapper is not None:
            chunk = self._data_wrapper(chunk)
        try:
            self.file_obj.write(chunk)
        except OSError as err:
            raise _FileReadWriteError(err)
",if not chunk :,200
"def _swig_extract_dependency_files(self, src):
    dep = []
    for line in open(src):
        if line.startswith(""#include"") or line.startswith(""%include""):
            line = line.split("" "")[1].strip(""""""'""\r\n"""""")
            if not (""<"" in line or line in dep):
                dep.append(line)
    return [i for i in dep if os.path.exists(i)]
","if line . startswith ( ""#include"" ) or line . startswith ( ""%include"" ) :",111
"def buffer(self, lines, scroll_end=True, scroll_if_editing=False):
    ""Add data to be displayed in the buffer.""
    self.values.extend(lines)
    if scroll_end:
        if not self.editing:
            self.start_display_at = len(self.values) - len(self._my_widgets)
        elif scroll_if_editing:
            self.start_display_at = len(self.values) - len(self._my_widgets)
",elif scroll_if_editing :,124
"def test_getline(self):
    with tokenize.open(self.file_name) as fp:
        for index, line in enumerate(fp):
            if not line.endswith(""\n""):
                line += ""\n""
            cached_line = linecache.getline(self.file_name, index + 1)
            self.assertEqual(line, cached_line)
","if not line . endswith ( ""\n"" ) :",97
"def selectRow(self, rowNumber, highlight=None):
    if rowNumber == ""h"":
        rowNumber = 0
    else:
        rowNumber = int(rowNumber) + 1
    if 1 > rowNumber >= len(self.cells) + 1:
        raise Exception(""Invalid row number."")
    else:
        selected = self.cells[rowNumber][0].selected
        for cell in self.cells[rowNumber]:
            if highlight is None:
                if selected:
                    cell.deselect()
                else:
                    cell.select()
            else:
                if highlight:
                    cell.mouseEnter()
                else:
                    cell.mouseLeave()
",if highlight is None :,193
"def put(self, session):
    with sess_lock:
        self.parent.put(session)
        # Do not store the session if skip paths
        for sp in self.skip_paths:
            if request.path.startswith(sp):
                return
        if session.sid in self._cache:
            try:
                del self._cache[session.sid]
            except Exception:
                pass
        self._cache[session.sid] = session
    self._normalize()
",if session . sid in self . _cache :,133
"def TryMerge(self, d):
    while d.avail() > 0:
        tt = d.getVarInt32()
        if tt == 10:
            length = d.getVarInt32()
            tmp = ProtocolBuffer.Decoder(d.buffer(), d.pos(), d.pos() + length)
            d.skip(length)
            self.add_status().TryMerge(tmp)
            continue
        if tt == 18:
            self.add_doc_id(d.getPrefixedString())
            continue
        if tt == 0:
            raise ProtocolBuffer.ProtocolBufferDecodeError
        d.skipData(tt)
",if tt == 18 :,169
"def extract(self, zip):
    max_nb = maxNbFile(self)
    for index, field in enumerate(zip.array(""file"")):
        if max_nb is not None and max_nb <= index:
            self.warning(
                ""ZIP archive contains many files, but only first %s files are processed""
                % max_nb
            )
            break
        self.processFile(field)
",if max_nb is not None and max_nb <= index :,110
"def get_norm(norm, out_channels):
    if isinstance(norm, str):
        if len(norm) == 0:
            return None
        norm = {
            ""BN"": BatchNorm2d,
            ""GN"": lambda channels: nn.GroupNorm(32, channels),
            ""nnSyncBN"": nn.SyncBatchNorm,  # keep for debugging
            """": lambda x: x,
        }[norm]
    return norm(out_channels)
",if len ( norm ) == 0 :,118
"def execute(self):
    if self._dirty or not self._qr:
        model_class = self.model_class
        query_meta = self.get_query_meta()
        if self._tuples:
            ResultWrapper = TuplesQueryResultWrapper
        elif self._dicts:
            ResultWrapper = DictQueryResultWrapper
        elif self._naive or not self._joins or self.verify_naive():
            ResultWrapper = NaiveQueryResultWrapper
        elif self._aggregate_rows:
            ResultWrapper = AggregateQueryResultWrapper
        else:
            ResultWrapper = ModelQueryResultWrapper
        self._qr = ResultWrapper(model_class, self._execute(), query_meta)
        self._dirty = False
        return self._qr
    else:
        return self._qr
",elif self . _aggregate_rows :,198
"def emitIpToDomainsData(self, data, event):
    self.emitRawRirData(data, event)
    domains = data.get(""domains"")
    if isinstance(domains, list):
        for domain in domains:
            if self.checkForStop():
                return None
            domain = domain.strip()
            if domain:
                self.emitHostname(domain, event)
",if domain :,105
"def delete(self):
    from weblate.trans.models import Change, Suggestion, Vote
    fast_deletes = []
    for item in self.fast_deletes:
        if item.model is Suggestion:
            fast_deletes.append(Vote.objects.filter(suggestion__in=item))
            fast_deletes.append(Change.objects.filter(suggestion__in=item))
        fast_deletes.append(item)
    self.fast_deletes = fast_deletes
    return super().delete()
",if item . model is Suggestion :,124
"def token(self):
    if not self._token:
        try:
            cookie_token = self.state[""request""].headers.cookie[CSRF_TOKEN].value
        except KeyError:
            cookie_token = """"
        if len(cookie_token) == TOKEN_LENGTH:
            self._token = cookie_token
        else:
            self._token = get_random_string(TOKEN_LENGTH)
    return self._token
",if len ( cookie_token ) == TOKEN_LENGTH :,113
"def get_logs(last_file=None, last_time=None):
    try:
        response = client.get_logs(last_file=last_file, last_time=last_time)
        get_logs_streamer(
            show_timestamp=not hide_time,
            all_containers=all_containers,
            all_info=all_info,
        )(response)
        return response
    except (ApiException, HTTPError) as e:
        if not follow:
            handle_cli_error(
                e,
                message=""Could not get logs for run `{}`."".format(client.run_uuid),
            )
            sys.exit(1)
",if not follow :,179
"def update(self, targets):
    Section.update(self, targets)
    outputNames = set()
    for target in targets:
        g = target.globals()
        outputNames.update([k for k in g.keys() if k.startswith(""output:"")])
    rows = []
    outputNames = sorted(outputNames)
    for outputName in outputNames:
        row = self.__rows.get(outputName)
        if row is None:
            row = _OutputRow(outputName)
            self.__rows[outputName] = row
        row.update(targets)
        row.setAlternate(len(rows) % 2)
        rows.append(row)
    self._mainColumn()[:] = rows
",if row is None :,181
"def getBranches(self):
    returned = []
    for git_branch_line in self._executeGitCommandAssertSuccess(""branch"").stdout:
        if git_branch_line.startswith(""*""):
            git_branch_line = git_branch_line[1:]
        git_branch_line = git_branch_line.strip()
        if BRANCH_ALIAS_MARKER in git_branch_line:
            alias_name, aliased = git_branch_line.split(BRANCH_ALIAS_MARKER)
            returned.append(branch.LocalBranchAlias(self, alias_name, aliased))
        else:
            returned.append(branch.LocalBranch(self, git_branch_line))
    return returned
",if BRANCH_ALIAS_MARKER in git_branch_line :,178
"def has_bad_headers(self):
    headers = [self.sender, self.reply_to] + self.recipients
    for header in headers:
        if _has_newline(header):
            return True
    if self.subject:
        if _has_newline(self.subject):
            for linenum, line in enumerate(self.subject.split(""\r\n"")):
                if not line:
                    return True
                if linenum > 0 and line[0] not in ""\t "":
                    return True
                if _has_newline(line):
                    return True
                if len(line.strip()) == 0:
                    return True
    return False
","if linenum > 0 and line [ 0 ] not in ""\t "" :",186
"def resolve_references(self, note, reflist):
    assert len(note[""ids""]) == 1
    id = note[""ids""][0]
    for ref in reflist:
        if ref.resolved:
            continue
        ref.delattr(""refname"")
        ref[""refid""] = id
        assert len(ref[""ids""]) == 1
        note.add_backref(ref[""ids""][0])
        ref.resolved = 1
    note.resolved = 1
",if ref . resolved :,118
"def pickPath(self, color):
    self.path[color] = ()
    currentPos = self.starts[color]
    while True:
        minDist = None
        minGuide = None
        for guide in self.guides[color]:
            guideDist = dist(currentPos, guide)
            if minDist == None or guideDist < minDist:
                minDist = guideDist
                minGuide = guide
        if dist(currentPos, self.ends[color]) == 1:
            return
        if minGuide == None:
            return
        self.path[color] = self.path[color] + (minGuide,)
        currentPos = minGuide
        self.guides[color].remove(minGuide)
","if dist ( currentPos , self . ends [ color ] ) == 1 :",192
"def __hierarchyViewKeyPress(hierarchyView, event):
    if event == __editSourceKeyPress:
        selectedPath = __hierarchyViewSelectedPath(hierarchyView)
        if selectedPath is not None:
            __editSourceNode(
                hierarchyView.getContext(), hierarchyView.scene(), selectedPath
            )
        return True
    elif event == __editTweaksKeyPress:
        selectedPath = __hierarchyViewSelectedPath(hierarchyView)
        if selectedPath is not None:
            __editTweaksNode(
                hierarchyView.getContext(), hierarchyView.scene(), selectedPath
            )
        return True
",if selectedPath is not None :,173
"def getSubsegments(self):
    for num, localdata in self.lfh.LocalData:
        for bucket, seginfo in localdata.SegmentInfo:
            if seginfo.ActiveSubsegment == 0:
                continue
            yield Win32Subsegment(self.trace, self.heap, seginfo.ActiveSubsegment)
",if seginfo . ActiveSubsegment == 0 :,84
"def test_full_hd_bluray(self):
    cur_test = ""full_hd_bluray""
    cur_qual = common.Quality.FULLHDBLURAY
    for name, tests in iteritems(self.test_cases):
        for test in tests:
            if name == cur_test:
                self.assertEqual(cur_qual, common.Quality.name_quality(test))
            else:
                self.assertNotEqual(cur_qual, common.Quality.name_quality(test))
",if name == cur_test :,135
"def calc(self, arg):
    op = arg[""op""]
    if op == ""C"":
        self.clear()
        return str(self.current)
    num = decimal.Decimal(arg[""num""])
    if self.op:
        if self.op == ""+"":
            self.current += num
        elif self.op == ""-"":
            self.current -= num
        elif self.op == ""*"":
            self.current *= num
        elif self.op == ""/"":
            self.current /= num
        self.op = op
    else:
        self.op = op
        self.current = num
    res = str(self.current)
    if op == ""="":
        self.clear()
    return res
","elif self . op == ""/"" :",187
"def strip_export_type(path):
    matched = re.search(r""#([a-zA-Z0-9\-]+\\+[a-zA-Z0-9\-]+)?$"", path.encode(""utf-8""))
    mime_type = None
    if matched:
        fragment = matched.group(0)
        mime_type = matched.group(1)
        if mime_type is not None:
            mime_type = mime_type.replace(""+"", ""/"")
        path = path[: -len(fragment)]
    return (path, mime_type)
",if mime_type is not None :,137
"def _save_as_module(file, data, binary=False):
    if not data:
        return
    with open(file, ""w"") as f:
        f.write(""DATA="")
        if binary:
            f.write('""')
            f.write(base64.b64encode(data).decode(""ascii""))
            f.write('""')
        else:
            f.write(str(data).replace(""\\\\"", ""\\""))
        f.flush()
",if binary :,120
"def ProcessStringLiteral(self):
    if self._lastToken == None or self._lastToken.type == self.OpenBrace:
        text = super(JavaScriptBaseLexer, self).text
        if text == '""use strict""' or text == ""'use strict'"":
            if len(self._scopeStrictModes) > 0:
                self._scopeStrictModes.pop()
            self._useStrictCurrent = True
            self._scopeStrictModes.append(self._useStrictCurrent)
","if text == '""use strict""' or text == ""'use strict'"" :",124
"def run(self, ttl=None):
    self.zeroconf = zeroconf.Zeroconf()
    zeroconf.ServiceBrowser(self.zeroconf, self.domain, MDNSHandler(self))
    if ttl:
        gobject.timeout_add(ttl * 1000, self.shutdown)
    self.__running = True
    self.__mainloop = gobject.MainLoop()
    context = self.__mainloop.get_context()
    while self.__running:
        try:
            if context.pending():
                context.iteration(True)
            else:
                time.sleep(0.1)
        except KeyboardInterrupt:
            break
    self.zeroconf.close()
    logger.debug(""MDNSListener.run() quit"")
",if context . pending ( ) :,187
"def topology_change_notify(self, port_state):
    notice = False
    if port_state is PORT_STATE_FORWARD:
        for port in self.ports.values():
            if port.role is DESIGNATED_PORT:
                notice = True
                break
    else:
        notice = True
    if notice:
        self.send_event(EventTopologyChange(self.dp))
        if self.is_root_bridge:
            self._transmit_tc_bpdu()
        else:
            self._transmit_tcn_bpdu()
",if self . is_root_bridge :,154
"def close_open_fds(keep=None):  # noqa
    keep = [maybe_fileno(f) for f in (keep or []) if maybe_fileno(f) is not None]
    for fd in reversed(range(get_fdmax(default=2048))):
        if fd not in keep:
            try:
                os.close(fd)
            except OSError as exc:
                if exc.errno != errno.EBADF:
                    raise
",if exc . errno != errno . EBADF :,120
"def collect_attributes(options, node, master_list):
    """"""Collect all attributes""""""
    for ii in node.instructions:
        if field_check(ii, ""attributes""):
            s = getattr(ii, ""attributes"")
            if isinstance(s, list):
                for x in s:
                    if x not in master_list:
                        master_list.append(x)
            elif s != None and s not in master_list:
                master_list.append(s)
    for nxt in node.next.values():
        collect_attributes(options, nxt, master_list)
",elif s != None and s not in master_list :,161
"def remove_test_run_directories(expiry_time: int = 60 * 60) -> int:
    removed = 0
    directories = glob.glob(os.path.join(UUID_VAR_DIR, ""test-backend"", ""run_*""))
    for test_run in directories:
        if round(time.time()) - os.path.getmtime(test_run) > expiry_time:
            try:
                shutil.rmtree(test_run)
                removed += 1
            except FileNotFoundError:
                pass
    return removed
",if round ( time . time ( ) ) - os . path . getmtime ( test_run ) > expiry_time :,136
"def read_work_titles(fields):
    found = []
    if ""240"" in fields:
        for line in fields[""240""]:
            title = join_subfield_values(line, [""a"", ""m"", ""n"", ""p"", ""r""])
            if title not in found:
                found.append(title)
    if ""130"" in fields:
        for line in fields[""130""]:
            title = "" "".join(get_lower_subfields(line))
            if title not in found:
                found.append(title)
    return {""work_titles"": found} if found else {}
",if title not in found :,157
"def _process_v1_msg(prot, msg):
    header = None
    body = msg[1]
    if not isinstance(body, (binary_type, mmap, memoryview)):
        raise ValidationError(body, ""Body must be a bytestream."")
    if len(msg) > 2:
        header = msg[2]
        if not isinstance(header, dict):
            raise ValidationError(header, ""Header must be a dict."")
        for k, v in header.items():
            header[k] = msgpack.unpackb(v)
    ctx = MessagePackMethodContext(prot, MessagePackMethodContext.SERVER)
    ctx.in_string = [body]
    ctx.transport.in_header = header
    return ctx
","if not isinstance ( header , dict ) :",177
"def find(self, node):
    typename = type(node).__name__
    method = getattr(self, ""find_{}"".format(typename), None)
    if method is None:
        fields = getattr(node, ""_fields"", None)
        if fields is None:
            return
        for field in fields:
            value = getattr(node, field)
            for result in self.find(value):
                yield result
    else:
        for result in method(node):
            yield result
",if fields is None :,129
"def _str_param_list(self, name):
    out = []
    if self[name]:
        out += self._str_header(name)
        for param in self[name]:
            parts = []
            if param.name:
                parts.append(param.name)
            if param.type:
                parts.append(param.type)
            out += ["" : "".join(parts)]
            if param.desc and """".join(param.desc).strip():
                out += self._str_indent(param.desc)
        out += [""""]
    return out
",if param . name :,157
"def _get_image(self, image_list, source):
    if source.startswith(""wx""):
        img = wx.ArtProvider_GetBitmap(source, wx.ART_OTHER, _SIZE)
    else:
        path = os.path.join(_BASE, source)
        if source.endswith(""gif""):
            img = wx.Image(path, wx.BITMAP_TYPE_GIF).ConvertToBitmap()
        else:
            img = wx.Image(path, wx.BITMAP_TYPE_PNG).ConvertToBitmap()
    return image_list.Add(img)
","if source . endswith ( ""gif"" ) :",144
"def change_opacity_function(self, new_f):
    self.opacity_function = new_f
    dr = self.radius / self.num_levels
    sectors = []
    for submob in self.submobjects:
        if type(submob) == AnnularSector:
            sectors.append(submob)
    for (r, submob) in zip(np.arange(0, self.radius, dr), sectors):
        if type(submob) != AnnularSector:
            # it's the shadow, don't dim it
            continue
        alpha = self.opacity_function(r)
        submob.set_fill(opacity=alpha)
",if type ( submob ) == AnnularSector :,180
"def _sqlite_post_configure_engine(url, engine, follower_ident):
    from sqlalchemy import event
    @event.listens_for(engine, ""connect"")
    def connect(dbapi_connection, connection_record):
        # use file DBs in all cases, memory acts kind of strangely
        # as an attached
        if not follower_ident:
            dbapi_connection.execute('ATTACH DATABASE ""test_schema.db"" AS test_schema')
        else:
            dbapi_connection.execute(
                'ATTACH DATABASE ""%s_test_schema.db"" AS test_schema' % follower_ident
            )
",if not follower_ident :,164
"def apply_conf_file(fn, conf_filename):
    for env in LSF_CONF_ENV:
        conf_file = get_conf_file(conf_filename, env)
        if conf_file:
            with open(conf_file) as conf_handle:
                value = fn(conf_handle)
            if value:
                return value
    return None
",if value :,101
"def test_call_extern_c_fn(self):
    global memcmp
    memcmp = cffi_support.ExternCFunction(
        ""memcmp"",
        (""int memcmp ( const uint8_t * ptr1, "" ""const uint8_t * ptr2, size_t num )""),
    )
    @udf(BooleanVal(FunctionContext, StringVal, StringVal))
    def fn(context, a, b):
        if a.is_null != b.is_null:
            return False
        if a is None:
            return True
        if len(a) != b.len:
            return False
        if a.ptr == b.ptr:
            return True
        return memcmp(a.ptr, b.ptr, a.len) == 0
",if len ( a ) != b . len :,199
"def _get_initialized_app(app):
    """"""Returns a reference to an initialized App instance.""""""
    if app is None:
        return firebase_admin.get_app()
    if isinstance(app, firebase_admin.App):
        initialized_app = firebase_admin.get_app(app.name)
        if app is not initialized_app:
            raise ValueError(
                ""Illegal app argument. App instance not ""
                ""initialized via the firebase module.""
            )
        return app
    raise ValueError(
        ""Illegal app argument. Argument must be of type ""
        ' firebase_admin.App, but given ""{0}"".'.format(type(app))
    )
",if app is not initialized_app :,177
"def compiled_query(self):
    if self.compiled_query_ is None:
        self.lazy_init_lock_.acquire()
        try:
            if self.compiled_query_ is None:
                self.compiled_query_ = CompiledQuery()
        finally:
            self.lazy_init_lock_.release()
    return self.compiled_query_
",if self . compiled_query_ is None :,96
"def clean_subevent(event, subevent):
    if event.has_subevents:
        if not subevent:
            raise ValidationError(_(""Subevent cannot be null for event series.""))
        if event != subevent.event:
            raise ValidationError(_(""The subevent does not belong to this event.""))
    else:
        if subevent:
            raise ValidationError(_(""The subevent does not belong to this event.""))
",if not subevent :,102
"def get_blob_type_declaration_sql(self, column):
    length = column.get(""length"")
    if length:
        if length <= self.LENGTH_LIMIT_TINYBLOB:
            return ""TINYBLOB""
        if length <= self.LENGTH_LIMIT_BLOB:
            return ""BLOB""
        if length <= self.LENGTH_LIMIT_MEDIUMBLOB:
            return ""MEDIUMBLOB""
    return ""LONGBLOB""
",if length <= self . LENGTH_LIMIT_MEDIUMBLOB :,115
"def decompress(self, data):
    if not data:
        return data
    if not self._first_try:
        return self._obj.decompress(data)
    self._data += data
    try:
        decompressed = self._obj.decompress(data)
        if decompressed:
            self._first_try = False
            self._data = None
        return decompressed
    except zlib.error:
        self._first_try = False
        self._obj = zlib.decompressobj(-zlib.MAX_WBITS)
        try:
            return self.decompress(self._data)
        finally:
            self._data = None
",if decompressed :,163
"def _record_event(self, path, fsevent_handle, filename, events, error):
    with self.lock:
        self.events[path].append(events)
        if events | pyuv.fs.UV_RENAME:
            if not os.path.exists(path):
                self.watches.pop(path).close()
",if not os . path . exists ( path ) :,89
"def __init__(self, duration, batch_shape, event_shape, validate_args=None):
    if duration is None:
        if event_shape[0] != 1:
            # Infer duration from event_shape.
            duration = event_shape[0]
    elif duration != event_shape[0]:
        if event_shape[0] != 1:
            raise ValueError(
                ""duration, event_shape mismatch: {} vs {}"".format(duration, event_shape)
            )
        # Infer event_shape from duration.
        event_shape = torch.Size((duration,) + event_shape[1:])
    self._duration = duration
    super().__init__(batch_shape, event_shape, validate_args)
",if event_shape [ 0 ] != 1 :,183
"def _CheckPrerequisites(self):
    """"""Exits if any of the prerequisites is not met.""""""
    if not FLAGS.kubectl:
        raise Exception(
            ""Please provide path to kubectl tool using --kubectl "" ""flag. Exiting.""
        )
    if not FLAGS.kubeconfig:
        raise Exception(
            ""Please provide path to kubeconfig using --kubeconfig "" ""flag. Exiting.""
        )
    if self.disk_specs and self.disk_specs[0].disk_type == disk.STANDARD:
        if not FLAGS.ceph_monitors:
            raise Exception(
                ""Please provide a list of Ceph Monitors using "" ""--ceph_monitors flag.""
            )
",if not FLAGS . ceph_monitors :,181
"def invalidateDependentSlices(self, iFirstCurve):
    # only user defined curve can have slice dependency relationships
    if self.isSystemCurveIndex(iFirstCurve):
        return
    nCurves = self.getNCurves()
    for i in range(iFirstCurve, nCurves):
        c = self.getSystemCurve(i)
        if isinstance(c.getSymbol().getSymbolType(), SymbolType.PieSliceSymbolType):
            c.invalidate()
        elif i == iFirstCurve:
            # if first curve isn't a slice,
            break
            # there are no dependent slices
","if isinstance ( c . getSymbol ( ) . getSymbolType ( ) , SymbolType . PieSliceSymbolType ) :",154
"def find_backwards(self, offset):
    try:
        for _, token_type, token_value in reversed(self.tokens[self.offset : offset]):
            if token_type in (""comment"", ""linecomment""):
                try:
                    prefix, comment = token_value.split(None, 1)
                except ValueError:
                    continue
                if prefix in self.comment_tags:
                    return [comment.rstrip()]
        return []
    finally:
        self.offset = offset
",if prefix in self . comment_tags :,140
"def parse_column_definitions(self, elem):
    for column_elem in elem.findall(""column""):
        name = column_elem.get(""name"", None)
        assert name is not None, ""Required 'name' attribute missing from column def""
        index = column_elem.get(""index"", None)
        assert index is not None, ""Required 'index' attribute missing from column def""
        index = int(index)
        self.columns[name] = index
        if index > self.largest_index:
            self.largest_index = index
    assert ""value"" in self.columns, ""Required 'value' column missing from column def""
    if ""name"" not in self.columns:
        self.columns[""name""] = self.columns[""value""]
",if index > self . largest_index :,188
"def __find_smallest(self):
    """"""Find the smallest uncovered value in the matrix.""""""
    minval = sys.maxsize
    for i in range(self.n):
        for j in range(self.n):
            if (not self.row_covered[i]) and (not self.col_covered[j]):
                if minval > self.C[i][j]:
                    minval = self.C[i][j]
    return minval
",if minval > self . C [ i ] [ j ] :,114
"def includes_tools_for_display_in_tool_panel(self):
    if self.includes_tools:
        tool_dicts = self.metadata[""tools""]
        for tool_dict in tool_dicts:
            if tool_dict.get(""add_to_tool_panel"", True):
                return True
    return False
","if tool_dict . get ( ""add_to_tool_panel"" , True ) :",84
"def commit(self, notify=False):
    if self.editing:
        text = self._text
        if text:
            try:
                value = self.type(text)
            except ValueError:
                return
            value = self.clamp_value(value)
        else:
            value = self.empty
            if value is NotImplemented:
                return
        self.value = value
        self.insertion_point = None
        if notify:
            self.change_text(unicode(value))
        else:
            self._text = unicode(value)
        self.editing = False
    else:
        self.insertion_point = None
",if notify :,183
"def GeneratePageMetatadata(self, task):
    address_space = self.session.GetParameter(""default_address_space"")
    for vma in task.mm.mmap.walk_list(""vm_next""):
        start = vma.vm_start
        end = vma.vm_end
        # Skip the entire region.
        if end < self.plugin_args.start:
            continue
        # Done.
        if start > self.plugin_args.end:
            break
        for vaddr in utils.xrange(start, end, 0x1000):
            if self.plugin_args.start <= vaddr <= self.plugin_args.end:
                yield vaddr, self._CreateMetadata(address_space.describe_vtop(vaddr))
",if start > self . plugin_args . end :,195
"def _check_for_duplicate_host_entries(self, task_entries):
    non_host_statuses = (
        models.HostQueueEntry.Status.PARSING,
        models.HostQueueEntry.Status.ARCHIVING,
    )
    for task_entry in task_entries:
        using_host = (
            task_entry.host is not None and task_entry.status not in non_host_statuses
        )
        if using_host:
            self._assert_host_has_no_agent(task_entry)
",if using_host :,136
"def get_biggest_wall_time(jsons):
    lowest_wall = None
    for j in jsons:
        if lowest_wall is None:
            lowest_wall = j[""wall_time""]
        if lowest_wall < j[""wall_time""]:
            lowest_wall = j[""wall_time""]
    return lowest_wall
",if lowest_wall is None :,87
"def log_change_report(self, old_value, new_value, include_details=False):
    from octoprint.util import map_boolean
    with self._check_mutex:
        self._logger.info(
            ""Connectivity changed from {} to {}"".format(
                map_boolean(old_value, ""online"", ""offline""),
                map_boolean(new_value, ""online"", ""offline""),
            )
        )
        if include_details:
            self.log_details()
",if include_details :,136
"def _include_block(self, value, context=None):
    if hasattr(value, ""render_as_block""):
        if context:
            new_context = context.get_all()
        else:
            new_context = {}
        return jinja2.Markup(value.render_as_block(context=new_context))
    return jinja2.Markup(value)
",if context :,96
"def __lt__(self, other):
    # 0: clock 1: timestamp 3: process id
    try:
        A, B = self[0], other[0]
        # uses logical clock value first
        if A and B:  # use logical clock if available
            if A == B:  # equal clocks use lower process id
                return self[2] < other[2]
            return A < B
        return self[1] < other[1]  # ... or use timestamp
    except IndexError:
        return NotImplemented
",if A == B :,135
"def _get_port():
    while True:
        port = 20000 + random.randint(1, 9999)
        for i in range(5):
            sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
            result = sock.connect_ex((""127.0.0.1"", port))
            if result == 0:
                continue
        else:
            return port
",if result == 0 :,107
"def fetch_all(self, api_client, fetchstatuslogger, q, targets):
    self.fetchstatuslogger = fetchstatuslogger
    if targets != None:
        # Ensure targets is a tuple
        if type(targets) != list and type(targets) != tuple:
            targets = tuple(
                targets,
            )
        elif type(targets) != tuple:
            targets = tuple(targets)
    for target in targets:
        self._fetch_targets(api_client, q, target)
",elif type ( targets ) != tuple :,130
"def migrate_node_facts(facts):
    """"""Migrate facts from various roles into node""""""
    params = {
        ""common"": (""dns_ip""),
    }
    if ""node"" not in facts:
        facts[""node""] = {}
    # pylint: disable=consider-iterating-dictionary
    for role in params.keys():
        if role in facts:
            for param in params[role]:
                if param in facts[role]:
                    facts[""node""][param] = facts[role].pop(param)
    return facts
",if role in facts :,136
"def build_dimension_param(self, dimension, params):
    prefix = ""Dimensions.member""
    i = 0
    for dim_name in dimension:
        dim_value = dimension[dim_name]
        if dim_value:
            if isinstance(dim_value, six.string_types):
                dim_value = [dim_value]
            for value in dim_value:
                params[""%s.%d.Name"" % (prefix, i + 1)] = dim_name
                params[""%s.%d.Value"" % (prefix, i + 1)] = value
                i += 1
        else:
            params[""%s.%d.Name"" % (prefix, i + 1)] = dim_name
            i += 1
",if dim_value :,192
"def add_if_unique(self, issuer, use, keys):
    if use in self.issuer_keys[issuer] and self.issuer_keys[issuer][use]:
        for typ, key in keys:
            flag = 1
            for _typ, _key in self.issuer_keys[issuer][use]:
                if _typ == typ and key is _key:
                    flag = 0
                    break
            if flag:
                self.issuer_keys[issuer][use].append((typ, key))
    else:
        self.issuer_keys[issuer][use] = keys
",if _typ == typ and key is _key :,158
"def run(self):
    while True:
        message = self.in_queue.get()
        if message == RESET:
            self.reset()
        elif message == EXIT:
            return
        else:
            index, transaction = message
            self.results_queue.put((index, self.validate(transaction)))
",if message == RESET :,89
"def __run(self):
    threads = self.parameters()[""threads""].getTypedValue()
    with IECore.tbb_global_control(
        IECore.tbb_global_control.parameter.max_allowed_parallelism,
        IECore.hardwareConcurrency() if threads == 0 else threads,
    ):
        self._executeStartupFiles(self.root().getName())
        # Append DEBUG message with process information to all messages
        defaultMessageHandler = IECore.MessageHandler.getDefaultHandler()
        if not isinstance(defaultMessageHandler, Gaffer.ProcessMessageHandler):
            IECore.MessageHandler.setDefaultHandler(
                Gaffer.ProcessMessageHandler(defaultMessageHandler)
            )
        return self._run(self.parameters().getValidatedValue())
","if not isinstance ( defaultMessageHandler , Gaffer . ProcessMessageHandler ) :",194
"def adjust_uri(self, uri, relativeto):
    """"""Adjust the given ``uri`` based on the given relative URI.""""""
    key = (uri, relativeto)
    if key in self._uri_cache:
        return self._uri_cache[key]
    if uri[0] != ""/"":
        if relativeto is not None:
            v = self._uri_cache[key] = posixpath.join(
                posixpath.dirname(relativeto), uri
            )
        else:
            v = self._uri_cache[key] = ""/"" + uri
    else:
        v = self._uri_cache[key] = uri
    return v
",if relativeto is not None :,164
"def decoder(s):
    r = []
    decode = []
    for c in s:
        if c == ""&"" and not decode:
            decode.append(""&"")
        elif c == ""-"" and decode:
            if len(decode) == 1:
                r.append(""&"")
            else:
                r.append(modified_unbase64("""".join(decode[1:])))
            decode = []
        elif decode:
            decode.append(c)
        else:
            r.append(c)
    if decode:
        r.append(modified_unbase64("""".join(decode[1:])))
    bin_str = """".join(r)
    return (bin_str, len(s))
","if c == ""&"" and not decode :",188
"def _process_file(self, content):
    args = []
    for line in content.splitlines():
        line = line.strip()
        if line.startswith(""-""):
            args.extend(self._split_option(line))
        elif line and not line.startswith(""#""):
            args.append(line)
    return args
","elif line and not line . startswith ( ""#"" ) :",83
"def _method_events_callback(self, values):
    try:
        previous_echoed = (
            values[""child_result_list""][-1].decode().split(""\n"")[-2].strip()
        )
        if previous_echoed.endswith(""foo1""):
            return ""echo foo2\n""
        elif previous_echoed.endswith(""foo2""):
            return ""echo foo3\n""
        elif previous_echoed.endswith(""foo3""):
            return ""exit\n""
        else:
            raise Exception(""Unexpected output {0!r}"".format(previous_echoed))
    except IndexError:
        return ""echo foo1\n""
","elif previous_echoed . endswith ( ""foo3"" ) :",172
"def __delete_hook(self, rpc):
    try:
        rpc.check_success()
    except apiproxy_errors.Error:
        return None
    result = []
    for status in rpc.response.delete_status_list():
        if status == MemcacheDeleteResponse.DELETED:
            result.append(DELETE_SUCCESSFUL)
        elif status == MemcacheDeleteResponse.NOT_FOUND:
            result.append(DELETE_ITEM_MISSING)
        else:
            result.append(DELETE_NETWORK_FAILURE)
    return result
",if status == MemcacheDeleteResponse . DELETED :,139
"def __createRandom(plug):
    node = plug.node()
    parentNode = node.ancestor(Gaffer.Node)
    with Gaffer.UndoScope(node.scriptNode()):
        randomNode = Gaffer.Random()
        parentNode.addChild(randomNode)
        if isinstance(plug, (Gaffer.FloatPlug, Gaffer.IntPlug)):
            plug.setInput(randomNode[""outFloat""])
        elif isinstance(plug, Gaffer.Color3fPlug):
            plug.setInput(randomNode[""outColor""])
    GafferUI.NodeEditor.acquire(randomNode)
","if isinstance ( plug , ( Gaffer . FloatPlug , Gaffer . IntPlug ) ) :",158
"def escapeentities(self, line):
    ""Escape all Unicode characters to HTML entities.""
    result = """"
    pos = TextPosition(line)
    while not pos.finished():
        if ord(pos.current()) > 128:
            codepoint = hex(ord(pos.current()))
            if codepoint == ""0xd835"":
                codepoint = hex(ord(pos.next()) + 0xF800)
            result += ""&#"" + codepoint[1:] + "";""
        else:
            result += pos.current()
        pos.skipcurrent()
    return result
","if codepoint == ""0xd835"" :",143
"def get_and_set_all_aliases(self):
    all_aliases = []
    for page in self.pages:
        if page.relations.aliases_norm is not None:
            all_aliases.extend(page.relations.aliases_norm)
        if page.relations.aliases is not None:
            all_aliases.extend(page.relations.aliases)
    return set(all_aliases)
",if page . relations . aliases_norm is not None :,101
"def _list_cases(suite):
    for test in suite:
        if isinstance(test, unittest.TestSuite):
            _list_cases(test)
        elif isinstance(test, unittest.TestCase):
            if support.match_test(test):
                print(test.id())
","if isinstance ( test , unittest . TestSuite ) :",75
"def get_next_requests(self, max_n_requests, **kwargs):
    next_pages = []
    partitions = set(kwargs.pop(""partitions"", []))
    for partition_id in range(0, self.queue_partitions):
        if partition_id not in partitions:
            continue
        results = self.queue.get_next_requests(max_n_requests, partition_id)
        next_pages.extend(results)
        self.logger.debug(
            ""Got %d requests for partition id %d"", len(results), partition_id
        )
    return next_pages
",if partition_id not in partitions :,149
"def __iter__(self):
    if (self.query is not None) and sqlite.is_read_only_query(self.query):
        cur = self.connection.cursor()
        results = cur.execute(self.query)
        if self.headers:
            yield [col[0] for col in cur.description]
        for i, row in enumerate(results):
            if i >= self.limit:
                break
            yield [val for val in row]
    else:
        yield
",if self . headers :,131
"def rollback(self):
    for operation, values in self.current_transaction_state[::-1]:
        if operation == ""insert"":
            values.remove()
        elif operation == ""update"":
            old_value, new_value = values
            if new_value.full_filename != old_value.full_filename:
                os.unlink(new_value.full_filename)
            old_value.write()
    self._post_xact_cleanup()
","elif operation == ""update"" :",121
"def index(self, value):
    if self._growing:
        if self._start <= value < self._stop:
            q, r = divmod(value - self._start, self._step)
            if r == self._zero:
                return int(q)
    else:
        if self._start >= value > self._stop:
            q, r = divmod(self._start - value, -self._step)
            if r == self._zero:
                return int(q)
    raise ValueError(""{} is not in numeric range"".format(value))
",if r == self . _zero :,146
"def validate_name_and_description(body, check_length=True):
    for attribute in [""name"", ""description"", ""display_name"", ""display_description""]:
        value = body.get(attribute)
        if value is not None:
            if isinstance(value, six.string_types):
                body[attribute] = value.strip()
            if check_length:
                try:
                    utils.check_string_length(
                        body[attribute], attribute, min_length=0, max_length=255
                    )
                except exception.InvalidInput as error:
                    raise webob.exc.HTTPBadRequest(explanation=error.msg)
",if value is not None :,184
"def printWiki():
    firstHeading = False
    for m in protocol:
        if m[0] == """":
            if firstHeading:
                output(""|}"")
            __printWikiHeader(m[1], m[2])
            firstHeading = True
        else:
            output(""|-"")
            output(
                '| <span style=""white-space:nowrap;""><tt>'
                + m[0]
                + ""</tt></span> || || ""
                + m[1]
            )
    output(""|}"")
",if firstHeading :,155
"def _get_platforms(data):
    platform_list = []
    for item in data:
        if item.startswith(""PlatformEdit.html?""):
            parameter_list = item.split(""PlatformEdit.html?"", 1)[1].split(""&"")
            for parameter in parameter_list:
                if parameter.startswith(""platformName""):
                    platform_list.append(parameter.split(""="")[1])
    return platform_list
","if parameter . startswith ( ""platformName"" ) :",110
"def find_scintilla_constants(f):
    lexers = []
    states = []
    for name in f.order:
        v = f.features[name]
        if v[""Category""] != ""Deprecated"":
            if v[""FeatureType""] == ""val"":
                if name.startswith(""SCE_""):
                    states.append((name, v[""Value""]))
                elif name.startswith(""SCLEX_""):
                    lexers.append((name, v[""Value""]))
    return (lexers, states)
","if name . startswith ( ""SCE_"" ) :",137
"def get_operation_ast(document_ast, operation_name=None):
    operation = None
    for definition in document_ast.definitions:
        if isinstance(definition, ast.OperationDefinition):
            if not operation_name:
                # If no operation name is provided, only return an Operation if it is the only one present in the
                # document. This means that if we've encountered a second operation as we were iterating over the
                # definitions in the document, there are more than one Operation defined, and we should return None.
                if operation:
                    return None
                operation = definition
            elif definition.name and definition.name.value == operation_name:
                return definition
    return operation
",if not operation_name :,186
"def _insertNewItemAtParent(self, targetIndex):
    if not self.isContainer(targetIndex):
        return
    elif not self.isContainerOpen(targetIndex):
        uri = self._rows[targetIndex].uri
        modelNode = self.getNodeForURI(uri)
        if modelNode:
            modelNode.markForRefreshing()
        return
    self.refreshView(targetIndex)
",if modelNode :,103
"def _get_trace(self, model, guide, args, kwargs):
    model_trace, guide_trace = super()._get_trace(model, guide, args, kwargs)
    # Mark all sample sites with require_backward to gather enumerated
    # sites and adjust cond_indep_stack of all sample sites.
    for node in model_trace.nodes.values():
        if node[""type""] == ""sample"" and not node[""is_observed""]:
            log_prob = node[""packed""][""unscaled_log_prob""]
            require_backward(log_prob)
    self._saved_state = model, model_trace, guide_trace, args, kwargs
    return model_trace, guide_trace
","if node [ ""type"" ] == ""sample"" and not node [ ""is_observed"" ] :",166
"def _url_encode_impl(obj, charset, encode_keys, sort, key):
    from .datastructures import iter_multi_items
    iterable = iter_multi_items(obj)
    if sort:
        iterable = sorted(iterable, key=key)
    for key, value in iterable:
        if value is None:
            continue
        if not isinstance(key, bytes):
            key = text_type(key).encode(charset)
        if not isinstance(value, bytes):
            value = text_type(value).encode(charset)
        yield _fast_url_quote_plus(key) + ""="" + _fast_url_quote_plus(value)
","if not isinstance ( value , bytes ) :",168
"def handle_parse_result(self, ctx, opts, args):
    with augment_usage_errors(ctx, param=self):
        value = self.consume_value(ctx, opts)
        try:
            value = self.full_process_value(ctx, value)
        except Exception:
            if not ctx.resilient_parsing:
                raise
            value = None
        if self.callback is not None:
            try:
                value = invoke_param_callback(self.callback, ctx, self, value)
            except Exception:
                if not ctx.resilient_parsing:
                    raise
    if self.expose_value:
        ctx.params[self.name] = value
    return value, args
",if not ctx . resilient_parsing :,195
"def word_pattern(pattern, str):
    dict = {}
    set_value = set()
    list_str = str.split()
    if len(list_str) != len(pattern):
        return False
    for i in range(len(pattern)):
        if pattern[i] not in dict:
            if list_str[i] in set_value:
                return False
            dict[pattern[i]] = list_str[i]
            set_value.add(list_str[i])
        else:
            if dict[pattern[i]] != list_str[i]:
                return False
    return True
",if list_str [ i ] in set_value :,165
"def create(self, path, wipe=False):
    # type: (Text, bool) -> bool
    _path = self.validatepath(path)
    with ftp_errors(self, path):
        if wipe or not self.isfile(path):
            empty_file = io.BytesIO()
            self.ftp.storbinary(
                str(""STOR "") + _encode(_path, self.ftp.encoding), empty_file
            )
            return True
    return False
",if wipe or not self . isfile ( path ) :,124
"def build_output_for_item(self, item):
    output = []
    for field in self.fields:
        values = self._get_item(item, field)
        if not isinstance(values, list):
            values = [values]
        for value in values:
            if value:
                output.append(self.build_output_for_single_value(value))
    return """".join(output)
","if not isinstance ( values , list ) :",109
"def get_resource_public_actions(resource_class):
    resource_class_members = inspect.getmembers(resource_class)
    resource_methods = {}
    for name, member in resource_class_members:
        if not name.startswith(""_""):
            if not name[0].isupper():
                if not name.startswith(""wait_until""):
                    if is_resource_action(member):
                        resource_methods[name] = member
    return resource_methods
","if not name . startswith ( ""wait_until"" ) :",122
"def get_command(cls):
    ifconfig_cmd = ""ifconfig""
    for path in [""/sbin"", ""/usr/sbin"", ""/bin"", ""/usr/bin""]:
        if os.path.exists(os.path.join(path, ifconfig_cmd)):
            ifconfig_cmd = os.path.join(path, ifconfig_cmd)
            break
    ifconfig_cmd = ifconfig_cmd + "" -a""
    return ifconfig_cmd
","if os . path . exists ( os . path . join ( path , ifconfig_cmd ) ) :",109
"def main():
    base_dir = os.path.join(os.path.split(__file__)[0], "".."", "".."")
    for path in PATHS:
        path = os.path.join(base_dir, path)
        for root, _, files in os.walk(path):
            for file in files:
                extension = os.path.splitext(file)[1]
                if extension in EXTENSIONS:
                    path = os.path.join(root, file)
                    validate_header(path)
",if extension in EXTENSIONS :,137
"def auth_login(request):
    form = RegistrationForm(request.POST or None)
    if form.is_valid():
        authed_user = authenticate(
            username=form.cleaned_data[""username""],
            password=form.cleaned_data[""password""],
        )
        if authed_user:
            login(request, authed_user)
            return HttpResponse(""Success"")
    raise Http404
",if authed_user :,110
"def set(self, _key, _new_login=True):
    with self.lock:
        user = self.users.get(current_user.id, None)
        if user is None:
            self.users[current_user.id] = dict(session_count=1, key=_key)
        else:
            if _new_login:
                user[""session_count""] += 1
            user[""key""] = _key
",if _new_login :,116
"def fetch(self, fingerprints):
    to_fetch = [f for f in fingerprints if f not in self._cache]
    self._logger.debug(""cache size %s"" % len(self._cache))
    self._logger.debug(""to fetch %d from %d"" % (len(to_fetch), len(fingerprints)))
    [self._redis_pipeline.hgetall(key) for key in to_fetch]
    responses = self._redis_pipeline.execute()
    for index, key in enumerate(to_fetch):
        response = responses[index]
        if len(response) > 0 and FIELD_STATE in response:
            self._cache[key] = response[FIELD_STATE]
        else:
            self._cache[key] = self.NOT_CRAWLED
",if len ( response ) > 0 and FIELD_STATE in response :,187
"def _append_to_io_queue(self, data, stream_name):
    # Make sure ANSI CSI codes and object links are stored as separate events
    # TODO: try to complete previously submitted incomplete code
    parts = re.split(OUTPUT_SPLIT_REGEX, data)
    for part in parts:
        if part:  # split may produce empty string in the beginning or start
            # split the data so that very long lines separated
            for block in re.split(
                ""(.{%d,})"" % (self._get_squeeze_threshold() + 1), part
            ):
                if block:
                    self._queued_io_events.append((block, stream_name))
",if block :,174
"def find_file_at_path_with_indexes(self, path, url):
    if url.endswith(""/""):
        path = os.path.join(path, self.index_file)
        return self.get_static_file(path, url)
    elif url.endswith(""/"" + self.index_file):
        if os.path.isfile(path):
            return self.redirect(url, url[: -len(self.index_file)])
    else:
        try:
            return self.get_static_file(path, url)
        except IsDirectoryError:
            if os.path.isfile(os.path.join(path, self.index_file)):
                return self.redirect(url, url + ""/"")
    raise MissingFileError(path)
",if os . path . isfile ( path ) :,193
"def module_list(target, fast):
    """"""Find the list of modules to be compiled""""""
    modules = []
    native = native_modules(target)
    basedir = os.path.join(ouroboros_repo_folder(), ""ouroboros"")
    for name in os.listdir(basedir):
        module_name, ext = os.path.splitext(name)
        if ext == "".py"" or ext == """" and os.path.isdir(os.path.join(basedir, name)):
            if module_name not in IGNORE_MODULES and module_name not in native:
                if not (fast and module_name in KNOWN_PROBLEM_MODULES):
                    modules.append(module_name)
    return set(modules)
",if not ( fast and module_name in KNOWN_PROBLEM_MODULES ) :,185
"def housenumber(self):
    if self.address:
        expression = r""\d+""
        pattern = re.compile(expression)
        match = pattern.search(self.address)
        if match:
            return int(match.group(0))
",if match :,67
"def get_pip_version(import_path=BASE_IMPORT_PATH):
    try:
        pip = importlib.import_module(import_path)
    except ImportError:
        if import_path != ""pip"":
            return get_pip_version(import_path=""pip"")
        else:
            import subprocess
            version = subprocess.check_output([""pip"", ""--version""])
            if version:
                version = version.decode(""utf-8"").split()[1]
                return version
            return ""0.0.0""
    version = getattr(pip, ""__version__"", None)
    return version
","if import_path != ""pip"" :",160
"def __animate_progress(self):
    """"""Change the status message, mostly used to animate progress.""""""
    while True:
        sleep_time = ThreadPool.PROGRESS_IDLE_DELAY
        with self.__progress_lock:
            if not self.__progress_status:
                sleep_time = ThreadPool.PROGRESS_IDLE_DELAY
            elif self.__show_animation:
                self.__progress_status.update_progress(self.__current_operation_name)
                sleep_time = ThreadPool.PROGRESS_UPDATE_DELAY
            else:
                self.__progress_status.show_as_ready()
                sleep_time = ThreadPool.PROGRESS_IDLE_DELAY
        # Allow some time for progress status to be updated.
        time.sleep(sleep_time)
",elif self . __show_animation :,195
"def range_key_names(self):
    keys = [self.range_key_attr]
    for index in self.global_indexes:
        range_key = None
        for key in index.schema:
            if key[""KeyType""] == ""RANGE"":
                range_key = keys.append(key[""AttributeName""])
        keys.append(range_key)
    return keys
","if key [ ""KeyType"" ] == ""RANGE"" :",99
"def run(self):
    dist = self.distribution
    commands = dist.command_options.keys()
    settings = {}
    for cmd in commands:
        if cmd == ""saveopts"":
            continue  # don't save our own options!
        for opt, (src, val) in dist.get_option_dict(cmd).items():
            if src == ""command line"":
                settings.setdefault(cmd, {})[opt] = val
    edit_config(self.filename, settings, self.dry_run)
","if src == ""command line"" :",131
"def parse_move(self, node):
    old, new = """", """"
    for child in node:
        tag, text = child.tag, child.text
        text = text.strip() if text else None
        if tag == ""Old"" and text:
            old = text
        elif tag == ""New"" and text:
            new = text
    return Move(old, new)
","elif tag == ""New"" and text :",99
"def __codeanalysis_settings_changed(self, current_finfo):
    if self.data:
        run_pyflakes, run_pep8 = self.pyflakes_enabled, self.pep8_enabled
        for finfo in self.data:
            self.__update_editor_margins(finfo.editor)
            finfo.cleanup_analysis_results()
            if (run_pyflakes or run_pep8) and current_finfo is not None:
                if current_finfo is not finfo:
                    finfo.run_code_analysis(run_pyflakes, run_pep8)
",if current_finfo is not finfo :,148
"def tchg(var, width):
    ""Convert time string to given length""
    ret = ""%2dh%02d"" % (var / 60, var % 60)
    if len(ret) > width:
        ret = ""%2dh"" % (var / 60)
        if len(ret) > width:
            ret = ""%2dd"" % (var / 60 / 24)
            if len(ret) > width:
                ret = ""%2dw"" % (var / 60 / 24 / 7)
    return ret
",if len ( ret ) > width :,132
"def spider_log_activity(self, messages):
    for i in range(0, messages):
        if i % 2 == 0:
            self.sp_sl_p.send(
                sha1(str(randint(1, 1000))),
                b""http://helloworld.com/way/to/the/sun/"" + b""0"",
            )
        else:
            self.sp_sl_p.send(
                sha1(str(randint(1, 1000))), b""http://way.to.the.sun"" + b""0""
            )
    self.sp_sl_p.flush()
",if i % 2 == 0 :,165
"def decode_serial(self, offset):
    serialnum = (
        (self.cache[offset + 3] << 24)
        + (self.cache[offset + 2] << 16)
        + (self.cache[offset + 1] << 8)
        + self.cache[offset]
    )
    serialstr = """"
    is_alnum = True
    for i in range(4):
        if not chr(self.cache[offset + 3 - i]).isalnum():
            is_alnum = False
            break
        serialstr += chr(self.cache[offset + 3 - i])
    serial = serialstr if is_alnum else str(serialnum)
    self.ann_field(offset, offset + 3, ""Serial "" + serial)
",if not chr ( self . cache [ offset + 3 - i ] ) . isalnum ( ) :,182
"def gettext(rv):
    for child in rv.childNodes:
        if child.nodeType == child.TEXT_NODE:
            yield child.nodeValue
        if child.nodeType == child.ELEMENT_NODE:
            for item in gettext(child):
                yield item
",if child . nodeType == child . ELEMENT_NODE :,73
"def determine_block_hints(self, text):
    hints = """"
    if text:
        if text[0] in "" \n\x85\u2028\u2029"":
            hints += str(self.best_indent)
        if text[-1] not in ""\n\x85\u2028\u2029"":
            hints += ""-""
        elif len(text) == 1 or text[-2] in ""\n\x85\u2028\u2029"":
            hints += ""+""
    return hints
","elif len ( text ) == 1 or text [ - 2 ] in ""\n\x85\u2028\u2029"" :",132
"def _infer_return_type(*args):
    """"""Look at the type of all args and divine their implied return type.""""""
    return_type = None
    for arg in args:
        if arg is None:
            continue
        if isinstance(arg, bytes):
            if return_type is str:
                raise TypeError(""Can't mix bytes and non-bytes in "" ""path components."")
            return_type = bytes
        else:
            if return_type is bytes:
                raise TypeError(""Can't mix bytes and non-bytes in "" ""path components."")
            return_type = str
    if return_type is None:
        return str  # tempfile APIs return a str by default.
    return return_type
","if isinstance ( arg , bytes ) :",186
"def as_iconbitmap(cls, rkey):
    """"""Get image path for use in iconbitmap property""""""
    img = None
    if rkey in cls._stock:
        data = cls._stock[rkey]
        if data[""type""] not in (""stock"", ""data"", ""image""):
            fpath = data[""filename""]
            fname = os.path.basename(fpath)
            name, file_ext = os.path.splitext(fname)
            file_ext = str(file_ext).lower()
            if file_ext in TK_BITMAP_FORMATS:
                img = BITMAP_TEMPLATE.format(fpath)
    return img
","if data [ ""type"" ] not in ( ""stock"" , ""data"" , ""image"" ) :",167
"def anonymize_ip(ip):
    if ip:
        match = RE_FIRST_THREE_OCTETS_OF_IP.findall(str(ip))
        if match:
            return ""%s%s"" % (match[0][0], ""0"")
    return """"
",if match :,69
"def serialize_tail(self):
    msg = bytearray()
    for v in self.info:
        if v[""type""] == BMP_TERM_TYPE_STRING:
            value = v[""value""].encode(""utf-8"")
        elif v[""type""] == BMP_TERM_TYPE_REASON:
            value = struct.pack(""!H"", v[""value""])
        v[""len""] = len(value)
        msg += struct.pack(self._TLV_PACK_STR, v[""type""], v[""len""])
        msg += value
    return msg
","if v [ ""type"" ] == BMP_TERM_TYPE_STRING :",139
"def get_django_comment(text: str, i: int) -> str:
    end = i + 4
    unclosed_end = 0
    while end <= len(text):
        if text[end - 2 : end] == ""#}"":
            return text[i:end]
        if not unclosed_end and text[end] == ""<"":
            unclosed_end = end
        end += 1
    raise TokenizationException(""Unclosed comment"", text[i:unclosed_end])
","if text [ end - 2 : end ] == ""#}"" :",126
"def ComboBoxDroppedHeightTest(windows):
    ""Check if each combobox height is the same as the reference""
    bugs = []
    for win in windows:
        if not win.ref:
            continue
        if win.Class() != ""ComboBox"" or win.ref.Class() != ""ComboBox"":
            continue
        if win.DroppedRect().height() != win.ref.DroppedRect().height():
            bugs.append(
                (
                    [
                        win,
                    ],
                    {},
                    testname,
                    0,
                )
            )
    return bugs
",if win . DroppedRect ( ) . height ( ) != win . ref . DroppedRect ( ) . height ( ) :,181
"def testBadModeArgument(self):
    # verify that we get a sensible error message for bad mode argument
    bad_mode = ""qwerty""
    try:
        f = self.open(TESTFN, bad_mode)
    except ValueError as msg:
        if msg.args[0] != 0:
            s = str(msg)
            if TESTFN in s or bad_mode not in s:
                self.fail(""bad error message for invalid mode: %s"" % s)
        # if msg.args[0] == 0, we're probably on Windows where there may be
        # no obvious way to discover why open() failed.
    else:
        f.close()
        self.fail(""no error for invalid mode: %s"" % bad_mode)
",if msg . args [ 0 ] != 0 :,191
"def command_group_expired(self, command_group_name):
    try:
        deprecate_info = self._command_loader.command_group_table[
            command_group_name
        ].group_kwargs.get(""deprecate_info"", None)
        if deprecate_info:
            return deprecate_info.expired()
    except AttributeError:
        # Items with only token presence in the command table will not have any data. They can't be expired.
        pass
    return False
",if deprecate_info :,129
"def test_non_uniform_probabilities_over_elements(self):
    param = iap.Choice([0, 1], p=[0.25, 0.75])
    samples = param.draw_samples((10000,))
    unique, counts = np.unique(samples, return_counts=True)
    assert len(unique) == 2
    for val, count in zip(unique, counts):
        if val == 0:
            assert 2500 - 500 < count < 2500 + 500
        elif val == 1:
            assert 7500 - 500 < count < 7500 + 500
        else:
            assert False
",elif val == 1 :,145
"def get_labels(directory):
    cache = get_labels.__cache
    if directory not in cache:
        l = {}
        for t in get_visual_configs(directory)[0][LABEL_SECTION]:
            if t.storage_form() in l:
                Messager.warning(
                    ""In configuration, labels for '%s' defined more than once. Only using the last set.""
                    % t.storage_form(),
                    -1,
                )
            # first is storage for, rest are labels.
            l[t.storage_form()] = t.terms[1:]
        cache[directory] = l
    return cache[directory]
",if t . storage_form ( ) in l :,179
"def try_split(self, split_text: List[str]):
    ret = []
    for i in split_text:
        if len(i) == 0:
            continue
        val = int(i, 2)
        if val > 255 or val < 0:
            return None
        ret.append(val)
    if len(ret) != 0:
        ret = bytes(ret)
        logger.debug(f""binary successful, returning {ret.__repr__()}"")
        return ret
",if val > 255 or val < 0 :,127
"def setCellValue(self, row_idx, col, value):
    assert col.id == ""repls-marked""
    with self._lock:
        rgroup = self.events[row_idx]
        if not isinstance(rgroup, findlib2.ReplaceHitGroup):
            return
        rgroup._marked = value == ""true"" and True or False
    if self._tree:
        self._tree.invalidateCell(row_idx, col)
","if not isinstance ( rgroup , findlib2 . ReplaceHitGroup ) :",113
"def create(cls, settlement_manager, resource_id):
    """"""Create a production chain that can produce the given resource.""""""
    resource_producer = {}
    for abstract_building in AbstractBuilding.buildings.values():
        for resource, production_line in abstract_building.lines.items():
            if resource not in resource_producer:
                resource_producer[resource] = []
            resource_producer[resource].append((production_line, abstract_building))
    return ProductionChain(settlement_manager, resource_id, resource_producer)
",if resource not in resource_producer :,137
"def get_all_partition_sets(self):
    partition_sets = []
    if self.partitions_handle:
        partition_sets.extend(self.partitions_handle.get_partition_sets())
    if self.scheduler_handle:
        partition_sets.extend(
            [
                schedule_def.get_partition_set()
                for schedule_def in self.scheduler_handle.all_schedule_defs()
                if isinstance(schedule_def, PartitionScheduleDefinition)
            ]
        )
    return partition_sets
","if isinstance ( schedule_def , PartitionScheduleDefinition )",140
"def _sendDatapointsNow(self, datapoints):
    metrics = {}
    payload_pb = Payload()
    for metric, datapoint in datapoints:
        if metric not in metrics:
            metric_pb = payload_pb.metrics.add()
            metric_pb.metric = metric
            metrics[metric] = metric_pb
        else:
            metric_pb = metrics[metric]
        point_pb = metric_pb.points.add()
        point_pb.timestamp = int(datapoint[0])
        point_pb.value = datapoint[1]
    self.sendString(payload_pb.SerializeToString())
",if metric not in metrics :,159
"def execute(self):
    if self._dirty or not self._qr:
        model_class = self.model_class
        query_meta = self.get_query_meta()
        if self._tuples:
            ResultWrapper = TuplesQueryResultWrapper
        elif self._dicts:
            ResultWrapper = DictQueryResultWrapper
        elif self._naive or not self._joins or self.verify_naive():
            ResultWrapper = NaiveQueryResultWrapper
        elif self._aggregate_rows:
            ResultWrapper = AggregateQueryResultWrapper
        else:
            ResultWrapper = ModelQueryResultWrapper
        self._qr = ResultWrapper(model_class, self._execute(), query_meta)
        self._dirty = False
        return self._qr
    else:
        return self._qr
",elif self . _dicts :,198
"def get_metrics():
    classifier, feature_labels = load_classifier()
    available_metrics = ImgageMetrics.get_metric_classes()
    # todo review: DONE IN DOCS
    #  effective_metrics isn't used after filling it with values
    #  in the loops below
    effective_metrics = []
    for metric in available_metrics:
        for label in feature_labels:
            for label_part in metric.get_labels():
                if label_part == label and metric not in effective_metrics:
                    effective_metrics.append(metric)
    return (classifier, feature_labels, available_metrics)
",if label_part == label and metric not in effective_metrics :,156
"def test_nic_names(self):
    p = subprocess.Popen([""ipconfig"", ""/all""], stdout=subprocess.PIPE)
    out = p.communicate()[0]
    if PY3:
        out = str(out, sys.stdout.encoding)
    nics = psutil.net_io_counters(pernic=True).keys()
    for nic in nics:
        if ""pseudo-interface"" in nic.replace("" "", ""-"").lower():
            continue
        if nic not in out:
            self.fail(""%r nic wasn't found in 'ipconfig /all' output"" % nic)
",if nic not in out :,145
"def convert_with_key(self, key, value, replace=True):
    result = self.configurator.convert(value)
    # If the converted value is different, save for next time
    if value is not result:
        if replace:
            self[key] = result
        if type(result) in (ConvertingDict, ConvertingList, ConvertingTuple):
            result.parent = self
            result.key = key
    return result
","if type ( result ) in ( ConvertingDict , ConvertingList , ConvertingTuple ) :",111
"def _EvaluateFile(self, test_list, file):
    (name, ext) = os.path.splitext(file)
    if ext == "".cc"" or ext == "".cpp"" or ext == "".c"":
        if re.search(""_test$|_test_$|_unittest$|_unittest_$|^test_|Tests$"", name):
            logger.SilentLog(""Found native test file %s"" % file)
            test_list.append(name)
","if re . search ( ""_test$|_test_$|_unittest$|_unittest_$|^test_|Tests$"" , name ) :",112
"def leading_whitespace(self, inputstring):
    """"""Get leading whitespace.""""""
    leading_ws = []
    for i, c in enumerate(inputstring):
        if c in legal_indent_chars:
            leading_ws.append(c)
        else:
            break
        if self.indchar is None:
            self.indchar = c
        elif c != self.indchar:
            self.strict_err_or_warn(""found mixing of tabs and spaces"", inputstring, i)
    return """".join(leading_ws)
",if c in legal_indent_chars :,139
"def ident_values(self):
    value = self._ident_values
    if value is False:
        value = None
        # XXX: how will this interact with orig_prefix ?
        #      not exposing attrs for now if orig_prefix is set.
        if not self.orig_prefix:
            wrapped = self.wrapped
            idents = getattr(wrapped, ""ident_values"", None)
            if idents:
                value = [self._wrap_hash(ident) for ident in idents]
            ##else:
            ##    ident = self.ident
            ##    if ident is not None:
            ##        value = [ident]
        self._ident_values = value
    return value
",if not self . orig_prefix :,200
"def _available_symbols(self, scoperef, expr):
    cplns = []
    found_names = set()
    while scoperef:
        elem = self._elem_from_scoperef(scoperef)
        for child in elem:
            name = child.get(""name"", """")
            if name.startswith(expr):
                if name not in found_names:
                    found_names.add(name)
                    ilk = child.get(""ilk"") or child.tag
                    cplns.append((ilk, name))
        scoperef = self.parent_scoperef_from_scoperef(scoperef)
        if not scoperef:
            break
    return sorted(cplns, key=operator.itemgetter(1))
",if name . startswith ( expr ) :,196
"def pid_from_name(name):
    # quick and dirty, works with all linux not depending on ps output
    for pid in os.listdir(""/proc""):
        try:
            int(pid)
        except:
            continue
        pname = """"
        with open(""/proc/%s/cmdline"" % pid, ""r"") as f:
            pname = f.read()
        if name in pname:
            return int(pid)
    raise ProcessException(""No process with such name: %s"" % name)
",if name in pname :,134
"def touch(self):
    if not self.exists():
        try:
            self.parent().touch()
        except ValueError:
            pass
        node = self._fs.touch(self.pathnames, {})
        if not node.isdir:
            raise AssertionError(""Not a folder: %s"" % self.path)
        if self.watcher:
            self.watcher.emit(""created"", self)
",if not node . isdir :,107
"def setUp(self):
    BaseTestCase.setUp(self)
    self.rawData = []
    self.dataByKey = {}
    for i in range(1, 11):
        stringCol = ""String %d"" % i
        fixedCharCol = (""Fixed Char %d"" % i).ljust(40)
        rawCol = ""Raw %d"" % i
        if i % 2:
            nullableCol = ""Nullable %d"" % i
        else:
            nullableCol = None
        dataTuple = (i, stringCol, rawCol, fixedCharCol, nullableCol)
        self.rawData.append(dataTuple)
        self.dataByKey[i] = dataTuple
",if i % 2 :,173
"def GenerateVector(self, hits, vector, level):
    """"""Generate possible hit vectors which match the rules.""""""
    for item in hits.get(level, []):
        if vector:
            if item < vector[-1]:
                continue
            if item > self.max_separation + vector[-1]:
                break
        new_vector = vector + [item]
        if level + 1 == len(hits):
            yield new_vector
        elif level + 1 < len(hits):
            for result in self.GenerateVector(hits, new_vector, level + 1):
                yield result
",if level + 1 == len ( hits ) :,157
"def __repr__(self):
    attrs = []
    for k in self.keydata:
        if k == ""p"":
            attrs.append(""p(%d)"" % (self.size() + 1,))
        elif hasattr(self.key, k):
            attrs.append(k)
    if self.has_private():
        attrs.append(""private"")
    return ""<%s @0x%x %s>"" % (self.__class__.__name__, id(self), "","".join(attrs))
","if k == ""p"" :",122
"def autoload(self):
    if self._app.config.THEME == ""auto"":
        if sys.platform == ""darwin"":
            if get_osx_theme() == 1:
                theme = DARK
            else:
                theme = LIGHT
        else:
            theme = self.guess_system_theme()
            if theme == Dark:
                theme = MacOSDark
    else:  # user settings have highest priority
        theme = self._app.config.THEME
    self.load_theme(theme)
","if sys . platform == ""darwin"" :",141
"def _get_matching_bracket(self, s, pos):
    if s[pos] != ""{"":
        return None
    end = len(s)
    depth = 1
    pos += 1
    while pos != end:
        c = s[pos]
        if c == ""{"":
            depth += 1
        elif c == ""}"":
            depth -= 1
        if depth == 0:
            break
        pos += 1
    if pos < end and s[pos] == ""}"":
        return pos
    return None
",if depth == 0 :,132
"def update_meter(self, output, target, meters={""accuracy""}):
    output = self.__to_tensor(output)
    target = self.__to_tensor(target)
    for meter in meters:
        if meter not in self.meter.keys():
            self.__addmeter(meter)
        if meter in [""ap"", ""map"", ""confusion""]:
            target_th = self._ver2tensor(target)
            self.meter[meter].add(output, target_th)
        else:
            self.meter[meter].add(output, target)
",if meter not in self . meter . keys ( ) :,147
"def _reinit_optimizers_with_oss(self):
    optimizers = self.lightning_module.trainer.optimizers
    for x, optimizer in enumerate(optimizers):
        if is_lightning_optimizer(optimizer):
            optimizer = optimizer._optimizer
        if not isinstance(optimizer, OSS):
            optim_class = type(optimizer)
            zero_optimizer = OSS(
                params=optimizer.param_groups, optim=optim_class, **optimizer.defaults
            )
            optimizers[x] = zero_optimizer
            del optimizer
    trainer = self.lightning_module.trainer
    trainer.optimizers = optimizers
    trainer.convert_to_lightning_optimizers()
","if not isinstance ( optimizer , OSS ) :",175
"def OnSelChanged(self, event):
    self.item = event.GetItem()
    if self.item:
        self.log.write(""OnSelChanged: %s"" % self.GetItemText(self.item))
        if wx.Platform == ""__WXMSW__"":
            self.log.write(
                "", BoundingRect: %s\n"" % self.GetBoundingRect(self.item, True)
            )
        else:
            self.log.write(""\n"")
    event.Skip()
","if wx . Platform == ""__WXMSW__"" :",131
"def parse_batch(args):
    errmsg = ""Invalid batch definition: batch entry has to be defined as RULE=BATCH/BATCHES (with integers BATCH <= BATCHES, BATCH >= 1).""
    if args.batch is not None:
        rule, batchdef = parse_key_value_arg(args.batch, errmsg=errmsg)
        try:
            batch, batches = batchdef.split(""/"")
            batch = int(batch)
            batches = int(batches)
        except ValueError:
            raise ValueError(errmsg)
        if batch > batches or batch < 1:
            raise ValueError(errmsg)
        return Batch(rule, batch, batches)
    return None
",if batch > batches or batch < 1 :,167
"def get_foreign_key_columns(self, engine, table_name):
    foreign_keys = set()
    table = db_utils.get_table(engine, table_name)
    inspector = reflection.Inspector.from_engine(engine)
    for column_dict in inspector.get_columns(table_name):
        column_name = column_dict[""name""]
        column = getattr(table.c, column_name)
        if column.foreign_keys:
            foreign_keys.add(column_name)
    return foreign_keys
",if column . foreign_keys :,135
"def update(self, t):
    l = int(t * self.nr_of_tiles)
    for i in range(self.nr_of_tiles):
        t = self.tiles_order[i]
        if i < l:
            self.turn_off_tile(t)
        else:
            self.turn_on_tile(t)
",if i < l :,93
"def read(self, amt=None):
    # the _rbuf test is only in this first if for speed.  It's not
    # logically necessary
    if self._rbuf and not amt is None:
        L = len(self._rbuf)
        if amt > L:
            amt -= L
        else:
            s = self._rbuf[:amt]
            self._rbuf = self._rbuf[amt:]
            return s
    s = self._rbuf + self._raw_read(amt)
    self._rbuf = b""""
    return s
",if amt > L :,153
"def draw_menu_button(self, context, layout, node, text):
    if (
        hasattr(node.id_data, ""sv_show_socket_menus"")
        and node.id_data.sv_show_socket_menus
    ):
        if self.is_output or self.is_linked or not self.use_prop:
            layout.menu(""SV_MT_SocketOptionsMenu"", text="""", icon=""TRIA_DOWN"")
",if self . is_output or self . is_linked or not self . use_prop :,110
"def __enter__(self):
    with DB.connection_context():
        session_record = SessionRecord()
        session_record.f_session_id = self._session_id
        session_record.f_engine_name = self._engine_name
        session_record.f_engine_type = EngineType.STORAGE
        # TODO: engine address
        session_record.f_engine_address = {}
        session_record.f_create_time = current_timestamp()
        rows = session_record.save(force_insert=True)
        if rows != 1:
            raise Exception(f""create session record {self._session_id} failed"")
        LOGGER.debug(f""save session {self._session_id} record"")
    self.create()
    return self
",if rows != 1 :,195
"def tearDown(self):
    """"""Shutdown the server.""""""
    try:
        if self.server:
            self.server.stop(2.0)
        if self.sl_hdlr:
            self.root_logger.removeHandler(self.sl_hdlr)
            self.sl_hdlr.close()
    finally:
        BaseTest.tearDown(self)
",if self . sl_hdlr :,96
"def _dec_device(self, srcdev, dstdev):
    if srcdev:
        self.srcdevs[srcdev] -= 1
        if self.srcdevs[srcdev] == 0:
            del self.srcdevs[srcdev]
        self._set_limits(""read"", self.srcdevs)
    if dstdev:
        self.dstdevs[dstdev] -= 1
        if self.dstdevs[dstdev] == 0:
            del self.dstdevs[dstdev]
        self._set_limits(""write"", self.dstdevs)
",if self . srcdevs [ srcdev ] == 0 :,141
"def array_for(self, i):
    if 0 <= i < self._cnt:
        if i >= self.tailoff():
            return self._tail
        node = self._root
        level = self._shift
        while level > 0:
            assert isinstance(node, Node)
            node = node._array[(i >> level) & 0x01F]
            level -= 5
        return node._array
    affirm(False, u""Index out of Range"")
",if i >= self . tailoff ( ) :,125
"def convert_tensor(self, offsets, sizes):
    results = []
    for b, batch in enumerate(offsets):
        utterances = []
        for p, utt in enumerate(batch):
            size = sizes[b][p]
            if sizes[b][p] > 0:
                utterances.append(utt[0:size])
            else:
                utterances.append(torch.tensor([], dtype=torch.int))
        results.append(utterances)
    return results
",if sizes [ b ] [ p ] > 0 :,126
"def _predict_proba(self, X, preprocess=True):
    if preprocess:
        X = self.preprocess(X)
    if self.problem_type == REGRESSION:
        return self.model.predict(X)
    y_pred_proba = self.model.predict_proba(X)
    if self.problem_type == BINARY:
        if len(y_pred_proba.shape) == 1:
            return y_pred_proba
        elif y_pred_proba.shape[1] > 1:
            return y_pred_proba[:, 1]
        else:
            return y_pred_proba
    elif y_pred_proba.shape[1] > 2:
        return y_pred_proba
    else:
        return y_pred_proba[:, 1]
",elif y_pred_proba . shape [ 1 ] > 1 :,198
"def timeout(self):
    now = ptime.time()
    dt = now - self.lastPlayTime
    if dt < 0:
        return
    n = int(self.playRate * dt)
    if n != 0:
        self.lastPlayTime += float(n) / self.playRate
        if self.currentIndex + n > self.image.shape[self.axes[""t""]]:
            self.play(0)
        self.jumpFrames(n)
","if self . currentIndex + n > self . image . shape [ self . axes [ ""t"" ] ] :",117
"def __init__(self, data, weights=None, ddof=0):
    self.data = np.asarray(data)
    if weights is None:
        self.weights = np.ones(self.data.shape[0])
    else:
        self.weights = np.asarray(weights).astype(float)
        # TODO: why squeeze?
        if len(self.weights.shape) > 1 and len(self.weights) > 1:
            self.weights = self.weights.squeeze()
    self.ddof = ddof
",if len ( self . weights . shape ) > 1 and len ( self . weights ) > 1 :,130
"def writerow(self, row):
    unicode_row = []
    for col in row:
        if type(col) == str or type(col) == unicode:
            unicode_row.append(col.encode(""utf-8"").strip())
        else:
            unicode_row.append(col)
    self.writer.writerow(unicode_row)
    # Fetch UTF-8 output from the queue ...
    data = self.queue.getvalue()
    data = data.decode(""utf-8"")
    # ... and reencode it into the target encoding
    data = self.encoder.encode(data)
    # write to the target stream
    self.stream.write(data)
    # empty queue
    self.queue.truncate(0)
",if type ( col ) == str or type ( col ) == unicode :,182
"def __init__(self, choices, allow_blank=False, **kwargs):
    self.choiceset = choices
    self.allow_blank = allow_blank
    self._choices = dict()
    # Unpack grouped choices
    for k, v in choices:
        if type(v) in [list, tuple]:
            for k2, v2 in v:
                self._choices[k2] = v2
        else:
            self._choices[k] = v
    super().__init__(**kwargs)
","if type ( v ) in [ list , tuple ] :",127
"def simp_ext(_, expr):
    if expr.op.startswith(""zeroExt_""):
        arg = expr.args[0]
        if expr.size == arg.size:
            return arg
        return ExprCompose(arg, ExprInt(0, expr.size - arg.size))
    if expr.op.startswith(""signExt_""):
        arg = expr.args[0]
        add_size = expr.size - arg.size
        new_expr = ExprCompose(
            arg,
            ExprCond(
                arg.msb(), ExprInt(size2mask(add_size), add_size), ExprInt(0, add_size)
            ),
        )
        return new_expr
    return expr
",if expr . size == arg . size :,191
"def mark_differences(value: str, compare_against: str):
    result = []
    for i, char in enumerate(value):
        try:
            if char != compare_against[i]:
                result.append('<font color=""red"">{}</font>'.format(char))
            else:
                result.append(char)
        except IndexError:
            result.append(char)
    return """".join(result)
",if char != compare_against [ i ] :,111
"def run_query(self, query, user):
    url = ""%s%s"" % (self.base_url, ""&"".join(query.split(""\n"")))
    error = None
    data = None
    try:
        response = requests.get(url, auth=self.auth, verify=self.verify)
        if response.status_code == 200:
            data = _transform_result(response)
        else:
            error = ""Failed getting results (%d)"" % response.status_code
    except Exception as ex:
        data = None
        error = str(ex)
    return data, error
",if response . status_code == 200 :,153
"def on_enter(self):
    """"""Fired when mouse enter the bbox of the widget.""""""
    if hasattr(self, ""md_bg_color"") and self.focus_behavior:
        if hasattr(self, ""theme_cls"") and not self.focus_color:
            self.md_bg_color = self.theme_cls.bg_normal
        else:
            if not self.focus_color:
                self.md_bg_color = App.get_running_app().theme_cls.bg_normal
            else:
                self.md_bg_color = self.focus_color
","if hasattr ( self , ""theme_cls"" ) and not self . focus_color :",154
"def tearDown(self):
    if not self.is_playback():
        try:
            if self.hosted_service_name is not None:
                self.sms.delete_hosted_service(self.hosted_service_name)
        except:
            pass
        try:
            if self.storage_account_name is not None:
                self.sms.delete_storage_account(self.storage_account_name)
        except:
            pass
        try:
            self.sms.delete_affinity_group(self.affinity_group_name)
        except:
            pass
    return super(LegacyMgmtAffinityGroupTest, self).tearDown()
",if self . hosted_service_name is not None :,180
"def name2cp(k):
    if k == ""apos"":
        return ord(""'"")
    if hasattr(htmlentitydefs, ""name2codepoint""):  # requires Python 2.3
        return htmlentitydefs.name2codepoint[k]
    else:
        k = htmlentitydefs.entitydefs[k]
        if k.startswith(""&#"") and k.endswith("";""):
            return int(k[2:-1])  # not in latin-1
        return ord(codecs.latin_1_decode(k)[0])
","if k . startswith ( ""&#"" ) and k . endswith ( "";"" ) :",125
"def _para_set(self, params, part):
    if len(params) == 0:
        result = suggest([i.get_name() for i in self._options], part)
        return result
    elif len(params) == 1:
        paramName = params[0]
        if paramName not in self._options:
            return []
        opt = self._options[paramName]
        paramType = opt.get_type()
        if paramType == ""boolean"":
            values = [opt.get_default_value() == ""True"" and ""False"" or ""True""]
        else:
            values = self._memory[paramName]
        return suggest(values, part)
    else:
        return []
","if paramType == ""boolean"" :",186
"def hexcmp(x, y):
    try:
        a = int(x, 16)
        b = int(y, 16)
        if a < b:
            return -1
        if a > b:
            return 1
        return 0
    except:
        return cmp(x, y)
",if a < b :,83
"def execute(self, statement, arguments=None):
    while True:
        try:
            if arguments:
                self.cursor.execute(statement, arguments)
            else:
                self.cursor.execute(statement)
        except sqlite3.OperationalError as ex:
            if ""locked"" not in getSafeExString(ex):
                raise
        else:
            break
    if statement.lstrip().upper().startswith(""SELECT""):
        return self.cursor.fetchall()
","if ""locked"" not in getSafeExString ( ex ) :",130
"def _test_forever(self, tests):
    while True:
        for test_name in tests:
            yield test_name
            if self.bad:
                return
            if self.ns.fail_env_changed and self.environment_changed:
                return
",if self . bad :,76
"def removeUser(self, username):
    hideFromOSD = not constants.SHOW_DIFFERENT_ROOM_OSD
    if username in self._users:
        user = self._users[username]
        if user.room:
            if self.isRoomSame(user.room):
                hideFromOSD = not constants.SHOW_SAME_ROOM_OSD
    if username in self._users:
        self._users.pop(username)
        message = getMessage(""left-notification"").format(username)
        self.ui.showMessage(message, hideFromOSD)
        self._client.lastLeftTime = time.time()
        self._client.lastLeftUser = username
    self.userListChange()
",if self . isRoomSame ( user . room ) :,184
"def AutoTest():
    with open(sys.argv[1], ""rb"") as f:
        for line in f.read().split(b""\n""):
            line = BYTES2SYSTEMSTR(line.strip())
            if not line:
                continue
            elif line.startswith(""#""):
                print(line)
            else:
                print("">>> "" + line)
                os.system(line)
                sys.stdout.write(""\npress enter to continue..."")
                if PY3:
                    input()
                else:
                    raw_input()
                sys.stdout.write(""\n"")
",if PY3 :,179
"def get_first_field(layout, clz):
    for layout_object in layout.fields:
        if issubclass(layout_object.__class__, clz):
            return layout_object
        elif hasattr(layout_object, ""get_field_names""):
            gf = get_first_field(layout_object, clz)
            if gf:
                return gf
","if issubclass ( layout_object . __class__ , clz ) :",94
"def sanitize_event_keys(kwargs, valid_keys):
    # Sanity check: Don't honor keys that we don't recognize.
    for key in list(kwargs.keys()):
        if key not in valid_keys:
            kwargs.pop(key)
    # Truncate certain values over 1k
    for key in [""play"", ""role"", ""task"", ""playbook""]:
        if isinstance(kwargs.get(""event_data"", {}).get(key), str):
            if len(kwargs[""event_data""][key]) > 1024:
                kwargs[""event_data""][key] = Truncator(kwargs[""event_data""][key]).chars(
                    1024
                )
","if len ( kwargs [ ""event_data"" ] [ key ] ) > 1024 :",168
"def visit_productionlist(self, node):
    self.new_state()
    names = []
    for production in node:
        names.append(production[""tokenname""])
    maxlen = max(len(name) for name in names)
    for production in node:
        if production[""tokenname""]:
            self.add_text(production[""tokenname""].ljust(maxlen) + "" ::="")
            lastname = production[""tokenname""]
        else:
            self.add_text(""%s    "" % ("" "" * len(lastname)))
        self.add_text(production.astext() + self.nl)
    self.end_state(wrap=False)
    raise nodes.SkipNode
","if production [ ""tokenname"" ] :",168
"def uuid(self):
    if not getattr(self, ""_uuid"", None):
        if self.repository is not None:
            self._uuid = self.repository._kp_uuid(
                self.path
            )  # Use repository UUID (even if None)
        else:
            self._uuid = str(uuid.uuid4())
    return self._uuid
",if self . repository is not None :,95
"def remove(self, values):
    if not isinstance(values, (list, tuple, set)):
        values = [values]
    for v in values:
        v = str(v)
        if isinstance(self._definition, dict):
            self._definition.pop(v, None)
        elif self._definition == ""ANY"":
            if v == ""ANY"":
                self._definition = []
        elif v in self._definition:
            self._definition.remove(v)
    if (
        self._value is not None
        and self._value not in self._definition
        and self._not_any()
    ):
        raise ConanException(bad_value_msg(self._name, self._value, self.values_range))
","if isinstance ( self . _definition , dict ) :",192
"def make(self):
    pygments_dir = join(self.dir, ""externals"", ""pygments"")
    if exists(pygments_dir):
        run_in_dir(""hg pull"", pygments_dir, self.log.info)
        run_in_dir(""hg update"", pygments_dir, self.log.info)
    else:
        if not exists(dirname(pygments_dir)):
            os.makedirs(dirname(pygments_dir))
        run_in_dir(
            ""hg clone http://dev.pocoo.org/hg/pygments-main %s""
            % basename(pygments_dir),
            dirname(pygments_dir),
            self.log.info,
        )
",if not exists ( dirname ( pygments_dir ) ) :,177
"def set_field(self):
    i = 0
    for string in self.display_string:
        if self.conversion_fn:
            self.config[self.field + str(i)] = self.conversion_fn(self.str[i])
        else:
            self.config[self.field + str(i)] = self.str[i]
        i = i + 1
",if self . conversion_fn :,99
"def cleanup(self):
    with self.lock:
        for proc in self.processes:
            if proc.is_alive():
                continue
            proc.join()
            self.processes.remove(proc)
            log.debug(""Subprocess %s cleaned up"", proc.name)
",if proc . is_alive ( ) :,79
"def setup(self, gen):
    Node.setup(self, gen)
    for c in self.children:
        c.setup(gen)
    if not self.accepts_epsilon:
        # If it's not already accepting epsilon, it might now do so.
        for c in self.children:
            # any non-epsilon means all is non-epsilon
            if not c.accepts_epsilon:
                break
        else:
            self.accepts_epsilon = 1
            gen.changed()
",if not c . accepts_epsilon :,135
"def __call__(self, message):
    with self._lock:
        self._pending_ack += 1
        self.max_pending_ack = max(self.max_pending_ack, self._pending_ack)
        self.seen_message_ids.append(int(message.attributes[""seq_num""]))
    time.sleep(self._processing_time)
    with self._lock:
        self._pending_ack -= 1
        message.ack()
        self.completed_calls += 1
        if self.completed_calls >= self._resolve_at_msg_count:
            if not self.done_future.done():
                self.done_future.set_result(None)
",if not self . done_future . done ( ) :,173
"def build_canned_image_list(path):
    layers_path = get_bitbake_var(""BBLAYERS"")
    canned_wks_layer_dirs = []
    if layers_path is not None:
        for layer_path in layers_path.split():
            for wks_path in (WIC_DIR, SCRIPTS_CANNED_IMAGE_DIR):
                cpath = os.path.join(layer_path, wks_path)
                if os.path.isdir(cpath):
                    canned_wks_layer_dirs.append(cpath)
    cpath = os.path.join(path, CANNED_IMAGE_DIR)
    canned_wks_layer_dirs.append(cpath)
    return canned_wks_layer_dirs
",if os . path . isdir ( cpath ) :,199
"def _recv_loop(self) -> None:
    async with self._ws as connection:
        self._connected = True
        self.connection = connection
        while self._connected:
            try:
                resp = await self.connection.recv()
                if resp:
                    await self._on_message(resp)
            except (websockets.ConnectionClosed, ConnectionResetError):
                logger.info(""connection closed"")
                break
            await asyncio.sleep(0)
    if self._connected:
        self._loop.create_task(self.dispose())
",if resp :,156
"def _get_between(content, start, end=None):
    should_yield = False
    for line in content.split(""\n""):
        if start in line:
            should_yield = True
            continue
        if end and end in line:
            return
        if should_yield and line:
            yield line.strip().split("" "")[0]
",if end and end in line :,94
"def handle_parse_result(self, ctx, opts, args):
    if self.name in opts:
        if self.mutually_exclusive.intersection(opts):
            self._raise_exclusive_error()
        if self.multiple and len(set(opts[self.name])) > 1:
            self._raise_exclusive_error()
    return super(MutuallyExclusiveOption, self).handle_parse_result(ctx, opts, args)
",if self . multiple and len ( set ( opts [ self . name ] ) ) > 1 :,108
"def write(self, s):
    if self.interactive:
        if isinstance(self.active_mode, deluge.ui.console.modes.cmdline.CmdLine):
            self.active_mode.write(s)
        else:
            component.get(""CmdLine"").add_line(s, False)
            self.events.append(s)
    else:
        print(colors.strip_colors(s))
","if isinstance ( self . active_mode , deluge . ui . console . modes . cmdline . CmdLine ) :",110
"def findfiles(path):
    files = []
    for name in os.listdir(path):
        # ignore hidden files/dirs and other unwanted files
        if name.startswith(""."") or name == ""lastsnap.jpg"":
            continue
        pathname = os.path.join(path, name)
        st = os.lstat(pathname)
        mode = st.st_mode
        if stat.S_ISDIR(mode):
            files.extend(findfiles(pathname))
        elif stat.S_ISREG(mode):
            files.append((pathname, name, st))
    return files
",if stat . S_ISDIR ( mode ) :,150
"def _get_documented_completions(self, table, startswith=None):
    names = []
    for key, command in table.items():
        if getattr(command, ""_UNDOCUMENTED"", False):
            # Don't tab complete undocumented commands/params
            continue
        if startswith is not None and not key.startswith(startswith):
            continue
        if getattr(command, ""positional_arg"", False):
            continue
        names.append(key)
    return names
","if getattr ( command , ""positional_arg"" , False ) :",118
"def fix_newlines(lines):
    """"""Convert newlines to unix.""""""
    for i, line in enumerate(lines):
        if line.endswith(""\r\n""):
            lines[i] = line[:-2] + ""\n""
        elif line.endswith(""\r""):
            lines[i] = line[:-1] + ""\n""
","if line . endswith ( ""\r\n"" ) :",83
"def GeneratePageMetatadata(self, task):
    address_space = self.session.GetParameter(""default_address_space"")
    for vma in task.mm.mmap.walk_list(""vm_next""):
        start = vma.vm_start
        end = vma.vm_end
        # Skip the entire region.
        if end < self.plugin_args.start:
            continue
        # Done.
        if start > self.plugin_args.end:
            break
        for vaddr in utils.xrange(start, end, 0x1000):
            if self.plugin_args.start <= vaddr <= self.plugin_args.end:
                yield vaddr, self._CreateMetadata(address_space.describe_vtop(vaddr))
",if self . plugin_args . start <= vaddr <= self . plugin_args . end :,195
"def get_shape_at_node(self, node, assumptions):
    for k, v in assumptions.items():
        if k in node.names:
            return v
    if node.inputs:
        return node.container.shape(
            input_shapes=[
                self.get_shape_at_node(input_node, assumptions)
                for input_node in node.inputs
            ]
        )
    else:
        return node.container.shape(None)
",if k in node . names :,127
"def fix_doc(self, doc):
    type = doc.get(""type"", {}).get(""key"")
    if type == ""/type/work"":
        if doc.get(""authors""):
            # some record got empty author records because of an error
            # temporary hack to fix
            doc[""authors""] = [
                a for a in doc[""authors""] if ""author"" in a and ""key"" in a[""author""]
            ]
    elif type == ""/type/edition"":
        # get rid of title_prefix.
        if ""title_prefix"" in doc:
            title = doc[""title_prefix""].strip() + "" "" + doc.get(""title"", """")
            doc[""title""] = title.strip()
            del doc[""title_prefix""]
    return doc
","if doc . get ( ""authors"" ) :",199
"def modify_column(self, column: List[Optional[""Cell""]]):
    for i in range(len(column)):
        gate = column[i]
        if gate is self:
            continue
        elif isinstance(gate, ParityControlCell):
            # The first parity control to modify the column must merge all
            # of the other parity controls into itself.
            column[i] = None
            self._basis_change += gate._basis_change
            self.qubits += gate.qubits
        elif gate is not None:
            column[i] = gate.controlled_by(self.qubits[0])
",if gate is self :,164
"def onSync(self, auto=False, reload=True):
    if not auto or (
        self.pm.profile[""syncKey""] and self.pm.profile[""autoSync""] and not self.safeMode
    ):
        from aqt.sync import SyncManager
        if not self.unloadCollection():
            return
        # set a sync state so the refresh timer doesn't fire while deck
        # unloaded
        self.state = ""sync""
        self.syncer = SyncManager(self, self.pm)
        self.syncer.sync()
    if reload:
        if not self.col:
            self.loadCollection()
",if not self . col :,161
"def _has_url_match(self, match, request_url):
    url = match[""url""]
    if _is_string(url):
        if match[""match_querystring""]:
            return self._has_strict_url_match(url, request_url)
        else:
            url_without_qs = request_url.split(""?"", 1)[0]
            return url == url_without_qs
    elif isinstance(url, re._pattern_type) and url.match(request_url):
        return True
    else:
        return False
","if match [ ""match_querystring"" ] :",140
"def pool_image(self, image):
    if self.count < self.pool_size:
        self.pool.append(image)
        self.count += 1
        return image
    else:
        p = random.random()
        if p > 0.5:
            random_id = random.randint(0, self.pool_size - 1)
            temp = self.pool[random_id]
            self.pool[random_id] = image
            return temp
        else:
            return image
",if p > 0.5 :,137
"def get_target_dimensions(self):
    width, height = self.engine.size
    for operation in self.operations:
        if operation[""type""] == ""crop"":
            width = operation[""right""] - operation[""left""]
            height = operation[""bottom""] - operation[""top""]
        if operation[""type""] == ""resize"":
            width = operation[""width""]
            height = operation[""height""]
    return (width, height)
","if operation [ ""type"" ] == ""crop"" :",112
"def validate_matrix(matrix):
    if not matrix:
        return None
    for key, value in matrix.items():
        if value.is_distribution and not value.is_uniform:
            raise ValidationError(
                ""`{}` defines a non uniform distribution, ""
                ""and it cannot be used with bayesian optimization."".format(key)
            )
    return matrix
",if value . is_distribution and not value . is_uniform :,98
"def scm_to_conandata(self):
    try:
        scm_to_conandata = get_env(""CONAN_SCM_TO_CONANDATA"")
        if scm_to_conandata is None:
            scm_to_conandata = self.get_item(""general.scm_to_conandata"")
        return scm_to_conandata.lower() in (""1"", ""true"")
    except ConanException:
        return False
",if scm_to_conandata is None :,124
"def _link_vrf_table(self, vrf_table, rt_list):
    route_family = vrf_table.route_family
    for rt in rt_list:
        rt_rf_id = rt + "":"" + str(route_family)
        table_set = self._tables_for_rt.get(rt_rf_id)
        if table_set is None:
            table_set = set()
            self._tables_for_rt[rt_rf_id] = table_set
        table_set.add(vrf_table)
        LOG.debug(""Added VrfTable %s to import RT table list: %s"", vrf_table, rt)
",if table_set is None :,172
"def add_tags(
    self, cve_results: Dict[str, Dict[str, Dict[str, str]]], file_object: FileObject
):
    # results structure: {'component': {'cve_id': {'score2': '6.4', 'score3': 'N/A'}}}
    for component in cve_results:
        for cve_id in cve_results[component]:
            entry = cve_results[component][cve_id]
            if self._entry_has_critical_rating(entry):
                self.add_analysis_tag(
                    file_object, ""CVE"", ""critical CVE"", TagColor.RED, True
                )
                return
",if self . _entry_has_critical_rating ( entry ) :,180
"def _validate(self):
    try:
        super(CustomClassifier, self)._validate()
    except UnsupportedDataType:
        if self.dtype in FACTOR_DTYPES:
            raise UnsupportedDataType(
                typename=type(self).__name__,
                dtype=self.dtype,
                hint=""Did you mean to create a CustomFactor?"",
            )
        elif self.dtype in FILTER_DTYPES:
            raise UnsupportedDataType(
                typename=type(self).__name__,
                dtype=self.dtype,
                hint=""Did you mean to create a CustomFilter?"",
            )
        raise
",elif self . dtype in FILTER_DTYPES :,167
"def formatMessage(self, record):
    recordcopy = copy(record)
    levelname = recordcopy.levelname
    seperator = "" "" * (8 - len(recordcopy.levelname))
    if self.use_colors:
        levelname = self.color_level_name(levelname, recordcopy.levelno)
        if ""color_message"" in recordcopy.__dict__:
            recordcopy.msg = recordcopy.__dict__[""color_message""]
            recordcopy.__dict__[""message""] = recordcopy.getMessage()
    recordcopy.__dict__[""levelprefix""] = levelname + "":"" + seperator
    return super().formatMessage(recordcopy)
","if ""color_message"" in recordcopy . __dict__ :",152
"def dumpregs(self):
    for reg in (
        list(self.regs.retaddr)
        + list(self.regs.misc)
        + list(self.regs.common)
        + list(self.regs.flags)
    ):
        enum = self.get_reg_enum(reg)
        if not reg or enum is None:
            debug(""# Could not dump register %r"" % reg)
            continue
        name = ""U.x86_const.UC_X86_REG_%s"" % reg.upper()
        value = self.uc.reg_read(enum)
        debug(""uc.reg_read(%(name)s) ==> %(value)x"" % locals())
",if not reg or enum is None :,177
"def filter(self, lexer, stream):
    current_type = None
    current_value = None
    for ttype, value in stream:
        if ttype is current_type:
            current_value += value
        else:
            if current_type is not None:
                yield current_type, current_value
            current_type = ttype
            current_value = value
    if current_type is not None:
        yield current_type, current_value
",if current_type is not None :,121
"def _get_between(content, start, end=None):
    should_yield = False
    for line in content.split(""\n""):
        if start in line:
            should_yield = True
            continue
        if end and end in line:
            return
        if should_yield and line:
            yield line.strip().split("" "")[0]
",if start in line :,94
"def parse_git_config(path):
    """"""Parse git config file.""""""
    config = dict()
    section = None
    with open(os.path.join(path, ""config""), ""r"") as f:
        for line in f:
            line = line.strip()
            if line.startswith(""[""):
                section = line[1:-1].strip()
                config[section] = dict()
            elif section:
                key, value = line.replace("" "", """").split(""="")
                config[section][key] = value
    return config
",elif section :,146
"def test_has_arg(fn, name, accept_all, expected):
    if isinstance(fn, str):
        context = dict()
        try:
            exec(""def {}: pass"".format(fn), context)
        except SyntaxError:
            if sys.version_info >= (3,):
                raise
            pytest.skip(""Function is not compatible with Python 2"")
        # Sometimes exec adds builtins to the context
        context.pop(""__builtins__"", None)
        (fn,) = context.values()
    assert has_arg(fn, name, accept_all) is expected
","if sys . version_info >= ( 3 , ) :",147
"def ObjectExpression(self, properties, **kwargs):
    data = []
    for prop in properties:
        self.emit(prop[""value""])
        if prop[""computed""]:
            raise NotImplementedError(
                ""ECMA 5.1 does not support computed object properties!""
            )
        data.append((to_key(prop[""key""]), prop[""kind""][0]))
    self.emit(""LOAD_OBJECT"", tuple(data))
","if prop [ ""computed"" ] :",108
"def run(self):
    for domain, locale, po in self.locales:
        if self.inplace:
            path = os.path.join(""locale"", locale, ""LC_MESSAGES"")
        else:
            path = os.path.join(self.build_dir, locale, ""LC_MESSAGES"")
        mo = os.path.join(path, ""%s.mo"" % domain)
        self.mkpath(path)
        self.spawn([""msgfmt"", ""-o"", mo, po])
",if self . inplace :,128
"def _compute_map(self, first_byte, second_byte=None):
    if first_byte != 0x0F:
        return ""XED_ILD_MAP0""
    else:
        if second_byte == None:
            return ""XED_ILD_MAP1""
        if second_byte == 0x38:
            return ""XED_ILD_MAP2""
        if second_byte == 0x3A:
            return ""XED_ILD_MAP3""
        if second_byte == 0x0F and self.amd_enabled:
            return ""XED_ILD_MAPAMD""
    die(""Unhandled escape {} / map {} bytes"".format(first_byte, second_byte))
",if second_byte == 0x3A :,181
"def parse_tag(self):
    buf = []
    escaped = False
    for c in self.get_next_chars():
        if escaped:
            buf.append(c)
        elif c == ""\\"":
            escaped = True
        elif c == "">"":
            return """".join(buf)
        else:
            buf.append(c)
    raise Exception(""Unclosed tag "" + """".join(buf))
",if escaped :,110
"def print_pairs(attrs=None, offset_y=0):
    fmt = "" ({0}:{1}) ""
    fmt_len = len(fmt)
    for bg, fg in get_fg_bg():
        try:
            color = curses.color_pair(pair_number(fg, bg))
            if not attrs is None:
                for attr in attrs:
                    color |= attr
            screen.addstr(offset_y + bg, fg * fmt_len, fmt.format(fg, bg), color)
            pass
        except curses.error:
            pass
",if not attrs is None :,152
"def _impl(inputs, input_types):
    data = inputs[0]
    axis = None
    keepdims = False
    if len(inputs) > 2:  # default, torch have only data, axis=None, keepdims=False
        if isinstance(inputs[1], int):
            axis = int(inputs[1])
        elif _is_int_seq(inputs[1]):
            axis = inputs[1]
        else:
            axis = list(_infer_shape(inputs[1]))
        keepdims = bool(inputs[2])
    return get_relay_op(name)(data, axis=axis, keepdims=keepdims)
","if isinstance ( inputs [ 1 ] , int ) :",158
"def run(self, args, **kwargs):
    # Filtering options
    if args.trace_tag:
        kwargs[""trace_tag""] = args.trace_tag
    if args.trigger_instance:
        kwargs[""trigger_instance""] = args.trigger_instance
    if args.execution:
        kwargs[""execution""] = args.execution
    if args.rule:
        kwargs[""rule""] = args.rule
    if args.sort_order:
        if args.sort_order in [""asc"", ""ascending""]:
            kwargs[""sort_asc""] = True
        elif args.sort_order in [""desc"", ""descending""]:
            kwargs[""sort_desc""] = True
    return self.manager.query_with_count(limit=args.last, **kwargs)
","if args . sort_order in [ ""asc"" , ""ascending"" ] :",188
"def retaddr():
    sp = pwndbg.regs.sp
    stack = pwndbg.vmmap.find(sp)
    # Enumerate all return addresses
    frame = gdb.newest_frame()
    addresses = []
    while frame:
        addresses.append(frame.pc())
        frame = frame.older()
    # Find all of them on the stack
    start = stack.vaddr
    stop = start + stack.memsz
    while addresses and start < sp < stop:
        value = pwndbg.memory.u(sp)
        if value in addresses:
            index = addresses.index(value)
            del addresses[:index]
            print(pwndbg.chain.format(sp))
        sp += pwndbg.arch.ptrsize
",if value in addresses :,193
"def update_from_dictio(self, dictio_item):
    for index, dictio_payload in enumerate(dictio_item, 1):
        fuzz_payload = None
        for fuzz_payload in self.payloads[index]:
            fuzz_payload.content = dictio_payload.content
            fuzz_payload.type = dictio_payload.type
        # payload generated not used in seed but in filters
        if fuzz_payload is None:
            self.add(
                {""full_marker"": None, ""word"": None, ""index"": index, ""field"": None},
                dictio_item[index - 1],
            )
",if fuzz_payload is None :,169
"def check_expected(result, expected, contains=False):
    if sys.version_info[0] >= 3:
        if isinstance(result, str):
            result = result.encode(""ascii"")
        if isinstance(expected, str):
            expected = expected.encode(""ascii"")
    resultlines = result.splitlines()
    expectedlines = expected.splitlines()
    if len(resultlines) != len(expectedlines):
        return False
    for rline, eline in zip(resultlines, expectedlines):
        if contains:
            if eline not in rline:
                return False
        else:
            if not rline.endswith(eline):
                return False
    return True
","if isinstance ( result , str ) :",181
"def execute_sql(self, sql, params=None, commit=True):
    try:
        cursor = super(RetryOperationalError, self).execute_sql(sql, params, commit)
    except OperationalError:
        if not self.is_closed():
            self.close()
        with __exception_wrapper__:
            cursor = self.cursor()
            cursor.execute(sql, params or ())
            if commit and not self.in_transaction():
                self.commit()
    return cursor
",if commit and not self . in_transaction ( ) :,127
"def get_operation_ast(document_ast, operation_name=None):
    operation = None
    for definition in document_ast.definitions:
        if isinstance(definition, ast.OperationDefinition):
            if not operation_name:
                # If no operation name is provided, only return an Operation if it is the only one present in the
                # document. This means that if we've encountered a second operation as we were iterating over the
                # definitions in the document, there are more than one Operation defined, and we should return None.
                if operation:
                    return None
                operation = definition
            elif definition.name and definition.name.value == operation_name:
                return definition
    return operation
",if operation :,186
"def removeTrailingWs(self, aList):
    i = 0
    while i < len(aList):
        if self.is_ws(aList[i]):
            j = i
            i = self.skip_ws(aList, i)
            assert j < i
            if i >= len(aList) or aList[i] == ""\n"":
                # print ""removing trailing ws:"", `i-j`
                del aList[j:i]
                i = j
        else:
            i += 1
","if i >= len ( aList ) or aList [ i ] == ""\n"" :",147
"def _process_filter(self, query, host_state):
    """"""Recursively parse the query structure.""""""
    if not query:
        return True
    cmd = query[0]
    method = self.commands[cmd]
    cooked_args = []
    for arg in query[1:]:
        if isinstance(arg, list):
            arg = self._process_filter(arg, host_state)
        elif isinstance(arg, basestring):
            arg = self._parse_string(arg, host_state)
        if arg is not None:
            cooked_args.append(arg)
    result = method(self, cooked_args)
    return result
","if isinstance ( arg , list ) :",163
"def handle_sent(self, elt):
    sent = []
    for child in elt:
        if child.tag in (""mw"", ""hi"", ""corr"", ""trunc""):
            sent += [self.handle_word(w) for w in child]
        elif child.tag in (""w"", ""c""):
            sent.append(self.handle_word(child))
        elif child.tag not in self.tags_to_ignore:
            raise ValueError(""Unexpected element %s"" % child.tag)
    return BNCSentence(elt.attrib[""n""], sent)
",elif child . tag not in self . tags_to_ignore :,141
"def get_display_price(
    base: Union[TaxedMoney, TaxedMoneyRange], display_gross: bool = False
) -> Money:
    """"""Return the price amount that should be displayed based on settings.""""""
    if not display_gross:
        display_gross = display_gross_prices()
    if isinstance(base, TaxedMoneyRange):
        if display_gross:
            base = MoneyRange(start=base.start.gross, stop=base.stop.gross)
        else:
            base = MoneyRange(start=base.start.net, stop=base.stop.net)
    if isinstance(base, TaxedMoney):
        base = base.gross if display_gross else base.net
    return base
",if display_gross :,164
"def check_classes(self, node):
    if isinstance(node, nodes.Element):
        for class_value in node[""classes""][:]:
            if class_value in self.strip_classes:
                node[""classes""].remove(class_value)
            if class_value in self.strip_elements:
                return 1
",if class_value in self . strip_elements :,86
"def validate(outfile=sys.stdout, silent_success=False):
    ""Validates all installed models.""
    try:
        num_errors = get_validation_errors(outfile)
        if silent_success and num_errors == 0:
            return
        outfile.write(
            ""%s error%s found.\n"" % (num_errors, num_errors != 1 and ""s"" or """")
        )
    except ImproperlyConfigured:
        outfile.write(""Skipping validation because things aren't configured properly."")
",if silent_success and num_errors == 0 :,124
"def check_basename_conflicts(self, targets):
    """"""Apps' basenames are used as bundle directory names. Ensure they are all unique.""""""
    basename_seen = {}
    for target in targets:
        if target.basename in basename_seen:
            raise self.BasenameConflictError(
                ""Basename must be unique, found two targets use ""
                ""the same basename: {}'\n\t{} and \n\t{}"".format(
                    target.basename,
                    basename_seen[target.basename].address.spec,
                    target.address.spec,
                )
            )
        basename_seen[target.basename] = target
",if target . basename in basename_seen :,176
"def __init__(self, api_version_str):
    try:
        self.latest = self.preview = False
        self.yyyy = self.mm = self.dd = None
        if api_version_str == ""latest"":
            self.latest = True
        else:
            if ""preview"" in api_version_str:
                self.preview = True
            parts = api_version_str.split(""-"")
            self.yyyy = int(parts[0])
            self.mm = int(parts[1])
            self.dd = int(parts[2])
    except (ValueError, TypeError):
        raise ValueError(
            ""The API version {} is not in a "" ""supported format"".format(api_version_str)
        )
","if api_version_str == ""latest"" :",199
"def _osp2ec(self, bytes):
    compressed = self._from_bytes(bytes)
    y = compressed >> self._bits
    x = compressed & (1 << self._bits) - 1
    if x == 0:
        y = self._curve.b
    else:
        result = self.sqrtp(
            x ** 3 + self._curve.a * x + self._curve.b, self._curve.field.p
        )
        if len(result) == 1:
            y = result[0]
        elif len(result) == 2:
            y1, y2 = result
            y = y1 if (y1 & 1 == y) else y2
        else:
            return None
    return ec.Point(self._curve, x, y)
",if len ( result ) == 1 :,200
"def _visit_import_alike(self, node: Union[cst.Import, cst.ImportFrom]) -> bool:
    names = node.names
    if isinstance(names, cst.ImportStar):
        return False
    # make sure node.names is Sequence[ImportAlias]
    for name in names:
        self.provider.set_metadata(name, self.scope)
        asname = name.asname
        if asname is not None:
            name_values = _gen_dotted_names(cst.ensure_type(asname.name, cst.Name))
        else:
            name_values = _gen_dotted_names(name.name)
        for name_value, _ in name_values:
            self.scope.record_assignment(name_value, node)
    return False
",if asname is not None :,200
"def test_sanity_no_unmatched_parentheses(CorpusType: Type[ColumnCorpus]):
    corpus = CorpusType()
    unbalanced_entities = []
    for sentence in corpus.get_all_sentences():
        entities = sentence.get_spans(""ner"")
        for entity in entities:
            entity_text = """".join(t.text for t in entity.tokens)
            if not has_balanced_parantheses(entity_text):
                unbalanced_entities.append(entity_text)
    assert unbalanced_entities == []
",if not has_balanced_parantheses ( entity_text ) :,128
"def _learn_rate_adjust(self):
    if self.learn_rate_decays == 1.0:
        return
    learn_rate_decays = self._vp(self.learn_rate_decays)
    learn_rate_minimums = self._vp(self.learn_rate_minimums)
    for index, decay in enumerate(learn_rate_decays):
        new_learn_rate = self.net_.learnRates[index] * decay
        if new_learn_rate >= learn_rate_minimums[index]:
            self.net_.learnRates[index] = new_learn_rate
    if self.verbose >= 2:
        print(""Learn rates: {}"".format(self.net_.learnRates))
",if new_learn_rate >= learn_rate_minimums [ index ] :,176
"def set_attr_from_xmp_tag(self, attr, xmp_tags, tags, cast=None):
    v = self.get_xmp_tag(xmp_tags, tags)
    if v is not None:
        if cast is None:
            setattr(self, attr, v)
        else:
            # Handle fractions
            if (cast == float or cast == int) and ""/"" in v:
                v = self.try_parse_fraction(v)
            setattr(self, attr, cast(v))
",if cast is None :,139
"def _merge_scientific_float_tokens(tokens: Iterable[str]) -> List[str]:
    tokens = list(tokens)
    i = 0
    while ""e"" in tokens[i + 1 :]:
        i = tokens.index(""e"", i + 1)
        s = i - 1
        e = i + 1
        if not re.match(""[0-9]"", str(tokens[s])):
            continue
        if re.match(""[+-]"", str(tokens[e])):
            e += 1
        if re.match(""[0-9]"", str(tokens[e])):
            e += 1
            tokens[s:e] = ["""".join(tokens[s:e])]
            i -= 1
    return tokens
","if not re . match ( ""[0-9]"" , str ( tokens [ s ] ) ) :",184
"def anypython(request):
    name = request.param
    executable = getexecutable(name)
    if executable is None:
        if sys.platform == ""win32"":
            executable = winpymap.get(name, None)
            if executable:
                executable = py.path.local(executable)
                if executable.check():
                    return executable
        pytest.skip(""no suitable %s found"" % (name,))
    return executable
","if sys . platform == ""win32"" :",119
"def set_meta(self, dataset, overwrite=True, **kwd):
    super().set_meta(dataset, overwrite=overwrite, **kwd)
    try:
        if dataset and tarfile.is_tarfile(dataset.file_name):
            with tarfile.open(dataset.file_name, ""r"") as temptar:
                dataset.metadata.fast5_count = sum(
                    1 for f in temptar if f.name.endswith("".fast5"")
                )
    except Exception as e:
        log.warning(""%s, set_meta Exception: %s"", self, e)
",if dataset and tarfile . is_tarfile ( dataset . file_name ) :,150
"def run(self):
    for k in list(iterkeys(self.objs)):
        if k.startswith(""_""):
            continue
        v = self.objs[k]
        if v[""_class""] == ""User"":
            self.split_user(k, v)
        elif v[""_class""] in [
            ""Message"",
            ""PrintJob"",
            ""Question"",
            ""Submission"",
            ""UserTest"",
        ]:
            v[""participation""] = v[""user""]
            del v[""user""]
    return self.objs
","if k . startswith ( ""_"" ) :",150
"def _findInTree(t, n):
    ret = []
    if type(t) is dict:
        if ""_name"" in t and t[""_name""] == n:
            ret.append(t)
        for k, v in t.items():
            ret += _findInTree(v, n)
    if type(t) is list:
        for v in t:
            ret += _findInTree(v, n)
    return ret
","if ""_name"" in t and t [ ""_name"" ] == n :",117
"def parseArrayPattern(self):
    node = Node()
    elements = []
    self.expect(""["")
    while not self.match(""]""):
        if self.match("",""):
            self.lex()
            elements.append(null)
        else:
            if self.match(""...""):
                restNode = Node()
                self.lex()
                rest = self.parseVariableIdentifier()
                elements.append(restNode.finishRestElement(rest))
                break
            else:
                elements.append(self.parsePatternWithDefault())
            if not self.match(""]""):
                self.expect("","")
    self.expect(""]"")
    return node.finishArrayPattern(elements)
","if self . match ( "","" ) :",190
"def _set_log_writer(self):
    if self.config[""logging""]:
        config = self.config[""log_writer_config""]
        if config[""writer""] == ""json"":
            self.log_writer = LogWriter(**config)
        elif config[""writer""] == ""tensorboard"":
            self.log_writer = TensorBoardWriter(**config)
        else:
            raise ValueError(f""Unrecognized writer option: {config['writer']}"")
    else:
        self.log_writer = None
","if config [ ""writer"" ] == ""json"" :",127
"def _parse(self, contents):
    entries = []
    hostnames_found = set()
    for line in contents.splitlines():
        if not len(line.strip()):
            entries.append((""blank"", [line]))
            continue
        (head, tail) = chop_comment(line.strip(), ""#"")
        if not len(head):
            entries.append((""all_comment"", [line]))
            continue
        entries.append((""hostname"", [head, tail]))
        hostnames_found.add(head)
    if len(hostnames_found) > 1:
        raise IOError(""Multiple hostnames (%s) found!"" % (hostnames_found))
    return entries
",if not len ( line . strip ( ) ) :,167
"def get_all_values(self, project):
    if isinstance(project, models.Model):
        project_id = project.id
    else:
        project_id = project
    if project_id not in self.__cache:
        cache_key = self._make_key(project_id)
        result = cache.get(cache_key)
        if result is None:
            result = self.reload_cache(project_id)
        else:
            self.__cache[project_id] = result
    return self.__cache.get(project_id, {})
",if result is None :,144
"def needed_libraries(self):
    for cmd in self.load_commands_of_type(0xC):  # LC_LOAD_DYLIB
        tname = self._get_typename(""dylib_command"")
        dylib_command = cmd.cast(tname)
        name_addr = cmd.obj_offset + dylib_command.name
        dylib_name = self.obj_vm.read(name_addr, 256)
        if dylib_name:
            idx = dylib_name.find(""\x00"")
            if idx != -1:
                dylib_name = dylib_name[:idx]
            yield dylib_name
",if dylib_name :,164
"def compress(self, data_list):
    warn_untested()
    if data_list:
        if data_list[1] in forms.fields.EMPTY_VALUES:
            error = self.error_messages[""invalid_year""]
            raise forms.ValidationError(error)
        if data_list[0] in forms.fields.EMPTY_VALUES:
            error = self.error_messages[""invalid_month""]
            raise forms.ValidationError(error)
        year = int(data_list[1])
        month = int(data_list[0])
        # find last day of the month
        day = monthrange(year, month)[1]
        return date(year, month, day)
    return None
",if data_list [ 1 ] in forms . fields . EMPTY_VALUES :,181
"def put(self, obj, block=True, timeout=None):
    assert not self._closed
    if not self._sem.acquire(block, timeout):
        raise Full
    with self._notempty:
        with self._cond:
            if self._thread is None:
                self._start_thread()
            self._buffer.append(obj)
            self._unfinished_tasks.release()
            self._notempty.notify()
",if self . _thread is None :,115
"def has_module(self, module, version):
    has_module = False
    for directory in self.directories:
        module_directory = join(directory, module)
        has_module_directory = isdir(module_directory)
        if not version:
            has_module = has_module_directory or exists(
                module_directory
            )  # could be a bare modulefile
        else:
            modulefile = join(module_directory, version)
            has_modulefile = exists(modulefile)
            has_module = has_module_directory and has_modulefile
        if has_module:
            break
    return has_module
",if has_module :,171
"def expanduser(path):
    if path[:1] == ""~"":
        c = path[1:2]
        if not c:
            return gethome()
        if c == os.sep:
            return asPyString(File(gethome(), path[2:]).getPath())
    return path
",if not c :,76
"def mock_touch(self, bearer, version=None, revision=None, **kwargs):
    if version:
        if self.versions:
            try:
                return self.versions[int(version) - 1]
            except (IndexError, ValueError):
                return None
        else:
            return None
    return file_models.FileVersion()
",if self . versions :,95
"def _get_field_value(self, test, key, match):
    if test.ver == ofproto_v1_0.OFP_VERSION:
        members = inspect.getmembers(match)
        for member in members:
            if member[0] == key:
                field_value = member[1]
            elif member[0] == ""wildcards"":
                wildcards = member[1]
        if key == ""nw_src"":
            field_value = test.nw_src_to_str(wildcards, field_value)
        elif key == ""nw_dst"":
            field_value = test.nw_dst_to_str(wildcards, field_value)
    else:
        field_value = match[key]
    return field_value
","elif member [ 0 ] == ""wildcards"" :",200
"def check_expected(result, expected, contains=False):
    if sys.version_info[0] >= 3:
        if isinstance(result, str):
            result = result.encode(""ascii"")
        if isinstance(expected, str):
            expected = expected.encode(""ascii"")
    resultlines = result.splitlines()
    expectedlines = expected.splitlines()
    if len(resultlines) != len(expectedlines):
        return False
    for rline, eline in zip(resultlines, expectedlines):
        if contains:
            if eline not in rline:
                return False
        else:
            if not rline.endswith(eline):
                return False
    return True
",if contains :,181
"def OnKeyUp(self, event):
    if self._properties.modifiable:
        if event.GetKeyCode() == wx.WXK_ESCAPE:
            self._cancel_editing()
        elif event.GetKeyCode() == wx.WXK_RETURN:
            self._update_value()
        elif event.GetKeyCode() == wx.WXK_DELETE:
            self.SetValue("""")
    if event.GetKeyCode() != wx.WXK_RETURN:
        # Don't send skip event if enter key is pressed
        # On some platforms this event is sent too late and causes crash
        event.Skip()
",elif event . GetKeyCode ( ) == wx . WXK_RETURN :,145
"def load_modules(
    to_load, load, attr, modules_dict, excluded_aliases, loading_message=None
):
    if loading_message:
        print(loading_message)
    for name in to_load:
        module = load(name)
        if module is None or not hasattr(module, attr):
            continue
        cls = getattr(module, attr)
        if hasattr(cls, ""initialize"") and not cls.initialize():
            continue
        if hasattr(module, ""aliases""):
            for alias in module.aliases():
                if alias not in excluded_aliases:
                    modules_dict[alias] = module
        else:
            modules_dict[name] = module
    if loading_message:
        print()
","if module is None or not hasattr ( module , attr ) :",195
"def eventIterator():
    while True:
        yield eventmodule.wait()
        while True:
            event = eventmodule.poll()
            if event.type == NOEVENT:
                break
            else:
                yield event
",if event . type == NOEVENT :,67
"def _get_state_without_padding(self, state_with_padding, padding):
    lean_state = {}
    for key, value in state_with_padding.items():
        if torch.is_tensor(value):
            lean_length = value.numel() - padding
            lean_state[key] = value[:lean_length]
        else:
            lean_state[key] = value
    return lean_state
",if torch . is_tensor ( value ) :,110
"def _get_validate(data):
    """"""Retrieve items to validate, from single samples or from combined joint calls.""""""
    if data.get(""vrn_file"") and tz.get_in([""config"", ""algorithm"", ""validate""], data):
        return utils.deepish_copy(data)
    elif ""group_orig"" in data:
        for sub in multi.get_orig_items(data):
            if ""validate"" in sub[""config""][""algorithm""]:
                sub_val = utils.deepish_copy(sub)
                sub_val[""vrn_file""] = data[""vrn_file""]
                return sub_val
    return None
","if ""validate"" in sub [ ""config"" ] [ ""algorithm"" ] :",163
"def OnPopup(self, form, popup_handle):
    for num, action_name, menu_name, shortcut in self.actions:
        if menu_name is None:
            ida_kernwin.attach_action_to_popup(form, popup_handle, None)
        else:
            handler = command_handler_t(self, num, 2)
            desc = ida_kernwin.action_desc_t(action_name, menu_name, handler, shortcut)
            ida_kernwin.attach_dynamic_action_to_popup(form, popup_handle, desc)
",if menu_name is None :,153
"def show(self, indent=0):
    """"""Pretty print this structure.""""""
    if indent == 0:
        print(""struct {}"".format(self.name))
    for field in self.fields:
        if field.offset is None:
            offset = ""0x??""
        else:
            offset = ""0x{:02x}"".format(field.offset)
        print(""{}+{} {} {}"".format("" "" * indent, offset, field.name, field.type))
        if isinstance(field.type, Structure):
            field.type.show(indent + 1)
",if field . offset is None :,143
"def get_operation_ast(document_ast, operation_name=None):
    operation = None
    for definition in document_ast.definitions:
        if isinstance(definition, ast.OperationDefinition):
            if not operation_name:
                # If no operation name is provided, only return an Operation if it is the only one present in the
                # document. This means that if we've encountered a second operation as we were iterating over the
                # definitions in the document, there are more than one Operation defined, and we should return None.
                if operation:
                    return None
                operation = definition
            elif definition.name and definition.name.value == operation_name:
                return definition
    return operation
","if isinstance ( definition , ast . OperationDefinition ) :",186
"def getSubMenu(self, callingWindow, context, mainItem, selection, rootMenu, i, pitem):
    msw = True if ""wxMSW"" in wx.PlatformInfo else False
    self.context = context
    self.abilityIds = {}
    sub = wx.Menu()
    for ability in self.fighter.abilities:
        if not ability.effect.isImplemented:
            continue
        menuItem = self.addAbility(rootMenu if msw else sub, ability)
        sub.Append(menuItem)
        menuItem.Check(ability.active)
    return sub
",if not ability . effect . isImplemented :,143
"def consume(self, event: Dict[str, Any]) -> None:
    with self.lock:
        logging.debug(""Received missedmessage_emails event: %s"", event)
        # When we process an event, just put it into the queue and ensure we have a timer going.
        user_profile_id = event[""user_profile_id""]
        if user_profile_id not in self.batch_start_by_recipient:
            self.batch_start_by_recipient[user_profile_id] = time.time()
        self.events_by_recipient[user_profile_id].append(event)
        self.ensure_timer()
",if user_profile_id not in self . batch_start_by_recipient :,160
"def __init__(self, start_enabled=False, use_hardware=True):
    self._use_hardware = use_hardware
    if use_hardware:
        self._button = Button(BUTTON_GPIO_PIN)
        self._enabled = start_enabled
        if not start_enabled:
            self._button.when_pressed = self._enable
",if not start_enabled :,87
"def execute(cls, ctx, op: ""DataFrameGroupByAgg""):
    try:
        pd.set_option(""mode.use_inf_as_na"", op.use_inf_as_na)
        if op.stage == OperandStage.map:
            cls._execute_map(ctx, op)
        elif op.stage == OperandStage.combine:
            cls._execute_combine(ctx, op)
        elif op.stage == OperandStage.agg:
            cls._execute_agg(ctx, op)
        else:  # pragma: no cover
            raise ValueError(""Aggregation operand not executable"")
    finally:
        pd.reset_option(""mode.use_inf_as_na"")
",if op . stage == OperandStage . map :,171
"def load_package(name, path):
    if os.path.isdir(path):
        extensions = machinery.SOURCE_SUFFIXES[:] + machinery.BYTECODE_SUFFIXES[:]
        for extension in extensions:
            init_path = os.path.join(path, ""__init__"" + extension)
            if os.path.exists(init_path):
                path = init_path
                break
        else:
            raise ValueError(""{!r} is not a package"".format(path))
    spec = util.spec_from_file_location(name, path, submodule_search_locations=[])
    if name in sys.modules:
        return _exec(spec, sys.modules[name])
    else:
        return _load(spec)
",if os . path . exists ( init_path ) :,187
"def setup(level=None):
    from pipeline.logging import pipeline_logger as logger
    from pipeline.log.handlers import EngineLogHandler
    if level in set(logging._levelToName.values()):
        logger.setLevel(level)
    logging._acquireLock()
    try:
        for hdl in logger.handlers:
            if isinstance(hdl, EngineLogHandler):
                break
        else:
            hdl = EngineLogHandler()
            hdl.setLevel(logger.level)
            logger.addHandler(hdl)
    finally:
        logging._releaseLock()
","if isinstance ( hdl , EngineLogHandler ) :",150
"def find_approximant(x):
    c = 1e-4
    it = sympy.ntheory.continued_fraction_convergents(
        sympy.ntheory.continued_fraction_iterator(x)
    )
    for i in it:
        p, q = i.as_numer_denom()
        tol = c / q ** 2
        if abs(i - x) <= tol:
            return i
        if tol < machine_epsilon:
            break
    return x
",if tol < machine_epsilon :,122
"def resolve(
    self, debug: bool = False, silent: bool = False, level: Optional[int] = None
) -> bool:
    if silent:
        spinner = nullcontext(type(""Mock"", (), {}))
    else:
        spinner = yaspin(text=""resolving..."")
    with spinner as spinner:
        while True:
            resolved = self._resolve(
                debug=debug, silent=silent, level=level, spinner=spinner
            )
            if resolved is None:
                continue
            self.graph.clear()  # remove unused deps from graph
            return resolved
",if resolved is None :,158
"def canonicalize_instruction_name(instr):
    name = instr.insn_name().upper()
    # XXX bypass a capstone bug that incorrectly labels some insns as mov
    if name == ""MOV"":
        if instr.mnemonic.startswith(""lsr""):
            return ""LSR""
        elif instr.mnemonic.startswith(""lsl""):
            return ""LSL""
        elif instr.mnemonic.startswith(""asr""):
            return ""ASR""
    return OP_NAME_MAP.get(name, name)
","if instr . mnemonic . startswith ( ""lsr"" ) :",135
"def run_all(rule_list, defined_variables, defined_actions, stop_on_first_trigger=False):
    rule_was_triggered = False
    for rule in rule_list:
        result = run(rule, defined_variables, defined_actions)
        if result:
            rule_was_triggered = True
            if stop_on_first_trigger:
                return True
    return rule_was_triggered
",if stop_on_first_trigger :,108
"def get_filters(self, request):
    filter_specs = []
    if self.lookup_opts.admin.list_filter and not self.opts.one_to_one_field:
        filter_fields = [
            self.lookup_opts.get_field(field_name)
            for field_name in self.lookup_opts.admin.list_filter
        ]
        for f in filter_fields:
            spec = FilterSpec.create(f, request, self.params, self.model)
            if spec and spec.has_output():
                filter_specs.append(spec)
    return filter_specs, bool(filter_specs)
",if spec and spec . has_output ( ) :,167
"def get_type(type_ref):
    kind = type_ref.get(""kind"")
    if kind == TypeKind.LIST:
        item_ref = type_ref.get(""ofType"")
        if not item_ref:
            raise Exception(""Decorated type deeper than introspection query."")
        return GraphQLList(get_type(item_ref))
    elif kind == TypeKind.NON_NULL:
        nullable_ref = type_ref.get(""ofType"")
        if not nullable_ref:
            raise Exception(""Decorated type deeper than introspection query."")
        return GraphQLNonNull(get_type(nullable_ref))
    return get_named_type(type_ref[""name""])
",if not item_ref :,171
"def _1_0_cloud_ips_cip_jsjc5_map(self, method, url, body, headers):
    if method == ""POST"":
        body = json.loads(body)
        if ""destination"" in body:
            return self.test_response(httplib.ACCEPTED, """")
        else:
            data = '{""error_name"":""bad destination"", ""errors"": [""Bad destination""]}'
            return self.test_response(httplib.BAD_REQUEST, data)
","if ""destination"" in body :",126
"def _get_prefixed_values(data, prefix):
    """"""Collect lines which start with prefix; with trimming""""""
    matches = []
    for line in data.splitlines():
        line = line.strip()
        if line.startswith(prefix):
            match = line[len(prefix) :]
            match = match.strip()
            matches.append(match)
    return matches
",if line . startswith ( prefix ) :,97
"def _power_exact(y, xc, yc, xe):
    yc, ye = y.int, y.exp
    while yc % 10 == 0:
        yc //= 10
        ye += 1
    if xc == 1:
        xe *= yc
        while xe % 10 == 0:
            xe //= 10
            ye += 1
        if ye < 0:
            return None
        exponent = xe * 10 ** ye
        if y and xe:
            xc = exponent
        else:
            xc = 0
        return 5
",if ye < 0 :,144
"def init(self, view, items=None):
    selections = []
    if view.sel():
        for region in view.sel():
            selections.append(view.substr(region))
    values = []
    for idx, index in enumerate(map(int, items)):
        if idx >= len(selections):
            break
        i = index - 1
        if i >= 0 and i < len(selections):
            values.append(selections[i])
        else:
            values.append(None)
    # fill up
    for idx, value in enumerate(selections):
        if len(values) + 1 < idx:
            values.append(value)
    self.stack = values
",if len ( values ) + 1 < idx :,178
"def toggleFactorReload(self, value=None):
    self.serviceFittingOptions[""useGlobalForceReload""] = (
        value
        if value is not None
        else not self.serviceFittingOptions[""useGlobalForceReload""]
    )
    fitIDs = set()
    for fit in set(self._loadedFits):
        if fit is None:
            continue
        if fit.calculated:
            fit.factorReload = self.serviceFittingOptions[""useGlobalForceReload""]
            fit.clearFactorReloadDependentData()
            fitIDs.add(fit.ID)
    return fitIDs
",if fit . calculated :,149
"def init_weights(self):
    """"""Initialize model weights.""""""
    for m in self.predict_layers.modules():
        if isinstance(m, nn.Conv2d):
            kaiming_init(m)
        elif isinstance(m, nn.BatchNorm2d):
            constant_init(m, 1)
        elif isinstance(m, nn.Linear):
            normal_init(m, std=0.01)
","elif isinstance ( m , nn . BatchNorm2d ) :",107
"def _unzip_file(self, filepath, ext):
    try:
        if ext == "".zip"":
            zf = zipfile.ZipFile(filepath)
            zf.extractall(os.path.dirname(filepath))
            zf.close()
        elif ext == "".tar"":
            tf = tarfile.open(filepath)
            tf.extractall(os.path.dirname(filepath))
            tf.close()
    except Exception as e:
        raise ValueError(""Error reading file %r!\n%s"" % (filepath, e))
","if ext == "".zip"" :",136
"def add_multiple_tasks(data, parent):
    data = json.loads(data)
    new_doc = {
        ""doctype"": ""Task"",
        ""parent_task"": parent if parent != ""All Tasks"" else """",
    }
    new_doc[""project""] = frappe.db.get_value(""Task"", {""name"": parent}, ""project"") or """"
    for d in data:
        if not d.get(""subject""):
            continue
        new_doc[""subject""] = d.get(""subject"")
        new_task = frappe.get_doc(new_doc)
        new_task.insert()
","if not d . get ( ""subject"" ) :",158
"def filterSimilarKeywords(keyword, kwdsIterator):
    """"""Return a sorted list of keywords similar to the one given.""""""
    seenDict = {}
    kwdSndx = soundex(keyword.encode(""ascii"", ""ignore""))
    matches = []
    matchesappend = matches.append
    checkContained = False
    if len(keyword) > 4:
        checkContained = True
    for movieID, key in kwdsIterator:
        if key in seenDict:
            continue
        seenDict[key] = None
        if checkContained and keyword in key:
            matchesappend(key)
            continue
        if kwdSndx == soundex(key.encode(""ascii"", ""ignore"")):
            matchesappend(key)
    return _sortKeywords(keyword, matches)
",if key in seenDict :,193
"def visit_If(self, node):
    self.newline()
    self.write(""if "")
    self.visit(node.test)
    self.write("":"")
    self.body(node.body)
    while True:
        else_ = node.orelse
        if len(else_) == 1 and isinstance(else_[0], If):
            node = else_[0]
            self.newline()
            self.write(""elif "")
            self.visit(node.test)
            self.write("":"")
            self.body(node.body)
        else:
            self.newline()
            self.write(""else:"")
            self.body(else_)
            break
","if len ( else_ ) == 1 and isinstance ( else_ [ 0 ] , If ) :",181
"def _eyeLinkHardwareAndSoftwareVersion(self):
    try:
        tracker_software_ver = 0
        eyelink_ver = self._eyelink.getTrackerVersion()
        if eyelink_ver == 3:
            tvstr = self._eyelink.getTrackerVersionString()
            vindex = tvstr.find(""EYELINK CL"")
            tracker_software_ver = int(
                float(tvstr[(vindex + len(""EYELINK CL"")) :].strip())
            )
        return eyelink_ver, tracker_software_ver
    except Exception:
        print2err(""EYELINK Error during _eyeLinkHardwareAndSoftwareVersion:"")
        printExceptionDetailsToStdErr()
        return EyeTrackerConstants.EYETRACKER_ERROR
",if eyelink_ver == 3 :,200
"def execute(self, context):
    for monad in context.blend_data.node_groups:
        if monad.bl_idname == ""SverchGroupTreeType"":
            if not getattr(bpy.types, monad.cls_bl_idname, None):
                try:
                    monad.update_cls()
                except Exception as err:
                    print(err)
                    print(""{} group class could not be created"".format(monad.name))
    return {""FINISHED""}
","if not getattr ( bpy . types , monad . cls_bl_idname , None ) :",137
"def word_pattern(pattern, str):
    dict = {}
    set_value = set()
    list_str = str.split()
    if len(list_str) != len(pattern):
        return False
    for i in range(len(pattern)):
        if pattern[i] not in dict:
            if list_str[i] in set_value:
                return False
            dict[pattern[i]] = list_str[i]
            set_value.add(list_str[i])
        else:
            if dict[pattern[i]] != list_str[i]:
                return False
    return True
",if dict [ pattern [ i ] ] != list_str [ i ] :,165
"def decorator_handle(tokens):
    """"""Process decorators.""""""
    defs = []
    decorates = []
    for i, tok in enumerate(tokens):
        if ""simple"" in tok and len(tok) == 1:
            decorates.append(""@"" + tok[0])
        elif ""test"" in tok and len(tok) == 1:
            varname = decorator_var + ""_"" + str(i)
            defs.append(varname + "" = "" + tok[0])
            decorates.append(""@"" + varname)
        else:
            raise CoconutInternalException(""invalid decorator tokens"", tok)
    return ""\n"".join(defs + decorates) + ""\n""
","elif ""test"" in tok and len ( tok ) == 1 :",171
"def wait_impl(self, cpid):
    for i in range(10):
        # wait3() shouldn't hang, but some of the buildbots seem to hang
        # in the forking tests.  This is an attempt to fix the problem.
        spid, status, rusage = os.wait3(os.WNOHANG)
        if spid == cpid:
            break
        time.sleep(1.0)
    self.assertEqual(spid, cpid)
    self.assertEqual(status, 0, ""cause = %d, exit = %d"" % (status & 0xFF, status >> 8))
    self.assertTrue(rusage)
",if spid == cpid :,163
"def test_non_uniform_probabilities_over_elements(self):
    param = iap.Choice([0, 1], p=[0.25, 0.75])
    samples = param.draw_samples((10000,))
    unique, counts = np.unique(samples, return_counts=True)
    assert len(unique) == 2
    for val, count in zip(unique, counts):
        if val == 0:
            assert 2500 - 500 < count < 2500 + 500
        elif val == 1:
            assert 7500 - 500 < count < 7500 + 500
        else:
            assert False
",if val == 0 :,145
"def dispatch_return(self, frame, arg):
    if self.stop_here(frame) or frame == self.returnframe:
        # Ignore return events in generator except when stepping.
        if self.stopframe and frame.f_code.co_flags & CO_GENERATOR:
            return self.trace_dispatch
        try:
            self.frame_returning = frame
            self.user_return(frame, arg)
        finally:
            self.frame_returning = None
        if self.quitting:
            raise BdbQuit
        # The user issued a 'next' or 'until' command.
        if self.stopframe is frame and self.stoplineno != -1:
            self._set_stopinfo(None, None)
    return self.trace_dispatch
",if self . quitting :,199
"def mouse(self, button, mods, x, y):
    if button == 1:
        for i in range(4):
            if hypot(x - self.coords[i][0], y - self.coords[i][1]) < 4:
                self.hit = i
    elif button == -1:
        self.hit = None
    elif self.hit != None:
        self.coords[self.hit] = (x, y)
        self.view.dirty()
","if hypot ( x - self . coords [ i ] [ 0 ] , y - self . coords [ i ] [ 1 ] ) < 4 :",123
"def __init__(self, *commands):
    self.all_cmds = list(
        map(lambda cmd: cmd[0] if isinstance(cmd, list) else cmd, commands)
    )
    for command in commands:
        self.cmd = command if isinstance(command, list) else [command]
        self.cmd_path = pwndbg.which.which(self.cmd[0])
        if self.cmd_path:
            break
",if self . cmd_path :,111
"def _recv_obj(self, suppress_error=False):
    """"""Receive a (picklable) object""""""
    if self.conn.closed:
        raise OSError(""handle is closed"")
    try:
        buf = self.conn.recv_bytes()
    except (ConnectionError, EOFError) as e:
        if suppress_error:
            return
        logger.debug(""receive has failed"", exc_info=e)
        try:
            self._set_remote_close_cause(e)
            raise PipeShutdownError()
        finally:
            self._close()
    obj = RemoteObjectUnpickler.loads(buf, self)
    logger.debug(""received %r"", obj)
    return obj
",if suppress_error :,177
"def act(self, obs):
    with chainer.no_backprop_mode():
        batch_obs = self.batch_states([obs], self.xp, self.phi)
        action_distrib = self.model(batch_obs)
        if self.act_deterministically:
            return chainer.cuda.to_cpu(action_distrib.most_probable.array)[0]
        else:
            return chainer.cuda.to_cpu(action_distrib.sample().array)[0]
",if self . act_deterministically :,126
"def _classify(nodes_by_level):
    missing, invalid, downloads = [], [], []
    for level in nodes_by_level:
        for node in level:
            if node.binary == BINARY_MISSING:
                missing.append(node)
            elif node.binary == BINARY_INVALID:
                invalid.append(node)
            elif node.binary in (BINARY_UPDATE, BINARY_DOWNLOAD):
                downloads.append(node)
    return missing, invalid, downloads
","elif node . binary in ( BINARY_UPDATE , BINARY_DOWNLOAD ) :",126
"def persist(self, *_):
    for key, obj in self._objects.items():
        try:
            state = obj.get_state()
            if not state:
                continue
            md5 = hashlib.md5(state).hexdigest()
            if self._last_state.get(key) == md5:
                continue
            self._persist_provider.store(key, state)
        except Exception as e:
            system_log.exception(""PersistHelper.persist fail"")
        else:
            self._last_state[key] = md5
",if not state :,153
"def enter(self, doc, **kwds):
    """"""Enters the mode, arranging for necessary grabs ASAP""""""
    super(ColorPickMode, self).enter(doc, **kwds)
    if self._started_from_key_press:
        # Pick now using the last recorded event position
        doc = self.doc
        tdw = self.doc.tdw
        t, x, y = doc.get_last_event_info(tdw)
        if None not in (x, y):
            self._pick_color_mode(tdw, x, y, self._pickmode)
        # Start the drag when possible
        self._start_drag_on_next_motion_event = True
        self._needs_drag_start = True
","if None not in ( x , y ) :",187
"def on_profiles_loaded(self, profiles):
    cb = self.builder.get_object(""cbProfile"")
    model = cb.get_model()
    model.clear()
    for f in profiles:
        name = f.get_basename()
        if name.endswith("".mod""):
            continue
        if name.endswith("".sccprofile""):
            name = name[0:-11]
        model.append((name, f, None))
    cb.set_active(0)
","if name . endswith ( "".mod"" ) :",122
"def subprocess_post_check(
    completed_process: subprocess.CompletedProcess, raise_error: bool = True
) -> None:
    if completed_process.returncode:
        if completed_process.stdout is not None:
            print(completed_process.stdout, file=sys.stdout, end="""")
        if completed_process.stderr is not None:
            print(completed_process.stderr, file=sys.stderr, end="""")
        if raise_error:
            raise PipxError(
                f""{' '.join([str(x) for x in completed_process.args])!r} failed""
            )
        else:
            logger.info(f""{' '.join(completed_process.args)!r} failed"")
",if completed_process . stderr is not None :,185
"def test_connect(
    ipaddr, port, device, partition, method, path, headers=None, query_string=None
):
    if path == ""/a"":
        for k, v in headers.iteritems():
            if k.lower() == test_header.lower() and v == test_value:
                break
        else:
            test_errors.append(""%s: %s not in %s"" % (test_header, test_value, headers))
",if k . lower ( ) == test_header . lower ( ) and v == test_value :,116
"def test_stat_result_pickle(self):
    result = os.stat(self.fname)
    for proto in range(pickle.HIGHEST_PROTOCOL + 1):
        p = pickle.dumps(result, proto)
        self.assertIn(b""stat_result"", p)
        if proto < 4:
            self.assertIn(b""cos\nstat_result\n"", p)
        unpickled = pickle.loads(p)
        self.assertEqual(result, unpickled)
",if proto < 4 :,118
"def run_sql(sql):
    table = sql.split("" "")[5]
    logger.info(""Updating table {}"".format(table))
    with transaction.atomic():
        with connection.cursor() as cursor:
            cursor.execute(sql)
            rows = cursor.fetchall()
            if not rows:
                raise Exception(""Sentry notification that {} is migrated"".format(table))
",if not rows :,98
"def countbox(self):
    self.box = [1000, 1000, -1000, -1000]
    for x, y in self.body:
        if x < self.box[0]:
            self.box[0] = x
        if x > self.box[2]:
            self.box[2] = x
        if y < self.box[1]:
            self.box[1] = y
        if y > self.box[3]:
            self.box[3] = y
",if x > self . box [ 2 ] :,131
"def _packageFocusOutViaKeyPress(self, row, column, txt):
    if txt:
        self._set_current_cell(row + 1, column)
    else:
        widget = self.cellWidget(row + 1, column)
        if widget and isinstance(widget, PackageSelectWidget):
            self._delete_cell(row, column)
        new_request = self.get_request()
        self.context_model.set_request(new_request)
        self._update_request_column(column, self.context_model)
","if widget and isinstance ( widget , PackageSelectWidget ) :",140
"def parse_bash_set_output(output):
    """"""Parse Bash-like 'set' output""""""
    if not sys.platform.startswith(""win""):
        # Replace ""\""-continued lines in *Linux* environment dumps.
        # Cannot do this on Windows because a ""\"" at the end of the
        # line does not imply a continuation.
        output = output.replace(""\\\n"", """")
    environ = {}
    for line in output.splitlines(0):
        line = line.rstrip()
        if not line:
            continue  # skip black lines
        item = _ParseBashEnvStr(line)
        if item:
            environ[item[0]] = item[1]
    return environ
",if not line :,177
"def _get(self, domain):
    with self.lock:
        try:
            record = self.cache[domain]
            time_now = time.time()
            if time_now - record[""update""] > self.ttl:
                record = None
        except KeyError:
            record = None
        if not record:
            record = {""r"": ""unknown"", ""dns"": {}, ""g"": 1, ""query_count"": 0}
        # self.cache[domain] = record
        return record
","if time_now - record [ ""update"" ] > self . ttl :",137
"def test_filehash(self):
    """"""tests the hashes of the files in data/""""""
    fp = self.get_data_path()
    for fn in os.listdir(fp):
        if ""."" in fn:
            # file used for something else
            continue
        expected_hash = fn
        fullp = os.path.join(fp, fn)
        output = self.run_command(""sha1sum "" + fullp, exitcode=0)
        result = output.split("" "")[0]
        self.assertEqual(result, expected_hash)
","if ""."" in fn :",139
"def test_new_vs_reference_code_stream_read_during_iter(read_idx, read_len, bytecode):
    reference = SlowCodeStream(bytecode)
    latest = CodeStream(bytecode)
    for index, (actual, expected) in enumerate(zip(latest, reference)):
        assert actual == expected
        if index == read_idx:
            readout_actual = latest.read(read_len)
            readout_expected = reference.read(read_len)
            assert readout_expected == readout_actual
        if reference.program_counter >= len(reference):
            assert latest.program_counter >= len(reference)
        else:
            assert latest.program_counter == reference.program_counter
",if reference . program_counter >= len ( reference ) :,179
"def setup_logging():
    try:
        logconfig = config.get(""logging_config_file"")
        if logconfig and os.path.exists(logconfig):
            logging.config.fileConfig(logconfig, disable_existing_loggers=False)
        logger.info(""logging initialized"")
        logger.debug(""debug"")
    except Exception as e:
        print(""Unable to set logging configuration:"", str(e), file=sys.stderr)
        raise
",if logconfig and os . path . exists ( logconfig ) :,116
"def all_words(filename):
    start_char = True
    for c in characters(filename):
        if start_char == True:
            word = """"
            if c.isalnum():
                # We found the start of a word
                word = c.lower()
                start_char = False
            else:
                pass
        else:
            if c.isalnum():
                word += c.lower()
            else:
                # We found end of word, emit it
                start_char = True
                yield word
",if start_char == True :,158
"def _get_nonce(self, url, new_nonce_url):
    if not self._nonces:
        logger.debug(""Requesting fresh nonce"")
        if new_nonce_url is None:
            response = self.head(url)
        else:
            # request a new nonce from the acme newNonce endpoint
            response = self._check_response(self.head(new_nonce_url), content_type=None)
        self._add_nonce(response)
    return self._nonces.pop()
",if new_nonce_url is None :,131
"def paragraph_is_fully_commented(lines, comment, main_language):
    """"""Is the paragraph fully commented?""""""
    for i, line in enumerate(lines):
        if line.startswith(comment):
            if line[len(comment) :].lstrip().startswith(comment):
                continue
            if is_magic(line, main_language):
                return False
            continue
        return i > 0 and _BLANK_LINE.match(line)
    return True
",if line [ len ( comment ) : ] . lstrip ( ) . startswith ( comment ) :,121
"def gvariant_args(args: List[Any]) -> str:
    """"""Convert args into gvariant.""""""
    gvariant = """"
    for arg in args:
        if isinstance(arg, bool):
            gvariant += "" {}"".format(str(arg).lower())
        elif isinstance(arg, (int, float)):
            gvariant += f"" {arg}""
        elif isinstance(arg, str):
            gvariant += f' ""{arg}""'
        else:
            gvariant += f"" {arg!s}""
    return gvariant.lstrip()
","elif isinstance ( arg , ( int , float ) ) :",139
"def _SkipGroup(buffer, pos, end):
    """"""Skip sub-group.  Returns the new position.""""""
    while 1:
        (tag_bytes, pos) = ReadTag(buffer, pos)
        new_pos = SkipField(buffer, pos, end, tag_bytes)
        if new_pos == -1:
            return pos
        pos = new_pos
",if new_pos == - 1 :,93
"def update_participants(self, refresh=True):
    for participant in list(self.participants_dict):
        if participant is None or participant == self.simulator_config.broadcast_part:
            continue
        self.removeItem(self.participants_dict[participant])
        self.participant_items.remove(self.participants_dict[participant])
        del self.participants_dict[participant]
    for participant in self.simulator_config.participants:
        if participant in self.participants_dict:
            self.participants_dict[participant].refresh()
        else:
            self.insert_participant(participant)
    if refresh:
        self.update_view()
",if participant in self . participants_dict :,182
"def feature_reddit(layer_data, graph):
    feature = {}
    times = {}
    indxs = {}
    for _type in layer_data:
        if len(layer_data[_type]) == 0:
            continue
        idxs = np.array(list(layer_data[_type].keys()))
        tims = np.array(list(layer_data[_type].values()))[:, 1]
        feature[_type] = np.array(
            list(graph.node_feature[_type].loc[idxs, ""emb""]), dtype=np.float
        )
        times[_type] = tims
        indxs[_type] = idxs
        if _type == ""def"":
            attr = feature[_type]
    return feature, times, indxs, attr
","if _type == ""def"" :",195
"def _get_sort_map(tags):
    """"""See TAG_TO_SORT""""""
    tts = {}
    for name, tag in tags.items():
        if tag.has_sort:
            if tag.user:
                tts[name] = ""%ssort"" % name
            if tag.internal:
                tts[""~%s"" % name] = ""~%ssort"" % name
    return tts
",if tag . user :,111
"def max_radius(iterator):
    radius_result = dict()
    for k, v in iterator:
        if v[0] not in radius_result:
            radius_result[v[0]] = v[1]
        elif v[1] >= radius_result[v[0]]:
            radius_result[v[0]] = v[1]
    return radius_result
",elif v [ 1 ] >= radius_result [ v [ 0 ] ] :,96
"def run(self):
    pwd_found = []
    if constant.user_dpapi and constant.user_dpapi.unlocked:
        main_vault_directory = os.path.join(
            constant.profile[""APPDATA""], u"".."", u""Local"", u""Microsoft"", u""Vault""
        )
        if os.path.exists(main_vault_directory):
            for vault_directory in os.listdir(main_vault_directory):
                cred = constant.user_dpapi.decrypt_vault(
                    os.path.join(main_vault_directory, vault_directory)
                )
                if cred:
                    pwd_found.append(cred)
    return pwd_found
",if cred :,197
"def disconnect_sync(self, connection, close_connection=False):
    key = id(connection)
    ts = self.in_use.pop(key)
    if close_connection:
        self.connections_map.pop(key)
        self._connection_close_sync(connection)
    else:
        if self.stale_timeout and self.is_stale(ts):
            self.connections_map.pop(key)
            self._connection_close_sync(connection)
        else:
            with self._lock_sync:
                heapq.heappush(self.connections_sync, (ts, key))
",if self . stale_timeout and self . is_stale ( ts ) :,159
"def _populate_tree(self, element, d):
    """"""Populates an etree with attributes & elements, given a dict.""""""
    for k, v in d.iteritems():
        if isinstance(v, dict):
            self._populate_dict(element, k, v)
        elif isinstance(v, list):
            self._populate_list(element, k, v)
        elif isinstance(v, bool):
            self._populate_bool(element, k, v)
        elif isinstance(v, basestring):
            self._populate_str(element, k, v)
        elif type(v) in [int, float, long, complex]:
            self._populate_number(element, k, v)
","elif type ( v ) in [ int , float , long , complex ] :",178
"def readframes(self, nframes):
    if self._ssnd_seek_needed:
        self._ssnd_chunk.seek(0)
        dummy = self._ssnd_chunk.read(8)
        pos = self._soundpos * self._framesize
        if pos:
            self._ssnd_chunk.seek(pos + 8)
        self._ssnd_seek_needed = 0
    if nframes == 0:
        return """"
    data = self._ssnd_chunk.read(nframes * self._framesize)
    if self._convert and data:
        data = self._convert(data)
    self._soundpos = self._soundpos + len(data) / (self._nchannels * self._sampwidth)
    return data
",if pos :,185
"def target_glob(tgt, hosts):
    ret = {}
    for host in hosts:
        if fnmatch.fnmatch(tgt, host):
            ret[host] = copy.deepcopy(__opts__.get(""roster_defaults"", {}))
            ret[host].update({""host"": host})
            if __opts__.get(""ssh_user""):
                ret[host].update({""user"": __opts__[""ssh_user""]})
    return ret
","if fnmatch . fnmatch ( tgt , host ) :",110
"def get_attribute_value(self, nodeid, attr):
    with self._lock:
        self.logger.debug(""get attr val: %s %s"", nodeid, attr)
        if nodeid not in self._nodes:
            dv = ua.DataValue()
            dv.StatusCode = ua.StatusCode(ua.StatusCodes.BadNodeIdUnknown)
            return dv
        node = self._nodes[nodeid]
        if attr not in node.attributes:
            dv = ua.DataValue()
            dv.StatusCode = ua.StatusCode(ua.StatusCodes.BadAttributeIdInvalid)
            return dv
        attval = node.attributes[attr]
        if attval.value_callback:
            return attval.value_callback()
        return attval.value
",if attval . value_callback :,200
"def remove_property(self, key):  # type: (str) -> None
    with self.secure() as config:
        keys = key.split(""."")
        current_config = config
        for i, key in enumerate(keys):
            if key not in current_config:
                return
            if i == len(keys) - 1:
                del current_config[key]
                break
            current_config = current_config[key]
",if key not in current_config :,122
"def _class_browser(parent):  # Wrapper for htest
    try:
        file = __file__
    except NameError:
        file = sys.argv[0]
        if sys.argv[1:]:
            file = sys.argv[1]
        else:
            file = sys.argv[0]
    dir, file = os.path.split(file)
    name = os.path.splitext(file)[0]
    flist = PyShell.PyShellFileList(parent)
    global file_open
    file_open = flist.open
    ClassBrowser(flist, name, [dir], _htest=True)
",if sys . argv [ 1 : ] :,161
"def get_only_text_part(self, msg):
    count = 0
    only_text_part = None
    for part in msg.walk():
        if part.is_multipart():
            continue
        count += 1
        mimetype = part.get_content_type() or ""text/plain""
        if mimetype != ""text/plain"" or count != 1:
            return False
        else:
            only_text_part = part
    return only_text_part
","if mimetype != ""text/plain"" or count != 1 :",123
"def should_keep_alive(commit_msg):
    result = False
    ci = get_current_ci() or """"
    for line in commit_msg.splitlines():
        parts = line.strip(""# "").split("":"", 1)
        (key, val) = parts if len(parts) > 1 else (parts[0], """")
        if key == ""CI_KEEP_ALIVE"":
            ci_names = val.replace("","", "" "").lower().split() if val else []
            if len(ci_names) == 0 or ci.lower() in ci_names:
                result = True
    return result
",if len ( ci_names ) == 0 or ci . lower ( ) in ci_names :,150
"def _calc_block_io(self, blkio):
    """"""Calculate block IO stats.""""""
    for stats in blkio[""io_service_bytes_recursive""]:
        if stats[""op""] == ""Read"":
            self._blk_read += stats[""value""]
        elif stats[""op""] == ""Write"":
            self._blk_write += stats[""value""]
","elif stats [ ""op"" ] == ""Write"" :",92
"def value_to_db_datetime(self, value):
    if value is None:
        return None
    # Oracle doesn't support tz-aware datetimes
    if timezone.is_aware(value):
        if settings.USE_TZ:
            value = value.astimezone(timezone.utc).replace(tzinfo=None)
        else:
            raise ValueError(
                ""Oracle backend does not support timezone-aware datetimes when USE_TZ is False.""
            )
    return six.text_type(value)
",if settings . USE_TZ :,134
"def load_state_dict(self, state_dict):
    for module_name, module_state_dict in state_dict.items():
        if module_name in self.module_pool:
            if self.config[""dataparallel""]:
                self.module_pool[module_name].module.load_state_dict(module_state_dict)
            else:
                self.module_pool[module_name].load_state_dict(module_state_dict)
        else:
            logging.info(f""Missing {module_name} in module_pool, skip it.."")
","if self . config [ ""dataparallel"" ] :",150
"def _unpack_scales(scales, vidxs):
    scaleData = [None, None, None]
    for i in range(3):
        if i >= min(len(scales), len(vidxs) // 2):
            break
        scale = scales[i]
        if not math.isnan(scale):
            vidx1, vidx2 = vidxs[i * 2], vidxs[i * 2 + 1]
            scaleData[i] = (int(vidx1), int(vidx2), float(scale))
    return scaleData
",if not math . isnan ( scale ) :,138
"def __init__(self, factors, contrast_matrices, num_columns):
    self.factors = tuple(factors)
    factor_set = frozenset(factors)
    if not isinstance(contrast_matrices, dict):
        raise ValueError(""contrast_matrices must be dict"")
    for factor, contrast_matrix in six.iteritems(contrast_matrices):
        if factor not in factor_set:
            raise ValueError(""Unexpected factor in contrast_matrices dict"")
        if not isinstance(contrast_matrix, ContrastMatrix):
            raise ValueError(""Expected a ContrastMatrix, not %r"" % (contrast_matrix,))
    self.contrast_matrices = contrast_matrices
    if not isinstance(num_columns, six.integer_types):
        raise ValueError(""num_columns must be an integer"")
    self.num_columns = num_columns
",if factor not in factor_set :,193
"def app(scope, receive, send):
    while True:
        message = await receive()
        if message[""type""] == ""websocket.connect"":
            await send({""type"": ""websocket.accept""})
        elif message[""type""] == ""websocket.receive"":
            pass
        elif message[""type""] == ""websocket.disconnect"":
            break
","if message [ ""type"" ] == ""websocket.connect"" :",93
"def value__set(self, value):
    for i, (option, checked) in enumerate(self.options):
        if option == str(value):
            self.selectedIndex = i
            break
    else:
        raise ValueError(
            ""Option %r not found (from %s)""
            % (value, "", "".join([repr(o) for o, c in self.options]))
        )
",if option == str ( value ) :,106
"def init_links(self):
    links = LinkCallback.find_links(self)
    callbacks = []
    for link, src_plot, tgt_plot in links:
        cb = Link._callbacks[""bokeh""][type(link)]
        if src_plot is None or (link._requires_target and tgt_plot is None):
            continue
        callbacks.append(cb(self.root, link, src_plot, tgt_plot))
    return callbacks
",if src_plot is None or ( link . _requires_target and tgt_plot is None ) :,112
"def _validate_scalar_extensions(self) -> List[str]:
    errors = []
    for extension in [
        x for x in self.extensions if isinstance(x, GraphQLScalarTypeExtension)
    ]:
        extended = self.type_definitions.get(extension.name)
        ext_errors = _validate_extension(
            extended, extension.name, GraphQLScalarType, ""SCALAR""
        )
        errors.extend(ext_errors)
        if not ext_errors:
            errors.extend(_validate_extension_directives(extension, extended, ""SCALAR""))
    return errors
",if not ext_errors :,149
"def copy_tcltk(src, dest, symlink):
    """"""copy tcl/tk libraries on Windows (issue #93)""""""
    for libversion in ""8.5"", ""8.6"":
        for libname in ""tcl"", ""tk"":
            srcdir = join(src, ""tcl"", libname + libversion)
            destdir = join(dest, ""tcl"", libname + libversion)
            # Only copy the dirs from the above combinations that exist
            if os.path.exists(srcdir) and not os.path.exists(destdir):
                copyfileordir(srcdir, destdir, symlink)
",if os . path . exists ( srcdir ) and not os . path . exists ( destdir ) :,153
"def parse(self, response):
    try:
        content = response.content.decode(""utf-8"", ""ignore"")
        content = json.loads(content, strict=False)
    except:
        self.logger.error(""Fail to parse the response in json format"")
        return
    for item in content[""data""]:
        if ""objURL"" in item:
            img_url = self._decode_url(item[""objURL""])
        elif ""hoverURL"" in item:
            img_url = item[""hoverURL""]
        else:
            continue
        yield dict(file_url=img_url)
","elif ""hoverURL"" in item :",158
"def check_and_reload(self):
    # Check if tables have been modified, if so reload
    for table_name, table_version in self._table_versions.items():
        table = self.app.tool_data_tables.get(table_name, None)
        if table is not None and not table.is_current_version(table_version):
            return self.reload_genomes()
",if table is not None and not table . is_current_version ( table_version ) :,100
"def _get_query_defaults(self, query_defns):
    defaults = {}
    for k, v in query_defns.items():
        try:
            if v[""schema""][""type""] == ""object"":
                defaults[k] = self._get_default_obj(v[""schema""])
            else:
                defaults[k] = v[""schema""][""default""]
        except KeyError:
            pass
    return defaults
","if v [ ""schema"" ] [ ""type"" ] == ""object"" :",114
"def ftp_login(host, port, username=None, password=None, anonymous=False):
    ret = False
    try:
        ftp = ftplib.FTP()
        ftp.connect(host, port, timeout=6)
        if anonymous:
            ftp.login()
        else:
            ftp.login(username, password)
        ret = True
        ftp.quit()
    except Exception:
        pass
    return ret
",if anonymous :,116
"def _getVolumeScalar(self):
    if self._volumeScalar is not None:
        return self._volumeScalar
    # use default
    elif self._value in dynamicStrToScalar:
        return dynamicStrToScalar[self._value]
    else:
        thisDynamic = self._value
        # ignore leading s like in sf
        if ""s"" in thisDynamic:
            thisDynamic = thisDynamic[1:]
        # ignore closing z like in fz
        if thisDynamic[-1] == ""z"":
            thisDynamic = thisDynamic[:-1]
        if thisDynamic in dynamicStrToScalar:
            return dynamicStrToScalar[thisDynamic]
        else:
            return dynamicStrToScalar[None]
",if thisDynamic in dynamicStrToScalar :,183
"def processCoords(coords):
    newcoords = deque()
    for (x, y, z) in coords:
        for _dir, offsets in faceDirections:
            if _dir == FaceYIncreasing:
                continue
            dx, dy, dz = offsets
            p = (x + dx, y + dy, z + dz)
            if p not in box:
                continue
            nx, ny, nz = p
            if level.blockAt(nx, ny, nz) == 0:
                level.setBlockAt(nx, ny, nz, waterID)
                newcoords.append(p)
    return newcoords
",if p not in box :,173
"def _set_property(self, target_widget, pname, value):
    if pname == ""text"":
        wstate = str(target_widget[""state""])
        if wstate != ""normal"":
            # change state temporarily
            target_widget[""state""] = ""normal""
        target_widget.delete(""0"", tk.END)
        target_widget.insert(""0"", value)
        target_widget[""state""] = wstate
    else:
        super(EntryBaseBO, self)._set_property(target_widget, pname, value)
","if wstate != ""normal"" :",138
"def teardown():
    try:
        time.sleep(1)
    except KeyboardInterrupt:
        return
    while launchers:
        p = launchers.pop()
        if p.poll() is None:
            try:
                p.stop()
            except Exception as e:
                print(e)
                pass
        if p.poll() is None:
            try:
                time.sleep(0.25)
            except KeyboardInterrupt:
                return
        if p.poll() is None:
            try:
                print(""cleaning up test process..."")
                p.signal(SIGKILL)
            except:
                print(""couldn't shutdown process: "", p)
",if p . poll ( ) is None :,198
"def checkAndRemoveDuplicate(self, node):
    for bucket in self.buckets:
        for n in bucket.getNodes():
            if (n.ip, n.port) == (node.ip, node.port) and n.id != node.id:
                self.removeContact(n)
","if ( n . ip , n . port ) == ( node . ip , node . port ) and n . id != node . id :",77
"def toString():
    flags = u""""
    try:
        if this.glob:
            flags += u""g""
        if this.ignore_case:
            flags += u""i""
        if this.multiline:
            flags += u""m""
    except:
        pass
    v = this.value if this.value else ""(?:)""
    return u""/%s/"" % v + flags
",if this . ignore_case :,106
"def import_submodules(package_name):
    package = sys.modules[package_name]
    results = {}
    for loader, name, is_pkg in pkgutil.iter_modules(package.__path__):
        full_name = package_name + ""."" + name
        module = importlib.import_module(full_name)
        setattr(sys.modules[__name__], name, module)
        results[full_name] = module
        if is_pkg:
            valid_pkg = import_submodules(full_name)
            if valid_pkg:
                results.update(valid_pkg)
    return results
",if valid_pkg :,153
"def _call(self, cmd):
    what = cmd[""command""]
    if what == ""list"":
        name = cmd[""properties""].get(""name"")
        if name is None:
            return {""watchers"": [""one"", ""two"", ""three""]}
        return {""pids"": [123, 456]}
    elif what == ""dstats"":
        return {""info"": {""pid"": 789}}
    elif what == ""listsockets"":
        return {
            ""status"": ""ok"",
            ""sockets"": [{""path"": self._unix, ""fd"": 5, ""name"": ""XXXX"", ""backlog"": 2048}],
            ""time"": 1369647058.967524,
        }
    raise NotImplementedError(cmd)
",if name is None :,182
"def select(self):
    e = xlib.XEvent()
    while xlib.XPending(self._display):
        xlib.XNextEvent(self._display, e)
        # Key events are filtered by the xlib window event
        # handler so they get a shot at the prefiltered event.
        if e.xany.type not in (xlib.KeyPress, xlib.KeyRelease):
            if xlib.XFilterEvent(e, e.xany.window):
                continue
        try:
            dispatch = self._window_map[e.xany.window]
        except KeyError:
            continue
        dispatch(e)
","if xlib . XFilterEvent ( e , e . xany . window ) :",171
"def translate(self, line):
    parsed = self.RE_LINE_PARSER.match(line)
    if parsed:
        value = parsed.group(3)
        stage = parsed.group(1)
        if stage == ""send"":  # query string is rendered here
            return ""\n# HTTP Request:\n"" + self.stripslashes(value)
        elif stage == ""reply"":
            return ""\n\n# HTTP Response:\n"" + self.stripslashes(value)
        elif stage == ""header"":
            return value + ""\n""
        else:
            return value
    return line
","if stage == ""send"" :",156
"def toString():
    flags = u""""
    try:
        if this.glob:
            flags += u""g""
        if this.ignore_case:
            flags += u""i""
        if this.multiline:
            flags += u""m""
    except:
        pass
    v = this.value if this.value else ""(?:)""
    return u""/%s/"" % v + flags
",if this . glob :,106
"def __exit__(self, *exc_info):
    super(WarningsChecker, self).__exit__(*exc_info)
    # only check if we're not currently handling an exception
    if all(a is None for a in exc_info):
        if self.expected_warning is not None:
            if not any(r.category in self.expected_warning for r in self):
                __tracebackhide__ = True
                pytest.fail(""DID NOT WARN"")
",if not any ( r . category in self . expected_warning for r in self ) :,115
"def run(self):
    for k, v in iteritems(self.objs):
        if k.startswith(""_""):
            continue
        if v[""_class""] == ""User"":
            if v[""email""] == """":
                v[""email""] = None
            if v[""ip""] == ""0.0.0.0"":
                v[""ip""] = None
    return self.objs
","if v [ ""_class"" ] == ""User"" :",102
"def list_stuff(self, upto=10, start_after=-1):
    for i in range(upto):
        if i <= start_after:
            continue
        if i == 2 and self.count < 1:
            self.count += 1
            raise TemporaryProblem
        if i == 7 and self.count < 4:
            self.count += 1
            raise TemporaryProblem
        yield i
",if i == 2 and self . count < 1 :,110
"def check(self):
    tcp_client = self.tcp_create()
    if tcp_client.connect():
        tcp_client.send(b""ABCDE"")
        response = tcp_client.recv(5)
        tcp_client.close()
        if response:
            if response.startswith(b""MMcS""):
                self.endianness = "">""  # BE
            elif response.startswith(b""ScMM""):
                self.endianness = ""<""  # LE
            return True  # target is vulnerable
    return False  # target is not vulnerable
","if response . startswith ( b""MMcS"" ) :",148
"def copy_tree(self, src_dir, dst_dir, skip_variables=False):
    for src_root, _, files in os.walk(src_dir):
        if src_root != src_dir:
            rel_root = os.path.relpath(src_root, src_dir)
        else:
            rel_root = """"
        if skip_variables and rel_root.startswith(""variables""):
            continue
        dst_root = os.path.join(dst_dir, rel_root)
        if not os.path.exists(dst_root):
            os.makedirs(dst_root)
        for f in files:
            shutil.copy(os.path.join(src_root, f), os.path.join(dst_root, f))
",if not os . path . exists ( dst_root ) :,197
"def _set_hostport(self, host, port):
    if port is None:
        i = host.rfind("":"")
        j = host.rfind(""]"")  # ipv6 addresses have [...]
        if i > j:
            try:
                port = int(host[i + 1 :])
            except ValueError:
                raise InvalidURL(""nonnumeric port: '%s'"" % host[i + 1 :])
            host = host[:i]
        else:
            port = self.default_port
        if host and host[0] == ""["" and host[-1] == ""]"":
            host = host[1:-1]
    self.host = host
    self.port = port
",if i > j :,176
"def _get_field_value(self, test, key, match):
    if test.ver == ofproto_v1_0.OFP_VERSION:
        members = inspect.getmembers(match)
        for member in members:
            if member[0] == key:
                field_value = member[1]
            elif member[0] == ""wildcards"":
                wildcards = member[1]
        if key == ""nw_src"":
            field_value = test.nw_src_to_str(wildcards, field_value)
        elif key == ""nw_dst"":
            field_value = test.nw_dst_to_str(wildcards, field_value)
    else:
        field_value = match[key]
    return field_value
","if key == ""nw_src"" :",200
"def _clear_storage():
    """"""Clear old files from storage.""""""
    hacs = get_hacs()
    storagefiles = [""hacs""]
    for s_f in storagefiles:
        path = f""{hacs.core.config_path}/.storage/{s_f}""
        if os.path.isfile(path):
            hacs.log.info(f""Cleaning up old storage file {path}"")
            os.remove(path)
",if os . path . isfile ( path ) :,111
"def action_delete(self, ids):
    try:
        count = 0
        # TODO: Optimize me
        for pk in ids:
            if self.delete_model(self.get_one(pk)):
                count += 1
        flash(
            ngettext(
                ""Record was successfully deleted."",
                ""%(count)s records were successfully deleted."",
                count,
                count=count,
            ),
            ""success"",
        )
    except Exception as ex:
        flash(gettext(""Failed to delete records. %(error)s"", error=str(ex)), ""error"")
",if self . delete_model ( self . get_one ( pk ) ) :,166
"def test_inclusion(all_values):
    for values in [{""guid_2"", ""guid_1""}, {""guid_5"", ""guid_XXX""}, {""guid_2""}]:
        test_predicate = in_set(values, ""volume_guid"")
        included_values = set()
        for val in all_values:
            if test_predicate.do_include({""volume_guid"": val}):
                included_values.add(val)
        assert included_values == all_values.intersection(values)
","if test_predicate . do_include ( { ""volume_guid"" : val } ) :",126
"def _get_attr(sdk_path, mod_attr_path, checked=True):
    try:
        attr_mod, attr_path = (
            mod_attr_path.split(""#"") if ""#"" in mod_attr_path else (mod_attr_path, """")
        )
        full_mod_path = ""{}.{}"".format(sdk_path, attr_mod) if attr_mod else sdk_path
        op = import_module(full_mod_path)
        if attr_path:
            # Only load attributes if needed
            for part in attr_path.split("".""):
                op = getattr(op, part)
        return op
    except (ImportError, AttributeError) as ex:
        if checked:
            return None
        raise ex
",if checked :,191
"def __exit__(self, exc_type, exc_val, exc_tb):
    if self.fusefat is not None:
        self.fusefat.send_signal(signal.SIGINT)
        # Allow 1s to return without sending terminate
        for count in range(10):
            time.sleep(0.1)
            if self.fusefat.poll() is not None:
                break
        else:
            self.fusefat.terminate()
        time.sleep(self.delay)
        assert not os.path.exists(self.canary)
    self.dev_null.close()
    shutil.rmtree(self.tmpdir)
",if self . fusefat . poll ( ) is not None :,165
"def check_context_processors(output):
    with output.section(""Context processors"") as section:
        processors = list(
            chain(
                *[
                    template[""OPTIONS""].get(""context_processors"", [])
                    for template in settings.TEMPLATES
                ]
            )
        )
        required_processors = (""cms.context_processors.cms_settings"",)
        for processor in required_processors:
            if processor not in processors:
                section.error(
                    ""%s context processor must be in TEMPLATES option context_processors""
                    % processor
                )
",if processor not in processors :,171
"def test_converters(self):
    response = self._get(""datatypes/converters"")
    self._assert_status_code_is(response, 200)
    converters_list = response.json()
    found_fasta_to_tabular = False
    for converter in converters_list:
        self._assert_has_key(converter, ""source"", ""target"", ""tool_id"")
        if converter[""source""] == ""fasta"" and converter[""target""] == ""tabular"":
            found_fasta_to_tabular = True
    assert found_fasta_to_tabular
","if converter [ ""source"" ] == ""fasta"" and converter [ ""target"" ] == ""tabular"" :",138
"def remove_pid(self, watcher, pid):
    if pid in self._pids[watcher]:
        logger.debug(""Removing %d from %s"" % (pid, watcher))
        self._pids[watcher].remove(pid)
        if len(self._pids[watcher]) == 0:
            logger.debug(""Stopping the periodic callback for {0}"".format(watcher))
            self._callbacks[watcher].stop()
",if len ( self . _pids [ watcher ] ) == 0 :,104
"def _fc_layer(self, sess, bottom, name, trainable=True, relu=True):
    with tf.variable_scope(name) as scope:
        shape = bottom.get_shape().as_list()
        dim = 1
        for d in shape[1:]:
            dim *= d
        x = tf.reshape(bottom, [-1, dim])
        weight = self._get_fc_weight(sess, name, trainable=trainable)
        bias = self._get_bias(sess, name, trainable=trainable)
        fc = tf.nn.bias_add(tf.matmul(x, weight), bias)
        if relu:
            fc = tf.nn.relu(fc)
        return fc
",if relu :,179
"def get_drive(self, root_path="""", volume_guid_path=""""):
    for drive in self.drives:
        if root_path:
            config_root_path = drive.get(""root_path"")
            if config_root_path and root_path == config_root_path:
                return drive
        elif volume_guid_path:
            config_volume_guid_path = drive.get(""volume_guid_path"")
            if config_volume_guid_path and config_volume_guid_path == volume_guid_path:
                return drive
",if config_root_path and root_path == config_root_path :,148
"def rewire_init(expr):
    new_args = []
    if expr[0] == HySymbol(""setv""):
        pairs = expr[1:]
        while len(pairs) > 0:
            k, v = (pairs.pop(0), pairs.pop(0))
            if k == HySymbol(""__init__""):
                v.append(HySymbol(""None""))
            new_args.append(k)
            new_args.append(v)
        expr = HyExpression([HySymbol(""setv"")] + new_args).replace(expr)
    return expr
","if k == HySymbol ( ""__init__"" ) :",149
"def doDir(elem):
    for child in elem.childNodes:
        if not isinstance(child, minidom.Element):
            continue
        if child.tagName == ""Directory"":
            doDir(child)
        elif child.tagName == ""Component"":
            for grandchild in child.childNodes:
                if not isinstance(grandchild, minidom.Element):
                    continue
                if grandchild.tagName != ""File"":
                    continue
                files.add(grandchild.getAttribute(""Source"").replace(os.sep, ""/""))
","if child . tagName == ""Directory"" :",152
"def _v2_common(self, cfg):
    LOG.debug(""v2_common: handling config:\n%s"", cfg)
    if ""nameservers"" in cfg:
        search = cfg.get(""nameservers"").get(""search"", [])
        dns = cfg.get(""nameservers"").get(""addresses"", [])
        name_cmd = {""type"": ""nameserver""}
        if len(search) > 0:
            name_cmd.update({""search"": search})
        if len(dns) > 0:
            name_cmd.update({""addresses"": dns})
        LOG.debug(""v2(nameserver) -> v1(nameserver):\n%s"", name_cmd)
        self.handle_nameserver(name_cmd)
",if len ( search ) > 0 :,178
"def __start_element_handler(self, name, attrs):
    if name == ""mime-type"":
        if self.type:
            for extension in self.extensions:
                self[extension] = self.type
        self.type = attrs[""type""].lower()
        self.extensions = []
    elif name == ""glob"":
        pattern = attrs[""pattern""]
        if pattern.startswith(""*.""):
            self.extensions.append(pattern[1:].lower())
","if pattern . startswith ( ""*."" ) :",120
"def get_attr_by_data_model(self, dmodel, exclude_record=False):
    if exclude_record:
        return list(
            filter(
                lambda x: x.data_model == dmodel and x.value == """"
                if x.attribute != ""Record"" and hasattr(x, ""data_model"")
                else False,
                self._inferred_intent,
            )
        )
    else:
        return list(
            filter(
                lambda x: x.data_model == dmodel and x.value == """"
                if hasattr(x, ""data_model"")
                else False,
                self._inferred_intent,
            )
        )
","if x . attribute != ""Record"" and hasattr ( x , ""data_model"" )",196
"def general(metadata, value):
    if metadata.get(""commands"") and value:
        if not metadata.get(""nargs""):
            v = quote(value)
        else:
            v = value
        return u""{0} {1}"".format(metadata[""commands""][0], v)
    else:
        if not value:
            return None
        elif not metadata.get(""nargs""):
            return quote(value)
        else:
            return value
","elif not metadata . get ( ""nargs"" ) :",122
"def get_images(self):
    images = []
    try:
        tag = MP4(self[""~filename""])
    except Exception:
        return []
    for cover in tag.get(""covr"", []):
        if cover.imageformat == MP4Cover.FORMAT_JPEG:
            mime = ""image/jpeg""
        elif cover.imageformat == MP4Cover.FORMAT_PNG:
            mime = ""image/png""
        else:
            mime = ""image/""
        f = get_temp_cover_file(cover)
        images.append(EmbeddedImage(f, mime))
    return images
",if cover . imageformat == MP4Cover . FORMAT_JPEG :,157
"def run_cmd(self, util, value):
    state = util.state
    if not state.argument_supplied:
        state.argument_supplied = True
        if value == ""by_four"":
            state.argument_value = 4
        elif value == ""negative"":
            state.argument_negative = True
        else:
            state.argument_value = value
    elif value == ""by_four"":
        state.argument_value *= 4
    elif isinstance(value, int):
        state.argument_value *= 10
        state.argument_value += value
    elif value == ""negative"":
        state.argument_value = -state.argument_value
","elif value == ""negative"" :",169
"def finish_character_data(self):
    if self.character_data:
        if not self.skip_ws or not self.character_data.isspace():
            line, column = self.character_pos
            token = XmlToken(
                XML_CHARACTER_DATA, self.character_data, None, line, column
            )
            self.tokens.append(token)
        self.character_data = """"
",if not self . skip_ws or not self . character_data . isspace ( ) :,109
"def check_syntax(filename, raise_error=False):
    """"""Return True if syntax is okay.""""""
    with autopep8.open_with_encoding(filename) as input_file:
        try:
            compile(input_file.read(), ""<string>"", ""exec"", dont_inherit=True)
            return True
        except (SyntaxError, TypeError, UnicodeDecodeError):
            if raise_error:
                raise
            else:
                return False
",if raise_error :,118
"def write(self, file):
    if not self._been_written:
        self._been_written = True
        for attribute, value in self.__dict__.items():
            if attribute[0] != ""_"":
                self.write_recursive(value, file)
        w = file.write
        w(""\t%s = {\n"" % self._id)
        w(""\t\tisa = %s;\n"" % self.__class__.__name__)
        for attribute, value in self.__dict__.items():
            if attribute[0] != ""_"":
                w(""\t\t%s = %s;\n"" % (attribute, self.tostring(value)))
        w(""\t};\n\n"")
","if attribute [ 0 ] != ""_"" :",181
"def update_service_key(kid, name=None, metadata=None):
    try:
        with db_transaction():
            key = db_for_update(ServiceKey.select().where(ServiceKey.kid == kid)).get()
            if name is not None:
                key.name = name
            if metadata is not None:
                key.metadata.update(metadata)
            key.save()
    except ServiceKey.DoesNotExist:
        raise ServiceKeyDoesNotExist
",if metadata is not None :,127
"def fill_buf(self, db, len_=None):
    with open(""/dev/urandom"", ""rb"") as rfh:
        first = True
        for (id_,) in db.query(""SELECT id FROM test""):
            if len_ is None and first:
                val = b""""  # We always want to check this case
                first = False
            elif len_ is None:
                val = rfh.read(random.randint(0, 140))
            else:
                val = rfh.read(len_)
            db.execute(""UPDATE test SET buf=? WHERE id=?"", (val, id_))
",elif len_ is None :,164
"def load_category_from_parser(self, parser):
    for cate in parser.keys():
        id = parser.get_id(cate)
        if self._is_init:
            self._data[""cates""][id] = 0
        else:
            self._data[""cates""][id] = self.count_unread(id)
    self._is_init = False
    self.save()
",if self . _is_init :,102
"def after_insert(self):
    if self.prescription:
        frappe.db.set_value(
            ""Lab Prescription"", self.prescription, ""lab_test_created"", 1
        )
        if frappe.db.get_value(""Lab Prescription"", self.prescription, ""invoiced""):
            self.invoiced = True
    if not self.lab_test_name and self.template:
        self.load_test_from_template()
        self.reload()
","if frappe . db . get_value ( ""Lab Prescription"" , self . prescription , ""invoiced"" ) :",130
"def sync_terminology(self):
    if self.is_source:
        return
    store = self.store
    missing = []
    for source in self.component.get_all_sources():
        if ""terminology"" not in source.all_flags:
            continue
        try:
            _unit, add = store.find_unit(source.context, source.source)
        except UnitNotFound:
            add = True
        # Unit is already present
        if not add:
            continue
        missing.append((source.context, source.source, """"))
    if missing:
        self.add_units(None, missing)
","if ""terminology"" not in source . all_flags :",166
"def refresh(self):
    if self._obj:
        base = self._db.get_media_from_handle(self._obj.get_reference_handle())
        if base:
            self._title = base.get_description()
            self._value = base.get_path()
",if base :,74
"def _set_parse_context(self, tag, tag_attrs):
    # special case: script or style parse context
    if not self._wb_parse_context:
        if tag == ""style"":
            self._wb_parse_context = ""style""
        elif tag == ""script"":
            if self._allow_js_type(tag_attrs):
                self._wb_parse_context = ""script""
","elif tag == ""script"" :",106
"def can_read(self):
    if hasattr(self.file, ""__iter__""):
        iterator = iter(self.file)
        head = next(iterator, None)
        if head is None:
            self.repaired = []
            return True
        if isinstance(head, str):
            self.repaired = itertools.chain([head], iterator)
            return True
        else:
            # We may have mangled a generator at this point, so just abort
            raise IOSourceError(
                ""Could not open source: %r (mode: %r)""
                % (self.file, self.options[""mode""])
            )
    return False
","if isinstance ( head , str ) :",176
"def wrapped_request_method(*args, **kwargs):
    """"""Modifies HTTP headers to include a specified user-agent.""""""
    if kwargs.get(""headers"") is not None:
        if kwargs[""headers""].get(""user-agent""):
            if user_agent not in kwargs[""headers""][""user-agent""]:
                # Save the existing user-agent header and tack on our own.
                kwargs[""headers""][""user-agent""] = (
                    f""{user_agent} "" f'{kwargs[""headers""][""user-agent""]}'
                )
        else:
            kwargs[""headers""][""user-agent""] = user_agent
    else:
        kwargs[""headers""] = {""user-agent"": user_agent}
    return request_method(*args, **kwargs)
","if user_agent not in kwargs [ ""headers"" ] [ ""user-agent"" ] :",191
"def execute(self):
    if self._dirty or not self._qr:
        model_class = self.model_class
        query_meta = self.get_query_meta()
        if self._tuples:
            ResultWrapper = TuplesQueryResultWrapper
        elif self._dicts:
            ResultWrapper = DictQueryResultWrapper
        elif self._naive or not self._joins or self.verify_naive():
            ResultWrapper = NaiveQueryResultWrapper
        elif self._aggregate_rows:
            ResultWrapper = AggregateQueryResultWrapper
        else:
            ResultWrapper = ModelQueryResultWrapper
        self._qr = ResultWrapper(model_class, self._execute(), query_meta)
        self._dirty = False
        return self._qr
    else:
        return self._qr
",elif self . _naive or not self . _joins or self . verify_naive ( ) :,198
"def populate_data(apps, schema_editor):
    Menu = apps.get_model(""menu"", ""Menu"")
    for menu in Menu.objects.all():
        if isinstance(menu.json_content, str):
            json_str = menu.json_content
            while isinstance(json_str, str):
                json_str = json.loads(json_str)
            menu.json_content_new = json_str
            menu.save()
","if isinstance ( menu . json_content , str ) :",118
"def virtualenv_exists(self):
    if os.path.exists(self.virtualenv_location):
        if os.name == ""nt"":
            extra = [""Scripts"", ""activate.bat""]
        else:
            extra = [""bin"", ""activate""]
        return os.path.isfile(os.sep.join([self.virtualenv_location] + extra))
    return False
","if os . name == ""nt"" :",96
"def get_minkowski_function(name, variable):
    fn_name = name + get_postfix(variable)
    if hasattr(MEB, fn_name):
        return getattr(MEB, fn_name)
    else:
        if variable.is_cuda:
            raise ValueError(
                f""Function {fn_name} not available. Please compile MinkowskiEngine with `torch.cuda.is_available()` is `True`.""
            )
        else:
            raise ValueError(f""Function {fn_name} not available."")
",if variable . is_cuda :,134
"def build_temp_workspace(files):
    tempdir = tempfile.mkdtemp(prefix=""yamllint-tests-"")
    for path, content in files.items():
        path = os.path.join(tempdir, path).encode(""utf-8"")
        if not os.path.exists(os.path.dirname(path)):
            os.makedirs(os.path.dirname(path))
        if type(content) is list:
            os.mkdir(path)
        else:
            mode = ""wb"" if isinstance(content, bytes) else ""w""
            with open(path, mode) as f:
                f.write(content)
    return tempdir
",if not os . path . exists ( os . path . dirname ( path ) ) :,169
"def clean_form(self, request, user, form, cleaned_data):
    for field in self.get_fields():
        if field.fieldname not in cleaned_data:
            continue
        try:
            cleaned_data[field.fieldname] = field.clean(
                request, user, cleaned_data[field.fieldname]
            )
        except ValidationError as e:
            form.add_error(field.fieldname, e)
    return cleaned_data
",if field . fieldname not in cleaned_data :,121
"def setUp(self):
    self.realm = service.InMemoryWordsRealm(""realmname"")
    self.checker = checkers.InMemoryUsernamePasswordDatabaseDontUse()
    self.portal = portal.Portal(self.realm, [self.checker])
    self.factory = service.IRCFactory(self.realm, self.portal)
    c = []
    for nick in self.STATIC_USERS:
        if isinstance(nick, bytes):
            nick = nick.decode(""utf-8"")
        c.append(self.realm.createUser(nick))
        self.checker.addUser(nick, nick + ""_password"")
    return DeferredList(c)
","if isinstance ( nick , bytes ) :",165
"def __call__(self, message):
    with self._lock:
        self._pending_ack += 1
        self.max_pending_ack = max(self.max_pending_ack, self._pending_ack)
        self.seen_message_ids.append(int(message.attributes[""seq_num""]))
    time.sleep(self._processing_time)
    with self._lock:
        self._pending_ack -= 1
        message.ack()
        self.completed_calls += 1
        if self.completed_calls >= self._resolve_at_msg_count:
            if not self.done_future.done():
                self.done_future.set_result(None)
",if self . completed_calls >= self . _resolve_at_msg_count :,173
"def fill_in_standard_formats(book):
    for x in std_format_code_types.keys():
        if x not in book.format_map:
            ty = std_format_code_types[x]
            # Note: many standard format codes (mostly CJK date formats) have
            # format strings that vary by locale; xlrd does not (yet)
            # handle those; the type (date or numeric) is recorded but the fmt_str will be None.
            fmt_str = std_format_strings.get(x)
            fmtobj = Format(x, ty, fmt_str)
            book.format_map[x] = fmtobj
",if x not in book . format_map :,171
"def FetchFn(bigger_than_3_only=None, less_than_7_only=None, even_only=None):
    result = []
    for i in range(10):
        # This line introduces a bug.
        if bigger_than_3_only and less_than_7_only and i == 4:
            continue
        if bigger_than_3_only and i <= 3:
            continue
        if less_than_7_only and i >= 7:
            continue
        if even_only and i % 2 != 0:
            continue
        result.append(i)
    return result
",if less_than_7_only and i >= 7 :,158
"def next_instruction_is_function_or_class(lines):
    """"""Is the first non-empty, non-commented line of the cell either a function or a class?""""""
    parser = StringParser(""python"")
    for i, line in enumerate(lines):
        if parser.is_quoted():
            parser.read_line(line)
            continue
        parser.read_line(line)
        if not line.strip():  # empty line
            if i > 0 and not lines[i - 1].strip():
                return False
            continue
        if line.startswith(""def "") or line.startswith(""class ""):
            return True
        if line.startswith((""#"", ""@"", "" "", "")"")):
            continue
        return False
    return False
",if parser . is_quoted ( ) :,194
"def __getattr__(self, key):
    for tag in self.tag.children:
        if tag.name not in (""input"",):
            continue
        if ""name"" in tag.attrs and tag.attrs[""name""] in (key,):
            from thug.DOM.W3C.Core.DOMImplementation import DOMImplementation
            return DOMImplementation.createHTMLElement(self.doc, tag)
    raise AttributeError
","if ""name"" in tag . attrs and tag . attrs [ ""name"" ] in ( key , ) :",104
"def process_signature(app, what, name, obj, options, signature, return_annotation):
    if signature:
        # replace Mock function names
        signature = re.sub(""<Mock name='([^']+)'.*>"", ""\g<1>"", signature)
        signature = re.sub(""tensorflow"", ""tf"", signature)
        # add scope name to layer signatures:
        if hasattr(obj, ""use_scope""):
            if obj.use_scope:
                signature = signature[0] + ""variable_scope_name, "" + signature[1:]
            elif obj.use_scope is None:
                signature = signature[0] + ""[variable_scope_name,] "" + signature[1:]
    # signature: arg list
    return signature, return_annotation
","if hasattr ( obj , ""use_scope"" ) :",188
"def countbox(self):
    self.box = [1000, 1000, -1000, -1000]
    for x, y in self.body:
        if x < self.box[0]:
            self.box[0] = x
        if x > self.box[2]:
            self.box[2] = x
        if y < self.box[1]:
            self.box[1] = y
        if y > self.box[3]:
            self.box[3] = y
",if y > self . box [ 3 ] :,131
"def find_shell():
    global DEFAULT_SHELL
    if not DEFAULT_SHELL:
        for shell in propose_shell():
            if os.path.isfile(shell) and os.access(shell, os.X_OK):
                DEFAULT_SHELL = shell
                break
    if not DEFAULT_SHELL:
        DEFAULT_SHELL = ""/bin/sh""
    return DEFAULT_SHELL
","if os . path . isfile ( shell ) and os . access ( shell , os . X_OK ) :",110
"def addAggregators(sheet, cols, aggrnames):
    ""Add each aggregator in list of *aggrnames* to each of *cols*.""
    for aggrname in aggrnames:
        aggrs = vd.aggregators.get(aggrname)
        aggrs = aggrs if isinstance(aggrs, list) else [aggrs]
        for aggr in aggrs:
            for c in cols:
                if not hasattr(c, ""aggregators""):
                    c.aggregators = []
                if aggr and aggr not in c.aggregators:
                    c.aggregators += [aggr]
",if aggr and aggr not in c . aggregators :,149
"def run(self, paths=[]):
    items = []
    for item in SideBarSelection(paths).getSelectedItems():
        items.append(item.pathAbsoluteFromProjectEncoded())
    if len(items) > 0:
        sublime.set_clipboard(""\n"".join(items))
        if len(items) > 1:
            sublime.status_message(""Items copied"")
        else:
            sublime.status_message(""Item copied"")
",if len ( items ) > 1 :,117
"def social_user(backend, uid, user=None, *args, **kwargs):
    provider = backend.name
    social = backend.strategy.storage.user.get_social_auth(provider, uid)
    if social:
        if user and social.user != user:
            msg = ""This account is already in use.""
            raise AuthAlreadyAssociated(backend, msg)
        elif not user:
            user = social.user
    return {
        ""social"": social,
        ""user"": user,
        ""is_new"": user is None,
        ""new_association"": social is None,
    }
",if user and social . user != user :,170
"def _text(bitlist):
    out = """"
    for typ, text in bitlist:
        if not typ:
            out += text
        elif typ == ""em"":
            out += ""\\fI%s\\fR"" % text
        elif typ in [""strong"", ""code""]:
            out += ""\\fB%s\\fR"" % text
        else:
            raise ValueError(""unexpected tag %r inside text"" % (typ,))
    out = out.strip()
    out = re.sub(re.compile(r""^\s+"", re.M), """", out)
    return out
","elif typ in [ ""strong"" , ""code"" ] :",150
"def OnRadioSelect(self, event):
    fitID = self.mainFrame.getActiveFit()
    if fitID is not None:
        self.mainFrame.command.Submit(
            cmd.GuiChangeImplantLocationCommand(
                fitID=fitID,
                source=ImplantLocation.FIT
                if self.rbFit.GetValue()
                else ImplantLocation.CHARACTER,
            )
        )
",if self . rbFit . GetValue ( ),119
"def hexdump(data):
    """"""yield lines with hexdump of data""""""
    values = []
    ascii = []
    offset = 0
    for h, a in sixteen(data):
        if h is None:
            yield (offset, "" "".join(["""".join(values), """".join(ascii)]))
            del values[:]
            del ascii[:]
            offset += 0x10
        else:
            values.append(h)
            ascii.append(a)
",if h is None :,124
"def submit(self):
    bot_token = self.config[""bot_token""]
    chat_ids = self.config[""chat_id""]
    chat_ids = [chat_ids] if isinstance(chat_ids, str) else chat_ids
    text = ""\n"".join(super().submit())
    if not text:
        logger.debug(""Not calling telegram API (no changes)"")
        return
    result = None
    for chunk in chunkstring(text, self.MAX_LENGTH, numbering=True):
        for chat_id in chat_ids:
            res = self.submitToTelegram(bot_token, chat_id, chunk)
            if res.status_code != requests.codes.ok or res is None:
                result = res
    return result
",if res . status_code != requests . codes . ok or res is None :,187
"def onMessage(self, payload, isBinary):
    if not isBinary:
        self.result = ""Expected binary message with payload, but got binary.""
    else:
        if len(payload) != self.DATALEN:
            self.result = (
                ""Expected binary message with payload of length %d, but got %d.""
                % (self.DATALEN, len(payload))
            )
        else:
            ## FIXME : check actual content
            ##
            self.behavior = Case.OK
            self.result = ""Received binary message of length %d."" % len(payload)
    self.p.createWirelog = True
    self.p.sendClose(self.p.CLOSE_STATUS_CODE_NORMAL)
",if len ( payload ) != self . DATALEN :,192
"def verify_output(actual, expected):
    actual = _read_file(actual, ""Actual"")
    expected = _read_file(join(CURDIR, expected), ""Expected"")
    if len(expected) != len(actual):
        raise AssertionError(
            ""Lengths differ. Expected %d lines but got %d""
            % (len(expected), len(actual))
        )
    for exp, act in zip(expected, actual):
        tester = fnmatchcase if ""*"" in exp else eq
        if not tester(act.rstrip(), exp.rstrip()):
            raise AssertionError(
                ""Lines differ.\nExpected: %s\nActual:   %s"" % (exp, act)
            )
","if not tester ( act . rstrip ( ) , exp . rstrip ( ) ) :",179
"def _in_out_vector_helper(self, name1, name2, ceil):
    vector = []
    stats = self.record
    if ceil is None:
        ceil = self._get_max_rate(name1, name2)
    maxlen = self.config.get_stats_history_length()
    for n in [name1, name2]:
        for i in range(maxlen + 1):
            if i < len(stats):
                vector.append(float(stats[i][n]) / ceil)
            else:
                vector.append(0.0)
    return vector
",if i < len ( stats ) :,153
"def _init_param(param, mode):
    if isinstance(param, str):
        param = _resolve(param)
    elif isinstance(param, (list, tuple)):
        param = [_init_param(p, mode) for p in param]
    elif isinstance(param, dict):
        if {""ref"", ""class_name"", ""config_path""}.intersection(param.keys()):
            param = from_params(param, mode=mode)
        else:
            param = {k: _init_param(v, mode) for k, v in param.items()}
    return param
","if { ""ref"" , ""class_name"" , ""config_path"" } . intersection ( param . keys ( ) ) :",145
"def link_pantsrefs(soups, precomputed):
    """"""Transorm soups: <a pantsref=""foo""> becomes <a href=""../foo_page.html#foo"">""""""
    for (page, soup) in soups.items():
        for a in soup.find_all(""a""):
            if not a.has_attr(""pantsref""):
                continue
            pantsref = a[""pantsref""]
            if pantsref not in precomputed.pantsref:
                raise TaskError(
                    f'Page {page} has pantsref ""{pantsref}"" and I cannot find pantsmark for it'
                )
            a[""href""] = rel_href(page, precomputed.pantsref[pantsref])
",if pantsref not in precomputed . pantsref :,194
"def _gridconvvalue(self, value):
    if isinstance(value, (str, _tkinter.Tcl_Obj)):
        try:
            svalue = str(value)
            if not svalue:
                return None
            elif ""."" in svalue:
                return getdouble(svalue)
            else:
                return getint(svalue)
        except ValueError:
            pass
    return value
","elif ""."" in svalue :",116
"def default(self, o):
    try:
        if type(o) == datetime.datetime:
            return str(o)
        else:
            # remove unwanted attributes from the provider object during conversion to json
            if hasattr(o, ""profile""):
                del o.profile
            if hasattr(o, ""credentials""):
                del o.credentials
            if hasattr(o, ""metadata_path""):
                del o.metadata_path
            if hasattr(o, ""services_config""):
                del o.services_config
            return vars(o)
    except Exception as e:
        return str(o)
",if type ( o ) == datetime . datetime :,172
"def transform_kwarg(self, name, value, split_single_char_options):
    if len(name) == 1:
        if value is True:
            return [""-%s"" % name]
        elif value not in (False, None):
            if split_single_char_options:
                return [""-%s"" % name, ""%s"" % value]
            else:
                return [""-%s%s"" % (name, value)]
    else:
        if value is True:
            return [""--%s"" % dashify(name)]
        elif value is not False and value is not None:
            return [""--%s=%s"" % (dashify(name), value)]
    return []
",if value is True :,183
"def handle(self, context, sign, *args):
    if context.rounding in (ROUND_HALF_UP, ROUND_HALF_EVEN, ROUND_HALF_DOWN, ROUND_UP):
        return Infsign[sign]
    if sign == 0:
        if context.rounding == ROUND_CEILING:
            return Infsign[sign]
        return Decimal((sign, (9,) * context.prec, context.Emax - context.prec + 1))
    if sign == 1:
        if context.rounding == ROUND_FLOOR:
            return Infsign[sign]
        return Decimal((sign, (9,) * context.prec, context.Emax - context.prec + 1))
",if context . rounding == ROUND_CEILING :,184
"def OnLeftUp(self, event):
    # Stop Drawing
    if self.Drawing:
        self.Drawing = False
        if self.RBRect:
            world_rect = (
                self.Canvas.PixelToWorld(self.RBRect[0]),
                self.Canvas.ScalePixelToWorld(self.RBRect[1]),
            )
            wx.CallAfter(self.CallBack, world_rect)
    self.RBRect = None
",if self . RBRect :,127
"def _map_answers(answers):
    result = []
    for a in answers.split(""|""):
        user_answers = []
        result.append(dict(sourcerAnswers=user_answers))
        for r in a.split("",""):
            if r == ""None"":
                user_answers.append(dict(noAnswer=True))
            else:
                start_, end_ = map(int, r.split("":""))
                user_answers.append(dict(s=start_, e=end_))
    return result
","if r == ""None"" :",138
"def parse_edges(self, pcb):
    edges = []
    drawings = list(pcb.GetDrawings())
    bbox = None
    for m in pcb.GetModules():
        for g in m.GraphicalItems():
            drawings.append(g)
    for d in drawings:
        if d.GetLayer() == pcbnew.Edge_Cuts:
            parsed_drawing = self.parse_drawing(d)
            if parsed_drawing:
                edges.append(parsed_drawing)
                if bbox is None:
                    bbox = d.GetBoundingBox()
                else:
                    bbox.Merge(d.GetBoundingBox())
    if bbox:
        bbox.Normalize()
    return edges, bbox
",if d . GetLayer ( ) == pcbnew . Edge_Cuts :,197
"def get_size(self):
    size = self.start_size
    for operation in self.ran_operations:
        if operation[0] == ""resize"":
            size = operation[1][0]
        elif operation[0] == ""crop"":
            crop = operation[1][0]
            size = crop[2] - crop[0], crop[3] - crop[1]
    return size
","if operation [ 0 ] == ""resize"" :",104
"def migrate_account_metadata(account_id):
    from inbox.models.session import session_scope
    from inbox.models import Account
    with session_scope(versioned=False) as db_session:
        account = db_session.query(Account).get(account_id)
        if account.discriminator == ""easaccount"":
            create_categories_for_easfoldersyncstatuses(account, db_session)
        else:
            create_categories_for_folders(account, db_session)
        if account.discriminator == ""gmailaccount"":
            set_labels_for_imapuids(account, db_session)
        db_session.commit()
","if account . discriminator == ""easaccount"" :",168
"def OnEndDrag(self, event):
    self.StopDragging()
    dropTarget = event.GetItem()
    if not dropTarget:
        dropTarget = self.GetRootItem()
    if self.IsValidDropTarget(dropTarget):
        self.UnselectAll()
        if dropTarget != self.GetRootItem():
            self.SelectItem(dropTarget)
        self.OnDrop(dropTarget, self._dragItem)
",if dropTarget != self . GetRootItem ( ) :,109
"def validate(self, frame, value):
    if self.sep and isinstance(value, string_types):
        value = value.split(self.sep)
    if isinstance(value, list):
        if len(self.specs) == 1:
            return [self.specs[0].validate(frame, v) for v in value]
        else:
            return [
                [s.validate(frame, v) for (v, s) in izip(val, self.specs)]
                for val in value
            ]
    raise ValueError(""Invalid MultiSpec data: %r"" % value)
",if len ( self . specs ) == 1 :,153
"def __init__(self, action_space=None, network=None, network_kwargs=None, hparams=None):
    QNetBase.__init__(self, hparams=hparams)
    with tf.variable_scope(self.variable_scope):
        if action_space is None:
            action_space = Space(low=0, high=self._hparams.action_space, dtype=np.int32)
        self._action_space = action_space
        self._append_output_layer()
",if action_space is None :,120
"def n_weights(self):
    """"""Return the number of weights (parameters) in this network.""""""
    n_weights = 0
    for i, w in enumerate(self.all_weights):
        n = 1
        # for s in p.eval().shape:
        for s in w.get_shape():
            try:
                s = int(s)
            except:
                s = 1
            if s:
                n = n * s
        n_weights = n_weights + n
    # print(""num of weights (parameters) %d"" % n_weights)
    return n_weights
",if s :,161
"def _arg_desc(name, ctx):
    for param in ctx.command.params:
        if param.name == name:
            desc = param.opts[-1]
            if desc[0] != ""-"":
                desc = param.human_readable_name
            return desc
    raise AssertionError(name)
","if desc [ 0 ] != ""-"" :",82
"def walk(directory, path_so_far):
    for name in sorted(os.listdir(directory)):
        if any(fnmatch(name, pattern) for pattern in basename_ignore):
            continue
        path = path_so_far + ""/"" + name if path_so_far else name
        if any(fnmatch(path, pattern) for pattern in path_ignore):
            continue
        full_name = os.path.join(directory, name)
        if os.path.isdir(full_name):
            for file_path in walk(full_name, path):
                yield file_path
        elif os.path.isfile(full_name):
            yield path
",elif os . path . isfile ( full_name ) :,172
"def cache_dst(self):
    final_dst = None
    final_linenb = None
    for linenb, assignblk in enumerate(self):
        for dst, src in viewitems(assignblk):
            if dst.is_id(""IRDst""):
                if final_dst is not None:
                    raise ValueError(""Multiple destinations!"")
                final_dst = src
                final_linenb = linenb
    self._dst = final_dst
    self._dst_linenb = final_linenb
    return final_dst
",if final_dst is not None :,144
"def run(self, args, **kwargs):
    if args.resource_ref or args.policy_type:
        filters = {}
        if args.resource_ref:
            filters[""resource_ref""] = args.resource_ref
        if args.policy_type:
            filters[""policy_type""] = args.policy_type
        filters.update(**kwargs)
        return self.manager.query(**filters)
    else:
        return self.manager.get_all(**kwargs)
",if args . resource_ref :,123
"def __init__(self, folders):
    self.folders = folders
    self.duplicates = {}
    for folder, path in folders.items():
        duplicates = []
        for other_folder, other_path in folders.items():
            if other_folder == folder:
                continue
            if other_path == path:
                duplicates.append(other_folder)
        if len(duplicates):
            self.duplicates[folder] = duplicates
",if len ( duplicates ) :,117
"def limit_clause(self, select, **kw):
    text = """"
    if select._limit_clause is not None:
        text += ""\n LIMIT "" + self.process(select._limit_clause, **kw)
    if select._offset_clause is not None:
        if select._limit_clause is None:
            text += ""\n LIMIT "" + self.process(sql.literal(-1))
        text += "" OFFSET "" + self.process(select._offset_clause, **kw)
    else:
        text += "" OFFSET "" + self.process(sql.literal(0), **kw)
    return text
",if select . _limit_clause is None :,150
"def _get_activation(self, act):
    """"""Get activation block based on the name.""""""
    if isinstance(act, str):
        if act.lower() == ""gelu"":
            return GELU()
        elif act.lower() == ""approx_gelu"":
            return GELU(approximate=True)
        else:
            return gluon.nn.Activation(act)
    assert isinstance(act, gluon.Block)
    return act
","elif act . lower ( ) == ""approx_gelu"" :",112
"def __eq__(self, other):
    try:
        if self.type != other.type:
            return False
        if self.type == ""ASK"":
            return self.askAnswer == other.askAnswer
        elif self.type == ""SELECT"":
            return self.vars == other.vars and self.bindings == other.bindings
        else:
            return self.graph == other.graph
    except:
        return False
","elif self . type == ""SELECT"" :",116
"def _get_text_nodes(nodes, html_body):
    text = []
    open_tags = 0
    for node in nodes:
        if isinstance(node, HtmlTag):
            if node.tag_type == OPEN_TAG:
                open_tags += 1
            elif node.tag_type == CLOSE_TAG:
                open_tags -= 1
        elif (
            isinstance(node, HtmlDataFragment)
            and node.is_text_content
            and open_tags == 0
        ):
            text.append(html_body[node.start : node.end])
    return text
",if node . tag_type == OPEN_TAG :,165
"def test_do_change(self):
    """"""Test if VTK object changes when trait is changed.""""""
    p = Prop()
    p.edge_visibility = not p.edge_visibility
    p.representation = ""p""
    p.opacity = 0.5
    p.color = (0, 1, 0)
    p.diffuse_color = (1, 1, 1)
    p.specular_color = (1, 1, 0)
    for t, g in p._updateable_traits_:
        val = getattr(p._vtk_obj, g)()
        if t == ""representation"":
            self.assertEqual(val, getattr(p, t + ""_""))
        else:
            self.assertEqual(val, getattr(p, t))
","if t == ""representation"" :",185
"def update_item(source_doc, target_doc, source_parent):
    target_doc.t_warehouse = """"
    if source_doc.material_request_item and source_doc.material_request:
        add_to_transit = frappe.db.get_value(
            ""Stock Entry"", source_name, ""add_to_transit""
        )
        if add_to_transit:
            warehouse = frappe.get_value(
                ""Material Request Item"", source_doc.material_request_item, ""warehouse""
            )
            target_doc.t_warehouse = warehouse
    target_doc.s_warehouse = source_doc.t_warehouse
    target_doc.qty = source_doc.qty - source_doc.transferred_qty
",if add_to_transit :,198
"def get_drive(self, root_path="""", volume_guid_path=""""):
    for drive in self.drives:
        if root_path:
            config_root_path = drive.get(""root_path"")
            if config_root_path and root_path == config_root_path:
                return drive
        elif volume_guid_path:
            config_volume_guid_path = drive.get(""volume_guid_path"")
            if config_volume_guid_path and config_volume_guid_path == volume_guid_path:
                return drive
",if root_path :,148
"def f_freeze(_):
    repos = utils.get_repos()
    for name, path in repos.items():
        url = """"
        cp = subprocess.run([""git"", ""remote"", ""-v""], cwd=path, capture_output=True)
        if cp.returncode == 0:
            url = cp.stdout.decode(""utf-8"").split(""\n"")[0].split()[1]
        print(f""{url},{name},{path}"")
",if cp . returncode == 0 :,111
"def conj(self):
    dtype = self.dtype
    if issubclass(self.dtype.type, np.complexfloating):
        if not self.flags.forc:
            raise RuntimeError(
                ""only contiguous arrays may "" ""be used as arguments to this operation""
            )
        if self.flags.f_contiguous:
            order = ""F""
        else:
            order = ""C""
        result = self._new_like_me(order=order)
        func = elementwise.get_conj_kernel(dtype)
        func.prepared_async_call(
            self._grid, self._block, None, self.gpudata, result.gpudata, self.mem_size
        )
        return result
    else:
        return self
",if not self . flags . forc :,198
"def detect_reentrancy(self, contract):
    for function in contract.functions_and_modifiers_declared:
        if function.is_implemented:
            if self.KEY in function.context:
                continue
            self._explore(function.entry_point, [])
            function.context[self.KEY] = True
",if self . KEY in function . context :,87
"def test_default_configuration_no_encoding(self):
    transformations = []
    for i in range(2):
        transformation, original = _test_preprocessing(NoEncoding)
        self.assertEqual(transformation.shape, original.shape)
        self.assertTrue((transformation == original).all())
        transformations.append(transformation)
        if len(transformations) > 1:
            self.assertTrue((transformations[-1] == transformations[-2]).all())
",if len ( transformations ) > 1 :,114
"def main():
    """"""main function""""""
    # todo: lookuo real description
    parser = argparse.ArgumentParser(description=""Let a cow speak for you"")
    parser.add_argument(""text"", nargs=""*"", default=None, help=""text to say"")
    ns = parser.parse_args()
    if (ns.text is None) or (len(ns.text) == 0):
        text = """"
        while True:
            inp = sys.stdin.read(4096)
            if inp.endswith(""\n""):
                inp = inp[:-1]
            if not inp:
                break
            text += inp
    else:
        text = "" "".join(ns.text)
    cow = get_cow(text)
    print(cow)
",if not inp :,193
"def prehook(self, emu, op, eip):
    if op in self.badops:
        emu.stopEmu()
        raise v_exc.BadOpBytes(op.va)
    if op.mnem in STOS:
        if self.arch == ""i386"":
            reg = emu.getRegister(envi.archs.i386.REG_EDI)
        elif self.arch == ""amd64"":
            reg = emu.getRegister(envi.archs.amd64.REG_RDI)
        if self.vw.isValidPointer(reg) and self.vw.getLocation(reg) is None:
            self.vw.makePointer(reg, follow=True)
",if self . vw . isValidPointer ( reg ) and self . vw . getLocation ( reg ) is None :,186
"def get_boarding_status(project):
    status = ""Pending""
    if project:
        doc = frappe.get_doc(""Project"", project)
        if flt(doc.percent_complete) > 0.0 and flt(doc.percent_complete) < 100.0:
            status = ""In Process""
        elif flt(doc.percent_complete) == 100.0:
            status = ""Completed""
        return status
",elif flt ( doc . percent_complete ) == 100.0 :,116
"def set_weights(self, new_weights):
    weights = self.get_weights()
    if len(weights) != len(new_weights):
        raise ValueError(""len of lists mismatch"")
    tuples = []
    for w, new_w in zip(weights, new_weights):
        if len(w.shape) != new_w.shape:
            new_w = new_w.reshape(w.shape)
        tuples.append((w, new_w))
    nn.batch_set_value(tuples)
",if len ( w . shape ) != new_w . shape :,129
"def reload_json_api_settings(*args, **kwargs):
    django_setting = kwargs[""setting""]
    setting = django_setting.replace(JSON_API_SETTINGS_PREFIX, """")
    value = kwargs[""value""]
    if setting in DEFAULTS.keys():
        if value is not None:
            setattr(json_api_settings, setting, value)
        elif hasattr(json_api_settings, setting):
            delattr(json_api_settings, setting)
",if value is not None :,115
"def knamn(self, sup, cdict):
    cname = cdict[sup].class_name
    if not cname:
        (namesp, tag) = cdict[sup].name.split(""."")
        if namesp:
            ctag = self.root.modul[namesp].factory(tag).__class__.__name__
            cname = ""%s.%s"" % (namesp, ctag)
        else:
            cname = tag + ""_""
    return cname
",if namesp :,117
"def setdefault(self, key, default=None):
    try:
        o = self.data[key]()
    except KeyError:
        o = None
    if o is None:
        if self._pending_removals:
            self._commit_removals()
        self.data[key] = KeyedRef(default, self._remove, key)
        return default
    else:
        return o
",if self . _pending_removals :,104
"def __on_item_activated(self, event):
    if self.__module_view:
        module = self.get_event_module(event)
        self.__module_view.set_selection(module.module_num)
        if event.EventObject is self.list_ctrl:
            self.input_list_ctrl.deactivate_active_item()
        else:
            self.list_ctrl.deactivate_active_item()
            for index in range(self.list_ctrl.GetItemCount()):
                if self.list_ctrl.IsSelected(index):
                    self.list_ctrl.Select(index, False)
    self.__controller.enable_module_controls_panel_buttons()
",if event . EventObject is self . list_ctrl :,181
"def _create_valid_graph(graph):
    nodes = graph.nodes()
    for i in range(len(nodes)):
        for j in range(len(nodes)):
            if i == j:
                continue
            edge = (nodes[i], nodes[j])
            if graph.has_edge(edge):
                graph.del_edge(edge)
            graph.add_edge(edge, 1)
",if graph . has_edge ( edge ) :,112
"def _parse_param_value(name, datatype, default):
    if datatype == ""bool"":
        if default.lower() == ""true"":
            return True
        elif default.lower() == ""false"":
            return False
        else:
            _s = ""{}: Invalid default value '{}' for bool parameter {}""
            raise SyntaxError(_s.format(self.name, default, p))
    elif datatype == ""int"":
        if type(default) == int:
            return default
        else:
            return int(default, 0)
    elif datatype == ""real"":
        if type(default) == float:
            return default
        else:
            return float(default)
    else:
        return str(default)
","if default . lower ( ) == ""true"" :",191
"def get_size(self, shape_info):
    # The size is the data, that have constant size.
    state = np.random.RandomState().get_state()
    size = 0
    for elem in state:
        if isinstance(elem, str):
            size += len(elem)
        elif isinstance(elem, np.ndarray):
            size += elem.size * elem.itemsize
        elif isinstance(elem, int):
            size += np.dtype(""int"").itemsize
        elif isinstance(elem, float):
            size += np.dtype(""float"").itemsize
        else:
            raise NotImplementedError()
    return size
","if isinstance ( elem , str ) :",159
"def _merge_substs(self, subst, new_substs):
    subst = subst.copy()
    for new_subst in new_substs:
        for name, var in new_subst.items():
            if name not in subst:
                subst[name] = var
            elif subst[name] is not var:
                subst[name].PasteVariable(var)
    return subst
",elif subst [ name ] is not var :,109
"def _load_weights_if_possible(self, model, init_weight_path=None):
    """"""Loads model weights when it is provided.""""""
    if init_weight_path:
        logging.info(""Load weights: {}"".format(init_weight_path))
        if self.use_tpu:
            checkpoint = tf.train.Checkpoint(
                model=model, optimizer=self._create_optimizer()
            )
            checkpoint.restore(init_weight_path)
        else:
            model.load_weights(init_weight_path)
    else:
        logging.info(""Weights not loaded from path:{}"".format(init_weight_path))
",if self . use_tpu :,168
"def _cleanup_inactive_receivexlogs(self, site):
    if site in self.receivexlogs:
        if not self.receivexlogs[site].running:
            if self.receivexlogs[site].is_alive():
                self.receivexlogs[site].join()
            del self.receivexlogs[site]
",if self . receivexlogs [ site ] . is_alive ( ) :,92
"def get_asset(self, path):
    """"""Loads an asset by path.""""""
    clean_path = cleanup_path(path).strip(""/"")
    nodes = [self.asset_root] + self.theme_asset_roots
    for node in nodes:
        for piece in clean_path.split(""/""):
            node = node.get_child(piece)
            if node is None:
                break
        if node is not None:
            return node
    return None
",if node is None :,119
"def palindromic_substrings(s):
    if not s:
        return [[]]
    results = []
    for i in range(len(s), 0, -1):
        sub = s[:i]
        if sub == sub[::-1]:
            for rest in palindromic_substrings(s[i:]):
                results.append([sub] + rest)
    return results
",if sub == sub [ : : - 1 ] :,102
"def debug_tree(tree):
    l = []
    for elt in tree:
        if isinstance(elt, (int, long)):
            l.append(_names.get(elt, elt))
        elif isinstance(elt, str):
            l.append(elt)
        else:
            l.append(debug_tree(elt))
    return l
","if isinstance ( elt , ( int , long ) ) :",92
"def shared_username(account):
    username = os.environ.get(""SHARED_USERNAME"", ""PKKid"")
    for user in account.users():
        if user.title.lower() == username.lower():
            return username
        elif (
            user.username
            and user.email
            and user.id
            and username.lower()
            in (user.username.lower(), user.email.lower(), str(user.id))
        ):
            return username
    pytest.skip(""Shared user %s wasn`t found in your MyPlex account"" % username)
",if user . title . lower ( ) == username . lower ( ) :,152
"def process_schema_element(self, e):
    if e.name is None:
        return
    self.debug1(""adding element: %s"", e.name)
    t = self.get_type(e.type)
    if t:
        if e.name in self.pending_elements:
            del self.pending_elements[e.name]
        self.retval[self.tns].elements[e.name] = e
    else:
        self.pending_elements[e.name] = e
",if e . name in self . pending_elements :,129
"def __setitem__(self, key, value):
    with self._lock:
        try:
            link = self._get_link_and_move_to_front_of_ll(key)
        except KeyError:
            if len(self) < self.max_size:
                self._set_key_and_add_to_front_of_ll(key, value)
            else:
                evicted = self._set_key_and_evict_last_in_ll(key, value)
                super(LRI, self).__delitem__(evicted)
            super(LRI, self).__setitem__(key, value)
        else:
            link[VALUE] = value
",if len ( self ) < self . max_size :,181
"def __delattr__(self, name):
    if name == ""__dict__"":
        raise AttributeError(
            ""%r object attribute '__dict__' is read-only"" % self.__class__.__name__
        )
    if name in self._local_type_vars:
        if name in self._local_type_del_descriptors:
            # A data descriptor, like a property or a slot.
            type_attr = getattr(self._local_type, name, _marker)
            type(type_attr).__delete__(type_attr, self)
            return
    # Otherwise it goes directly in the dict
    # Begin inlined function _get_dict()
    dct = _local_get_dict(self)
    try:
        del dct[name]
    except KeyError:
        raise AttributeError(name)
",if name in self . _local_type_del_descriptors :,199
"def update_participants(self, refresh=True):
    for participant in list(self.participants_dict):
        if participant is None or participant == self.simulator_config.broadcast_part:
            continue
        self.removeItem(self.participants_dict[participant])
        self.participant_items.remove(self.participants_dict[participant])
        del self.participants_dict[participant]
    for participant in self.simulator_config.participants:
        if participant in self.participants_dict:
            self.participants_dict[participant].refresh()
        else:
            self.insert_participant(participant)
    if refresh:
        self.update_view()
",if participant is None or participant == self . simulator_config . broadcast_part :,182
"def insert_bigger_b_add(node):
    if node.op == theano.tensor.add:
        inputs = list(node.inputs)
        if inputs[-1].owner is None:
            inputs[-1] = theano.tensor.concatenate((inputs[-1], inputs[-1]))
            return [node.op(*inputs)]
    return False
",if inputs [ - 1 ] . owner is None :,87
"def _activate_cancel_status(self, cancel_status):
    if self._cancel_status is not None:
        self._cancel_status._tasks.remove(self)
    self._cancel_status = cancel_status
    if self._cancel_status is not None:
        self._cancel_status._tasks.add(self)
        if self._cancel_status.effectively_cancelled:
            self._attempt_delivery_of_any_pending_cancel()
",if self . _cancel_status . effectively_cancelled :,113
"def writeLibraryGeometry(fp, meshes, config, shapes=None):
    progress = Progress(len(meshes), None)
    fp.write(""\n  <library_geometries>\n"")
    for mIdx, mesh in enumerate(meshes):
        if shapes is None:
            shape = None
        else:
            shape = shapes[mIdx]
        writeGeometry(fp, mesh, config, shape)
        progress.step()
    fp.write(""  </library_geometries>\n"")
",if shapes is None :,128
"def init_module_config(module_json, config, config_path=default_config_path):
    if ""config"" in module_json[""meta""]:
        if module_json[""meta""][""config""]:
            if module_json[""name""] not in config:
                config.add_section(module_json[""name""])
            for config_var in module_json[""meta""][""config""]:
                if config_var not in config[module_json[""name""]]:
                    config.set(module_json[""name""], config_var, """")
    return config
","if config_var not in config [ module_json [ ""name"" ] ] :",142
"def get_const_defines(flags, prefix=""""):
    defs = []
    for k, v in globals().items():
        if isinstance(v, int):
            if v & flags:
                if prefix:
                    if k.startswith(prefix):
                        defs.append(k)
                else:
                    defs.append(k)
    return defs
",if prefix :,104
"def __init__(self, source, encoding=DEFAULT_ENCODING):
    self.data = {}
    with open(source, encoding=encoding) as file_:
        for line in file_:
            line = line.strip()
            if not line or line.startswith(""#"") or ""="" not in line:
                continue
            k, v = line.split(""="", 1)
            k = k.strip()
            v = v.strip()
            if len(v) >= 2 and (
                (v[0] == ""'"" and v[-1] == ""'"") or (v[0] == '""' and v[-1] == '""')
            ):
                v = v.strip(""'\"""")
            self.data[k] = v
","if not line or line . startswith ( ""#"" ) or ""="" not in line :",189
"def __detect_console_logger(self):
    logger = self.log
    while logger:
        for handler in logger.handlers[:]:
            if isinstance(handler, StreamHandler):
                if handler.stream in (sys.stdout, sys.stderr):
                    self.logger_handlers.append(handler)
        if logger.root == logger:
            break
        else:
            logger = logger.root
","if isinstance ( handler , StreamHandler ) :",109
"def check_heuristic_in_sql():
    heurs = set()
    excluded = [""Equal assembly or pseudo-code"", ""All or most attributes""]
    for heur in HEURISTICS:
        name = heur[""name""]
        if name in excluded:
            continue
        sql = heur[""sql""]
        if sql.lower().find(name.lower()) == -1:
            print((""SQL command not correctly associated to %s"" % repr(name)))
            print(sql)
            assert sql.find(name) != -1
        heurs.add(name)
    print(""Heuristics:"")
    import pprint
    pprint.pprint(heurs)
",if name in excluded :,171
"def read(self, size=-1):
    buf = bytearray()
    while size != 0 and self.cursor < self.maxpos:
        if not self.in_current_block(self.cursor):
            self.seek_to_block(self.cursor)
        part = self.current_stream.read(size)
        if size > 0:
            if len(part) == 0:
                raise EOFError()
            size -= len(part)
        self.cursor += len(part)
        buf += part
    return bytes(buf)
",if not self . in_current_block ( self . cursor ) :,142
"def get_project_dir(env):
    project_file = workon_home / env / "".project""
    if project_file.exists():
        with project_file.open() as f:
            project_dir = f.readline().strip()
            if os.path.exists(project_dir):
                return project_dir
            else:
                err(
                    ""Corrupted or outdated:"",
                    project_file,
                    ""\nDirectory"",
                    project_dir,
                    ""doesn't exist."",
                )
",if os . path . exists ( project_dir ) :,158
"def _cache_mem(curr_out, prev_mem, mem_len, reuse_len=None):
    """"""cache hidden states into memory.""""""
    if mem_len is None or mem_len == 0:
        return None
    else:
        if reuse_len is not None and reuse_len > 0:
            curr_out = curr_out[:reuse_len]
        if prev_mem is None:
            new_mem = curr_out[-mem_len:]
        else:
            new_mem = tf.concat([prev_mem, curr_out], 0)[-mem_len:]
    return tf.keras.backend.stop_gradient(new_mem)
",if reuse_len is not None and reuse_len > 0 :,165
"def cleanup_channel(self, to_cleanup):
    public_key, id_ = to_cleanup
    # TODO: Maybe run it threaded?
    try:
        with db_session:
            channel = self.session.mds.ChannelMetadata.get_for_update(
                public_key=public_key, id_=id_
            )
            if not channel:
                return
            channel.local_version = 0
            channel.contents.delete(bulk=True)
    except Exception as e:
        self._logger.warning(""Exception while cleaning unsubscribed channel: %"", str(e))
",if not channel :,159
"def best_image(width, height):
    # A heuristic for finding closest sized image to required size.
    image = images[0]
    for img in images:
        if img.width == width and img.height == height:
            # Exact match always used
            return img
        elif img.width >= width and img.width * img.height > image.width * image.height:
            # At least wide enough, and largest area
            image = img
    return image
",elif img . width >= width and img . width * img . height > image . width * image . height :,120
"def add_peer_to_blob(self, contact: ""KademliaPeer"", key: bytes) -> None:
    now = self.loop.time()
    if key in self._data_store:
        current = list(filter(lambda x: x[0] == contact, self._data_store[key]))
        if len(current) > 0:
            self._data_store[key][self._data_store[key].index(current[0])] = (
                contact,
                now,
            )
        else:
            self._data_store[key].append((contact, now))
    else:
        self._data_store[key] = [(contact, now)]
",if len ( current ) > 0 :,180
"def dump(self):
    self.ql.log.info(""[*] Dumping object: %s"" % (self.sf_name))
    for field in self._fields_:
        if isinstance(getattr(self, field[0]), POINTER64):
            self.ql.log.info(""%s: 0x%x"" % (field[0], getattr(self, field[0]).value))
        elif isinstance(getattr(self, field[0]), int):
            self.ql.log.info(""%s: %d"" % (field[0], getattr(self, field[0])))
        elif isinstance(getattr(self, field[0]), bytes):
            self.ql.log.info(""%s: %s"" % (field[0], getattr(self, field[0]).decode()))
","elif isinstance ( getattr ( self , field [ 0 ] ) , bytes ) :",188
"def GeneratePageMetatadata(self, task):
    address_space = self.session.GetParameter(""default_address_space"")
    for vma in task.mm.mmap.walk_list(""vm_next""):
        start = vma.vm_start
        end = vma.vm_end
        # Skip the entire region.
        if end < self.plugin_args.start:
            continue
        # Done.
        if start > self.plugin_args.end:
            break
        for vaddr in utils.xrange(start, end, 0x1000):
            if self.plugin_args.start <= vaddr <= self.plugin_args.end:
                yield vaddr, self._CreateMetadata(address_space.describe_vtop(vaddr))
",if end < self . plugin_args . start :,195
"def _available_symbols(self, scoperef, expr):
    cplns = []
    found_names = set()
    while scoperef:
        elem = self._elem_from_scoperef(scoperef)
        for child in elem:
            name = child.get(""name"", """")
            if name.startswith(expr):
                if name not in found_names:
                    found_names.add(name)
                    ilk = child.get(""ilk"") or child.tag
                    cplns.append((ilk, name))
        scoperef = self.parent_scoperef_from_scoperef(scoperef)
        if not scoperef:
            break
    return sorted(cplns, key=operator.itemgetter(1))
",if not scoperef :,196
"def get_xenapi_host(self):
    """"""Return the xenapi host on which nova-compute runs on.""""""
    with self._get_session() as session:
        if self.host_uuid:
            return session.xenapi.host.get_by_uuid(self.host_uuid)
        else:
            return session.xenapi.session.get_this_host(session.handle)
",if self . host_uuid :,105
"def stream_docker_log(log_stream):
    async for line in log_stream:
        if ""stream"" in line and line[""stream""].strip():
            logger.debug(line[""stream""].strip())
        elif ""status"" in line:
            logger.debug(line[""status""].strip())
        elif ""error"" in line:
            logger.error(line[""error""].strip())
            raise DockerBuildError
","if ""stream"" in line and line [ ""stream"" ] . strip ( ) :",108
"def test_wildcard_import():
    bonobo = __import__(""bonobo"")
    assert bonobo.__version__
    for name in dir(bonobo):
        # ignore attributes starting by underscores
        if name.startswith(""_""):
            continue
        attr = getattr(bonobo, name)
        if inspect.ismodule(attr):
            continue
        assert name in bonobo.__all__
","if name . startswith ( ""_"" ) :",97
"def _coerce_to_bool(self, node, var, true_val=True):
    """"""Coerce the values in a variable to bools.""""""
    bool_var = self.program.NewVariable()
    for b in var.bindings:
        v = b.data
        if isinstance(v, mixin.PythonConstant) and isinstance(v.pyval, bool):
            const = v.pyval is true_val
        elif not compare.compatible_with(v, True):
            const = not true_val
        elif not compare.compatible_with(v, False):
            const = true_val
        else:
            const = None
        bool_var.AddBinding(self.convert.bool_values[const], {b}, node)
    return bool_var
","elif not compare . compatible_with ( v , True ) :",192
"def _parse_policies(self, policies_yaml):
    for item in policies_yaml:
        id_ = required_key(item, ""id"")
        controls_ids = required_key(item, ""controls"")
        if not isinstance(controls_ids, list):
            if controls_ids != ""all"":
                msg = ""Policy {id_} contains invalid controls list {controls}."".format(
                    id_=id_, controls=str(controls_ids)
                )
                raise ValueError(msg)
        self.policies[id_] = controls_ids
","if controls_ids != ""all"" :",155
"def pong(self, payload: Union[str, bytes] = """") -> None:
    if self.trace_enabled and self.ping_pong_trace_enabled:
        if isinstance(payload, bytes):
            payload = payload.decode(""utf-8"")
        self.logger.debug(
            ""Sending a pong data frame ""
            f""(session id: {self.session_id}, payload: {payload})""
        )
    data = _build_data_frame_for_sending(payload, FrameHeader.OPCODE_PONG)
    with self.sock_send_lock:
        self.sock.send(data)
","if isinstance ( payload , bytes ) :",158
"def _extract_curve_feature_log(arg):
    """"""extract sampled curve feature for log items""""""
    try:
        inp, res = arg
        config = inp.config
        with inp.target:
            sch, args = inp.task.instantiate(config)
        fea = feature.get_buffer_curve_sample_flatten(sch, args, sample_n=20)
        x = np.concatenate((fea, list(config.get_other_option().values())))
        if res.error_no == 0:
            y = inp.task.flop / np.mean(res.costs)
        else:
            y = 0.0
        return x, y
    except Exception:  # pylint: disable=broad-except
        return None
",if res . error_no == 0 :,192
"def messageSourceStamps(self, source_stamps):
    text = """"
    for ss in source_stamps:
        source = """"
        if ss[""branch""]:
            source += ""[branch %s] "" % ss[""branch""]
        if ss[""revision""]:
            source += str(ss[""revision""])
        else:
            source += ""HEAD""
        if ss[""patch""] is not None:
            source += "" (plus patch)""
        discriminator = """"
        if ss[""codebase""]:
            discriminator = "" '%s'"" % ss[""codebase""]
        text += ""Build Source Stamp%s: %s\n"" % (discriminator, source)
    return text
","if ss [ ""revision"" ] :",176
"def find_repository():
    orig_path = path = os.path.realpath(""."")
    drive, path = os.path.splitdrive(path)
    while path:
        current_path = os.path.join(drive, path)
        current_repo = LocalRepository(current_path)
        if current_repo.isValid():
            return current_repo
        path, path_tail = os.path.split(current_path)
        if not path_tail:
            raise CannotFindRepository(""Cannot find repository for %s"" % (orig_path,))
",if not path_tail :,139
"def compute_indices(text: str, tokens):
    indices = []
    for i, token in enumerate(tokens):
        if 1 <= i:
            current_index = indices[-1] + len(tokens[i - 1][0])
            indices.append(current_index + text[current_index:].find(token[0]))
        else:
            indices.append(text.find(token[0]))
    return indices
",if 1 <= i :,108
"def _add_defaults_data_files(self):
    # getting distribution.data_files
    if self.distribution.has_data_files():
        for item in self.distribution.data_files:
            if isinstance(item, str):
                # plain file
                item = convert_path(item)
                if os.path.isfile(item):
                    self.filelist.append(item)
            else:
                # a (dirname, filenames) tuple
                dirname, filenames = item
                for f in filenames:
                    f = convert_path(f)
                    if os.path.isfile(f):
                        self.filelist.append(f)
",if os . path . isfile ( item ) :,192
"def libcxx_define(settings):
    compiler = _base_compiler(settings)
    libcxx = settings.get_safe(""compiler.libcxx"")
    if not compiler or not libcxx:
        return """"
    if str(compiler) in GCC_LIKE:
        if str(libcxx) == ""libstdc++"":
            return ""_GLIBCXX_USE_CXX11_ABI=0""
        elif str(libcxx) == ""libstdc++11"":
            return ""_GLIBCXX_USE_CXX11_ABI=1""
    return """"
","if str ( libcxx ) == ""libstdc++"" :",146
"def _populate_tree(self, element, d):
    """"""Populates an etree with attributes & elements, given a dict.""""""
    for k, v in d.iteritems():
        if isinstance(v, dict):
            self._populate_dict(element, k, v)
        elif isinstance(v, list):
            self._populate_list(element, k, v)
        elif isinstance(v, bool):
            self._populate_bool(element, k, v)
        elif isinstance(v, basestring):
            self._populate_str(element, k, v)
        elif type(v) in [int, float, long, complex]:
            self._populate_number(element, k, v)
","elif isinstance ( v , basestring ) :",178
"def test_seek(self):
    if verbose:
        print(""create large file via seek (may be sparse file) ..."")
    with self.open(TESTFN, ""wb"") as f:
        f.write(b""z"")
        f.seek(0)
        f.seek(size)
        f.write(b""a"")
        f.flush()
        if verbose:
            print(""check file size with os.fstat"")
        self.assertEqual(os.fstat(f.fileno())[stat.ST_SIZE], size + 1)
",if verbose :,139
"def serialize_review_url_field(self, obj, **kwargs):
    if obj.review_ui:
        review_request = obj.get_review_request()
        if review_request.local_site_id:
            local_site_name = review_request.local_site.name
        else:
            local_site_name = None
        return local_site_reverse(
            ""file-attachment"",
            local_site_name=local_site_name,
            kwargs={
                ""review_request_id"": review_request.display_id,
                ""file_attachment_id"": obj.pk,
            },
        )
    return """"
",if review_request . local_site_id :,180
"def on_item_down_clicked(self, button):
    model = self.treeview.get_model()
    for s in self._get_selected():
        if s[0] < len(model) - 1:  # XXX need model.swap
            old = model.get_iter(s[0])
            iter = model.insert(s[0] + 2)
            for i in range(3):
                model.set_value(iter, i, model.get_value(old, i))
            model.remove(old)
            self.treeview.get_selection().select_iter(iter)
    self._update_filter_string()
",if s [ 0 ] < len ( model ) - 1 :,167
"def writer(self):
    """"""loop forever and copy socket->serial""""""
    while self.alive:
        try:
            data = self.socket.recv(1024)
            if not data:
                break
            self.serial.write(b"""".join(self.rfc2217.filter(data)))
        except socket.error as msg:
            self.log.error(""{}"".format(msg))
            # probably got disconnected
            break
    self.stop()
",if not data :,124
"def __getitem__(self, key):
    if key == 1:
        return self.get_value()
    elif key == 0:
        return self.cell[0]
    elif isinstance(key, slice):
        s = list(self.cell.__getitem__(key))
        if self.cell[1] in s:
            s[s.index(self.cell[1])] = self.get_value()
        return s
    else:
        raise IndexError(key)
",if self . cell [ 1 ] in s :,120
"def test_error_stream(environ, start_response):
    writer = start_response(""200 OK"", [])
    wsgi_errors = environ[""wsgi.errors""]
    error_msg = None
    for method in [
        ""flush"",
        ""write"",
        ""writelines"",
    ]:
        if not hasattr(wsgi_errors, method):
            error_msg = ""wsgi.errors has no '%s' attr"" % method
        if not error_msg and not callable(getattr(wsgi_errors, method)):
            error_msg = ""wsgi.errors.%s attr is not callable"" % method
        if error_msg:
            break
    return_msg = error_msg or ""success""
    writer(return_msg)
    return []
","if not hasattr ( wsgi_errors , method ) :",185
"def job_rule_modules(app):
    rules_module_list = []
    for rules_module_name in __job_rule_module_names(app):
        rules_module = sys.modules.get(rules_module_name, None)
        if not rules_module:
            # if using a non-default module, it's not imported until a JobRunnerMapper is instantiated when the first
            # JobWrapper is created
            rules_module = importlib.import_module(rules_module_name)
        rules_module_list.append(rules_module)
    return rules_module_list
",if not rules_module :,148
"def discover_hdfstore(f):
    d = dict()
    for key in f.keys():
        d2 = d
        key2 = key.lstrip(""/"")
        while ""/"" in key2:
            group, key2 = key2.split(""/"", 1)
            if group not in d2:
                d2[group] = dict()
            d2 = d2[group]
        d2[key2] = f.get_storer(key)
    return discover(d)
",if group not in d2 :,128
"def test_update_zone(self):
    zone = self.driver.list_zones()[0]
    updated_zone = self.driver.update_zone(zone=zone, domain="""", extra={""paused"": True})
    self.assertEqual(zone.id, updated_zone.id)
    self.assertEqual(zone.domain, updated_zone.domain)
    self.assertEqual(zone.type, updated_zone.type)
    self.assertEqual(zone.ttl, updated_zone.ttl)
    for key in set(zone.extra) | set(updated_zone.extra):
        if key in (""paused"", ""modified_on""):
            self.assertNotEqual(zone.extra[key], updated_zone.extra[key])
        else:
            self.assertEqual(zone.extra[key], updated_zone.extra[key])
","if key in ( ""paused"" , ""modified_on"" ) :",199
"def ESP(phrase):
    for num, name in enumerate(devname):
        if name.lower() in phrase:
            dev = devid[num]
            if custom_action_keyword[""Dict""][""On""] in phrase:
                ctrl = ""=ON""
                say(""Turning On "" + name)
            elif custom_action_keyword[""Dict""][""Off""] in phrase:
                ctrl = ""=OFF""
                say(""Turning Off "" + name)
            rq = requests.head(""https://"" + ip + dev + ctrl, verify=False)
",if name . lower ( ) in phrase :,153
"def filter_ports(self, dpid, in_port, nw_id, allow_nw_id_external=None):
    assert nw_id != self.nw_id_unknown
    ret = []
    for port in self.get_ports(dpid):
        nw_id_ = port.network_id
        if port.port_no == in_port:
            continue
        if nw_id_ == nw_id:
            ret.append(port.port_no)
        elif allow_nw_id_external is not None and nw_id_ == allow_nw_id_external:
            ret.append(port.port_no)
    return ret
",elif allow_nw_id_external is not None and nw_id_ == allow_nw_id_external :,167
"def tail(filename):
    if os.path.isfile(filename):
        file = open(filename, ""r"")
        st_results = os.stat(filename)
        st_size = st_results[6]
        file.seek(st_size)
        while 1:
            where = file.tell()
            line = file.readline()
            if not line:
                time.sleep(1)
                file.seek(where)
            else:
                print(
                    line,
                )  # already has newline
    else:
        print_error(""File not found, cannot tail."")
",if not line :,172
"def proc_day_of_week(d):
    if expanded[4][0] != ""*"":
        diff_day_of_week = nearest_diff_method(d.isoweekday() % 7, expanded[4], 7)
        if diff_day_of_week is not None and diff_day_of_week != 0:
            if is_prev:
                d += relativedelta(days=diff_day_of_week, hour=23, minute=59, second=59)
            else:
                d += relativedelta(days=diff_day_of_week, hour=0, minute=0, second=0)
            return True, d
    return False, d
",if is_prev :,175
"def __call__(self):
    """"""Run all check_* methods.""""""
    if self.on:
        oldformatwarning = warnings.formatwarning
        warnings.formatwarning = self.formatwarning
        try:
            for name in dir(self):
                if name.startswith(""check_""):
                    method = getattr(self, name)
                    if method and callable(method):
                        method()
        finally:
            warnings.formatwarning = oldformatwarning
",if method and callable ( method ) :,127
"def get(self, request, *args, **kwargs):
    if self.revision:
        if settings.USE_LOCAL_PATH:
            try:
                return send_file(
                    request,
                    self.revision.file.path,
                    self.revision.created,
                    self.attachment.original_filename,
                )
            except OSError:
                pass
        else:
            return HttpResponseRedirect(self.revision.file.url)
    raise Http404
",if settings . USE_LOCAL_PATH :,141
"def _close(self):
    super(Recording, self)._close()
    if self._log_n is not None:
        for i in range(self.n):
            if self._log_n[i] is not None:
                self._log_n[i].close()
                self._log_n[i] = None
",if self . _log_n [ i ] is not None :,88
"def addTags(self, rpcObjects=None):
    hosts = self._getOnlyHostObjects(rpcObjects)
    if hosts:
        title = ""Add Tags""
        body = ""What tags should be added?\n\nUse a comma or space between each""
        (tags, choice) = self.getText(title, body, """")
        if choice:
            tags = str(tags).replace("" "", "","").split("","")
            for host in hosts:
                self.cuebotCall(
                    host.addTags, ""Add Tags to %s Failed"" % host.data.name, tags
                )
            self._update()
",if choice :,167
"def available_datasets(self):
    """"""Automatically determine datasets provided by this file""""""
    res = self.resolution
    coordinates = [""pixel_longitude"", ""pixel_latitude""]
    for var_name, val in self.file_content.items():
        if isinstance(val, netCDF4.Variable):
            ds_info = {
                ""file_type"": self.filetype_info[""file_type""],
                ""resolution"": res,
            }
            if not self.is_geo:
                ds_info[""coordinates""] = coordinates
            yield DatasetID(name=var_name, resolution=res), ds_info
","if isinstance ( val , netCDF4 . Variable ) :",165
"def extract_from_file(fname: PathIsh) -> Iterator[Extraction]:
    path = Path(fname)
    fallback_dt = file_mtime(path)
    p = Parser(path)
    for r in p.walk():
        if isinstance(r, Exception):
            yield r
        else:
            yield Visit(
                url=r.url,
                dt=fallback_dt,
                locator=Loc.file(fname),  # TODO line number
                context=r.context,
            )
","if isinstance ( r , Exception ) :",141
"def init_module_config(module_json, config, config_path=default_config_path):
    if ""config"" in module_json[""meta""]:
        if module_json[""meta""][""config""]:
            if module_json[""name""] not in config:
                config.add_section(module_json[""name""])
            for config_var in module_json[""meta""][""config""]:
                if config_var not in config[module_json[""name""]]:
                    config.set(module_json[""name""], config_var, """")
    return config
","if module_json [ ""name"" ] not in config :",142
"def _create_entities(parsed_entities, sidx, eidx):
    entities = []
    for k, vs in parsed_entities.items():
        if not isinstance(vs, list):
            vs = [vs]
        for value in vs:
            entities.append(
                {
                    ""entity"": k,
                    ""start"": sidx,
                    ""end"": eidx,  # can't be more specific
                    ""value"": value,
                }
            )
    return entities
","if not isinstance ( vs , list ) :",145
"def _telegram_upload_stream(self, stream, **kwargs):
    """"""Perform upload defined in a stream.""""""
    msg = None
    try:
        stream.accept()
        msg = self._telegram_special_message(
            chat_id=stream.identifier.id,
            content=stream.raw,
            msg_type=stream.stream_type,
            **kwargs,
        )
    except Exception:
        log.exception(f""Upload of {stream.name} to {stream.identifier} failed."")
    else:
        if msg is None:
            stream.error()
        else:
            stream.success()
",if msg is None :,166
"def readlines(self, size=-1):
    if self._nbr == self._size:
        return []
    # leave all additional logic to our readline method, we just check the size
    out = []
    nbr = 0
    while True:
        line = self.readline()
        if not line:
            break
        out.append(line)
        if size > -1:
            nbr += len(line)
            if nbr > size:
                break
        # END handle size constraint
    # END readline loop
    return out
",if not line :,145
"def clean_permissions(
    cls,
    requestor: ""User"",
    group: auth_models.Group,
    errors: Dict[Optional[str], List[ValidationError]],
    cleaned_input: dict,
):
    field = ""add_permissions""
    permission_items = cleaned_input.get(field)
    if permission_items:
        cleaned_input[field] = get_permissions(permission_items)
        if not requestor.is_superuser:
            cls.ensure_can_manage_permissions(
                requestor, errors, field, permission_items
            )
",if not requestor . is_superuser :,142
"def _bwd(subj=None, obj=None, seen=None):
    seen.add(obj)
    for s, o in evalPath(graph, (None, self.path, obj)):
        if not subj or subj == s:
            yield s, o
        if self.more:
            if s in seen:
                continue
            for s2, o2 in _bwd(None, s, seen):
                yield s2, o
",if not subj or subj == s :,120
"def generate_data(self, request):
    """"""Generate data for the widget.""""""
    uptime = {}
    cache_stats = get_cache_stats()
    if cache_stats:
        for hosts, stats in cache_stats:
            if stats[""uptime""] > 86400:
                uptime[""value""] = stats[""uptime""] / 60 / 60 / 24
                uptime[""unit""] = _(""days"")
            elif stats[""uptime""] > 3600:
                uptime[""value""] = stats[""uptime""] / 60 / 60
                uptime[""unit""] = _(""hours"")
            else:
                uptime[""value""] = stats[""uptime""] / 60
                uptime[""unit""] = _(""minutes"")
    return {""cache_stats"": cache_stats, ""uptime"": uptime}
","if stats [ ""uptime"" ] > 86400 :",195
"def refresh(self):
    if self._handle:
        source = self._db.get_repository_from_handle(self._handle)
        if source:
            self._title = str(source.get_type())
            self._value = source.get_name()
",if source :,70
"def _gridconvvalue(self, value):
    if isinstance(value, (str, _tkinter.Tcl_Obj)):
        try:
            svalue = str(value)
            if not svalue:
                return None
            elif ""."" in svalue:
                return getdouble(svalue)
            else:
                return getint(svalue)
        except ValueError:
            pass
    return value
",if not svalue :,116
"def parseGrants(self, tree):
    for grant in tree.findall("".//Grant""):
        grantee = Grantee()
        g = grant.find("".//Grantee"")
        grantee.xsi_type = g.attrib[""{http://www.w3.org/2001/XMLSchema-instance}type""]
        grantee.permission = grant.find(""Permission"").text
        for el in g:
            if el.tag == ""DisplayName"":
                grantee.display_name = el.text
            else:
                grantee.tag = el.tag
                grantee.name = el.text
        self.grantees.append(grantee)
","if el . tag == ""DisplayName"" :",188
"def __init__(self, name: Optional[str] = None, order: int = 0):
    if name is None:
        if order == 0:
            name = ""std_dev""
        elif order == 1:
            name = ""sample_std_dev""
        else:
            name = f""std_dev{order})""
    super().__init__(name=name, order=order)
    self.order = order
",elif order == 1 :,109
"def _shouldRollover(self):
    if self.maxBytes > 0:  # are we rolling over?
        try:
            self.stream.seek(0, 2)  # due to non-posix-compliant Windows feature
        except IOError:
            return True
        if self.stream.tell() >= self.maxBytes:
            return True
        else:
            self._degrade(False, ""Rotation done or not needed at this time"")
    return False
",if self . stream . tell ( ) >= self . maxBytes :,124
"def userfullname():
    """"""Get the user's full name.""""""
    global _userfullname
    if not _userfullname:
        uid = os.getuid()
        entry = pwd_from_uid(uid)
        if entry:
            _userfullname = entry[4].split("","")[0] or entry[0]
        if not _userfullname:
            _userfullname = ""user%d"" % uid
    return _userfullname
",if entry :,108
"def drop(self):
    # mssql
    sql = ""if object_id('%s') is not null drop table %s"" % (self.tname, self.tname)
    try:
        self.execute(sql)
    except Exception as e:
        self.conn.rollback()
        if ""syntax error"" not in str(e):
            raise
        # sqlite
        sql = ""drop table if exists %s"" % self.tname
        self.execute(sql)
","if ""syntax error"" not in str ( e ) :",124
"def _find_delimiter(f, block_size=2 ** 16):
    delimiter = b""\n""
    if f.tell() == 0:
        return 0
    while True:
        b = f.read(block_size)
        if not b:
            return f.tell()
        elif delimiter in b:
            return f.tell() - len(b) + b.index(delimiter) + 1
",if not b :,105
"def _convert(container):
    if _value_marker in container:
        force_list = False
        values = container.pop(_value_marker)
        if container.pop(_list_marker, False):
            force_list = True
            values.extend(_convert(x[1]) for x in sorted(container.items()))
        if not force_list and len(values) == 1:
            values = values[0]
        if not container:
            return values
        return _convert(container)
    elif container.pop(_list_marker, False):
        return [_convert(x[1]) for x in sorted(container.items())]
    return dict_cls((k, _convert(v)) for k, v in iteritems(container))
","if container . pop ( _list_marker , False ) :",188
"def fitting(self, value):
    self._fitting = value
    if self._fitting is not None:
        if not os.path.exists(dirname(self.checkpoint_path())):
            try:
                os.makedirs(dirname(self.checkpoint_path()))
            except FileExistsError as ex:
                pass  # race to create
        if not os.path.exists(dirname(self.tensorboard_path())):
            try:
                os.makedirs(dirname(self.tensorboard_path()))
            except FileExistsError as ex:
                pass  # race to create
",if not os . path . exists ( dirname ( self . checkpoint_path ( ) ) ) :,149
"def _make_headers(self):
    libraries = self._df.columns.to_list()
    columns = []
    for library in libraries:
        version = self._package_versions[library]
        library_description = self._libraries_description.get(library)
        if library_description:
            library += "" {}"".format(library_description)
        columns.append(
            ""{library}<br><small>{version}</small>"".format(
                library=library, version=version
            )
        )
    return [""""] + columns
",if library_description :,138
"def plugin_on_song_ended(self, song, stopped):
    if song is not None:
        poll = self.rating_box.poll_vote()
        if poll[0] >= 1 or poll[1] >= 1:
            ups = int(song.get(""~#wins"") or 0)
            downs = int(song.get(""~#losses"") or 0)
            ups += poll[0]
            downs += poll[1]
            song[""~#wins""] = ups
            song[""~#losses""] = downs
            song[""~#rating""] = ups / max((ups + downs), 2)
            # note: ^^^ Look into implementing w/ confidence intervals!
            song[""~#score""] = ups - downs
",if poll [ 0 ] >= 1 or poll [ 1 ] >= 1 :,196
"def submit(self, pig_script, params):
    workflow = None
    try:
        workflow = self._create_workflow(pig_script, params)
        mapping = dict(
            [(param[""name""], param[""value""]) for param in workflow.get_parameters()]
        )
        oozie_wf = _submit_workflow(self.user, self.fs, self.jt, workflow, mapping)
    finally:
        if workflow:
            workflow.delete(skip_trash=True)
    return oozie_wf
",if workflow :,131
"def test_parse(self):
    correct = 0
    for example in EXAMPLES:
        try:
            schema.parse(example.schema_string)
            if example.valid:
                correct += 1
            else:
                self.fail(""Invalid schema was parsed: "" + example.schema_string)
        except:
            if not example.valid:
                correct += 1
            else:
                self.fail(""Valid schema failed to parse: "" + example.schema_string)
    fail_msg = ""Parse behavior correct on %d out of %d schemas."" % (
        correct,
        len(EXAMPLES),
    )
    self.assertEqual(correct, len(EXAMPLES), fail_msg)
",if example . valid :,188
"def handle_sent(self, elt):
    sent = []
    for child in elt:
        if child.tag in (""wf"", ""punc""):
            itm = self.handle_word(child)
            if self._unit == ""word"":
                sent.extend(itm)
            else:
                sent.append(itm)
        else:
            raise ValueError(""Unexpected element %s"" % child.tag)
    return SemcorSentence(elt.attrib[""snum""], sent)
","if self . _unit == ""word"" :",126
"def _set_property(self, target_widget, pname, value):
    if pname == ""text"":
        state = target_widget.cget(""state"")
        if state == tk.DISABLED:
            target_widget.configure(state=tk.NORMAL)
            target_widget.insert(""0.0"", value)
            target_widget.configure(state=tk.DISABLED)
        else:
            target_widget.insert(""0.0"", value)
    else:
        super(TKText, self)._set_property(target_widget, pname, value)
",if state == tk . DISABLED :,145
"def get_vrf_tables(self, vrf_rf=None):
    vrf_tables = {}
    for (scope_id, table_id), table in self._tables.items():
        if scope_id is None:
            continue
        if vrf_rf is not None and table_id != vrf_rf:
            continue
        vrf_tables[(scope_id, table_id)] = table
    return vrf_tables
",if vrf_rf is not None and table_id != vrf_rf :,112
"def new_f(self, *args, **kwargs):
    for obj in f(self, *args, **kwargs):
        if self.protected == False:
            if ""user"" in obj and obj[""user""][""protected""]:
                continue
            elif ""protected"" in obj and obj[""protected""]:
                continue
        yield obj
","elif ""protected"" in obj and obj [ ""protected"" ] :",88
"def draw(self, context):
    col = self.layout.column()
    col.operator(""node.sv_show_latest_commits"")
    if context.scene.sv_new_version:
        col_alert = self.layout.column()
        col_alert.alert = True
        col_alert.operator(""node.sverchok_update_addon"", text=""Upgrade Sverchok addon"")
    else:
        col.operator(""node.sverchok_check_for_upgrades_wsha"", text=""Check for updates"")
    with sv_preferences() as prefs:
        if prefs.developer_mode:
            col.operator(""node.sv_run_pydoc"")
",if prefs . developer_mode :,173
"def generate_tag_1_data(ids):
    if len(ids) != SAMPLE_NUM:
        raise ValueError(""len ids should equal to sample number"")
    counter = 0
    for sample_i in range(SAMPLE_NUM):
        one_data = [ids[sample_i]]
        valid_set = [x for x in range(TAG_INTERVAL[0], TAG_INTERVAL[1])]
        features = np.random.choice(valid_set, FEATURE_NUM, replace=False)
        one_data += ["":"".join([x, ""1.0""]) for x in features]
        counter += 1
        if counter % 10000 == 0:
            print(""generate data {}"".format(counter))
        yield one_data
",if counter % 10000 == 0 :,179
"def handle_api_languages(self, http_context):
    mgr = PluginManager.get(aj.context)
    languages = set()
    for id in mgr:
        locale_dir = mgr.get_content_path(id, ""locale"")
        if os.path.isdir(locale_dir):
            for lang in os.listdir(locale_dir):
                if lang != ""app.pot"":
                    languages.add(lang)
    return sorted(list(languages))
",if os . path . isdir ( locale_dir ) :,125
"def update(self, t):
    # direction right - up
    for i in range(self.grid.x):
        for j in range(self.grid.y):
            distance = self.test_func(i, j, t)
            if distance == 0:
                self.turn_off_tile(i, j)
            elif distance < 1:
                self.transform_tile(i, j, distance)
            else:
                self.turn_on_tile(i, j)
",if distance == 0 :,135
"def _handle_autocomplete_request_for_text(text):
    if not hasattr(text, ""autocompleter""):
        if isinstance(text, (CodeViewText, ShellText)) and text.is_python_text():
            if isinstance(text, CodeViewText):
                text.autocompleter = Completer(text)
            elif isinstance(text, ShellText):
                text.autocompleter = ShellCompleter(text)
            text.bind(""<1>"", text.autocompleter.on_text_click)
        else:
            return
    text.autocompleter.handle_autocomplete_request()
","elif isinstance ( text , ShellText ) :",151
"def test_create_repository(repo_name, expected_status, client):
    with client_with_identity(""devtable"", client) as cl:
        body = {
            ""namespace"": ""devtable"",
            ""repository"": repo_name,
            ""visibility"": ""public"",
            ""description"": ""foo"",
        }
        result = conduct_api_call(
            client, RepositoryList, ""post"", None, body, expected_code=expected_status
        ).json
        if expected_status == 201:
            assert result[""name""] == repo_name
            assert (
                model.repository.get_repository(""devtable"", repo_name).name == repo_name
            )
",if expected_status == 201 :,186
"def _apply_filter(filter_item, filter_list):
    for filter_method in filter_list:
        try:
            if not filter_method(context, filter_item):
                return False
        except Exception as e:
            raise MessageException(
                ""Toolbox filter exception from '{}': {}."".format(
                    filter_method.__name__, unicodify(e)
                )
            )
    return True
","if not filter_method ( context , filter_item ) :",116
"def printsumfp(fp, filename, out=sys.stdout):
    m = md5()
    try:
        while 1:
            data = fp.read(bufsize)
            if not data:
                break
            if isinstance(data, str):
                data = data.encode(fp.encoding)
            m.update(data)
    except IOError as msg:
        sys.stderr.write(""%s: I/O error: %s\n"" % (filename, msg))
        return 1
    out.write(""%s %s\n"" % (m.hexdigest(), filename))
    return 0
","if isinstance ( data , str ) :",159
"def get_block_loc_keys(block):
    """"""Extract loc_keys used by @block""""""
    symbols = set()
    for instr in block.lines:
        if isinstance(instr, AsmRaw):
            if isinstance(instr.raw, list):
                for expr in instr.raw:
                    symbols.update(get_expr_locs(expr))
        else:
            for arg in instr.args:
                symbols.update(get_expr_locs(arg))
    return symbols
","if isinstance ( instr , AsmRaw ) :",131
"def get_operations(cls, info, operations: List[ProductAttributeAssignInput]):
    """"""Resolve all passed global ids into integer PKs of the Attribute type.""""""
    product_attrs_pks = []
    variant_attrs_pks = []
    for operation in operations:
        pk = from_global_id_strict_type(
            operation.id, only_type=Attribute, field=""operations""
        )
        if operation.type == ProductAttributeType.PRODUCT:
            product_attrs_pks.append(pk)
        else:
            variant_attrs_pks.append(pk)
    return product_attrs_pks, variant_attrs_pks
",if operation . type == ProductAttributeType . PRODUCT :,156
"def _collect_manual_intervention_nodes(pipeline_tree):
    for act in pipeline_tree[""activities""].values():
        if act[""type""] == ""SubProcess"":
            _collect_manual_intervention_nodes(act[""pipeline""])
        elif act[""component""][""code""] in MANUAL_INTERVENTION_COMP_CODES:
            manual_intervention_nodes.add(act[""id""])
","elif act [ ""component"" ] [ ""code"" ] in MANUAL_INTERVENTION_COMP_CODES :",105
"def prompt_authorization(self, stacks: List[Stack]):
    auth_required_per_resource = auth_per_resource(stacks)
    for resource, authorization_required in auth_required_per_resource:
        if not authorization_required:
            auth_confirm = confirm(
                f""\t{self.start_bold}{resource} may not have authorization defined, Is this okay?{self.end_bold}"",
                default=False,
            )
            if not auth_confirm:
                raise GuidedDeployFailedError(msg=""Security Constraints Not Satisfied!"")
",if not authorization_required :,148
"def get_cloud_credential(self):
    """"""Return the credential which is directly tied to the inventory source type.""""""
    credential = None
    for cred in self.credentials.all():
        if self.source in CLOUD_PROVIDERS:
            if cred.kind == self.source.replace(""ec2"", ""aws""):
                credential = cred
                break
        else:
            # these need to be returned in the API credential field
            if cred.credential_type.kind != ""vault"":
                credential = cred
                break
    return credential
","if cred . kind == self . source . replace ( ""ec2"" , ""aws"" ) :",149
"def validate_party_details(self):
    if self.party:
        if not frappe.db.exists(self.party_type, self.party):
            frappe.throw(_(""Invalid {0}: {1}"").format(self.party_type, self.party))
        if self.party_account and self.party_type in (""Customer"", ""Supplier""):
            self.validate_account_type(
                self.party_account, [erpnext.get_party_account_type(self.party_type)]
            )
","if self . party_account and self . party_type in ( ""Customer"" , ""Supplier"" ) :",140
"def __iter__(self):
    it = DiskHashMerger.__iter__(self)
    direct_upstreams = self.direct_upstreams
    for k, groups in it:
        t = list([[] for _ in range(self.size)])
        for i, g in enumerate(groups):
            if g:
                if i in direct_upstreams:
                    t[i] = g
                else:
                    g.sort(key=itemgetter(0))
                    g1 = []
                    for _, vs in g:
                        g1.extend(vs)
                    t[i] = g1
        yield k, tuple(t)
",if g :,185
"def _unpack_scales(scales, vidxs):
    scaleData = [None, None, None]
    for i in range(3):
        if i >= min(len(scales), len(vidxs) // 2):
            break
        scale = scales[i]
        if not math.isnan(scale):
            vidx1, vidx2 = vidxs[i * 2], vidxs[i * 2 + 1]
            scaleData[i] = (int(vidx1), int(vidx2), float(scale))
    return scaleData
","if i >= min ( len ( scales ) , len ( vidxs ) // 2 ) :",138
"def _make_ext_obj(self, obj):
    ext = self._get_ext_class(obj.objname)()
    for name, val in obj.body:
        if not isinstance(val, list):
            raise Exception(
                ""Error val should be a list, this is a python-opcua bug"",
                name,
                type(val),
                val,
            )
        else:
            for attname, v in val:
                self._set_attr(ext, attname, v)
    return ext
","if not isinstance ( val , list ) :",148
"def insertLine(self, refnum, linenum, line):
    i = -1
    for i, row in enumerate(self.rows):
        if row[0] == linenum:
            if row[refnum + 1] is None:
                row[refnum + 1] = line
                return
            # else keep looking
        elif row[0] > linenum:
            break
    self.rows.insert(i, self.newRow(linenum, refnum, line))
",elif row [ 0 ] > linenum :,125
"def valid_localparts(strip_delimiters=False):
    for line in ABRIDGED_LOCALPART_VALID_TESTS.split(""\n""):
        # strip line, skip over empty lines
        line = line.strip()
        if line == """":
            continue
        # skip over comments or empty lines
        match = COMMENT.match(line)
        if match:
            continue
        # skip over localparts with delimiters
        if strip_delimiters:
            if "","" in line or "";"" in line:
                continue
        yield line
","if "","" in line or "";"" in line :",145
"def encodingChanged(self, idx):
    encoding = str(self.mode_combo.currentText())
    validator = None
    if encoding == ""hex"":
        # only clear the box if there are non-hex chars
        # before setting the validator.
        txt = str(self.data_edit.text())
        if not all(c in string.hexdigits for c in txt):
            self.data_edit.setText("""")
        regex = QtCore.QRegExp(""^[0-9A-Fa-f]+$"")
        validator = QtGui.QRegExpValidator(regex)
    self.data_edit.setValidator(validator)
    self.renderMemory()
",if not all ( c in string . hexdigits for c in txt ) :,164
"def _compare_single_run(self, compares_done):
    try:
        compare_id, redo = self.in_queue.get(
            timeout=float(self.config[""ExpertSettings""][""block_delay""])
        )
    except Empty:
        pass
    else:
        if self._decide_whether_to_process(compare_id, redo, compares_done):
            if redo:
                self.db_interface.delete_old_compare_result(compare_id)
            compares_done.add(compare_id)
            self._process_compare(compare_id)
            if self.callback:
                self.callback()
",if redo :,177
"def _transform_bin(self, X: DataFrame):
    if self._bin_map:
        if not self.inplace:
            X = X.copy(deep=True)
        with pd.option_context(""mode.chained_assignment"", None):
            # Pandas complains about SettingWithCopyWarning, but this should be valid.
            for column in self._bin_map:
                X[column] = binning.bin_column(
                    series=X[column],
                    mapping=self._bin_map[column],
                    dtype=self._astype_map[column],
                )
    return X
",if not self . inplace :,166
"def escape(text, newline=False):
    """"""Escape special html characters.""""""
    if isinstance(text, str):
        if ""&"" in text:
            text = text.replace(""&"", ""&amp;"")
        if "">"" in text:
            text = text.replace("">"", ""&gt;"")
        if ""<"" in text:
            text = text.replace(""<"", ""&lt;"")
        if '""' in text:
            text = text.replace('""', ""&quot;"")
        if ""'"" in text:
            text = text.replace(""'"", ""&quot;"")
        if newline:
            if ""\n"" in text:
                text = text.replace(""\n"", ""<br>"")
    return text
","if ""'"" in text :",170
"def read(self):
    """"""Reads the robots.txt URL and feeds it to the parser.""""""
    try:
        f = urllib.request.urlopen(self.url)
    except urllib.error.HTTPError as err:
        if err.code in (401, 403):
            self.disallow_all = True
        elif err.code >= 400 and err.code < 500:
            self.allow_all = True
    else:
        raw = f.read()
        self.parse(raw.decode(""utf-8"").splitlines())
","if err . code in ( 401 , 403 ) :",134
"def post_create(self, user, billing=None):
    from weblate.trans.models import Change
    if billing:
        billing.projects.add(self)
        if billing.plan.change_access_control:
            self.access_control = Project.ACCESS_PRIVATE
        else:
            self.access_control = Project.ACCESS_PUBLIC
        self.save()
    if not user.is_superuser:
        self.add_user(user, ""@Administration"")
    Change.objects.create(
        action=Change.ACTION_CREATE_PROJECT, project=self, user=user, author=user
    )
",if billing . plan . change_access_control :,158
"def visitConst(self, node):
    if self.documentable:
        if type(node.value) in (StringType, UnicodeType):
            self.documentable.append(make_docstring(node.value, node.lineno))
        else:
            self.documentable = None
","if type ( node . value ) in ( StringType , UnicodeType ) :",73
"def requires(self):
    requires = copy.deepcopy(self._requires)
    # Auto add dependencies when parameters reference the Ouptuts of
    # another stack.
    parameters = self.parameters
    for value in parameters.values():
        if isinstance(value, basestring) and ""::"" in value:
            stack_name, _ = value.split(""::"")
        else:
            continue
        if stack_name not in requires:
            requires.add(stack_name)
    return requires
",if stack_name not in requires :,123
"def __load_protos():
    g = globals()
    for k, v in g.items():
        if k.startswith(""PPP_""):
            name = k[4:]
            modname = name.lower()
            try:
                mod = __import__(modname, g, level=1)
                PPP.set_p(v, getattr(mod, name))
            except (ImportError, AttributeError):
                continue
","if k . startswith ( ""PPP_"" ) :",115
"def init_weights(self):
    """"""Initialize model weights.""""""
    for m in self.predict_layers.modules():
        if isinstance(m, nn.Conv2d):
            kaiming_init(m)
        elif isinstance(m, nn.BatchNorm2d):
            constant_init(m, 1)
        elif isinstance(m, nn.Linear):
            normal_init(m, std=0.01)
","elif isinstance ( m , nn . Linear ) :",107
"def get_data(self):
    """"""get all data from sockets""""""
    si = self.inputs
    parameters = []
    for socket in si:
        if len(socket.prop_name) > 0:
            parameters.append(socket.sv_get())
        else:
            parameters.append(socket.sv_get(default=[[]]))
    return match_long_repeat(parameters)
",if len ( socket . prop_name ) > 0 :,98
"def test_parse_query_params_comparable_field(self):
    query_params = {""filter[int_field][gt]"": 42, ""filter[int_field][lte]"": 9000}
    fields = self.view.parse_query_params(query_params)
    for key, field_name in fields.items():
        if field_name[""int_field""][""op""] == ""gt"":
            assert_equal(field_name[""int_field""][""value""], 42)
        elif field_name[""int_field""][""op""] == ""lte"":
            assert_equal(field_name[""int_field""][""value""], 9000)
        else:
            self.fail()
","elif field_name [ ""int_field"" ] [ ""op"" ] == ""lte"" :",168
"def _create_examples(self, lines, set_type):
    """"""Creates examples for the training and dev sets.""""""
    examples = []
    for (i, line) in enumerate(lines):
        if i == 0:
            continue
        guid = ""%s-%s"" % (set_type, i)
        text = line[0]
        bbox = line[1]
        label = line[2]
        examples.append(
            DocExample(guid=guid, text_a=text, text_b=None, bbox=bbox, label=label)
        )
    return examples
",if i == 0 :,149
"def _get_attr(sdk_path, mod_attr_path, checked=True):
    try:
        attr_mod, attr_path = (
            mod_attr_path.split(""#"") if ""#"" in mod_attr_path else (mod_attr_path, """")
        )
        full_mod_path = ""{}.{}"".format(sdk_path, attr_mod) if attr_mod else sdk_path
        op = import_module(full_mod_path)
        if attr_path:
            # Only load attributes if needed
            for part in attr_path.split("".""):
                op = getattr(op, part)
        return op
    except (ImportError, AttributeError) as ex:
        if checked:
            return None
        raise ex
",if attr_path :,191
"def _load_ui_modules(self, modules: Any) -> None:
    if isinstance(modules, types.ModuleType):
        self._load_ui_modules(dict((n, getattr(modules, n)) for n in dir(modules)))
    elif isinstance(modules, list):
        for m in modules:
            self._load_ui_modules(m)
    else:
        assert isinstance(modules, dict)
        for name, cls in modules.items():
            try:
                if issubclass(cls, UIModule):
                    self.ui_modules[name] = cls
            except TypeError:
                pass
","if issubclass ( cls , UIModule ) :",162
"def _remove_obsolete_leafs(input_dict):
    if not isinstance(input_dict, dict):
        return
    if input_dict[LEAF_MARKER]:
        bottom_leafs = input_dict[LEAF_MARKER]
        for leaf in bottom_leafs:
            if leaf in input_dict:
                input_dict[LEAF_MARKER].remove(leaf)
    for subtree in input_dict.keys():
        _remove_obsolete_leafs(input_dict[subtree])
",if leaf in input_dict :,124
"def decode(self, value, force=False):
    ""Return a unicode string from the bytes-like representation""
    if self.decode_responses or force:
        if isinstance(value, memoryview):
            value = value.tobytes()
        if isinstance(value, bytes):
            value = value.decode(self.encoding, self.encoding_errors)
    return value
","if isinstance ( value , memoryview ) :",91
"def audit(self, directive):
    value = _get_value(directive)
    if not value:
        return
    server_side = directive.name.startswith(""proxy_"")
    for var in compile_script(value):
        char = """"
        if var.can_contain(""\n""):
            char = ""\\n""
        elif not server_side and var.can_contain(""\r""):
            char = ""\\r""
        else:
            continue
        reason = 'At least variable ""${var}"" can contain ""{char}""'.format(
            var=var.name, char=char
        )
        self.add_issue(directive=[directive] + var.providers, reason=reason)
","if var . can_contain ( ""\n"" ) :",176
"def checkFilename(filename):  # useful in case of drag and drop
    while True:
        if filename[0] == ""'"":
            filename = filename[1:]
        if filename[len(filename) - 1] == ""'"":
            filename = filename[:-1]
        if os.path.exists(filename):
            return filename
        filename = input(
            ""[!] Cannot find '%s'.\n[*] Enter a valid name of the file containing the paths to test -> ""
            % filename
        )
","if filename [ len ( filename ) - 1 ] == ""'"" :",130
"def findfiles(self, dir, base, rec):
    try:
        names = os.listdir(dir or os.curdir)
    except os.error as msg:
        print(msg)
        return []
    list = []
    subdirs = []
    for name in names:
        fn = os.path.join(dir, name)
        if os.path.isdir(fn):
            subdirs.append(fn)
        else:
            if fnmatch.fnmatch(name, base):
                list.append(fn)
    if rec:
        for subdir in subdirs:
            list.extend(self.findfiles(subdir, base, rec))
    return list
",if os . path . isdir ( fn ) :,174
"def loop(handler, obj):
    handler.response.write(""<table>"")
    for k, v in obj.__dict__.items():
        if not k in (""data"", ""gae_user"", ""credentials"", ""content"", ""config""):
            style = ""color: red"" if not v else """"
            handler.response.write(
                '<tr style=""{}""><td>{}:</td><td>{}</td></tr>'.format(style, k, v)
            )
    handler.response.write(""</table>"")
","if not k in ( ""data"" , ""gae_user"" , ""credentials"" , ""content"" , ""config"" ) :",127
"def anypython(request):
    name = request.param
    executable = getexecutable(name)
    if executable is None:
        if sys.platform == ""win32"":
            executable = winpymap.get(name, None)
            if executable:
                executable = py.path.local(executable)
                if executable.check():
                    return executable
        pytest.skip(""no suitable %s found"" % (name,))
    return executable
",if executable . check ( ) :,119
"def __init__(self, socketpath=None):
    if socketpath is None:
        if sys.platform == ""darwin"":
            socketpath = ""/var/run/usbmuxd""
        else:
            socketpath = ""/var/run/usbmuxd""
    self.socketpath = socketpath
    self.listener = MuxConnection(socketpath, BinaryProtocol)
    try:
        self.listener.listen()
        self.version = 0
        self.protoclass = BinaryProtocol
    except MuxVersionError:
        self.listener = MuxConnection(socketpath, PlistProtocol)
        self.listener.listen()
        self.protoclass = PlistProtocol
        self.version = 1
    self.devices = self.listener.devices
","if sys . platform == ""darwin"" :",194
"def _validate_distinct_on_different_types_and_field_orders(
    self, collection, query, expected_results, get_mock_result
):
    self.count = 0
    self.get_mock_result = get_mock_result
    query_iterable = collection.query_items(query, enable_cross_partition_query=True)
    results = list(query_iterable)
    for i in range(len(expected_results)):
        if isinstance(results[i], dict):
            self.assertDictEqual(results[i], expected_results[i])
        elif isinstance(results[i], list):
            self.assertListEqual(results[i], expected_results[i])
        else:
            self.assertEqual(results[i], expected_results[i])
    self.count = 0
","if isinstance ( results [ i ] , dict ) :",196
"def getRootId(self, id):
    with self.connect() as cu:
        while True:
            stmt = ""select parent_path_id from hierarchy where path_id = ?""
            cu.execute(stmt, (id,))
            parent_id = cu.fetchone()[0]
            if parent_id is None or parent_id == id:
                return id
            id = parent_id
",if parent_id is None or parent_id == id :,109
"def add(self, path):
    with self.get_lock(path):
        if not path in self.entries:
            self.entries[path] = {}
            self.entries[path][""lock""] = self.new_locks[path]
            del self.new_locks[path]
            self.lru.append(path)
",if not path in self . entries :,88
"def _get_coordinates_for_dataset_key(self, dsid):
    """"""Get the coordinate dataset keys for *dsid*.""""""
    ds_info = self.ids[dsid]
    cids = []
    for cinfo in ds_info.get(""coordinates"", []):
        if not isinstance(cinfo, dict):
            cinfo = {""name"": cinfo}
        cinfo[""resolution""] = ds_info[""resolution""]
        if ""polarization"" in ds_info:
            cinfo[""polarization""] = ds_info[""polarization""]
        cid = DatasetID(**cinfo)
        cids.append(self.get_dataset_key(cid))
    return cids
","if ""polarization"" in ds_info :",170
"def build_from_gdobj(cls, gdobj, steal=False):
    # Avoid calling cls.__init__ by first instanciating a placeholder, then
    # overloading it __class__ to turn it into an instance of the right class
    ret = BuiltinInitPlaceholder()
    if steal:
        assert ffi.typeof(gdobj).kind == ""pointer""
        ret._gd_ptr = gdobj
    else:
        if ffi.typeof(gdobj).kind == ""pointer"":
            ret._gd_ptr = cls._copy_gdobj(gdobj)
        else:
            ret._gd_ptr = cls._copy_gdobj(ffi.addressof(gdobj))
    ret.__class__ = cls
    return ret
","if ffi . typeof ( gdobj ) . kind == ""pointer"" :",182
"def _listen_output(self):
    ""NB! works in background thread""
    try:
        while True:
            chars = self._proc.read(1)
            if len(chars) > 0:
                as_bytes = chars.encode(self.encoding)
                self._make_output_available(as_bytes)
            else:
                self._error = ""EOF""
                break
    except Exception as e:
        self._error = str(e)
",if len ( chars ) > 0 :,131
"def result(
    metrics: Dict[metric_types.MetricKey, Any]
) -> Dict[metric_types.AttributionsKey, Dict[Text, Union[float, np.ndarray]]]:
    """"""Returns mean attributions.""""""
    total_attributions = metrics[total_attributions_key]
    weighted_count = metrics[weighted_example_count_key]
    attributions = {}
    for k, v in total_attributions.items():
        if np.isclose(weighted_count, 0.0):
            attributions[k] = float(""nan"")
        else:
            attributions[k] = v / weighted_count
    return {key: attributions}
","if np . isclose ( weighted_count , 0.0 ) :",162
"def write_if_changed(path, data):
    if isinstance(data, str):
        data = data.encode()
    changed = False
    with open(os.open(path, os.O_CREAT | os.O_RDWR), ""wb+"") as f:
        f.seek(0)
        current = f.read()
        if current != data:
            changed = True
            f.seek(0)
            f.write(data)
            f.truncate()
        os.fsync(f)
    return changed
",if current != data :,138
"def detect_ssl_option(self):
    for option in self.ssl_options():
        if scan_argv(self.argv, option) is not None:
            for other_option in self.ssl_options():
                if option != other_option:
                    if scan_argv(self.argv, other_option) is not None:
                        raise ConfigurationError(
                            ""Cannot give both %s and %s"" % (option, other_option)
                        )
            return option
",if option != other_option :,140
"def _infer_return_type(*args):
    """"""Look at the type of all args and divine their implied return type.""""""
    return_type = None
    for arg in args:
        if arg is None:
            continue
        if isinstance(arg, bytes):
            if return_type is str:
                raise TypeError(""Can't mix bytes and non-bytes in "" ""path components."")
            return_type = bytes
        else:
            if return_type is bytes:
                raise TypeError(""Can't mix bytes and non-bytes in "" ""path components."")
            return_type = str
    if return_type is None:
        return str  # tempfile APIs return a str by default.
    return return_type
",if arg is None :,186
"def _get_app(self, body=None):
    app = self._app
    if app is None:
        try:
            tasks = self.tasks.tasks  # is a group
        except AttributeError:
            tasks = self.tasks
        if len(tasks):
            app = tasks[0]._app
        if app is None and body is not None:
            app = body._app
    return app if app is not None else current_app
",if len ( tasks ) :,117
"def add_field(self, field):
    self.remove_field(field.name)
    self.fields[field.name] = field
    self.columns[field.db_column] = field
    self._sorted_field_list.insert(field)
    self._update_field_lists()
    if field.default is not None:
        self.defaults[field] = field.default
        if callable(field.default):
            self._default_callables[field] = field.default
            self._default_callable_list.append((field.name, field.default))
        else:
            self._default_dict[field] = field.default
            self._default_by_name[field.name] = field.default
",if callable ( field . default ) :,184
"def _get_families(self):
    families = []
    for name, ext in self._get_family_dirs():
        if ext is None:  # is a directory
            family = self.get_resource(
                FileSystemPackageFamilyResource.key, location=self.location, name=name
            )
        else:
            family = self.get_resource(
                FileSystemCombinedPackageFamilyResource.key,
                location=self.location,
                name=name,
                ext=ext,
            )
        families.append(family)
    return families
",if ext is None :,157
"def test(model, data_loader, device=None):
    device = device or torch.device(""cpu"")
    model.eval()
    correct = 0
    total = 0
    with torch.no_grad():
        for batch_idx, (data, target) in enumerate(data_loader):
            if batch_idx * len(data) > TEST_SIZE:
                break
            data, target = data.to(device), target.to(device)
            outputs = model(data)
            _, predicted = torch.max(outputs.data, 1)
            total += target.size(0)
            correct += (predicted == target).sum().item()
    return correct / total
",if batch_idx * len ( data ) > TEST_SIZE :,175
"def __animate_progress(self):
    """"""Change the status message, mostly used to animate progress.""""""
    while True:
        sleep_time = ThreadPool.PROGRESS_IDLE_DELAY
        with self.__progress_lock:
            if not self.__progress_status:
                sleep_time = ThreadPool.PROGRESS_IDLE_DELAY
            elif self.__show_animation:
                self.__progress_status.update_progress(self.__current_operation_name)
                sleep_time = ThreadPool.PROGRESS_UPDATE_DELAY
            else:
                self.__progress_status.show_as_ready()
                sleep_time = ThreadPool.PROGRESS_IDLE_DELAY
        # Allow some time for progress status to be updated.
        time.sleep(sleep_time)
",if not self . __progress_status :,195
"def _parse_subtitles(self, video_data, url_key):
    subtitles = {}
    for translation in video_data.get(""translations"", []):
        vtt_path = translation.get(url_key)
        if not vtt_path:
            continue
        lang = translation.get(""language_w3c"") or ISO639Utils.long2short(
            translation[""language_medium""]
        )
        subtitles.setdefault(lang, []).append(
            {
                ""ext"": ""vtt"",
                ""url"": vtt_path,
            }
        )
    return subtitles
",if not vtt_path :,164
"def postprocess_message(self, msg):
    if msg[""type""] == ""sample"" and msg[""value""] is not None:
        fn, value = msg[""fn""], msg[""value""]
        value_batch_ndims = jnp.ndim(value) - fn.event_dim
        fn_batch_ndim = len(fn.batch_shape)
        if fn_batch_ndim < value_batch_ndims:
            prepend_shapes = (1,) * (value_batch_ndims - fn_batch_ndim)
            msg[""fn""] = tree_map(
                lambda x: jnp.reshape(x, prepend_shapes + jnp.shape(x)), fn
            )
",if fn_batch_ndim < value_batch_ndims :,170
"def TryMerge(self, d):
    while d.avail() > 0:
        tt = d.getVarInt32()
        if tt == 10:
            self.set_filename(d.getPrefixedString())
            continue
        if tt == 0:
            raise ProtocolBuffer.ProtocolBufferDecodeError
        d.skipData(tt)
",if tt == 10 :,90
"def createError(self, line, pos, description):
    global ENABLE_PYIMPORT
    msg = ""Line "" + unicode(line) + "": "" + unicode(description)
    if ENABLE_JS2PY_ERRORS:
        if isinstance(ENABLE_JS2PY_ERRORS, bool):
            import js2py.base
            return js2py.base.MakeError(""SyntaxError"", msg)
        else:
            return ENABLE_JS2PY_ERRORS(msg)
    else:
        return JsSyntaxError(msg)
","if isinstance ( ENABLE_JS2PY_ERRORS , bool ) :",129
"def extract(self, page, start_index=0, end_index=None):
    items = []
    for extractor in self.extractors:
        extracted = extractor.extract(
            page, start_index, end_index, self.template.ignored_regions
        )
        for item in arg_to_iter(extracted):
            if item:
                if isinstance(item, (ItemProcessor, dict)):
                    item[u""_template""] = self.template.id
                items.append(item)
    return items
",if item :,141
"def create_volume(self, volume):
    """"""Create a volume.""""""
    try:
        cmd = [""volume"", ""create"", volume[""name""], ""%sG"" % (volume[""size""])]
        if self.configuration.eqlx_pool != ""default"":
            cmd.append(""pool"")
            cmd.append(self.configuration.eqlx_pool)
        if self.configuration.san_thin_provision:
            cmd.append(""thin-provision"")
        out = self._eql_execute(*cmd)
        self.add_multihost_access(volume)
        return self._get_volume_data(out)
    except Exception:
        with excutils.save_and_reraise_exception():
            LOG.error('Failed to create volume ""%s"".', volume[""name""])
","if self . configuration . eqlx_pool != ""default"" :",199
"def clean(self):
    # TODO: check for clashes if the random code is already taken
    if not self.code:
        self.code = u""static-%s"" % uuid.uuid4()
    if not self.site:
        placeholders = StaticPlaceholder.objects.filter(
            code=self.code, site__isnull=True
        )
        if self.pk:
            placeholders = placeholders.exclude(pk=self.pk)
        if placeholders.exists():
            raise ValidationError(
                _(""A static placeholder with the same site and code already exists"")
            )
",if self . pk :,149
"def spawnMenu(self, event):
    clickedPos = self.getRowByAbs(event.Position)
    self.ensureSelection(clickedPos)
    selection = self.getSelectedBoosters()
    mainBooster = None
    if clickedPos != -1:
        try:
            booster = self.boosters[clickedPos]
        except IndexError:
            pass
        else:
            if booster in self.original:
                mainBooster = booster
    itemContext = None if mainBooster is None else _t(""Booster"")
    menu = ContextMenu.getMenu(
        self,
        mainBooster,
        selection,
        (""boosterItem"", itemContext),
        (""boosterItemMisc"", itemContext),
    )
    if menu:
        self.PopupMenu(menu)
",if booster in self . original :,199
"def init_errorhandler():
    # http error handling
    for ex in default_exceptions:
        if ex < 500:
            app.register_error_handler(ex, error_http)
        elif ex == 500:
            app.register_error_handler(ex, internal_error)
    if services.ldap:
        # Only way of catching the LDAPException upon logging in with LDAP server down
        @app.errorhandler(services.ldap.LDAPException)
        def handle_exception(e):
            log.debug(""LDAP server not accessible while trying to login to opds feed"")
            return error_http(FailedDependency())
",if ex < 500 :,168
"def reloadCols(self):
    self.columns = []
    for i, (name, fmt, *shape) in enumerate(self.npy.dtype.descr):
        if shape:
            t = anytype
        elif ""M"" in fmt:
            self.addColumn(Column(name, type=date, getter=lambda c, r, i=i: str(r[i])))
            continue
        elif ""i"" in fmt:
            t = int
        elif ""f"" in fmt:
            t = float
        else:
            t = anytype
        self.addColumn(ColumnItem(name, i, type=t))
",if shape :,168
"def Proc2(IntParIO):
    IntLoc = IntParIO + 10
    while True:
        if Char1Glob == ""A"":
            IntLoc = IntLoc - 1
            IntParIO = IntLoc - IntGlob
            EnumLoc = Ident1
        if EnumLoc == Ident1:
            break
    return IntParIO
",if EnumLoc == Ident1 :,90
"def opengroup(self, name=None):
    gid = self.groups
    self.groupwidths.append(None)
    if self.groups > MAXGROUPS:
        raise error(""too many groups"")
    if name is not None:
        ogid = self.groupdict.get(name, None)
        if ogid is not None:
            raise error(
                ""redefinition of group name %r as group %d; ""
                ""was group %d"" % (name, gid, ogid)
            )
        self.groupdict[name] = gid
    return gid
",if ogid is not None :,148
"def __setattr__(self, name: str, val: Any):
    if name.startswith(""COMPUTED_""):
        if name in self:
            old_val = self[name]
            if old_val == val:
                return
            raise KeyError(
                ""Computed attributed '{}' already exists ""
                ""with a different value! old={}, new={}."".format(name, old_val, val)
            )
        self[name] = val
    else:
        super().__setattr__(name, val)
",if old_val == val :,137
"def get_all_function_symbols(self, module=""kernel""):
    """"""Gets all the function tuples for the given module""""""
    ret = []
    symtable = self.type_map
    if module in symtable:
        mod = symtable[module]
        for (addr, (name, _sym_types)) in mod.items():
            if self.shift_address and addr:
                addr = addr + self.shift_address
            ret.append([name, addr])
    else:
        debug.info(""All symbols requested for non-existent module %s"" % module)
    return ret
",if self . shift_address and addr :,147
"def __call__(self, frame: FrameType, event: str, arg: Any) -> ""CallTracer"":
    code = frame.f_code
    if (
        event not in SUPPORTED_EVENTS
        or code.co_name == ""trace_types""
        or self.should_trace
        and not self.should_trace(code)
    ):
        return self
    try:
        if event == EVENT_CALL:
            self.handle_call(frame)
        elif event == EVENT_RETURN:
            self.handle_return(frame, arg)
        else:
            logger.error(""Cannot handle event %s"", event)
    except Exception:
        logger.exception(""Failed collecting trace"")
    return self
",if event == EVENT_CALL :,185
"def test_update_topic(self):
    async with self.chat_client:
        await self._create_thread()
        topic = ""update topic""
        async with self.chat_thread_client:
            await self.chat_thread_client.update_topic(topic=topic)
        # delete chat threads
        if not self.is_playback():
            await self.chat_client.delete_chat_thread(self.thread_id)
",if not self . is_playback ( ) :,114
"def render_observation(self):
    x = self.read_head_position
    label = ""Observation Grid    : ""
    x_str = """"
    for j in range(-1, self.rows + 1):
        if j != -1:
            x_str += "" "" * len(label)
        for i in range(-2, self.input_width + 2):
            if i == x[0] and j == x[1]:
                x_str += colorize(self._get_str_obs((i, j)), ""green"", highlight=True)
            else:
                x_str += self._get_str_obs((i, j))
        x_str += ""\n""
    x_str = label + x_str
    return x_str
",if i == x [ 0 ] and j == x [ 1 ] :,200
"def build(opt):
    dpath = os.path.join(opt[""datapath""], ""QA-ZRE"")
    version = None
    if not build_data.built(dpath, version_string=version):
        print(""[building data: "" + dpath + ""]"")
        if build_data.built(dpath):
            # An older version exists, so remove these outdated files.
            build_data.remove_dir(dpath)
        build_data.make_dir(dpath)
        # Download the data.
        for downloadable_file in RESOURCES:
            downloadable_file.download_file(dpath)
        # Mark the data as built.
        build_data.mark_done(dpath, version_string=version)
",if build_data . built ( dpath ) :,183
"def git_pull(args):
    if len(args) <= 1:
        repo = _get_repo()
        _confirm_dangerous()
        url = args[0] if len(args) == 1 else repo.remotes.get(""origin"", """")
        if url in repo.remotes:
            origin = url
            url = repo.remotes.get(origin)
        if url:
            repo.pull(origin_uri=url)
        else:
            print(""No pull URL."")
    else:
        print(command_help[""git pull""])
",if url in repo . remotes :,147
"def FindAndDelete(script, sig):
    """"""Consensus critical, see FindAndDelete() in Satoshi codebase""""""
    r = b""""
    last_sop_idx = sop_idx = 0
    skip = True
    for (opcode, data, sop_idx) in script.raw_iter():
        if not skip:
            r += script[last_sop_idx:sop_idx]
        last_sop_idx = sop_idx
        if script[sop_idx : sop_idx + len(sig)] == sig:
            skip = True
        else:
            skip = False
    if not skip:
        r += script[last_sop_idx:]
    return CScript(r)
",if not skip :,187
"def get_ip_info(ipaddress):
    """"""Returns device information by IP address""""""
    result = {}
    try:
        ip = IPAddress.objects.select_related().get(address=ipaddress)
    except IPAddress.DoesNotExist:
        pass
    else:
        if ip.venture is not None:
            result[""venture_id""] = ip.venture.id
        if ip.device is not None:
            result[""device_id""] = ip.device.id
            if ip.device.venture is not None:
                result[""venture_id""] = ip.device.venture.id
    return result
",if ip . device . venture is not None :,162
"def restore(self, state):
    """"""Restore the state of a mesh previously saved using save()""""""
    import pickle
    state = pickle.loads(state)
    for k in state:
        if isinstance(state[k], list):
            if isinstance(state[k][0], QtGui.QVector3D):
                state[k] = [[v.x(), v.y(), v.z()] for v in state[k]]
            state[k] = np.array(state[k])
        setattr(self, k, state[k])
","if isinstance ( state [ k ] [ 0 ] , QtGui . QVector3D ) :",135
"def get_extra_lines(tup):
    ext_name, pyopencl_ver = tup
    if ext_name is not None:
        if ext_name.startswith(""CL_""):
            # capital letters -> CL version, not extension
            yield """"
            yield ""    Available with OpenCL %s."" % (ext_name[3:])
            yield """"
        else:
            yield """"
            yield ""    Available with the ``%s`` extension."" % ext_name
            yield """"
    if pyopencl_ver is not None:
        yield """"
        yield ""    .. versionadded:: %s"" % pyopencl_ver
        yield """"
","if ext_name . startswith ( ""CL_"" ) :",174
"def _gen_remote_uri(
    fileobj: IO[bytes],
    remote_uri: Optional[ParseResult],
    remote_path_prefix: Optional[str],
    remote_path_suffix: Optional[str],
    sha256sum: Optional[str],
) -> ParseResult:
    if remote_uri is None:
        assert remote_path_prefix is not None and remote_path_suffix is not None
        if sha256sum is None:
            sha256sum = _hash_fileobj(fileobj)
        return urlparse(
            os.path.join(remote_path_prefix, f""{sha256sum}{remote_path_suffix}"")
        )
    else:
        return remote_uri
",if sha256sum is None :,171
"def queries(self):
    if DEV:
        cmd = ShellCommand(""docker"", ""ps"", ""-qf"", ""name=%s"" % self.path.k8s)
        if not cmd.check(f""docker check for {self.path.k8s}""):
            if not cmd.stdout.strip():
                log_cmd = ShellCommand(
                    ""docker"", ""logs"", self.path.k8s, stderr=subprocess.STDOUT
                )
                if log_cmd.check(f""docker logs for {self.path.k8s}""):
                    print(cmd.stdout)
                pytest.exit(f""container failed to start for {self.path.k8s}"")
    return ()
",if not cmd . stdout . strip ( ) :,188
"def get_range(self):
    present = self.xml.find(""{%s}range"" % self.namespace)
    if present is not None:
        attributes = present.attrib
        return_value = dict()
        if ""min"" in attributes:
            return_value[""minimum""] = attributes[""min""]
        if ""max"" in attributes:
            return_value[""maximum""] = attributes[""max""]
        return return_value
    return False
","if ""min"" in attributes :",113
"def _configuredOn(self, workerid, builderid=None, masterid=None):
    cfg = []
    for cs in itervalues(self.configured):
        if cs[""workerid""] != workerid:
            continue
        bid, mid = self.db.builders.builder_masters[cs[""buildermasterid""]]
        if builderid is not None and bid != builderid:
            continue
        if masterid is not None and mid != masterid:
            continue
        cfg.append({""builderid"": bid, ""masterid"": mid})
    return cfg
","if cs [ ""workerid"" ] != workerid :",143
"def __exit__(self, type, value, traceback):
    try:
        if type is not None:
            return self.exception_handler(type, value, traceback)
    finally:
        final_contexts = _state.contexts
        _state.contexts = self.old_contexts
        if final_contexts is not self.new_contexts:
            raise StackContextInconsistentError(
                ""stack_context inconsistency (may be caused by yield ""
                'within a ""with StackContext"" block)'
            )
        # Break up a reference to itself to allow for faster GC on CPython.
        self.new_contexts = None
",if final_contexts is not self . new_contexts :,162
"def del_(self, key):
    initial_hash = hash_ = self.hash(key)
    while True:
        if self._keys[hash_] is self._empty:
            # That key was never assigned
            return None
        elif self._keys[hash_] == key:
            # key found, assign with deleted sentinel
            self._keys[hash_] = self._deleted
            self._values[hash_] = self._deleted
            self._len -= 1
            return
        hash_ = self._rehash(hash_)
        if initial_hash == hash_:
            # table is full and wrapped around
            return None
",if self . _keys [ hash_ ] is self . _empty :,166
"def TryMerge(self, d):
    while d.avail() > 0:
        tt = d.getVarInt32()
        if tt == 10:
            self.set_logout_url(d.getPrefixedString())
            continue
        if tt == 0:
            raise ProtocolBuffer.ProtocolBufferDecodeError
        d.skipData(tt)
",if tt == 10 :,92
"def data_generator():
    i = 0
    max_batch_index = len(X_train) // batch_size
    tot = 0
    while 1:
        if tot > 3 * len(X_train):
            yield (
                np.ones([batch_size, input_dim]) * np.nan,
                np.ones([batch_size, num_classes]) * np.nan,
            )
        else:
            yield (
                X_train[i * batch_size : (i + 1) * batch_size],
                y_train[i * batch_size : (i + 1) * batch_size],
            )
        i += 1
        tot += 1
        i = i % max_batch_index
",if tot > 3 * len ( X_train ) :,198
"def title(self):
    ret = theme[""title""]
    if isinstance(self.name, six.string_types):
        width = self.statwidth()
        return (
            ret + self.name[0:width].center(width).replace("" "", ""-"") + theme[""default""]
        )
    for i, name in enumerate(self.name):
        width = self.colwidth()
        ret = ret + name[0:width].center(width).replace("" "", ""-"")
        if i + 1 != len(self.vars):
            if op.color:
                ret = ret + theme[""frame""] + char[""dash""] + theme[""title""]
            else:
                ret = ret + char[""space""]
    return ret
",if i + 1 != len ( self . vars ) :,188
"def get_container_from_dport(dport, docker_client):
    for container in docker_client.containers():
        try:
            ports = container[""Ports""]
            for port in ports:
                if ""PublicPort"" in port:
                    if port[""PublicPort""] == int(dport):
                        return container
        except KeyError:
            print(ports)
            pass
","if ""PublicPort"" in port :",112
"def _get_parents_data(self, data):
    parents = 0
    if data[COLUMN_PARENT]:
        family = self.db.get_family_from_handle(data[COLUMN_PARENT][0])
        if family.get_father_handle():
            parents += 1
        if family.get_mother_handle():
            parents += 1
    return parents
",if family . get_mother_handle ( ) :,98
"def wrapper(filename):
    mtime = getmtime(filename)
    with lock:
        if filename in cache:
            old_mtime, result = cache.pop(filename)
            if old_mtime == mtime:
                # Move to the end
                cache[filename] = old_mtime, result
                return result
    result = function(filename)
    with lock:
        cache[filename] = mtime, result  # at the end
        if len(cache) > max_size:
            cache.popitem(last=False)
    return result
",if len ( cache ) > max_size :,144
"def execute(cls, ctx, op: ""DataFrameGroupByAgg""):
    try:
        pd.set_option(""mode.use_inf_as_na"", op.use_inf_as_na)
        if op.stage == OperandStage.map:
            cls._execute_map(ctx, op)
        elif op.stage == OperandStage.combine:
            cls._execute_combine(ctx, op)
        elif op.stage == OperandStage.agg:
            cls._execute_agg(ctx, op)
        else:  # pragma: no cover
            raise ValueError(""Aggregation operand not executable"")
    finally:
        pd.reset_option(""mode.use_inf_as_na"")
",elif op . stage == OperandStage . agg :,171
"def FindAndDelete(script, sig):
    """"""Consensus critical, see FindAndDelete() in Satoshi codebase""""""
    r = b""""
    last_sop_idx = sop_idx = 0
    skip = True
    for (opcode, data, sop_idx) in script.raw_iter():
        if not skip:
            r += script[last_sop_idx:sop_idx]
        last_sop_idx = sop_idx
        if script[sop_idx : sop_idx + len(sig)] == sig:
            skip = True
        else:
            skip = False
    if not skip:
        r += script[last_sop_idx:]
    return CScript(r)
",if script [ sop_idx : sop_idx + len ( sig ) ] == sig :,187
"def extractall(zip: typing.Any, path: str) -> NoneType:
    for name in zip.namelist():
        member = zip.getinfo(name)
        extracted_path = zip._extract_member(member, path, None)
        attr = member.external_attr >> 16
        if attr != 0:
            os.chmod(extracted_path, attr)
",if attr != 0 :,93
"def find_all_gyptest_files(directory):
    result = []
    for root, dirs, files in os.walk(directory):
        if "".svn"" in dirs:
            dirs.remove("".svn"")
        result.extend([os.path.join(root, f) for f in files if is_test_name(f)])
    result.sort()
    return result
","if "".svn"" in dirs :",94
"def load(cls, storefile, template_store):
    # Did we get file or filename?
    if not hasattr(storefile, ""read""):
        storefile = open(storefile, ""rb"")
    # Adjust store to have translations
    store = cls.convertfile(storefile, template_store)
    for unit in store.units:
        if unit.isheader():
            continue
        # HTML does this properly on loading, others need it
        if cls.needs_target_sync:
            unit.target = unit.source
            unit.rich_target = unit.rich_source
    return store
",if cls . needs_target_sync :,152
"def postOptions(self):
    _BasicOptions.postOptions(self)
    if self[""jobs""]:
        conflicts = [""debug"", ""profile"", ""debug-stacktraces"", ""exitfirst""]
        for option in conflicts:
            if self[option]:
                raise usage.UsageError(
                    ""You can't specify --%s when using --jobs"" % option
                )
    if self[""nopm""]:
        if not self[""debug""]:
            raise usage.UsageError(""You must specify --debug when using "" ""--nopm "")
        failure.DO_POST_MORTEM = False
","if not self [ ""debug"" ] :",151
"def filterTokenLocation():
    i = None
    entry = None
    token = None
    tokens = []
    i = 0
    while 1:
        if not (i < len(extra.tokens)):
            break
        entry = extra.tokens[i]
        token = jsdict(
            {
                ""type"": entry.type,
                ""value"": entry.value,
            }
        )
        if extra.range:
            token.range = entry.range
        if extra.loc:
            token.loc = entry.loc
        tokens.append(token)
        i += 1
    extra.tokens = tokens
",if extra . loc :,172
"def on_rebalance_end(self) -> None:
    """"""Call when rebalancing is done.""""""
    self.rebalancing = False
    if self._rebalancing_span:
        self._rebalancing_span.finish()
    self._rebalancing_span = None
    sensor_state = self._rebalancing_sensor_state
    try:
        if not sensor_state:
            self.log.warning(
                ""Missing sensor state for rebalance #%s"", self.rebalancing_count
            )
        else:
            self.sensors.on_rebalance_end(self, sensor_state)
    finally:
        self._rebalancing_sensor_state = None
",if not sensor_state :,184
"def decorator(request, *args, **kwargs):
    if CALENDAR_VIEW_PERM:
        user = request.user
        if not user:
            return HttpResponseRedirect(settings.LOGIN_URL)
        occurrence, event, calendar = get_objects(request, **kwargs)
        if calendar:
            allowed = CHECK_CALENDAR_PERM_FUNC(calendar, user)
            if not allowed:
                return HttpResponseRedirect(settings.LOGIN_URL)
            # all checks passed
            return function(request, *args, **kwargs)
        return HttpResponseNotFound(""<h1>Page not found</h1>"")
    return function(request, *args, **kwargs)
",if not allowed :,170
"def reduce_arguments(self, args):
    assert isinstance(args, nodes.Arguments)
    if args.incorrect_order():
        raise InvalidArguments(
            ""All keyword arguments must be after positional arguments.""
        )
    reduced_pos = [self.reduce_single(arg) for arg in args.arguments]
    reduced_kw = {}
    for key in args.kwargs.keys():
        if not isinstance(key, str):
            raise InvalidArguments(""Keyword argument name is not a string."")
        a = args.kwargs[key]
        reduced_kw[key] = self.reduce_single(a)
    return (reduced_pos, reduced_kw)
","if not isinstance ( key , str ) :",163
"def _encode(n, nbytes, little_endian=False):
    retval = []
    n = long(n)
    for i in range(nbytes):
        if little_endian:
            retval.append(chr(n & 0xFF))
        else:
            retval.insert(0, chr(n & 0xFF))
        n >>= 8
    return """".join(retval)
",if little_endian :,96
"def copy_shell(self):
    cls = self.__class__
    old_id = cls.id
    new_i = cls()  # create a new group
    new_i.id = self.id  # with the same id
    cls.id = old_id  # Reset the Class counter
    # Copy all properties
    for prop in cls.properties:
        if prop is not ""members"":
            if self.has(prop):
                val = getattr(self, prop)
                setattr(new_i, prop, val)
    # but no members
    new_i.members = []
    return new_i
",if self . has ( prop ) :,156
"def dataspec(config):
    master = yield fakemaster.make_master()
    data = connector.DataConnector()
    data.setServiceParent(master)
    if config[""out""] != ""--"":
        dirs = os.path.dirname(config[""out""])
        if dirs and not os.path.exists(dirs):
            os.makedirs(dirs)
        f = open(config[""out""], ""w"")
    else:
        f = sys.stdout
    if config[""global""] is not None:
        f.write(""window."" + config[""global""] + ""="")
    f.write(json.dumps(data.allEndpoints(), indent=2))
    f.close()
    defer.returnValue(0)
",if dirs and not os . path . exists ( dirs ) :,176
"def _parseSCDOCDC(self, src):
    """"""[S|CDO|CDC]*""""""
    while 1:
        src = src.lstrip()
        if src.startswith(""<!--""):
            src = src[4:]
        elif src.startswith(""-->""):
            src = src[3:]
        else:
            break
    return src
","if src . startswith ( ""<!--"" ) :",92
"def command(filenames, dirnames, fix):
    for filename in gather_files(dirnames, filenames):
        visitor = process_file(filename)
        if visitor.needs_fix():
            print(""%s: %s"" % (filename, visitor.get_stats()))
            if fix:
                print(""Fixing: %s"" % filename)
                fix_file(filename)
",if fix :,100
"def shutdown(self):
    """"""Shutdown host system.""""""
    self._check_dbus(MANAGER)
    use_logind = self.sys_dbus.logind.is_connected
    _LOGGER.info(""Initialize host power off %s"", ""logind"" if use_logind else ""systemd"")
    try:
        await self.sys_core.shutdown()
    finally:
        if use_logind:
            await self.sys_dbus.logind.power_off()
        else:
            await self.sys_dbus.systemd.power_off()
",if use_logind :,140
"def _run_split_on_punc(self, text, never_split=None):
    """"""Splits punctuation on a piece of text.""""""
    if never_split is not None and text in never_split:
        return [text]
    chars = list(text)
    i = 0
    start_new_word = True
    output = []
    while i < len(chars):
        char = chars[i]
        if _is_punctuation(char):
            output.append([char])
            start_new_word = True
        else:
            if start_new_word:
                output.append([])
            start_new_word = False
            output[-1].append(char)
        i += 1
    return ["""".join(x) for x in output]
",if start_new_word :,199
"def _terminal_messenger(tp=""write"", msg="""", out=sys.stdout):
    try:
        if tp == ""write"":
            out.write(msg)
        elif tp == ""flush"":
            out.flush()
        elif tp == ""write_flush"":
            out.write(msg)
            out.flush()
        elif tp == ""print"":
            print(msg, file=out)
        else:
            raise ValueError(""Unsupported type: "" + tp)
    except IOError as e:
        logger.critical(""{}: {}"".format(type(e).__name__, ucd(e)))
        pass
","elif tp == ""flush"" :",160
"def checkClassDeclation(file):
    localResult = []
    with open(file, ""rb"") as f:
        lineNumber = 0
        for line in f:
            m = re.search(""class\s+[^\(]*:"", line)
            if m:
                localResult.append(
                    ""Old class definition found on {0}"".format(m.group())
                )
    return localResult
",if m :,112
"def _evaluate_local_single(self, iterator):
    for batch in iterator:
        in_arrays = convert._call_converter(self.converter, batch, self.device)
        with function.no_backprop_mode():
            if isinstance(in_arrays, tuple):
                results = self.calc_local(*in_arrays)
            elif isinstance(in_arrays, dict):
                results = self.calc_local(**in_arrays)
            else:
                results = self.calc_local(in_arrays)
        if self._progress_hook:
            self._progress_hook(batch)
        yield results
","elif isinstance ( in_arrays , dict ) :",166
"def check_billing_view(user, permission, obj):
    if hasattr(obj, ""all_projects""):
        if user.is_superuser or obj.owners.filter(pk=user.pk).exists():
            return True
        # This is a billing object
        return any(check_permission(user, permission, prj) for prj in obj.all_projects)
    return check_permission(user, permission, obj)
",if user . is_superuser or obj . owners . filter ( pk = user . pk ) . exists ( ) :,106
"def ensure_output_spaces_contain_the_same_data(self, y, y_ensured):
    stride = y.shape[1]
    self.assertEqual(y.shape[0] * y.shape[1], y_ensured.shape[0])
    self.assertEqual(len(y_ensured.shape), 1)
    for row in range(y.shape[0]):
        for column in range(y.shape[1]):
            if sp.issparse(y):
                self.assertEqual(y[row, column], y_ensured[row * stride + column])
            else:
                self.assertEqual(y[row][column], y_ensured[row * stride + column])
",if sp . issparse ( y ) :,176
"def train(
    self,
    training_data: TrainingData,
    config: Optional[RasaNLUModelConfig] = None,
    **kwargs: Any,
) -> None:
    """"""Tokenize all training data.""""""
    for example in training_data.training_examples:
        for attribute in MESSAGE_ATTRIBUTES:
            if example.get(attribute) is not None and not example.get(attribute) == """":
                if attribute in [INTENT, ACTION_NAME, INTENT_RESPONSE_KEY]:
                    tokens = self._split_name(example, attribute)
                else:
                    tokens = self.tokenize(example, attribute)
                example.set(TOKENS_NAMES[attribute], tokens)
","if attribute in [ INTENT , ACTION_NAME , INTENT_RESPONSE_KEY ] :",183
"def refresh_token(self, strategy, *args, **kwargs):
    token = self.extra_data.get(""refresh_token"") or self.extra_data.get(""access_token"")
    backend = self.get_backend(strategy)
    if token and backend and hasattr(backend, ""refresh_token""):
        backend = backend(strategy=strategy)
        response = backend.refresh_token(token, *args, **kwargs)
        extra_data = backend.extra_data(self, self.uid, response, self.extra_data)
        if self.set_extra_data(extra_data):
            self.save()
",if self . set_extra_data ( extra_data ) :,154
"def _verify_environ(_collected_environ):
    try:
        yield
    finally:
        new_environ = dict(os.environ)
        current_test = new_environ.pop(""PYTEST_CURRENT_TEST"", None)
        old_environ = dict(_collected_environ)
        old_environ.pop(""PYTEST_CURRENT_TEST"", None)
        if new_environ != old_environ:
            raise DirtyTest(
                ""Left over environment variables"",
                current_test,
                _compare_eq_dict(new_environ, old_environ, verbose=2),
            )
",if new_environ != old_environ :,157
"def clean_len(self, line):
    """"""Calculate wisible length of string""""""
    if isinstance(line, basestring):
        return len(self.screen.markup.clean_markup(line))
    elif isinstance(line, tuple) or isinstance(line, list):
        markups = self.screen.markup.get_markup_vars()
        length = 0
        for i in line:
            if i not in markups:
                length += len(i)
        return length
",if i not in markups :,123
"def _build_merged_dataset_args(datasets):
    merged_dataset_args = []
    for dataset in datasets:
        dataset_code_column = _parse_dataset_code(dataset)
        arg = dataset_code_column[""code""]
        column_index = dataset_code_column[""column_index""]
        if column_index is not None:
            arg = (dataset_code_column[""code""], {""column_index"": [column_index]})
        merged_dataset_args.append(arg)
    return merged_dataset_args
",if column_index is not None :,134
"def update_watch_data_table_paths(self):
    if hasattr(self.tool_data_watcher, ""monitored_dirs""):
        for tool_data_table_path in self.tool_data_paths:
            if tool_data_table_path not in self.tool_data_watcher.monitored_dirs:
                self.tool_data_watcher.watch_directory(tool_data_table_path)
",if tool_data_table_path not in self . tool_data_watcher . monitored_dirs :,107
"def getsource(obj):
    """"""Wrapper around inspect.getsource""""""
    try:
        try:
            src = encoding.to_unicode(inspect.getsource(obj))
        except TypeError:
            if hasattr(obj, ""__class__""):
                src = encoding.to_unicode(inspect.getsource(obj.__class__))
            else:
                # Bindings like VTK or ITK require this case
                src = getdoc(obj)
        return src
    except (TypeError, IOError):
        return
","if hasattr ( obj , ""__class__"" ) :",134
"def __iter__(self):
    for model in self.app_config.get_models():
        admin_model = AdminModel(model, **self.options)
        for model_re in self.model_res:
            if model_re.search(admin_model.name):
                break
        else:
            if self.model_res:
                continue
        yield admin_model
",if self . model_res :,105
"def run(self):
    while True:
        try:
            with DelayedKeyboardInterrupt():
                raw_inputs = self._parent_task_queue.get()
                if self._has_stop_signal(raw_inputs):
                    self._rq.put(raw_inputs, block=True)
                    break
                if self._flow_type == BATCH:
                    self._rq.put(raw_inputs, block=True)
                elif self._flow_type == REALTIME:
                    try:
                        self._rq.put(raw_inputs, block=False)
                    except:
                        pass
        except KeyboardInterrupt:
            continue
",elif self . _flow_type == REALTIME :,199
"def dump(self):
    self.ql.log.info(""[*] Dumping object: %s"" % (self.sf_name))
    for field in self._fields_:
        if isinstance(getattr(self, field[0]), POINTER64):
            self.ql.log.info(""%s: 0x%x"" % (field[0], getattr(self, field[0]).value))
        elif isinstance(getattr(self, field[0]), int):
            self.ql.log.info(""%s: %d"" % (field[0], getattr(self, field[0])))
        elif isinstance(getattr(self, field[0]), bytes):
            self.ql.log.info(""%s: %s"" % (field[0], getattr(self, field[0]).decode()))
","if isinstance ( getattr ( self , field [ 0 ] ) , POINTER64 ) :",188
"def validate_configuration(self, configuration: Optional[ExpectationConfiguration]):
    """"""Validating that user has inputted a value set and that configuration has been initialized""""""
    super().validate_configuration(configuration)
    try:
        assert ""value_set"" in configuration.kwargs, ""value_set is required""
        assert isinstance(
            configuration.kwargs[""value_set""], (list, set, dict)
        ), ""value_set must be a list or a set""
        if isinstance(configuration.kwargs[""value_set""], dict):
            assert (
                ""$PARAMETER"" in configuration.kwargs[""value_set""]
            ), 'Evaluation Parameter dict for value_set kwarg must have ""$PARAMETER"" key'
    except AssertionError as e:
        raise InvalidExpectationConfigurationError(str(e))
    return True
","if isinstance ( configuration . kwargs [ ""value_set"" ] , dict ) :",196
"def test_one_dead_branch():
    with deterministic_PRNG():
        seen = set()
        @run_to_buffer
        def x(data):
            i = data.draw_bytes(1)[0]
            if i > 0:
                data.mark_invalid()
            i = data.draw_bytes(1)[0]
            if len(seen) < 255:
                seen.add(i)
            elif i not in seen:
                data.mark_interesting()
",elif i not in seen :,138
"def __on_item_activated(self, event):
    if self.__module_view:
        module = self.get_event_module(event)
        self.__module_view.set_selection(module.module_num)
        if event.EventObject is self.list_ctrl:
            self.input_list_ctrl.deactivate_active_item()
        else:
            self.list_ctrl.deactivate_active_item()
            for index in range(self.list_ctrl.GetItemCount()):
                if self.list_ctrl.IsSelected(index):
                    self.list_ctrl.Select(index, False)
    self.__controller.enable_module_controls_panel_buttons()
",if self . list_ctrl . IsSelected ( index ) :,181
"def prime(self, callback):
    if self.cbhdl is None:
        # import pdb
        # pdb.set_trace()
        self.cbhdl = simulator.register_rwsynch_callback(callback, self)
        if self.cbhdl is None:
            raise_error(self, ""Unable set up %s Trigger"" % (str(self)))
    Trigger.prime(self)
",if self . cbhdl is None :,102
"def fstab_configuration(middleware):
    for command in (
        [
            [""systemctl"", ""daemon-reload""],
            [""systemctl"", ""restart"", ""local-fs.target""],
        ]
        if osc.IS_LINUX
        else [[""mount"", ""-uw"", ""/""]]
    ):
        ret = subprocess.run(command, capture_output=True)
        if ret.returncode:
            middleware.logger.debug(
                f'Failed to execute ""{"" "".join(command)}"": {ret.stderr.decode()}'
            )
",if ret . returncode :,148
"def _generate_table(self, fromdesc, todesc, diffs):
    if fromdesc or todesc:
        yield (
            simple_colorize(fromdesc, ""description""),
            simple_colorize(todesc, ""description""),
        )
    for i, line in enumerate(diffs):
        if line is None:
            # mdiff yields None on separator lines; skip the bogus ones
            # generated for the first line
            if i > 0:
                yield (
                    simple_colorize(""---"", ""separator""),
                    simple_colorize(""---"", ""separator""),
                )
        else:
            yield line
",if line is None :,170
"def update_completion(self):
    """"""Update completion model with exist tags""""""
    orig_text = self.widget.text()
    text = "", "".join(orig_text.replace("", "", "","").split("","")[:-1])
    tags = []
    for tag in self.tags_list:
        if "","" in orig_text:
            if orig_text[-1] not in ("","", "" ""):
                tags.append(""%s,%s"" % (text, tag))
            tags.append(""%s, %s"" % (text, tag))
        else:
            tags.append(tag)
    if tags != self.completer_model.stringList():
        self.completer_model.setStringList(tags)
","if orig_text [ - 1 ] not in ( "","" , "" "" ) :",177
"def cart_number_checksum_validation(cls, number):
    digits = []
    even = False
    if not number.isdigit():
        return False
    for digit in reversed(number):
        digit = ord(digit) - ord(""0"")
        if even:
            digit *= 2
            if digit >= 10:
                digit = digit % 10 + digit // 10
        digits.append(digit)
        even = not even
    return sum(digits) % 10 == 0 if digits else False
",if even :,127
"def __get_param_string__(params):
    params_string = []
    for key in sorted(params.keys()):
        if ""REFUND"" in params[key] or ""|"" in params[key]:
            return
        value = params[key]
        params_string.append("""" if value == ""null"" else str(value))
    return ""|"".join(params_string)
","if ""REFUND"" in params [ key ] or ""|"" in params [ key ] :",93
"def _map_handlers(self, session, event_class, mapfn):
    for event in DOC_EVENTS:
        event_handler_name = event.replace(""-"", ""_"")
        if hasattr(self, event_handler_name):
            event_handler = getattr(self, event_handler_name)
            format_string = DOC_EVENTS[event]
            num_args = len(format_string.split(""."")) - 2
            format_args = (event_class,) + (""*"",) * num_args
            event_string = event + format_string % format_args
            unique_id = event_class + event_handler_name
            mapfn(event_string, event_handler, unique_id)
","if hasattr ( self , event_handler_name ) :",180
"def _create_param_lr(self, param_and_grad):
    # create learning rate variable for every parameter
    param = param_and_grad[0]
    param_lr = param.optimize_attr[""learning_rate""]
    if type(param_lr) == Variable:
        return param_lr
    else:
        if param_lr == 1.0:
            return self._global_learning_rate()
        else:
            with default_main_program()._lr_schedule_guard(
                is_with_opt=True
            ), framework.name_scope(""scale_with_param_lr""):
                return self._global_learning_rate() * param_lr
",if param_lr == 1.0 :,174
"def __getitem__(self, key):
    try:
        return self._clsmap[key]
    except KeyError as e:
        if not self.initialized:
            self._mutex.acquire()
            try:
                if not self.initialized:
                    self._init()
                    self.initialized = True
                return self._clsmap[key]
            finally:
                self._mutex.release()
        raise e
",if not self . initialized :,125
"def save(self, force=False):
    if not force:
        if not self.need_save:
            return
        if time.time() - self.last_save_time < 10:
            return
    with self.lock:
        with open(self.file_path, ""w"") as fd:
            for ip in self.cache:
                record = self.cache[ip]
                rule = record[""r""]
                connect_time = record[""c""]
                update_time = record[""update""]
                fd.write(""%s %s %d %d\n"" % (ip, rule, connect_time, update_time))
    self.last_save_time = time.time()
    self.need_save = False
",if not self . need_save :,198
"def pick(items, sel):
    for x, s in zip(items, sel):
        if match(s):
            yield x
        elif not x.is_atom() and not s.is_atom():
            yield x.restructure(x.head, pick(x.leaves, s.leaves), evaluation)
",if match ( s ) :,79
"def isValidFloat(config_param_name, value, constraints):
    if isinstance(value, float):
        constraints.setdefault(""min"", MIN_VALID_FLOAT_VALUE)
        constraints.setdefault(""max"", MAX_VALID_FLOAT_VALUE)
        minv = float(constraints.get(""min""))
        maxv = float(constraints.get(""max""))
        if value >= minv:
            if value <= maxv:
                return value
    raise FloatValueError(config_param_name, value, constraints)
",if value >= minv :,125
"def get_files(d):
    f = []
    for root, dirs, files in os.walk(d):
        for name in files:
            if ""meta-environment"" in root or ""cross-canadian"" in root:
                continue
            if ""qemux86copy-"" in root or ""qemux86-"" in root:
                continue
            if ""do_build"" not in name and ""do_populate_sdk"" not in name:
                f.append(os.path.join(root, name))
    return f
","if ""qemux86copy-"" in root or ""qemux86-"" in root :",143
"def __get_photo(self, person_or_marriage):
    """"""returns the first photo in the media list or None""""""
    media_list = person_or_marriage.get_media_list()
    for media_ref in media_list:
        media_handle = media_ref.get_reference_handle()
        media = self.database.get_media_from_handle(media_handle)
        mime_type = media.get_mime_type()
        if mime_type and mime_type.startswith(""image""):
            return media
    return None
","if mime_type and mime_type . startswith ( ""image"" ) :",140
"def filter(this, args):
    array = to_object(this, args.space)
    callbackfn = get_arg(args, 0)
    arr_len = js_arr_length(array)
    if not is_callable(callbackfn):
        raise MakeError(""TypeError"", ""callbackfn must be a function"")
    _this = get_arg(args, 1)
    k = 0
    res = []
    while k < arr_len:
        if array.has_property(unicode(k)):
            kValue = array.get(unicode(k))
            if to_boolean(callbackfn.call(_this, (kValue, float(k), array))):
                res.append(kValue)
        k += 1
    return args.space.ConstructArray(res)
","if to_boolean ( callbackfn . call ( _this , ( kValue , float ( k ) , array ) ) ) :",194
"def optimize(self, graph: Graph):
    for v in graph.inputs:
        if not v.has_attribute(SplitTarget):
            continue
        if flags.DEBUG:
            DumpGraph().optimize(graph)
        raise NotImplementedError(
            f""Input Variable {v} is too large to handle in WebGL backend""
        )
    return graph, False
",if flags . DEBUG :,94
"def detach_volume(self, volume):
    # We need to find the node using this volume
    for node in self.list_nodes():
        if type(node.image) is not list:
            # This node has only one associated image. It is not the one we
            # are after.
            continue
        for disk in node.image:
            if disk.id == volume.id:
                # Node found. We can now detach the volume
                disk_id = disk.extra[""disk_id""]
                return self._do_detach_volume(node.id, disk_id)
    return False
",if disk . id == volume . id :,160
"def Yield(value, level=1):
    g = greenlet.getcurrent()
    while level != 0:
        if not isinstance(g, genlet):
            raise RuntimeError(""yield outside a genlet"")
        if level > 1:
            g.parent.set_child(g)
        g = g.parent
        level -= 1
    g.switch(value)
",if level > 1 :,96
"def get_all_pipeline_nodes(
    pipeline: pipeline_pb2.Pipeline,
) -> List[pipeline_pb2.PipelineNode]:
    """"""Returns all pipeline nodes in the given pipeline.""""""
    result = []
    for pipeline_or_node in pipeline.nodes:
        which = pipeline_or_node.WhichOneof(""node"")
        # TODO(goutham): Handle sub-pipelines.
        # TODO(goutham): Handle system nodes.
        if which == ""pipeline_node"":
            result.append(pipeline_or_node.pipeline_node)
        else:
            raise NotImplementedError(""Only pipeline nodes supported."")
    return result
","if which == ""pipeline_node"" :",160
"def __init__(self, **settings):
    default_settings = self.get_default_settings()
    for name, value in default_settings.items():
        if not hasattr(self, name):
            setattr(self, name, value)
    for name, value in settings.items():
        if name not in default_settings:
            raise ImproperlyConfigured(
                ""Invalid setting '{}' for {}"".format(
                    name,
                    self.__class__.__name__,
                )
            )
        setattr(self, name, value)
",if name not in default_settings :,144
"def _check_choice(self):
    if self.type == ""choice"":
        if self.choices is None:
            raise OptionError(""must supply a list of choices for type 'choice'"", self)
        elif type(self.choices) not in (types.TupleType, types.ListType):
            raise OptionError(
                ""choices must be a list of strings ('%s' supplied)""
                % str(type(self.choices)).split(""'"")[1],
                self,
            )
    elif self.choices is not None:
        raise OptionError(""must not supply choices for type %r"" % self.type, self)
",if self . choices is None :,162
"def prepare(self, size=None):
    if _is_seekable(self.file):
        start_pos = self.file.tell()
        self.file.seek(0, 2)
        end_pos = self.file.tell()
        self.file.seek(start_pos)
        fsize = end_pos - start_pos
        if size is None:
            self.remain = fsize
        else:
            self.remain = min(fsize, size)
    return self.remain
",if size is None :,128
"def _setSitemapTargets():
    if not conf.sitemapUrl:
        return
    infoMsg = ""parsing sitemap '%s'"" % conf.sitemapUrl
    logger.info(infoMsg)
    found = False
    for item in parseSitemap(conf.sitemapUrl):
        if re.match(r""[^ ]+\?(.+)"", item, re.I):
            found = True
            kb.targets.add((item.strip(), None, None, None, None))
    if not found and not conf.forms and not conf.crawlDepth:
        warnMsg = ""no usable links found (with GET parameters)""
        logger.warn(warnMsg)
","if re . match ( r""[^ ]+\?(.+)"" , item , re . I ) :",159
"def test_CY_decomposition(self, tol):
    """"""Tests that the decomposition of the CY gate is correct""""""
    op = qml.CY(wires=[0, 1])
    res = op.decomposition(op.wires)
    mats = []
    for i in reversed(res):
        if len(i.wires) == 1:
            mats.append(np.kron(i.matrix, np.eye(2)))
        else:
            mats.append(i.matrix)
    decomposed_matrix = np.linalg.multi_dot(mats)
    assert np.allclose(decomposed_matrix, op.matrix, atol=tol, rtol=0)
",if len ( i . wires ) == 1 :,169
"def _line_ranges(statements, lines):
    """"""Produce a list of ranges for `format_lines`.""""""
    statements = sorted(statements)
    lines = sorted(lines)
    pairs = []
    start = None
    lidx = 0
    for stmt in statements:
        if lidx >= len(lines):
            break
        if stmt == lines[lidx]:
            lidx += 1
            if not start:
                start = stmt
            end = stmt
        elif start:
            pairs.append((start, end))
            start = None
    if start:
        pairs.append((start, end))
    return pairs
",if stmt == lines [ lidx ] :,167
"def init_params(net):
    """"""Init layer parameters.""""""
    for module in net.modules():
        if isinstance(module, nn.Conv2d):
            init.kaiming_normal(module.weight, mode=""fan_out"")
            if module.bias:
                init.constant(module.bias, 0)
        elif isinstance(module, nn.BatchNorm2d):
            init.constant(module.weight, 1)
            init.constant(module.bias, 0)
        elif isinstance(module, nn.Linear):
            init.normal(module.weight, std=1e-3)
            if module.bias:
                init.constant(module.bias, 0)
",if module . bias :,180
"def _get_directory_size_in_bytes(directory):
    total = 0
    try:
        for entry in os.scandir(directory):
            if entry.is_file():
                # if it's a file, use stat() function
                total += entry.stat().st_size
            elif entry.is_dir():
                # if it's a directory, recursively call this function
                total += _get_directory_size_in_bytes(entry.path)
    except NotADirectoryError:
        # if `directory` isn't a directory, get the file size then
        return os.path.getsize(directory)
    except PermissionError:
        # if for whatever reason we can't open the folder, return 0
        return 0
    return total
",if entry . is_file ( ) :,193
"def run_cmd(self, util, to, always_push_mark=False):
    if to == ""bof"":
        util.push_mark_and_goto_position(0)
    elif to == ""eof"":
        util.push_mark_and_goto_position(self.view.size())
    elif to in (""eow"", ""bow""):
        visible = self.view.visible_region()
        pos = visible.a if to == ""bow"" else visible.b
        if always_push_mark:
            util.push_mark_and_goto_position(pos)
        else:
            util.set_cursors([sublime.Region(pos)])
",if always_push_mark :,170
"def parse_results(cwd):
    optimal_dd = None
    optimal_measure = numpy.inf
    for tup in tools.find_conf_files(cwd):
        dd = tup[1]
        if ""results.train_y_misclass"" in dd:
            if dd[""results.train_y_misclass""] < optimal_measure:
                optimal_measure = dd[""results.train_y_misclass""]
                optimal_dd = dd
    print(""Optimal results.train_y_misclass:"", str(optimal_measure))
    for key, value in optimal_dd.items():
        if ""hyper_parameters"" in key:
            print(key + "": "" + str(value))
","if ""hyper_parameters"" in key :",177
"def clean_vc_position(self):
    vc_position = self.cleaned_data[""vc_position""]
    if self.validate_vc_position:
        conflicting_members = Device.objects.filter(
            virtual_chassis=self.instance.virtual_chassis, vc_position=vc_position
        )
        if conflicting_members.exists():
            raise forms.ValidationError(
                ""A virtual chassis member already exists in position {}."".format(
                    vc_position
                )
            )
    return vc_position
",if conflicting_members . exists ( ) :,148
"def cal_pads(auto_pad, pad_shape):
    spatial_size = len(pad_shape)
    pads = [0] * spatial_size * 2
    for i in range(spatial_size):
        if auto_pad == ""SAME_LOWER"":
            pads[i + spatial_size] = pad_shape[i] // 2
            pads[i] = pad_shape[i] - pads[i + spatial_size]
        elif auto_pad == ""SAME_UPPER"":
            pads[i] = pad_shape[i] // 2
            pads[i + spatial_size] = pad_shape[i] - pads[i]
    return pads
","elif auto_pad == ""SAME_UPPER"" :",173
"def TryMerge(self, d):
    while d.avail() > 0:
        tt = d.getVarInt32()
        if tt == 10:
            length = d.getVarInt32()
            tmp = ProtocolBuffer.Decoder(d.buffer(), d.pos(), d.pos() + length)
            d.skip(length)
            self.add_presence_response().TryMerge(tmp)
            continue
        if tt == 0:
            raise ProtocolBuffer.ProtocolBufferDecodeError
        d.skipData(tt)
",if tt == 10 :,140
"def test_cwl_rnaseq(self, install_test_files):
    with install_cwl_test_files() as work_dir:
        with utils.chdir(os.path.join(work_dir, ""rnaseq"")):
            if os.path.exists(""cromwell_work""):
                shutil.rmtree(""cromwell_work"")
            subprocess.check_call(
                [""bcbio_vm.py"", ""cwlrun"", ""cromwell"", ""rnaseq-workflow""]
            )
","if os . path . exists ( ""cromwell_work"" ) :",139
"def files_per_version(self):
    xpath = ""./files/file""
    files = self.root.findall(xpath)
    versions = {}
    for file in files:
        vfile = file.findall(""version"")
        for version in vfile:
            nb = version.attrib[""nb""]
            if not nb in versions:
                versions[nb] = []
            versions[nb].append(file.attrib[""url""])
    return versions
",if not nb in versions :,117
"def value_to_db_datetime(self, value):
    if value is None:
        return None
    # SQLite doesn't support tz-aware datetimes
    if timezone.is_aware(value):
        if settings.USE_TZ:
            value = value.astimezone(timezone.utc).replace(tzinfo=None)
        else:
            raise ValueError(
                ""SQLite backend does not support timezone-aware datetimes when USE_TZ is False.""
            )
    return six.text_type(value)
",if settings . USE_TZ :,131
"def _toplevelTryFunc(func, *args, status=status, **kwargs):
    with ThreadProfiler(threading.current_thread()) as prof:
        t = threading.current_thread()
        t.name = func.__name__
        try:
            t.status = func(*args, **kwargs)
        except EscapeException as e:  # user aborted
            t.status = ""aborted by user""
            if status:
                status(""%s aborted"" % t.name, priority=2)
        except Exception as e:
            t.exception = e
            t.status = ""exception""
            vd.exceptionCaught(e)
        if t.sheet:
            t.sheet.currentThreads.remove(t)
",if status :,193
"def ESP(phrase):
    for num, name in enumerate(devname):
        if name.lower() in phrase:
            dev = devid[num]
            if custom_action_keyword[""Dict""][""On""] in phrase:
                ctrl = ""=ON""
                say(""Turning On "" + name)
            elif custom_action_keyword[""Dict""][""Off""] in phrase:
                ctrl = ""=OFF""
                say(""Turning Off "" + name)
            rq = requests.head(""https://"" + ip + dev + ctrl, verify=False)
","if custom_action_keyword [ ""Dict"" ] [ ""On"" ] in phrase :",153
"def _table_schema(self, table):
    rows = self.db.execute_sql(""PRAGMA table_info('%s')"" % table).fetchall()
    # Build list of fields from table information
    result = {}
    for _, name, data_type, not_null, _, primary_key in rows:
        parts = [data_type]
        if primary_key:
            parts.append(""PRIMARY KEY"")
        if not_null:
            parts.append(""NOT NULL"")
        result[name] = "" "".join(parts)
    return result
",if primary_key :,137
"def _validate_forward_input(x, n_in):
    if n_in != 1:
        if not isinstance(x, (tuple, list)):
            raise TypeError(
                f""Expected input to be a tuple or list; instead got {type(x)}.""
            )
        if len(x) != n_in:
            raise ValueError(
                f""Input tuple length ({len(x)}) does not equal required ""
                f""number of inputs ({n_in}).""
            )
",if len ( x ) != n_in :,133
"def _table_reprfunc(self, row, col, val):
    if self._table.column_names[col].endswith(""Size""):
        if isinstance(val, compat.string_types):
            return ""  %s"" % val
        elif val < 1024 ** 2:
            return ""  %.1f KB"" % (val / 1024.0 ** 1)
        elif val < 1024 ** 3:
            return ""  %.1f MB"" % (val / 1024.0 ** 2)
        else:
            return ""  %.1f GB"" % (val / 1024.0 ** 3)
    if col in (0, """"):
        return str(val)
    else:
        return ""  %s"" % val
",elif val < 1024 ** 2 :,182
"def get_path_name(self):
    if self.is_root():
        return ""@"" + self.name
    else:
        parent_name = self.parent.get_path_name()
        if parent_name:
            return ""/"".join([parent_name, ""@"" + self.name])
        else:
            return ""@"" + self.name
",if parent_name :,90
"def parse(cls, api, json):
    lst = List(api)
    setattr(lst, ""_json"", json)
    for k, v in json.items():
        if k == ""user"":
            setattr(lst, k, User.parse(api, v))
        elif k == ""created_at"":
            setattr(lst, k, parse_datetime(v))
        else:
            setattr(lst, k, v)
    return lst
","if k == ""user"" :",115
"def _bytecode_filenames(self, py_filenames):
    bytecode_files = []
    for py_file in py_filenames:
        if not py_file.endswith("".py""):
            continue
        if self.compile:
            bytecode_files.append(py_file + ""c"")
        if self.optimize > 0:
            bytecode_files.append(py_file + ""o"")
    return bytecode_files
",if self . compile :,107
"def to_json_dict(self):
    d = super().to_json_dict()
    d[""bullet_list""] = RenderedContent.rendered_content_list_to_json(self.bullet_list)
    if self.header is not None:
        if isinstance(self.header, RenderedContent):
            d[""header""] = self.header.to_json_dict()
        else:
            d[""header""] = self.header
    if self.subheader is not None:
        if isinstance(self.subheader, RenderedContent):
            d[""subheader""] = self.subheader.to_json_dict()
        else:
            d[""subheader""] = self.subheader
    return d
","if isinstance ( self . header , RenderedContent ) :",172
"def makeSomeFiles(pathobj, dirdict):
    pathdict = {}
    for (key, value) in dirdict.items():
        child = pathobj.child(key)
        if isinstance(value, bytes):
            pathdict[key] = child
            child.setContent(value)
        elif isinstance(value, dict):
            child.createDirectory()
            pathdict[key] = makeSomeFiles(child, value)
        else:
            raise ValueError(""only strings and dicts allowed as values"")
    return pathdict
","elif isinstance ( value , dict ) :",138
"def Restore(self):
    picker, obj = self._window, self._pObject
    value = obj.RestoreValue(PERSIST_FILEDIRPICKER_PATH)
    if value is not None:
        if issubclass(picker.__class__, wx.FileDialog):
            if type(value) == list:
                value = value[-1]
        picker.SetPath(value)
        return True
    return False
","if issubclass ( picker . __class__ , wx . FileDialog ) :",102
"def recv(self, buffer_size):
    try:
        return super(SSLConnection, self).recv(buffer_size)
    except ssl.SSLError as err:
        if err.args[0] in (ssl.SSL_ERROR_WANT_READ, ssl.SSL_ERROR_WANT_WRITE):
            return b""""
        if err.args[0] in (ssl.SSL_ERROR_EOF, ssl.SSL_ERROR_ZERO_RETURN):
            self.handle_close()
            return b""""
        raise
","if err . args [ 0 ] in ( ssl . SSL_ERROR_WANT_READ , ssl . SSL_ERROR_WANT_WRITE ) :",133
"def IncrementErrorCount(self, category):
    """"""Bumps the module's error statistic.""""""
    self.error_count += 1
    if self.counting in (""toplevel"", ""detailed""):
        if self.counting != ""detailed"":
            category = category.split(""/"")[0]
        if category not in self.errors_by_category:
            self.errors_by_category[category] = 0
        self.errors_by_category[category] += 1
","if self . counting != ""detailed"" :",115
"def _get_y(self, data_inst):
    if self.stratified:
        y = [v for i, v in data_inst.mapValues(lambda v: v.label).collect()]
        if self.need_transform:
            y = self.transform_regression_label(data_inst)
    else:
        # make dummy y
        y = [0] * (data_inst.count())
    return y
",if self . need_transform :,109
"def test_all_project_files(self):
    if sys.platform.startswith(""win""):
        # XXX something with newlines goes wrong on Windows.
        return
    for filepath in support.all_project_files():
        with open(filepath, ""rb"") as fp:
            encoding = tokenize.detect_encoding(fp.readline)[0]
        self.assertIsNotNone(encoding, ""can't detect encoding for %s"" % filepath)
        with open(filepath, ""r"") as fp:
            source = fp.read()
            source = source.decode(encoding)
        tree = driver.parse_string(source)
        new = unicode(tree)
        if diff(filepath, new, encoding):
            self.fail(""Idempotency failed: %s"" % filepath)
","if diff ( filepath , new , encoding ) :",195
"def test_resource_arn_override_generator(self):
    overrides = set()
    for k, v in manager.resources.items():
        arn_gen = bool(v.__dict__.get(""get_arns"") or v.__dict__.get(""generate_arn""))
        if arn_gen:
            overrides.add(k)
    overrides = overrides.difference(
        {
            ""account"",
            ""s3"",
            ""hostedzone"",
            ""log-group"",
            ""rest-api"",
            ""redshift-snapshot"",
            ""rest-stage"",
        }
    )
    if overrides:
        raise ValueError(""unknown arn overrides in %s"" % ("", "".join(overrides)))
",if arn_gen :,185
"def _check_dsl_runner(self) -> None:
    """"""Checks if runner in dsl is Kubeflow V2 runner.""""""
    with open(self.flags_dict[labels.PIPELINE_DSL_PATH], ""r"") as f:
        dsl_contents = f.read()
        if ""KubeflowV2DagRunner"" not in dsl_contents:
            raise RuntimeError(""KubeflowV2DagRunner not found in dsl."")
","if ""KubeflowV2DagRunner"" not in dsl_contents :",116
"def create_warehouse(warehouse_name, properties=None, company=None):
    if not company:
        company = ""_Test Company""
    warehouse_id = erpnext.encode_company_abbr(warehouse_name, company)
    if not frappe.db.exists(""Warehouse"", warehouse_id):
        warehouse = frappe.new_doc(""Warehouse"")
        warehouse.warehouse_name = warehouse_name
        warehouse.parent_warehouse = ""All Warehouses - _TCUV""
        warehouse.company = company
        warehouse.account = get_warehouse_account(warehouse_name, company)
        if properties:
            warehouse.update(properties)
        warehouse.save()
        return warehouse.name
    else:
        return warehouse_id
",if properties :,186
"def _parse(self, contents):
    entries = []
    hostnames_found = set()
    for line in contents.splitlines():
        if not len(line.strip()):
            entries.append((""blank"", [line]))
            continue
        (head, tail) = chop_comment(line.strip(), ""#"")
        if not len(head):
            entries.append((""all_comment"", [line]))
            continue
        entries.append((""hostname"", [head, tail]))
        hostnames_found.add(head)
    if len(hostnames_found) > 1:
        raise IOError(""Multiple hostnames (%s) found!"" % (hostnames_found))
    return entries
",if not len ( head ) :,167
"def _get_omega(self):
    if self._omega is None:
        n = self.get_drift_dim() // 2
        omg = sympl.calc_omega(n)
        if self.oper_dtype == Qobj:
            self._omega = Qobj(omg, dims=self.dyn_dims)
            self._omega_qobj = self._omega
        elif self.oper_dtype == sp.csr_matrix:
            self._omega = sp.csr_matrix(omg)
        else:
            self._omega = omg
    return self._omega
",elif self . oper_dtype == sp . csr_matrix :,163
"def get_in_inputs(key, data):
    if isinstance(data, dict):
        for k, v in data.items():
            if k == key:
                return v
            elif isinstance(v, (list, tuple, dict)):
                out = get_in_inputs(key, v)
                if out:
                    return out
    elif isinstance(data, (list, tuple)):
        out = [get_in_inputs(key, x) for x in data]
        out = [x for x in out if x]
        if out:
            return out[0]
","elif isinstance ( v , ( list , tuple , dict ) ) :",160
"def visit_binary(binary):
    if binary.operator == operators.eq:
        cols = util.column_set(chain(*[c.proxy_set for c in columns.difference(omit)]))
        if binary.left in cols and binary.right in cols:
            for c in reversed(columns):
                if c.shares_lineage(binary.right) and (
                    not only_synonyms or c.name == binary.left.name
                ):
                    omit.add(c)
                    break
",if binary . left in cols and binary . right in cols :,136
"def wait_tasks_or_abort(futures, timeout=60, kill_switch_ev=None):
    try:
        LazySingletonTasksCoordinator.wait_tasks(
            futures, return_when=FIRST_EXCEPTION, raise_exceptions=True
        )
    except Exception as e:
        if kill_switch_ev is not None:
            # Used when we want to keep both raise the exception and wait for all tasks to finish
            kill_switch_ev.set()
            LazySingletonTasksCoordinator.wait_tasks(
                futures,
                return_when=ALL_COMPLETED,
                raise_exceptions=False,
                timeout=timeout,
            )
        raise e
",if kill_switch_ev is not None :,187
"def is_valid(sample):
    if sample is None:
        return False
    if isinstance(sample, tuple):
        for s in sample:
            if s is None:
                return False
            elif isinstance(s, np.ndarray) and s.size == 0:
                return False
            elif isinstance(s, collections.abc.Sequence) and len(s) == 0:
                return False
    return True
",if s is None :,114
"def setVaName(self, va, parent=None):
    if parent is None:
        parent = self
    curname = self.vw.getName(va)
    if curname is None:
        curname = """"
    name, ok = QInputDialog.getText(parent, ""Enter..."", ""Name"", text=curname)
    if ok:
        name = str(name)
        if self.vw.vaByName(name):
            raise Exception(""Duplicate Name: %s"" % name)
        self.vw.makeName(va, name)
",if self . vw . vaByName ( name ) :,142
"def generic_tag_compiler(params, defaults, name, node_class, parser, token):
    ""Returns a template.Node subclass.""
    bits = token.split_contents()[1:]
    bmax = len(params)
    def_len = defaults and len(defaults) or 0
    bmin = bmax - def_len
    if len(bits) < bmin or len(bits) > bmax:
        if bmin == bmax:
            message = ""%s takes %s arguments"" % (name, bmin)
        else:
            message = ""%s takes between %s and %s arguments"" % (name, bmin, bmax)
        raise TemplateSyntaxError(message)
    return node_class(bits)
",if bmin == bmax :,176
"def extract_segmentation_mask(annotation):
    poly_specs = annotation[DensePoseDataRelative.S_KEY]
    if isinstance(poly_specs, torch.Tensor):
        # data is already given as mask tensors, no need to decode
        return poly_specs
    import pycocotools.mask as mask_utils
    segm = torch.zeros((DensePoseDataRelative.MASK_SIZE,) * 2, dtype=torch.float32)
    for i in range(DensePoseDataRelative.N_BODY_PARTS):
        poly_i = poly_specs[i]
        if poly_i:
            mask_i = mask_utils.decode(poly_i)
            segm[mask_i > 0] = i + 1
    return segm
",if poly_i :,184
"def module_list(target, fast):
    """"""Find the list of modules to be compiled""""""
    modules = []
    native = native_modules(target)
    basedir = os.path.join(ouroboros_repo_folder(), ""ouroboros"")
    for name in os.listdir(basedir):
        module_name, ext = os.path.splitext(name)
        if ext == "".py"" or ext == """" and os.path.isdir(os.path.join(basedir, name)):
            if module_name not in IGNORE_MODULES and module_name not in native:
                if not (fast and module_name in KNOWN_PROBLEM_MODULES):
                    modules.append(module_name)
    return set(modules)
","if ext == "".py"" or ext == """" and os . path . isdir ( os . path . join ( basedir , name ) ) :",185
"def filelist_from_patterns(pats, rootdir=None):
    if rootdir is None:
        rootdir = "".""
    # filelist = []
    fileset = set([])
    lines = [line.strip() for line in pats]
    for line in lines:
        pat = line[2:]
        newfiles = glob(osp.join(rootdir, pat))
        if line.startswith(""+""):
            fileset.update(newfiles)
        elif line.startswith(""-""):
            fileset.difference_update(newfiles)
        else:
            raise ValueError(""line must start with + or -"")
    filelist = list(fileset)
    return filelist
","elif line . startswith ( ""-"" ) :",165
"def get_upstream_statuses_events(self, upstream: Set) -> Dict[str, V1Statuses]:
    statuses_by_refs = {u: [] for u in upstream}
    events = self.events or []  # type: List[V1EventTrigger]
    for e in events:
        entity_ref = contexts_refs.get_entity_ref(e.ref)
        if not entity_ref:
            continue
        if entity_ref not in statuses_by_refs:
            continue
        for kind in e.kinds:
            status = V1EventKind.events_statuses_mapping.get(kind)
            if status:
                statuses_by_refs[entity_ref].append(status)
    return statuses_by_refs
",if status :,191
"def __setitem__(self, key, value):
    if isinstance(value, (tuple, list)):
        info, reference = value
        if info not in self._reverse_infos:
            self._reverse_infos[info] = len(self._infos)
            self._infos.append(info)
        if reference not in self._reverse_references:
            self._reverse_references[reference] = len(self._references)
            self._references.append(reference)
        self._trails[key] = ""%d,%d"" % (
            self._reverse_infos[info],
            self._reverse_references[reference],
        )
    else:
        raise Exception(""unsupported type '%s'"" % type(value))
",if info not in self . _reverse_infos :,184
"def ChangeStyle(self, combos):
    style = 0
    for combo in combos:
        if combo.GetValue() == 1:
            if combo.GetLabel() == ""TR_VIRTUAL"":
                style = style | HTL.TR_VIRTUAL
            else:
                try:
                    style = style | eval(""wx."" + combo.GetLabel())
                except:
                    style = style | eval(""HTL."" + combo.GetLabel())
    if self.GetAGWWindowStyleFlag() != style:
        self.SetAGWWindowStyleFlag(style)
",if combo . GetValue ( ) == 1 :,153
"def _parse_csrf(self, response):
    for d in response:
        if d.startswith(""Set-Cookie:""):
            for c in d.split("":"", 1)[1].split("";""):
                if c.strip().startswith(""CSRF-Token-""):
                    self._CSRFtoken = c.strip("" \r\n"")
                    log.verbose(""Got new cookie: %s"", self._CSRFtoken)
                    break
            if self._CSRFtoken != None:
                break
","if c . strip ( ) . startswith ( ""CSRF-Token-"" ) :",135
"def test_page_size_matching_max_returned_rows(
    app_client_returned_rows_matches_page_size,
):
    fetched = []
    path = ""/fixtures/no_primary_key.json""
    while path:
        response = app_client_returned_rows_matches_page_size.get(path)
        fetched.extend(response.json[""rows""])
        assert len(response.json[""rows""]) in (1, 50)
        path = response.json[""next_url""]
        if path:
            path = path.replace(""http://localhost"", """")
    assert 201 == len(fetched)
",if path :,155
"def get_mapping_exception_message(mappings: List[Tuple[Text, Text]]):
    """"""Return a message given a list of duplicates.""""""
    message = """"
    for name, action_name in mappings:
        if message:
            message += ""\n""
        message += (
            ""Intent '{}' is set to trigger action '{}', which is ""
            ""not defined in the domain."".format(name, action_name)
        )
    return message
",if message :,113
"def cut(sentence):
    sentence = strdecode(sentence)
    blocks = re_han.split(sentence)
    for blk in blocks:
        if re_han.match(blk):
            for word in __cut(blk):
                if word not in Force_Split_Words:
                    yield word
                else:
                    for c in word:
                        yield c
        else:
            tmp = re_skip.split(blk)
            for x in tmp:
                if x:
                    yield x
",if x :,156
"def chop(expr, delta=10.0 ** (-10.0)):
    if isinstance(expr, Real):
        if -delta < expr.get_float_value() < delta:
            return Integer(0)
    elif isinstance(expr, Complex) and expr.is_inexact():
        real, imag = expr.real, expr.imag
        if -delta < real.get_float_value() < delta:
            real = Integer(0)
        if -delta < imag.get_float_value() < delta:
            imag = Integer(0)
        return Complex(real, imag)
    elif isinstance(expr, Expression):
        return Expression(chop(expr.head), *[chop(leaf) for leaf in expr.leaves])
    return expr
",if - delta < imag . get_float_value ( ) < delta :,186
"def make_row(self):
    res = []
    for i in range(self.num_cols):
        t = sqlite3_column_type(self.stmnt, i)
        # print(""type"", t)
        if t == SQLITE_INTEGER:
            res.append(sqlite3_column_int(self.stmnt, i))
        elif t == SQLITE_FLOAT:
            res.append(sqlite3_column_double(self.stmnt, i))
        elif t == SQLITE_TEXT:
            res.append(sqlite3_column_text(self.stmnt, i))
        else:
            raise NotImplementedError
    return tuple(res)
",elif t == SQLITE_TEXT :,172
"def try_convert(self, string):
    string = string.strip()
    try:
        return int(string)
    except:
        try:
            return float(string)
        except:
            if string == ""True"":
                return True
            if string == ""False"":
                return False
            return string
","if string == ""False"" :",93
"def configure_create_table_epilogue(store):
    for val in ["""", "" ENGINE=InnoDB""]:
        store.config[""create_table_epilogue""] = val
        store._set_sql_flavour()
        if store._test_transaction():
            store.log.info(""create_table_epilogue='%s'"", val)
            return
    raise Exception(""Can not create a transactional table."")
",if store . _test_transaction ( ) :,104
"def _check_rule(self, match, target_dict, cred_dict):
    """"""Recursively checks credentials based on the brains rules.""""""
    try:
        new_match_list = self.rules[match]
    except KeyError:
        if self.default_rule and match != self.default_rule:
            new_match_list = (""rule:%s"" % self.default_rule,)
        else:
            return False
    return self.check(new_match_list, target_dict, cred_dict)
",if self . default_rule and match != self . default_rule :,129
"def get_civil_names(self):
    congresspeople_ids = self.get_all_congresspeople_ids()
    for i, congress_id in enumerate(congresspeople_ids):
        if not np.math.isnan(float(congress_id)):
            percentage = i / self.total * 100
            msg = ""Processed {} out of {} ({:.2f}%)""
            print(msg.format(i, self.total, percentage), end=""\r"")
            data = self.fetch_data_repository(congress_id)
            if data is not None:
                yield dict(data)
",if data is not None :,160
"def parse_network_whitelist(self, network_whitelist_location):
    networks = []
    with open(network_whitelist_location, ""r"") as text_file:
        for line in text_file:
            line = line.strip().strip(""'"").strip('""')
            if isIPv4(line) or isIPv6(line):
                networks.append(line)
    return networks
",if isIPv4 ( line ) or isIPv6 ( line ) :,98
"def _pick(self, cum):
    if self._isleaf():
        return self.bd[0], self.s
    else:
        if cum < self.left.s:
            return self.left._pick(cum)
        else:
            return self.right._pick(cum - self.left.s)
",if cum < self . left . s :,83
"def serialize_content_range(value):
    if isinstance(value, (tuple, list)):
        if len(value) not in (2, 3):
            raise ValueError(
                ""When setting content_range to a list/tuple, it must ""
                ""be length 2 or 3 (not %r)"" % value
            )
        if len(value) == 2:
            begin, end = value
            length = None
        else:
            begin, end, length = value
        value = ContentRange(begin, end, length)
    value = str(value).strip()
    if not value:
        return None
    return value
",if len ( value ) == 2 :,169
"def make_index_fields(rec):
    fields = {}
    for k, v in rec.iteritems():
        if k in (""lccn"", ""oclc"", ""isbn""):
            fields[k] = v
            continue
        if k == ""full_title"":
            fields[""title""] = [read_short_title(v)]
    return fields
","if k in ( ""lccn"" , ""oclc"" , ""isbn"" ) :",93
"def _sample_translation(reference, max_len):
    translation = reference[:]
    while np.random.uniform() < 0.8 and 1 < len(translation) < max_len:
        trans_len = len(translation)
        ind = np.random.randint(trans_len)
        action = np.random.choice(actions)
        if action == ""deletion"":
            del translation[ind]
        elif action == ""replacement"":
            ind_rep = np.random.randint(trans_len)
            translation[ind] = translation[ind_rep]
        else:
            ind_insert = np.random.randint(trans_len)
            translation.insert(ind, translation[ind_insert])
    return translation
","elif action == ""replacement"" :",186
"def __call__(self, text: str) -> str:
    for t in self.cleaner_types:
        if t == ""tacotron"":
            text = tacotron_cleaner.cleaners.custom_english_cleaners(text)
        elif t == ""jaconv"":
            text = jaconv.normalize(text)
        elif t == ""vietnamese"":
            if vietnamese_cleaners is None:
                raise RuntimeError(""Please install underthesea"")
            text = vietnamese_cleaners.vietnamese_cleaner(text)
        else:
            raise RuntimeError(f""Not supported: type={t}"")
    return text
","elif t == ""vietnamese"" :",174
"def hook_GetVariable(ql, address, params):
    if params[""VariableName""] in ql.env:
        var = ql.env[params[""VariableName""]]
        read_len = read_int64(ql, params[""DataSize""])
        if params[""Attributes""] != 0:
            write_int64(ql, params[""Attributes""], 0)
        write_int64(ql, params[""DataSize""], len(var))
        if read_len < len(var):
            return EFI_BUFFER_TOO_SMALL
        if params[""Data""] != 0:
            ql.mem.write(params[""Data""], var)
        return EFI_SUCCESS
    return EFI_NOT_FOUND
","if params [ ""Attributes"" ] != 0 :",177
"def test_setupapp(self, overrideRootMenu):
    ""Call setupApp with each possible graphics type.""
    root = self.root
    flist = FileList(root)
    for tktype in alltypes:
        with self.subTest(tktype=tktype):
            macosx._tk_type = tktype
            macosx.setupApp(root, flist)
            if tktype in (""carbon"", ""cocoa""):
                self.assertTrue(overrideRootMenu.called)
            overrideRootMenu.reset_mock()
","if tktype in ( ""carbon"" , ""cocoa"" ) :",139
"def names(self, persistent=None):
    u = set()
    result = []
    for s in [
        self.__storage(None),
        self.__storage(self.__category),
    ]:
        for b in s:
            if persistent is not None and b.persistent != persistent:
                continue
            if b.name.startswith(""__""):
                continue
            if b.name not in u:
                result.append(b.name)
                u.add(b.name)
    return result
","if b . name . startswith ( ""__"" ) :",139
"def _check_extra_specs(key, value=None):
    extra_specs = diff.get(""extra_specs"")
    specific_type = extra_specs.get(key) if extra_specs else None
    old_type = None
    new_type = None
    if specific_type:
        old_type, new_type = specific_type
        if value:
            old_type = True if old_type and old_type.upper() == value else False
            new_type = True if new_type and new_type.upper() == value else False
    return old_type, new_type
",if value :,148
"def _write_lock_file(self, repo, force=True):  # type: (Repository, bool) -> None
    if force or (self._update and self._write_lock):
        updated_lock = self._locker.set_lock_data(self._package, repo.packages)
        if updated_lock:
            self._io.write_line("""")
            self._io.write_line(""<info>Writing lock file</>"")
",if updated_lock :,109
"def process_message(self, msg):
    if msg[""type""] == ""sample"":
        batch_shape = msg[""fn""].batch_shape
        if len(batch_shape) < -self.dim or batch_shape[self.dim] != self.size:
            batch_shape = [1] * (-self.dim - len(batch_shape)) + list(batch_shape)
            batch_shape[self.dim] = self.size
            msg[""fn""] = msg[""fn""].expand(torch.Size(batch_shape))
",if len ( batch_shape ) < - self . dim or batch_shape [ self . dim ] != self . size :,133
"def _test_reducibility(self):
    # make a copy of the graph
    graph = networkx.DiGraph(self._graph)
    # preprocess: make it a super graph
    self._make_supergraph(graph)
    while True:
        changed = False
        # find a node with a back-edge, remove the edge (deleting the loop), and replace it with a MultiNode
        changed |= self._remove_self_loop(graph)
        # find a node that has only one predecessor, and merge it with its predecessor (replace them with a
        # MultiNode)
        changed |= self._merge_single_entry_node(graph)
        if not changed:
            # a fixed-point is reached
            break
",if not changed :,178
"def __init__(self, roberta, num_classes=2, dropout=0.0, prefix=None, params=None):
    super(RoBERTaClassifier, self).__init__(prefix=prefix, params=params)
    self.roberta = roberta
    self._units = roberta._units
    with self.name_scope():
        self.classifier = nn.HybridSequential(prefix=prefix)
        if dropout:
            self.classifier.add(nn.Dropout(rate=dropout))
        self.classifier.add(nn.Dense(units=self._units, activation=""tanh""))
        if dropout:
            self.classifier.add(nn.Dropout(rate=dropout))
        self.classifier.add(nn.Dense(units=num_classes))
",if dropout :,185
"def get_object_from_name(self, name, check_symlinks=True):
    if not name:
        return None
    name = name.rstrip(""\\"")
    for a, o in self.objects.items():
        if not o.name:
            continue
        if o.name.lower() == name.lower():
            return o
    if check_symlinks:
        m = [sl[1] for sl in self.symlinks if name.lower() == sl[0].lower()]
        if m:
            name = m[0]
        return self.get_object_from_name(name, False)
",if o . name . lower ( ) == name . lower ( ) :,156
"def __call__(self):
    """"""Run all check_* methods.""""""
    if self.on:
        oldformatwarning = warnings.formatwarning
        warnings.formatwarning = self.formatwarning
        try:
            for name in dir(self):
                if name.startswith(""check_""):
                    method = getattr(self, name)
                    if method and callable(method):
                        method()
        finally:
            warnings.formatwarning = oldformatwarning
","if name . startswith ( ""check_"" ) :",127
"def __print__(self, defaults=False):
    if defaults:
        print_func = str
    else:
        print_func = repr
    pieces = []
    default_values = self.__defaults__
    for k in self.__fields__:
        value = getattr(self, k)
        if not defaults and value == default_values[k]:
            continue
        if isinstance(value, basestring):
            print_func = repr  # keep quotes around strings
        pieces.append(""%s=%s"" % (k, print_func(value)))
    if pieces or self.__base__:
        return ""%s(%s)"" % (self.__class__.__name__, "", "".join(pieces))
    else:
        return """"
","if isinstance ( value , basestring ) :",178
"def apply(self, **kwargs: Any) -> None:
    for node in self.document.traverse(nodes.target):
        if not node[""ids""]:
            continue
        if (
            ""ismod"" in node
            and node.parent.__class__ is nodes.section
            and
            # index 0 is the section title node
            node.parent.index(node) == 1
        ):
            node.parent[""ids""][0:0] = node[""ids""]
            node.parent.remove(node)
","if not node [ ""ids"" ] :",139
"def add_special_token_2d(
    values: List[List[int]], special_token: int = 0, use_first_value: bool = False
) -> List[List[int]]:
    results = torch.jit.annotate(List[List[int]], [])
    for value in values:
        result = torch.jit.annotate(List[int], [])
        if use_first_value and len(value) > 0:
            special_token = value[0]
        result.append(special_token)
        result.extend(value)
        result.append(special_token)
        results.append(result)
    return results
",if use_first_value and len ( value ) > 0 :,159
"def test_import(self):
    TIMEOUT = 5
    # Test for a deadlock when importing a module that runs the
    # ThreadedResolver at import-time. See resolve_test.py for
    # full explanation.
    command = [sys.executable, ""-c"", ""import tornado.test.resolve_test_helper""]
    start = time.time()
    popen = Popen(command, preexec_fn=lambda: signal.alarm(TIMEOUT))
    while time.time() - start < TIMEOUT:
        return_code = popen.poll()
        if return_code is not None:
            self.assertEqual(0, return_code)
            return  # Success.
        time.sleep(0.05)
    self.fail(""import timed out"")
",if return_code is not None :,183
"def find_item_for_key(self, e):
    for item in self._items:
        if item.keycode == e.key and item.shift == e.shift and item.alt == e.alt:
            focus = get_focus()
            if self.command_is_enabled(item, focus):
                return self._items.index(item)
            else:
                return -1
    return -1
","if self . command_is_enabled ( item , focus ) :",112
"def check_app_config_brackets(self):
    for sn, app in cherrypy.tree.apps.items():
        if not isinstance(app, cherrypy.Application):
            continue
        if not app.config:
            continue
        for key in app.config.keys():
            if key.startswith(""["") or key.endswith(""]""):
                warnings.warn(
                    ""The application mounted at %r has config ""
                    ""section names with extraneous brackets: %r. ""
                    ""Config *files* need brackets; config *dicts* ""
                    ""(e.g. passed to tree.mount) do not."" % (sn, key)
                )
",if not app . config :,186
"def got_arbiter_module_type_defined(self, mod_type):
    for a in self.arbiters:
        # Do like the linkify will do after....
        for m in getattr(a, ""modules"", []):
            # So look at what the arbiter try to call as module
            m = m.strip()
            # Ok, now look in modules...
            for mod in self.modules:
                # try to see if this module is the good type
                if getattr(mod, ""module_type"", """").strip() == mod_type.strip():
                    # if so, the good name?
                    if getattr(mod, ""module_name"", """").strip() == m:
                        return True
    return False
","if getattr ( mod , ""module_type"" , """" ) . strip ( ) == mod_type . strip ( ) :",199
"def write_config_to_file(self, folder, filename, config):
    do_not_write = [""hyperparameter_search_space_updates""]
    with open(os.path.join(folder, filename), ""w"") as f:
        f.write(
            ""\n"".join(
                [
                    (key + ""="" + str(value))
                    for (key, value) in sorted(config.items(), key=lambda x: x[0])
                    if not key in do_not_write
                ]
            )
        )
",if not key in do_not_write,152
"def parsing(self, parsing):  # type: (bool) -> None
    self._parsed = parsing
    for k, v in self._body:
        if isinstance(v, Table):
            v.value.parsing(parsing)
        elif isinstance(v, AoT):
            for t in v.body:
                t.value.parsing(parsing)
","if isinstance ( v , Table ) :",93
"def test_crashers_crash(self):
    for fname in glob.glob(CRASHER_FILES):
        if os.path.basename(fname) in infinite_loops:
            continue
        # Some ""crashers"" only trigger an exception rather than a
        # segfault. Consider that an acceptable outcome.
        if test.support.verbose:
            print(""Checking crasher:"", fname)
        assert_python_failure(fname)
",if os . path . basename ( fname ) in infinite_loops :,110
"def __getitem__(self, k) -> ""SimMemView"":
    if isinstance(k, slice):
        if k.step is not None:
            raise ValueError(""Slices with strides are not supported"")
        elif k.start is None:
            raise ValueError(""Must specify start index"")
        elif k.stop is not None:
            raise ValueError(""Slices with stop index are not supported"")
        else:
            addr = k.start
    elif self._type is not None and self._type._can_refine_int:
        return self._type._refine(self, k)
    else:
        addr = k
    return self._deeper(addr=addr)
",elif k . stop is not None :,169
"def get_lowest_wall_time(jsons):
    lowest_wall = None
    for j in jsons:
        if lowest_wall is None:
            lowest_wall = j[""wall_time""]
        if lowest_wall > j[""wall_time""]:
            lowest_wall = j[""wall_time""]
    return lowest_wall
",if lowest_wall is None :,86
"def extract_wav_headers(data):
    # def search_subchunk(data, subchunk_id):
    pos = 12  # The size of the RIFF chunk descriptor
    subchunks = []
    while pos + 8 <= len(data) and len(subchunks) < 10:
        subchunk_id = data[pos : pos + 4]
        subchunk_size = struct.unpack_from(""<I"", data[pos + 4 : pos + 8])[0]
        subchunks.append(WavSubChunk(subchunk_id, pos, subchunk_size))
        if subchunk_id == b""data"":
            # 'data' is the last subchunk
            break
        pos += subchunk_size + 8
    return subchunks
","if subchunk_id == b""data"" :",183
"def _any_targets_have_native_sources(self, targets):
    # TODO(#5949): convert this to checking if the closure of python requirements has any
    # platform-specific packages (maybe find the platforms there too?).
    for tgt in targets:
        for type_constraint, target_predicate in self._native_target_matchers.items():
            if type_constraint.satisfied_by(tgt) and target_predicate(tgt):
                return True
    return False
",if type_constraint . satisfied_by ( tgt ) and target_predicate ( tgt ) :,115
"def validate_memory(self, value):
    for k, v in value.viewitems():
        if v is None:  # use NoneType to unset a value
            continue
        if not re.match(PROCTYPE_MATCH, k):
            raise serializers.ValidationError(""Process types can only contain [a-z]"")
        if not re.match(MEMLIMIT_MATCH, str(v)):
            raise serializers.ValidationError(
                ""Limit format: <number><unit>, where unit = B, K, M or G""
            )
    return value
","if not re . match ( PROCTYPE_MATCH , k ) :",141
"def cart_number_checksum_validation(cls, number):
    digits = []
    even = False
    if not number.isdigit():
        return False
    for digit in reversed(number):
        digit = ord(digit) - ord(""0"")
        if even:
            digit *= 2
            if digit >= 10:
                digit = digit % 10 + digit // 10
        digits.append(digit)
        even = not even
    return sum(digits) % 10 == 0 if digits else False
",if digit >= 10 :,127
"def transform(a, cmds):
    buf = a.split(""\n"")
    for cmd in cmds:
        ctype, line, col, char = cmd
        if ctype == ""D"":
            if char != ""\n"":
                buf[line] = buf[line][:col] + buf[line][col + len(char) :]
            else:
                buf[line] = buf[line] + buf[line + 1]
                del buf[line + 1]
        elif ctype == ""I"":
            buf[line] = buf[line][:col] + char + buf[line][col:]
        buf = ""\n"".join(buf).split(""\n"")
    return ""\n"".join(buf)
","if char != ""\n"" :",182
"def get_partners(self) -> Dict[AbstractNode, Set[int]]:
    partners = {}  # type: Dict[AbstractNode, Set[int]]
    for edge in self.edges:
        if edge.is_dangling():
            raise ValueError(""Cannot contract copy tensor with dangling edges"")
        if self._is_my_trace(edge):
            continue
        partner_node, shared_axis = self._get_partner(edge)
        if partner_node not in partners:
            partners[partner_node] = set()
        partners[partner_node].add(shared_axis)
    return partners
",if partner_node not in partners :,163
"def _bind_interactive_rez(self):
    if config.set_prompt and self.settings.prompt:
        stored_prompt = os.getenv(""REZ_STORED_PROMPT_CMD"")
        curr_prompt = stored_prompt or os.getenv(""PROMPT"", """")
        if not stored_prompt:
            self.setenv(""REZ_STORED_PROMPT_CMD"", curr_prompt)
        new_prompt = ""%%REZ_ENV_PROMPT%%""
        new_prompt = (
            (new_prompt + "" %s"") if config.prefix_prompt else (""%s "" + new_prompt)
        )
        new_prompt = new_prompt % curr_prompt
        self._addline(""set PROMPT=%s"" % new_prompt)
",if not stored_prompt :,182
"def __listingColumns(self):
    columns = []
    for name in self.__getColumns():
        definition = column(name)
        if not definition:
            IECore.msg(
                IECore.Msg.Level.Error,
                ""GafferImageUI.CatalogueUI"",
                ""No column registered with name '%s'"" % name,
            )
            continue
        if isinstance(definition, IconColumn):
            c = GafferUI.PathListingWidget.IconColumn(definition.title(), """", name)
        else:
            c = GafferUI.PathListingWidget.StandardColumn(definition.title(), name)
        columns.append(c)
    return columns
","if isinstance ( definition , IconColumn ) :",184
"def _check_invalid_keys(self, section_name, section):
    for key in section:
        key_name = str(key)
        valid_key_names = [s[0] for s in self.keys]
        is_valid_key = key_name in valid_key_names
        if not is_valid_key:
            err_msg = (
                ""'{0}' is not a valid key name for '{1}'. Must "" ""be one of these: {2}""
            ).format(key_name, section_name, "", "".join(valid_key_names))
            raise InvalidConfig(err_msg)
",if not is_valid_key :,160
"def _get_startup_packages(lib_path: Path, packages) -> Set[str]:
    names = set()
    for path in lib_path.iterdir():
        name = path.name
        if name == ""__pycache__"":
            continue
        if name.endswith("".py""):
            names.add(name.split(""."")[0])
        elif path.is_dir() and ""."" not in name:
            names.add(name)
    if packages:
        packages = {package.lower().replace(""-"", ""_"") for package in packages}
        if len(names & packages) == len(packages):
            return packages
    return names
","elif path . is_dir ( ) and ""."" not in name :",159
"def sortkeypicker(keynames):
    negate = set()
    for i, k in enumerate(keynames):
        if k[:1] == ""-"":
            keynames[i] = k[1:]
            negate.add(k[1:])
    def getit(adict):
        composite = [adict[k] for k in keynames]
        for i, (k, v) in enumerate(zip(keynames, composite)):
            if k in negate:
                composite[i] = -v
        return composite
    return getit
","if k [ : 1 ] == ""-"" :",140
"def iter_symbols(code):
    """"""Yield names and strings used by `code` and its nested code objects""""""
    for name in code.co_names:
        yield name
    for const in code.co_consts:
        if isinstance(const, six.string_types):
            yield const
        elif isinstance(const, CodeType):
            for name in iter_symbols(const):
                yield name
","elif isinstance ( const , CodeType ) :",104
"def set_study_directions(
    self, study_id: int, directions: Sequence[StudyDirection]
) -> None:
    with self._lock:
        if study_id in self._studies:
            current_directions = self._studies[study_id].directions
            if directions == current_directions:
                return
            elif (
                len(current_directions) == 1
                and current_directions[0] == StudyDirection.NOT_SET
            ):
                self._studies[study_id].directions = list(directions)
                self._backend.set_study_directions(study_id, directions)
                return
    self._backend.set_study_directions(study_id, directions)
",if study_id in self . _studies :,198
"def PreprocessConditionalStatement(self, IfList, ReplacedLine):
    while self:
        if self.__Token:
            x = 1
        elif not IfList:
            if self <= 2:
                continue
            RegionSizeGuid = 3
            if not RegionSizeGuid:
                RegionLayoutLine = 5
                continue
            RegionLayoutLine = self.CurrentLineNumber
    return 1
",if self . __Token :,111
"def _check_blocking(self, current_time):
    if self._switch_flag is False:
        active_greenlet = self._active_greenlet
        if active_greenlet is not None and active_greenlet != self._hub:
            self._notify_greenlet_blocked(active_greenlet, current_time)
    self._switch_flag = False
",if active_greenlet is not None and active_greenlet != self . _hub :,90
"def detect(get_page):
    retval = False
    for vector in WAF_ATTACK_VECTORS:
        page, headers, code = get_page(get=vector)
        retval = (
            re.search(r""BlockDos\.net"", headers.get(HTTP_HEADER.SERVER, """"), re.I)
            is not None
        )
        if retval:
            break
    return retval
",if retval :,109
"def _fastqc_data_section(self, section_name):
    out = []
    in_section = False
    data_file = os.path.join(self._dir, ""fastqc_data.txt"")
    if os.path.exists(data_file):
        with open(data_file) as in_handle:
            for line in in_handle:
                if line.startswith("">>%s"" % section_name):
                    in_section = True
                elif in_section:
                    if line.startswith("">>END""):
                        break
                    out.append(line.rstrip(""\r\n""))
    return out
","if line . startswith ( "">>%s"" % section_name ) :",173
"def shortcut(self, input, ch_out, stride, is_first, name):
    ch_in = input.shape[1]
    if ch_in != ch_out or stride != 1:
        if is_first or stride == 1:
            return self.conv_bn_layer(input, ch_out, 1, stride, name=name)
        else:
            return self.conv_bn_layer_new(input, ch_out, 1, stride, name=name)
    elif is_first:
        return self.conv_bn_layer(input, ch_out, 1, stride, name=name)
    else:
        return input
",if is_first or stride == 1 :,162
"def get_value_from_string(self, string_value):
    """"""Return internal representation starting from CFN/user-input value.""""""
    param_value = self.get_default_value()
    try:
        if string_value is not None:
            string_value = str(string_value).strip()
            if string_value != ""NONE"":
                param_value = int(string_value)
    except ValueError:
        self.pcluster_config.warn(
            ""Unable to convert the value '{0}' to an Integer. ""
            ""Using default value for parameter '{1}'"".format(string_value, self.key)
        )
    return param_value
","if string_value != ""NONE"" :",172
"def get_running(workers):
    running = []
    for worker in workers:
        current_test_name = worker.current_test_name
        if not current_test_name:
            continue
        dt = time.monotonic() - worker.start_time
        if dt >= PROGRESS_MIN_TIME:
            text = ""%s (%s)"" % (current_test_name, format_duration(dt))
            running.append(text)
    return running
",if not current_test_name :,120
"def generate_data(self, request):
    """"""Generate data for the widget.""""""
    uptime = {}
    cache_stats = get_cache_stats()
    if cache_stats:
        for hosts, stats in cache_stats:
            if stats[""uptime""] > 86400:
                uptime[""value""] = stats[""uptime""] / 60 / 60 / 24
                uptime[""unit""] = _(""days"")
            elif stats[""uptime""] > 3600:
                uptime[""value""] = stats[""uptime""] / 60 / 60
                uptime[""unit""] = _(""hours"")
            else:
                uptime[""value""] = stats[""uptime""] / 60
                uptime[""unit""] = _(""minutes"")
    return {""cache_stats"": cache_stats, ""uptime"": uptime}
","elif stats [ ""uptime"" ] > 3600 :",195
"def add_actors(self):
    """"""Adds `self.actors` to the scene.""""""
    if not self._actors_added:
        self.reader.render_window = self.scene.render_window
        self._update_reader()
        self._actors_added = True
        if not self.visible:
            self._visible_changed(self.visible)
        self.scene.render()
",if not self . visible :,103
"def _add_uniqu_suffix(self, titles):
    counters = dict()
    titles_with_suffix = []
    for title in titles:
        counters[title] = counters[title] + 1 if title in counters else 1
        if counters[title] > 1:
            title = f""{title} ({counters[title]})""
        titles_with_suffix.append(title)
    return titles_with_suffix
",if counters [ title ] > 1 :,103
"def _verify_udf_resources(self, job, config):
    udf_resources = config.get(""userDefinedFunctionResources"", ())
    self.assertEqual(len(job.udf_resources), len(udf_resources))
    for found, expected in zip(job.udf_resources, udf_resources):
        if ""resourceUri"" in expected:
            self.assertEqual(found.udf_type, ""resourceUri"")
            self.assertEqual(found.value, expected[""resourceUri""])
        else:
            self.assertEqual(found.udf_type, ""inlineCode"")
            self.assertEqual(found.value, expected[""inlineCode""])
","if ""resourceUri"" in expected :",157
"def __init__(
    self, layout, value=None, string=None, *, dtype: np.dtype = np.float64
) -> None:
    """"""Constructor.""""""
    self.layout = layout
    if value is None:
        if string is None:
            self.value = np.zeros((self.layout.gaDims,), dtype=dtype)
        else:
            self.value = layout.parse_multivector(string).value
    else:
        self.value = np.array(value)
        if self.value.shape != (self.layout.gaDims,):
            raise ValueError(
                ""value must be a sequence of length %s"" % self.layout.gaDims
            )
",if string is None :,180
"def read_file(filename, print_error=True):
    """"""Returns the contents of a file.""""""
    try:
        for encoding in [""utf-8"", ""latin1""]:
            try:
                with io.open(filename, encoding=encoding) as fp:
                    return fp.read()
            except UnicodeDecodeError:
                pass
    except IOError as exception:
        if print_error:
            print(exception, file=sys.stderr)
        return None
",if print_error :,126
"def get_albums_for_iter(self, iter_):
    obj = self.get_value(iter_)
    if isinstance(obj, AlbumNode):
        return {obj.album}
    albums = set()
    for child_iter, value in self.iterrows(iter_):
        if isinstance(value, AlbumNode):
            albums.add(value.album)
        else:
            albums.update(self.get_albums_for_iter(child_iter))
    return albums
","if isinstance ( value , AlbumNode ) :",120
"def wait_til_ready(cls, connector=None):
    if connector is None:
        connector = cls.connector
    while True:
        now = time.time()
        next_iteration = now // 1.0 + 1
        if connector.ready:
            break
        else:
            await cls._clock.run_til(next_iteration)
        await asyncio.sleep(1.0)
",if connector . ready :,106
"def remove_property(self, key):  # type: (str) -> None
    with self.secure() as config:
        keys = key.split(""."")
        current_config = config
        for i, key in enumerate(keys):
            if key not in current_config:
                return
            if i == len(keys) - 1:
                del current_config[key]
                break
            current_config = current_config[key]
",if i == len ( keys ) - 1 :,122
"def get(self, hash160, default=None):
    v = self.p2s_for_hash(hash160)
    if v:
        return v
    if hash160 not in self._secret_exponent_cache:
        v = self.path_for_hash160(hash160)
        if v:
            fingerprint, path = v
            for key in self._secrets.get(fingerprint, []):
                subkey = key.subkey_for_path(path)
                self._add_key_to_cache(subkey)
    return self._secret_exponent_cache.get(hash160, default)
",if v :,155
"def fetch_all(self, api_client, fetchstatuslogger, q, targets):
    self.fetchstatuslogger = fetchstatuslogger
    if targets != None:
        # Ensure targets is a tuple
        if type(targets) != list and type(targets) != tuple:
            targets = tuple(
                targets,
            )
        elif type(targets) != tuple:
            targets = tuple(targets)
    for target in targets:
        self._fetch_targets(api_client, q, target)
",if type ( targets ) != list and type ( targets ) != tuple :,130
"def dgl_mp_batchify_fn(data):
    if isinstance(data[0], tuple):
        data = zip(*data)
        return [dgl_mp_batchify_fn(i) for i in data]
    for dt in data:
        if dt is not None:
            if isinstance(dt, dgl.DGLGraph):
                return [d for d in data if isinstance(d, dgl.DGLGraph)]
            elif isinstance(dt, nd.NDArray):
                pad = Pad(axis=(1, 2), num_shards=1, ret_length=False)
                data_list = [dt for dt in data if dt is not None]
                return pad(data_list)
",if dt is not None :,183
"def capture_server(evt, buf, serv):
    try:
        serv.listen(5)
        conn, addr = serv.accept()
    except socket.timeout:
        pass
    else:
        n = 200
        while n > 0:
            r, w, e = select.select([conn], [], [])
            if r:
                data = conn.recv(10)
                # keep everything except for the newline terminator
                buf.write(data.replace(""\n"", """"))
                if ""\n"" in data:
                    break
            n -= 1
            time.sleep(0.01)
        conn.close()
    finally:
        serv.close()
        evt.set()
",if r :,195
"def elem():
    if ints_only:
        return random.randint(0, 10000000000)
    else:
        t = random.randint(0, 2)
        if t == 0:
            return random.randint(0, 10000000000)
        elif t == 1:
            return float(random.randint(0, 10000000000))
        elif strings is not None:
            return strings[random.randint(0, len(strings) - 1)]
        return random_string(random.randint(100, 1000))
",elif strings is not None :,132
"def has_changed(self, initial, data):
    if self.disabled:
        return False
    if initial is None:
        initial = ["""" for x in range(0, len(data))]
    else:
        if not isinstance(initial, list):
            initial = self.widget.decompress(initial)
    for field, initial, data in zip(self.fields, initial, data):
        try:
            initial = field.to_python(initial)
        except ValidationError:
            return True
        if field.has_changed(initial, data):
            return True
    return False
","if not isinstance ( initial , list ) :",151
"def _load_testfile(filename, package, module_relative):
    if module_relative:
        package = _normalize_module(package, 3)
        filename = _module_relative_path(package, filename)
        if hasattr(package, ""__loader__""):
            if hasattr(package.__loader__, ""get_data""):
                file_contents = package.__loader__.get_data(filename)
                # get_data() opens files as 'rb', so one must do the equivalent
                # conversion as universal newlines would do.
                return file_contents.replace(os.linesep, ""\n""), filename
    return open(filename).read(), filename
","if hasattr ( package , ""__loader__"" ) :",163
"def release(self):
    tid = _thread.get_ident()
    with self.lock:
        if self.owner != tid:
            raise RuntimeError(""cannot release un-acquired lock"")
        assert self.count > 0
        self.count -= 1
        if self.count == 0:
            self.owner = None
            if self.waiters:
                self.waiters -= 1
                self.wakeup.release()
",if self . waiters :,117
"def stage(
    self, x, num_modules, num_blocks, channels, multi_scale_output=True, name=None
):
    out = x
    for i in range(num_modules):
        if i == num_modules - 1 and multi_scale_output == False:
            out = self.high_resolution_module(
                out,
                num_blocks,
                channels,
                multi_scale_output=False,
                name=name + ""_"" + str(i + 1),
            )
        else:
            out = self.high_resolution_module(
                out, num_blocks, channels, name=name + ""_"" + str(i + 1)
            )
    return out
",if i == num_modules - 1 and multi_scale_output == False :,193
"def changeFrontAlteration(intV, alter):
    # fati = front alteration transpose interval
    fati = self.frontAlterationTransposeInterval
    if fati:
        newFati = interval.add([fati, intV])
        self.frontAlterationTransposeInterval = newFati
        self.frontAlterationAccidental.alter = (
            self.frontAlterationAccidental.alter + alter
        )
        if self.frontAlterationAccidental.alter == 0:
            self.frontAlterationTransposeInterval = None
            self.frontAlterationAccidental = None
    else:
        self.frontAlterationTransposeInterval = intV
        self.frontAlterationAccidental = pitch.Accidental(alter)
",if self . frontAlterationAccidental . alter == 0 :,199
"def set_to_train(self):
    for T in self.trainable_attributes():
        for k, v in T.items():
            if k in self.freeze_these:
                c_f.set_requires_grad(v, requires_grad=False)
                v.eval()
            else:
                v.train()
    self.maybe_freeze_trunk_batchnorm()
",if k in self . freeze_these :,107
"def _migrate(self, sig=None, compact=True):
    with self.lock:
        sig = sig or self.sig
        if sig in GPL_NEVER_MIGRATE:
            return
        if sig in self.WORDS and len(self.WORDS[sig]) > 0:
            PostingList.Append(
                self.session, sig, self.WORDS[sig], sig=sig, compact=compact
            )
            del self.WORDS[sig]
",if sig in GPL_NEVER_MIGRATE :,123
"def on_prediction_step(self, args, state, control, eval_dataloader=None, **kwargs):
    if self.prediction_bar is None:
        if self.training_tracker is not None:
            self.prediction_bar = self.training_tracker.add_child(len(eval_dataloader))
        else:
            self.prediction_bar = NotebookProgressBar(len(eval_dataloader))
        self.prediction_bar.update(1)
    else:
        self.prediction_bar.update(self.prediction_bar.value + 1)
",if self . training_tracker is not None :,137
"def show(self, indent=0):
    """"""Pretty print this structure.""""""
    if indent == 0:
        print(""struct {}"".format(self.name))
    for field in self.fields:
        if field.offset is None:
            offset = ""0x??""
        else:
            offset = ""0x{:02x}"".format(field.offset)
        print(""{}+{} {} {}"".format("" "" * indent, offset, field.name, field.type))
        if isinstance(field.type, Structure):
            field.type.show(indent + 1)
","if isinstance ( field . type , Structure ) :",143
"def __exit__(self, exc, value, tb):
    for key in self.overrides.keys():
        old_value = self.old[key]
        if old_value is NULL:
            delattr(self.instance, key)
        else:
            setattr(self.instance, key, old_value)
    self.instance.save()
",if old_value is NULL :,88
"def complete(self, block):
    with self._condition:
        if not self._final:
            return False
        if self._complete():
            self._calculate_state_root_if_not_already_done()
            return True
        if block:
            self._condition.wait_for(self._complete)
            self._calculate_state_root_if_not_already_done()
            return True
        return False
",if not self . _final :,117
"def parseArguments(self):
    args = []
    self.expect(""("")
    if not self.match("")""):
        while self.startIndex < self.length:
            args.append(self.isolateCoverGrammar(self.parseAssignmentExpression))
            if self.match("")""):
                break
            self.expectCommaSeparator()
    self.expect("")"")
    return args
","if self . match ( "")"" ) :",95
"def isValidDateString(config_param_name, value, valid_value):
    try:
        if value == ""DD-MM-YYYY"":
            return value
        day, month, year = value.split(""-"")
        if int(day) < 1 or int(day) > 31:
            raise DateStringValueError(config_param_name, value)
        if int(month) < 1 or int(month) > 12:
            raise DateStringValueError(config_param_name, value)
        if int(year) < 1900 or int(year) > 2013:
            raise DateStringValueError(config_param_name, value)
        return value
    except Exception:
        raise DateStringValueError(config_param_name, value)
",if int ( day ) < 1 or int ( day ) > 31 :,187
"def build_tree(path):
    tree = Tree()
    for basename, entry in trees[path].items():
        if isinstance(entry, dict):
            mode = stat.S_IFDIR
            sha = build_tree(pathjoin(path, basename))
        else:
            (mode, sha) = entry
        tree.add(basename, mode, sha)
    object_store.add_object(tree)
    return tree.id
","if isinstance ( entry , dict ) :",113
"def get_quarantine_count(self):
    """"""get obj/container/account quarantine counts""""""
    qcounts = {""objects"": 0, ""containers"": 0, ""accounts"": 0}
    qdir = ""quarantined""
    for device in os.listdir(self.devices):
        for qtype in qcounts:
            qtgt = os.path.join(self.devices, device, qdir, qtype)
            if os.path.exists(qtgt):
                linkcount = os.lstat(qtgt).st_nlink
                if linkcount > 2:
                    qcounts[qtype] += linkcount - 2
    return qcounts
",if os . path . exists ( qtgt ) :,171
"def _is_static_shape(self, shape):
    if shape is None or not isinstance(shape, list):
        return False
    for dim_value in shape:
        if not isinstance(dim_value, int):
            return False
        if dim_value < 0:
            raise Exception(""Negative dimension is illegal: %d"" % dim_value)
    return True
","if not isinstance ( dim_value , int ) :",94
"def BraceDetectAll(words):
    # type: (List[compound_word]) -> List[word_t]
    """"""Return a new list of words, possibly with BracedTree instances.""""""
    out = []  # type: List[word_t]
    for w in words:
        # The shortest possible brace expansion is {,}.  This heuristic prevents
        # a lot of garbage from being created, since otherwise nearly every word
        # would be checked.  We could be even more precise but this is cheap.
        if len(w.parts) >= 3:
            brace_tree = _BraceDetect(w)
            if brace_tree:
                out.append(brace_tree)
                continue
        out.append(w)
    return out
",if brace_tree :,191
"def __init__(original, self, *args, **kwargs):
    data = args[0] if len(args) > 0 else kwargs.get(""data"")
    if data is not None:
        try:
            if isinstance(data, str):
                raise Exception(
                    ""cannot gather example input when dataset is loaded from a file.""
                )
            input_example_info = _InputExampleInfo(
                input_example=deepcopy(data[:INPUT_EXAMPLE_SAMPLE_ROWS])
            )
        except Exception as e:
            input_example_info = _InputExampleInfo(error_msg=str(e))
        setattr(self, ""input_example_info"", input_example_info)
    original(self, *args, **kwargs)
","if isinstance ( data , str ) :",198
"def setRow(self, row, vals):
    if row > self.rowCount() - 1:
        self.setRowCount(row + 1)
    for col in range(len(vals)):
        val = vals[col]
        item = self.itemClass(val, row)
        item.setEditable(self.editable)
        sortMode = self.sortModes.get(col, None)
        if sortMode is not None:
            item.setSortMode(sortMode)
        format = self._formats.get(col, self._formats[None])
        item.setFormat(format)
        self.items.append(item)
        self.setItem(row, col, item)
        item.setValue(val)  # Required--the text-change callback is invoked
",if sortMode is not None :,198
"def wakeUp(self):
    """"""Write one byte to the pipe, and flush it.""""""
    # We don't use fdesc.writeToFD since we need to distinguish
    # between EINTR (try again) and EAGAIN (do nothing).
    if self.o is not None:
        try:
            util.untilConcludes(os.write, self.o, b""x"")
        except OSError as e:
            # XXX There is no unit test for raising the exception
            # for other errnos. See #4285.
            if e.errno != errno.EAGAIN:
                raise
",if e . errno != errno . EAGAIN :,158
"def _setup(self, field_name, owner_model):
    # Resolve possible name-based model references.
    resolved_classes = []
    for m in self.model_classes:
        if isinstance(m, string_type):
            if m == owner_model.__name__:
                resolved_classes.append(owner_model)
            else:
                raise Exception(
                    ""PolyModelType: Unable to resolve model '{}'."".format(m)
                )
        else:
            resolved_classes.append(m)
    self.model_classes = tuple(resolved_classes)
    super(PolyModelType, self)._setup(field_name, owner_model)
","if isinstance ( m , string_type ) :",176
"def _wrap_forwarded(self, key, value):
    if isinstance(value, SourceCode) and value.late_binding:
        # get cached return value if present
        value_ = self._late_binding_returnvalues.get(key, KeyError)
        if value_ is KeyError:
            # evaluate the late-bound function
            value_ = self._eval_late_binding(value)
            schema = self.late_bind_schemas.get(key)
            if schema is not None:
                value_ = schema.validate(value_)
            # cache result of late bound func
            self._late_binding_returnvalues[key] = value_
        return value_
    else:
        return value
",if schema is not None :,187
"def convert(self, ctx, argument):
    arg = argument.replace(""0x"", """").lower()
    if arg[0] == ""#"":
        arg = arg[1:]
    try:
        value = int(arg, base=16)
        if not (0 <= value <= 0xFFFFFF):
            raise BadColourArgument(arg)
        return discord.Colour(value=value)
    except ValueError:
        arg = arg.replace("" "", ""_"")
        method = getattr(discord.Colour, arg, None)
        if arg.startswith(""from_"") or method is None or not inspect.ismethod(method):
            raise BadColourArgument(arg)
        return method()
",if not ( 0 <= value <= 0xFFFFFF ) :,172
"def get_versions(*, all=False, quiet=None):
    import bonobo
    from bonobo.util.pkgs import bonobo_packages
    yield _format_version(bonobo, quiet=quiet)
    if all:
        for name in sorted(bonobo_packages):
            if name != ""bonobo"":
                try:
                    mod = __import__(name.replace(""-"", ""_""))
                    try:
                        yield _format_version(mod, name=name, quiet=quiet)
                    except Exception as exc:
                        yield ""{} ({})"".format(name, exc)
                except ImportError as exc:
                    yield ""{} is not importable ({})."".format(name, exc)
","if name != ""bonobo"" :",188
"def assertOperationsInjected(self, plan, **kwargs):
    for migration, _backward in plan:
        operations = iter(migration.operations)
        for operation in operations:
            if isinstance(operation, migrations.RenameModel):
                next_operation = next(operations)
                self.assertIsInstance(
                    next_operation, contenttypes_management.RenameContentType
                )
                self.assertEqual(next_operation.app_label, migration.app_label)
                self.assertEqual(next_operation.old_model, operation.old_name_lower)
                self.assertEqual(next_operation.new_model, operation.new_name_lower)
","if isinstance ( operation , migrations . RenameModel ) :",177
"def valid_localparts(strip_delimiters=False):
    for line in ABRIDGED_LOCALPART_VALID_TESTS.split(""\n""):
        # strip line, skip over empty lines
        line = line.strip()
        if line == """":
            continue
        # skip over comments or empty lines
        match = COMMENT.match(line)
        if match:
            continue
        # skip over localparts with delimiters
        if strip_delimiters:
            if "","" in line or "";"" in line:
                continue
        yield line
",if strip_delimiters :,145
"def read_lccn(line, is_marc8=False):
    found = []
    for k, v in get_raw_subfields(line, [""a""]):
        lccn = v.strip()
        if re_question.match(lccn):
            continue
        m = re_lccn.search(lccn)
        if not m:
            continue
        # remove letters and bad chars
        lccn = re_letters_and_bad.sub("""", m.group(1)).strip()
        if lccn:
            found.append(lccn)
    return found
",if lccn :,151
"def test_named_parameters_and_constraints(self):
    likelihood = gpytorch.likelihoods.GaussianLikelihood()
    model = ExactGPModel(None, None, likelihood)
    for name, _param, constraint in model.named_parameters_and_constraints():
        if name == ""likelihood.noise_covar.raw_noise"":
            self.assertIsInstance(constraint, gpytorch.constraints.GreaterThan)
        elif name == ""mean_module.constant"":
            self.assertIsNone(constraint)
        elif name == ""covar_module.raw_outputscale"":
            self.assertIsInstance(constraint, gpytorch.constraints.Positive)
        elif name == ""covar_module.base_kernel.raw_lengthscale"":
            self.assertIsInstance(constraint, gpytorch.constraints.Positive)
","elif name == ""covar_module.base_kernel.raw_lengthscale"" :",192
"def _cleanupSocket(self):
    """"""Close the Connection's socket.""""""
    try:
        self._sock.shutdown(socket.SHUT_WR)
    except:
        return
    try:
        while True:
            r, w, e = select.select([self._sock], [], [])
            if not r or not self._sock.recv(1024):
                break
    except:
        pass
    self._sock.close()
",if not r or not self . _sock . recv ( 1024 ) :,113
"def fadeIn(self, acts=None, t=None, duration=None):
    """"""Gradually switch on the input list of meshes by increasing opacity.""""""
    if self.bookingMode:
        acts, t, duration, rng = self._parse(acts, t, duration)
        for tt in rng:
            alpha = linInterpolate(tt, [t, t + duration], [0, 1])
            self.events.append((tt, self.fadeIn, acts, alpha))
    else:
        for a in self._performers:
            if a.alpha() >= self._inputvalues:
                continue
            a.alpha(self._inputvalues)
    return self
",if a . alpha ( ) >= self . _inputvalues :,172
"def get_config_updates_recursive(self):
    config_updates = self.config_updates.copy()
    for sr_path, subrunner in self.subrunners.items():
        if not is_prefix(self.path, sr_path):
            continue
        update = subrunner.get_config_updates_recursive()
        if update:
            config_updates[rel_path(self.path, sr_path)] = update
    return config_updates
","if not is_prefix ( self . path , sr_path ) :",115
"def setArgs(self, **kwargs):
    """"""See GridSearchCostGamma""""""
    for key, value in list(kwargs.items()):
        if key in (""folds"", ""nfolds""):
            self._n_folds = int(value)
        elif key in (""max_epochs""):
            self._validator_kwargs[""max_epochs""] = value
        else:
            GridSearchDOE.setArgs(self, **{key: value})
","elif key in ( ""max_epochs"" ) :",111
"def _parse_composite_axis(composite_axis_name: str):
    axes_names = [axis for axis in composite_axis_name.split("" "") if len(axis) > 0]
    for axis in axes_names:
        if axis == ""_"":
            continue
        assert ""a"" <= axis[0] <= ""z""
        for letter in axis:
            assert str.isdigit(letter) or ""a"" <= letter <= ""z""
    return axes_names
","if axis == ""_"" :",117
"def visit_For(self, node, for_branch=""body"", **kwargs):
    if for_branch == ""body"":
        self.sym_visitor.visit(node.target, store_as_param=True)
        branch = node.body
    elif for_branch == ""else"":
        branch = node.else_
    elif for_branch == ""test"":
        self.sym_visitor.visit(node.target, store_as_param=True)
        if node.test is not None:
            self.sym_visitor.visit(node.test)
        return
    else:
        raise RuntimeError(""Unknown for branch"")
    for item in branch or ():
        self.sym_visitor.visit(item)
",if node . test is not None :,178
"def contains_only_whitespace(node):
    if is_tag(node):
        if not any([not is_text(s) for s in node.contents]):
            if not any([unicode(s).strip() for s in node.contents]):
                return True
    return False
",if not any ( [ not is_text ( s ) for s in node . contents ] ) :,72
"def dir_tag_click(event):
    mouse_index = self.path_bar.index(""@%d,%d"" % (event.x, event.y))
    lineno = int(float(mouse_index))
    if lineno == 1:
        self.request_focus_into("""")
    else:
        assert lineno == 2
        dir_range = get_dir_range(event)
        if dir_range:
            _, end_index = dir_range
            path = self.path_bar.get(""2.0"", end_index)
            if path.endswith("":""):
                path += ""\\""
            self.request_focus_into(path)
","if path . endswith ( "":"" ) :",168
"def validate_employee_id(self):
    if self.employee:
        sales_person = frappe.db.get_value(""Sales Person"", {""employee"": self.employee})
        if sales_person and sales_person != self.name:
            frappe.throw(
                _(""Another Sales Person {0} exists with the same Employee id"").format(
                    sales_person
                )
            )
",if sales_person and sales_person != self . name :,116
"def pytest_collection_modifyitems(items):
    for item in items:
        if item.nodeid.startswith(""tests/infer""):
            if ""stage"" not in item.keywords:
                item.add_marker(pytest.mark.stage(""unit""))
            if ""init"" not in item.keywords:
                item.add_marker(pytest.mark.init(rng_seed=123))
","if ""init"" not in item . keywords :",102
"def poll(self, timeout):
    if timeout < 0:
        timeout = None  # kqueue behaviour
    events = self._kqueue.control(None, KqueueLoop.MAX_EVENTS, timeout)
    results = defaultdict(lambda: POLL_NULL)
    for e in events:
        fd = e.ident
        if e.filter == select.KQ_FILTER_READ:
            results[fd] |= POLL_IN
        elif e.filter == select.KQ_FILTER_WRITE:
            results[fd] |= POLL_OUT
    return results.items()
",elif e . filter == select . KQ_FILTER_WRITE :,142
"def _read_dimensions(self, *dimnames, **kwargs):
    path = kwargs.get(""path"", ""/"")
    try:
        if path == ""/"":
            return [self.rootgrp.dimensions[dname] for dname in dimnames]
        group = self.path2group[path]
        return [group.dimensions[dname] for dname in dimnames]
    except KeyError:
        raise self.Error(
            ""In file %s:\nError while reading dimensions: `%s` with kwargs: `%s`""
            % (self.path, dimnames, kwargs)
        )
","if path == ""/"" :",150
"def spam_to_me(address):
    sock = eventlet.connect(address)
    while True:
        try:
            sock.sendall(b""hello world"")
            # Arbitrary delay to not use all available CPU, keeps the test
            # running quickly and reliably under a second
            time.sleep(0.001)
        except socket.error as e:
            if get_errno(e) == errno.EPIPE:
                return
            raise
",if get_errno ( e ) == errno . EPIPE :,122
"def has_hash_of(self, destpath, code, package_level):
    """"""Determine if a file has the hash of the code.""""""
    if destpath is not None and os.path.isfile(destpath):
        with univ_open(destpath, ""r"") as opened:
            compiled = readfile(opened)
        hashash = gethash(compiled)
        if hashash is not None and hashash == self.comp.genhash(code, package_level):
            return True
    return False
","if hashash is not None and hashash == self . comp . genhash ( code , package_level ) :",126
"def insert(self, index, item):
    if len(self.lists) == 1:
        self.lists[0].insert(index, item)
        self._balance_list(0)
    else:
        list_idx, rel_idx = self._translate_index(index)
        if list_idx is None:
            raise IndexError()
        self.lists[list_idx].insert(rel_idx, item)
        self._balance_list(list_idx)
    return
",if list_idx is None :,123
"def _parse_class_simplified(symbol):
    results = {}
    name = symbol.name + ""(""
    name += "", "".join([analyzer.expand_attribute(base) for base in symbol.bases])
    name += "")""
    for sym in symbol.body:
        if isinstance(sym, ast.FunctionDef):
            result = _parse_function_simplified(sym, symbol.name)
            results.update(result)
        elif isinstance(sym, ast.ClassDef):
            result = _parse_class_simplified(sym)
            results.update(result)
    lineno = symbol.lineno
    for decorator in symbol.decorator_list:
        lineno += 1
    results[lineno] = (name, ""c"")
    return results
","elif isinstance ( sym , ast . ClassDef ) :",181
"def append_vars(pairs, result):
    for name, value in sorted(pairs.items()):
        if isinstance(value, list):
            value = ""[%s]"" % "","".join(value)
        if package:
            result.append(""%s:%s=%s"" % (package, name, value))
        else:
            result.append(""%s=%s"" % (name, value))
",if package :,99
"def nextEditable(self):
    """"""Moves focus of the cursor to the next editable window""""""
    if self.currentEditable is None:
        if len(self._editableChildren):
            self._currentEditableRef = self._editableChildren[0]
    else:
        for ref in weakref.getweakrefs(self.currentEditable):
            if ref in self._editableChildren:
                cei = self._editableChildren.index(ref)
                nei = cei + 1
                if nei >= len(self._editableChildren):
                    nei = 0
                self._currentEditableRef = self._editableChildren[nei]
    return self.currentEditable
",if len ( self . _editableChildren ) :,179
"def everythingIsUnicode(d):
    """"""Takes a dictionary, recursively verifies that every value is unicode""""""
    for k, v in d.iteritems():
        if isinstance(v, dict) and k != ""headers"":
            if not everythingIsUnicode(v):
                return False
        elif isinstance(v, list):
            for i in v:
                if isinstance(i, dict) and not everythingIsUnicode(i):
                    return False
                elif isinstance(i, _bytes):
                    return False
        elif isinstance(v, _bytes):
            return False
    return True
","elif isinstance ( v , _bytes ) :",158
"def is_valid(sample):
    if sample is None:
        return False
    if isinstance(sample, tuple):
        for s in sample:
            if s is None:
                return False
            elif isinstance(s, np.ndarray) and s.size == 0:
                return False
            elif isinstance(s, collections.abc.Sequence) and len(s) == 0:
                return False
    return True
","elif isinstance ( s , np . ndarray ) and s . size == 0 :",114
"def scan_resource_conf(self, conf):
    if ""properties"" in conf:
        if ""attributes"" in conf[""properties""]:
            if ""exp"" in conf[""properties""][""attributes""]:
                if conf[""properties""][""attributes""][""exp""]:
                    return CheckResult.PASSED
    return CheckResult.FAILED
","if conf [ ""properties"" ] [ ""attributes"" ] [ ""exp"" ] :",82
"def encode(self):
    if self.expr in gpregs.expr:
        self.value = gpregs.expr.index(self.expr)
        self.parent.rot2.value = 0
    elif isinstance(self.expr, ExprOp) and self.expr.op == allshifts[3]:
        reg, value = self.expr.args
        if reg not in gpregs.expr:
            return False
        self.value = gpregs.expr.index(reg)
        if not isinstance(value, ExprInt):
            return False
        value = int(value)
        if not value in [8, 16, 24]:
            return False
        self.parent.rot2.value = value // 8
    return True
",if reg not in gpregs . expr :,192
"def validate_transaction_reference(self):
    bank_account = self.paid_to if self.payment_type == ""Receive"" else self.paid_from
    bank_account_type = frappe.db.get_value(""Account"", bank_account, ""account_type"")
    if bank_account_type == ""Bank"":
        if not self.reference_no or not self.reference_date:
            frappe.throw(
                _(""Reference No and Reference Date is mandatory for Bank transaction"")
            )
",if not self . reference_no or not self . reference_date :,128
"def monad(self):
    if not self.cls_bl_idname:
        return None
    for monad in bpy.data.node_groups:
        if hasattr(monad, ""cls_bl_idname""):
            if monad.cls_bl_idname == self.cls_bl_idname:
                return monad
    return None
",if monad . cls_bl_idname == self . cls_bl_idname :,93
"def _create_mask(self, plen):
    mask = []
    for i in range(16):
        if plen >= 8:
            mask.append(0xFF)
        elif plen > 0:
            mask.append(0xFF >> (8 - plen) << (8 - plen))
        else:
            mask.append(0x00)
        plen -= 8
    return mask
",elif plen > 0 :,107
"def dataset_to_stream(dataset, input_name):
    """"""Takes a tf.Dataset and creates a numpy stream of ready batches.""""""
    # All input-pipeline processing should be on CPU.
    for example in fastmath.dataset_as_numpy(dataset):
        features = example[0]
        inp, out = features[input_name], example[1]
        mask = features[""mask""] if ""mask"" in features else None
        # Some accelerators don't handle uint8 well, cast to int.
        if isinstance(inp, np.uint8):
            inp = inp.astype(np.int32)
        if isinstance(out, np.uint8):
            out = out.astype(np.int32)
        yield (inp, out) if mask is None else (inp, out, mask)
","if isinstance ( inp , np . uint8 ) :",198
"def _idle_redraw_cb(self):
    assert self._idle_redraw_src_id is not None
    queue = self._idle_redraw_queue
    if len(queue) > 0:
        bbox = queue.pop(0)
        if bbox is None:
            super(CanvasRenderer, self).queue_draw()
        else:
            super(CanvasRenderer, self).queue_draw_area(*bbox)
    if len(queue) == 0:
        self._idle_redraw_src_id = None
        return False
    return True
",if bbox is None :,138
"def mutated(self, indiv):
    """"""mutate some genes of the given individual""""""
    res = indiv.copy()
    # to avoid having a child identical to one of the currentpopulation'''
    for i in range(self.numParameters):
        if random() < self.mutationProb:
            if self.xBound is None:
                res[i] = indiv[i] + gauss(0, self.mutationStdDev)
            else:
                res[i] = max(
                    min(indiv[i] + gauss(0, self.mutationStdDev), self.maxs[i]),
                    self.mins[i],
                )
    return res
",if random ( ) < self . mutationProb :,177
"def _justifyDrawParaLine(tx, offset, extraspace, words, last=0):
    setXPos(tx, offset)
    text = b"" "".join(words)
    if last:
        # last one, left align
        tx._textOut(text, 1)
    else:
        nSpaces = len(words) - 1
        if nSpaces:
            tx.setWordSpace(extraspace / float(nSpaces))
            tx._textOut(text, 1)
            tx.setWordSpace(0)
        else:
            tx._textOut(text, 1)
    setXPos(tx, -offset)
    return offset
",if nSpaces :,168
"def _read_0(self, stream):
    r = b""""
    while True:
        c = stream.read(2)
        if len(c) != 2:
            raise EOFError()
        if c == b""\x00\x00"":
            break
        r += c
    return r.decode(self.encoding)
",if len ( c ) != 2 :,87
"def run(self, app, editor, args):
    line_nums = []
    for cursor in editor.cursors:
        if cursor.y not in line_nums:
            line_nums.append(cursor.y)
            data = editor.lines[cursor.y].get_data().upper()
            editor.lines[cursor.y].set_data(data)
",if cursor . y not in line_nums :,94
"def create_default_energy_point_rules():
    for rule in get_default_energy_point_rules():
        # check if any rule for ref. doctype exists
        rule_exists = frappe.db.exists(
            ""Energy Point Rule"", {""reference_doctype"": rule.get(""reference_doctype"")}
        )
        if rule_exists:
            continue
        doc = frappe.get_doc(rule)
        doc.insert(ignore_permissions=True)
",if rule_exists :,127
"def __new__(cls, *nodes):
    if not nodes:
        raise TypeError(""DisjunctionNode() requires at least one node"")
    elif len(nodes) == 1:
        return nodes[0]
    self = super(DisjunctionNode, cls).__new__(cls)
    self.__nodes = []
    # TODO: Remove duplicates?
    for node in nodes:
        if not isinstance(node, Node):
            raise TypeError(
                ""DisjunctionNode() expects Node instances as arguments;""
                "" received a non-Node instance %r"" % node
            )
        if isinstance(node, DisjunctionNode):
            self.__nodes.extend(node.__nodes)
        else:
            self.__nodes.append(node)
    return self
","if isinstance ( node , DisjunctionNode ) :",191
"def dfs(v: str) -> Iterator[Set[str]]:
    index[v] = len(stack)
    stack.append(v)
    boundaries.append(index[v])
    for w in edges[v]:
        if w not in index:
            yield from dfs(w)
        elif w not in identified:
            while index[w] < boundaries[-1]:
                boundaries.pop()
    if boundaries[-1] == index[v]:
        boundaries.pop()
        scc = set(stack[index[v] :])
        del stack[index[v] :]
        identified.update(scc)
        yield scc
",elif w not in identified :,162
"def unpack_item_obj(map_uuid_global_id, misp_obj):
    obj_meta = get_object_metadata(misp_obj)
    obj_id = None
    io_content = None
    for attribute in misp_obj.attributes:
        if attribute.object_relation == ""raw-data"":
            obj_id = attribute.value  # # TODO: sanitize
            io_content = attribute.data  # # TODO: check if type == io
    if obj_id and io_content:
        res = Item.create_item(obj_id, obj_meta, io_content)
        map_uuid_global_id[misp_obj.uuid] = get_global_id(""item"", obj_id)
","if attribute . object_relation == ""raw-data"" :",182
"def parse(self, response):
    soup = BeautifulSoup(response.content.decode(""utf-8"", ""ignore""), ""lxml"")
    image_divs = soup.find_all(""div"", class_=""imgpt"")
    pattern = re.compile(r""murl\"":\""(.*?)\.jpg"")
    for div in image_divs:
        href_str = html_parser.HTMLParser().unescape(div.a[""m""])
        match = pattern.search(href_str)
        if match:
            name = match.group(1) if six.PY3 else match.group(1).encode(""utf-8"")
            img_url = ""{}.jpg"".format(name)
            yield dict(file_url=img_url)
",if match :,180
"def filter_errors(self, errors: List[str]) -> List[str]:
    real_errors: List[str] = list()
    current_file = __file__
    current_path = os.path.split(current_file)
    for line in errors:
        line = line.strip()
        if not line:
            continue
        fn, lno, lvl, msg = self.parse_trace_line(line)
        if fn is not None:
            _path = os.path.split(fn)
            if _path[-1] != current_path[-1]:
                continue
        real_errors.append(line)
    return real_errors
",if fn is not None :,171
"def decompileFormat1(self, reader, otFont):
    self.classDefs = classDefs = []
    startGlyphID = reader.readUShort()
    glyphCount = reader.readUShort()
    for i in range(glyphCount):
        glyphName = otFont.getglyphName(startGlyphID + i)
        classValue = reader.readUShort()
        if classValue:
            classDefs.append((glyphName, classValue))
",if classValue :,118
"def compress(self, data_list):
    if len(data_list) == 2:
        value, lookup_expr = data_list
        if value not in EMPTY_VALUES:
            if lookup_expr not in EMPTY_VALUES:
                return Lookup(value=value, lookup_expr=lookup_expr)
            else:
                raise forms.ValidationError(
                    self.error_messages[""lookup_required""], code=""lookup_required""
                )
    return None
",if value not in EMPTY_VALUES :,127
"def open_compat(path, mode=""r""):
    if mode in [""r"", ""rb""] and not os.path.exists(path):
        raise FileNotFoundError(u'The file ""%s"" could not be found' % path)
    if sys.version_info >= (3,):
        encoding = ""utf-8""
        errors = ""replace""
        if mode in [""rb"", ""wb"", ""ab""]:
            encoding = None
            errors = None
        return open(path, mode, encoding=encoding, errors=errors)
    else:
        return open(path, mode)
","if mode in [ ""rb"" , ""wb"" , ""ab"" ] :",145
"def filter_errors(self, errors: List[str]) -> List[str]:
    real_errors: List[str] = list()
    current_file = __file__
    current_path = os.path.split(current_file)
    for line in errors:
        line = line.strip()
        if not line:
            continue
        fn, lno, lvl, msg = self.parse_trace_line(line)
        if fn is not None:
            _path = os.path.split(fn)
            if _path[-1] != current_path[-1]:
                continue
        real_errors.append(line)
    return real_errors
",if _path [ - 1 ] != current_path [ - 1 ] :,171
"def filter_by_level(record, level_per_module):
    name = record[""name""]
    level = 0
    if name in level_per_module:
        level = level_per_module[name]
    elif name is not None:
        lookup = """"
        if """" in level_per_module:
            level = level_per_module[""""]
        for n in name.split("".""):
            lookup += n
            if lookup in level_per_module:
                level = level_per_module[lookup]
            lookup += "".""
    if level is False:
        return False
    return record[""level""].no >= level
",if lookup in level_per_module :,166
"def CountButtons(self):
    """"""Returns the number of visible buttons in the docked pane.""""""
    n = 0
    if self.HasCaption() or self.HasCaptionLeft():
        if isinstance(wx.GetTopLevelParent(self.window), AuiFloatingFrame):
            return 1
        if self.HasCloseButton():
            n += 1
        if self.HasMaximizeButton():
            n += 1
        if self.HasMinimizeButton():
            n += 1
        if self.HasPinButton():
            n += 1
    return n
",if self . HasMaximizeButton ( ) :,149
"def search(a, b, desired):
    if a == b:
        return a
    if abs(b - a) < 0.005:
        ca = count(a)
        cb = count(b)
        dista = abs(desired - ca)
        distb = abs(desired - cb)
        if dista < distb:
            return a
        else:
            return b
    m = (a + b) / 2.0
    cm = count(m)
    if desired < cm:
        return search(m, b, desired)
    else:
        return search(a, m, desired)
",if dista < distb :,161
"def force_ipv4(self, *args):
    """"""only ipv4 localhost in /etc/hosts""""""
    logg.debug(""checking /etc/hosts for '::1 localhost'"")
    lines = []
    for line in open(self.etc_hosts()):
        if ""::1"" in line:
            newline = re.sub(""\\slocalhost\\s"", "" "", line)
            if line != newline:
                logg.info(""/etc/hosts: '%s' => '%s'"", line.rstrip(), newline.rstrip())
                line = newline
        lines.append(line)
    f = open(self.etc_hosts(), ""w"")
    for line in lines:
        f.write(line)
    f.close()
","if ""::1"" in line :",182
"def aiter_cogs(cls) -> AsyncIterator[Tuple[str, str]]:
    yield ""Core"", ""0""
    for _dir in data_manager.cog_data_path().iterdir():
        fpath = _dir / ""settings.json""
        if not fpath.exists():
            continue
        with fpath.open() as f:
            try:
                data = json.load(f)
            except json.JSONDecodeError:
                continue
        if not isinstance(data, dict):
            continue
        cog_name = _dir.stem
        for cog_id, inner in data.items():
            if not isinstance(inner, dict):
                continue
            yield cog_name, cog_id
",if not fpath . exists ( ) :,192
"def _get_dbutils():
    try:
        import IPython
        ip_shell = IPython.get_ipython()
        if ip_shell is None:
            raise _NoDbutilsError
        return ip_shell.ns_table[""user_global""][""dbutils""]
    except ImportError:
        raise _NoDbutilsError
    except KeyError:
        raise _NoDbutilsError
",if ip_shell is None :,97
"def _bytecode_filenames(self, py_filenames):
    bytecode_files = []
    for py_file in py_filenames:
        # Since build_py handles package data installation, the
        # list of outputs can contain more than just .py files.
        # Make sure we only report bytecode for the .py files.
        ext = os.path.splitext(os.path.normcase(py_file))[1]
        if ext != PYTHON_SOURCE_EXTENSION:
            continue
        if self.compile:
            bytecode_files.append(py_file + ""c"")
        if self.optimize > 0:
            bytecode_files.append(py_file + ""o"")
    return bytecode_files
",if ext != PYTHON_SOURCE_EXTENSION :,175
"def compute_distances_mu(line, pts, result, gates, tolerance):
    """"""calculate all distances with mathuutils""""""
    line_origin = V(line[0])
    line_end = V(line[-1])
    local_result = [[], [], [], [], []]
    for point in pts:
        data = compute_distance(V(point), line_origin, line_end, tolerance)
        for i, res in enumerate(local_result):
            res.append(data[i])
    for i, res in enumerate(result):
        if gates[i]:
            res.append(local_result[i])
",if gates [ i ] :,153
"def _get_next_segment(self, segment_path, page_size, segment_cursor=None):
    if segment_path:
        if self.end_time and self._is_later_than_end_time(segment_path):
            return None
        return Segment(self.client, segment_path, page_size, segment_cursor)
    return None
",if self . end_time and self . _is_later_than_end_time ( segment_path ) :,91
"def _check_number_of_sessions():
    nb_desktop_sessions = sessions.get_number_of_desktop_sessions(ignore_gdm=True)
    if nb_desktop_sessions > 1:
        print(
            ""WARNING : There are %d other desktop sessions open. The GPU switch will not become effective until you have manually""
            "" logged out from ALL desktop sessions.\n""
            ""Continue ? (y/N)"" % (nb_desktop_sessions - 1)
        )
        confirmation = ask_confirmation()
        if not confirmation:
            sys.exit(0)
",if not confirmation :,149
"def delete_compute_environment(self, compute_environment_name):
    if compute_environment_name is None:
        raise InvalidParameterValueException(""Missing computeEnvironment parameter"")
    compute_env = self.get_compute_environment(compute_environment_name)
    if compute_env is not None:
        # Pop ComputeEnvironment
        self._compute_environments.pop(compute_env.arn)
        # Delete ECS cluster
        self.ecs_backend.delete_cluster(compute_env.ecs_name)
        if compute_env.env_type == ""MANAGED"":
            # Delete compute environment
            instance_ids = [instance.id for instance in compute_env.instances]
            self.ec2_backend.terminate_instances(instance_ids)
","if compute_env . env_type == ""MANAGED"" :",187
"def run(self):
    results = {}
    for func_name in [
        # Execute every function starting with check_*
        fn
        for fn in self.check_functions
        # if the user does not specify any name
        if not self.args.get(""check"")
        # of if specify the current function name
        or self.args.get(""check"") == fn
    ]:
        function = getattr(self, func_name)
        log.warn(function.__doc__)
        result = function()
        if result:
            log.info(""\n"".join(result))
            results.update({func_name: result})
    return results
",if result :,167
"def invalidate(self, layers=None):
    if layers is None:
        layers = Layer.AllLayers
    if layers:
        layers = set(layers)
        self.invalidLayers.update(layers)
        blockRenderers = [
            br
            for br in self.blockRenderers
            if br.layer is Layer.Blocks or br.layer not in layers
        ]
        if len(blockRenderers) < len(self.blockRenderers):
            self.forgetDisplayLists()
        self.blockRenderers = blockRenderers
        if self.renderer.showRedraw and Layer.Blocks in layers:
            self.needsRedisplay = True
",if br . layer is Layer . Blocks or br . layer not in layers,184
"def get_library_dirs(platform, arch=None):
    if platform == ""win32"":
        jre_home = get_jre_home(platform)
        jdk_home = JAVA_HOME
        if isinstance(jre_home, bytes):
            jre_home = jre_home.decode(""utf-8"")
        return [join(jdk_home, ""lib""), join(jdk_home, ""bin"", ""server"")]
    elif platform == ""android"":
        return [""libs/{}"".format(arch)]
    return []
","if isinstance ( jre_home , bytes ) :",136
"def save_plugin_options(self):
    for name, option_widgets in self._plugin_option_widgets.items():
        if name not in self.config[""plugins""]:
            self.config[""plugins""][name] = {}
        plugin_config = self.config[""plugins""][
            name
        ]  # use or instead of get incase the value is actually None
        for option_name, option_widget in option_widgets.items():
            plugin_config[option_name] = option_widget.option.get_widget_value(
                option_widget.widget
            )
","if name not in self . config [ ""plugins"" ] :",149
"def _select_block(str_in, start_tag, end_tag):
    """"""Select first block delimited by start_tag and end_tag""""""
    start_pos = str_in.find(start_tag)
    if start_pos < 0:
        raise ValueError(""start_tag not found"")
    depth = 0
    for pos in range(start_pos, len(str_in)):
        if str_in[pos] == start_tag:
            depth += 1
        elif str_in[pos] == end_tag:
            depth -= 1
        if depth == 0:
            break
    sel = str_in[start_pos + 1 : pos]
    return sel
",if depth == 0 :,171
"def _coerce_to_bool(self, node, var, true_val=True):
    """"""Coerce the values in a variable to bools.""""""
    bool_var = self.program.NewVariable()
    for b in var.bindings:
        v = b.data
        if isinstance(v, mixin.PythonConstant) and isinstance(v.pyval, bool):
            const = v.pyval is true_val
        elif not compare.compatible_with(v, True):
            const = not true_val
        elif not compare.compatible_with(v, False):
            const = true_val
        else:
            const = None
        bool_var.AddBinding(self.convert.bool_values[const], {b}, node)
    return bool_var
","elif not compare . compatible_with ( v , False ) :",192
"def multiline_indentation(self):
    if self._multiline_indentation is None:
        offset = 0
        if self.show_aligned_keywords:
            offset = 2
        indentation = make_indentation(3 * self.indent_size + offset)
        self._multiline_indentation = indentation
    if self.current_rule:
        indent_extra = make_indentation(self.indent_size)
        return self._multiline_indentation + indent_extra
    return self._multiline_indentation
",if self . show_aligned_keywords :,120
"def __call__(self, event, data=None):
    datatype, delta = event
    self.midi_ctrl.delta += delta
    if TIMING_CLOCK in datatype and not self.played:
        self.midi_ctrl.pulse += 1
        if self.midi_ctrl.pulse == self.midi_ctrl.ppqn:
            t_master = 60.0
            self.midi_ctrl.bpm = round(60.0 / self.midi_ctrl.delta, 0)
            self.midi_ctrl.pulse = 0
            self.midi_ctrl.delta = 0.0
",if self . midi_ctrl . pulse == self . midi_ctrl . ppqn :,155
"def handle_sent(self, elt):
    sent = []
    for child in elt:
        if child.tag in (""wf"", ""punc""):
            itm = self.handle_word(child)
            if self._unit == ""word"":
                sent.extend(itm)
            else:
                sent.append(itm)
        else:
            raise ValueError(""Unexpected element %s"" % child.tag)
    return SemcorSentence(elt.attrib[""snum""], sent)
","if child . tag in ( ""wf"" , ""punc"" ) :",126
"def _handle_def_errors(testdef):
    # If the test generation had an error, raise
    if testdef.error:
        if testdef.exception:
            if isinstance(testdef.exception, Exception):
                raise testdef.exception
            else:
                raise Exception(testdef.exception)
        else:
            raise Exception(""Test parse failure"")
",if testdef . exception :,100
"def _authorized_sid(self, jid, sid, ifrom, iq):
    with self._preauthed_sids_lock:
        if (jid, sid, ifrom) in self._preauthed_sids:
            del self._preauthed_sids[(jid, sid, ifrom)]
            return True
        return False
","if ( jid , sid , ifrom ) in self . _preauthed_sids :",88
"def wait(self, timeout=None):
    if self.returncode is None:
        if timeout is None:
            msecs = _subprocess.INFINITE
        else:
            msecs = max(0, int(timeout * 1000 + 0.5))
        res = _subprocess.WaitForSingleObject(int(self._handle), msecs)
        if res == _subprocess.WAIT_OBJECT_0:
            code = _subprocess.GetExitCodeProcess(self._handle)
            if code == TERMINATE:
                code = -signal.SIGTERM
            self.returncode = code
    return self.returncode
",if timeout is None :,154
"def _gen_legal_y_s_t(self):
    while True:
        y = self._gen_random_scalar()
        s = self.tec_arithmetic.mul(
            scalar=y, a=self.tec_arithmetic.get_generator()
        )  # S = yG
        t = self._hash_tec_element(s)
        if self.tec_arithmetic.is_in_group(s) and type(t) != int:
            # Both S and T are legal
            LOGGER.info(""randomly generated y, S, T"")
            return y, s, t
",if self . tec_arithmetic . is_in_group ( s ) and type ( t ) != int :,160
"def write_out():
    while True:
        if self.instrument_queue.empty():
            time.sleep(0.1)
            continue
        data_str = self.instrument_queue.get()
        data_str = data_str.splitlines()
        tb.write("""")  # position cursor to end
        for line in data_str:
            tb.write(line)
        tb.write(""\n"")
",if self . instrument_queue . empty ( ) :,112
"def _parse_preamble(self):
    """"""Parse metadata about query (PRIVATE).""""""
    meta = {}
    while self.line:
        regx = re.search(_RE_QUERY, self.line)
        if regx:
            self.query_id = regx.group(1)
        if self.line.startswith(""Match_columns""):
            self.seq_len = int(self.line.strip().split()[1])
        self.line = self.handle.readline().strip()
    return meta
","if self . line . startswith ( ""Match_columns"" ) :",130
"def init_sequence(self, coll_name, seq_config):
    if not isinstance(seq_config, list):
        raise Exception('""sequence"" config must be a list')
    handlers = []
    for entry in seq_config:
        if not isinstance(entry, dict):
            raise Exception('""sequence"" entry must be a dict')
        name = entry.get(""name"", """")
        handler = self.load_coll(name, entry)
        handlers.append(handler)
    return HandlerSeq(handlers)
","if not isinstance ( entry , dict ) :",126
"def change_args_to_dict(string):
    if string is None:
        return None
    ans = []
    strings = string.split(""\n"")
    ind = 1
    start = 0
    while ind <= len(strings):
        if ind < len(strings) and strings[ind].startswith("" ""):
            ind += 1
        else:
            if start < ind:
                ans.append(""\n"".join(strings[start:ind]))
            start = ind
            ind += 1
    d = {}
    for line in ans:
        if "":"" in line and len(line) > 0:
            lines = line.split("":"")
            d[lines[0]] = lines[1].strip()
    return d
",if start < ind :,188
"def wait(self):
    while True:
        return_code = self._process.poll()
        if return_code is not None:
            line = self._process.stdout.readline().decode(""utf-8"")
            if line == """":
                break
            log.debug(line.strip(""\n""))
    return True
","if line == """" :",87
"def __getattr__(self, key):
    for tag in self.tag.children:
        if tag.name not in (""input"",):
            continue
        if ""name"" in tag.attrs and tag.attrs[""name""] in (key,):
            from thug.DOM.W3C.Core.DOMImplementation import DOMImplementation
            return DOMImplementation.createHTMLElement(self.doc, tag)
    raise AttributeError
","if tag . name not in ( ""input"" , ) :",104
"def compare_hash(hash_of_gold, path_to_file):
    with open(path_to_file, ""rb"") as f:
        hash_of_file = hashlib.sha256(f.read()).hexdigest()
        if hash_of_file != hash_of_gold:
            print(
                ""########## Hash sum of"",
                path_to_file,
                ""differs from the target, the topology will be deleted !!! ##########"",
            )
            shutil.rmtree(os.path.dirname(path_to_file))
",if hash_of_file != hash_of_gold :,147
"def on_completed2():
    doner[0] = True
    if not qr:
        if len(ql) > 0:
            observer.on_next(False)
            observer.on_completed()
        elif donel[0]:
            observer.on_next(True)
            observer.on_completed()
",if len ( ql ) > 0 :,86
"def get_other(self, data, items):
    is_tuple = False
    if type(data) == tuple:
        data = list(data)
        is_tuple = True
    if type(data) == list:
        m_items = items.copy()
        for idx, item in enumerate(items):
            if item < 0:
                m_items[idx] = len(data) - abs(item)
        for i in sorted(set(m_items), reverse=True):
            if i < len(data) and i > -1:
                del data[i]
        if is_tuple:
            return tuple(data)
        else:
            return data
    else:
        return None
",if item < 0 :,191
"def _open_url(cls, url):
    if config.browser:
        cmd = [config.browser, url]
        if not config.quiet:
            print(""running command: %s"" % "" "".join(cmd))
        p = Popen(cmd)
        p.communicate()
    else:
        if not config.quiet:
            print(""opening URL in browser: %s"" % url)
        webbrowser.open_new(url)
",if not config . quiet :,117
"def setLabel(self, s, protect=False):
    """"""Set the label of the minibuffer.""""""
    c, k, w = self.c, self, self.w
    if w:
        # Support for the curses gui.
        if hasattr(g.app.gui, ""set_minibuffer_label""):
            g.app.gui.set_minibuffer_label(c, s)
        w.setAllText(s)
        n = len(s)
        w.setSelectionRange(n, n, insert=n)
        if protect:
            k.mb_prefix = s
",if protect :,151
"def __init__(self, path):
    self.symcaches = []
    for path in path.split("";""):
        if os.path.isdir(path):
            self.symcaches.append(SymbolCache(dirname=path))
            continue
        if path.startswith(""cobra://"") or path.startswith(""cobrassl://""):
            import cobra
            self.symcaches.append(cobra.CobraProxy(path))
            continue
","if path . startswith ( ""cobra://"" ) or path . startswith ( ""cobrassl://"" ) :",111
"def init_params(net):
    """"""Init layer parameters.""""""
    for module in net.modules():
        if isinstance(module, nn.Conv2d):
            init.kaiming_normal(module.weight, mode=""fan_out"")
            if module.bias:
                init.constant(module.bias, 0)
        elif isinstance(module, nn.BatchNorm2d):
            init.constant(module.weight, 1)
            init.constant(module.bias, 0)
        elif isinstance(module, nn.Linear):
            init.normal(module.weight, std=1e-3)
            if module.bias:
                init.constant(module.bias, 0)
","elif isinstance ( module , nn . BatchNorm2d ) :",180
"def _diff_dict(self, old, new):
    diff = {}
    removed = []
    added = []
    for key, value in old.items():
        if key not in new:
            removed.append(key)
        elif old[key] != new[key]:
            # modified is indicated by a remove and add
            removed.append(key)
            added.append(key)
    for key, value in new.items():
        if key not in old:
            added.append(key)
    if removed:
        diff[""removed""] = sorted(removed)
    if added:
        diff[""added""] = sorted(added)
    return diff
",if key not in new :,172
"def __init__(self, *args, **kwargs):
    _kwargs = {
        ""max_length"": 20,
        ""widget"": forms.TextInput(attrs={""autocomplete"": ""off""}),
        ""label"": _(""Card number""),
    }
    if ""types"" in kwargs:
        self.accepted_cards = set(kwargs.pop(""types""))
        difference = self.accepted_cards - VALID_CARDS
        if difference:
            raise ImproperlyConfigured(
                ""The following accepted_cards are "" ""unknown: %s"" % difference
            )
    _kwargs.update(kwargs)
    super().__init__(*args, **_kwargs)
",if difference :,160
"def dumps(self):
    sections = []
    for name, env_info in self._dependencies_.items():
        sections.append(""[ENV_%s]"" % name)
        for var, values in sorted(env_info.vars.items()):
            tmp = ""%s="" % var
            if isinstance(values, list):
                tmp += ""[%s]"" % "","".join(['""%s""' % val for val in values])
            else:
                tmp += ""%s"" % values
            sections.append(tmp)
    return ""\n"".join(sections)
","if isinstance ( values , list ) :",144
"def air_quality(self):
    aqi_data = self._get_aqi_data()
    if aqi_data:
        if aqi_data.get(""status"") == ""ok"":
            aqi_data = self._organize(aqi_data)
            aqi_data = self._manipulate(aqi_data)
        elif aqi_data.get(""status"") == ""error"":
            self.py3.error(aqi_data.get(""data""))
    return {
        ""cached_until"": self.py3.time_in(self.cache_timeout),
        ""full_text"": self.py3.safe_format(self.format, aqi_data),
    }
","elif aqi_data . get ( ""status"" ) == ""error"" :",190
"def _blend(x, y):  # pylint: disable=invalid-name
    """"""Implements the ""blend"" strategy for `deep_merge`.""""""
    if isinstance(x, (dict, OrderedDict)):
        if not isinstance(y, (dict, OrderedDict)):
            return y
        return _merge(x, y, recursion_func=_blend)
    if isinstance(x, (list, tuple)):
        if not isinstance(y, (list, tuple)):
            return y
        result = [_blend(*i) for i in zip(x, y)]
        if len(x) > len(y):
            result += x[len(y) :]
        elif len(x) < len(y):
            result += y[len(x) :]
        return result
    return y
","if not isinstance ( y , ( list , tuple ) ) :",194
"def _rate(cls, sample1, sample2):
    ""Simple rate""
    try:
        interval = sample2[0] - sample1[0]
        if interval == 0:
            raise Infinity()
        delta = sample2[1] - sample1[1]
        if delta < 0:
            raise UnknownValue()
        return (sample2[0], delta / interval, sample2[2], sample2[3])
    except Infinity:
        raise
    except UnknownValue:
        raise
    except Exception as e:
        raise NaN(e)
",if interval == 0 :,146
"def wrapped_request_method(*args, **kwargs):
    """"""Modifies HTTP headers to include a specified user-agent.""""""
    if kwargs.get(""headers"") is not None:
        if kwargs[""headers""].get(""user-agent""):
            if user_agent not in kwargs[""headers""][""user-agent""]:
                # Save the existing user-agent header and tack on our own.
                kwargs[""headers""][""user-agent""] = (
                    f""{user_agent} "" f'{kwargs[""headers""][""user-agent""]}'
                )
        else:
            kwargs[""headers""][""user-agent""] = user_agent
    else:
        kwargs[""headers""] = {""user-agent"": user_agent}
    return request_method(*args, **kwargs)
","if kwargs [ ""headers"" ] . get ( ""user-agent"" ) :",191
"def remove_addons(auth, resource_object_list):
    for config in AbstractNode.ADDONS_AVAILABLE:
        try:
            settings_model = config.node_settings
        except LookupError:
            settings_model = None
        if settings_model:
            addon_list = settings_model.objects.filter(
                owner__in=resource_object_list, is_deleted=False
            )
            for addon in addon_list:
                addon.after_delete(auth.user)
",if settings_model :,138
"def Decorator(*args, **kwargs):
    delay = 0.2
    num_attempts = 15
    cur_attempt = 0
    while True:
        try:
            return f(*args, **kwargs)
        except exceptions.WebDriverException as e:
            logging.warning(""Selenium raised %s"", utils.SmartUnicode(e))
            cur_attempt += 1
            if cur_attempt == num_attempts:
                raise
            time.sleep(delay)
",if cur_attempt == num_attempts :,122
"def _cleanup_parts_dir(parts_dir, local_plugins_dir, parts):
    if os.path.exists(parts_dir):
        logger.info(""Cleaning up parts directory"")
        for subdirectory in os.listdir(parts_dir):
            path = os.path.join(parts_dir, subdirectory)
            if path != local_plugins_dir:
                try:
                    shutil.rmtree(path)
                except NotADirectoryError:
                    os.remove(path)
    for part in parts:
        part.mark_cleaned(steps.BUILD)
        part.mark_cleaned(steps.PULL)
",if path != local_plugins_dir :,165
"def traverse_trees(node_pos, sample, trees: List[HeteroDecisionTreeGuest]):
    if node_pos[""reach_leaf_node""].all():
        return node_pos
    for t_idx, tree in enumerate(trees):
        cur_node_idx = node_pos[""node_pos""][t_idx]
        # reach leaf
        if cur_node_idx == -1:
            continue
        rs, reach_leaf = HeteroSecureBoostingTreeGuest.traverse_a_tree(
            tree, sample, cur_node_idx
        )
        if reach_leaf:
            node_pos[""reach_leaf_node""][t_idx] = True
        node_pos[""node_pos""][t_idx] = rs
    return node_pos
",if cur_node_idx == - 1 :,196
"def get_measurements(self, pipeline, object_name, category):
    if self.get_categories(pipeline, object_name) == [category]:
        results = []
        if self.do_corr_and_slope:
            if object_name == ""Image"":
                results += [""Correlation"", ""Slope""]
            else:
                results += [""Correlation""]
        if self.do_overlap:
            results += [""Overlap"", ""K""]
        if self.do_manders:
            results += [""Manders""]
        if self.do_rwc:
            results += [""RWC""]
        if self.do_costes:
            results += [""Costes""]
        return results
    return []
",if self . do_manders :,195
"def create_connection(self, infos, f2, laddr_infos, protocol):
    for family in infos:
        try:
            if f2:
                for laddr in laddr_infos:
                    try:
                        break
                    except OSError:
                        protocol = ""foo""
                else:
                    continue
        except OSError:
            protocol = ""bar""
        else:
            break
    else:
        raise
    return protocol
",if f2 :,139
"def app_middleware(next, root, info, **kwargs):
    app_auth_header = ""HTTP_AUTHORIZATION""
    prefix = ""bearer""
    request = info.context
    if request.path == API_PATH:
        if not hasattr(request, ""app""):
            request.app = None
            auth = request.META.get(app_auth_header, """").split()
            if len(auth) == 2:
                auth_prefix, auth_token = auth
                if auth_prefix.lower() == prefix:
                    request.app = SimpleLazyObject(lambda: get_app(auth_token))
    return next(root, info, **kwargs)
",if auth_prefix . lower ( ) == prefix :,171
"def when(self, matches, context):
    ret = []
    for episode in matches.named(""episode"", lambda match: len(match.initiator) == 1):
        group = matches.markers.at_match(
            episode, lambda marker: marker.name == ""group"", index=0
        )
        if group:
            if not matches.range(
                *group.span, predicate=lambda match: match.name == ""title""
            ):
                ret.append(episode)
    return ret
",if group :,132
"def locate_via_pep514(spec):
    with _PY_LOCK:
        if not _PY_AVAILABLE:
            from . import pep514
            _PY_AVAILABLE.extend(pep514.discover_pythons())
            _PY_AVAILABLE.append(CURRENT)
    for cur_spec in _PY_AVAILABLE:
        if cur_spec.satisfies(spec):
            return cur_spec.path
",if cur_spec . satisfies ( spec ) :,110
"def setCorkImageDefault(self):
    if settings.corkBackground[""image""] != """":
        i = self.cmbCorkImage.findData(settings.corkBackground[""image""])
        if i != -1:
            self.cmbCorkImage.setCurrentIndex(i)
",if i != - 1 :,72
"def _split_key(key):
    if isinstance(key, util.string_types):
        # coerce fooload('*') into ""default loader strategy""
        if key == _WILDCARD_TOKEN:
            return (_DEFAULT_TOKEN,)
        # coerce fooload("".*"") into ""wildcard on default entity""
        elif key.startswith(""."" + _WILDCARD_TOKEN):
            key = key[1:]
        return key.split(""."")
    else:
        return (key,)
","elif key . startswith ( ""."" + _WILDCARD_TOKEN ) :",122
"def detach_volume(self, volume):
    # We need to find the node using this volume
    for node in self.list_nodes():
        if type(node.image) is not list:
            # This node has only one associated image. It is not the one we
            # are after.
            continue
        for disk in node.image:
            if disk.id == volume.id:
                # Node found. We can now detach the volume
                disk_id = disk.extra[""disk_id""]
                return self._do_detach_volume(node.id, disk_id)
    return False
",if type ( node . image ) is not list :,160
"def create(self, private=False):
    try:
        if private:
            log.info(""Creating private channel %s."", self)
            self._bot.api_call(
                ""conversations.create"", data={""name"": self.name, ""is_private"": True}
            )
        else:
            log.info(""Creating channel %s."", self)
            self._bot.api_call(""conversations.create"", data={""name"": self.name})
    except SlackAPIResponseError as e:
        if e.error == ""user_is_bot"":
            raise RoomError(f""Unable to create channel. {USER_IS_BOT_HELPTEXT}"")
        else:
            raise RoomError(e)
",if private :,189
"def test_dataset_has_valid_etag(self, dataset_name):
    py_script_path = list(filter(lambda x: x, dataset_name.split(""/"")))[-1] + "".py""
    dataset_url = hf_bucket_url(dataset_name, filename=py_script_path, dataset=True)
    etag = None
    try:
        response = requests.head(
            dataset_url, allow_redirects=True, proxies=None, timeout=10
        )
        if response.status_code == 200:
            etag = response.headers.get(""Etag"")
    except (EnvironmentError, requests.exceptions.Timeout):
        pass
    self.assertIsNotNone(etag)
",if response . status_code == 200 :,173
"def set_dir_modes(self, dirname, mode):
    if not self.is_chmod_supported():
        return
    for dirpath, dirnames, fnames in os.walk(dirname):
        if os.path.islink(dirpath):
            continue
        log.info(""changing mode of %s to %o"", dirpath, mode)
        if not self.dry_run:
            os.chmod(dirpath, mode)
",if not self . dry_run :,105
"def _clean(self):
    logger.info(""Cleaning up..."")
    if self._process is not None:
        if self._process.poll() is None:
            for _ in range(3):
                self._process.terminate()
                time.sleep(0.5)
                if self._process.poll() is not None:
                    break
            else:
                self._process.kill()
                self._process.wait()
                logger.error(""KILLED"")
    if os.path.exists(self._tmp_dir):
        shutil.rmtree(self._tmp_dir)
    self._process = None
    self._ws = None
    logger.info(""Cleanup complete"")
",if self . _process . poll ( ) is None :,189
"def iter_chars_to_words(self, chars):
    current_word = []
    for char in chars:
        if not self.keep_blank_chars and char[""text""].isspace():
            if current_word:
                yield current_word
                current_word = []
        elif current_word and self.char_begins_new_word(current_word, char):
            yield current_word
            current_word = [char]
        else:
            current_word.append(char)
    if current_word:
        yield current_word
",if current_word :,150
"def _lookup(components, specs, provided, name, i, l):
    if i < l:
        for spec in specs[i].__sro__:
            comps = components.get(spec)
            if comps:
                r = _lookup(comps, specs, provided, name, i + 1, l)
                if r is not None:
                    return r
    else:
        for iface in provided:
            comps = components.get(iface)
            if comps:
                r = comps.get(name)
                if r is not None:
                    return r
    return None
",if r is not None :,166
"def run(cmd, task=None):
    process = subprocess.Popen(
        cmd, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, close_fds=True
    )
    output_lines = []
    while True:
        line = process.stdout.readline()
        if not line:
            break
        line = line.decode(""utf-8"")
        output_lines += [line]
        logger.info(line.rstrip(""\n""))
    process.stdout.close()
    exit_code = process.wait()
    if exit_code:
        output = """".join(output_lines)
        raise subprocess.CalledProcessError(exit_code, cmd, output=output)
",if not line :,169
"def process_response(self, request, response):
    if (
        response.status_code == 404
        and request.path_info.endswith(""/"")
        and not is_valid_path(request.path_info)
        and is_valid_path(request.path_info[:-1])
    ):
        # Use request.path because we munged app/locale in path_info.
        newurl = request.path[:-1]
        if request.GET:
            with safe_query_string(request):
                newurl += ""?"" + request.META[""QUERY_STRING""]
        return HttpResponsePermanentRedirect(newurl)
    return response
",if request . GET :,163
"def dependencies(self):
    deps = []
    midx = None
    if self.ref is not None:
        query = TypeQuery(self.ref)
        super = query.execute(self.schema)
        if super is None:
            log.debug(self.schema)
            raise TypeNotFound(self.ref)
        if not super.builtin():
            deps.append(super)
            midx = 0
    return (midx, deps)
",if not super . builtin ( ) :,120
"def _get_vtkjs(self):
    if self._vtkjs is None and self.object is not None:
        if isinstance(self.object, string_types) and self.object.endswith("".vtkjs""):
            if isfile(self.object):
                with open(self.object, ""rb"") as f:
                    vtkjs = f.read()
            else:
                data_url = urlopen(self.object)
                vtkjs = data_url.read()
        elif hasattr(self.object, ""read""):
            vtkjs = self.object.read()
        self._vtkjs = vtkjs
    return self._vtkjs
",if isfile ( self . object ) :,180
"def _save(self):
    fd, tempname = tempfile.mkstemp()
    fd = os.fdopen(fd, ""w"")
    json.dump(self._cache, fd, indent=2, separators=("","", "": ""))
    fd.close()
    # Silently ignore errors
    try:
        if not os.path.exists(os.path.dirname(self.filename)):
            os.makedirs(os.path.dirname(self.filename))
        shutil.move(tempname, self.filename)
    except (IOError, OSError):
        os.remove(tempname)
",if not os . path . exists ( os . path . dirname ( self . filename ) ) :,139
"def refiner_configs(self):
    rv = {}
    for refiner in refiner_manager:
        if self.config.has_section(refiner.name):
            rv[refiner.name] = {k: v for k, v in self.config.items(refiner.name)}
    return rv
",if self . config . has_section ( refiner . name ) :,78
"def com_slice(self, primary, node, assigning):
    # short_slice:  [lower_bound] "":"" [upper_bound]
    lower = upper = None
    if len(node.children) == 2:
        if node.children[0].type == token.COLON:
            upper = self.com_node(node.children[1])
        else:
            lower = self.com_node(node.children[0])
    elif len(node.children) == 3:
        lower = self.com_node(node.children[0])
        upper = self.com_node(node.children[2])
    return Slice(primary, assigning, lower, upper, lineno=extractLineNo(node))
",if node . children [ 0 ] . type == token . COLON :,177
"def close(self, *args, **kwargs):
    super(mytqdm, self).close(*args, **kwargs)
    # If it was not run in a notebook, sp is not assigned, check for it
    if hasattr(self, ""sp""):
        # Try to detect if there was an error or KeyboardInterrupt
        # in manual mode: if n < total, things probably got wrong
        if self.total and self.n < self.total:
            self.sp(bar_style=""danger"")
        else:
            if self.leave:
                self.sp(bar_style=""success"")
            else:
                self.sp(close=True)
",if self . leave :,167
"def test_alloc(self):
    b = bytearray()
    alloc = b.__alloc__()
    self.assertTrue(alloc >= 0)
    seq = [alloc]
    for i in range(100):
        b += b""x""
        alloc = b.__alloc__()
        self.assertTrue(alloc >= len(b))
        if alloc not in seq:
            seq.append(alloc)
",if alloc not in seq :,98
"def flush_file(self, key, f):
    f.flush()
    if self.compress:
        f.compress = zlib.compressobj(
            9, zlib.DEFLATED, -zlib.MAX_WBITS, zlib.DEF_MEM_LEVEL, 0
        )
    if len(self.files) > self.MAX_OPEN_FILES:
        if self.compress:
            open_files = sum(1 for f in self.files.values() if f.fileobj is not None)
            if open_files > self.MAX_OPEN_FILES:
                f.fileobj.close()
                f.fileobj = None
        else:
            f.close()
            self.files.pop(key)
",if self . compress :,183
"def _run(self):
    # Low-level run method to do the actual scheduling loop.
    self.running = True
    while self.running:
        try:
            self.sched.run()
        except Exception as x:
            logging.error(
                ""Error during scheduler execution: %s"" % str(x), exc_info=True
            )
        # queue is empty; sleep a short while before checking again
        if self.running:
            time.sleep(5)
",if self . running :,132
"def TryMerge(self, d):
    while d.avail() > 0:
        tt = d.getVarInt32()
        if tt == 10:
            self.set_app_id(d.getPrefixedString())
            continue
        if tt == 16:
            self.set_max_rows(d.getVarInt32())
            continue
        if tt == 0:
            raise ProtocolBuffer.ProtocolBufferDecodeError
        d.skipData(tt)
",if tt == 16 :,124
"def check(dbdef):
    ""drop script must clear the database""
    for version in dbdef:
        connector = MemConnector().bound(None)
        create(dbdef, version, connector)
        drop(dbdef, version, connector)
        remaining = connector.execute(
            ""SELECT * FROM sqlite_master WHERE name NOT LIKE 'sqlite_%'""
        ).fetchall()
        if remaining:
            yield ""{0}:drop.sql"".format(version), remaining
",if remaining :,120
"def test_open_overwrite_offset_size(self, sftp):
    """"""Test writing data at a specific offset""""""
    f = None
    try:
        self._create_file(""file"", ""xxxxyyyy"")
        f = yield from sftp.open(""file"", ""r+"")
        yield from f.write(""zz"", 3)
        yield from f.close()
        with open(""file"") as localf:
            self.assertEqual(localf.read(), ""xxxzzyyy"")
    finally:
        if f:  # pragma: no branch
            yield from f.close()
        remove(""file"")
",if f :,155
"def pump():
    import sys as _sys
    while self.countdown_active():
        if not (self.connected(""send"") and other.connected(""recv"")):
            break
        try:
            data = other.recv(timeout=0.05)
        except EOFError:
            break
        if not _sys:
            return
        if not data:
            continue
        try:
            self.send(data)
        except EOFError:
            break
        if not _sys:
            return
    self.shutdown(""send"")
    other.shutdown(""recv"")
",if not _sys :,158
"def parse_results(cwd):
    optimal_dd = None
    optimal_measure = numpy.inf
    for tup in tools.find_conf_files(cwd):
        dd = tup[1]
        if ""results.train_y_misclass"" in dd:
            if dd[""results.train_y_misclass""] < optimal_measure:
                optimal_measure = dd[""results.train_y_misclass""]
                optimal_dd = dd
    print(""Optimal results.train_y_misclass:"", str(optimal_measure))
    for key, value in optimal_dd.items():
        if ""hyper_parameters"" in key:
            print(key + "": "" + str(value))
","if dd [ ""results.train_y_misclass"" ] < optimal_measure :",177
"def valid(self):
    valid = True
    if os.path.exists(self.pathfile):
        return valid
    else:
        try:
            with io.open(self.pathfile, ""w"", encoding=""utf-8"") as f:
                f.close()  # do nothing
        except OSError:
            valid = False
        if os.path.exists(self.pathfile):
            os.remove(self.pathfile)
        return valid
",if os . path . exists ( self . pathfile ) :,124
"def __getitem__(self, key):
    try:
        value = self.cache[key]
    except KeyError:
        f = BytesIO(self.dict[key.encode(self.keyencoding)])
        value = Unpickler(f).load()
        if self.writeback:
            self.cache[key] = value
    return value
",if self . writeback :,87
"def hasMenu(cls, callingWindow, mainItem, selection, *fullContexts):
    for i, fullContext in enumerate(fullContexts):
        srcContext = fullContext[0]
        for menuHandler in cls.menus:
            m = menuHandler()
            if m._baseDisplay(callingWindow, srcContext, mainItem, selection):
                return True
        return False
","if m . _baseDisplay ( callingWindow , srcContext , mainItem , selection ) :",98
"def lr_read_tables(module=tab_module, optimize=0):
    global _lr_action, _lr_goto, _lr_productions, _lr_method
    try:
        exec(""import %s as parsetab"" % module)
        global parsetab  # declare the name of the imported module
        if (optimize) or (Signature.digest() == parsetab._lr_signature):
            _lr_action = parsetab._lr_action
            _lr_goto = parsetab._lr_goto
            _lr_productions = parsetab._lr_productions
            _lr_method = parsetab._lr_method
            return 1
        else:
            return 0
    except (ImportError, AttributeError):
        return 0
",if ( optimize ) or ( Signature . digest ( ) == parsetab . _lr_signature ) :,192
"def _Determine_Do(self):
    if sys.platform.startswith(""win""):
        self.applicable = 1
        for opt, optarg in self.chosenOptions:
            if opt == ""--moz-tools"":
                self.value = os.path.abspath(os.path.normpath(optarg))
                break
        else:
            if os.environ.has_key(self.name):
                self.value = os.environ[self.name]
            else:
                self.value = None
    else:
        self.applicable = 0
    self.determined = 1
",if os . environ . has_key ( self . name ) :,157
"def parse_chunked(self, unreader):
    (size, rest) = self.parse_chunk_size(unreader)
    while size > 0:
        while size > len(rest):
            size -= len(rest)
            yield rest
            rest = unreader.read()
            if not rest:
                raise NoMoreData()
        yield rest[:size]
        # Remove \r\n after chunk
        rest = rest[size:]
        while len(rest) < 2:
            rest += unreader.read()
        if rest[:2] != b""\r\n"":
            raise ChunkMissingTerminator(rest[:2])
        (size, rest) = self.parse_chunk_size(unreader, data=rest[2:])
","if rest [ : 2 ] != b""\r\n"" :",197
"def _scroll_down(self, cli):
    ""Scroll window down.""
    info = self.render_info
    if self.vertical_scroll < info.content_height - info.window_height:
        if info.cursor_position.y <= info.configured_scroll_offsets.top:
            self.content.move_cursor_down(cli)
        self.vertical_scroll += 1
",if info . cursor_position . y <= info . configured_scroll_offsets . top :,96
"def _add_defaults_data_files(self):
    # getting distribution.data_files
    if self.distribution.has_data_files():
        for item in self.distribution.data_files:
            if isinstance(item, str):
                # plain file
                item = convert_path(item)
                if os.path.isfile(item):
                    self.filelist.append(item)
            else:
                # a (dirname, filenames) tuple
                dirname, filenames = item
                for f in filenames:
                    f = convert_path(f)
                    if os.path.isfile(f):
                        self.filelist.append(f)
",if os . path . isfile ( f ) :,192
"def list_stuff(self, upto=10, start_after=-1):
    for i in range(upto):
        if i <= start_after:
            continue
        if i == 2 and self.count < 1:
            self.count += 1
            raise TemporaryProblem
        if i == 7 and self.count < 4:
            self.count += 1
            raise TemporaryProblem
        yield i
",if i <= start_after :,110
"def is_open(self):
    if self.signup_code:
        return True
    else:
        if self.signup_code_present:
            if self.messages.get(""invalid_signup_code""):
                messages.add_message(
                    self.request,
                    self.messages[""invalid_signup_code""][""level""],
                    self.messages[""invalid_signup_code""][""text""].format(
                        **{
                            ""code"": self.get_code(),
                        }
                    ),
                )
    return settings.ACCOUNT_OPEN_SIGNUP
",if self . signup_code_present :,172
"def on_delete_from_disk(self, widget, data=None):
    model, iter = self.get_selection().get_selected()
    if iter:
        path = model.get_value(iter, COLUMN_PATH)
        if self.is_defaultitem(path):
            ErrorDialog(_(""Can't delete system item from disk."")).launch()
        else:
            os.remove(path)
    self.update_items()
",if self . is_defaultitem ( path ) :,110
"def get_detections_for_batch(self, images):
    images = images[..., ::-1]
    detected_faces = self.face_detector.detect_from_batch(images.copy())
    results = []
    for i, d in enumerate(detected_faces):
        if len(d) == 0:
            results.append(None)
            continue
        d = d[0]
        d = np.clip(d, 0, None)
        x1, y1, x2, y2 = map(int, d[:-1])
        results.append((x1, y1, x2, y2))
    return results
",if len ( d ) == 0 :,159
"def on_update(self):
    #
    # Calculate maximum # of planes per well
    #
    self.max_per_well = 0
    for pd in list(self.plate_well_site.values()):
        for wd in list(pd.values()):
            nplanes = sum([len(x) for x in list(wd.values())])
            if nplanes > self.max_per_well:
                self.max_per_well = nplanes
    for registrant in self.registrants:
        registrant()
",if nplanes > self . max_per_well :,137
"def is_writable(self, path):
    result = False
    while not result:
        if os.path.exists(path):
            result = os.access(path, os.W_OK)
            break
        parent = os.path.dirname(path)
        if parent == path:
            break
        path = parent
    return result
",if parent == path :,92
"def _check_seed(self, seed):
    if seed is not None:
        if type(seed) != int:
            self._raise_error(
                ""The random number generator seed value, seed, should be integer type or None.""
            )
        if seed < 0:
            self._raise_error(
                ""The random number generator seed value, seed, should be non-negative integer or None.""
            )
",if type ( seed ) != int :,114
"def write(self, x):
    # try to use backslash and surrogate escape strategies before failing
    self._errors = ""backslashescape"" if self.encoding != ""mbcs"" else ""surrogateescape""
    try:
        return io.TextIOWrapper.write(self, to_text(x, errors=self._errors))
    except UnicodeDecodeError:
        if self._errors != ""surrogateescape"":
            self._errors = ""surrogateescape""
        else:
            self._errors = ""replace""
        return io.TextIOWrapper.write(self, to_text(x, errors=self._errors))
","if self . _errors != ""surrogateescape"" :",141
"def post(self, request, *args, **kwargs):
    validated_session = []
    for session_id in request.data:
        session = get_object_or_none(Session, id=session_id)
        if session and not session.is_finished:
            validated_session.append(session_id)
            self.model.objects.create(
                name=""kill_session"",
                args=session.id,
                terminal=session.terminal,
            )
    return Response({""ok"": validated_session})
",if session and not session . is_finished :,141
"def _has_list_or_dict_var_value_before(self, arg_index):
    for idx, value in enumerate(self.args):
        if idx > arg_index:
            return False
        if variablematcher.is_list_variable(
            value
        ) and not variablematcher.is_list_variable_subitem(value):
            return True
        if robotapi.is_dict_var(value) and not variablematcher.is_dict_var_access(
            value
        ):
            return True
    return False
",if idx > arg_index :,142
"def test_return_correct_type(self):
    for proto in protocols:
        # Protocol 0 supports only ASCII strings.
        if proto == 0:
            self._check_return_correct_type(""abc"", 0)
        else:
            for obj in [b""abc\n"", ""abc\n"", -1, -1.1 * 0.1, str]:
                self._check_return_correct_type(obj, proto)
",if proto == 0 :,113
"def backward_impl(self, inputs, outputs, prop_down, accum):
    # inputs: [inputs_fwd_graph] + [inputs_bwd_graph] or
    # [inputs_fwd_graph] + [outputs_fwd_graph] + [inputs_bwd_graph]
    # Args
    axis = self.forward_func.info.args[""axis""]
    # Compute
    ## w.r.t. dy
    if prop_down[-1]:
        g_dy = inputs[-1].grad
        g_dy_ = F.stack(*[o.grad for o in outputs], axis=axis)
        if accum[-1]:
            g_dy += g_dy_
        else:
            g_dy.copy_from(g_dy_)
",if accum [ - 1 ] :,190
"def remove(self, url):
    try:
        i = self.items.index(url)
    except (ValueError, IndexError):
        pass
    else:
        was_selected = i in self.selectedindices()
        self.list.delete(i)
        del self.items[i]
        if not self.items:
            self.mp.hidepanel(self.name)
        elif was_selected:
            if i >= len(self.items):
                i = len(self.items) - 1
            self.list.select_set(i)
",if i >= len ( self . items ) :,150
"def prepend(self, value):
    """"""prepend value to nodes""""""
    root, root_text = self._get_root(value)
    for i, tag in enumerate(self):
        if not tag.text:
            tag.text = """"
        if len(root) > 0:
            root[-1].tail = tag.text
            tag.text = root_text
        else:
            tag.text = root_text + tag.text
        if i > 0:
            root = deepcopy(list(root))
        tag[:0] = root
        root = tag[: len(root)]
    return self
",if i > 0 :,160
"def _get_tracks_compositors_list():
    tracks_list = []
    tracks = current_sequence().tracks
    compositors = current_sequence().compositors
    for track_index in range(1, len(tracks) - 1):
        track_compositors = []
        for j in range(0, len(compositors)):
            comp = compositors[j]
            if comp.transition.b_track == track_index:
                track_compositors.append(comp)
        tracks_list.append(track_compositors)
    return tracks_list
",if comp . transition . b_track == track_index :,143
"def __getattr__(self, name):
    if name in self._sections:
        return ""\n"".join(self._sections[name])
    else:
        if self._allowed_fields and name in self._allowed_fields:
            return """"
        else:
            raise ConanException(""ConfigParser: Unrecognized field '%s'"" % name)
",if self . _allowed_fields and name in self . _allowed_fields :,86
"def get_first_param_index(self, group_id, param_group, partition_id):
    for index, param in enumerate(param_group):
        param_id = self.get_param_id(param)
        if partition_id in self.param_to_partition_ids[group_id][param_id]:
            return index
    return None
",if partition_id in self . param_to_partition_ids [ group_id ] [ param_id ] :,90
"def handle_uv_sockets(self, context):
    u_socket = self.inputs[""U""]
    v_socket = self.inputs[""V""]
    if self.cast_mode == ""Sphere"":
        u_socket.hide_safe = True
        v_socket.hide_safe = True
    elif self.cast_mode in [""Cylinder"", ""Prism""]:
        v_socket.hide_safe = True
        if u_socket.hide_safe:
            u_socket.hide_safe = False
    else:
        if u_socket.hide_safe:
            u_socket.hide_safe = False
        if v_socket.hide_safe:
            v_socket.hide_safe = False
",if u_socket . hide_safe :,184
"def _scrub_generated_timestamps(self, target_workdir):
    """"""Remove the first line of comment from each file if it contains a timestamp.""""""
    for root, _, filenames in safe_walk(target_workdir):
        for filename in filenames:
            source = os.path.join(root, filename)
            with open(source, ""r"") as f:
                lines = f.readlines()
            if len(lines) < 1:
                return
            with open(source, ""w"") as f:
                if not self._COMMENT_WITH_TIMESTAMP_RE.match(lines[0]):
                    f.write(lines[0])
                for line in lines[1:]:
                    f.write(line)
",if len ( lines ) < 1 :,196
"def inner(request, *args, **kwargs):
    page = request.current_page
    if page:
        if page.login_required and not request.user.is_authenticated:
            return redirect_to_login(
                urlquote(request.get_full_path()), settings.LOGIN_URL
            )
        site = get_current_site()
        if not user_can_view_page(request.user, page, site):
            return _handle_no_page(request)
    return func(request, *args, **kwargs)
","if not user_can_view_page ( request . user , page , site ) :",141
"def flush(self, *args, **kwargs):
    with self._lock:
        self._last_updated = time.time()
        try:
            if kwargs.get(""in_place"", False):
                self._locked_flush_without_tempfile()
            else:
                mailbox.mbox.flush(self, *args, **kwargs)
        except OSError:
            if ""_create_temporary"" in traceback.format_exc():
                self._locked_flush_without_tempfile()
            else:
                raise
        self._last_updated = time.time()
","if kwargs . get ( ""in_place"" , False ) :",157
"def sanitize_event_keys(kwargs, valid_keys):
    # Sanity check: Don't honor keys that we don't recognize.
    for key in list(kwargs.keys()):
        if key not in valid_keys:
            kwargs.pop(key)
    # Truncate certain values over 1k
    for key in [""play"", ""role"", ""task"", ""playbook""]:
        if isinstance(kwargs.get(""event_data"", {}).get(key), str):
            if len(kwargs[""event_data""][key]) > 1024:
                kwargs[""event_data""][key] = Truncator(kwargs[""event_data""][key]).chars(
                    1024
                )
","if isinstance ( kwargs . get ( ""event_data"" , { } ) . get ( key ) , str ) :",168
"def parse_auth(val):
    if val is not None:
        authtype, params = val.split("" "", 1)
        if authtype in known_auth_schemes:
            if authtype == ""Basic"" and '""' not in params:
                # this is the ""Authentication: Basic XXXXX=="" case
                pass
            else:
                params = parse_auth_params(params)
        return authtype, params
    return val
",if authtype in known_auth_schemes :,117
"def _memoized(*args):
    now = time.time()
    try:
        value, last_update = self.cache[args]
        age = now - last_update
        if self._call_count > self.ctl or age > self.ttl:
            self._call_count = 0
            raise AttributeError
        if self.ctl:
            self._call_count += 1
        return value
    except (KeyError, AttributeError):
        value = func(*args)
        if value:
            self.cache[args] = (value, now)
        return value
    except TypeError:
        return func(*args)
",if value :,164
"def _get_md_bg_color_down(self):
    t = self.theme_cls
    c = self.md_bg_color  # Default to no change on touch
    # Material design specifies using darker hue when on Dark theme
    if t.theme_style == ""Dark"":
        if self.md_bg_color == t.primary_color:
            c = t.primary_dark
        elif self.md_bg_color == t.accent_color:
            c = t.accent_dark
    return c
",elif self . md_bg_color == t . accent_color :,135
"def _init_table_h():
    _table_h = []
    for i in range(256):
        part_l = i
        part_h = 0
        for j in range(8):
            rflag = part_l & 1
            part_l >>= 1
            if part_h & 1:
                part_l |= 1 << 31
            part_h >>= 1
            if rflag:
                part_h ^= 0xD8000000
        _table_h.append(part_h)
    return _table_h
",if part_h & 1 :,147
"def migrate_Stats(self):
    for old_obj in self.session_old.query(self.model_from[""Stats""]):
        if not old_obj.summary:
            self.entries_count[""Stats""] -= 1
            continue
        new_obj = self.model_to[""Stats""]()
        for key in new_obj.__table__.columns._data.keys():
            if key not in old_obj.__table__.columns:
                continue
            setattr(new_obj, key, getattr(old_obj, key))
        self.session_new.add(new_obj)
",if key not in old_obj . __table__ . columns :,152
"def get_in_turn_repetition(pred, is_cn=False):
    """"""Get in-turn repetition.""""""
    if len(pred) == 0:
        return 1.0
    if isinstance(pred[0], str):
        pred = [tok.lower() for tok in pred]
        if is_cn:
            pred = """".join(pred)
    tri_grams = set()
    for i in range(len(pred) - 2):
        tri_gram = tuple(pred[i : i + 3])
        if tri_gram in tri_grams:
            return 1.0
        tri_grams.add(tri_gram)
    return 0.0
",if tri_gram in tri_grams :,169
"def translate():
    assert Lex.next() is AttributeList
    reader.read()  # Discard attribute list from reader.
    attrs = {}
    d = AttributeList.match.groupdict()
    for k, v in d.items():
        if v is not None:
            if k == ""attrlist"":
                v = subs_attrs(v)
                if v:
                    parse_attributes(v, attrs)
            else:
                AttributeList.attrs[k] = v
    AttributeList.subs(attrs)
    AttributeList.attrs.update(attrs)
","if k == ""attrlist"" :",150
"def _parse(self, engine):
    """"""Parse the layer.""""""
    if isinstance(self.args, dict):
        if ""axis"" in self.args:
            self.axis = engine.evaluate(self.args[""axis""], recursive=True)
            if not isinstance(self.axis, int):
                raise ParsingError('""axis"" must be an integer.')
        if ""momentum"" in self.args:
            self.momentum = engine.evaluate(self.args[""momentum""], recursive=True)
            if not isinstance(self.momentum, (int, float)):
                raise ParsingError('""momentum"" must be numeric.')
","if not isinstance ( self . axis , int ) :",157
"def __getattr__(self, attrname):
    if attrname in (""visamp"", ""visamperr"", ""visphi"", ""visphierr""):
        return ma.masked_array(self.__dict__[""_"" + attrname], mask=self.flag)
    elif attrname in (""cflux"", ""cfluxerr""):
        if self.__dict__[""_"" + attrname] != None:
            return ma.masked_array(self.__dict__[""_"" + attrname], mask=self.flag)
        else:
            return None
    else:
        raise AttributeError(attrname)
","if self . __dict__ [ ""_"" + attrname ] != None :",141
"def draw(self, context):
    layout = self.layout
    presets.draw_presets_ops(layout, context=context)
    for category in presets.get_category_names():
        if category in preset_category_menus:
            if category in preset_category_menus:
                class_name = preset_category_menus[category].__name__
                layout.menu(class_name)
",if category in preset_category_menus :,107
"def __setitem__(self, key, value):
    if isinstance(value, (tuple, list)):
        info, reference = value
        if info not in self._reverse_infos:
            self._reverse_infos[info] = len(self._infos)
            self._infos.append(info)
        if reference not in self._reverse_references:
            self._reverse_references[reference] = len(self._references)
            self._references.append(reference)
        self._trails[key] = ""%d,%d"" % (
            self._reverse_infos[info],
            self._reverse_references[reference],
        )
    else:
        raise Exception(""unsupported type '%s'"" % type(value))
",if reference not in self . _reverse_references :,184
"def format_bpe_text(symbols, delimiter=b""@@""):
    """"""Convert a sequence of bpe words into sentence.""""""
    words = []
    word = b""""
    if isinstance(symbols, str):
        symbols = symbols.encode()
    delimiter_len = len(delimiter)
    for symbol in symbols:
        if len(symbol) >= delimiter_len and symbol[-delimiter_len:] == delimiter:
            word += symbol[:-delimiter_len]
        else:  # end of a word
            word += symbol
            words.append(word)
            word = b""""
    return b"" "".join(words)
",if len ( symbol ) >= delimiter_len and symbol [ - delimiter_len : ] == delimiter :,154
"def output_type(data, request, response):
    accept = request.accept
    if accept in ("""", ""*"", ""/""):
        handler = default or handlers and next(iter(handlers.values()))
    else:
        handler = default
        accepted = [accept_quality(accept_type) for accept_type in accept.split("","")]
        accepted.sort(key=itemgetter(0))
        for _quality, accepted_content_type in reversed(accepted):
            if accepted_content_type in handlers:
                handler = handlers[accepted_content_type]
                break
    if not handler:
        raise falcon.HTTPNotAcceptable(error)
    response.content_type = handler.content_type
    return handler(data, request=request, response=response)
",if accepted_content_type in handlers :,189
"def _render_raw_list(bytes_items):
    flatten_items = []
    for item in bytes_items:
        if item is None:
            flatten_items.append(b"""")
        elif isinstance(item, bytes):
            flatten_items.append(item)
        elif isinstance(item, int):
            flatten_items.append(str(item).encode())
        elif isinstance(item, list):
            flatten_items.append(_render_raw_list(item))
    return b""\n"".join(flatten_items)
","elif isinstance ( item , int ) :",138
"def TryMerge(self, d):
    while d.avail() > 0:
        tt = d.getVarInt32()
        if tt == 8:
            self.set_mime_type(d.getVarInt32())
            continue
        if tt == 16:
            self.set_quality(d.getVarInt32())
            continue
        if tt == 0:
            raise ProtocolBuffer.ProtocolBufferDecodeError
        d.skipData(tt)
",if tt == 8 :,122
"def delete(self, waiters):
    # Delete flow.
    msgs = self.ofctl.get_all_flow(waiters)
    for msg in msgs:
        for stats in msg.body:
            vlan_id = VlanRouter._cookie_to_id(REST_VLANID, stats.cookie)
            if vlan_id == self.vlan_id:
                self.ofctl.delete_flow(stats)
    assert len(self.packet_buffer) == 0
",if vlan_id == self . vlan_id :,127
"def missing_push_allowance(push_allowances: List[PushAllowance]) -> bool:
    for push_allowance in push_allowances:
        # a null databaseId indicates this is not a GitHub App.
        if push_allowance.actor.databaseId is None:
            continue
        if str(push_allowance.actor.databaseId) == str(app_config.GITHUB_APP_ID):
            return False
    return True
",if str ( push_allowance . actor . databaseId ) == str ( app_config . GITHUB_APP_ID ) :,112
"def _cluster_page(self, htmlpage):
    template_cluster, preferred = _CLUSTER_NA, None
    if self.clustering:
        self.clustering.add_page(htmlpage)
        if self.clustering.is_fit:
            clt = self.clustering.classify(htmlpage)
            if clt != -1:
                template_cluster = preferred = self.template_names[clt]
            else:
                template_cluster = _CLUSTER_OUTLIER
    return template_cluster, preferred
",if self . clustering . is_fit :,136
"def readlines(self, size=-1):
    if self._nbr == self._size:
        return []
    # leave all additional logic to our readline method, we just check the size
    out = []
    nbr = 0
    while True:
        line = self.readline()
        if not line:
            break
        out.append(line)
        if size > -1:
            nbr += len(line)
            if nbr > size:
                break
        # END handle size constraint
    # END readline loop
    return out
",if nbr > size :,145
"def post_mortem(t=None):
    # handling the default
    if t is None:
        # sys.exc_info() returns (type, value, traceback) if an exception is
        # being handled, otherwise it returns None
        t = sys.exc_info()[2]
        if t is None:
            raise ValueError(
                ""A valid traceback must be passed if no exception is being handled.""
            )
    p = BPdb()
    p.reset()
    p.interaction(None, t)
",if t is None :,132
"def fixup(m):
    txt = m.group(0)
    if txt[:2] == ""&#"":
        # character reference
        try:
            if txt[:3] == ""&#x"":
                return unichr(int(txt[3:-1], 16))
            else:
                return unichr(int(txt[2:-1]))
        except ValueError:
            pass
    else:
        # named entity
        try:
            txt = unichr(htmlentitydefs.name2codepoint[txt[1:-1]])
        except KeyError:
            pass
    return txt  # leave as is
","if txt [ : 3 ] == ""&#x"" :",157
"def parse_converter_args(argstr: str) -> t.Tuple[t.Tuple, t.Dict[str, t.Any]]:
    argstr += "",""
    args = []
    kwargs = {}
    for item in _converter_args_re.finditer(argstr):
        value = item.group(""stringval"")
        if value is None:
            value = item.group(""value"")
        value = _pythonize(value)
        if not item.group(""name""):
            args.append(value)
        else:
            name = item.group(""name"")
            kwargs[name] = value
    return tuple(args), kwargs
",if value is None :,164
"def IT(cpu):
    cc = cpu.instruction.cc
    true_case = cpu._evaluate_conditional(cc)
    # this is incredibly hacky--how else does capstone expose this?
    # TODO: find a better way than string parsing the mnemonic -GR, 2017-07-13
    for c in cpu.instruction.mnemonic[1:]:
        if c == ""t"":
            cpu._it_conditional.append(true_case)
        elif c == ""e"":
            cpu._it_conditional.append(not true_case)
","if c == ""t"" :",138
"def flatten(self):
    # this is similar to fill_messages except it uses a list instead
    # of a queue to place the messages in.
    result = []
    channel = await self.messageable._get_channel()
    self.channel = channel
    while self._get_retrieve():
        data = await self._retrieve_messages(self.retrieve)
        if len(data) < 100:
            self.limit = 0  # terminate the infinite loop
        if self.reverse:
            data = reversed(data)
        if self._filter:
            data = filter(self._filter, data)
        for element in data:
            result.append(self.state.create_message(channel=channel, data=element))
    return result
",if len ( data ) < 100 :,187
"def _get_beta_accumulators(self):
    with tf.init_scope():
        if tf.executing_eagerly():
            graph = None
        else:
            graph = tf.get_default_graph()
        return (
            self._get_non_slot_variable(""beta1_power"", graph=graph),
            self._get_non_slot_variable(""beta2_power"", graph=graph),
        )
",if tf . executing_eagerly ( ) :,113
"def prefixed(self, prefix: _StrType) -> typing.Iterator[""Env""]:
    """"""Context manager for parsing envvars with a common prefix.""""""
    try:
        old_prefix = self._prefix
        if old_prefix is None:
            self._prefix = prefix
        else:
            self._prefix = f""{old_prefix}{prefix}""
        yield self
    finally:
        # explicitly reset the stored prefix on completion and exceptions
        self._prefix = None
    self._prefix = old_prefix
",if old_prefix is None :,126
"def decode_content(self):
    """"""Return the best possible representation of the response body.""""""
    ct = self.headers.get(""content-type"")
    if ct:
        ct, options = parse_options_header(ct)
        charset = options.get(""charset"")
        if ct in JSON_CONTENT_TYPES:
            return self.json(charset)
        elif ct.startswith(""text/""):
            return self.text(charset)
        elif ct == FORM_URL_ENCODED:
            return parse_qsl(self.content.decode(charset), keep_blank_values=True)
    return self.content
","elif ct . startswith ( ""text/"" ) :",156
"def test_incrementaldecoder(self):
    UTF8Writer = codecs.getwriter(""utf-8"")
    for sizehint in [None, -1] + list(range(1, 33)) + [64, 128, 256, 512, 1024]:
        istream = BytesIO(self.tstring[0])
        ostream = UTF8Writer(BytesIO())
        decoder = self.incrementaldecoder()
        while 1:
            data = istream.read(sizehint)
            if not data:
                break
            else:
                u = decoder.decode(data)
                ostream.write(u)
        self.assertEqual(ostream.getvalue(), self.tstring[1])
",if not data :,178
"def delete_all(path):
    ppath = os.getcwd()
    os.chdir(path)
    for fn in glob.glob(""*""):
        fn_full = os.path.join(path, fn)
        if os.path.isdir(fn):
            delete_all(fn_full)
        elif fn.endswith("".png""):
            os.remove(fn_full)
        elif fn.endswith("".md""):
            os.remove(fn_full)
        elif DELETE_ALL_OLD:
            os.remove(fn_full)
    os.chdir(ppath)
    os.rmdir(path)
",if os . path . isdir ( fn ) :,158
"def _delete_reason(self):
    for i in range(_lib.X509_REVOKED_get_ext_count(self._revoked)):
        ext = _lib.X509_REVOKED_get_ext(self._revoked, i)
        obj = _lib.X509_EXTENSION_get_object(ext)
        if _lib.OBJ_obj2nid(obj) == _lib.NID_crl_reason:
            _lib.X509_EXTENSION_free(ext)
            _lib.X509_REVOKED_delete_ext(self._revoked, i)
            break
",if _lib . OBJ_obj2nid ( obj ) == _lib . NID_crl_reason :,158
"def hexcmp(x, y):
    try:
        a = int(x, 16)
        b = int(y, 16)
        if a < b:
            return -1
        if a > b:
            return 1
        return 0
    except:
        return cmp(x, y)
",if a > b :,83
"def get_indentation_count(view, start):
    indent_count = 0
    i = start - 1
    while i > 0:
        ch = view.substr(i)
        scope = view.scope_name(i)
        # Skip preprocessors, strings, characaters and comments
        if ""string.quoted"" in scope or ""comment"" in scope or ""preprocessor"" in scope:
            extent = view.extract_scope(i)
            i = extent.a - 1
            continue
        else:
            i -= 1
        if ch == ""}"":
            indent_count -= 1
        elif ch == ""{"":
            indent_count += 1
    return indent_count
","if ch == ""}"" :",177
"def set(self, name, value, ex=None, px=None, nx=False, xx=False):
    if (
        (not nx and not xx)
        or (nx and self._db.get(name, None) is None)
        or (xx and not self._db.get(name, None) is None)
    ):
        if ex > 0:
            self._db.expire(name, datetime.now() + timedelta(seconds=ex))
        elif px > 0:
            self._db.expire(name, datetime.now() + timedelta(milliseconds=px))
        self._db[name] = str(value)
        return True
    else:
        return None
",elif px > 0 :,174
"def _get_between(content, start, end=None):
    should_yield = False
    for line in content.split(""\n""):
        if start in line:
            should_yield = True
            continue
        if end and end in line:
            return
        if should_yield and line:
            yield line.strip().split("" "")[0]
",if should_yield and line :,94
"def iter_event_handlers(
    self,
    resource: resources_.Resource,
    event: bodies.RawEvent,
) -> Iterator[handlers.ResourceWatchingHandler]:
    warnings.warn(
        ""SimpleRegistry.iter_event_handlers() is deprecated; use ""
        ""ResourceWatchingRegistry.iter_handlers()."",
        DeprecationWarning,
    )
    cause = _create_watching_cause(resource, event)
    for handler in self._handlers:
        if not isinstance(handler, handlers.ResourceWatchingHandler):
            pass
        elif registries.match(handler=handler, cause=cause, ignore_fields=True):
            yield handler
","if not isinstance ( handler , handlers . ResourceWatchingHandler ) :",160
"def __enter__(self):
    if log_timer:
        if self.logger:
            self.logger.debug(""%s starting"" % self.name)
        else:
            print((""[%s starting]..."" % self.name))
        self.tstart = time.time()
",if self . logger :,74
"def _handle_errors(errors):
    """"""Log out and possibly reraise errors during import.""""""
    if not errors:
        return
    log_all = True  # pylint: disable=unused-variable
    err_msg = ""T2T: skipped importing {num_missing} data_generators modules.""
    print(err_msg.format(num_missing=len(errors)))
    for module, err in errors:
        err_str = str(err)
        if log_all:
            print(""Did not import module: %s; Cause: %s"" % (module, err_str))
        if not _is_import_err_msg(err_str, module):
            print(""From module %s"" % module)
            raise err
",if log_all :,184
"def _ungroup(sequence, groups=None):
    for v in sequence:
        if isinstance(v, (list, tuple)):
            if groups is not None:
                groups.append(list(_ungroup(v, groups=None)))
            for v in _ungroup(v, groups):
                yield v
        else:
            yield v
",if groups is not None :,95
"def run(self):
    while not self.completed:
        if self.block:
            time.sleep(self.period)
        else:
            self._completed.wait(self.period)
        self.counter += 1
        try:
            self.callback(self.counter)
        except Exception:
            self.stop()
        if self.timeout is not None:
            dt = time.time() - self._start_time
            if dt > self.timeout:
                self.stop()
        if self.counter == self.count:
            self.stop()
",if dt > self . timeout :,159
"def dont_let_stderr_buffer():
    while True:
        line = context.daemon.stderr.readline()
        if not line:
            return
        if DEAD_DEPLOYD_WORKER_MESSAGE.encode(""utf-8"") in line:
            context.num_workers_crashed += 1
        print(f""deployd stderr: {line}"")
",if not line :,93
"def mergeHiLo(self, x_stats):
    """"""Merge the highs and lows of another accumulator into myself.""""""
    if x_stats.firsttime is not None:
        if self.firsttime is None or x_stats.firsttime < self.firsttime:
            self.firsttime = x_stats.firsttime
            self.first = x_stats.first
    if x_stats.lasttime is not None:
        if self.lasttime is None or x_stats.lasttime >= self.lasttime:
            self.lasttime = x_stats.lasttime
            self.last = x_stats.last
",if self . lasttime is None or x_stats . lasttime >= self . lasttime :,157
"def test_rlimit_get(self):
    import resource
    p = psutil.Process(os.getpid())
    names = [x for x in dir(psutil) if x.startswith(""RLIMIT"")]
    assert names
    for name in names:
        value = getattr(psutil, name)
        self.assertGreaterEqual(value, 0)
        if name in dir(resource):
            self.assertEqual(value, getattr(resource, name))
            self.assertEqual(p.rlimit(value), resource.getrlimit(value))
        else:
            ret = p.rlimit(value)
            self.assertEqual(len(ret), 2)
            self.assertGreaterEqual(ret[0], -1)
            self.assertGreaterEqual(ret[1], -1)
",if name in dir ( resource ) :,192
"def _calculate_writes_for_built_in_indices(self, entity):
    writes = 0
    for prop_name in entity.keys():
        if not prop_name in entity.unindexed_properties():
            prop_vals = entity[prop_name]
            if isinstance(prop_vals, (list)):
                num_prop_vals = len(prop_vals)
            else:
                num_prop_vals = 1
            writes += 2 * num_prop_vals
    return writes
","if isinstance ( prop_vals , ( list ) ) :",131
"def check_value_check(self, x_data, t_data, use_cudnn):
    x = chainer.Variable(x_data)
    t = chainer.Variable(t_data)
    with chainer.using_config(""use_cudnn"", use_cudnn):
        if self.valid:
            # Check if it throws nothing
            functions.softmax_cross_entropy(
                x, t, enable_double_backprop=self.enable_double_backprop
            )
        else:
            with self.assertRaises(ValueError):
                functions.softmax_cross_entropy(
                    x, t, enable_double_backprop=self.enable_double_backprop
                )
",if self . valid :,188
"def get_note_title_file(note):
    mo = note_title_re.match(note.get(""content"", """"))
    if mo:
        fn = mo.groups()[0]
        fn = fn.replace("" "", ""_"")
        fn = fn.replace(""/"", ""_"")
        if not fn:
            return """"
        if isinstance(fn, str):
            fn = unicode(fn, ""utf-8"")
        else:
            fn = unicode(fn)
        if note_markdown(note):
            fn += "".mkdn""
        else:
            fn += "".txt""
        return fn
    else:
        return """"
",if not fn :,169
"def _parseparam(s):
    plist = []
    while s[:1] == "";"":
        s = s[1:]
        end = s.find("";"")
        while end > 0 and (s.count('""', 0, end) - s.count('\\""', 0, end)) % 2:
            end = s.find("";"", end + 1)
        if end < 0:
            end = len(s)
        f = s[:end]
        if ""="" in f:
            i = f.index(""="")
            f = f[:i].strip().lower() + ""="" + f[i + 1 :].strip()
        plist.append(f.strip())
        s = s[end:]
    return plist
","if ""="" in f :",177
"def doDir(elem):
    for child in elem.childNodes:
        if not isinstance(child, minidom.Element):
            continue
        if child.tagName == ""Directory"":
            doDir(child)
        elif child.tagName == ""Component"":
            for grandchild in child.childNodes:
                if not isinstance(grandchild, minidom.Element):
                    continue
                if grandchild.tagName != ""File"":
                    continue
                files.add(grandchild.getAttribute(""Source"").replace(os.sep, ""/""))
","if grandchild . tagName != ""File"" :",152
"def date_to_format(value, target_format):
    """"""Convert date to specified format""""""
    if target_format == str:
        if isinstance(value, datetime.date):
            ret = value.strftime(""%d/%m/%y"")
        elif isinstance(value, datetime.datetime):
            ret = value.strftime(""%d/%m/%y"")
        elif isinstance(value, datetime.time):
            ret = value.strftime(""%H:%M:%S"")
    else:
        ret = value
    return ret
","if isinstance ( value , datetime . date ) :",130
"def __listingColumns(self):
    columns = []
    for name in self.__getColumns():
        definition = column(name)
        if not definition:
            IECore.msg(
                IECore.Msg.Level.Error,
                ""GafferImageUI.CatalogueUI"",
                ""No column registered with name '%s'"" % name,
            )
            continue
        if isinstance(definition, IconColumn):
            c = GafferUI.PathListingWidget.IconColumn(definition.title(), """", name)
        else:
            c = GafferUI.PathListingWidget.StandardColumn(definition.title(), name)
        columns.append(c)
    return columns
",if not definition :,184
"def metrics_to_scalars(self, metrics):
    new_metrics = {}
    for k, v in metrics.items():
        if isinstance(v, torch.Tensor):
            v = v.item()
        if isinstance(v, dict):
            v = self.metrics_to_scalars(v)
        new_metrics[k] = v
    return new_metrics
","if isinstance ( v , torch . Tensor ) :",95
"def start(self, connection):
    try:
        if self.client_name:
            creds = gssapi.Credentials(name=gssapi.Name(self.client_name))
        else:
            creds = None
        hostname = self.get_hostname(connection)
        name = gssapi.Name(
            b""@"".join([self.service, hostname]), gssapi.NameType.hostbased_service
        )
        context = gssapi.SecurityContext(name=name, creds=creds)
        return context.step(None)
    except gssapi.raw.misc.GSSError:
        if self.fail_soft:
            return NotImplemented
        else:
            raise
",if self . fail_soft :,186
"def nanmax(self, axis=None, dtype=None, keepdims=None):
    ret = self._reduction(
        ""nanmax"", axis=axis, dtype=dtype, keepdims=keepdims, todense=True
    )
    if not issparse(ret):
        if get_array_module(ret).isscalar(ret):
            return ret
        xps = get_sparse_module(self.spmatrix)
        ret = SparseNDArray(xps.csr_matrix(ret))
        return ret
    return ret
",if get_array_module ( ret ) . isscalar ( ret ) :,120
"def utterance_to_sample(query_data, tagging_scheme, language):
    tokens, tags = [], []
    current_length = 0
    for chunk in query_data:
        chunk_tokens = tokenize(chunk[TEXT], language)
        tokens += [
            Token(t.value, current_length + t.start, current_length + t.end)
            for t in chunk_tokens
        ]
        current_length += len(chunk[TEXT])
        if SLOT_NAME not in chunk:
            tags += negative_tagging(len(chunk_tokens))
        else:
            tags += positive_tagging(
                tagging_scheme, chunk[SLOT_NAME], len(chunk_tokens)
            )
    return {TOKENS: tokens, TAGS: tags}
",if SLOT_NAME not in chunk :,200
"def use_index(
    self, term: Union[str, Index], *terms: Union[str, Index]
) -> ""QueryBuilder"":
    for t in (term, *terms):
        if isinstance(t, Index):
            self._use_indexes.append(t)
        elif isinstance(t, str):
            self._use_indexes.append(Index(t))
","elif isinstance ( t , str ) :",94
"def reconfigServiceWithBuildbotConfig(self, new_config):
    if new_config.manhole != self.manhole:
        if self.manhole:
            yield self.manhole.disownServiceParent()
            self.manhole = None
        if new_config.manhole:
            self.manhole = new_config.manhole
            yield self.manhole.setServiceParent(self)
    # chain up
    yield service.ReconfigurableServiceMixin.reconfigServiceWithBuildbotConfig(
        self, new_config
    )
",if new_config . manhole :,142
"def cleanup_folder(target_folder):
    for file in os.listdir(target_folder):
        file_path = os.path.join(target_folder, file)
        try:
            if os.path.isfile(file_path):
                os.remove(file_path)
        except Exception as e:
            logging.error(e)
",if os . path . isfile ( file_path ) :,93
"def to_key(literal_or_identifier):
    """"""returns string representation of this object""""""
    if literal_or_identifier[""type""] == ""Identifier"":
        return literal_or_identifier[""name""]
    elif literal_or_identifier[""type""] == ""Literal"":
        k = literal_or_identifier[""value""]
        if isinstance(k, float):
            return unicode(float_repr(k))
        elif ""regex"" in literal_or_identifier:
            return compose_regex(k)
        elif isinstance(k, bool):
            return ""true"" if k else ""false""
        elif k is None:
            return ""null""
        else:
            return unicode(k)
","if isinstance ( k , float ) :",179
"def decompile(decompiler):
    for pos, next_pos, opname, arg in decompiler.instructions:
        if pos in decompiler.targets:
            decompiler.process_target(pos)
        method = getattr(decompiler, opname, None)
        if method is None:
            throw(DecompileError(""Unsupported operation: %s"" % opname))
        decompiler.pos = pos
        decompiler.next_pos = next_pos
        x = method(*arg)
        if x is not None:
            decompiler.stack.append(x)
",if method is None :,143
"def shutdown(self, timeout, callback=None):
    logger.debug(""background worker got shutdown request"")
    with self._lock:
        if self.is_alive:
            self._queue.put_nowait(_TERMINATOR)
            if timeout > 0.0:
                self._wait_shutdown(timeout, callback)
        self._thread = None
        self._thread_for_pid = None
    logger.debug(""background worker shut down"")
",if timeout > 0.0 :,113
"def getDOMImplementation(features=None):
    if features:
        if isinstance(features, str):
            features = domreg._parse_feature_string(features)
        for f, v in features:
            if not Document.implementation.hasFeature(f, v):
                return None
    return Document.implementation
","if isinstance ( features , str ) :",83
"def validate_subevent(self, subevent):
    if self.context[""event""].has_subevents:
        if not subevent:
            raise ValidationError(""You need to set a subevent."")
        if subevent.event != self.context[""event""]:
            raise ValidationError(
                ""The specified subevent does not belong to this event.""
            )
    elif subevent:
        raise ValidationError(""You cannot set a subevent for this event."")
    return subevent
",if not subevent :,120
"def einsum(job_id, idx, einsum_expr, data_list):
    _, all_parties = session_init(job_id, idx)
    with SPDZ():
        if idx == 0:
            x = FixedPointTensor.from_source(""x"", data_list[0])
            y = FixedPointTensor.from_source(""y"", all_parties[1])
        else:
            x = FixedPointTensor.from_source(""x"", all_parties[0])
            y = FixedPointTensor.from_source(""y"", data_list[1])
        return x.einsum(y, einsum_expr).get()
",if idx == 0 :,162
"def slowSorted(qq):
    ""Reference sort peformed by insertion using only <""
    rr = list()
    for q in qq:
        i = 0
        for i in range(len(rr)):
            if q < rr[i]:
                rr.insert(i, q)
                break
        else:
            rr.append(q)
    return rr
",if q < rr [ i ] :,101
"def _format_entry(entry, src):
    if entry:
        result = []
        for x in entry.split("",""):
            x = x.strip()
            if os.path.exists(os.path.join(src, x)):
                result.append(relpath(os.path.join(src, x), src))
            elif os.path.exists(x):
                result.append(relpath(os.path.abspath(x), src))
            else:
                raise RuntimeError(""No entry script %s found"" % x)
        return "","".join(result)
",elif os . path . exists ( x ) :,153
"def reloadCols(self):
    self.columns = []
    for i, (name, fmt, *shape) in enumerate(self.npy.dtype.descr):
        if shape:
            t = anytype
        elif ""M"" in fmt:
            self.addColumn(Column(name, type=date, getter=lambda c, r, i=i: str(r[i])))
            continue
        elif ""i"" in fmt:
            t = int
        elif ""f"" in fmt:
            t = float
        else:
            t = anytype
        self.addColumn(ColumnItem(name, i, type=t))
","elif ""f"" in fmt :",168
"def tool_lineages(self, trans):
    rval = []
    for id, tool in self.app.toolbox.tools():
        if hasattr(tool, ""lineage""):
            lineage_dict = tool.lineage.to_dict()
        else:
            lineage_dict = None
        entry = dict(id=id, lineage=lineage_dict)
        rval.append(entry)
    return rval
","if hasattr ( tool , ""lineage"" ) :",102
"def item(self, tensor):
    numel = 0
    if len(tensor.shape) > 0:
        numel = fct.reduce(op.mul, tensor.shape)
        if numel != 1:
            raise ValueError(
                f""expected tensor with one element, "" f""got {tensor.shape}""
            )
    if numel == 1:
        return tensor[0]
    return tensor
",if numel != 1 :,105
"def get_host_metadata(self):
    meta = {}
    if self.agent_url:
        try:
            resp = requests.get(
                self.agent_url + ECS_AGENT_METADATA_PATH, timeout=1
            ).json()
            if ""Version"" in resp:
                match = AGENT_VERSION_EXP.search(resp.get(""Version""))
                if match is not None and len(match.groups()) == 1:
                    meta[""ecs_version""] = match.group(1)
        except Exception as e:
            self.log.debug(""Error getting ECS version: %s"" % str(e))
    return meta
",if match is not None and len ( match . groups ( ) ) == 1 :,176
"def generate():
    for leaf in u.leaves:
        if isinstance(leaf, Integer):
            val = leaf.get_int_value()
            if val in (0, 1):
                yield val
            else:
                raise _NoBoolVector
        elif isinstance(leaf, Symbol):
            if leaf == SymbolTrue:
                yield 1
            elif leaf == SymbolFalse:
                yield 0
            else:
                raise _NoBoolVector
        else:
            raise _NoBoolVector
",if leaf == SymbolTrue :,138
"def _test_set_metadata(self, metadata, mask=None):
    header = ofproto.OXM_OF_METADATA
    match = OFPMatch()
    if mask is None:
        match.set_metadata(metadata)
    else:
        if (mask + 1) >> 64 != 1:
            header = ofproto.OXM_OF_METADATA_W
        match.set_metadata_masked(metadata, mask)
        metadata &= mask
    self._test_serialize_and_parser(match, header, metadata, mask)
",if ( mask + 1 ) >> 64 != 1 :,134
"def pixbufrenderer(self, column, crp, model, it):
    tok = model.get_value(it, 0)
    if tok.type == ""class"":
        icon = ""class""
    else:
        if tok.visibility == ""private"":
            icon = ""method_priv""
        elif tok.visibility == ""protected"":
            icon = ""method_prot""
        else:
            icon = ""method""
    crp.set_property(""pixbuf"", imagelibrary.pixbufs[icon])
","elif tok . visibility == ""protected"" :",132
"def path_sum2(root, s):
    if root is None:
        return []
    res = []
    stack = [(root, [root.val])]
    while stack:
        node, ls = stack.pop()
        if node.left is None and node.right is None and sum(ls) == s:
            res.append(ls)
        if node.left is not None:
            stack.append((node.left, ls + [node.left.val]))
        if node.right is not None:
            stack.append((node.right, ls + [node.right.val]))
    return res
",if node . left is not None :,157
"def clear_slot(self, slot_id, trigger_changed):
    if self.slots[slot_id] is not None:
        old_resource_id = self.slots[slot_id].resource_id
        if self.slots[slot_id].selling:
            del self.sell_list[old_resource_id]
        else:
            del self.buy_list[old_resource_id]
    self.slots[slot_id] = None
    if trigger_changed:
        self._changed()
",if self . slots [ slot_id ] . selling :,132
"def OnRightUp(self, event):
    self.HandleMouseEvent(event)
    self.Unbind(wx.EVT_RIGHT_UP, handler=self.OnRightUp)
    self.Unbind(wx.EVT_MOUSE_CAPTURE_LOST, handler=self.OnRightUp)
    self._right = False
    if not self._left:
        self.Unbind(wx.EVT_MOTION, handler=self.OnMotion)
        self.SendChangeEvent()
        self.SetToolTip(wx.ToolTip(self._tooltip))
        if self.HasCapture():
            self.ReleaseMouse()
",if self . HasCapture ( ) :,150
"def __init__(self, *args, **kwargs):
    for arg in args:
        for k, v in arg.items():
            if isinstance(v, dict):
                arg[k] = AttrDict(v)
            else:
                arg[k] = v
    super(AttrDict, self).__init__(*args, **kwargs)
","if isinstance ( v , dict ) :",89
"def _toplevelTryFunc(func, *args, status=status, **kwargs):
    with ThreadProfiler(threading.current_thread()) as prof:
        t = threading.current_thread()
        t.name = func.__name__
        try:
            t.status = func(*args, **kwargs)
        except EscapeException as e:  # user aborted
            t.status = ""aborted by user""
            if status:
                status(""%s aborted"" % t.name, priority=2)
        except Exception as e:
            t.exception = e
            t.status = ""exception""
            vd.exceptionCaught(e)
        if t.sheet:
            t.sheet.currentThreads.remove(t)
",if t . sheet :,193
"def comboSelectionChanged(self, index):
    text = self.comboBox.cb.itemText(index)
    for i in range(self.labelList.count()):
        if text == """":
            self.labelList.item(i).setCheckState(2)
        elif text != self.labelList.item(i).text():
            self.labelList.item(i).setCheckState(0)
        else:
            self.labelList.item(i).setCheckState(2)
",elif text != self . labelList . item ( i ) . text ( ) :,120
"def __attempt_add_to_linked_match(
    self, input_name, hdca, collection_type_description, subcollection_type
):
    structure = get_structure(
        hdca, collection_type_description, leaf_subcollection_type=subcollection_type
    )
    if not self.linked_structure:
        self.linked_structure = structure
        self.collections[input_name] = hdca
        self.subcollection_types[input_name] = subcollection_type
    else:
        if not self.linked_structure.can_match(structure):
            raise exceptions.MessageException(CANNOT_MATCH_ERROR_MESSAGE)
        self.collections[input_name] = hdca
        self.subcollection_types[input_name] = subcollection_type
",if not self . linked_structure . can_match ( structure ) :,194
"def _wait_for_bot_presense(self, online):
    for _ in range(10):
        time.sleep(2)
        if online and self._is_testbot_online():
            break
        if not online and not self._is_testbot_online():
            break
    else:
        raise AssertionError(
            ""test bot is still {}"".format(""offline"" if online else ""online"")
        )
",if online and self . _is_testbot_online ( ) :,111
"def find(self, path):
    if os.path.isfile(path) or os.path.islink(path):
        self.num_files = self.num_files + 1
        if self.match_function(path):
            self.files.append(path)
    elif os.path.isdir(path):
        for content in os.listdir(path):
            file = os.path.join(path, content)
            if os.path.isfile(file) or os.path.islink(file):
                self.num_files = self.num_files + 1
                if self.match_function(file):
                    self.files.append(file)
            else:
                self.find(file)
",if self . match_function ( path ) :,192
"def optimize(self, graph: Graph):
    MAX_TEXTURE_SIZE = config.WEBGL_MAX_TEXTURE_SIZE
    flag_changed = False
    for v in traverse.listup_variables(graph):
        if not Placeholder.check_resolved(v.size):
            continue
        height, width = TextureShape.get(v)
        if height <= MAX_TEXTURE_SIZE and width <= MAX_TEXTURE_SIZE:
            continue
        if not v.has_attribute(SplitTarget):
            flag_changed = True
            v.attributes.add(SplitTarget())
    return graph, flag_changed
",if height <= MAX_TEXTURE_SIZE and width <= MAX_TEXTURE_SIZE :,157
"def brightness_func(args):
    device = _get_device_from_filter(args)
    if args.set is None:
        # Get brightness
        if args.raw:
            print(str(device.brightness))
        else:
            print(""Brightness: {0}%"".format(device.brightness))
    else:
        brightness_value = float(_clamp_u8(args.set))
        if not args.raw:
            print(""Setting brightness to {0}%"".format(brightness_value))
        device.brightness = brightness_value
",if not args . raw :,139
"def _setup(self, field_name, owner_model):
    # Resolve possible name-based model reference.
    if not self.model_class:
        if self.model_name == owner_model.__name__:
            self.model_class = owner_model
        else:
            raise Exception(
                ""ModelType: Unable to resolve model '{}'."".format(self.model_name)
            )
    super(ModelType, self)._setup(field_name, owner_model)
",if self . model_name == owner_model . __name__ :,124
"def build_json_schema_object(cls, parent_builder=None):
    builder = builders.ObjectBuilder(cls, parent_builder)
    if builder.count_type(builder.type) > 1:
        return builder
    for _, name, field in cls.iterate_with_name():
        if isinstance(field, fields.EmbeddedField):
            builder.add_field(name, field, _parse_embedded(field, builder))
        elif isinstance(field, fields.ListField):
            builder.add_field(name, field, _parse_list(field, builder))
        else:
            builder.add_field(name, field, _create_primitive_field_schema(field))
    return builder
","elif isinstance ( field , fields . ListField ) :",178
"def filter_module(mod, type_req=None, subclass_req=None):
    for name in dir(mod):
        val = getattr(mod, name)
        if type_req is not None and not isinstance(val, type_req):
            continue
        if subclass_req is not None and not issubclass(val, subclass_req):
            continue
        yield name, val
","if subclass_req is not None and not issubclass ( val , subclass_req ) :",97
"def get_icon(self):
    if self.icon is not None:
        # Load it from an absolute filename
        if os.path.exists(self.icon):
            try:
                return GdkPixbuf.Pixbuf.new_from_file_at_size(self.icon, 24, 24)
            except GObject.GError as ge:
                pass
        # Load it from the current icon theme
        (icon_name, extension) = os.path.splitext(os.path.basename(self.icon))
        theme = Gtk.IconTheme()
        if theme.has_icon(icon_name):
            return theme.load_icon(icon_name, 24, 0)
",if theme . has_icon ( icon_name ) :,174
"def sysctlTestAndSet(name, limit):
    ""Helper function to set sysctl limits""
    # convert non-directory names into directory names
    if ""/"" not in name:
        name = ""/proc/sys/"" + name.replace(""."", ""/"")
    # read limit
    with open(name, ""r"") as readFile:
        oldLimit = readFile.readline()
        if isinstance(limit, int):
            # compare integer limits before overriding
            if int(oldLimit) < limit:
                with open(name, ""w"") as writeFile:
                    writeFile.write(""%d"" % limit)
        else:
            # overwrite non-integer limits
            with open(name, ""w"") as writeFile:
                writeFile.write(limit)
",if int ( oldLimit ) < limit :,197
"def _wait_for_bot_presense(self, online):
    for _ in range(10):
        time.sleep(2)
        if online and self._is_testbot_online():
            break
        if not online and not self._is_testbot_online():
            break
    else:
        raise AssertionError(
            ""test bot is still {}"".format(""offline"" if online else ""online"")
        )
",if not online and not self . _is_testbot_online ( ) :,111
"def handle(self, context, sign, *args):
    if context.rounding in (ROUND_HALF_UP, ROUND_HALF_EVEN, ROUND_HALF_DOWN, ROUND_UP):
        return Infsign[sign]
    if sign == 0:
        if context.rounding == ROUND_CEILING:
            return Infsign[sign]
        return Decimal((sign, (9,) * context.prec, context.Emax - context.prec + 1))
    if sign == 1:
        if context.rounding == ROUND_FLOOR:
            return Infsign[sign]
        return Decimal((sign, (9,) * context.prec, context.Emax - context.prec + 1))
",if context . rounding == ROUND_FLOOR :,184
"def _get_item_columns_panel(items, rows):
    hbox = Gtk.HBox(False, 4)
    n_item = 0
    col_items = 0
    vbox = Gtk.VBox()
    hbox.pack_start(vbox, False, False, 0)
    while n_item < len(items):
        item = items[n_item]
        vbox.pack_start(item, False, False, 0)
        n_item += 1
        col_items += 1
        if col_items > rows:
            vbox = Gtk.VBox()
            hbox.pack_start(vbox, False, False, 0)
            col_items = 0
    return hbox
",if col_items > rows :,179
"def _changed(self):
    if self.gtk_range.get_sensitive():
        if self.timer:
            self.timer.cancel()
        self.timer = _Timer(0.5, lambda: GLib.idle_add(self._write))
        self.timer.start()
",if self . timer :,74
"def unlock_graph(result, callback, interval=1, propagate=False, max_retries=None):
    if result.ready():
        second_level_res = result.get()
        if second_level_res.ready():
            with allow_join_result():
                signature(callback).delay(
                    list(joinall(second_level_res, propagate=propagate))
                )
    else:
        unlock_graph.retry(countdown=interval, max_retries=max_retries)
",if second_level_res . ready ( ) :,131
"def update(self, other=None, /, **kwargs):
    if self._pending_removals:
        self._commit_removals()
    d = self.data
    if other is not None:
        if not hasattr(other, ""items""):
            other = dict(other)
        for key, o in other.items():
            d[key] = KeyedRef(o, self._remove, key)
    for key, o in kwargs.items():
        d[key] = KeyedRef(o, self._remove, key)
","if not hasattr ( other , ""items"" ) :",135
"def default(self, o):
    try:
        if type(o) == datetime.datetime:
            return str(o)
        else:
            # remove unwanted attributes from the provider object during conversion to json
            if hasattr(o, ""profile""):
                del o.profile
            if hasattr(o, ""credentials""):
                del o.credentials
            if hasattr(o, ""metadata_path""):
                del o.metadata_path
            if hasattr(o, ""services_config""):
                del o.services_config
            return vars(o)
    except Exception as e:
        return str(o)
","if hasattr ( o , ""services_config"" ) :",172
"def read(self, count=True, timeout=None, ignore_non_errors=True, ignore_timeouts=True):
    try:
        return self._read(count, timeout)
    except usb.USBError as e:
        if DEBUG_COMM:
            log.info(
                ""read: e.errno=%s e.strerror=%s e.message=%s repr=%s""
                % (e.errno, e.strerror, e.message, repr(e))
            )
        if ignore_timeouts and is_timeout(e):
            return []
        if ignore_non_errors and is_noerr(e):
            return []
        raise
",if DEBUG_COMM :,174
"def heal(self):
    if not self.doctors:
        return
    proc_ids = self._get_process_ids()
    for proc_id in proc_ids:
        # get proc every time for latest state
        proc = PipelineProcess.objects.get(id=proc_id)
        if not proc.is_alive or proc.is_frozen:
            continue
        for dr in self.doctors:
            if dr.confirm(proc):
                dr.cure(proc)
                break
",if not proc . is_alive or proc . is_frozen :,138
"def to_value(self, value):
    # Tip: 'value' is the object returned by
    #      taiga.projects.history.models.HistoryEntry.values_diff()
    ret = {}
    for key, val in value.items():
        if key in [""attachments"", ""custom_attributes"", ""description_diff""]:
            ret[key] = val
        elif key == ""points"":
            ret[key] = {k: {""from"": v[0], ""to"": v[1]} for k, v in val.items()}
        else:
            ret[key] = {""from"": val[0], ""to"": val[1]}
    return ret
","if key in [ ""attachments"" , ""custom_attributes"" , ""description_diff"" ] :",169
"def default_generator(
    self, dataset, epochs=1, mode=""fit"", deterministic=True, pad_batches=True
):
    for epoch in range(epochs):
        for (X_b, y_b, w_b, ids_b) in dataset.iterbatches(
            batch_size=self.batch_size,
            deterministic=deterministic,
            pad_batches=pad_batches,
        ):
            if mode == ""predict"":
                dropout = np.array(0.0)
            else:
                dropout = np.array(1.0)
            yield ([X_b, dropout], [y_b], [w_b])
","if mode == ""predict"" :",172
"def _cygwin_hack_find_addresses(target):
    addresses = []
    for h in [
        target,
        ""localhost"",
        ""127.0.0.1"",
    ]:
        try:
            addr = get_local_ip_for(h)
            if addr not in addresses:
                addresses.append(addr)
        except socket.gaierror:
            pass
    return defer.succeed(addresses)
",if addr not in addresses :,116
"def _get_notify(self, action_node):
    if action_node.name not in self._skip_notify_tasks:
        if action_node.notify:
            task_notify = NotificationsHelper.to_model(action_node.notify)
            return task_notify
        elif self._chain_notify:
            return self._chain_notify
    return None
",elif self . _chain_notify :,95
"def filterTokenLocation():
    i = None
    entry = None
    token = None
    tokens = []
    i = 0
    while 1:
        if not (i < len(extra.tokens)):
            break
        entry = extra.tokens[i]
        token = jsdict(
            {
                ""type"": entry.type,
                ""value"": entry.value,
            }
        )
        if extra.range:
            token.range = entry.range
        if extra.loc:
            token.loc = entry.loc
        tokens.append(token)
        i += 1
    extra.tokens = tokens
",if not ( i < len ( extra . tokens ) ) :,172
"def read(self, size=-1):
    buf = bytearray()
    while size != 0 and self.cursor < self.maxpos:
        if not self.in_current_block(self.cursor):
            self.seek_to_block(self.cursor)
        part = self.current_stream.read(size)
        if size > 0:
            if len(part) == 0:
                raise EOFError()
            size -= len(part)
        self.cursor += len(part)
        buf += part
    return bytes(buf)
",if size > 0 :,142
"def get_properties_from_model(model_class):
    """"""Show properties from a model""""""
    properties = []
    attr_names = [name for (name, value) in inspect.getmembers(model_class, isprop)]
    for attr_name in attr_names:
        if attr_name.endswith(""pk""):
            attr_names.remove(attr_name)
        else:
            properties.append(
                dict(label=attr_name, name=attr_name.strip(""_"").replace(""_"", "" ""))
            )
    return sorted(properties, key=lambda k: k[""label""])
","if attr_name . endswith ( ""pk"" ) :",151
"def __getitem__(self, name, set=set, getattr=getattr, id=id):
    visited = set()
    mydict = self.basedict
    while 1:
        value = mydict[name]
        if value is not None:
            return value
        myid = id(mydict)
        assert myid not in visited
        visited.add(myid)
        mydict = mydict.Parent
        if mydict is None:
            return
",if mydict is None :,120
"def multicolumn(self, list, format, cols=4):
    """"""Format a list of items into a multi-column list.""""""
    result = """"
    rows = (len(list) + cols - 1) // cols
    for col in range(cols):
        result = result + '<td width=""%d%%"" valign=top>' % (100 // cols)
        for i in range(rows * col, rows * col + rows):
            if i < len(list):
                result = result + format(list[i]) + ""<br>\n""
        result = result + ""</td>""
    return '<table width=""100%%"" summary=""list""><tr>%s</tr></table>' % result
",if i < len ( list ) :,167
"def format_exc(exc=None):
    """"""Return exc (or sys.exc_info if None), formatted.""""""
    try:
        if exc is None:
            exc = _exc_info()
        if exc == (None, None, None):
            return """"
        import traceback
        return """".join(traceback.format_exception(*exc))
    finally:
        del exc
",if exc is None :,98
"def assert_counts(res, lang, files, blank, comment, code):
    for line in res:
        fields = line.split()
        if len(fields) >= 5:
            if fields[0] == lang:
                self.assertEqual(files, int(fields[1]))
                self.assertEqual(blank, int(fields[2]))
                self.assertEqual(comment, int(fields[3]))
                self.assertEqual(code, int(fields[4]))
                return
    self.fail(""Found no output line for {}"".format(lang))
",if fields [ 0 ] == lang :,147
"def __iter__(self):
    for name, value in self.__class__.__dict__.items():
        if isinstance(value, alias_flag_value):
            continue
        if isinstance(value, flag_value):
            yield (name, self._has_flag(value.flag))
","if isinstance ( value , alias_flag_value ) :",71
"def optimize_models(args, use_cuda, models):
    """"""Optimize ensemble for generation""""""
    for model in models:
        model.make_generation_fast_(
            beamable_mm_beam_size=None if args.no_beamable_mm else args.beam,
            need_attn=args.print_alignment,
        )
        if args.fp16:
            model.half()
        if use_cuda:
            model.cuda()
",if use_cuda :,122
"def convertstore(self, mydict):
    targetheader = self.mypofile.header()
    targetheader.addnote(""extracted from web2py"", ""developer"")
    for source_str in mydict.keys():
        target_str = mydict[source_str]
        if target_str == source_str:
            # a convention with new (untranslated) web2py files
            target_str = u""""
        elif target_str.startswith(u""*** ""):
            # an older convention
            target_str = u""""
        pounit = self.convertunit(source_str, target_str)
        self.mypofile.addunit(pounit)
    return self.mypofile
","elif target_str . startswith ( u""*** "" ) :",180
"def __sparse_values_set(instances, static_col_indexes: list):
    tmp_result = {idx: set() for idx in static_col_indexes}
    for _, instance in instances:
        data_generator = instance.features.get_all_data()
        for idx, value in data_generator:
            if idx not in tmp_result:
                continue
            tmp_result[idx].add(value)
    result = [tmp_result[x] for x in static_col_indexes]
    return result
",if idx not in tmp_result :,132
"def puts(self):
    if self.puts_ is None:
        self.lazy_init_lock_.acquire()
        try:
            if self.puts_ is None:
                self.puts_ = PutRequest()
        finally:
            self.lazy_init_lock_.release()
    return self.puts_
",if self . puts_ is None :,89
"def run(self, args, **kwargs):
    if args.resource_ref or args.policy_type:
        filters = {}
        if args.resource_ref:
            filters[""resource_ref""] = args.resource_ref
        if args.policy_type:
            filters[""policy_type""] = args.policy_type
        filters.update(**kwargs)
        return self.manager.query(**filters)
    else:
        return self.manager.get_all(**kwargs)
",if args . policy_type :,123
"def Get_Gene(self, id):
    """"""Retreive the gene name (GN).""""""
    entry = self.Get(id)
    if not entry:
        return None
    GN = """"
    for line in string.split(entry, ""\n""):
        if line[0:5] == ""GN   "":
            GN = string.strip(line[5:])
            if GN[-1] == ""."":
                GN = GN[0:-1]
            return GN
        if line[0:2] == ""//"":
            break
    return GN
","if GN [ - 1 ] == ""."" :",150
"def processMovie(self, atom):
    for field in atom:
        if ""track"" in field:
            self.processTrack(field[""track""])
        if ""movie_hdr"" in field:
            self.processMovieHeader(field[""movie_hdr""])
","if ""track"" in field :",69
"def get_next_video_frame(self, skip_empty_frame=True):
    if not self.video_format:
        return
    while True:
        # We skip video packets which are not video frames
        # This happens in mkv files for the first few frames.
        video_packet = self._get_video_packet()
        if video_packet.image == 0:
            self._decode_video_packet(video_packet)
        if video_packet.image is not None or not skip_empty_frame:
            break
    if _debug:
        print(""Returning"", video_packet)
    return video_packet.image
",if video_packet . image == 0 :,162
"def get_devices(display=None):
    base = ""/dev/input""
    for filename in os.listdir(base):
        if filename.startswith(""event""):
            path = os.path.join(base, filename)
            if path in _devices:
                continue
            try:
                _devices[path] = EvdevDevice(display, path)
            except OSError:
                pass
    return list(_devices.values())
",if path in _devices :,120
"def _ensure_header_written(self, datasize):
    if not self._headerwritten:
        if not self._nchannels:
            raise Error(""# channels not specified"")
        if not self._sampwidth:
            raise Error(""sample width not specified"")
        if not self._framerate:
            raise Error(""sampling rate not specified"")
        self._write_header(datasize)
",if not self . _sampwidth :,99
"def process(self, fuzzresult):
    base_url = urljoin(fuzzresult.url, "".."")
    for line in fuzzresult.history.content.splitlines():
        record = line.split(""/"")
        if len(record) == 6 and record[1]:
            self.queue_url(urljoin(base_url, record[1]))
            # Directory
            if record[0] == ""D"":
                self.queue_url(urljoin(base_url, record[1]))
                self.queue_url(urljoin(base_url, ""%s/CVS/Entries"" % (record[1])))
","if record [ 0 ] == ""D"" :",153
"def tearDown(self):
    """"""Shutdown the UDP server.""""""
    try:
        if self.server:
            self.server.stop(2.0)
        if self.sock_hdlr:
            self.root_logger.removeHandler(self.sock_hdlr)
            self.sock_hdlr.close()
    finally:
        BaseTest.tearDown(self)
",if self . server :,97
"def get_backend(find_library=None):
    try:
        global _lib, _ctx
        if _lib is None:
            _lib = _load_library(find_library)
            _setup_prototypes(_lib)
            _ctx = _Context()
        _logger.warning(
            ""OpenUSB backend deprecated (https://github.com/pyusb/pyusb/issues/284)""
        )
        return _OpenUSB()
    except usb.libloader.LibraryException:
        # exception already logged (if any)
        _logger.error(""Error loading OpenUSB backend"", exc_info=False)
        return None
    except Exception:
        _logger.error(""Error loading OpenUSB backend"", exc_info=True)
        return None
",if _lib is None :,199
"def __init__(self, event, event_info, fields=[]):
    _wmi_object.__init__(self, event, fields=fields)
    _set(self, ""event_type"", None)
    _set(self, ""timestamp"", None)
    _set(self, ""previous"", None)
    if event_info:
        event_type = self.event_type_re.match(event_info.Path_.Class).group(1).lower()
        _set(self, ""event_type"", event_type)
        if hasattr(event_info, ""TIME_CREATED""):
            _set(self, ""timestamp"", from_1601(event_info.TIME_CREATED))
        if hasattr(event_info, ""PreviousInstance""):
            _set(self, ""previous"", event_info.PreviousInstance)
","if hasattr ( event_info , ""PreviousInstance"" ) :",199
"def _getListNextPackagesReadyToBuild():
    for pkg in Scheduler.listOfPackagesToBuild:
        if pkg in Scheduler.listOfPackagesCurrentlyBuilding:
            continue
        if constants.rpmCheck or Scheduler._checkNextPackageIsReadyToBuild(pkg):
            Scheduler.listOfPackagesNextToBuild.put((-Scheduler._getPriority(pkg), pkg))
            Scheduler.logger.debug(""Adding "" + pkg + "" to the schedule list"")
",if pkg in Scheduler . listOfPackagesCurrentlyBuilding :,113
"def process_all(self, lines, times=1):
    gap = False
    for _ in range(times):
        for line in lines:
            if gap:
                self.write("""")
            self.process(line)
            if not is_command(line):
                gap = True
    return 0
",if gap :,86
"def diff(old, new, display=True):
    """"""Nice colored diff implementation""""""
    if not isinstance(old, list):
        old = decolorize(str(old)).splitlines()
    if not isinstance(new, list):
        new = decolorize(str(new)).splitlines()
    line_types = {"" "": ""%Reset"", ""-"": ""%Red"", ""+"": ""%Green"", ""?"": ""%Pink""}
    if display:
        for line in difflib.Differ().compare(old, new):
            if line.startswith(""?""):
                continue
            print(colorize(line_types[line[0]], line))
    return old != new
","if line . startswith ( ""?"" ) :",155
"def get_limit(self, request):
    if self.limit_query_param:
        try:
            limit = int(request.query_params[self.limit_query_param])
            if limit < 0:
                raise ValueError()
            # Enforce maximum page size, if defined
            if settings.MAX_PAGE_SIZE:
                if limit == 0:
                    return settings.MAX_PAGE_SIZE
                else:
                    return min(limit, settings.MAX_PAGE_SIZE)
            return limit
        except (KeyError, ValueError):
            pass
    return self.default_limit
",if limit == 0 :,169
"def slice_fill(self, slice_):
    ""Fills the slice with zeroes for the dimensions that have single elements and squeeze_dims true""
    if isinstance(self.indexes, int):
        new_slice_ = [0]
        offset = 0
    else:
        new_slice_ = [slice_[0]]
        offset = 1
    for i in range(1, len(self.nums)):
        if self.squeeze_dims[i]:
            new_slice_.append(0)
        elif offset < len(slice_):
            new_slice_.append(slice_[offset])
            offset += 1
    new_slice_ += slice_[offset:]
    return new_slice_
",elif offset < len ( slice_ ) :,171
"def wrapper(*args, **kw):
    instance = args[0]
    try:
        if request.get_json() is None:
            ret_dict = instance._create_ret_object(
                instance.FAILURE, None, True, instance.MUST_JSON
            )
            instance.logger.error(instance.MUST_JSON)
            return jsonify(ret_dict), 400
    except BadRequest:
        ret_dict = instance._create_ret_object(
            instance.FAILURE, None, True, instance.MUST_JSON
        )
        instance.logger.error(instance.MUST_JSON)
        return jsonify(ret_dict), 400
    instance.logger.debug(""JSON is valid"")
    return f(*args, **kw)
",if request . get_json ( ) is None :,191
"def add_css(self, data):
    if data:
        for medium, paths in data.items():
            for path in paths:
                if not self._css.get(medium) or path not in self._css[medium]:
                    self._css.setdefault(medium, []).append(path)
",if not self . _css . get ( medium ) or path not in self . _css [ medium ] :,80
"def mangle_template(template: str, template_vars: Set[str]) -> str:
    if TEMPLATE_PREFIX in template or TEMPLATE_SUFFIX in template:
        raise Exception(""Cannot parse a template containing reserved strings"")
    for var in template_vars:
        original = f""{{{var}}}""
        if original not in template:
            raise Exception(
                f'Template string is missing a reference to ""{var}"" referred to in kwargs'
            )
        template = template.replace(original, mangled_name(var))
    return template
",if original not in template :,135
"def filterSimilarKeywords(keyword, kwdsIterator):
    """"""Return a sorted list of keywords similar to the one given.""""""
    seenDict = {}
    kwdSndx = soundex(keyword.encode(""ascii"", ""ignore""))
    matches = []
    matchesappend = matches.append
    checkContained = False
    if len(keyword) > 4:
        checkContained = True
    for movieID, key in kwdsIterator:
        if key in seenDict:
            continue
        seenDict[key] = None
        if checkContained and keyword in key:
            matchesappend(key)
            continue
        if kwdSndx == soundex(key.encode(""ascii"", ""ignore"")):
            matchesappend(key)
    return _sortKeywords(keyword, matches)
","if kwdSndx == soundex ( key . encode ( ""ascii"" , ""ignore"" ) ) :",193
"def GetInfo(self):
    for k, v in sorted(self.memory_parameters.items()):
        if k.startswith(""Pad""):
            continue
        if not v:
            continue
        print(""%s: \t%#08x (%s)"" % (k, v, v))
    print(""Memory ranges:"")
    print(""Start\t\tEnd\t\tLength"")
    for start, length in self.runs:
        print(""0x%X\t\t0x%X\t\t0x%X"" % (start, start + length, length))
","if k . startswith ( ""Pad"" ) :",145
"def Children(self):
    """"""Returns a list of all of this object's owned (strong) children.""""""
    children = []
    for property, attributes in self._schema.iteritems():
        (is_list, property_type, is_strong) = attributes[0:3]
        if is_strong and property in self._properties:
            if not is_list:
                children.append(self._properties[property])
            else:
                children.extend(self._properties[property])
    return children
",if not is_list :,130
"def normalize_res_identifier(self, emu, cw, val):
    mask = (16 ** (emu.get_ptr_size() // 2) - 1) << 16
    if val & mask:  # not an INTRESOURCE
        name = emu.read_mem_string(val, cw)
        if name[0] == ""#"":
            try:
                name = int(name[1:])
            except Exception:
                return 0
    else:
        name = val
    return name
","if name [ 0 ] == ""#"" :",132
"def _optimize(self, solutions):
    best_a = None
    best_silhouette = None
    best_k = None
    for a, silhouette, k in solutions():
        if best_silhouette is None:
            pass
        elif silhouette <= best_silhouette:
            break
        best_silhouette = silhouette
        best_a = a
        best_k = k
    return best_a, best_silhouette, best_k
",elif silhouette <= best_silhouette :,109
"def find_commit_type(sha):
    try:
        o = obj_store[sha]
    except KeyError:
        if not ignore_unknown:
            raise
    else:
        if isinstance(o, Commit):
            commits.add(sha)
        elif isinstance(o, Tag):
            tags.add(sha)
            commits.add(o.object[1])
        else:
            raise KeyError(""Not a commit or a tag: %s"" % sha)
",if not ignore_unknown :,127
"def on_search_entry_keypress(self, widget, event):
    key = Gdk.keyval_name(event.keyval)
    if key == ""Escape"":
        self.hide_search_box()
    elif key == ""Return"":
        # Combine with Shift?
        if event.state & Gdk.ModifierType.SHIFT_MASK:
            self.search_prev = False
            self.do_search(None)
        else:
            self.search_prev = True
",if event . state & Gdk . ModifierType . SHIFT_MASK :,121
"def process_webhook_prop(namespace):
    if not isinstance(namespace.webhook_properties, list):
        return
    result = {}
    for each in namespace.webhook_properties:
        if each:
            if ""="" in each:
                key, value = each.split(""="", 1)
            else:
                key, value = each, """"
            result[key] = value
    namespace.webhook_properties = result
",if each :,111
"def run(self):
    global WAITING_BEFORE_START
    time.sleep(WAITING_BEFORE_START)
    while self.keep_alive:
        path_id, module, resolve = self.queue_receive.get()
        if path_id is None:
            continue
        self.lock.acquire()
        self.modules[path_id] = module
        self.lock.release()
        if resolve:
            resolution = self._resolve_with_other_modules(resolve)
            self._relations[path_id] = []
            for package in resolution:
                self._relations[path_id].append(resolution[package])
            self.queue_send.put((path_id, module, False, resolution))
",if path_id is None :,190
"def _get_download_link(self, url, download_type=""torrent""):
    links = {
        ""torrent"": """",
        ""magnet"": """",
    }
    try:
        data = self.session.get(url).text
        with bs4_parser(data) as html:
            downloads = html.find(""div"", {""class"": ""download""})
            if downloads:
                for download in downloads.findAll(""a""):
                    link = download[""href""]
                    if link.startswith(""magnet""):
                        links[""magnet""] = link
                    else:
                        links[""torrent""] = urljoin(self.urls[""base_url""], link)
    except Exception:
        pass
    return links[download_type]
","if link . startswith ( ""magnet"" ) :",200
"def _parse_fields(cls, read):
    read = unicode_to_str(read)
    if type(read) is not str:
        _wrong_type_for_arg(read, ""str"", ""read"")
    fields = {}
    while read and read[0] != "";"":
        if read and read[0] != "","":
            DeserializeError(read, ""does not separate fields with commas"")
        read = read[1:]
        key, _type, value, read = cls._parse_field(read)
        fields[key] = (_type, value)
    if read:
        # read[0] == ';'
        read = read[1:]
    return fields, read
","if read and read [ 0 ] != "","" :",173
"def _convertDict(self, d):
    r = {}
    for k, v in d.items():
        if isinstance(v, bytes):
            v = str(v, ""utf-8"")
        elif isinstance(v, list) or isinstance(v, tuple):
            v = self._convertList(v)
        elif isinstance(v, dict):
            v = self._convertDict(v)
        if isinstance(k, bytes):
            k = str(k, ""utf-8"")
        r[k] = v
    return r
","if isinstance ( v , bytes ) :",142
"def wrapper(filename):
    mtime = getmtime(filename)
    with lock:
        if filename in cache:
            old_mtime, result = cache.pop(filename)
            if old_mtime == mtime:
                # Move to the end
                cache[filename] = old_mtime, result
                return result
    result = function(filename)
    with lock:
        cache[filename] = mtime, result  # at the end
        if len(cache) > max_size:
            cache.popitem(last=False)
    return result
",if old_mtime == mtime :,144
"def isFinished(self):
    # returns true if episode timesteps has reached episode length and resets the task
    if self.count > self.epiLen:
        self.res()
        return True
    else:
        if self.count == 1:
            self.pertGlasPos(0)
        if self.count == self.epiLen / 2 + 1:
            self.env.reset()
            self.pertGlasPos(1)
        self.count += 1
        return False
",if self . count == 1 :,132
"def _check_vulnerabilities(self, processed_analysis):
    matched_vulnerabilities = list()
    for vulnerability in self._rule_base_vulnerabilities:
        if evaluate(processed_analysis, vulnerability.rule):
            vulnerability_data = vulnerability.get_dict()
            name = vulnerability_data.pop(""short_name"")
            matched_vulnerabilities.append((name, vulnerability_data))
    return matched_vulnerabilities
","if evaluate ( processed_analysis , vulnerability . rule ) :",100
"def _table_reprfunc(self, row, col, val):
    if self._table.column_names[col].endswith(""Size""):
        if isinstance(val, compat.string_types):
            return ""  %s"" % val
        elif val < 1024 ** 2:
            return ""  %.1f KB"" % (val / 1024.0 ** 1)
        elif val < 1024 ** 3:
            return ""  %.1f MB"" % (val / 1024.0 ** 2)
        else:
            return ""  %.1f GB"" % (val / 1024.0 ** 3)
    if col in (0, """"):
        return str(val)
    else:
        return ""  %s"" % val
","if isinstance ( val , compat . string_types ) :",182
"def serve_until_stopped(self) -> None:
    while True:
        rd, wr, ex = select.select([self.socket.fileno()], [], [], self.timeout)
        if rd:
            self.handle_request()
        if self.event is not None and self.event.is_set():
            break
",if self . event is not None and self . event . is_set ( ) :,83
"def resize(self, *e):
    bold = (""helvetica"", -self._size.get(), ""bold"")
    helv = (""helvetica"", -self._size.get())
    xspace = self._size.get()
    yspace = self._size.get()
    for widget in self._widgets:
        widget[""node_font""] = bold
        widget[""leaf_font""] = helv
        widget[""xspace""] = xspace
        widget[""yspace""] = yspace
        if self._size.get() < 20:
            widget[""line_width""] = 1
        elif self._size.get() < 30:
            widget[""line_width""] = 2
        else:
            widget[""line_width""] = 3
    self._layout()
",elif self . _size . get ( ) < 30 :,185
"def __assertTilesChangedInRegion(self, t1, t2, region):
    for tileOriginTuple in t1.keys():
        tileOrigin = imath.V2i(*tileOriginTuple)
        tileRegion = imath.Box2i(
            tileOrigin, tileOrigin + imath.V2i(GafferImage.ImagePlug.tileSize())
        )
        if GafferImage.BufferAlgo.intersects(tileRegion, region):
            self.assertNotEqual(t1[tileOriginTuple], t2[tileOriginTuple])
        else:
            self.assertEqual(t1[tileOriginTuple], t2[tileOriginTuple])
","if GafferImage . BufferAlgo . intersects ( tileRegion , region ) :",165
"def grouped_by_prefix(args, prefixes):
    """"""Group behave args by (directory) scope into multiple test-runs.""""""
    group_args = []
    current_scope = None
    for arg in args.strip().split():
        assert not arg.startswith(""-""), ""REQUIRE: arg, not options""
        scope = select_prefix_for(arg, prefixes)
        if scope != current_scope:
            if group_args:
                # -- DETECTED GROUP-END:
                yield "" "".join(group_args)
                group_args = []
            current_scope = scope
        group_args.append(arg)
    if group_args:
        yield "" "".join(group_args)
",if group_args :,183
"def __print__(self, defaults=False):
    if defaults:
        print_func = str
    else:
        print_func = repr
    pieces = []
    default_values = self.__defaults__
    for k in self.__fields__:
        value = getattr(self, k)
        if not defaults and value == default_values[k]:
            continue
        if isinstance(value, basestring):
            print_func = repr  # keep quotes around strings
        pieces.append(""%s=%s"" % (k, print_func(value)))
    if pieces or self.__base__:
        return ""%s(%s)"" % (self.__class__.__name__, "", "".join(pieces))
    else:
        return """"
",if not defaults and value == default_values [ k ] :,178
"def setInnerHTML(self, html):
    log.HTMLClassifier.classify(
        log.ThugLogging.url if log.ThugOpts.local else log.last_url, html
    )
    self.tag.clear()
    for node in bs4.BeautifulSoup(html, ""html.parser"").contents:
        self.tag.append(node)
        name = getattr(node, ""name"", None)
        if name is None:
            continue
        handler = getattr(log.DFT, ""handle_%s"" % (name,), None)
        if handler:
            handler(node)
",if name is None :,151
"def createFields(self):
    yield Enum(Bits(self, ""class"", 2), self.CLASS_DESC)
    yield Enum(Bit(self, ""form""), self.FORM_DESC)
    if self[""class""].value == 0:
        yield Enum(Bits(self, ""type"", 5), self.TYPE_DESC)
    else:
        yield Bits(self, ""type"", 5)
    yield ASNInteger(self, ""size"", ""Size in bytes"")
    size = self[""size""].value
    if size:
        if self._handler:
            for field in self._handler(self, size):
                yield field
        else:
            yield RawBytes(self, ""raw"", size)
",if self . _handler :,175
"def _process_service_request(self, pkttype, pktid, packet):
    """"""Process a service request""""""
    # pylint: disable=unused-argument
    service = packet.get_string()
    packet.check_end()
    if service == self._next_service:
        self.logger.debug2(""Accepting request for service %s"", service)
        self._next_service = None
        self.send_packet(MSG_SERVICE_ACCEPT, String(service))
        if self.is_server() and service == _USERAUTH_SERVICE:  # pragma: no branch
            self._auth_in_progress = True
            self._send_deferred_packets()
    else:
        raise DisconnectError(
            DISC_SERVICE_NOT_AVAILABLE, ""Unexpected service request received""
        )
",if self . is_server ( ) and service == _USERAUTH_SERVICE :,198
"def _read_fixed_body(
    self, content_length: int, delegate: httputil.HTTPMessageDelegate
) -> None:
    while content_length > 0:
        body = await self.stream.read_bytes(
            min(self.params.chunk_size, content_length), partial=True
        )
        content_length -= len(body)
        if not self._write_finished or self.is_client:
            with _ExceptionLoggingContext(app_log):
                ret = delegate.data_received(body)
                if ret is not None:
                    await ret
",if not self . _write_finished or self . is_client :,158
"def wait_for_child(pid, timeout=1.0):
    deadline = mitogen.core.now() + timeout
    while timeout < mitogen.core.now():
        try:
            target_pid, status = os.waitpid(pid, os.WNOHANG)
            if target_pid == pid:
                return
        except OSError:
            e = sys.exc_info()[1]
            if e.args[0] == errno.ECHILD:
                return
        time.sleep(0.05)
    assert False, ""wait_for_child() timed out""
",if target_pid == pid :,156
"def execute(cls, ctx, op: ""DataFrameGroupByAgg""):
    try:
        pd.set_option(""mode.use_inf_as_na"", op.use_inf_as_na)
        if op.stage == OperandStage.map:
            cls._execute_map(ctx, op)
        elif op.stage == OperandStage.combine:
            cls._execute_combine(ctx, op)
        elif op.stage == OperandStage.agg:
            cls._execute_agg(ctx, op)
        else:  # pragma: no cover
            raise ValueError(""Aggregation operand not executable"")
    finally:
        pd.reset_option(""mode.use_inf_as_na"")
",elif op . stage == OperandStage . combine :,171
"def cut(sentence):
    sentence = strdecode(sentence)
    blocks = re_han.split(sentence)
    for blk in blocks:
        if re_han.match(blk):
            for word in __cut(blk):
                if word not in Force_Split_Words:
                    yield word
                else:
                    for c in word:
                        yield c
        else:
            tmp = re_skip.split(blk)
            for x in tmp:
                if x:
                    yield x
",if word not in Force_Split_Words :,156
"def _iter_tags(self, type=None):
    """"""Yield all raw tags (limit to |type| if specified)""""""
    for n in itertools.count():
        tag = self._get_tag(n)
        if type is None or tag[""d_tag""] == type:
            yield tag
        if tag[""d_tag""] == ""DT_NULL"":
            break
","if type is None or tag [ ""d_tag"" ] == type :",95
"def reverse_search_history(self, searchfor, startpos=None):
    if startpos is None:
        startpos = self.history_cursor
    if _ignore_leading_spaces:
        res = [
            (idx, line.lstrip())
            for idx, line in enumerate(self.history[startpos:0:-1])
            if line.lstrip().startswith(searchfor.lstrip())
        ]
    else:
        res = [
            (idx, line)
            for idx, line in enumerate(self.history[startpos:0:-1])
            if line.startswith(searchfor)
        ]
    if res:
        self.history_cursor -= res[0][0]
        return res[0][1].get_line_text()
    return """"
",if line . startswith ( searchfor ),198
"def value_to_db_datetime(self, value):
    if value is None:
        return None
    # Oracle doesn't support tz-aware datetimes
    if timezone.is_aware(value):
        if settings.USE_TZ:
            value = value.astimezone(timezone.utc).replace(tzinfo=None)
        else:
            raise ValueError(
                ""Oracle backend does not support timezone-aware datetimes when USE_TZ is False.""
            )
    return unicode(value)
",if settings . USE_TZ :,130
"def _sniff(filename, oxlitype):
    try:
        with open(filename, ""rb"") as fileobj:
            header = fileobj.read(4)
            if header == b""OXLI"":
                fileobj.read(1)  # skip the version number
                ftype = fileobj.read(1)
                if binascii.hexlify(ftype) == oxlitype:
                    return True
        return False
    except OSError:
        return False
",if binascii . hexlify ( ftype ) == oxlitype :,126
"def unget(self, char):
    # Only one character is allowed to be ungotten at once - it must
    # be consumed again before any further call to unget
    if char is not EOF:
        if self.chunkOffset == 0:
            # unget is called quite rarely, so it's a good idea to do
            # more work here if it saves a bit of work in the frequently
            # called char and charsUntil.
            # So, just prepend the ungotten character onto the current
            # chunk:
            self.chunk = char + self.chunk
            self.chunkSize += 1
        else:
            self.chunkOffset -= 1
            assert self.chunk[self.chunkOffset] == char
",if self . chunkOffset == 0 :,188
"def scan(rule, extensions, paths, ignore_paths=None):
    """"""The libsast scan.""""""
    try:
        options = {
            ""match_rules"": rule,
            ""match_extensions"": extensions,
            ""ignore_paths"": ignore_paths,
            ""show_progress"": False,
        }
        scanner = Scanner(options, paths)
        res = scanner.scan()
        if res:
            return format_findings(res[""pattern_matcher""], paths[0])
    except Exception:
        logger.exception(""libsast scan"")
    return {}
",if res :,150
"def _getPatternTemplate(pattern, key=None):
    if key is None:
        key = pattern
        if ""%"" not in pattern:
            key = pattern.upper()
    template = DD_patternCache.get(key)
    if not template:
        if key in (""EPOCH"", ""{^LN-BEG}EPOCH"", ""^EPOCH""):
            template = DateEpoch(lineBeginOnly=(key != ""EPOCH""))
        elif key in (""TAI64N"", ""{^LN-BEG}TAI64N"", ""^TAI64N""):
            template = DateTai64n(wordBegin=(""start"" if key != ""TAI64N"" else False))
        else:
            template = DatePatternRegex(pattern)
    DD_patternCache.set(key, template)
    return template
","if ""%"" not in pattern :",195
"def _forward_response(self, src, dst):
    """"""Forward an SCP response between two remote SCP servers""""""
    # pylint: disable=no-self-use
    try:
        exc = yield from src.await_response()
        if exc:
            dst.send_error(exc)
            return exc
        else:
            dst.send_ok()
            return None
    except OSError as exc:
        return exc
",if exc :,114
"def _maybe_signal_recovery_end() -> None:
    if self.in_recovery and not self.active_remaining_total():
        # apply anything stuck in the buffers
        self.flush_buffers()
        self._set_recovery_ended()
        if self._actives_span is not None:
            self._actives_span.set_tag(""Actives-Ready"", True)
        self.signal_recovery_end.set()
",if self . _actives_span is not None :,115
"def main():
    tmpdir = None
    try:
        # Create a temporary working directory
        tmpdir = tempfile.mkdtemp()
        # Unpack the zipfile into the temporary directory
        pip_zip = os.path.join(tmpdir, ""pip.zip"")
        with open(pip_zip, ""wb"") as fp:
            fp.write(b85decode(DATA.replace(b""\n"", b"""")))
        # Add the zipfile to sys.path so that we can import it
        sys.path.insert(0, pip_zip)
        # Run the bootstrap
        bootstrap(tmpdir=tmpdir)
    finally:
        # Clean up our temporary working directory
        if tmpdir:
            shutil.rmtree(tmpdir, ignore_errors=True)
",if tmpdir :,185
"def __init__(self, api_version_str):
    try:
        self.latest = self.preview = False
        self.yyyy = self.mm = self.dd = None
        if api_version_str == ""latest"":
            self.latest = True
        else:
            if ""preview"" in api_version_str:
                self.preview = True
            parts = api_version_str.split(""-"")
            self.yyyy = int(parts[0])
            self.mm = int(parts[1])
            self.dd = int(parts[2])
    except (ValueError, TypeError):
        raise ValueError(
            ""The API version {} is not in a "" ""supported format"".format(api_version_str)
        )
","if ""preview"" in api_version_str :",199
"def _merge(self, items, map_id, dep_id, use_disk, meminfo, mem_limit):
    combined = self.combined
    merge_combiner = self.aggregator.mergeCombiners
    for k, v in items:
        o = combined.get(k)
        combined[k] = merge_combiner(o, v) if o is not None else v
        if use_disk and meminfo.rss > mem_limit:
            mem_limit = self._rotate()
",if use_disk and meminfo . rss > mem_limit :,120
"def TryMerge(self, d):
    while d.avail() > 0:
        tt = d.getVarInt32()
        if tt == 8:
            self.set_value(d.getVarInt32())
            continue
        if tt == 0:
            raise ProtocolBuffer.ProtocolBufferDecodeError
        d.skipData(tt)
",if tt == 8 :,90
"def nice(deltat):
    # singular,plural
    times = _(
        ""second,seconds:minute,minutes:hour,hours:day,days:week,weeks:month,months:year,years""
    ).split("":"")
    d = abs(int(deltat))
    for div, time in zip((60, 60, 24, 7, 4, 12, 100), times):
        if d < div * 5:
            return ""%s%i %s"" % (deltat < 0 and ""-"" or """", d, time.split("","")[d != 1])
        d /= div
",if d < div * 5 :,143
"def after_get_object(self, event, view_kwargs):
    if event and event.state == ""draft"":
        if not is_logged_in() or not has_access(""is_coorganizer"", event_id=event.id):
            raise ObjectNotFound({""parameter"": ""{id}""}, ""Event: not found"")
","if not is_logged_in ( ) or not has_access ( ""is_coorganizer"" , event_id = event . id ) :",79
"def daemonize_if_required(self):
    if self.options.daemon:
        if self._setup_mp_logging_listener_ is True:
            # Stop the logging queue listener for the current process
            # We'll restart it once forked
            log.shutdown_multiprocessing_logging_listener(daemonizing=True)
        # Late import so logging works correctly
        salt.utils.process.daemonize()
    # Setup the multiprocessing log queue listener if enabled
    self._setup_mp_logging_listener()
",if self . _setup_mp_logging_listener_ is True :,128
"def iter_modules(self, by_clients=False, clients_filter=None):
    """"""iterate over all modules""""""
    clients = None
    if by_clients:
        clients = self.get_clients(clients_filter)
        if not clients:
            return
    self._refresh_modules()
    for module_name in self.modules:
        try:
            module = self.get_module(module_name)
        except PupyModuleDisabled:
            continue
        if clients is not None:
            for client in clients:
                if module.is_compatible_with(client):
                    yield module
                    break
        else:
            yield module
",if module . is_compatible_with ( client ) :,181
"def _incremental_avg_dp(self, avg, new_el, idx):
    for attr in [""coarse_segm"", ""fine_segm"", ""u"", ""v""]:
        setattr(
            avg, attr, (getattr(avg, attr) * idx + getattr(new_el, attr)) / (idx + 1)
        )
        if idx:
            # Deletion of the > 0 index intermediary values to prevent GPU OOM
            setattr(new_el, attr, None)
    return avg
",if idx :,129
"def run(self, paths=[]):
    collapsed = False
    for item in SideBarSelection(paths).getSelectedDirectories():
        for view in item.views():
            if not collapsed:
                Window().focus_view(view)
                self.collapse_sidebar_folder()
                collapsed = True
            view.close()
",if not collapsed :,90
"def test_reductions(expr, rdd):
    result = compute(expr, rdd)
    expected = compute(expr, data)
    if not result == expected:
        print(result)
        print(expected)
        if isinstance(result, float):
            assert abs(result - expected) < 0.001
        else:
            assert result == expected
","if isinstance ( result , float ) :",93
"def deltask(task, d):
    if task[:3] != ""do_"":
        task = ""do_"" + task
    bbtasks = d.getVar(""__BBTASKS"", False) or []
    if task in bbtasks:
        bbtasks.remove(task)
        d.delVarFlag(task, ""task"")
        d.setVar(""__BBTASKS"", bbtasks)
    d.delVarFlag(task, ""deps"")
    for bbtask in d.getVar(""__BBTASKS"", False) or []:
        deps = d.getVarFlag(bbtask, ""deps"", False) or []
        if task in deps:
            deps.remove(task)
            d.setVarFlag(bbtask, ""deps"", deps)
",if task in deps :,184
"def _apply_weightnorm(self, list_layers):
    """"""Try apply weightnorm for all layer in list_layers.""""""
    for i in range(len(list_layers)):
        try:
            layer_name = list_layers[i].name.lower()
            if ""conv1d"" in layer_name or ""dense"" in layer_name:
                list_layers[i] = WeightNormalization(list_layers[i])
        except Exception:
            pass
","if ""conv1d"" in layer_name or ""dense"" in layer_name :",120
"def __init__(self, execution_context, aggregate_operators):
    super(_QueryExecutionAggregateEndpointComponent, self).__init__(execution_context)
    self._local_aggregators = []
    self._results = None
    self._result_index = 0
    for operator in aggregate_operators:
        if operator == ""Average"":
            self._local_aggregators.append(_AverageAggregator())
        elif operator == ""Count"":
            self._local_aggregators.append(_CountAggregator())
        elif operator == ""Max"":
            self._local_aggregators.append(_MaxAggregator())
        elif operator == ""Min"":
            self._local_aggregators.append(_MinAggregator())
        elif operator == ""Sum"":
            self._local_aggregators.append(_SumAggregator())
","elif operator == ""Max"" :",192
"def _conv_layer(self, sess, bottom, name, trainable=True, padding=""SAME"", relu=True):
    with tf.variable_scope(name) as scope:
        filt = self._get_conv_filter(sess, name, trainable=trainable)
        conv_biases = self._get_bias(sess, name, trainable=trainable)
        conv = tf.nn.conv2d(bottom, filt, [1, 1, 1, 1], padding=padding)
        bias = tf.nn.bias_add(conv, conv_biases)
        if relu:
            bias = tf.nn.relu(bias)
        return bias
",if relu :,159
"def get_partners(self) -> Dict[AbstractNode, Set[int]]:
    partners = {}  # type: Dict[AbstractNode, Set[int]]
    for edge in self.edges:
        if edge.is_dangling():
            raise ValueError(""Cannot contract copy tensor with dangling edges"")
        if self._is_my_trace(edge):
            continue
        partner_node, shared_axis = self._get_partner(edge)
        if partner_node not in partners:
            partners[partner_node] = set()
        partners[partner_node].add(shared_axis)
    return partners
",if self . _is_my_trace ( edge ) :,163
"def close(self):
    with self._lock:
        """"""Close this _MultiFileWatcher object forever.""""""
        if len(self._folder_handlers) != 0:
            self._folder_handlers = {}
            LOGGER.debug(
                ""Stopping observer thread even though there is a non-zero ""
                ""number of event observers!""
            )
        else:
            LOGGER.debug(""Stopping observer thread"")
        self._observer.stop()
        self._observer.join(timeout=5)
",if len ( self . _folder_handlers ) != 0 :,135
"def comboSelectionChanged(self, index):
    text = self.comboBox.cb.itemText(index)
    for i in range(self.labelList.count()):
        if text == """":
            self.labelList.item(i).setCheckState(2)
        elif text != self.labelList.item(i).text():
            self.labelList.item(i).setCheckState(0)
        else:
            self.labelList.item(i).setCheckState(2)
","if text == """" :",120
"def _get_messages(self):
    r = []
    try:
        self._connect()
        self._login()
        for message in self._fetch():
            if message:
                r.append(message)
        self._connection.expunge()
        self._connection.close()
        self._connection.logout()
    except MailFetcherError as e:
        self.log(""error"", str(e))
    return r
",if message :,114
"def get_current_user(self):
    try:
        if config.get(""development"") and config.get(""json_authentication_override""):
            return config.get(""json_authentication_override"")
        tkn_header = self.request.headers[""authorization""]
    except KeyError:
        raise WebAuthNError(reason=""Missing Authorization Header"")
    else:
        tkn_str = tkn_header.split("" "")[-1]
    try:
        tkn = self.jwt_validator(tkn_str)
    except AuthenticationError as e:
        raise WebAuthNError(reason=e.message)
    else:
        return tkn
","if config . get ( ""development"" ) and config . get ( ""json_authentication_override"" ) :",157
"def _get_data(self):
    formdata = self._formdata
    if formdata:
        data = []
        # TODO: Optimize?
        for item in formdata:
            model = self.loader.get_one(item) if item else None
            if model:
                data.append(model)
            else:
                self._invalid_formdata = True
        self._set_data(data)
    return self._data
",if model :,118
"def _getSubstrings(self, va, size, ltyp):
    # rip through the desired memory range to populate any substrings
    subs = set()
    end = va + size
    for offs in range(va, end, 1):
        loc = self.getLocation(offs, range=True)
        if loc and loc[L_LTYPE] == LOC_STRING and loc[L_VA] > va:
            subs.add((loc[L_VA], loc[L_SIZE]))
            if loc[L_TINFO]:
                subs = subs.union(set(loc[L_TINFO]))
    return list(subs)
",if loc [ L_TINFO ] :,161
"def monad(self):
    if not self.cls_bl_idname:
        return None
    for monad in bpy.data.node_groups:
        if hasattr(monad, ""cls_bl_idname""):
            if monad.cls_bl_idname == self.cls_bl_idname:
                return monad
    return None
","if hasattr ( monad , ""cls_bl_idname"" ) :",93
"def _set_peer_statuses(self):
    """"""Set peer statuses.""""""
    cutoff = time.time() - STALE_SECS
    for peer in self.peers:
        if peer.bad:
            peer.status = PEER_BAD
        elif peer.last_good > cutoff:
            peer.status = PEER_GOOD
        elif peer.last_good:
            peer.status = PEER_STALE
        else:
            peer.status = PEER_NEVER
",elif peer . last_good > cutoff :,128
"def title_by_index(self, trans, index, context):
    d_type = self.get_datatype(trans, context)
    for i, (composite_name, composite_file) in enumerate(d_type.writable_files.items()):
        if i == index:
            rval = composite_name
            if composite_file.description:
                rval = ""{} ({})"".format(rval, composite_file.description)
            if composite_file.optional:
                rval = ""%s [optional]"" % rval
            return rval
    if index < self.get_file_count(trans, context):
        return ""Extra primary file""
    return None
",if composite_file . description :,167
"def testUiViewServerDump_windowIntM1(self):
    device = None
    try:
        device = MockDevice(version=15, startviewserver=True)
        vc = ViewClient(device, device.serialno, adb=TRUE, autodump=False)
        vc.dump(window=-1)
        vc.findViewByIdOrRaise(""id/home"")
    finally:
        if device:
            device.shutdownMockViewServer()
",if device :,116
"def _convertDict(self, d):
    r = {}
    for k, v in d.items():
        if isinstance(v, bytes):
            v = str(v, ""utf-8"")
        elif isinstance(v, list) or isinstance(v, tuple):
            v = self._convertList(v)
        elif isinstance(v, dict):
            v = self._convertDict(v)
        if isinstance(k, bytes):
            k = str(k, ""utf-8"")
        r[k] = v
    return r
","elif isinstance ( v , dict ) :",142
"def _testSendmsgTimeout(self):
    try:
        self.cli_sock.settimeout(0.03)
        try:
            while True:
                self.sendmsgToServer([b""a"" * 512])
        except socket.timeout:
            pass
        except OSError as exc:
            if exc.errno != errno.ENOMEM:
                raise
            # bpo-33937 the test randomly fails on Travis CI with
            # ""OSError: [Errno 12] Cannot allocate memory""
        else:
            self.fail(""socket.timeout not raised"")
    finally:
        self.misc_event.set()
",if exc . errno != errno . ENOMEM :,170
"def addError(self, test, err):
    if err[0] is SkipTest:
        if self.showAll:
            self.stream.writeln(str(err[1]))
        elif self.dots:
            self.stream.write(""s"")
            self.stream.flush()
        return
    _org_AddError(self, test, err)
",elif self . dots :,93
"def mouse_down(self, event):
    if event.button == 1:
        if self.scrolling:
            p = event.local
            if self.scroll_up_rect().collidepoint(p):
                self.scroll_up()
                return
            elif self.scroll_down_rect().collidepoint(p):
                self.scroll_down()
                return
    if event.button == 4:
        self.scroll_up()
    if event.button == 5:
        self.scroll_down()
    GridView.mouse_down(self, event)
",elif self . scroll_down_rect ( ) . collidepoint ( p ) :,160
"def find_file_copyright_notices(fname):
    ret = set()
    f = open(fname)
    lines = f.readlines()
    for l in lines[:80]:  # hmmm, assume copyright to be in first 80 lines
        idx = l.lower().find(""copyright"")
        if idx < 0:
            continue
        copyright = l[idx + 9 :].strip()
        if not copyright:
            continue
        copyright = sanitise(copyright)
        # hmm, do a quick check to see if there's a year,
        # if not, skip it
        if not copyright.find(""200"") >= 0 and not copyright.find(""199"") >= 0:
            continue
        ret.add(copyright)
    return ret
",if not copyright :,186
"def get_selectable_values(self, request):
    shop = lfs.core.utils.get_default_shop(request)
    countries = []
    for country in shop.shipping_countries.all():
        if country in self.value.all():
            selected = True
        else:
            selected = False
        countries.append(
            {
                ""id"": country.id,
                ""name"": country.name,
                ""selected"": selected,
            }
        )
    return countries
",if country in self . value . all ( ) :,139
"def _addItemToLayout(self, sample, label):
    col = self.layout.columnCount()
    row = self.layout.rowCount()
    if row:
        row -= 1
    nCol = self.columnCount * 2
    # FIRST ROW FULL
    if col == nCol:
        for col in range(0, nCol, 2):
            # FIND RIGHT COLUMN
            if not self.layout.itemAt(row, col):
                break
        if col + 2 == nCol:
            # MAKE NEW ROW
            col = 0
            row += 1
    self.layout.addItem(sample, row, col)
    self.layout.addItem(label, row, col + 1)
",if col + 2 == nCol :,185
"def contains_only_whitespace(node):
    if is_tag(node):
        if not any([not is_text(s) for s in node.contents]):
            if not any([unicode(s).strip() for s in node.contents]):
                return True
    return False
",if not any ( [ unicode ( s ) . strip ( ) for s in node . contents ] ) :,72
"def tokenize_generator(cw):
    ret = []
    done = {}
    for op in ops:
        ch = op.symbol[0]
        if ch in done:
            continue
        sops = start_symbols[ch]
        cw.write(""case '%s':"" % ch)
        for t in gen_tests(sops, 1):
            cw.write(t)
        done[ch] = True
    return ret
",if ch in done :,113
"def _convertNbCharsInNbBits(self, nbChars):
    nbMinBit = None
    nbMaxBit = None
    if nbChars is not None:
        if isinstance(nbChars, int):
            nbMinBit = nbChars * 8
            nbMaxBit = nbMinBit
        else:
            if nbChars[0] is not None:
                nbMinBit = nbChars[0] * 8
            if nbChars[1] is not None:
                nbMaxBit = nbChars[1] * 8
    return (nbMinBit, nbMaxBit)
","if isinstance ( nbChars , int ) :",158
"def init(self, *args, **kwargs):
    if ""_state"" not in kwargs:
        state = {}
        # Older versions have the _state entries as individual kwargs
        for arg in (""children"", ""windowState"", ""detachedPanels""):
            if arg in kwargs:
                state[arg] = kwargs[arg]
                del kwargs[arg]
        if state:
            kwargs[""_state""] = state
    originalInit(self, *args, **kwargs)
",if state :,125
"def spm_decode(tokens: List[str]) -> List[str]:
    words = []
    pieces: List[str] = []
    for t in tokens:
        if t[0] == DecodeMixin.spm_bos_token:
            if len(pieces) > 0:
                words.append("""".join(pieces))
            pieces = [t[1:]]
        else:
            pieces.append(t)
    if len(pieces) > 0:
        words.append("""".join(pieces))
    return words
",if t [ 0 ] == DecodeMixin . spm_bos_token :,133
"def _compare_dirs(self, dir1: str, dir2: str) -> List[str]:
    # check that dir1 and dir2 are equivalent,
    # return the diff
    diff = []  # type: List[str]
    for root, dirs, files in os.walk(dir1):
        for file_ in files:
            path = os.path.join(root, file_)
            target_path = os.path.join(dir2, os.path.split(path)[-1])
            if not os.path.exists(target_path):
                diff.append(file_)
    return diff
",if not os . path . exists ( target_path ) :,155
"def credentials(self):
    """"""The session credentials as a dict""""""
    creds = {}
    if self._creds:
        if self._creds.access_key:  # pragma: no branch
            creds[""aws_access_key_id""] = self._creds.access_key
        if self._creds.secret_key:  # pragma: no branch
            creds[""aws_secret_access_key""] = self._creds.secret_key
        if self._creds.token:
            creds[""aws_session_token""] = self._creds.token
    if self._session.region_name:
        creds[""aws_region""] = self._session.region_name
    if self.requester_pays:
        creds[""aws_request_payer""] = ""requester""
    return creds
",if self . _creds . access_key :,194
"def got_arbiter_module_type_defined(self, mod_type):
    for a in self.arbiters:
        # Do like the linkify will do after....
        for m in getattr(a, ""modules"", []):
            # So look at what the arbiter try to call as module
            m = m.strip()
            # Ok, now look in modules...
            for mod in self.modules:
                # try to see if this module is the good type
                if getattr(mod, ""module_type"", """").strip() == mod_type.strip():
                    # if so, the good name?
                    if getattr(mod, ""module_name"", """").strip() == m:
                        return True
    return False
","if getattr ( mod , ""module_name"" , """" ) . strip ( ) == m :",199
"def find_file_at_path_with_indexes(self, path, url):
    if url.endswith(""/""):
        path = os.path.join(path, self.index_file)
        return self.get_static_file(path, url)
    elif url.endswith(""/"" + self.index_file):
        if os.path.isfile(path):
            return self.redirect(url, url[: -len(self.index_file)])
    else:
        try:
            return self.get_static_file(path, url)
        except IsDirectoryError:
            if os.path.isfile(os.path.join(path, self.index_file)):
                return self.redirect(url, url + ""/"")
    raise MissingFileError(path)
","if os . path . isfile ( os . path . join ( path , self . index_file ) ) :",193
"def _use_full_params(self) -> None:
    for p in self.params:
        if not p._is_sharded:
            if self.mixed_precision:
                assert p._fp16_shard.storage().size() != 0
                p.data = p._fp16_shard
        else:
            assert p._full_param_padded.storage().size() != 0
            p.data = p._full_param_padded[: p._orig_size.numel()].view(p._orig_size)
",if self . mixed_precision :,136
"def _attrdata(self, cont, name, *val):
    if not name:
        return None, False
    if isinstance(name, Mapping):
        if val:
            raise TypeError(""Cannot set a value to %s"" % name)
        return name, True
    else:
        if val:
            if len(val) == 1:
                return {name: val[0]}, True
            else:
                raise TypeError(""Too may arguments"")
        else:
            cont = self._extra.get(cont)
            return cont.get(name) if cont else None, False
",if len ( val ) == 1 :,158
"def evaluate(env, net, device=""cpu""):
    obs = env.reset()
    reward = 0.0
    steps = 0
    while True:
        obs_v = ptan.agent.default_states_preprocessor([obs]).to(device)
        action_v = net(obs_v)
        action = action_v.data.cpu().numpy()[0]
        obs, r, done, _ = env.step(action)
        reward += r
        steps += 1
        if done:
            break
    return reward, steps
",if done :,137
"def convert_html_js_files(app: Sphinx, config: Config) -> None:
    """"""This converts string styled html_js_files to tuple styled one.""""""
    html_js_files = []  # type: List[Tuple[str, Dict]]
    for entry in config.html_js_files:
        if isinstance(entry, str):
            html_js_files.append((entry, {}))
        else:
            try:
                filename, attrs = entry
                html_js_files.append((filename, attrs))
            except Exception:
                logger.warning(__(""invalid js_file: %r, ignored""), entry)
                continue
    config.html_js_files = html_js_files  # type: ignore
","if isinstance ( entry , str ) :",193
"def _check_duplications(self, regs):
    """"""n^2 loop which verifies that each reg exists only once.""""""
    for reg in regs:
        count = 0
        for r in regs:
            if reg == r:
                count += 1
        if count > 1:
            genutil.die(""reg %s defined more than once"" % reg)
",if reg == r :,94
"def PyJsHoisted_vault_(key, forget, this, arguments, var=var):
    var = Scope(
        {u""this"": this, u""forget"": forget, u""key"": key, u""arguments"": arguments}, var
    )
    var.registers([u""forget"", u""key""])
    if PyJsStrictEq(var.get(u""key""), var.get(u""passkey"")):
        return (
            var.put(u""secret"", var.get(u""null""))
            if var.get(u""forget"")
            else (
                var.get(u""secret"")
                or var.put(u""secret"", var.get(u""secretCreatorFn"")(var.get(u""object"")))
            )
        )
","if var . get ( u""forget"" )",198
"def sort_nested_dictionary_lists(d):
    for k, v in d.items():
        if isinstance(v, list):
            for i in range(0, len(v)):
                if isinstance(v[i], dict):
                    v[i] = await sort_nested_dictionary_lists(v[i])
                d[k] = sorted(v)
        if isinstance(v, dict):
            d[k] = await sort_nested_dictionary_lists(v)
    return d
","if isinstance ( v , dict ) :",134
"def transceiver(self, data):
    out = []
    for t in range(8):
        if data[t] == 0:
            continue
        value = data[t]
        for b in range(8):
            if value & 0x80:
                if len(TRANSCEIVER[t]) < b + 1:
                    out.append(""(unknown)"")
                else:
                    out.append(TRANSCEIVER[t][b])
            value <<= 1
    self.annotate(""Transceiver compliance"", "", "".join(out))
",if len ( TRANSCEIVER [ t ] ) < b + 1 :,155
"def process_string(self, remove_repetitions, sequence):
    string = """"
    for i, char in enumerate(sequence):
        if char != self.int_to_char[self.blank_index]:
            # if this char is a repetition and remove_repetitions=true,
            # skip.
            if remove_repetitions and i != 0 and char == sequence[i - 1]:
                pass
            elif char == self.labels[self.space_index]:
                string += "" ""
            else:
                string = string + char
    return string
",if remove_repetitions and i != 0 and char == sequence [ i - 1 ] :,152
"def clean(self):
    username = self.cleaned_data.get(""username"")
    password = self.cleaned_data.get(""password"")
    if username and password:
        self.user_cache = authenticate(username=username, password=password)
        if self.user_cache is None:
            raise forms.ValidationError(self.error_messages[""invalid_login""])
        elif not self.user_cache.is_active:
            raise forms.ValidationError(self.error_messages[""inactive""])
    self.check_for_test_cookie()
    return self.cleaned_data
",if self . user_cache is None :,143
"def is_listening_for_message(conversation_id: Text, endpoint: EndpointConfig) -> bool:
    """"""Check if the conversation is in need for a user message.""""""
    tracker = await retrieve_tracker(endpoint, conversation_id, EventVerbosity.APPLIED)
    for i, e in enumerate(reversed(tracker.get(""events"", []))):
        if e.get(""event"") == UserUttered.type_name:
            return False
        elif e.get(""event"") == ActionExecuted.type_name:
            return e.get(""name"") == ACTION_LISTEN_NAME
    return False
","elif e . get ( ""event"" ) == ActionExecuted . type_name :",154
"def getReferences(view, name=""""):
    """"""Find all reference definitions.""""""
    # returns {name -> Region}
    refs = []
    name = re.escape(name)
    if name == """":
        refs.extend(view.find_all(r""(?<=^\[)([^\]]+)(?=\]:)"", 0))
    else:
        refs.extend(view.find_all(r""(?<=^\[)(%s)(?=\]:)"" % name, 0))
    regions = refs
    ids = {}
    for reg in regions:
        name = view.substr(reg).strip()
        key = name.lower()
        if key in ids:
            ids[key].regions.append(reg)
        else:
            ids[key] = Obj(regions=[reg], label=name)
    return ids
",if key in ids :,199
"def _get_header(self, requester, header_name):
    hits = sum([header_name in headers for _, headers in requester.requests])
    self.assertEquals(hits, 2 if self.revs_enabled else 1)
    for url, headers in requester.requests:
        if header_name in headers:
            if self.revs_enabled:
                self.assertTrue(url.endswith(""/latest""), msg=url)
            else:
                self.assertTrue(url.endswith(""/download_urls""), msg=url)
            return headers.get(header_name)
",if header_name in headers :,143
"def TryMerge(self, d):
    while d.avail() > 0:
        tt = d.getVarInt32()
        if tt == 10:
            self.set_shuffle_name(d.getPrefixedString())
            continue
        if tt == 0:
            raise ProtocolBuffer.ProtocolBufferDecodeError
        d.skipData(tt)
",if tt == 10 :,92
"def make_release_tree(self, base_dir, files):
    """"""Make the release tree.""""""
    self.mkpath(base_dir)
    create_tree(base_dir, files, dry_run=self.dry_run)
    if not files:
        self.log.warning(""no files to distribute -- empty manifest?"")
    else:
        self.log.info(""copying files to %s..."", base_dir)
    for filename in files:
        if not os.path.isfile(filename):
            self.log.warning(""'%s' not a regular file -- skipping"", filename)
        else:
            dest = os.path.join(base_dir, filename)
            self.copy_file(filename, dest)
    self.distribution.metadata.write_pkg_info(base_dir)
",if not os . path . isfile ( filename ) :,199
"def _parse_names_set(feature_names):
    """"""Helping function of `_parse_feature_names` that parses a set of feature names.""""""
    feature_collection = OrderedDict()
    for feature_name in feature_names:
        if isinstance(feature_name, str):
            feature_collection[feature_name] = ...
        else:
            raise ValueError(""Failed to parse {}, expected string"".format(feature_name))
    return feature_collection
","if isinstance ( feature_name , str ) :",111
"def get_connection(self, url, proxies=None):
    with self.pools.lock:
        pool = self.pools.get(url)
        if pool:
            return pool
        pool = NpipeHTTPConnectionPool(
            self.npipe_path, self.timeout, maxsize=self.max_pool_size
        )
        self.pools[url] = pool
    return pool
",if pool :,102
"def _parse_dimensions(dimensions):
    arrays = []
    names = []
    for key in dimensions:
        values = [v[""name""] for v in key[""values""]]
        role = key.get(""role"", None)
        if role in (""time"", ""TIME_PERIOD""):
            values = [_fix_quarter_values(v) for v in values]
            values = pd.DatetimeIndex(values)
        arrays.append(values)
        names.append(key[""name""])
    midx = pd.MultiIndex.from_product(arrays, names=names)
    if len(arrays) == 1 and isinstance(midx, pd.MultiIndex):
        # Fix for pandas >= 0.21
        midx = midx.levels[0]
    return midx
","if role in ( ""time"" , ""TIME_PERIOD"" ) :",190
"def _add_trials(self, name, spec):
    """"""Add trial by invoking TrialRunner.""""""
    resource = {}
    resource[""trials""] = []
    trial_generator = BasicVariantGenerator()
    trial_generator.add_configurations({name: spec})
    while not trial_generator.is_finished():
        trial = trial_generator.next_trial()
        if not trial:
            break
        runner.add_trial(trial)
        resource[""trials""].append(self._trial_info(trial))
    return resource
",if not trial :,130
"def _retrieve_key(self):
    url = ""http://www.canadapost.ca/cpo/mc/personal/postalcode/fpc.jsf""
    text = """"
    try:
        r = requests.get(url, timeout=self.timeout, proxies=self.proxies)
        text = r.text
    except:
        self.error = ""ERROR - URL Connection""
    if text:
        expression = r""'(....-....-....-....)';""
        pattern = re.compile(expression)
        match = pattern.search(text)
        if match:
            self.key = match.group(1)
            return self.key
        else:
            self.error = ""ERROR - No API Key""
",if match :,190
"def test_net(net, env, count=10, device=""cpu""):
    rewards = 0.0
    steps = 0
    for _ in range(count):
        obs = env.reset()
        while True:
            obs_v = ptan.agent.float32_preprocessor([obs]).to(device)
            mu_v = net(obs_v)[0]
            action = mu_v.squeeze(dim=0).data.cpu().numpy()
            action = np.clip(action, -1, 1)
            obs, reward, done, _ = env.step(action)
            rewards += reward
            steps += 1
            if done:
                break
    return rewards / count, steps / count
",if done :,190
"def compile(self, filename, obfuscate=False, raw=False, magic=""\x00"" * 8):
    body = marshal.dumps(compile(self.visit(self._source_ast), filename, ""exec""))
    if obfuscate:
        body_len = len(body)
        offset = 0 if raw else 8
        output = bytearray(body_len + 8)
        for i, x in enumerate(body):
            output[i + offset] = ord(x) ^ ((2 ** ((65535 - i) % 65535)) % 251)
        if raw:
            for i in xrange(8):
                output[i] = 0
        return output
    elif raw:
        return body
    else:
        return magic + body
",if raw :,188
"def _map_saslprep(s):
    """"""Map stringprep table B.1 to nothing and C.1.2 to ASCII space""""""
    r = []
    for c in s:
        if stringprep.in_table_c12(c):
            r.append("" "")
        elif not stringprep.in_table_b1(c):
            r.append(c)
    return """".join(r)
",elif not stringprep . in_table_b1 ( c ) :,106
"def ensemble(self, pairs, other_preds):
    """"""Ensemble the dict with statistical model predictions.""""""
    lemmas = []
    assert len(pairs) == len(other_preds)
    for p, pred in zip(pairs, other_preds):
        w, pos = p
        if (w, pos) in self.composite_dict:
            lemma = self.composite_dict[(w, pos)]
        elif w in self.word_dict:
            lemma = self.word_dict[w]
        else:
            lemma = pred
        if lemma is None:
            lemma = w
        lemmas.append(lemma)
    return lemmas
",elif w in self . word_dict :,164
"def quiet_f(*args):
    vars = {arg_name: Real(arg) for arg_name, arg in zip(arg_names, args)}
    value = dynamic_scoping(quiet_expr.evaluate, vars, evaluation)
    if expect_list:
        if value.has_form(""List"", None):
            value = [extract_pyreal(item) for item in value.leaves]
            if any(item is None for item in value):
                return None
            return value
        else:
            return None
    else:
        value = extract_pyreal(value)
        if value is None or isinf(value) or isnan(value):
            return None
        return value
","if value . has_form ( ""List"" , None ) :",177
"def _copy_package_apps(
    local_bin_dir: Path, app_paths: List[Path], suffix: str = """"
) -> None:
    for src_unresolved in app_paths:
        src = src_unresolved.resolve()
        app = src.name
        dest = Path(local_bin_dir / add_suffix(app, suffix))
        if not dest.parent.is_dir():
            mkdir(dest.parent)
        if dest.exists():
            logger.warning(f""{hazard}  Overwriting file {str(dest)} with {str(src)}"")
            dest.unlink()
        if src.exists():
            shutil.copy(src, dest)
",if dest . exists ( ) :,177
"def assert_readback(vehicle, values):
    i = 10
    while i > 0:
        time.sleep(0.1)
        i -= 0.1
        for k, v in values.items():
            if vehicle.channels[k] != v:
                continue
        break
    if i <= 0:
        raise Exception(""Did not match in channels readback %s"" % values)
",if vehicle . channels [ k ] != v :,105
"def _get_linode_client(self):
    api_key = self.credentials.conf(""key"")
    api_version = self.credentials.conf(""version"")
    if api_version == """":
        api_version = None
    if not api_version:
        api_version = 3
        # Match for v4 api key
        regex_v4 = re.compile(""^[0-9a-f]{64}$"")
        regex_match = regex_v4.match(api_key)
        if regex_match:
            api_version = 4
    else:
        api_version = int(api_version)
    return _LinodeLexiconClient(api_key, api_version)
",if regex_match :,175
"def mergeHiLo(self, x_stats):
    """"""Merge the highs and lows of another accumulator into myself.""""""
    if x_stats.firsttime is not None:
        if self.firsttime is None or x_stats.firsttime < self.firsttime:
            self.firsttime = x_stats.firsttime
            self.first = x_stats.first
    if x_stats.lasttime is not None:
        if self.lasttime is None or x_stats.lasttime >= self.lasttime:
            self.lasttime = x_stats.lasttime
            self.last = x_stats.last
",if self . firsttime is None or x_stats . firsttime < self . firsttime :,157
"def _check_good_input(self, X, y=None):
    if isinstance(X, dict):
        lengths = [len(X1) for X1 in X.values()]
        if len(set(lengths)) > 1:
            raise ValueError(""Not all values of X are of equal length."")
        x_len = lengths[0]
    else:
        x_len = len(X)
    if y is not None:
        if len(y) != x_len:
            raise ValueError(""X and y are not of equal length."")
    if self.regression and y is not None and y.ndim == 1:
        y = y.reshape(-1, 1)
    return X, y
",if len ( set ( lengths ) ) > 1 :,175
"def set(self, obj, **kwargs):
    """"""Check for missing event functions and substitute these with""""""
    """"""the ignore method""""""
    ignore = getattr(self, ""ignore"")
    for k, v in kwargs.iteritems():
        setattr(self, k, getattr(obj, v))
        if k in self.combinations:
            for k1 in self.combinations[k]:
                if not hasattr(self, k1):
                    setattr(self, k1, ignore)
","if not hasattr ( self , k1 ) :",121
"def _parse_list(self, tokens):
    # Process left to right, allow descending in sub lists
    assert tokens[0] in (""["", ""("")
    delim = ""]"" if tokens.pop(0) == ""["" else "")""
    expr = ExpressionList()
    while tokens and tokens[0] != delim:
        item = self._parse(tokens)
        if tokens and tokens[0] != delim:
            if tokens.pop(0) != "","":
                raise ExpressionSyntaxError('Expected: "",""')
        expr.append(item)
    if not tokens or tokens[0] != delim:
        raise ExpressionSyntaxError('Missing: ""%s""' % delim)
    else:
        tokens.pop(0)
    return expr
",if tokens and tokens [ 0 ] != delim :,174
"def param_value(self):
    # This is part of the ""handle quoted extended parameters"" hack.
    for token in self:
        if token.token_type == ""value"":
            return token.stripped_value
        if token.token_type == ""quoted-string"":
            for token in token:
                if token.token_type == ""bare-quoted-string"":
                    for token in token:
                        if token.token_type == ""value"":
                            return token.stripped_value
    return """"
","if token . token_type == ""quoted-string"" :",143
"def paragraph_is_fully_commented(lines, comment, main_language):
    """"""Is the paragraph fully commented?""""""
    for i, line in enumerate(lines):
        if line.startswith(comment):
            if line[len(comment) :].lstrip().startswith(comment):
                continue
            if is_magic(line, main_language):
                return False
            continue
        return i > 0 and _BLANK_LINE.match(line)
    return True
",if line . startswith ( comment ) :,121
"def lots_connected_to_existing_roads(model):
    set = []
    for h in model.HarvestCells:
        for (i, j) in model.ExistingRoads:
            if (i in model.COriginNodeForCell[h]) or (j in model.COriginNodeForCell[h]):
                if h not in set:
                    set.append(h)
    return set
",if ( i in model . COriginNodeForCell [ h ] ) or ( j in model . COriginNodeForCell [ h ] ) :,108
"def detect(get_page):
    retval = False
    for vector in WAF_ATTACK_VECTORS:
        page, headers, code = get_page(get=vector)
        retval = (
            re.search(
                r""\Abarra_counter_session="",
                headers.get(HTTP_HEADER.SET_COOKIE, """"),
                re.I,
            )
            is not None
        )
        retval |= (
            re.search(
                r""(\A|\b)barracuda_"", headers.get(HTTP_HEADER.SET_COOKIE, """"), re.I
            )
            is not None
        )
        if retval:
            break
    return retval
",if retval :,194
"def test_files(self):
    # get names of files to test
    dist_dir = os.path.join(os.path.dirname(__file__), os.pardir, os.pardir)
    names = []
    for d in self.test_directories:
        test_dir = os.path.join(dist_dir, d)
        for n in os.listdir(test_dir):
            if n.endswith("".py"") and not n.startswith(""bad""):
                names.append(os.path.join(test_dir, n))
    for filename in names:
        if test_support.verbose:
            print(""Testing %s"" % filename)
        source = read_pyfile(filename)
        self.check_roundtrip(source)
","if n . endswith ( "".py"" ) and not n . startswith ( ""bad"" ) :",186
"def test_calibrate_target(create_target):
    mod, params = testing.synthetic.get_workload()
    dataset = get_calibration_dataset(mod, ""data"")
    with relay.quantize.qconfig(calibrate_mode=""kl_divergence""):
        if create_target:
            with tvm.target.Target(""llvm""):
                relay.quantize.quantize(mod, params, dataset)
        else:
            # current_target = None
            relay.quantize.quantize(mod, params, dataset)
",if create_target :,133
"def _cleanSubmodule(self, _=None):
    rc = RC_SUCCESS
    if self.submodules:
        command = [
            ""submodule"",
            ""foreach"",
            ""--recursive"",
            ""git"",
            ""clean"",
            ""-f"",
            ""-f"",
            ""-d"",
        ]
        if self.mode == ""full"" and self.method == ""fresh"":
            command.append(""-x"")
        rc = yield self._dovccmd(command)
    defer.returnValue(rc)
","if self . mode == ""full"" and self . method == ""fresh"" :",147
"def screen_length_to_bytes_count(string, screen_length_limit, encoding):
    bytes_count = 0
    screen_length = 0
    for unicode_char in string:
        screen_length += screen_len(unicode_char)
        char_bytes_count = len(unicode_char.encode(encoding))
        bytes_count += char_bytes_count
        if screen_length > screen_length_limit:
            bytes_count -= char_bytes_count
            break
    return bytes_count
",if screen_length > screen_length_limit :,129
"def tamper(payload, **kwargs):
    junk_chars = ""!#$%&()*~+-_.,:;?@[/|\]^`""
    retval = """"
    for i, char in enumerate(payload, start=1):
        amount = random.randint(10, 15)
        if char == "">"":
            retval += "">""
            for _ in range(amount):
                retval += random.choice(junk_chars)
        elif char == ""<"":
            retval += ""<""
            for _ in range(amount):
                retval += random.choice(junk_chars)
        elif char == "" "":
            for _ in range(amount):
                retval += random.choice(junk_chars)
        else:
            retval += char
    return retval
","elif char == ""<"" :",200
"def test_parse(self):
    correct = 0
    for example in EXAMPLES:
        try:
            schema.parse(example.schema_string)
            if example.valid:
                correct += 1
            else:
                self.fail(""Invalid schema was parsed: "" + example.schema_string)
        except:
            if not example.valid:
                correct += 1
            else:
                self.fail(""Valid schema failed to parse: "" + example.schema_string)
    fail_msg = ""Parse behavior correct on %d out of %d schemas."" % (
        correct,
        len(EXAMPLES),
    )
    self.assertEqual(correct, len(EXAMPLES), fail_msg)
",if not example . valid :,188
"def _on_change(self):
    changed = False
    self.save()
    for key, value in self.data.items():
        if isinstance(value, bool):
            if value:
                changed = True
                break
        if isinstance(value, int):
            if value != 1:
                changed = True
                break
        elif value is None:
            continue
        elif len(value) != 0:
            changed = True
            break
    self._reset_button.disabled = not changed
","if isinstance ( value , bool ) :",145
"def normalize(d: Dict[Any, Any]) -> Dict[str, Any]:
    first_exception = None
    for normalizer in normalizers:
        try:
            normalized = normalizer(d)
        except KeyError as e:
            if not first_exception:
                first_exception = e
        else:
            return normalized
    assert first_exception is not None
    raise first_exception
",if not first_exception :,103
"def gather_callback_args(self, obj, callbacks):
    session = sa.orm.object_session(obj)
    for callback in callbacks:
        backref = callback.backref
        root_objs = getdotattr(obj, backref) if backref else obj
        if root_objs:
            if not isinstance(root_objs, Iterable):
                root_objs = [root_objs]
            with session.no_autoflush:
                for root_obj in root_objs:
                    if root_obj:
                        args = self.get_callback_args(root_obj, callback)
                        if args:
                            yield args
",if root_obj :,182
"def test_opdm_to_oqdm(self):
    for file in filter(lambda x: x.endswith("".hdf5""), os.listdir(DATA_DIRECTORY)):
        molecule = MolecularData(filename=os.path.join(DATA_DIRECTORY, file))
        if molecule.fci_one_rdm is not None:
            test_oqdm = map_one_pdm_to_one_hole_dm(molecule.fci_one_rdm)
            true_oqdm = numpy.eye(molecule.n_qubits) - molecule.fci_one_rdm
            assert numpy.allclose(test_oqdm, true_oqdm)
",if molecule . fci_one_rdm is not None :,171
"def emitSubDomainData(self, subDomainData, event):
    self.emitRawRirData(subDomainData, event)
    for subDomainElem in subDomainData:
        if self.checkForStop():
            return None
        subDomain = subDomainElem.get(""subdomain"", """").strip()
        if subDomain:
            self.emitHostname(subDomain, event)
",if subDomain :,99
"def download_cve(
    download_path: str, years: Optional[List[int]] = None, update: bool = False
):
    if update:
        process_url(CVE_URL.format(""modified""), download_path)
    else:
        all_cve_urls = get_cve_links(CVE_URL, years)
        if not all_cve_urls:
            raise CveLookupException(""Error: No CVE links found"")
        for url in all_cve_urls:
            process_url(url, download_path)
",if not all_cve_urls :,142
"def is_special(s, i, directive):
    """"""Return True if the body text contains the @ directive.""""""
    # j = skip_line(s,i) ; trace(s[i:j],':',directive)
    assert directive and directive[0] == ""@""
    # 10/23/02: all directives except @others must start the line.
    skip_flag = directive in (""@others"", ""@all"")
    while i < len(s):
        if match_word(s, i, directive):
            return True, i
        else:
            i = skip_line(s, i)
            if skip_flag:
                i = skip_ws(s, i)
    return False, -1
",if skip_flag :,178
"def run_async(self, nuke_cursors):
    # type: (bool) -> None
    interface_type = self.view.settings().get(""git_savvy.interface"")
    for cls in subclasses:
        if cls.interface_type == interface_type:
            vid = self.view.id()
            interface = interfaces.get(vid, None)
            if not interface:
                interface = interfaces[vid] = cls(view=self.view)
            interface.render(nuke_cursors=nuke_cursors)  # type: ignore[union-attr]
            break
",if not interface :,155
"def scan_resource_conf(self, conf):
    if ""properties"" in conf:
        if ""sslEnforcement"" in conf[""properties""]:
            if str(conf[""properties""][""sslEnforcement""]).lower() == ""enabled"":
                return CheckResult.PASSED
    return CheckResult.FAILED
","if ""sslEnforcement"" in conf [ ""properties"" ] :",77
"def do_shorts(
    opts: List[Tuple[str, str]], optstring: str, shortopts: str, args: List[str]
) -> Tuple[List[Tuple[str, str]], List[str]]:
    while optstring != """":
        opt, optstring = optstring[0], optstring[1:]
        if short_has_arg(opt, shortopts):
            if optstring == """":
                if not args:
                    raise GetoptError(""option -%s requires argument"" % opt, opt)
                optstring, args = args[0], args[1:]
            optarg, optstring = optstring, """"
        else:
            optarg = """"
        opts.append((""-"" + opt, optarg))
    return opts, args
",if not args :,183
"def release(self):
    tid = _thread.get_ident()
    with self.lock:
        if self.owner != tid:
            raise RuntimeError(""cannot release un-acquired lock"")
        assert self.count > 0
        self.count -= 1
        if self.count == 0:
            self.owner = None
            if self.waiters:
                self.waiters -= 1
                self.wakeup.release()
",if self . owner != tid :,117
"def _summarize_kraken(fn):
    """"""get the value at species level""""""
    kraken = {}
    list_sp, list_value = [], []
    with open(fn) as handle:
        for line in handle:
            cols = line.strip().split(""\t"")
            sp = cols[5].strip()
            if len(sp.split("" "")) > 1 and not sp.startswith(""cellular""):
                list_sp.append(sp)
                list_value.append(cols[0])
    kraken = {""kraken_sp"": list_sp, ""kraken_value"": list_value}
    return kraken
","if len ( sp . split ( "" "" ) ) > 1 and not sp . startswith ( ""cellular"" ) :",159
"def _sync_remote_run(remote_run):
    assert remote_run.remote
    remote_name = remote_run.remote.name
    pull_args = click_util.Args(remote=remote_name, delete=False)
    try:
        remote_impl_support.pull_runs([remote_run], pull_args)
    except Exception as e:
        if log.getEffectiveLevel() <= logging.DEBUG:
            log.exception(""pull %s from %s"", remote_run.id, remote_name)
        else:
            log.error(""error pulling %s from %s: %s"", remote_run.id, remote_name, e)
",if log . getEffectiveLevel ( ) <= logging . DEBUG :,163
"def group_by_sign(seq, slop=sin(pi / 18), key=lambda x: x):
    sign = None
    subseq = []
    for i in seq:
        ki = key(i)
        if sign is None:
            subseq.append(i)
            if ki != 0:
                sign = ki / abs(ki)
        else:
            subseq.append(i)
            if sign * ki < -slop:
                sign = ki / abs(ki)
                yield subseq
                subseq = [i]
    if subseq:
        yield subseq
",if sign is None :,167
"def import_til(self):
    log(""Importing type libraries..."")
    cur = self.db_cursor()
    sql = ""select name from diff.program_data where type = 'til'""
    cur.execute(sql)
    for row in cur.fetchall():
        til = row[""name""]
        if type(til) is bytes:
            til = til.decode(""utf-8"")
        try:
            add_default_til(til)
        except:
            log(""Error loading til %s: %s"" % (row[""name""], str(sys.exc_info()[1])))
    cur.close()
    auto_wait()
",if type ( til ) is bytes :,168
"def getBranches(self):
    returned = []
    for git_branch_line in self._executeGitCommandAssertSuccess(""branch"").stdout:
        if git_branch_line.startswith(""*""):
            git_branch_line = git_branch_line[1:]
        git_branch_line = git_branch_line.strip()
        if BRANCH_ALIAS_MARKER in git_branch_line:
            alias_name, aliased = git_branch_line.split(BRANCH_ALIAS_MARKER)
            returned.append(branch.LocalBranchAlias(self, alias_name, aliased))
        else:
            returned.append(branch.LocalBranch(self, git_branch_line))
    return returned
","if git_branch_line . startswith ( ""*"" ) :",178
"def add_include_dirs(self, args):
    ids = []
    for a in args:
        # FIXME same hack, forcibly unpack from holder.
        if hasattr(a, ""includedirs""):
            a = a.includedirs
        if not isinstance(a, IncludeDirs):
            raise InvalidArguments(
                ""Include directory to be added is not an include directory object.""
            )
        ids.append(a)
    self.include_dirs += ids
","if hasattr ( a , ""includedirs"" ) :",120
"def _serialize_feature(self, feature):
    name = feature.unique_name()
    if name not in self._features_dict:
        self._features_dict[feature.unique_name()] = feature.to_dictionary()
        for dependency in feature.get_dependencies(deep=True):
            name = dependency.unique_name()
            if name not in self._features_dict:
                self._features_dict[name] = dependency.to_dictionary()
",if name not in self . _features_dict :,117
"def generate_io(chart_type, race_configs, environment):
    # output JSON structures
    structures = []
    for race_config in race_configs:
        if ""io"" in race_config.charts:
            title = chart_type.format_title(
                environment,
                race_config.track,
                es_license=race_config.es_license,
                suffix=""%s-io"" % race_config.label,
            )
            structures.append(chart_type.io(title, environment, race_config))
    return structures
","if ""io"" in race_config . charts :",150
"def format_partition(partition, partition_schema):
    tokens = []
    if isinstance(partition, dict):
        for name in partition_schema:
            if name in partition:
                tok = _format_partition_kv(
                    name, partition[name], partition_schema[name]
                )
            else:
                # dynamic partitioning
                tok = name
            tokens.append(tok)
    else:
        for name, value in zip(partition_schema, partition):
            tok = _format_partition_kv(name, value, partition_schema[name])
            tokens.append(tok)
    return ""PARTITION ({})"".format("", "".join(tokens))
",if name in partition :,183
"def to_dict(self, validate=True, ignore=(), context=None):
    context = context or {}
    condition = getattr(self, ""condition"", Undefined)
    copy = self  # don't copy unless we need to
    if condition is not Undefined:
        if isinstance(condition, core.SchemaBase):
            pass
        elif ""field"" in condition and ""type"" not in condition:
            kwds = parse_shorthand(condition[""field""], context.get(""data"", None))
            copy = self.copy(deep=[""condition""])
            copy.condition.update(kwds)
    return super(ValueChannelMixin, copy).to_dict(
        validate=validate, ignore=ignore, context=context
    )
","elif ""field"" in condition and ""type"" not in condition :",175
"def _checkForCommand(self):
    prompt = b""cftp> ""
    if self._expectingCommand and self._lineBuffer == prompt:
        buf = b""\n"".join(self._linesReceived)
        if buf.startswith(prompt):
            buf = buf[len(prompt) :]
        self.clearBuffer()
        d, self._expectingCommand = self._expectingCommand, None
        d.callback(buf)
",if buf . startswith ( prompt ) :,109
"def schedule_logger(job_id=None, delete=False):
    if not job_id:
        return getLogger(""fate_flow_schedule"")
    else:
        if delete:
            with LoggerFactory.lock:
                try:
                    for key in LoggerFactory.schedule_logger_dict.keys():
                        if job_id in key:
                            del LoggerFactory.schedule_logger_dict[key]
                except:
                    pass
            return True
        key = job_id + ""schedule""
        if key in LoggerFactory.schedule_logger_dict:
            return LoggerFactory.schedule_logger_dict[key]
        return LoggerFactory.get_schedule_logger(job_id)
",if key in LoggerFactory . schedule_logger_dict :,198
"def halfMultipartScore(nzb_name):
    try:
        wrong_found = 0
        for nr in [1, 2, 3, 4, 5, ""i"", ""ii"", ""iii"", ""iv"", ""v"", ""a"", ""b"", ""c"", ""d"", ""e""]:
            for wrong in [""cd"", ""part"", ""dis"", ""disc"", ""dvd""]:
                if ""%s%s"" % (wrong, nr) in nzb_name.lower():
                    wrong_found += 1
        if wrong_found == 1:
            return -30
        return 0
    except:
        log.error(""Failed doing halfMultipartScore: %s"", traceback.format_exc())
    return 0
",if wrong_found == 1 :,183
"def parse_converter_args(argstr: str) -> t.Tuple[t.Tuple, t.Dict[str, t.Any]]:
    argstr += "",""
    args = []
    kwargs = {}
    for item in _converter_args_re.finditer(argstr):
        value = item.group(""stringval"")
        if value is None:
            value = item.group(""value"")
        value = _pythonize(value)
        if not item.group(""name""):
            args.append(value)
        else:
            name = item.group(""name"")
            kwargs[name] = value
    return tuple(args), kwargs
","if not item . group ( ""name"" ) :",164
"def leaves(self, unique=True):
    """"""Get the leaves of the tree starting at this root.""""""
    if not self.children:
        return [self]
    else:
        res = list()
        for child in self.children:
            for sub_child in child.leaves(unique=unique):
                if not unique or sub_child not in res:
                    res.append(sub_child)
        return res
",if not unique or sub_child not in res :,112
"def to_tree(self, tagname=None, idx=None, namespace=None):
    axIds = set((ax.axId for ax in self._axes))
    for chart in self._charts:
        for id, axis in chart._axes.items():
            if id not in axIds:
                setattr(self, axis.tagname, axis)
                axIds.add(id)
    return super(PlotArea, self).to_tree(tagname)
",if id not in axIds :,116
"def update_neighbor(neigh_ip_address, changes):
    rets = []
    for k, v in changes.items():
        if k == neighbors.MULTI_EXIT_DISC:
            rets.append(_update_med(neigh_ip_address, v))
        if k == neighbors.ENABLED:
            rets.append(update_neighbor_enabled(neigh_ip_address, v))
        if k == neighbors.CONNECT_MODE:
            rets.append(_update_connect_mode(neigh_ip_address, v))
    return all(rets)
",if k == neighbors . CONNECT_MODE :,138
"def close_all_connections():
    global _managers, _lock, _in_use, _timer
    _lock.acquire()
    try:
        if _timer:
            _timer.cancel()
            _timer = None
        for domain, managers in _managers.items():
            for manager in managers:
                manager.close()
        _managers = {}
    finally:
        _lock.release()
",if _timer :,109
"def _instrument_model(self, model):
    for key, value in list(
        model.__dict__.items()
    ):  # avoid ""dictionary keys changed during iteration""
        if isinstance(value, tf.keras.layers.Layer):
            new_layer = self._instrument(value)
            if new_layer is not value:
                setattr(model, key, new_layer)
        elif isinstance(value, list):
            for i, item in enumerate(value):
                if isinstance(item, tf.keras.layers.Layer):
                    value[i] = self._instrument(item)
    return model
","if isinstance ( item , tf . keras . layers . Layer ) :",164
"def target_glob(tgt, hosts):
    ret = {}
    for host in hosts:
        if fnmatch.fnmatch(tgt, host):
            ret[host] = copy.deepcopy(__opts__.get(""roster_defaults"", {}))
            ret[host].update({""host"": host})
            if __opts__.get(""ssh_user""):
                ret[host].update({""user"": __opts__[""ssh_user""]})
    return ret
","if __opts__ . get ( ""ssh_user"" ) :",110
"def write(self, data):
    if mock_target._mirror_on_stderr:
        if self._write_line:
            sys.stderr.write(fn + "": "")
        if bytes:
            sys.stderr.write(data.decode(""utf8""))
        else:
            sys.stderr.write(data)
        if (data[-1]) == ""\n"":
            self._write_line = True
        else:
            self._write_line = False
    super(Buffer, self).write(data)
",if bytes :,137
"def task_thread():
    while not task_queue.empty():
        host, port, username, password = task_queue.get()
        logger.info(
            ""try burst {}:{} use username:{} password:{}"".format(
                host, port, username, password
            )
        )
        if telnet_login(host, port, username, password):
            with task_queue.mutex:
                task_queue.queue.clear()
            result_queue.put((username, password))
","if telnet_login ( host , port , username , password ) :",135
"def _format_results(name, ppl, scores, metrics):
    """"""Format results.""""""
    result_str = """"
    if ppl:
        result_str = ""%s ppl %.2f"" % (name, ppl)
    if scores:
        for metric in metrics:
            if result_str:
                result_str += "", %s %s %.1f"" % (name, metric, scores[metric])
            else:
                result_str = ""%s %s %.1f"" % (name, metric, scores[metric])
    return result_str
",if result_str :,142
"def info_query(self, query):
    """"""Send a query which only returns 1 row""""""
    self._cmysql.query(query)
    first_row = ()
    if self._cmysql.have_result_set:
        first_row = self._cmysql.fetch_row()
        if self._cmysql.fetch_row():
            self._cmysql.free_result()
            raise errors.InterfaceError(""Query should not return more than 1 row"")
    self._cmysql.free_result()
    return first_row
",if self . _cmysql . fetch_row ( ) :,131
"def reset_class(self):
    for f in self.fields_order:
        if f.strbits and isbin(f.strbits):
            f.value = int(f.strbits, 2)
        elif ""default_val"" in f.kargs:
            f.value = int(f.kargs[""default_val""], 2)
        else:
            f.value = None
        if f.fname:
            setattr(self, f.fname, f)
",if f . strbits and isbin ( f . strbits ) :,123
"def _walk_map_list(self, access_func):
    seen = []
    cur = self
    while cur:
        if cur.obj_offset in seen:
            break
        yield cur
        seen.append(cur.obj_offset)
        # check for signs of infinite looping
        if len(seen) > 1024:
            break
        cur = access_func(cur)
",if cur . obj_offset in seen :,102
"def bgdel():
    q = bgdelq
    while True:
        name = q.get()
        while os.path.exists(name):
            try:
                if os.path.isfile(name):
                    os.remove(name)
                else:
                    shutil.rmtree(name)
            except:
                pass
            if os.path.exists(name):
                time.sleep(0.1)
",if os . path . isfile ( name ) :,127
"def _find_all_variables(transfer_variable):
    d = {}
    for _k, _v in transfer_variable.__dict__.items():
        if isinstance(_v, Variable):
            d[_v._name] = _v
        elif isinstance(_v, BaseTransferVariables):
            d.update(_find_all_variables(_v))
    return d
","elif isinstance ( _v , BaseTransferVariables ) :",91
"def set_val():
    idx = 0
    for idx in range(0, len(model)):
        row = model[idx]
        if value and row[0] == value:
            break
        if idx == len(os_widget.get_model()) - 1:
            idx = -1
    os_widget.set_active(idx)
    if idx == -1:
        os_widget.set_active(0)
    if idx >= 0:
        return row[1]
    if self.show_all_os:
        return None
",if value and row [ 0 ] == value :,142
"def _make_cache_key(group, window, rate, value, methods):
    count, period = _split_rate(rate)
    safe_rate = ""%d/%ds"" % (count, period)
    parts = [group, safe_rate, value, str(window)]
    if methods is not None:
        if methods == ALL:
            methods = """"
        elif isinstance(methods, (list, tuple)):
            methods = """".join(sorted([m.upper() for m in methods]))
        parts.append(methods)
    prefix = getattr(settings, ""RATELIMIT_CACHE_PREFIX"", ""rl:"")
    return prefix + hashlib.md5(u"""".join(parts).encode(""utf-8"")).hexdigest()
","elif isinstance ( methods , ( list , tuple ) ) :",175
"def findfiles(path):
    files = []
    for name in os.listdir(path):
        # ignore hidden files/dirs and other unwanted files
        if name.startswith(""."") or name == ""lastsnap.jpg"":
            continue
        pathname = os.path.join(path, name)
        st = os.lstat(pathname)
        mode = st.st_mode
        if stat.S_ISDIR(mode):
            files.extend(findfiles(pathname))
        elif stat.S_ISREG(mode):
            files.append((pathname, name, st))
    return files
","if name . startswith ( ""."" ) or name == ""lastsnap.jpg"" :",150
"def __getitem__(self, key):
    if isinstance(key, str_types):
        keys = self.get_keys()
        if key not in keys:
            raise KeyError(' ""{0}"" is an invalid key'.format(key))
        else:
            return self[keys.index(key)]
    else:
        return list.__getitem__(self, key)
",if key not in keys :,93
"def test_assert_set_equal(estimate: tp.Iterable[int], message: str) -> None:
    reference = {1, 2, 3}
    try:
        testing.assert_set_equal(estimate, reference)
    except AssertionError as error:
        if not message:
            raise AssertionError(
                ""An error has been raised while it should not.""
            ) from error
        np.testing.assert_equal(error.args[0].split(""\n"")[1:], message)
    else:
        if message:
            raise AssertionError(""An error should have been raised."")
",if message :,148
"def get_directory_info(prefix, pth, recursive):
    res = []
    directory = os.listdir(pth)
    directory.sort()
    for p in directory:
        if p[0] != ""."":
            subp = os.path.join(pth, p)
            p = os.path.join(prefix, p)
            if recursive and os.path.isdir(subp):
                res.append([p, get_directory_info(prefix, subp, 1)])
            else:
                res.append([p, None])
    return res
","if p [ 0 ] != ""."" :",148
"def check(self, runner, script, info):
    if isinstance(info, ast.FunctionDef):
        for arg in info.args.args:
            if isinstance(arg, ast.Name):
                if arg.id in script.modelVars:
                    self.problem(
                        ""Function {0} may shadow model variable {1}"".format(
                            info.name, arg.id
                        ),
                        lineno=info.lineno,
                    )
","if isinstance ( arg , ast . Name ) :",137
"def db_lookup(field, key, publish_year=None):
    sql = ""select sum(ebook_count) as num from subjects where field=$field and key=$key""
    if publish_year:
        if isinstance(publish_year, (tuple, list)):
            sql += "" and publish_year between $y1 and $y2""
            (y1, y2) = publish_year
        else:
            sql += "" and publish_year=$publish_year""
    return list(ebook_count_db.query(sql, vars=locals()))[0].num
","if isinstance ( publish_year , ( tuple , list ) ) :",141
"def put(self, session):
    with sess_lock:
        self.parent.put(session)
        # Do not store the session if skip paths
        for sp in self.skip_paths:
            if request.path.startswith(sp):
                return
        if session.sid in self._cache:
            try:
                del self._cache[session.sid]
            except Exception:
                pass
        self._cache[session.sid] = session
    self._normalize()
",if request . path . startswith ( sp ) :,133
"def summarize(self):
    if self.bad_commit and self.good_commit:
        for subresult in self.subresults.values():
            sub = subresult.summarize()
            if sub:
                return sub
        return ""Detected bad commit in {} repository:\n{} {}"".format(
            self.repo_name, self.bad_commit, get_message(self.suite, self.bad_commit)
        )
    return """"
",if sub :,115
"def compute_nullable_nonterminals(self):
    nullable = {}
    num_nullable = 0
    while 1:
        for p in self.grammar.Productions[1:]:
            if p.len == 0:
                nullable[p.name] = 1
                continue
            for t in p.prod:
                if not t in nullable:
                    break
            else:
                nullable[p.name] = 1
        if len(nullable) == num_nullable:
            break
        num_nullable = len(nullable)
    return nullable
",if len ( nullable ) == num_nullable :,153
"def _cast_float64_to_float32(self, feeds):
    for input_name, input_type in self.inputs:
        if input_type == ""tensor(float)"":
            feed = feeds.get(input_name)
            if feed is not None and feed.dtype == np.float64:
                feeds[input_name] = feed.astype(np.float32)
    return feeds
","if input_type == ""tensor(float)"" :",103
"def proc_minute(d):
    if expanded[0][0] != ""*"":
        diff_min = nearest_diff_method(d.minute, expanded[0], 60)
        if diff_min is not None and diff_min != 0:
            if is_prev:
                d += relativedelta(minutes=diff_min, second=59)
            else:
                d += relativedelta(minutes=diff_min, second=0)
            return True, d
    return False, d
",if diff_min is not None and diff_min != 0 :,128
"def detype(self):
    if self._detyped is not None:
        return self._detyped
    ctx = {}
    for key, val in self._d.items():
        if not isinstance(key, str):
            key = str(key)
        detyper = self.get_detyper(key)
        if detyper is None:
            # cannot be detyped
            continue
        deval = detyper(val)
        if deval is None:
            # cannot be detyped
            continue
        ctx[key] = deval
    self._detyped = ctx
    return ctx
",if deval is None :,163
"def get_or_create_user(request, user_data):
    try:
        user = User.objects.get(sso_id=user_data[""id""])
        if user_needs_updating(user, user_data):
            update_user(user, user_data)
        return user
    except User.DoesNotExist:
        user = User.objects.create_user(
            user_data[""username""],
            user_data[""email""],
            is_active=user_data.get(""is_active"", True),
            sso_id=user_data[""id""],
        )
        user.update_acl_key()
        setup_new_user(request.settings, user)
        return user
","if user_needs_updating ( user , user_data ) :",185
"def _populate_tree(self, element, d):
    """"""Populates an etree with attributes & elements, given a dict.""""""
    for k, v in d.iteritems():
        if isinstance(v, dict):
            self._populate_dict(element, k, v)
        elif isinstance(v, list):
            self._populate_list(element, k, v)
        elif isinstance(v, bool):
            self._populate_bool(element, k, v)
        elif isinstance(v, basestring):
            self._populate_str(element, k, v)
        elif type(v) in [int, float, long, complex]:
            self._populate_number(element, k, v)
","elif isinstance ( v , bool ) :",178
"def load(cls):
    if not cls._loaded:
        cls.log.debug(""Loading action_sets..."")
        if not horizons.globals.fife.use_atlases:
            cls._find_action_sets(PATHS.ACTION_SETS_DIRECTORY)
        else:
            cls.action_sets = JsonDecoder.load(PATHS.ACTION_SETS_JSON_FILE)
        cls.log.debug(""Done!"")
        cls._loaded = True
",if not horizons . globals . fife . use_atlases :,118
"def Resolve(self, updater=None):
    if len(self.Conflicts):
        for setting, edge in self.Conflicts:
            answer = self.AskUser(self.Setting, setting)
            if answer == Gtk.ResponseType.YES:
                value = setting.Value.split(""|"")
                value.remove(edge)
                setting.Value = ""|"".join(value)
                if updater:
                    updater.UpdateSetting(setting)
            if answer == Gtk.ResponseType.NO:
                return False
    return True
",if answer == Gtk . ResponseType . YES :,146
"def read_tsv(input_file, quotechar=None):
    """"""Reads a tab separated value file.""""""
    with open(input_file, ""r"", encoding=""utf-8-sig"") as f:
        reader = csv.reader(f, delimiter=""\t"", quotechar=quotechar)
        lines = []
        for line in reader:
            if sys.version_info[0] == 2:
                line = list(str(cell, ""utf-8"") for cell in line)  # noqa: F821
            lines.append(line)
        return lines
",if sys . version_info [ 0 ] == 2 :,140
"def devd_devfs_hook(middleware, data):
    if data.get(""subsystem"") != ""CDEV"":
        return
    if data[""type""] == ""CREATE"":
        disks = await middleware.run_in_thread(
            lambda: sysctl.filter(""kern.disks"")[0].value.split()
        )
        # Device notified about is not a disk
        if data[""cdev""] not in disks:
            return
        await added_disk(middleware, data[""cdev""])
    elif data[""type""] == ""DESTROY"":
        # Device notified about is not a disk
        if not RE_ISDISK.match(data[""cdev""]):
            return
        await remove_disk(middleware, data[""cdev""])
","if not RE_ISDISK . match ( data [ ""cdev"" ] ) :",190
"def on_edit_button_clicked(self, event=None, a=None, col=None):
    tree, tree_id = self.treeView.get_selection().get_selected()
    watchdir_id = str(self.store.get_value(tree_id, 0))
    if watchdir_id:
        if col and col.get_title() == _(""Active""):
            if self.watchdirs[watchdir_id][""enabled""]:
                client.autoadd.disable_watchdir(watchdir_id)
            else:
                client.autoadd.enable_watchdir(watchdir_id)
        else:
            self.opts_dialog.show(self.watchdirs[watchdir_id], watchdir_id)
","if col and col . get_title ( ) == _ ( ""Active"" ) :",187
"def _execute(self, options, args):
    if len(args) < 1:
        raise CommandError(_(""Not enough arguments""))
    paths = args
    songs = [self.load_song(p) for p in paths]
    for song in songs:
        if not song.can_change_images:
            raise CommandError(
                _(""Image editing not supported for %(file_name)s "" ""(%(file_format)s)"")
                % {""file_name"": song(""~filename""), ""file_format"": song(""~format"")}
            )
    for song in songs:
        try:
            song.clear_images()
        except AudioFileError as e:
            raise CommandError(e)
",if not song . can_change_images :,176
"def filter_pricing_rule_based_on_condition(pricing_rules, doc=None):
    filtered_pricing_rules = []
    if doc:
        for pricing_rule in pricing_rules:
            if pricing_rule.condition:
                try:
                    if frappe.safe_eval(pricing_rule.condition, None, doc.as_dict()):
                        filtered_pricing_rules.append(pricing_rule)
                except:
                    pass
            else:
                filtered_pricing_rules.append(pricing_rule)
    else:
        filtered_pricing_rules = pricing_rules
    return filtered_pricing_rules
",if pricing_rule . condition :,179
"def ProcessStringLiteral(self):
    if self._lastToken == None or self._lastToken.type == self.OpenBrace:
        text = super(JavaScriptBaseLexer, self).text
        if text == '""use strict""' or text == ""'use strict'"":
            if len(self._scopeStrictModes) > 0:
                self._scopeStrictModes.pop()
            self._useStrictCurrent = True
            self._scopeStrictModes.append(self._useStrictCurrent)
",if len ( self . _scopeStrictModes ) > 0 :,124
"def _find_remote_inputs(metadata):
    out = []
    for fr_key in metadata.keys():
        if isinstance(fr_key, (list, tuple)):
            frs = fr_key
        else:
            frs = [fr_key]
        for fr in frs:
            if objectstore.is_remote(fr):
                out.append(fr)
    return out
",if objectstore . is_remote ( fr ) :,107
"def sub_paragraph(self, li):
    """"""Search for checkbox in sub-paragraph.""""""
    found = False
    if len(li):
        first = list(li)[0]
        if first.tag == ""p"" and first.text is not None:
            m = RE_CHECKBOX.match(first.text)
            if m is not None:
                first.text = self.markdown.htmlStash.store(
                    get_checkbox(m.group(""state"")), safe=True
                ) + m.group(""line"")
                found = True
    return found
",if m is not None :,152
"def list_files(basedir):
    """"""List files in the directory rooted at |basedir|.""""""
    if not os.path.isdir(basedir):
        raise NoSuchDirectory(basedir)
    directories = [""""]
    while directories:
        d = directories.pop()
        for basename in os.listdir(os.path.join(basedir, d)):
            filename = os.path.join(d, basename)
            if os.path.isdir(os.path.join(basedir, filename)):
                directories.append(filename)
            elif os.path.exists(os.path.join(basedir, filename)):
                yield filename
","if os . path . isdir ( os . path . join ( basedir , filename ) ) :",159
"def TryMerge(self, d):
    while d.avail() > 0:
        tt = d.getVarInt32()
        if tt == 10:
            self.set_version(d.getPrefixedString())
            continue
        if tt == 0:
            raise ProtocolBuffer.ProtocolBufferDecodeError
        d.skipData(tt)
",if tt == 0 :,90
"def _dump(self, fd):
    with self.no_unpicklable_properties():
        if self.__module__ == ""__main__"":
            d = pickle.dumps(self)
            module_name = os.path.basename(sys.argv[0]).rsplit(""."", 1)[0]
            d = d.replace(b""c__main__"", b""c"" + module_name.encode(""ascii""))
            fd.write(d)
        else:
            pickle.dump(self, fd)
","if self . __module__ == ""__main__"" :",128
"def assert_session_stack(classes):
    assert len(_SklearnTrainingSession._session_stack) == len(classes)
    for idx, (sess, (parent_clazz, clazz)) in enumerate(
        zip(_SklearnTrainingSession._session_stack, classes)
    ):
        assert sess.clazz == clazz
        if idx == 0:
            assert sess._parent is None
        else:
            assert sess._parent.clazz == parent_clazz
",if idx == 0 :,118
"def native_color(c):
    try:
        color = CACHE[c]
    except KeyError:
        if isinstance(c, str):
            c = NAMED_COLOR[c]
        color = Color.FromArgb(
            int(c.rgba.a * 255), int(c.rgba.r), int(c.rgba.g), int(c.rgba.b)
        )
        CACHE[c] = color
    return color
","if isinstance ( c , str ) :",115
"def callback(name):
    # XXX: move into Action
    for neighbor_name in reactor.configuration.neighbors.keys():
        neighbor = reactor.configuration.neighbors.get(neighbor_name, None)
        if not neighbor:
            continue
        neighbor.rib.outgoing.announce_watchdog(name)
        yield False
    reactor.processes.answer_done(service)
",if not neighbor :,97
"def token_producer(source):
    token = source.read_uint8()
    while token is not None:
        if is_push_data_token(token):
            yield DataToken(read_data(token, source))
        elif is_small_integer(token):
            yield SmallIntegerToken(read_small_integer(token))
        else:
            yield Token(token)
        token = source.read_uint8()
",if is_push_data_token ( token ) :,113
"def setattr(self, req, ino, attr, to_set, fi):
    print(""setattr:"", ino, to_set)
    a = self.attr[ino]
    for key in to_set:
        if key == ""st_mode"":
            # Keep the old file type bit fields
            a[""st_mode""] = S_IFMT(a[""st_mode""]) | S_IMODE(attr[""st_mode""])
        else:
            a[key] = attr[key]
    self.attr[ino] = a
    self.reply_attr(req, a, 1.0)
","if key == ""st_mode"" :",149
"def check_enum_exports(module, eq_callback, only=None):
    """"""Make sure module exports all mnemonics from enums""""""
    for attr in enumerate_module(module, enum.Enum):
        if only is not None and attr not in only:
            print(""SKIP"", attr)
            continue
        for flag, value in attr.__members__.items():
            print(module, flag, value)
            eq_callback(getattr(module, flag), value)
",if only is not None and attr not in only :,118
"def remove_edit_vars_to(self, n):
    try:
        removals = []
        for v, cei in self.edit_var_map.items():
            if cei.index >= n:
                removals.append(v)
        for v in removals:
            self.remove_edit_var(v)
        assert len(self.edit_var_map) == n
    except ConstraintNotFound:
        raise InternalError(""Constraint not found during internal removal"")
",if cei . index >= n :,129
"def fix_repeating_arguments(self):
    """"""Fix elements that should accumulate/increment values.""""""
    either = [list(child.children) for child in transform(self).children]
    for case in either:
        for e in [child for child in case if case.count(child) > 1]:
            if type(e) is Argument or type(e) is Option and e.argcount:
                if e.value is None:
                    e.value = []
                elif type(e.value) is not list:
                    e.value = e.value.split()
            if type(e) is Command or type(e) is Option and e.argcount == 0:
                e.value = 0
    return self
",if e . value is None :,190
"def add_I_prefix(current_line: List[str], ner: int, tag: str):
    for i in range(0, len(current_line)):
        if i == 0:
            f.write(line_list[i])
        elif i == ner:
            f.write("" I-"" + tag)
        else:
            f.write("" "" + current_line[i])
    f.write(""\n"")
",elif i == ner :,111
"def select_word_at_cursor(self):
    word_region = None
    selection = self.view.sel()
    for region in selection:
        word_region = self.view.word(region)
        if not word_region.empty():
            selection.clear()
            selection.add(word_region)
            return word_region
    return word_region
",if not word_region . empty ( ) :,96
"def calc(self, arg):
    op = arg[""op""]
    if op == ""C"":
        self.clear()
        return str(self.current)
    num = decimal.Decimal(arg[""num""])
    if self.op:
        if self.op == ""+"":
            self.current += num
        elif self.op == ""-"":
            self.current -= num
        elif self.op == ""*"":
            self.current *= num
        elif self.op == ""/"":
            self.current /= num
        self.op = op
    else:
        self.op = op
        self.current = num
    res = str(self.current)
    if op == ""="":
        self.clear()
    return res
","elif self . op == ""-"" :",187
"def strip_pod(lines):
    in_pod = False
    stripped_lines = []
    for line in lines:
        if re.match(r""^=(?:end|cut)"", line):
            in_pod = False
        elif re.match(r""^=\w+"", line):
            in_pod = True
        elif not in_pod:
            stripped_lines.append(line)
    return stripped_lines
",elif not in_pod :,108
"def __init__(self, patch_files, patch_directories):
    files = []
    files_data = {}
    for filename_data in patch_files:
        if isinstance(filename_data, list):
            filename, data = filename_data
        else:
            filename = filename_data
            data = None
        if not filename.startswith(os.sep):
            filename = ""{0}{1}"".format(FakeState.deploy_dir, filename)
        files.append(filename)
        if data:
            files_data[filename] = data
    self.files = files
    self.files_data = files_data
    self.directories = patch_directories
",if data :,171
"def loadPerfsFromModule(self, module):
    """"""Return a suite of all perfs cases contained in the given module""""""
    perfs = []
    for name in dir(module):
        obj = getattr(module, name)
        if type(obj) == types.ClassType and issubclass(obj, PerfCase):
            perfs.append(self.loadPerfsFromPerfCase(obj))
    return self.suiteClass(perfs)
","if type ( obj ) == types . ClassType and issubclass ( obj , PerfCase ) :",108
"def download_subtitle(self, subtitle):
    if isinstance(subtitle, XSubsSubtitle):
        # download the subtitle
        logger.info(""Downloading subtitle %r"", subtitle)
        r = self.session.get(
            subtitle.download_link, headers={""Referer"": subtitle.page_link}, timeout=10
        )
        r.raise_for_status()
        if not r.content:
            logger.debug(""Unable to download subtitle. No data returned from provider"")
            return
        subtitle.content = fix_line_ending(r.content)
",if not r . content :,145
"def get_inlaws(self, person):
    inlaws = []
    family_handles = person.get_family_handle_list()
    for handle in family_handles:
        fam = self.database.get_family_from_handle(handle)
        if fam.father_handle and not fam.father_handle == person.handle:
            inlaws.append(self.database.get_person_from_handle(fam.father_handle))
        elif fam.mother_handle and not fam.mother_handle == person.handle:
            inlaws.append(self.database.get_person_from_handle(fam.mother_handle))
    return inlaws
",elif fam . mother_handle and not fam . mother_handle == person . handle :,180
"def _check_xorg_conf():
    if is_there_a_default_xorg_conf_file():
        print(
            ""WARNING : Found a Xorg config file at /etc/X11/xorg.conf. If you did not""
            "" create it yourself, it was likely generated by your distribution or by an Nvidia utility.\n""
            ""This file may contain hard-coded GPU configuration that could interfere with optimus-manager,""
            "" so it is recommended that you delete it before proceeding.\n""
            ""Ignore this warning and proceed with GPU switching ? (y/N)""
        )
        confirmation = ask_confirmation()
        if not confirmation:
            sys.exit(0)
",if not confirmation :,178
"def _make_cache_key(group, window, rate, value, methods):
    count, period = _split_rate(rate)
    safe_rate = ""%d/%ds"" % (count, period)
    parts = [group, safe_rate, value, str(window)]
    if methods is not None:
        if methods == ALL:
            methods = """"
        elif isinstance(methods, (list, tuple)):
            methods = """".join(sorted([m.upper() for m in methods]))
        parts.append(methods)
    prefix = getattr(settings, ""RATELIMIT_CACHE_PREFIX"", ""rl:"")
    return prefix + hashlib.md5(u"""".join(parts).encode(""utf-8"")).hexdigest()
",if methods == ALL :,175
"def num_of_mapped_volumes(self, initiator):
    cnt = 0
    for lm_link in self.req(""lun-maps"")[""lun-maps""]:
        idx = lm_link[""href""].split(""/"")[-1]
        # NOTE(geguileo): There can be races so mapped elements retrieved
        # in the listing may no longer exist.
        try:
            lm = self.req(""lun-maps"", idx=int(idx))[""content""]
        except exception.NotFound:
            continue
        if lm[""ig-name""] == initiator:
            cnt += 1
    return cnt
","if lm [ ""ig-name"" ] == initiator :",157
"def _setAbsoluteY(self, value):
    if value is None:
        self._absoluteY = None
    else:
        if value == ""above"":
            value = 10
        elif value == ""below"":
            value = -70
        try:
            value = common.numToIntOrFloat(value)
        except ValueError as ve:
            raise TextFormatException(
                f""Not a supported absoluteY position: {value!r}""
            ) from ve
        self._absoluteY = value
","if value == ""above"" :",137
"def render_markdown(text):
    users = {u.username.lower(): u for u in get_mention_users(text)}
    parts = MENTION_RE.split(text)
    for pos, part in enumerate(parts):
        if not part.startswith(""@""):
            continue
        username = part[1:].lower()
        if username in users:
            user = users[username]
            parts[pos] = '**[{}]({} ""{}"")**'.format(
                part, user.get_absolute_url(), user.get_visible_name()
            )
    text = """".join(parts)
    return mark_safe(MARKDOWN(text))
",if username in users :,168
"def start_process(self):
    with self.thread_lock:
        if self.allow_process_request:
            self.allow_process_request = False
            t = threading.Thread(target=self.__start)
            t.daemon = True
            t.start()
",if self . allow_process_request :,75
"def close(self):
    if self._fh.closed:
        return
    self._fh.close()
    if os.path.isfile(self._filename):
        if salt.utils.win_dacl.HAS_WIN32:
            salt.utils.win_dacl.copy_security(
                source=self._filename, target=self._tmp_filename
            )
        else:
            shutil.copymode(self._filename, self._tmp_filename)
            st = os.stat(self._filename)
            os.chown(self._tmp_filename, st.st_uid, st.st_gid)
    atomic_rename(self._tmp_filename, self._filename)
",if salt . utils . win_dacl . HAS_WIN32 :,179
"def _splitSchemaNameDotFieldName(sn_fn, fnRequired=True):
    if sn_fn.find(""."") != -1:
        schemaName, fieldName = sn_fn.split(""."", 1)
        schemaName = schemaName.strip()
        fieldName = fieldName.strip()
        if schemaName and fieldName:
            return (schemaName, fieldName)
    elif not fnRequired:
        schemaName = sn_fn.strip()
        if schemaName:
            return (schemaName, None)
    controlflow.system_error_exit(
        2, f""{sn_fn} is not a valid custom schema.field name.""
    )
",if schemaName :,164
"def modified(self):
    paths = set()
    dictionary_list = []
    for op_list in self._operations:
        if not isinstance(op_list, list):
            op_list = (op_list,)
        for item in chain(*op_list):
            if item is None:
                continue
            dictionary = item.dictionary
            if dictionary.path in paths:
                continue
            paths.add(dictionary.path)
            dictionary_list.append(dictionary)
    return dictionary_list
","if not isinstance ( op_list , list ) :",139
"def apply(self, db, person):
    for family_handle in person.get_family_handle_list():
        family = db.get_family_from_handle(family_handle)
        if family:
            for event_ref in family.get_event_ref_list():
                if event_ref:
                    event = db.get_event_from_handle(event_ref.ref)
                    if not event.get_place_handle():
                        return True
                    if not event.get_date_object():
                        return True
    return False
",if family :,159
"def test_cleanup_params(self, body, rpc_mock):
    res = self._get_resp_post(body)
    self.assertEqual(http_client.ACCEPTED, res.status_code)
    rpc_mock.assert_called_once_with(self.context, mock.ANY)
    cleanup_request = rpc_mock.call_args[0][1]
    for key, value in body.items():
        if key in (""disabled"", ""is_up""):
            if value is not None:
                value = value == ""true""
        self.assertEqual(value, getattr(cleanup_request, key))
    self.assertEqual(self._expected_services(*SERVICES), res.json)
","if key in ( ""disabled"" , ""is_up"" ) :",177
"def get_billable_and_total_duration(activity, start_time, end_time):
    precision = frappe.get_precision(""Timesheet Detail"", ""hours"")
    activity_duration = time_diff_in_hours(end_time, start_time)
    billing_duration = 0.0
    if activity.billable:
        billing_duration = activity.billing_hours
        if activity_duration != activity.billing_hours:
            billing_duration = (
                activity_duration * activity.billing_hours / activity.hours
            )
    return flt(activity_duration, precision), flt(billing_duration, precision)
",if activity_duration != activity . billing_hours :,167
"def cpus(self):
    try:
        cpus = (
            self.inspect[""Spec""][""Resources""][""Reservations""][""NanoCPUs""] / 1000000000.0
        )
        if cpus == int(cpus):
            cpus = int(cpus)
        return cpus
    except TypeError:
        return None
    except KeyError:
        return 0
",if cpus == int ( cpus ) :,92
"def _create_object(self, obj_body):
    props = obj_body[SYMBOL_PROPERTIES]
    for prop_name, prop_value in props.items():
        if isinstance(prop_value, dict) and prop_value:
            # get the first key as the convert function
            func_name = list(prop_value.keys())[0]
            if func_name.startswith(""_""):
                func = getattr(self, func_name)
                props[prop_name] = func(prop_value[func_name])
    if SYMBOL_TYPE in obj_body and obj_body[SYMBOL_TYPE] in self.fake_func_mapping:
        return self.fake_func_mapping[obj_body[SYMBOL_TYPE]](**props)
    else:
        return props
","if isinstance ( prop_value , dict ) and prop_value :",199
"def _yield_unescaped(self, string):
    while ""\\"" in string:
        finder = EscapeFinder(string)
        yield finder.before + finder.backslashes
        if finder.escaped and finder.text:
            yield self._unescape(finder.text)
        else:
            yield finder.text
        string = finder.after
    yield string
",if finder . escaped and finder . text :,91
"def _check_matches(rule, matches):
    errors = 0
    for match in matches:
        filematch = _match_to_test_file(match)
        if not filematch.exists():
            utils.error(
                ""The match '{}' for rule '{}' points to a non existing test module path: {}"",
                match,
                rule,
                filematch,
            )
            errors += 1
    return errors
",if not filematch . exists ( ) :,118
"def focused_windows():
    tree = i3.get_tree()
    workspaces = tree.workspaces()
    for workspace in workspaces:
        container = workspace
        while container:
            if not hasattr(container, ""focus"") or not container.focus:
                break
            container_id = container.focus[0]
            container = container.find_by_id(container_id)
        if container:
            coname = container.name
            wsname = workspace.name
            print(""WS"", wsname + "":"", coname)
",if container :,146
"def normals(self, value):
    if value is not None:
        value = np.asanyarray(value, dtype=np.float32)
        value = np.ascontiguousarray(value)
        if value.shape != self.positions.shape:
            raise ValueError(""Incorrect normals shape"")
    self._normals = value
",if value . shape != self . positions . shape :,77
"def test_hexdigest(self):
    for cons in self.hash_constructors:
        h = cons()
        if h.name in self.shakes:
            self.assertIsInstance(h.digest(16), bytes)
            self.assertEqual(hexstr(h.digest(16)), h.hexdigest(16))
        else:
            self.assertIsInstance(h.digest(), bytes)
            self.assertEqual(hexstr(h.digest()), h.hexdigest())
",if h . name in self . shakes :,117
"def _get_cluster_status(self):
    try:
        return (
            self.dataproc_client.projects()
            .regions()
            .clusters()
            .get(
                projectId=self.gcloud_project_id,
                region=self.dataproc_region,
                clusterName=self.dataproc_cluster_name,
                fields=""status"",
            )
            .execute()
        )
    except HttpError as e:
        if e.resp.status == 404:
            return None  # We got a 404 so the cluster doesn't exist
        else:
            raise e
",if e . resp . status == 404 :,175
"def _items_from(self, context):
    self._context = context
    if self._is_local_variable(self._keyword_name, context):
        for item in self._items_from_controller(context):
            yield item
    else:
        for df in context.datafiles:
            self._yield_for_other_threads()
            if self._items_from_datafile_should_be_checked(df):
                for item in self._items_from_datafile(df):
                    yield item
",if self . _items_from_datafile_should_be_checked ( df ) :,135
"def Command(argv, funcs, path_val):
    arg, i = COMMAND_SPEC.Parse(argv)
    status = 0
    if arg.v:
        for kind, arg in _ResolveNames(argv[i:], funcs, path_val):
            if kind is None:
                status = 1  # nothing printed, but we fail
            else:
                # This is for -v, -V is more detailed.
                print(arg)
    else:
        util.warn(""*** command without -v not not implemented ***"")
        status = 1
    return status
",if kind is None :,147
"def delete_doc(elastic_document_id, node, index=None, category=None):
    index = index or INDEX
    if not category:
        if isinstance(node, Preprint):
            category = ""preprint""
        elif node.is_registration:
            category = ""registration""
        else:
            category = node.project_or_component
    client().delete(
        index=index,
        doc_type=category,
        id=elastic_document_id,
        refresh=True,
        ignore=[404],
    )
",elif node . is_registration :,143
"def getDictFromTree(tree):
    ret_dict = {}
    for child in tree.getchildren():
        if child.getchildren():
            ## Complex-type child. Recurse
            content = getDictFromTree(child)
        else:
            content = child.text
        if ret_dict.has_key(child.tag):
            if not type(ret_dict[child.tag]) == list:
                ret_dict[child.tag] = [ret_dict[child.tag]]
            ret_dict[child.tag].append(content or """")
        else:
            ret_dict[child.tag] = content or """"
    return ret_dict
",if child . getchildren ( ) :,175
"def get(self, block=True, timeout=None, ack=False):
    if not block:
        return self.get_nowait()
    start_time = time.time()
    while True:
        try:
            return self.get_nowait(ack)
        except BaseQueue.Empty:
            if timeout:
                lasted = time.time() - start_time
                if timeout > lasted:
                    time.sleep(min(self.max_timeout, timeout - lasted))
                else:
                    raise
            else:
                time.sleep(self.max_timeout)
",if timeout :,169
"def rewrite(self, string):
    string = super(JSReplaceFuzzy, self).rewrite(string)
    cdx = self.url_rewriter.rewrite_opts[""cdx""]
    if cdx.get(""is_fuzzy""):
        expected = unquote(cdx[""url""])
        actual = unquote(self.url_rewriter.wburl.url)
        exp_m = self.rx_obj.search(expected)
        act_m = self.rx_obj.search(actual)
        if exp_m and act_m:
            result = string.replace(exp_m.group(1), act_m.group(1))
            if result != string:
                string = result
    return string
",if exp_m and act_m :,179
"def locate_exe_dir(d, check=True):
    exe_dir = os.path.join(d, ""Scripts"") if ON_WINDOWS else os.path.join(d, ""bin"")
    if not os.path.isdir(exe_dir):
        if ON_WINDOWS:
            bin_dir = os.path.join(d, ""bin"")
            if os.path.isdir(bin_dir):
                return bin_dir
        if check:
            raise InvalidVirtualEnv(""Unable to locate executables directory."")
    return exe_dir
",if ON_WINDOWS :,140
"def _ensuresyspath(self, ensuremode, path):
    if ensuremode:
        s = str(path)
        if ensuremode == ""append"":
            if s not in sys.path:
                sys.path.append(s)
        else:
            if s != sys.path[0]:
                sys.path.insert(0, s)
",if s != sys . path [ 0 ] :,97
"def create_season_banners(self, show_obj):
    if self.season_banners and show_obj:
        result = []
        for season, episodes in show_obj.episodes.iteritems():  # @UnusedVariable
            if not self._has_season_banner(show_obj, season):
                logger.log(
                    u""Metadata provider ""
                    + self.name
                    + "" creating season banners for ""
                    + show_obj.name,
                    logger.DEBUG,
                )
                result = result + [self.save_season_banners(show_obj, season)]
        return all(result)
    return False
","if not self . _has_season_banner ( show_obj , season ) :",197
"def validate_nb(self, nb):
    super(MetadataValidatorV3, self).validate_nb(nb)
    ids = set([])
    for cell in nb.cells:
        if ""nbgrader"" not in cell.metadata:
            continue
        grade = cell.metadata[""nbgrader""][""grade""]
        solution = cell.metadata[""nbgrader""][""solution""]
        locked = cell.metadata[""nbgrader""][""locked""]
        if not grade and not solution and not locked:
            continue
        grade_id = cell.metadata[""nbgrader""][""grade_id""]
        if grade_id in ids:
            raise ValidationError(""Duplicate grade id: {}"".format(grade_id))
        ids.add(grade_id)
",if not grade and not solution and not locked :,186
"def read_version():
    regexp = re.compile(r""^__version__\W*=\W*'([\d.abrc]+)'"")
    init_py = os.path.join(os.path.dirname(__file__), ""aiopg"", ""__init__.py"")
    with open(init_py) as f:
        for line in f:
            match = regexp.match(line)
            if match is not None:
                return match.group(1)
        else:
            raise RuntimeError(""Cannot find version in aiopg/__init__.py"")
",if match is not None :,137
"def _column_keys(self):
    """"""Get a dictionary of all columns and their case mapping.""""""
    if not self.exists:
        return {}
    with self.db.lock:
        if self._columns is None:
            # Initialise the table if it doesn't exist
            table = self.table
            self._columns = {}
            for column in table.columns:
                name = normalize_column_name(column.name)
                key = normalize_column_key(name)
                if key in self._columns:
                    log.warning(""Duplicate column: %s"", name)
                self._columns[key] = name
        return self._columns
",if key in self . _columns :,180
"def find_controller_by_names(self, names, testname):
    namestring = ""."".join(names)
    if not namestring.startswith(self.name):
        return None
    if namestring == self.name:
        return self
    for suite in self.suites:
        res = suite.find_controller_by_names(
            namestring[len(self.name) + 1 :].split("".""), testname
        )
        if res:
            return res
",if res :,122
"def _volume_x_metadata_get_item(
    context, volume_id, key, model, notfound_exec, session=None
):
    result = (
        _volume_x_metadata_get_query(context, volume_id, model, session=session)
        .filter_by(key=key)
        .first()
    )
    if not result:
        if model is models.VolumeGlanceMetadata:
            raise notfound_exec(id=volume_id)
        else:
            raise notfound_exec(metadata_key=key, volume_id=volume_id)
    return result
",if model is models . VolumeGlanceMetadata :,155
"def parse_results(cwd):
    optimal_dd = None
    optimal_measure = numpy.inf
    for tup in tools.find_conf_files(cwd):
        dd = tup[1]
        if ""results.train_y_misclass"" in dd:
            if dd[""results.train_y_misclass""] < optimal_measure:
                optimal_measure = dd[""results.train_y_misclass""]
                optimal_dd = dd
    print(""Optimal results.train_y_misclass:"", str(optimal_measure))
    for key, value in optimal_dd.items():
        if ""hyper_parameters"" in key:
            print(key + "": "" + str(value))
","if ""results.train_y_misclass"" in dd :",177
"def _stop_by_max_time_mins(self):
    """"""Stop optimization process once maximum minutes have elapsed.""""""
    if self.max_time_mins:
        total_mins_elapsed = (
            datetime.now() - self._start_datetime
        ).total_seconds() / 60.0
        if total_mins_elapsed >= self.max_time_mins:
            raise KeyboardInterrupt(
                ""{:.2f} minutes have elapsed. TPOT will close down."".format(
                    total_mins_elapsed
                )
            )
",if total_mins_elapsed >= self . max_time_mins :,144
"def __new__(meta, cls_name, bases, cls_dict):
    func = cls_dict.get(""func"")
    monad_cls = super(FuncMonadMeta, meta).__new__(meta, cls_name, bases, cls_dict)
    if func:
        if type(func) is tuple:
            functions = func
        else:
            functions = (func,)
        for func in functions:
            registered_functions[func] = monad_cls
    return monad_cls
",if type ( func ) is tuple :,126
"def get_tokens_unprocessed(self, text):
    buffered = """"
    insertions = []
    lng_buffer = []
    for i, t, v in self.language_lexer.get_tokens_unprocessed(text):
        if t is self.needle:
            if lng_buffer:
                insertions.append((len(buffered), lng_buffer))
                lng_buffer = []
            buffered += v
        else:
            lng_buffer.append((i, t, v))
    if lng_buffer:
        insertions.append((len(buffered), lng_buffer))
    return do_insertions(insertions, self.root_lexer.get_tokens_unprocessed(buffered))
",if t is self . needle :,185
"def get_conditions(filters):
    conditions = {""docstatus"": (""="", 1)}
    if filters.get(""from_date"") and filters.get(""to_date""):
        conditions[""result_date""] = (
            ""between"",
            (filters.get(""from_date""), filters.get(""to_date"")),
        )
        filters.pop(""from_date"")
        filters.pop(""to_date"")
    for key, value in filters.items():
        if filters.get(key):
            conditions[key] = value
    return conditions
",if filters . get ( key ) :,140
"def _limit_value(key, value, config):
    if config[key].get(""upper_limit""):
        limit = config[key][""upper_limit""]
        # auto handle datetime
        if isinstance(value, datetime) and isinstance(limit, timedelta):
            if config[key][""inverse""] is True:
                if (datetime.now() - limit) > value:
                    value = datetime.now() - limit
            else:
                if (datetime.now() + limit) < value:
                    value = datetime.now() + limit
        elif value > limit:
            value = limit
    return value
","if config [ key ] [ ""inverse"" ] is True :",164
"def GetCurrentKeySet(self):
    ""Return CurrentKeys with 'darwin' modifications.""
    result = self.GetKeySet(self.CurrentKeys())
    if sys.platform == ""darwin"":
        # macOS (OS X) Tk variants do not support the ""Alt""
        # keyboard modifier.  Replace it with ""Option"".
        # TODO (Ned?): the ""Option"" modifier does not work properly
        #     for Cocoa Tk and XQuartz Tk so we should not use it
        #     in the default 'OSX' keyset.
        for k, v in result.items():
            v2 = [x.replace(""<Alt-"", ""<Option-"") for x in v]
            if v != v2:
                result[k] = v2
    return result
",if v != v2 :,200
"def _load_testfile(filename, package, module_relative):
    if module_relative:
        package = _normalize_module(package, 3)
        filename = _module_relative_path(package, filename)
        if hasattr(package, ""__loader__""):
            if hasattr(package.__loader__, ""get_data""):
                file_contents = package.__loader__.get_data(filename)
                # get_data() opens files as 'rb', so one must do the equivalent
                # conversion as universal newlines would do.
                return file_contents.replace(os.linesep, ""\n""), filename
    return open(filename).read(), filename
","if hasattr ( package . __loader__ , ""get_data"" ) :",163
"def iter_from_X_lengths(X, lengths):
    if lengths is None:
        yield 0, len(X)
    else:
        n_samples = X.shape[0]
        end = np.cumsum(lengths).astype(np.int32)
        start = end - lengths
        if end[-1] > n_samples:
            raise ValueError(
                ""more than {:d} samples in lengths array {!s}"".format(
                    n_samples, lengths
                )
            )
        for i in range(len(lengths)):
            yield start[i], end[i]
",if end [ - 1 ] > n_samples :,161
"def change_sel(self):
    """"""Change the view's selections.""""""
    if self.alter_select and len(self.sels) > 0:
        if self.multi_select is False:
            self.view.show(self.sels[0])
        self.view.sel().clear()
        self.view.sel().add_all(self.sels)
",if self . multi_select is False :,94
"def cb_syncthing_device_data_changed(
    self, daemon, nid, address, client_version, inbps, outbps, inbytes, outbytes
):
    if nid in self.devices:  # Should be always
        device = self.devices[nid]
        # Update strings
        device[""address""] = address
        if client_version not in (""?"", None):
            device[""version""] = client_version
        # Update rates
        device[""inbps""] = ""%s/s (%s)"" % (sizeof_fmt(inbps), sizeof_fmt(inbytes))
        device[""outbps""] = ""%s/s (%s)"" % (sizeof_fmt(outbps), sizeof_fmt(outbytes))
","if client_version not in ( ""?"" , None ) :",184
"def then(self, matches, when_response, context):
    if is_iterable(when_response):
        ret = []
        when_response = list(when_response)
        for match in when_response:
            if match not in matches:
                if self.match_name:
                    match.name = self.match_name
                matches.append(match)
                ret.append(match)
        return ret
    if self.match_name:
        when_response.name = self.match_name
    if when_response not in matches:
        matches.append(when_response)
        return when_response
",if self . match_name :,169
"def __update_parents(self, fileobj, path, delta):
    """"""Update all parent atoms with the new size.""""""
    if delta == 0:
        return
    for atom in path:
        fileobj.seek(atom.offset)
        size = cdata.uint_be(fileobj.read(4))
        if size == 1:  # 64bit
            # skip name (4B) and read size (8B)
            size = cdata.ulonglong_be(fileobj.read(12)[4:])
            fileobj.seek(atom.offset + 8)
            fileobj.write(cdata.to_ulonglong_be(size + delta))
        else:  # 32bit
            fileobj.seek(atom.offset)
            fileobj.write(cdata.to_uint_be(size + delta))
",if size == 1 :,200
"def _fields_to_index(cls):
    fields = []
    for field in cls._meta.sorted_fields:
        if field.primary_key:
            continue
        requires_index = any(
            (field.index, field.unique, isinstance(field, ForeignKeyField))
        )
        if requires_index:
            fields.append(field)
    return fields
",if requires_index :,99
"def __init__(self, value):
    """"""Initialize the integer to the given value.""""""
    self._mpz_p = new_mpz()
    self._initialized = False
    if isinstance(value, float):
        raise ValueError(""A floating point type is not a natural number"")
    self._initialized = True
    if isinstance(value, (int, long)):
        _gmp.mpz_init(self._mpz_p)
        result = _gmp.gmp_sscanf(tobytes(str(value)), b(""%Zd""), self._mpz_p)
        if result != 1:
            raise ValueError(""Error converting '%d'"" % value)
    else:
        _gmp.mpz_init_set(self._mpz_p, value._mpz_p)
",if result != 1 :,193
"def decode(cls, data):
    while data:
        length, format_type, control_flags, sequence, pid = unpack(
            cls.Header.PACK, data[: cls.Header.LEN]
        )
        if len(data) < length:
            raise NetLinkError(""Buffer underrun"")
        yield cls.format(
            format_type, control_flags, sequence, pid, data[cls.Header.LEN : length]
        )
        data = data[length:]
",if len ( data ) < length :,125
"def __post_init__(self):
    if self._node_id is not None:
        if not len(self._node_id) == constants.HASH_LENGTH:
            raise ValueError(
                ""invalid node_id: {}"".format(hexlify(self._node_id).decode())
            )
    if self.udp_port is not None and not 1 <= self.udp_port <= 65535:
        raise ValueError(""invalid udp port"")
    if self.tcp_port is not None and not 1 <= self.tcp_port <= 65535:
        raise ValueError(""invalid tcp port"")
    if not is_valid_public_ipv4(self.address, self.allow_localhost):
        raise ValueError(f""invalid ip address: '{self.address}'"")
",if not len ( self . _node_id ) == constants . HASH_LENGTH :,185
"def orderUp(self, items):
    sel = []  # new selection
    undoinfo = []
    for bid, lid in items:
        if isinstance(lid, int):
            undoinfo.append(self.orderUpLineUndo(bid, lid))
            sel.append((bid, lid - 1))
        elif lid is None:
            undoinfo.append(self.orderUpBlockUndo(bid))
            if bid == 0:
                return items
            else:
                sel.append((bid - 1, None))
    self.addUndo(undoinfo, ""Move Up"")
    return sel
",elif lid is None :,164
"def filter_data(self, min_len, max_len):
    logging.info(f""filtering data, min len: {min_len}, max len: {max_len}"")
    initial_len = len(self.src)
    filtered_src = []
    filtered_tgt = []
    for src, tgt in zip(self.src, self.tgt):
        if min_len <= len(src) <= max_len and min_len <= len(tgt) <= max_len:
            filtered_src.append(src)
            filtered_tgt.append(tgt)
    self.src = filtered_src
    self.tgt = filtered_tgt
    filtered_len = len(self.src)
    logging.info(f""pairs before: {initial_len}, after: {filtered_len}"")
",if min_len <= len ( src ) <= max_len and min_len <= len ( tgt ) <= max_len :,193
"def layer_pretrained(self, net, args, options):
    model = getattr(torchvision.models, args[0])(pretrained=True)
    model.train(True)
    if options.layer:
        layers = list(model.children())[: options.layer]
        if options.sublayer:
            layers[-1] = nn.Sequential(*layers[-1][: options.sublayer])
    else:
        layers = [model]
        print(""List of pretrained layers:"", layers)
        raise ValidationException(
            ""layer=-1 required for pretrained, sublayer=-1 optional.  Layers outputted above.""
        )
    return nn.Sequential(*layers)
",if options . sublayer :,163
"def deleteCalendar(users):
    calendarId = normalizeCalendarId(sys.argv[5])
    for user in users:
        user, cal = buildCalendarGAPIObject(user)
        if not cal:
            continue
        gapi.call(cal.calendarList(), ""delete"", soft_errors=True, calendarId=calendarId)
",if not cal :,84
"def iter_modules(self, by_clients=False, clients_filter=None):
    """"""iterate over all modules""""""
    clients = None
    if by_clients:
        clients = self.get_clients(clients_filter)
        if not clients:
            return
    self._refresh_modules()
    for module_name in self.modules:
        try:
            module = self.get_module(module_name)
        except PupyModuleDisabled:
            continue
        if clients is not None:
            for client in clients:
                if module.is_compatible_with(client):
                    yield module
                    break
        else:
            yield module
",if not clients :,181
"def update_me(self):
    try:
        while 1:
            line = self.queue.get_nowait()
            if line is None:
                self.delete(1.0, tk.END)
            else:
                self.insert(tk.END, str(line))
            self.see(tk.END)
            self.update_idletasks()
    except queue.Empty:
        pass
    self.after(100, self.update_me)
",if line is None :,128
"def request_power_state(self, state, force=False):
    if self.current_state != state or force:
        if not self.request_in_progress:
            self.request_in_progress = True
            logging.info(""Requesting %s"" % state)
            cb = PowerManager.Callback(self, state)
            rets = self.parent.Plugins.run(
                ""on_power_state_change_requested"", self, state, cb
            )
            cb.num_cb = len(rets)
            cb.check()
        else:
            logging.info(""Another request in progress"")
",if not self . request_in_progress :,165
"def __getitem__(self, idx):
    super(BatchDataset, self).__getitem__(idx)
    maxidx = len(self.dataset)
    samples = []
    for i in range(0, self.batchsize):
        j = idx * self.batchsize + i
        if j >= maxidx:
            break
        j = self.perm(j, maxidx)
        sample = self.dataset[j]
        if self.filter(sample):
            samples.append(sample)
    samples = self.makebatch(samples)
    return samples
",if self . filter ( sample ) :,135
"def __call__(self, request, *args, **kwargs):
    template_vars = {}
    for form_name, form_class in self.forms.iteritems():
        if form_class.must_display(request, *args, **kwargs):
            template_vars[form_name] = form_class(request)
        else:
            template_vars[form_name] = None
    if request.method == ""POST"":
        action = self.find_post_handler_action(request)
        form = self.handlers[action](request, data=request.POST, files=request.FILES)
        template_vars.update(form.dispatch(action, request, *args, **kwargs))
    return self.GET(template_vars, request, *args, **kwargs)
","if form_class . must_display ( request , * args , ** kwargs ) :",191
"def on_show_all(self, widget, another):
    if widget.get_active():
        if another.get_active():
            self.treeview.update_items(all=True, comment=True)
        else:
            self.treeview.update_items(all=True)
    else:
        if another.get_active():
            self.treeview.update_items(comment=True)
        else:
            self.treeview.update_items()
",if another . get_active ( ) :,121
"def close(self):
    if self._closed:
        return
    self._closed = True
    for proto in self._pipes.values():
        if proto is None:
            continue
        proto.pipe.close()
    if (
        self._proc is not None
        and
        # has the child process finished?
        self._returncode is None
        and
        # the child process has finished, but the
        # transport hasn't been notified yet?
        self._proc.poll() is None
    ):
        if self._loop.get_debug():
            logger.warning(""Close running child process: kill %r"", self)
        try:
            self._proc.kill()
        except ProcessLookupError:
            pass
",if self . _loop . get_debug ( ) :,191
"def runTest(self):
    self.poco(text=""wait UI"").click()
    bomb_count = 0
    while True:
        blue_fish = self.poco(""fish_emitter"").child(""blue"")
        yellow_fish = self.poco(""fish_emitter"").child(""yellow"")
        bomb = self.poco(""fish_emitter"").child(""bomb"")
        fish = self.poco.wait_for_any([blue_fish, yellow_fish, bomb])
        if fish is bomb:
            bomb_count += 1
            if bomb_count > 3:
                return
        else:
            fish.click()
        time.sleep(2.5)
",if bomb_count > 3 :,192
"def load_managers(*, loop, only):
    managers = {}
    for key in DB_CLASSES:
        if only and key not in only:
            continue
        params = DB_DEFAULTS.get(key) or {}
        params.update(DB_OVERRIDES.get(key) or {})
        database = DB_CLASSES[key](**params)
        managers[key] = peewee_async.Manager(database, loop=loop)
    return managers
",if only and key not in only :,112
"def links_extracted(self, request, links):
    for link in links:
        if link.meta[b""state""] == States.NOT_CRAWLED:
            r = self._create_request(link.url)
            r.meta[b""depth""] = request.meta[b""depth""] + 1
            self.schedule(r, self._get_score(r.meta[b""depth""]))
            link.meta[b""state""] = States.QUEUED
","if link . meta [ b""state"" ] == States . NOT_CRAWLED :",123
"def find_worktree_git_dir(dotgit):
    """"""Search for a gitdir for this worktree.""""""
    try:
        statbuf = os.stat(dotgit)
    except OSError:
        return None
    if not stat.S_ISREG(statbuf.st_mode):
        return None
    try:
        lines = open(dotgit, ""r"").readlines()
        for key, value in [line.strip().split("": "") for line in lines]:
            if key == ""gitdir"":
                return value
    except ValueError:
        pass
    return None
","if key == ""gitdir"" :",147
"def _is_static_shape(self, shape):
    if shape is None or not isinstance(shape, list):
        return False
    for dim_value in shape:
        if not isinstance(dim_value, int):
            return False
        if dim_value < 0:
            raise Exception(""Negative dimension is illegal: %d"" % dim_value)
    return True
",if dim_value < 0 :,94
"def init_logger():
    configured_loggers = [log_config.get(""root"", {})] + [
        logger for logger in log_config.get(""loggers"", {}).values()
    ]
    used_handlers = {
        handler for log in configured_loggers for handler in log.get(""handlers"", [])
    }
    for handler_id, handler in list(log_config[""handlers""].items()):
        if handler_id not in used_handlers:
            del log_config[""handlers""][handler_id]
        elif ""filename"" in handler.keys():
            filename = handler[""filename""]
            logfile_path = Path(filename).expanduser().resolve()
            handler[""filename""] = str(logfile_path)
    logging.config.dictConfig(log_config)
","elif ""filename"" in handler . keys ( ) :",192
"def __call__(self):
    dmin, dmax = self.viewlim_to_dt()
    ymin = self.base.le(dmin.year)
    ymax = self.base.ge(dmax.year)
    ticks = [dmin.replace(year=ymin, **self.replaced)]
    while 1:
        dt = ticks[-1]
        if dt.year >= ymax:
            return date2num(ticks)
        year = dt.year + self.base.get_base()
        ticks.append(dt.replace(year=year, **self.replaced))
",if dt . year >= ymax :,144
"def taiga(request, trigger_id, key):
    signature = request.META.get(""HTTP_X_TAIGA_WEBHOOK_SIGNATURE"")
    # check that the data are ok with the provided signature
    if verify_signature(request._request.body, key, signature):
        data = data_filter(trigger_id, **request.data)
        status = save_data(trigger_id, data)
        return (
            Response({""message"": ""Success""})
            if status
            else Response({""message"": ""Failed!""})
        )
    Response({""message"": ""Bad request""})
",if status,149
"def ParseResponses(
    self,
    knowledge_base: rdf_client.KnowledgeBase,
    responses: Iterable[rdfvalue.RDFValue],
) -> Iterator[rdf_client.User]:
    for response in responses:
        if not isinstance(response, rdf_client_fs.StatEntry):
            raise TypeError(f""Unexpected response type: `{type(response)}`"")
        # TODO: `st_mode` has to be an `int`, not `StatMode`.
        if stat.S_ISDIR(int(response.st_mode)):
            homedir = response.pathspec.path
            username = os.path.basename(homedir)
            if username not in self._ignore_users:
                yield rdf_client.User(username=username, homedir=homedir)
","if not isinstance ( response , rdf_client_fs . StatEntry ) :",198
"def _iter_lines(path=path, response=response, max_next=options.http_max_next):
    path.responses = []
    n = 0
    while response:
        path.responses.append(response)
        yield from response.iter_lines(decode_unicode=True)
        src = response.links.get(""next"", {}).get(""url"", None)
        if not src:
            break
        n += 1
        if n > max_next:
            vd.warning(f""stopping at max {max_next} pages"")
            break
        vd.status(f""fetching next page from {src}"")
        response = requests.get(src, stream=True)
",if n > max_next :,179
"def __enter__(self):
    """"""Open a file and read it.""""""
    if self.code is None:
        LOGGER.info(""File is reading: %s"", self.path)
        if sys.version_info >= (3,):
            self._file = open(self.path, encoding=""utf-8"")
        else:
            self._file = open(self.path, ""rU"")
        self.code = self._file.read()
    return self
","if sys . version_info >= ( 3 , ) :",117
"def facts_for_oauthclients(self, namespace):
    """"""Gathers facts for oauthclients used with logging""""""
    self.default_keys_for(""oauthclients"")
    a_list = self.oc_command(
        ""get"", ""oauthclients"", namespace=namespace, add_options=[""-l"", LOGGING_SELECTOR]
    )
    if len(a_list[""items""]) == 0:
        return
    for item in a_list[""items""]:
        name = item[""metadata""][""name""]
        comp = self.comp(name)
        if comp is not None:
            result = dict(redirectURIs=item[""redirectURIs""])
            self.add_facts_for(comp, ""oauthclients"", name, result)
",if comp is not None :,173
"def get(self, k):
    with self._lock:
        if k not in self._data1 and k in self._data2:
            self._data1[k] = self._data2[k]
            del self._data2[k]
    return self._data1.get(k)
",if k not in self . _data1 and k in self . _data2 :,77
"def _parseparam(s):
    plist = []
    while s[:1] == "";"":
        s = s[1:]
        end = s.find("";"")
        while end > 0 and (s.count('""', 0, end) - s.count('\\""', 0, end)) % 2:
            end = s.find("";"", end + 1)
        if end < 0:
            end = len(s)
        f = s[:end]
        if ""="" in f:
            i = f.index(""="")
            f = f[:i].strip().lower() + ""="" + f[i + 1 :].strip()
        plist.append(f.strip())
        s = s[end:]
    return plist
",if end < 0 :,177
"def __init__(self, **params):
    if ""length"" in params:
        if ""start"" in params or ""end"" in params:
            raise ValueError(""Supply either length or start and end to Player not both"")
        params[""start""] = 0
        params[""end""] = params.pop(""length"") - 1
    elif params.get(""start"", 0) > 0 and not ""value"" in params:
        params[""value""] = params[""start""]
    super(Player, self).__init__(**params)
","if ""start"" in params or ""end"" in params :",126
"def libcxx_define(settings):
    compiler = _base_compiler(settings)
    libcxx = settings.get_safe(""compiler.libcxx"")
    if not compiler or not libcxx:
        return """"
    if str(compiler) in GCC_LIKE:
        if str(libcxx) == ""libstdc++"":
            return ""_GLIBCXX_USE_CXX11_ABI=0""
        elif str(libcxx) == ""libstdc++11"":
            return ""_GLIBCXX_USE_CXX11_ABI=1""
    return """"
","elif str ( libcxx ) == ""libstdc++11"" :",146
"def _get_sort_map(tags):
    """"""See TAG_TO_SORT""""""
    tts = {}
    for name, tag in tags.items():
        if tag.has_sort:
            if tag.user:
                tts[name] = ""%ssort"" % name
            if tag.internal:
                tts[""~%s"" % name] = ""~%ssort"" % name
    return tts
",if tag . internal :,111
"def quiet_f(*args):
    vars = {arg_name: Real(arg) for arg_name, arg in zip(arg_names, args)}
    value = dynamic_scoping(quiet_expr.evaluate, vars, evaluation)
    if expect_list:
        if value.has_form(""List"", None):
            value = [extract_pyreal(item) for item in value.leaves]
            if any(item is None for item in value):
                return None
            return value
        else:
            return None
    else:
        value = extract_pyreal(value)
        if value is None or isinf(value) or isnan(value):
            return None
        return value
",if value is None or isinf ( value ) or isnan ( value ) :,177
"def on_action_chosen(self, id, action, mark_changed=True):
    before = self.set_action(self.current, id, action)
    if mark_changed:
        if before.to_string() != action.to_string():
            # TODO: Maybe better comparison
            self.undo.append(UndoRedo(id, before, action))
            self.builder.get_object(""btUndo"").set_sensitive(True)
        self.on_profile_modified()
    else:
        self.on_profile_modified(update_ui=False)
    return before
",if before . to_string ( ) != action . to_string ( ) :,149
"def setUp(self):
    super(OperaterTest, self).setUp()
    if is_cli:
        import clr
        self.load_iron_python_test()
        if is_netcoreapp:
            clr.AddReference(""System.Drawing.Primitives"")
        else:
            clr.AddReference(""System.Drawing"")
",if is_netcoreapp :,93
"def field_to_field_type(field):
    field_type = field[""type""]
    if isinstance(field_type, dict):
        field_type = field_type[""type""]
    if isinstance(field_type, list):
        field_type_length = len(field_type)
        if field_type_length == 0:
            raise Exception(""Zero-length type list encountered, invalid CWL?"")
        elif len(field_type) == 1:
            field_type = field_type[0]
    return field_type
",elif len ( field_type ) == 1 :,135
"def _flatten(*args):
    ahs = set()
    if len(args) > 0:
        for item in args:
            if type(item) is ActionHandle:
                ahs.add(item)
            elif type(item) in (list, tuple, dict, set):
                for ah in item:
                    if type(ah) is not ActionHandle:  # pragma:nocover
                        raise ActionManagerError(""Bad argument type %s"" % str(ah))
                    ahs.add(ah)
            else:  # pragma:nocover
                raise ActionManagerError(""Bad argument type %s"" % str(item))
    return ahs
",if type ( item ) is ActionHandle :,183
"def _Determine_Do(self):
    self.applicable = 1
    configTokens = black.configure.items[""configTokens""].Get()
    buildFlavour = black.configure.items[""buildFlavour""].Get()
    if buildFlavour == ""full"":
        self.value = False
    else:
        self.value = True
    for opt, optarg in self.chosenOptions:
        if opt == ""--with-tests"":
            if not self.value:
                configTokens.append(""tests"")
            self.value = True
        elif opt == ""--without-tests"":
            if self.value:
                configTokens.append(""notests"")
            self.value = False
    self.determined = 1
","if opt == ""--with-tests"" :",183
"def title_by_index(self, trans, index, context):
    d_type = self.get_datatype(trans, context)
    for i, (composite_name, composite_file) in enumerate(d_type.writable_files.items()):
        if i == index:
            rval = composite_name
            if composite_file.description:
                rval = ""{} ({})"".format(rval, composite_file.description)
            if composite_file.optional:
                rval = ""%s [optional]"" % rval
            return rval
    if index < self.get_file_count(trans, context):
        return ""Extra primary file""
    return None
",if i == index :,167
"def func(x, y):
    try:
        if x > y:
            z = x + 2 * math.sin(y)
            return z ** 2
        elif x == y:
            return 4
        else:
            return 2 ** 3
    except ValueError:
        foo = 0
        for i in range(4):
            foo += i
        return foo
    except TypeError:
        return 42
    else:
        return 33
    finally:
        print(""finished"")
",if x > y :,134
"def test_suite():
    suite = unittest.TestSuite()
    for fn in os.listdir(here):
        if fn.startswith(""test"") and fn.endswith("".py""):
            modname = ""distutils.tests."" + fn[:-3]
            __import__(modname)
            module = sys.modules[modname]
            suite.addTest(module.test_suite())
    return suite
","if fn . startswith ( ""test"" ) and fn . endswith ( "".py"" ) :",98
"def check_stack_names(self, frame, expected):
    names = []
    while frame:
        name = frame.f_code.co_name
        # Stop checking frames when we get to our test helper.
        if name.startswith(""check_"") or name.startswith(""call_""):
            break
        names.append(name)
        frame = frame.f_back
    self.assertEqual(names, expected)
","if name . startswith ( ""check_"" ) or name . startswith ( ""call_"" ) :",104
"def leave(self, reason=None):
    try:
        if self.id.startswith(""C""):
            log.info(""Leaving channel %s (%s)"", self, self.id)
            self._bot.api_call(""conversations.leave"", data={""channel"": self.id})
        else:
            log.info(""Leaving group %s (%s)"", self, self.id)
            self._bot.api_call(""conversations.leave"", data={""channel"": self.id})
    except SlackAPIResponseError as e:
        if e.error == ""user_is_bot"":
            raise RoomError(f""Unable to leave channel. {USER_IS_BOT_HELPTEXT}"")
        else:
            raise RoomError(e)
    self._id = None
","if e . error == ""user_is_bot"" :",197
"def ident(self):
    value = self._ident
    if value is False:
        value = None
        # XXX: how will this interact with orig_prefix ?
        #      not exposing attrs for now if orig_prefix is set.
        if not self.orig_prefix:
            wrapped = self.wrapped
            ident = getattr(wrapped, ""ident"", None)
            if ident is not None:
                value = self._wrap_hash(ident)
        self._ident = value
    return value
",if not self . orig_prefix :,135
"def is_ac_power_connected():
    for power_source_path in Path(""/sys/class/power_supply/"").iterdir():
        try:
            with open(power_source_path / ""type"", ""r"") as f:
                if f.read().strip() != ""Mains"":
                    continue
            with open(power_source_path / ""online"", ""r"") as f:
                if f.read(1) == ""1"":
                    return True
        except IOError:
            continue
    return False
","if f . read ( ) . strip ( ) != ""Mains"" :",144
"def _get_pending_by_app_token(self, app_token):
    result = []
    with self._pending_lock:
        self._remove_stale_pending()
        for data in self._pending_decisions:
            if data.app_token == app_token:
                result.append(data)
    return result
",if data . app_token == app_token :,86
"def do_create(specific_tables=None, base=Base):
    engine = get_engine()
    try:
        if specific_tables:
            logger.info(
                ""Initializing only a subset of tables as requested: {}"".format(
                    specific_tables
                )
            )
            base.metadata.create_all(engine, tables=specific_tables)
        else:
            base.metadata.create_all(engine)
    except Exception as err:
        raise Exception(""could not create/re-create DB tables - exception: "" + str(err))
",if specific_tables :,152
"def __setitem__(self, ndx, val):
    #
    # Get the expression data object
    #
    exprdata = None
    if ndx in self._data:
        exprdata = self._data[ndx]
    else:
        _ndx = normalize_index(ndx)
        if _ndx in self._data:
            exprdata = self._data[_ndx]
    if exprdata is None:
        raise KeyError(
            ""Cannot set the value of Expression '%s' with ""
            ""invalid index '%s'"" % (self.cname(True), str(ndx))
        )
    #
    # Set the value
    #
    exprdata.set_value(val)
",if _ndx in self . _data :,179
"def write(self, *bits):
    for bit in bits:
        if not self.bytestream:
            self.bytestream.append(0)
        byte = self.bytestream[self.bytenum]
        if self.bitnum == 8:
            if self.bytenum == len(self.bytestream) - 1:
                byte = 0
                self.bytestream += bytes([byte])
            self.bytenum += 1
            self.bitnum = 0
        mask = 2 ** self.bitnum
        if bit:
            byte |= mask
        else:
            byte &= ~mask
        self.bytestream[self.bytenum] = byte
        self.bitnum += 1
",if not self . bytestream :,186
"def terminate_subprocess(proc, timeout=0.1, log=None):
    if proc.poll() is None:
        if log:
            log.info(""Sending SIGTERM to %r"", proc)
        proc.terminate()
        timeout_time = time.time() + timeout
        while proc.poll() is None and time.time() < timeout_time:
            time.sleep(0.02)
        if proc.poll() is None:
            if log:
                log.info(""Sending SIGKILL to %r"", proc)
            proc.kill()
    return proc.returncode
",if log :,152
"def mkpanel(color, rows, cols, tly, tlx):
    win = curses.newwin(rows, cols, tly, tlx)
    pan = panel.new_panel(win)
    if curses.has_colors():
        if color == curses.COLOR_BLUE:
            fg = curses.COLOR_WHITE
        else:
            fg = curses.COLOR_BLACK
        bg = color
        curses.init_pair(color, fg, bg)
        win.bkgdset(ord("" ""), curses.color_pair(color))
    else:
        win.bkgdset(ord("" ""), curses.A_BOLD)
    return pan
",if color == curses . COLOR_BLUE :,162
"def all_words(filename):
    start_char = True
    for c in characters(filename):
        if start_char == True:
            word = """"
            if c.isalnum():
                # We found the start of a word
                word = c.lower()
                start_char = False
            else:
                pass
        else:
            if c.isalnum():
                word += c.lower()
            else:
                # We found end of word, emit it
                start_char = True
                yield word
",if c . isalnum ( ) :,158
"def get_tf_weights_as_numpy(path=""./ckpt/aeslc/model.ckpt-32000"") -> Dict:
    init_vars = tf.train.list_variables(path)
    tf_weights = {}
    ignore_name = [""Adafactor"", ""global_step""]
    for name, shape in tqdm(init_vars, desc=""converting tf checkpoint to dict""):
        skip_key = any([pat in name for pat in ignore_name])
        if skip_key:
            continue
        array = tf.train.load_variable(path, name)
        tf_weights[name] = array
    return tf_weights
",if skip_key :,156
"def app(scope, receive, send):
    while True:
        message = await receive()
        if message[""type""] == ""websocket.connect"":
            await send({""type"": ""websocket.accept""})
        elif message[""type""] == ""websocket.receive"":
            pass
        elif message[""type""] == ""websocket.disconnect"":
            break
","elif message [ ""type"" ] == ""websocket.receive"" :",93
"def autoload(self):
    if self._app.config.THEME == ""auto"":
        if sys.platform == ""darwin"":
            if get_osx_theme() == 1:
                theme = DARK
            else:
                theme = LIGHT
        else:
            theme = self.guess_system_theme()
            if theme == Dark:
                theme = MacOSDark
    else:  # user settings have highest priority
        theme = self._app.config.THEME
    self.load_theme(theme)
",if get_osx_theme ( ) == 1 :,141
"def example_reading_spec(self):
    data_fields = {""targets"": tf.VarLenFeature(tf.int64)}
    if self.has_inputs:
        data_fields[""inputs""] = tf.VarLenFeature(tf.int64)
    if self.packed_length:
        if self.has_inputs:
            data_fields[""inputs_segmentation""] = tf.VarLenFeature(tf.int64)
            data_fields[""inputs_position""] = tf.VarLenFeature(tf.int64)
        data_fields[""targets_segmentation""] = tf.VarLenFeature(tf.int64)
        data_fields[""targets_position""] = tf.VarLenFeature(tf.int64)
    data_items_to_decoders = None
    return (data_fields, data_items_to_decoders)
",if self . has_inputs :,188
"def _prepare_travel_graph(self):
    for op in self.op_dict.values():
        op.const = False
        if op.node.op in [""Const"", ""Placeholder""]:
            op.resolved = True
            if op.node.op == ""Const"":
                op.const = True
        else:
            op.resolved = False
","if op . node . op == ""Const"" :",96
"def get_filestream_file_items(self):
    data = {}
    fs_file_updates = self.get_filestream_file_updates()
    for k, v in six.iteritems(fs_file_updates):
        l = []
        for d in v:
            offset = d.get(""offset"")
            content = d.get(""content"")
            assert offset is not None
            assert content is not None
            assert offset == 0 or offset == len(l), (k, v, l, d)
            if not offset:
                l = []
            l.extend(map(json.loads, content))
        data[k] = l
    return data
",if not offset :,179
"def _rewrite_exprs(self, table, what):
    from ibis.expr.analysis import substitute_parents
    what = util.promote_list(what)
    all_exprs = []
    for expr in what:
        if isinstance(expr, ir.ExprList):
            all_exprs.extend(expr.exprs())
        else:
            bound_expr = ir.bind_expr(table, expr)
            all_exprs.append(bound_expr)
    return [substitute_parents(x, past_projection=False) for x in all_exprs]
","if isinstance ( expr , ir . ExprList ) :",139
"def _group_by_commit_and_time(self, hits):
    result = {}
    for hit in hits:
        source_hit = hit[""_source""]
        key = ""%s_%s"" % (source_hit[""commit_info""][""id""], source_hit[""datetime""])
        benchmark = self._benchmark_from_es_record(source_hit)
        if key in result:
            result[key][""benchmarks""].append(benchmark)
        else:
            run_info = self._run_info_from_es_record(source_hit)
            run_info[""benchmarks""] = [benchmark]
            result[key] = run_info
    return result
",if key in result :,170
"def _build_index(self):
    self._index = {}
    for start_char, sorted_offsets in self._offsets.items():
        self._index[start_char] = {}
        for i, offset in enumerate(sorted_offsets.get_offsets()):
            identifier = sorted_offsets.get_identifier_by_offset(offset)
            if identifier[0 : self.index_depth] not in self._index[start_char]:
                self._index[start_char][identifier[0 : self.index_depth]] = i
",if identifier [ 0 : self . index_depth ] not in self . _index [ start_char ] :,134
"def scan_resource_conf(self, conf):
    if ""properties"" in conf:
        if ""attributes"" in conf[""properties""]:
            if ""exp"" in conf[""properties""][""attributes""]:
                if conf[""properties""][""attributes""][""exp""]:
                    return CheckResult.PASSED
    return CheckResult.FAILED
","if ""attributes"" in conf [ ""properties"" ] :",82
"def _PatchArtifact(self, artifact: rdf_artifacts.Artifact) -> rdf_artifacts.Artifact:
    """"""Patches artifact to not contain byte-string source attributes.""""""
    patched = False
    for source in artifact.sources:
        attributes = source.attributes.ToDict()
        unicode_attributes = compatibility.UnicodeJson(attributes)
        if attributes != unicode_attributes:
            source.attributes = unicode_attributes
            patched = True
    if patched:
        self.DeleteArtifact(str(artifact.name))
        self.WriteArtifact(artifact)
    return artifact
",if attributes != unicode_attributes :,139
"def edit_file(self, filename):
    import subprocess
    editor = self.get_editor()
    if self.env:
        environ = os.environ.copy()
        environ.update(self.env)
    else:
        environ = None
    try:
        c = subprocess.Popen('%s ""%s""' % (editor, filename), env=environ, shell=True)
        exit_code = c.wait()
        if exit_code != 0:
            raise ClickException(""%s: Editing failed!"" % editor)
    except OSError as e:
        raise ClickException(""%s: Editing failed: %s"" % (editor, e))
",if exit_code != 0 :,157
"def findControlPointsInMesh(glyph, va, subsegments):
    controlPointIndices = np.zeros((len(va), 1))
    index = 0
    for i, c in enumerate(subsegments):
        segmentCount = len(glyph.contours[i].segments) - 1
        for j, s in enumerate(c):
            if j < segmentCount:
                if glyph.contours[i].segments[j].type == ""line"":
                    controlPointIndices[index] = 1
            index += s[1]
    return controlPointIndices
",if j < segmentCount :,143
"def to_representation(self, value):
    old_social_string_fields = [""twitter"", ""github"", ""linkedIn""]
    request = self.context.get(""request"")
    show_old_format = (
        request
        and is_deprecated(request.version, self.min_version)
        and request.method == ""GET""
    )
    if show_old_format:
        social = value.copy()
        for key in old_social_string_fields:
            if social.get(key):
                social[key] = value[key][0]
            elif social.get(key) == []:
                social[key] = """"
        value = social
    return super(SocialField, self).to_representation(value)
",if social . get ( key ) :,200
"def iter_raw_frames(path, packet_sizes, ctx):
    with open(path, ""rb"") as f:
        for i, size in enumerate(packet_sizes):
            packet = Packet(size)
            read_size = f.readinto(packet)
            assert size
            assert read_size == size
            if not read_size:
                break
            for frame in ctx.decode(packet):
                yield frame
        while True:
            try:
                frames = ctx.decode(None)
            except EOFError:
                break
            for frame in frames:
                yield frame
            if not frames:
                break
",if not frames :,189
"def get_shadows_zip(filename):
    import zipfile
    shadow_pkgs = set()
    with zipfile.ZipFile(filename) as lib_zip:
        already_test = []
        for fname in lib_zip.namelist():
            pname, fname = os.path.split(fname)
            if fname or (pname and fname):
                continue
            if pname not in already_test and ""/"" not in pname:
                already_test.append(pname)
                if is_shadowing(pname):
                    shadow_pkgs.add(pname)
    return shadow_pkgs
",if is_shadowing ( pname ) :,159
"def metrics_to_scalars(self, metrics):
    new_metrics = {}
    for k, v in metrics.items():
        if isinstance(v, torch.Tensor):
            v = v.item()
        if isinstance(v, dict):
            v = self.metrics_to_scalars(v)
        new_metrics[k] = v
    return new_metrics
","if isinstance ( v , dict ) :",95
"def insert_resets(f):
    newsync = dict()
    for k, v in f.sync.items():
        if f.clock_domains[k].rst is not None:
            newsync[k] = insert_reset(ResetSignal(k), v)
        else:
            newsync[k] = v
    f.sync = newsync
",if f . clock_domains [ k ] . rst is not None :,91
"def get_attached_nodes(self, external_account):
    for node in self.get_nodes_with_oauth_grants(external_account):
        if node is None:
            continue
        node_settings = node.get_addon(self.oauth_provider.short_name)
        if node_settings is None:
            continue
        if node_settings.external_account == external_account:
            yield node
",if node_settings is None :,110
"def visitIf(self, node, scope):
    for test, body in node.tests:
        if isinstance(test, ast.Const):
            if type(test.value) in self._const_types:
                if not test.value:
                    continue
        self.visit(test, scope)
        self.visit(body, scope)
    if node.else_:
        self.visit(node.else_, scope)
",if not test . value :,112
"def flatten(self):
    # this is similar to fill_messages except it uses a list instead
    # of a queue to place the messages in.
    result = []
    channel = await self.messageable._get_channel()
    self.channel = channel
    while self._get_retrieve():
        data = await self._retrieve_messages(self.retrieve)
        if len(data) < 100:
            self.limit = 0  # terminate the infinite loop
        if self.reverse:
            data = reversed(data)
        if self._filter:
            data = filter(self._filter, data)
        for element in data:
            result.append(self.state.create_message(channel=channel, data=element))
    return result
",if self . reverse :,187
"def compute(self, x, y=None, targets=None):
    if targets is None:
        targets = self.out_params
    in_params = list(self.in_x)
    if len(in_params) == 1:
        args = [x]
    else:
        args = list(zip(*x))
    if y is None:
        pipe = self.pipe
    else:
        pipe = self.train_pipe
        if len(self.in_y) == 1:
            args.append(y)
        else:
            args += list(zip(*y))
        in_params += self.in_y
    return self._compute(*args, pipe=pipe, param_names=in_params, targets=targets)
",if len ( self . in_y ) == 1 :,190
"def _import_top_module(self, name):
    # scan sys.path looking for a location in the filesystem that contains
    # the module, or an Importer object that can import the module.
    for item in sys.path:
        if isinstance(item, _StringType):
            module = self.fs_imp.import_from_dir(item, name)
        else:
            module = item.import_top(name)
        if module:
            return module
    return None
","if isinstance ( item , _StringType ) :",124
"def __getitem__(self, key, _get_mode=False):
    if not _get_mode:
        if isinstance(key, (int, long)):
            return self._list[key]
        elif isinstance(key, slice):
            return self.__class__(self._list[key])
    ikey = key.lower()
    for k, v in self._list:
        if k.lower() == ikey:
            return v
    # micro optimization: if we are in get mode we will catch that
    # exception one stack level down so we can raise a standard
    # key error instead of our special one.
    if _get_mode:
        raise KeyError()
    raise BadRequestKeyError(key)
",if k . lower ( ) == ikey :,176
"def execute(self, arbiter, props):
    watcher = self._get_watcher(arbiter, props.pop(""name""))
    action = 0
    for key, val in props.get(""options"", {}).items():
        if key == ""hooks"":
            new_action = 0
            for name, _val in val.items():
                action = watcher.set_opt(""hooks.%s"" % name, _val)
                if action == 1:
                    new_action = 1
        else:
            new_action = watcher.set_opt(key, val)
        if new_action == 1:
            action = 1
    # trigger needed action
    return watcher.do_action(action)
",if action == 1 :,186
"def OnBodyClick(self, event=None):
    try:
        c = self.c
        p = c.currentPosition()
        if not g.doHook(""bodyclick1"", c=c, p=p, v=p, event=event):
            self.OnActivateBody(event=event)
        g.doHook(""bodyclick2"", c=c, p=p, v=p, event=event)
    except:
        g.es_event_exception(""bodyclick"")
","if not g . doHook ( ""bodyclick1"" , c = c , p = p , v = p , event = event ) :",124
"def _class_weights(spec: config.MetricsSpec) -> Optional[Dict[int, float]]:
    """"""Returns class weights associated with AggregationOptions at offset.""""""
    if spec.aggregate.HasField(""top_k_list""):
        if spec.aggregate.class_weights:
            raise ValueError(
                ""class_weights are not supported when top_k_list used: ""
                ""spec={}"".format(spec)
            )
        return None
    return dict(spec.aggregate.class_weights) or None
",if spec . aggregate . class_weights :,130
"def _is_perf_file(file_path):
    f = get_file(file_path)
    for line in f:
        if line[0] == ""#"":
            continue
        r = event_regexp.search(line)
        if r:
            f.close()
            return True
        f.close()
        return False
",if r :,92
"def _get_before_insertion_node(self):
    if self._nodes_stack.is_empty():
        return None
    line = self._nodes_stack.parsed_until_line + 1
    node = self._new_module.get_last_leaf()
    while True:
        parent = node.parent
        if parent.type in (""suite"", ""file_input""):
            assert node.end_pos[0] <= line
            assert node.end_pos[1] == 0 or ""\n"" in self._prefix
            return node
        node = parent
","if parent . type in ( ""suite"" , ""file_input"" ) :",143
"def PyJsHoisted_parseClassRanges_(this, arguments, var=var):
    var = Scope({u""this"": this, u""arguments"": arguments}, var)
    var.registers([u""res""])
    pass
    if var.get(u""current"")(Js(u""]"")):
        return Js([])
    else:
        var.put(u""res"", var.get(u""parseNonemptyClassRanges"")())
        if var.get(u""res"").neg():
            var.get(u""bail"")(Js(u""nonEmptyClassRanges""))
        return var.get(u""res"")
","if var . get ( u""res"" ) . neg ( ) :",152
"def _recurse_children(self, offset):
    """"""Recurses thorugh the available children""""""
    while offset < self.obj_offset + self.Length:
        item = obj.Object(""VerStruct"", offset=offset, vm=self.obj_vm, parent=self)
        if item.Length < 1 or item.get_key() == None:
            raise StopIteration(
                ""Could not recover a key for a child at offset {0}"".format(
                    item.obj_offset
                )
            )
        yield item.get_key(), item.get_children()
        offset = self.offset_pad(offset + item.Length)
    raise StopIteration(""No children"")
",if item . Length < 1 or item . get_key ( ) == None :,177
"def _adapt_types(self, descr):
    names = []
    adapted_types = []
    for col in descr:
        names.append(col[0])
        impala_typename = col[1]
        typename = udf._impala_to_ibis_type[impala_typename.lower()]
        if typename == ""decimal"":
            precision, scale = col[4:6]
            adapted_types.append(dt.Decimal(precision, scale))
        else:
            adapted_types.append(typename)
    return names, adapted_types
","if typename == ""decimal"" :",144
"def sniff(self, filename):
    try:
        if filename and tarfile.is_tarfile(filename):
            with tarfile.open(filename, ""r"") as temptar:
                for f in temptar:
                    if not f.isfile():
                        continue
                    if f.name.endswith("".fast5""):
                        return True
                    else:
                        return False
    except Exception as e:
        log.warning(""%s, sniff Exception: %s"", self, e)
    return False
",if filename and tarfile . is_tarfile ( filename ) :,149
"def getValue(self):
    if getattr(self.object, ""type"", """") != ""CURVE"":
        return BezierSpline()
    evaluatedObject = getEvaluatedID(self.object)
    bSplines = evaluatedObject.data.splines
    if len(bSplines) > 0:
        spline = createSplineFromBlenderSpline(bSplines[0])
        # Is None when the spline type is not supported.
        if spline is not None:
            if self.useWorldSpace:
                spline.transform(evaluatedObject.matrix_world)
            return spline
    return BezierSpline()
",if self . useWorldSpace :,153
"def escape(text, newline=False):
    """"""Escape special html characters.""""""
    if isinstance(text, str):
        if ""&"" in text:
            text = text.replace(""&"", ""&amp;"")
        if "">"" in text:
            text = text.replace("">"", ""&gt;"")
        if ""<"" in text:
            text = text.replace(""<"", ""&lt;"")
        if '""' in text:
            text = text.replace('""', ""&quot;"")
        if ""'"" in text:
            text = text.replace(""'"", ""&quot;"")
        if newline:
            if ""\n"" in text:
                text = text.replace(""\n"", ""<br>"")
    return text
","if "">"" in text :",170
"def _get_ilo_version(self):
    try:
        self._get_ilo2('<?xml version=""1.0""?><RIBCL VERSION=""2.0""></RIBCL>')
    except ResponseError as e:
        if hasattr(e, ""code""):
            if e.code == 405:
                return 3
            if e.code == 501:
                return 1
        raise
    return 2
",if e . code == 405 :,113
"def convert_path(ctx, tpath):
    for points, code in tpath.iter_segments():
        if code == Path.MOVETO:
            ctx.move_to(*points)
        elif code == Path.LINETO:
            ctx.line_to(*points)
        elif code == Path.CURVE3:
            ctx.curve_to(
                points[0], points[1], points[0], points[1], points[2], points[3]
            )
        elif code == Path.CURVE4:
            ctx.curve_to(*points)
        elif code == Path.CLOSEPOLY:
            ctx.close_path()
",elif code == Path . CLOSEPOLY :,172
"def called_by_shrinker():
    frame = sys._getframe(0)
    while frame:
        fname = frame.f_globals.get(""__file__"", """")
        if os.path.basename(fname) == ""shrinker.py"":
            return True
        frame = frame.f_back
    return False
","if os . path . basename ( fname ) == ""shrinker.py"" :",83
"def _ensuresyspath(self, ensuremode, path):
    if ensuremode:
        s = str(path)
        if ensuremode == ""append"":
            if s not in sys.path:
                sys.path.append(s)
        else:
            if s != sys.path[0]:
                sys.path.insert(0, s)
",if s not in sys . path :,97
"def get_instances(self, region: str, vpc: str):
    try:
        await self._cache_instances(region)
        return [
            instance
            for instance in self._instances_cache[region]
            if instance[""VpcId""] == vpc
        ]
    except Exception as e:
        print_exception(f""Failed to get RDS instances: {e}"")
        return []
","if instance [ ""VpcId"" ] == vpc",105
"def get_and_set_all_disambiguation(self):
    all_disambiguations = []
    for page in self.pages:
        if page.relations.disambiguation_links_norm is not None:
            all_disambiguations.extend(page.relations.disambiguation_links_norm)
        if page.relations.disambiguation_links is not None:
            all_disambiguations.extend(page.relations.disambiguation_links)
    return set(all_disambiguations)
",if page . relations . disambiguation_links is not None :,113
"def __str__(self, prefix="""", printElemNumber=0):
    res = """"
    cnt = 0
    for e in self.options_:
        elm = """"
        if printElemNumber:
            elm = ""(%d)"" % cnt
        res += prefix + (""options%s <\n"" % elm)
        res += e.__str__(prefix + ""  "", printElemNumber)
        res += prefix + "">\n""
        cnt += 1
    return res
",if printElemNumber :,119
"def pre_save_task(self, task, credentials, verrors):
    if task[""attributes""][""encryption""] not in (None, """", ""AES256""):
        verrors.add(""encryption"", 'Encryption should be null or ""AES256""')
    if not credentials[""attributes""].get(""skip_region"", False):
        if not credentials[""attributes""].get(""region"", """").strip():
            response = await self.middleware.run_in_thread(
                self._get_client(credentials).get_bucket_location,
                Bucket=task[""attributes""][""bucket""],
            )
            task[""attributes""][""region""] = response[""LocationConstraint""] or ""us-east-1""
","if not credentials [ ""attributes"" ] . get ( ""region"" , """" ) . strip ( ) :",167
"def get_best_config_reward(self):
    """"""Returns the best configuration found so far, as well as the reward associated with this best config.""""""
    with self.LOCK:
        if self._results:
            config_pkl = max(self._results, key=self._results.get)
            return pickle.loads(config_pkl), self._results[config_pkl]
        else:
            return dict(), self._reward_while_pending()
",if self . _results :,112
"def parse_setup_cfg(self):
    # type: () -> Dict[STRING_TYPE, Any]
    if self.setup_cfg is not None and self.setup_cfg.exists():
        contents = self.setup_cfg.read_text()
        base_dir = self.setup_cfg.absolute().parent.as_posix()
        try:
            parsed = setuptools_parse_setup_cfg(self.setup_cfg.as_posix())
        except Exception:
            if six.PY2:
                contents = self.setup_cfg.read_bytes()
            parsed = parse_setup_cfg(contents, base_dir)
        if not parsed:
            return {}
        return parsed
    return {}
",if six . PY2 :,183
"def readall(read_fn, sz):
    buff = b""""
    have = 0
    while have < sz:
        chunk = yield from read_fn(sz - have)
        have += len(chunk)
        buff += chunk
        if len(chunk) == 0:
            raise TTransportException(
                TTransportException.END_OF_FILE, ""End of file reading from transport""
            )
    return buff
",if len ( chunk ) == 0 :,111
"def _get_use_previous(
    f,
):  # TODO Sort and group features for DateOffset with two different temporal values
    if isinstance(f, AggregationFeature) and f.use_previous is not None:
        if len(f.use_previous.times.keys()) > 1:
            return ("""", -1)
        else:
            unit = list(f.use_previous.times.keys())[0]
            value = f.use_previous.times[unit]
            return (unit, value)
    else:
        return ("""", -1)
",if len ( f . use_previous . times . keys ( ) ) > 1 :,140
"def istrue(self):
    try:
        return self._istrue()
    except Exception:
        self.exc = sys.exc_info()
        if isinstance(self.exc[1], SyntaxError):
            msg = [
                "" "" * (self.exc[1].offset + 4) + ""^"",
            ]
            msg.append(""SyntaxError: invalid syntax"")
        else:
            msg = traceback.format_exception_only(*self.exc[:2])
        pytest.fail(
            ""Error evaluating %r expression\n""
            ""    %s\n""
            ""%s"" % (self.name, self.expr, ""\n"".join(msg)),
            pytrace=False,
        )
","if isinstance ( self . exc [ 1 ] , SyntaxError ) :",194
"def wait_for_crm_operation(operation, crm):
    """"""Poll for cloud resource manager operation until finished.""""""
    logger.info(
        ""wait_for_crm_operation: ""
        ""Waiting for operation {} to finish..."".format(operation)
    )
    for _ in range(MAX_POLLS):
        result = crm.operations().get(name=operation[""name""]).execute()
        if ""error"" in result:
            raise Exception(result[""error""])
        if ""done"" in result and result[""done""]:
            logger.info(""wait_for_crm_operation: Operation done."")
            break
        time.sleep(POLL_INTERVAL)
    return result
","if ""error"" in result :",173
"def cb_blob_detail_from_elem_and_buf(self, elem, buf):
    if elem.get(""lang"") != buf.lang:  # multi-lang doc
        return ""%s Code in %s"" % (elem.get(""lang""), buf.path)
    else:
        dir, base = os.path.split(buf.path)
        if dir:
            return ""%s (%s)"" % (base, dir)
        else:
            return base
",if dir :,119
"def removedir(self, path):
    # type: (Text) -> None
    _path = self.validatepath(path)
    if _path == ""/"":
        raise errors.RemoveRootError()
    with ftp_errors(self, path):
        try:
            self.ftp.rmd(_encode(_path, self.ftp.encoding))
        except error_perm as error:
            code, _ = _parse_ftp_error(error)
            if code == ""550"":
                if self.isfile(path):
                    raise errors.DirectoryExpected(path)
                if not self.isempty(path):
                    raise errors.DirectoryNotEmpty(path)
            raise  # pragma: no cover
","if code == ""550"" :",189
"def p_clause(self, node, position):
    if isinstance(node, Graph):
        self.subjectDone(node)
        if position is OBJECT:
            self.write("" "")
        self.write(""{"")
        self.depth += 1
        serializer = N3Serializer(node, parent=self)
        serializer.serialize(self.stream)
        self.depth -= 1
        self.write(self.indent() + ""}"")
        return True
    else:
        return False
",if position is OBJECT :,124
"def get_default_shell_info(shell_name=None, settings=None):
    if not shell_name:
        settings = settings or load_settings(lazy=True)
        shell_name = settings.get(""shell"")
        if shell_name:
            return shell_name, None
        shell_path = os.environ.get(""SHELL"")
        if shell_path:
            shell_name = basepath(shell_path)
        else:
            shell_name = DEFAULT_SHELL
        return shell_name, shell_path
    return shell_name, None
",if shell_path :,150
"def GetCategory(self, pidls):
    ret = []
    for pidl in pidls:
        # Why don't we just get the size of the PIDL?
        val = self.sf.GetDetailsEx(pidl, PKEY_Sample_AreaSize)
        val = int(val)  # it probably came in a VT_BSTR variant
        if val < 255 // 3:
            cid = IDS_SMALL
        elif val < 2 * 255 // 3:
            cid = IDS_MEDIUM
        else:
            cid = IDS_LARGE
        ret.append(cid)
    return ret
",elif val < 2 * 255 // 3 :,158
"def Tokenize(s):
    # type: (str) -> Iterator[Token]
    for item in TOKEN_RE.findall(s):
        # The type checker can't know the true type of item!
        item = cast(TupleStr4, item)
        if item[0]:
            typ = ""number""
            val = item[0]
        elif item[1]:
            typ = ""name""
            val = item[1]
        elif item[2]:
            typ = item[2]
            val = item[2]
        elif item[3]:
            typ = item[3]
            val = item[3]
        yield Token(typ, val)
",elif item [ 3 ] :,181
"def add_package_declarations(generated_root_path):
    file_names = os.listdir(generated_root_path)
    for file_name in file_names:
        if not file_name.endswith("".java""):
            continue
        full_name = os.path.join(generated_root_path, file_name)
        add_package(full_name)
","if not file_name . endswith ( "".java"" ) :",93
"def _call_with_retry(out, retry, retry_wait, method, *args, **kwargs):
    for counter in range(retry + 1):
        try:
            return method(*args, **kwargs)
        except (
            NotFoundException,
            ForbiddenException,
            AuthenticationException,
            RequestErrorException,
        ):
            raise
        except ConanException as exc:
            if counter == retry:
                raise
            else:
                if out:
                    out.error(exc)
                    out.info(""Waiting %d seconds to retry..."" % retry_wait)
                time.sleep(retry_wait)
",if counter == retry :,180
"def to_wburl_str(
    url, type=BaseWbUrl.LATEST_REPLAY, mod="""", timestamp="""", end_timestamp=""""
):
    if WbUrl.is_query_type(type):
        tsmod = """"
        if mod:
            tsmod += mod + ""/""
        tsmod += timestamp
        tsmod += ""*""
        tsmod += end_timestamp
        tsmod += ""/"" + url
        if type == BaseWbUrl.URL_QUERY:
            tsmod += ""*""
        return tsmod
    else:
        tsmod = timestamp + mod
        if len(tsmod) > 0:
            return tsmod + ""/"" + url
        else:
            return url
",if mod :,180
"def _configured_ploidy(items):
    ploidies = collections.defaultdict(set)
    for data in items:
        ploidy = dd.get_ploidy(data)
        if isinstance(ploidy, dict):
            for k, v in ploidy.items():
                ploidies[k].add(v)
        else:
            ploidies[""default""].add(ploidy)
    out = {}
    for k, vs in ploidies.items():
        assert len(vs) == 1, ""Multiple ploidies set for group calling: %s %s"" % (
            k,
            list(vs),
        )
        out[k] = vs.pop()
    return out
","if isinstance ( ploidy , dict ) :",187
"def removeUser(self, username):
    hideFromOSD = not constants.SHOW_DIFFERENT_ROOM_OSD
    if username in self._users:
        user = self._users[username]
        if user.room:
            if self.isRoomSame(user.room):
                hideFromOSD = not constants.SHOW_SAME_ROOM_OSD
    if username in self._users:
        self._users.pop(username)
        message = getMessage(""left-notification"").format(username)
        self.ui.showMessage(message, hideFromOSD)
        self._client.lastLeftTime = time.time()
        self._client.lastLeftUser = username
    self.userListChange()
",if user . room :,184
"def _thd_cleanup_instance(self):
    container_name = self.getContainerName()
    instances = self.client.containers(all=1, filters=dict(name=container_name))
    for instance in instances:
        # hyper filtering will match 'hyper12"" if you search for 'hyper1' !
        if """".join(instance[""Names""]).strip(""/"") != container_name:
            continue
        try:
            self.client.remove_container(instance[""Id""], v=True, force=True)
        except NotFound:
            pass  # that's a race condition
        except docker.errors.APIError as e:
            if ""Conflict operation on container"" not in str(e):
                raise
","if """" . join ( instance [ ""Names"" ] ) . strip ( ""/"" ) != container_name :",182
"def handle_ctcp(self, conn, evt):
    args = evt.arguments()
    source = evt.source().split(""!"")[0]
    if args:
        if args[0] == ""VERSION"":
            conn.ctcp_reply(source, ""VERSION "" + BOT_VERSION)
        elif args[0] == ""PING"":
            conn.ctcp_reply(source, ""PING"")
        elif args[0] == ""CLIENTINFO"":
            conn.ctcp_reply(source, ""CLIENTINFO PING VERSION CLIENTINFO"")
","elif args [ 0 ] == ""PING"" :",136
"def new_func(self, *args, **kwargs):
    obj = self.obj_ref()
    attr = self.attr
    if obj is not None:
        args = tuple(TrackedValue.make(obj, attr, arg) for arg in args)
        if kwargs:
            kwargs = {
                key: TrackedValue.make(obj, attr, value)
                for key, value in iteritems(kwargs)
            }
    result = func(self, *args, **kwargs)
    self._changed_()
    return result
",if kwargs :,138
"def add_doc(target, variables, body_lines):
    if isinstance(target, ast.Name):
        # if it is a variable name add it to the doc
        name = target.id
        if name not in variables:
            doc = find_doc_for(target, body_lines)
            if doc is not None:
                variables[name] = doc
    elif isinstance(target, ast.Tuple):
        # if it is a tuple then iterate the elements
        # this can happen like this:
        # a, b = 1, 2
        for e in target.elts:
            add_doc(e, variables, body_lines)
",if name not in variables :,167
"def _terminal_messenger(tp=""write"", msg="""", out=sys.stdout):
    try:
        if tp == ""write"":
            out.write(msg)
        elif tp == ""flush"":
            out.flush()
        elif tp == ""write_flush"":
            out.write(msg)
            out.flush()
        elif tp == ""print"":
            print(msg, file=out)
        else:
            raise ValueError(""Unsupported type: "" + tp)
    except IOError as e:
        logger.critical(""{}: {}"".format(type(e).__name__, ucd(e)))
        pass
","elif tp == ""write_flush"" :",160
"def get_files(d):
    res = []
    for p in glob.glob(os.path.join(d, ""*"")):
        if not p:
            continue
        (pth, fname) = os.path.split(p)
        if skip_file(fname):
            continue
        if os.path.islink(p):
            continue
        if os.path.isdir(p):
            res += get_dir(p)
        else:
            res.append(p)
    return res
",if skip_file ( fname ) :,136
"def _list_outputs(self):
    outputs = super(VolSymm, self)._list_outputs()
    # Have to manually check for the grid files.
    if os.path.exists(outputs[""trans_file""]):
        if ""grid"" in open(outputs[""trans_file""], ""r"").read():
            outputs[""output_grid""] = re.sub(
                "".(nlxfm|xfm)$"", ""_grid_0.mnc"", outputs[""trans_file""]
            )
    return outputs
","if ""grid"" in open ( outputs [ ""trans_file"" ] , ""r"" ) . read ( ) :",126
"def _set_texture(self, texture):
    if texture.id is not self._texture.id:
        self._group = SpriteGroup(
            texture, self._group.blend_src, self._group.blend_dest, self._group.parent
        )
        if self._batch is None:
            self._vertex_list.tex_coords[:] = texture.tex_coords
        else:
            self._vertex_list.delete()
            self._texture = texture
            self._create_vertex_list()
    else:
        self._vertex_list.tex_coords[:] = texture.tex_coords
    self._texture = texture
",if self . _batch is None :,166
"def got_result(result):
    deployment = self.persistence_service.get()
    for node in deployment.nodes:
        if same_node(node, origin):
            dataset_ids = [
                (m.dataset.deleted, m.dataset.dataset_id)
                for m in node.manifestations.values()
            ]
            self.assertIn((True, expected_dataset_id), dataset_ids)
            break
    else:
        self.fail(""Node not found. {}"".format(node.uuid))
","if same_node ( node , origin ) :",138
"def check_result(result, func, arguments):
    if check_warning(result) and (result.value != ReturnCode.WARN_NODATA):
        log.warning(UcanWarning(result, func, arguments))
    elif check_error(result):
        if check_error_cmd(result):
            raise UcanCmdError(result, func, arguments)
        else:
            raise UcanError(result, func, arguments)
    return result
",if check_error_cmd ( result ) :,114
"def _compress_and_sort_bdg_files(out_dir, data):
    for fn in glob.glob(os.path.join(out_dir, ""*bdg"")):
        out_file = fn + "".gz""
        if utils.file_exists(out_file):
            continue
        bedtools = config_utils.get_program(""bedtools"", data)
        with file_transaction(out_file) as tx_out_file:
            cmd = f""sort -k1,1 -k2,2n {fn} | bgzip -c > {tx_out_file}""
            message = f""Compressing and sorting {fn}.""
            do.run(cmd, message)
",if utils . file_exists ( out_file ) :,176
"def kill_members(members, sig, hosts=nodes):
    for member in sorted(members):
        try:
            if ha_tools_debug:
                print(""killing %s"" % member)
            proc = hosts[member][""proc""]
            # Not sure if cygwin makes sense here...
            if sys.platform in (""win32"", ""cygwin""):
                os.kill(proc.pid, signal.CTRL_C_EVENT)
            else:
                os.kill(proc.pid, sig)
        except OSError:
            if ha_tools_debug:
                print(""%s already dead?"" % member)
","if sys . platform in ( ""win32"" , ""cygwin"" ) :",172
"def get_top_level_stats(self):
    for func, (cc, nc, tt, ct, callers) in self.stats.items():
        self.total_calls += nc
        self.prim_calls += cc
        self.total_tt += tt
        if (""jprofile"", 0, ""profiler"") in callers:
            self.top_level[func] = None
        if len(func_std_string(func)) > self.max_name_len:
            self.max_name_len = len(func_std_string(func))
","if ( ""jprofile"" , 0 , ""profiler"" ) in callers :",141
"def __str__(self):
    """"""Only keeps the True values.""""""
    result = [""SlicingSpec(""]
    if self.entire_dataset:
        result.append("" Entire dataset,"")
    if self.by_class:
        if isinstance(self.by_class, Iterable):
            result.append("" Into classes %s,"" % self.by_class)
        elif isinstance(self.by_class, int):
            result.append("" Up to class %d,"" % self.by_class)
        else:
            result.append("" By classes,"")
    if self.by_percentiles:
        result.append("" By percentiles,"")
    if self.by_classification_correctness:
        result.append("" By classification correctness,"")
    result.append("")"")
    return ""\n"".join(result)
","elif isinstance ( self . by_class , int ) :",197
"def save_params(self):
    if self._save_controller:
        if not os.path.exists(self._save_controller):
            os.makedirs(self._save_controller)
        output_dir = self._save_controller
    else:
        if not os.path.exists(""./.rlnas_controller""):
            os.makedirs(""./.rlnas_controller"")
        output_dir = ""./.rlnas_controller""
    with open(os.path.join(output_dir, ""rlnas.params""), ""wb"") as f:
        pickle.dump(self._params_dict, f)
    _logger.debug(""Save params done"")
","if not os . path . exists ( ""./.rlnas_controller"" ) :",166
"def unexport(self, pin):
    with self._lock:
        self._pin_refs[pin] -= 1
        if self._pin_refs[pin] == 0:
            with io.open(self.path(""unexport""), ""wb"") as f:
                f.write(str(pin).encode(""ascii""))
",if self . _pin_refs [ pin ] == 0 :,83
"def emit(self, type, info=None):
    # Overload emit() to send events to the proxy object at the other end
    ev = super().emit(type, info)
    if self._has_proxy is True and self._session.status > 0:
        # implicit: and self._disposed is False:
        if type in self.__proxy_properties__:
            self._session.send_command(""INVOKE"", self._id, ""_emit_at_proxy"", [ev])
        elif type in self.__event_types_at_proxy:
            self._session.send_command(""INVOKE"", self._id, ""_emit_at_proxy"", [ev])
",elif type in self . __event_types_at_proxy :,161
"def __call__(self, params):
    all_errs = {}
    for handler in self.handlers:
        out_headers, res, errs = handler(params)
        all_errs.update(errs)
        if res is not None:
            return out_headers, res, all_errs
    return None, None, all_errs
",if res is not None :,84
"def await_test_end(self):
    iterations = 0
    while True:
        if iterations > 100:
            self.log.debug(""Await: iteration limit reached"")
            return
        status = self.master.get_status()
        if status.get(""status"") == ""ENDED"":
            return
        iterations += 1
        time.sleep(1.0)
",if iterations > 100 :,100
"def _load(self, path: str):
    ds = DataSet()
    with open(path, ""r"", encoding=""utf-8"") as f:
        for line in f:
            line = line.strip()
            if line:
                parts = line.split(""\t"")
                raw_words1 = parts[1]
                raw_words2 = parts[2]
                target = parts[0]
                if raw_words1 and raw_words2 and target:
                    ds.append(
                        Instance(
                            raw_words1=raw_words1, raw_words2=raw_words2, target=target
                        )
                    )
    return ds
",if line :,200
"def avatar_delete(event_id, speaker_id):
    if request.method == ""DELETE"":
        speaker = (
            DataGetter.get_speakers(event_id)
            .filter_by(user_id=login.current_user.id, id=speaker_id)
            .first()
        )
        if speaker:
            speaker.photo = """"
            speaker.small = """"
            speaker.thumbnail = """"
            speaker.icon = """"
            save_to_db(speaker)
            return jsonify({""status"": ""ok""})
        else:
            abort(403)
",if speaker :,162
"def getline(filename, lineno, *args, **kwargs):
    line = py2exe_getline(filename, lineno, *args, **kwargs)
    if not line:
        try:
            with open(filename, ""rb"") as f:
                for i, line in enumerate(f):
                    line = line.decode(""utf-8"")
                    if lineno == i + 1:
                        break
                else:
                    line = """"
        except (IOError, OSError):
            line = """"
    return line
",if lineno == i + 1 :,149
"def write(self, data):
    if not isinstance(data, (bytes, bytearray, memoryview)):
        raise TypeError(""data argument must be byte-ish (%r)"", type(data))
    if not data:
        return
    if self._conn_lost:
        if self._conn_lost >= constants.LOG_THRESHOLD_FOR_CONNLOST_WRITES:
            logger.warning(""socket.send() raised exception."")
        self._conn_lost += 1
        return
    if not self._buffer:
        self._loop.add_writer(self._sock_fd, self._write_ready)
    # Add it to the buffer.
    self._buffer.extend(data)
    self._maybe_pause_protocol()
",if self . _conn_lost >= constants . LOG_THRESHOLD_FOR_CONNLOST_WRITES :,179
"def _get_x_for_y(self, xValue, x, y):
    # print(""searching ""+x+"" with the value ""+str(xValue)+"" and want to give back ""+y)
    if not self.xmlMap:
        return 0
    x_value = str(xValue)
    for anime in self.xmlMap.findall(""anime""):
        try:
            if anime.get(x, False) == x_value:
                return int(anime.get(y, 0))
        except ValueError as e:
            continue
    return 0
","if anime . get ( x , False ) == x_value :",145
"def _RewriteModinfo(
    self,
    modinfo,
    obj_kernel_version,
    this_kernel_version,
    info_strings=None,
    to_remove=None,
):
    new_modinfo = """"
    for line in modinfo.split(""\x00""):
        if not line:
            continue
        if to_remove and line.split(""="")[0] == to_remove:
            continue
        if info_strings is not None:
            info_strings.add(line.split(""="")[0])
        if line.startswith(""vermagic""):
            line = line.replace(obj_kernel_version, this_kernel_version)
        new_modinfo += line + ""\x00""
    return new_modinfo
","if line . startswith ( ""vermagic"" ) :",187
"def _score(self, X, y):
    for col in self.cols:
        # Score the column
        X[col] = X[col].map(self.mapping[col])
        # Randomization is meaningful only for training data -> we do it only if y is present
        if self.randomized and y is not None:
            random_state_generator = check_random_state(self.random_state)
            X[col] = X[col] * random_state_generator.normal(
                1.0, self.sigma, X[col].shape[0]
            )
    return X
",if self . randomized and y is not None :,155
"def onMouseWheel(self, event):
    if self.selectedHuman.isVisible():
        zoomOut = event.wheelDelta > 0
        if self.getSetting(""invertMouseWheel""):
            zoomOut = not zoomOut
        if event.x is not None:
            self.modelCamera.mousePickHumanCenter(event.x, event.y)
        if zoomOut:
            self.zoomOut()
        else:
            self.zoomIn()
","if self . getSetting ( ""invertMouseWheel"" ) :",121
"def prehook(self, emu, op, eip):
    if op in self.badops:
        emu.stopEmu()
        raise v_exc.BadOpBytes(op.va)
    if op.mnem in STOS:
        if self.arch == ""i386"":
            reg = emu.getRegister(envi.archs.i386.REG_EDI)
        elif self.arch == ""amd64"":
            reg = emu.getRegister(envi.archs.amd64.REG_RDI)
        if self.vw.isValidPointer(reg) and self.vw.getLocation(reg) is None:
            self.vw.makePointer(reg, follow=True)
","elif self . arch == ""amd64"" :",186
"def callback(actions, form, tablename=None):
    if actions:
        if tablename and isinstance(actions, dict):
            actions = actions.get(tablename, [])
        if not isinstance(actions, (list, tuple)):
            actions = [actions]
        [action(form) for action in actions]
","if not isinstance ( actions , ( list , tuple ) ) :",80
"def FetchFn(bigger_than_3_only=None, less_than_7_only=None, even_only=None):
    result = []
    for i in range(10):
        # This line introduces a bug.
        if bigger_than_3_only and less_than_7_only and i == 4:
            continue
        if bigger_than_3_only and i <= 3:
            continue
        if less_than_7_only and i >= 7:
            continue
        if even_only and i % 2 != 0:
            continue
        result.append(i)
    return result
",if even_only and i % 2 != 0 :,158
"def set_trial_values(self, trial_id: int, values: Sequence[float]) -> None:
    with self._lock:
        cached_trial = self._get_cached_trial(trial_id)
        if cached_trial is not None:
            self._check_trial_is_updatable(cached_trial)
            updates = self._get_updates(trial_id)
            cached_trial.values = values
            updates.values = values
            return
    self._backend._update_trial(trial_id, values=values)
",if cached_trial is not None :,141
"def _get_label_format(self, workunit):
    for label, label_format in self.LABEL_FORMATTING.items():
        if workunit.has_label(label):
            return label_format
    # Recursively look for a setting to suppress child label formatting.
    if workunit.parent:
        label_format = self._get_label_format(workunit.parent)
        if label_format == LabelFormat.CHILD_DOT:
            return LabelFormat.DOT
        if label_format == LabelFormat.CHILD_SUPPRESS:
            return LabelFormat.SUPPRESS
    return LabelFormat.FULL
",if label_format == LabelFormat . CHILD_SUPPRESS :,151
"def open_session(self, app, request):
    sid = request.cookies.get(app.session_cookie_name)
    if sid:
        stored_session = self.cls.objects(sid=sid).first()
        if stored_session:
            expiration = stored_session.expiration
            if not expiration.tzinfo:
                expiration = expiration.replace(tzinfo=utc)
            if expiration > datetime.datetime.utcnow().replace(tzinfo=utc):
                return MongoEngineSession(
                    initial=stored_session.data, sid=stored_session.sid
                )
    return MongoEngineSession(sid=str(uuid.uuid4()))
",if expiration > datetime . datetime . utcnow ( ) . replace ( tzinfo = utc ) :,174
"def _manage_torrent_cache(self):
    """"""Carry tracker/peer/file lists over to new torrent list""""""
    for torrent in self._torrent_cache:
        new_torrent = rtorrentlib.common.find_torrent(torrent.info_hash, self.torrents)
        if new_torrent is not None:
            new_torrent.files = torrent.files
            new_torrent.peers = torrent.peers
            new_torrent.trackers = torrent.trackers
    self._torrent_cache = self.torrents
",if new_torrent is not None :,142
"def _clean_regions(items, region):
    """"""Intersect region with target file if it exists""""""
    variant_regions = bedutils.population_variant_regions(items, merged=True)
    with utils.tmpfile() as tx_out_file:
        target = subset_variant_regions(variant_regions, region, tx_out_file, items)
        if target:
            if isinstance(target, six.string_types) and os.path.isfile(target):
                target = _load_regions(target)
            else:
                target = [target]
            return target
","if isinstance ( target , six . string_types ) and os . path . isfile ( target ) :",151
"def _get_stdout(self):
    while True:
        BUFFER_SIZE = 1000
        stdout_buffer = self.kernel.process.GetSTDOUT(BUFFER_SIZE)
        if len(stdout_buffer) == 0:
            break
        yield stdout_buffer
",if len ( stdout_buffer ) == 0 :,67
"def do_query(data, q):
    ret = []
    if not q:
        return ret
    qkey = q[0]
    for key, value in iterate(data):
        if len(q) == 1:
            if key == qkey:
                ret.append(value)
            elif is_iterable(value):
                ret.extend(do_query(value, q))
        else:
            if not is_iterable(value):
                continue
            if key == qkey:
                ret.extend(do_query(value, q[1:]))
            else:
                ret.extend(do_query(value, q))
    return ret
",if key == qkey :,185
"def test_expect_setecho_off(self):
    """"""This tests that echo may be toggled off.""""""
    p = pexpect.spawn(""cat"", echo=True, timeout=5)
    try:
        self._expect_echo_toggle(p)
    except IOError:
        if sys.platform.lower().startswith(""sunos""):
            if hasattr(unittest, ""SkipTest""):
                raise unittest.SkipTest(""Not supported on this platform."")
            return ""skip""
        raise
","if sys . platform . lower ( ) . startswith ( ""sunos"" ) :",123
"def _resolve_relative_config(dir, config):
    # Some code shared between Notebook and NotebookInfo
    # Resolve icon, can be relative
    icon = config.get(""icon"")
    if icon:
        if zim.fs.isabs(icon) or not dir:
            icon = File(icon)
        else:
            icon = dir.resolve_file(icon)
    # Resolve document_root, can also be relative
    document_root = config.get(""document_root"")
    if document_root:
        if zim.fs.isabs(document_root) or not dir:
            document_root = Dir(document_root)
        else:
            document_root = dir.resolve_dir(document_root)
    return icon, document_root
",if zim . fs . isabs ( icon ) or not dir :,191
"def _providers(self, descriptor):
    res = []
    for _md in self.metadata.values():
        for ent_id, ent_desc in _md.items():
            if descriptor in ent_desc:
                if ent_id in res:
                    # print(""duplicated entity_id: %s"" % res)
                    pass
                else:
                    res.append(ent_id)
    return res
",if descriptor in ent_desc :,118
"def poll_ms(self, timeout=-1):
    s = bytearray(self.evbuf)
    if timeout >= 0:
        deadline = utime.ticks_add(utime.ticks_ms(), timeout)
    while True:
        n = epoll_wait(self.epfd, s, 1, timeout)
        if not os.check_error(n):
            break
        if timeout >= 0:
            timeout = utime.ticks_diff(deadline, utime.ticks_ms())
            if timeout < 0:
                n = 0
                break
    res = []
    if n > 0:
        vals = struct.unpack(epoll_event, s)
        res.append((vals[1], vals[0]))
    return res
",if timeout < 0 :,192
"def banned():
    if request.endpoint == ""views.themes"":
        return
    if authed():
        user = get_current_user_attrs()
        team = get_current_team_attrs()
        if user and user.banned:
            return (
                render_template(
                    ""errors/403.html"", error=""You have been banned from this CTF""
                ),
                403,
            )
        if team and team.banned:
            return (
                render_template(
                    ""errors/403.html"",
                    error=""Your team has been banned from this CTF"",
                ),
                403,
            )
",if team and team . banned :,193
"def _update_read(self):
    """"""Update state when there is read event""""""
    try:
        msg = bytes(self._sock.recv(4096))
        if msg:
            self.on_message(msg)
            return True
        # normal close, remote is closed
        self.close()
    except socket.error as err:
        if err.args[0] in (errno.EAGAIN, errno.EWOULDBLOCK):
            pass
        else:
            self.on_error(err)
    return False
","if err . args [ 0 ] in ( errno . EAGAIN , errno . EWOULDBLOCK ) :",142
"def update_topic_attr_as_not(modeladmin, request, queryset, attr):
    for topic in queryset:
        if attr == ""sticky"":
            topic.sticky = not topic.sticky
        elif attr == ""closed"":
            topic.closed = not topic.closed
        elif attr == ""hidden"":
            topic.hidden = not topic.hidden
        topic.save()
","elif attr == ""hidden"" :",97
"def Startprobe(self, q):
    while not self.finished:
        try:
            sniff(iface=self.interface, count=10, prn=lambda x: q.put(x))
        except:
            pass
        if self.finished:
            break
",if self . finished :,73
"def _maybe_female(self, path_elements, female, strict):
    if female:
        if self.has_gender_differences:
            elements = path_elements + [""female""]
            try:
                return self._get_file(elements, "".png"", strict=strict)
            except ValueError:
                if strict:
                    raise
        elif strict:
            raise ValueError(""Pokemon %s has no gender differences"" % self.species_id)
    return self._get_file(path_elements, "".png"", strict=strict)
",if self . has_gender_differences :,146
"def change_args_to_dict(string):
    if string is None:
        return None
    ans = []
    strings = string.split(""\n"")
    ind = 1
    start = 0
    while ind <= len(strings):
        if ind < len(strings) and strings[ind].startswith("" ""):
            ind += 1
        else:
            if start < ind:
                ans.append(""\n"".join(strings[start:ind]))
            start = ind
            ind += 1
    d = {}
    for line in ans:
        if "":"" in line and len(line) > 0:
            lines = line.split("":"")
            d[lines[0]] = lines[1].strip()
    return d
","if ind < len ( strings ) and strings [ ind ] . startswith ( "" "" ) :",188
"def _send_with_auth(self, req_kwargs, desired_auth, rsession):
    if desired_auth.oauth:
        if self._oauth_creds.access_token_expired:
            self._oauth_creds.refresh(httplib2.Http())
        req_kwargs[""headers""] = req_kwargs.get(""headers"", {})
        req_kwargs[""headers""][""Authorization""] = (
            ""Bearer "" + self._oauth_creds.access_token
        )
    return rsession.request(**req_kwargs)
",if self . _oauth_creds . access_token_expired :,130
"def parse_search_response(json_data):
    """"""Construct response for any input""""""
    if json_data is None:
        return {""error"": ""Error parsing empty search engine response""}
    try:
        return json.loads(json_data)
    except json.JSONDecodeError:
        logger.exception(""Error parsing search engine response"")
        m = re_pre.search(json_data)
        if m is None:
            return {""error"": ""Error parsing search engine response""}
        error = web.htmlunquote(m.group(1))
        solr_error = ""org.apache.lucene.queryParser.ParseException: ""
        if error.startswith(solr_error):
            error = error[len(solr_error) :]
        return {""error"": error}
",if m is None :,198
"def wrapper(*args, **kws):
    missing = []
    saved = getattr(warnings, ""__warningregistry__"", missing).copy()
    try:
        return func(*args, **kws)
    finally:
        if saved is missing:
            try:
                del warnings.__warningregistry__
            except AttributeError:
                pass
        else:
            warnings.__warningregistry__ = saved
",if saved is missing :,100
"def parse_expression(self):
    """"""Return string containing command to run.""""""
    expression_el = self.root.find(""expression"")
    if expression_el is not None:
        expression_type = expression_el.get(""type"")
        if expression_type != ""ecma5.1"":
            raise Exception(
                ""Unknown expression type [%s] encountered"" % expression_type
            )
        return expression_el.text
    return None
","if expression_type != ""ecma5.1"" :",115
"def test_geocode():
    # look for tweets from New York ; the search radius is larger than NYC
    # so hopefully we'll find one from New York in the first 500?
    count = 0
    found = False
    for tweet in T.search(None, geocode=""40.7484,-73.9857,1mi""):
        if (tweet[""place""] or {}).get(""name"") == ""Manhattan"":
            found = True
            break
        if count > 500:
            break
        count += 1
    assert found
","if ( tweet [ ""place"" ] or { } ) . get ( ""name"" ) == ""Manhattan"" :",138
"def __init__(self, name: Optional[str] = None, order: int = 0):
    if name is None:
        if order == 0:
            name = ""std_dev""
        elif order == 1:
            name = ""sample_std_dev""
        else:
            name = f""std_dev{order})""
    super().__init__(name=name, order=order)
    self.order = order
",if order == 0 :,109
"def __cmp__(self, other):
    if isinstance(other, date) or isinstance(other, datetime):
        a = self._d.getTime()
        b = other._d.getTime()
        if a < b:
            return -1
        elif a == b:
            return 0
    else:
        raise TypeError(""expected date or datetime object"")
    return 1
",if a < b :,98
"def run(self):
    tid = self.ident
    try:
        with self._lock:
            _GUIS[tid] = self
            self._state(True)
        self.new_mail_notifications(summarize=True)
        loop_count = 0
        while self._sock:
            loop_count += 1
            self._select_sleep(1)  # FIXME: Lengthen this when possible
            self.change_state()
            if loop_count % 5 == 0:
                # FIXME: This involves a fair number of set operations,
                #        should only do this after new mail has arrived.
                self.new_mail_notifications()
    finally:
        del _GUIS[tid]
",if loop_count % 5 == 0 :,200
"def __cache_dimension_masks(self, *args):
    # cache masks for each feature map we'll need
    if len(self.masks) == 0:
        for m1 in args:
            batch_size, emb_dim, h, w = m1.size()
            # make mask
            if h not in self.masks:
                mask = self.feat_size_w_mask(h, m1)
                self.masks[h] = mask
",if h not in self . masks :,122
"def __call__(self, *flattened_representation):
    unflattened_representation = []
    for index, subtree in self.children:
        if subtree is None:
            unflattened_representation.append(flattened_representation[index])
        else:
            sub_representation = flattened_representation[index]
            unflattened_representation.append(subtree(*sub_representation))
    return self._cls(*unflattened_representation, **self._kwargs)
",if subtree is None :,109
"def click_outside(event):
    if event not in d:
        x, y, z = self.blockFaceUnderCursor[0]
        if y == 0:
            y = 64
        y += 3
        gotoPanel.X, gotoPanel.Y, gotoPanel.Z = x, y, z
        if event.num_clicks == 2:
            d.dismiss(""Goto"")
",if event . num_clicks == 2 :,100
"def get_mapped_input_keysequences(self, mode=""global"", prefix=u""""):
    # get all bindings in this mode
    globalmaps, modemaps = self.get_keybindings(mode)
    candidates = list(globalmaps.keys()) + list(modemaps.keys())
    if prefix is not None:
        prefixes = prefix + "" ""
        cand = [c for c in candidates if c.startswith(prefixes)]
        if prefix in candidates:
            candidates = cand + [prefix]
        else:
            candidates = cand
    return candidates
",if prefix in candidates :,136
"def _set_length(self, length):
    with self._cond:
        self._length = length
        if self._index == self._length:
            self._ready = True
            self._cond.notify()
            del self._cache[self._job]
",if self . _index == self . _length :,70
"def _pct_encoded_replace_unreserved(mo):
    try:
        i = int(mo.group(1), 16)
        if _unreserved[i]:
            return chr(i)
        else:
            return mo.group().upper()
    except ValueError:
        return mo.group()
",if _unreserved [ i ] :,81
"def is_open(self):
    if self.signup_code:
        return True
    else:
        if self.signup_code_present:
            if self.messages.get(""invalid_signup_code""):
                messages.add_message(
                    self.request,
                    self.messages[""invalid_signup_code""][""level""],
                    self.messages[""invalid_signup_code""][""text""].format(
                        **{
                            ""code"": self.get_code(),
                        }
                    ),
                )
    return settings.ACCOUNT_OPEN_SIGNUP
","if self . messages . get ( ""invalid_signup_code"" ) :",172
"def _get_field_value(self, test, key, match):
    if test.ver == ofproto_v1_0.OFP_VERSION:
        members = inspect.getmembers(match)
        for member in members:
            if member[0] == key:
                field_value = member[1]
            elif member[0] == ""wildcards"":
                wildcards = member[1]
        if key == ""nw_src"":
            field_value = test.nw_src_to_str(wildcards, field_value)
        elif key == ""nw_dst"":
            field_value = test.nw_dst_to_str(wildcards, field_value)
    else:
        field_value = match[key]
    return field_value
",if member [ 0 ] == key :,200
"def move_sender_strings_to_sender_model(apps, schema_editor):
    sender_model = apps.get_model(""documents"", ""Sender"")
    document_model = apps.get_model(""documents"", ""Document"")
    # Create the sender and log the relationship with the document
    for document in document_model.objects.all():
        if document.sender:
            (
                DOCUMENT_SENDER_MAP[document.pk],
                created,
            ) = sender_model.objects.get_or_create(
                name=document.sender, defaults={""slug"": slugify(document.sender)}
            )
",if document . sender :,160
"def compute_output_shape(self, input_shape):
    if None not in input_shape[1:]:
        if keras.backend.image_data_format() == ""channels_first"":
            total = np.prod(input_shape[2:4]) * self.num_anchors
        else:
            total = np.prod(input_shape[1:3]) * self.num_anchors
        return (input_shape[0], total, 4)
    else:
        return (input_shape[0], None, 4)
","if keras . backend . image_data_format ( ) == ""channels_first"" :",135
"def decompress(self, value):
    if value:
        if type(value) == PhoneNumber:
            if value.country_code and value.national_number:
                return [
                    ""+%d"" % value.country_code,
                    national_significant_number(value),
                ]
        else:
            return value.split(""."")
    return [None, """"]
",if value . country_code and value . national_number :,111
"def ignore(self, other):
    if isinstance(other, Suppress):
        if other not in self.ignoreExprs:
            super(ParseElementEnhance, self).ignore(other)
            if self.expr is not None:
                self.expr.ignore(self.ignoreExprs[-1])
    else:
        super(ParseElementEnhance, self).ignore(other)
        if self.expr is not None:
            self.expr.ignore(self.ignoreExprs[-1])
    return self
",if other not in self . ignoreExprs :,129
"def mkdir(self, mode=0o777, parents=False, exist_ok=False):
    if self._closed:
        self._raise_closed()
    if not parents:
        try:
            self._accessor.mkdir(self, mode)
        except FileExistsError:
            if not exist_ok or not self.is_dir():
                raise
    else:
        try:
            self._accessor.mkdir(self, mode)
        except FileExistsError:
            if not exist_ok or not self.is_dir():
                raise
        except OSError as e:
            if e.errno != ENOENT:
                raise
            self.parent.mkdir(parents=True)
            self._accessor.mkdir(self, mode)
",if not exist_ok or not self . is_dir ( ) :,199
"def _mark_lcs(mask, dirs, m, n):
    while m != 0 and n != 0:
        if dirs[m, n] == ""|"":
            m -= 1
            n -= 1
            mask[m] = 1
        elif dirs[m, n] == ""^"":
            m -= 1
        elif dirs[m, n] == ""<"":
            n -= 1
        else:
            raise UnboundLocalError(""Illegal move"")
    return mask
","elif dirs [ m , n ] == ""<"" :",122
"def clean(self, *args, **kwargs):
    data = super().clean(*args, **kwargs)
    if isinstance(data, File):
        filename = data.name
        ext = os.path.splitext(filename)[1]
        ext = ext.lower()
        if ext not in self.ext_whitelist:
            raise forms.ValidationError(_(""Filetype not allowed!""))
    return data
",if ext not in self . ext_whitelist :,97
"def get_doc_object(obj, what=None):
    if what is None:
        if inspect.isclass(obj):
            what = ""class""
        elif inspect.ismodule(obj):
            what = ""module""
        elif callable(obj):
            what = ""function""
        else:
            what = ""object""
    if what == ""class"":
        return SphinxClassDoc(obj, """", func_doc=SphinxFunctionDoc)
    elif what in (""function"", ""method""):
        return SphinxFunctionDoc(obj, """")
    else:
        return SphinxDocString(pydoc.getdoc(obj))
",elif inspect . ismodule ( obj ) :,161
"def apply_pssm(val):
    if val is not None:
        val_c = PSSM_VALUES.get(val, None)
        if val_c is None:
            assert isinstance(
                val, tuple(PSSM_VALUES.values())
            ), ""'store_as' should be one of: %r or an instance of %r not %r"" % (
                tuple(PSSM_VALUES.keys()),
                tuple(PSSM_VALUES.values()),
                val,
            )
            return val
        return val_c()
",if val_c is None :,155
"def read_postmaster_opts(self):
    """"""returns the list of option names/values from postgres.opts, Empty dict if read failed or no file""""""
    result = {}
    try:
        with open(os.path.join(self._postgresql.data_dir, ""postmaster.opts"")) as f:
            data = f.read()
            for opt in data.split('"" ""'):
                if ""="" in opt and opt.startswith(""--""):
                    name, val = opt.split(""="", 1)
                    result[name.strip(""-"")] = val.rstrip('""\n')
    except IOError:
        logger.exception(""Error when reading postmaster.opts"")
    return result
","if ""="" in opt and opt . startswith ( ""--"" ) :",169
"def detect(get_page):
    retval = False
    for vector in WAF_ATTACK_VECTORS:
        page, headers, code = get_page(get=vector)
        retval = (
            re.search(r""F5-TrafficShield"", headers.get(HTTP_HEADER.SERVER, """"), re.I)
            is not None
        )
        retval |= (
            re.search(r""\AASINFO="", headers.get(HTTP_HEADER.SET_COOKIE, """"), re.I)
            is not None
        )
        if retval:
            break
    return retval
",if retval :,159
"def on_task_start(self, task, config):
    for item in config:
        for plugin_name, plugin_config in item.items():
            try:
                thelist = plugin.get(plugin_name, self).get_list(plugin_config)
            except AttributeError:
                raise PluginError(
                    ""Plugin %s does not support list interface"" % plugin_name
                )
            if thelist.immutable:
                raise plugin.PluginError(thelist.immutable)
",if thelist . immutable :,139
"def nq(t):
    p = t[0] if (t and t[0] in ""-+"") else """"
    t = t[len(p) :]
    if t.startswith(""tag:"") or t.startswith(""in:""):
        try:
            raw_tag = session.config.get_tag(t.split("":"")[1])
            if raw_tag and raw_tag.hasattr(slug):
                t = ""in:%s"" % raw_tag.slug
        except (IndexError, KeyError, TypeError):
            pass
    return p + t
",if raw_tag and raw_tag . hasattr ( slug ) :,139
"def _recur_strip(s):
    if is_str(s):
        if bos_token == """":
            return "" "".join(s.strip().split())
        else:
            return "" "".join(s.strip().split()).replace(bos_token + "" "", """")
    else:
        s_ = [_recur_strip(si) for si in s]
        return _maybe_list_to_array(s_, s)
","if bos_token == """" :",110
"def __delitem__(self, key):
    ""Deleting tag[key] deletes all 'key' attributes for the tag.""
    for item in self.attrs:
        if item[0] == key:
            self.attrs.remove(item)
            # We don't break because bad HTML can define the same
            # attribute multiple times.
        self._getAttrMap()
        if self.attrMap.has_key(key):
            del self.attrMap[key]
",if item [ 0 ] == key :,119
"def comment_import_help(init_file, out_file):
    f_out = open(out_file, ""w"")
    output = """"
    updated = False
    with open(init_file, ""r"") as f_in:
        for line in f_in:
            if ""import"" in line and ""_help"" in line and not updated:
                updated = True
                line = ""# "" + line
            output += line
    f_out.write(output)
    f_out.close()
    return updated
","if ""import"" in line and ""_help"" in line and not updated :",136
"def prepare_text(lines):
    out = []
    for s in lines.split(""|""):
        s = s.strip()
        if s.startswith(""/""):
            # line beginning with '/' is in italics
            s = r""{\i1}%s{\i0}"" % s[1:].strip()
        out.append(s)
    return ""\\N"".join(out)
","if s . startswith ( ""/"" ) :",96
"def sqlctx(sc):
    pytest.importorskip(""pyspark"")
    from odo.backends.sparksql import HiveContext
    try:
        yield HiveContext(sc)
    finally:
        dbpath = ""metastore_db""
        logpath = ""derby.log""
        if os.path.exists(dbpath):
            assert os.path.isdir(dbpath)
            shutil.rmtree(dbpath)
        if os.path.exists(logpath):
            assert os.path.isfile(logpath)
            os.remove(logpath)
",if os . path . exists ( dbpath ) :,147
"def _user2dict(self, uid):
    usdict = None
    if uid in self.users:
        usdict = self.users[uid]
        if uid in self.users_info:
            infos = self.users_info[uid]
            for attr in infos:
                usdict[attr[""attr_type""]] = attr[""attr_data""]
        usdict[""uid""] = uid
    return usdict
",if uid in self . users_info :,109
"def _validate_options(self):
    for option in self.options:
        # if value type is bool or int, then we know the options is set
        if not type(self.options[option]) in [bool, int]:
            if self.options.required[option] is True and not self.options[option]:
                if option == Constants.PASSWORD_CLEAR:
                    option = ""password"".upper()
                raise FrameworkException(
                    ""Value required for the '%s' option."" % (option.upper())
                )
    return
","if not type ( self . options [ option ] ) in [ bool , int ] :",148
"def _copy_package_apps(
    local_bin_dir: Path, app_paths: List[Path], suffix: str = """"
) -> None:
    for src_unresolved in app_paths:
        src = src_unresolved.resolve()
        app = src.name
        dest = Path(local_bin_dir / add_suffix(app, suffix))
        if not dest.parent.is_dir():
            mkdir(dest.parent)
        if dest.exists():
            logger.warning(f""{hazard}  Overwriting file {str(dest)} with {str(src)}"")
            dest.unlink()
        if src.exists():
            shutil.copy(src, dest)
",if not dest . parent . is_dir ( ) :,177
"def truncate_seq_pair(tokens_a, tokens_b, max_length):
    """"""Truncates a sequence pair in place to the maximum length.""""""
    # This is a simple heuristic which will always truncate the longer sequence
    # one token at a time. This makes more sense than truncating an equal percent
    # of tokens from each, since if one sequence is very short then each token
    # that's truncated likely contains more information than a longer sequence.
    while True:
        total_length = len(tokens_a) + len(tokens_b)
        if total_length <= max_length:
            break
        if len(tokens_a) > len(tokens_b):
            tokens_a.pop()
        else:
            tokens_b.pop()
",if total_length <= max_length :,185
"def add_channels(cls, voucher, add_channels):
    for add_channel in add_channels:
        channel = add_channel[""channel""]
        defaults = {""currency"": channel.currency_code}
        if ""discount_value"" in add_channel.keys():
            defaults[""discount_value""] = add_channel.get(""discount_value"")
        if ""min_amount_spent"" in add_channel.keys():
            defaults[""min_spent_amount""] = add_channel.get(""min_amount_spent"", None)
        models.VoucherChannelListing.objects.update_or_create(
            voucher=voucher,
            channel=channel,
            defaults=defaults,
        )
","if ""min_amount_spent"" in add_channel . keys ( ) :",176
"def services(self, id=None, name=None):
    for service_dict in self.service_ls(id=id, name=name):
        service_id = service_dict[""ID""]
        service_name = service_dict[""NAME""]
        if not service_name.startswith(self._name_prefix):
            continue
        task_list = self.service_ps(service_id)
        yield DockerService.from_cli(self, service_dict, task_list)
",if not service_name . startswith ( self . _name_prefix ) :,121
"def lll(dirname):
    for name in os.listdir(dirname):
        if name not in (os.curdir, os.pardir):
            full = os.path.join(dirname, name)
            if os.path.islink(full):
                print(name, ""->"", os.readlink(full))
","if name not in ( os . curdir , os . pardir ) :",80
"def convertstore(self, mydict):
    targetheader = self.mypofile.header()
    targetheader.addnote(""extracted from web2py"", ""developer"")
    for source_str in mydict.keys():
        target_str = mydict[source_str]
        if target_str == source_str:
            # a convention with new (untranslated) web2py files
            target_str = u""""
        elif target_str.startswith(u""*** ""):
            # an older convention
            target_str = u""""
        pounit = self.convertunit(source_str, target_str)
        self.mypofile.addunit(pounit)
    return self.mypofile
",if target_str == source_str :,180
"def __init__(self, **kwargs):
    for k, v in kwargs.items():
        setattr(self, k, v)
    self.attempted_charsets = set()
    request = cherrypy.serving.request
    if request.handler is not None:
        # Replace request.handler with self
        if self.debug:
            cherrypy.log(""Replacing request.handler"", ""TOOLS.ENCODE"")
        self.oldhandler = request.handler
        request.handler = self
",if self . debug :,128
"def _fastqc_data_section(self, section_name):
    out = []
    in_section = False
    data_file = os.path.join(self._dir, ""fastqc_data.txt"")
    if os.path.exists(data_file):
        with open(data_file) as in_handle:
            for line in in_handle:
                if line.startswith("">>%s"" % section_name):
                    in_section = True
                elif in_section:
                    if line.startswith("">>END""):
                        break
                    out.append(line.rstrip(""\r\n""))
    return out
",elif in_section :,173
"def bit_length(n):
    try:
        return n.bit_length()
    except AttributeError:
        norm = deflate_long(n, False)
        hbyte = byte_ord(norm[0])
        if hbyte == 0:
            return 1
        bitlen = len(norm) * 8
        while not (hbyte & 0x80):
            hbyte <<= 1
            bitlen -= 1
        return bitlen
",if hbyte == 0 :,118
"def step(self, action):
    """"""Repeat action, sum reward, and max over last observations.""""""
    total_reward = 0.0
    done = None
    for i in range(self._skip):
        obs, reward, done, info = self.env.step(action)
        if i == self._skip - 2:
            self._obs_buffer[0] = obs
        if i == self._skip - 1:
            self._obs_buffer[1] = obs
        total_reward += reward
        if done:
            break
    # Note that the observation on the done=True frame
    # doesn't matter
    max_frame = self._obs_buffer.max(axis=0)
    return max_frame, total_reward, done, info
",if done :,189
"def _sample_translation(reference, max_len):
    translation = reference[:]
    while np.random.uniform() < 0.8 and 1 < len(translation) < max_len:
        trans_len = len(translation)
        ind = np.random.randint(trans_len)
        action = np.random.choice(actions)
        if action == ""deletion"":
            del translation[ind]
        elif action == ""replacement"":
            ind_rep = np.random.randint(trans_len)
            translation[ind] = translation[ind_rep]
        else:
            ind_insert = np.random.randint(trans_len)
            translation.insert(ind, translation[ind_insert])
    return translation
","if action == ""deletion"" :",186
"def group_by_sign(seq, slop=sin(pi / 18), key=lambda x: x):
    sign = None
    subseq = []
    for i in seq:
        ki = key(i)
        if sign is None:
            subseq.append(i)
            if ki != 0:
                sign = ki / abs(ki)
        else:
            subseq.append(i)
            if sign * ki < -slop:
                sign = ki / abs(ki)
                yield subseq
                subseq = [i]
    if subseq:
        yield subseq
",if ki != 0 :,167
"def get_dirlist(_rootdir):
    dirlist = []
    with os.scandir(_rootdir) as rit:
        for entry in rit:
            if not entry.name.startswith(""."") and entry.is_dir():
                dirlist.append(entry.path)
                dirlist += get_dirlist(entry.path)
    return dirlist
","if not entry . name . startswith ( ""."" ) and entry . is_dir ( ) :",91
"def __init__(
    self,
    fixed: MQTTFixedHeader = None,
    variable_header: PublishVariableHeader = None,
    payload=None,
):
    if fixed is None:
        header = MQTTFixedHeader(PUBLISH, 0x00)
    else:
        if fixed.packet_type is not PUBLISH:
            raise HBMQTTException(
                ""Invalid fixed packet type %s for PublishPacket init""
                % fixed.packet_type
            )
        header = fixed
    super().__init__(header)
    self.variable_header = variable_header
    self.payload = payload
",if fixed . packet_type is not PUBLISH :,160
"def get_files(d):
    res = []
    for p in glob.glob(os.path.join(d, ""*"")):
        if not p:
            continue
        (pth, fname) = os.path.split(p)
        if fname == ""output"":
            continue
        if fname == ""PureMVC_Python_1_0"":
            continue
        if fname[-4:] == "".pyc"":  # ehmm.. no.
            continue
        if os.path.isdir(p):
            get_dir(p)
        else:
            res.append(p)
    return res
","if fname == ""output"" :",162
"def reward(self):
    """"""Returns a tuple of sum of raw and processed rewards.""""""
    raw_rewards, processed_rewards = 0, 0
    for ts in self.time_steps:
        # NOTE: raw_reward and processed_reward are None for the first time-step.
        if ts.raw_reward is not None:
            raw_rewards += ts.raw_reward
        if ts.processed_reward is not None:
            processed_rewards += ts.processed_reward
    return raw_rewards, processed_rewards
",if ts . raw_reward is not None :,134
"def _process_file(self, content):
    args = []
    for line in content.splitlines():
        line = line.strip()
        if line.startswith(""-""):
            args.extend(self._split_option(line))
        elif line and not line.startswith(""#""):
            args.append(line)
    return args
","if line . startswith ( ""-"" ) :",83
"def __on_change_button_clicked(self, widget=None):
    """"""compute all primary objects and toggle the 'Change' attribute""""""
    self.change_status = not self.change_status
    for prim_obj, tmp in self.xobjects:
        obj_change = self.top.get_object(""%s_change"" % prim_obj)
        if not obj_change.get_sensitive():
            continue
        self.change_entries[prim_obj].set_val(self.change_status)
        obj_change.set_active(self.change_status)
",if not obj_change . get_sensitive ( ) :,145
"def aiter_cogs(cls) -> AsyncIterator[Tuple[str, str]]:
    yield ""Core"", ""0""
    for _dir in data_manager.cog_data_path().iterdir():
        fpath = _dir / ""settings.json""
        if not fpath.exists():
            continue
        with fpath.open() as f:
            try:
                data = json.load(f)
            except json.JSONDecodeError:
                continue
        if not isinstance(data, dict):
            continue
        cog_name = _dir.stem
        for cog_id, inner in data.items():
            if not isinstance(inner, dict):
                continue
            yield cog_name, cog_id
","if not isinstance ( data , dict ) :",192
"def _verifySubs(self):
    for inst in self.subs:
        if not isinstance(inst, (_Block, _Instantiator, Cosimulation)):
            raise BlockError(_error.ArgType % (self.name,))
        if isinstance(inst, (_Block, _Instantiator)):
            if not inst.modctxt:
                raise BlockError(_error.InstanceError % (self.name, inst.callername))
",if not inst . modctxt :,110
"def _is_xml(accepts):
    if accepts.startswith(b""application/""):
        has_xml = accepts.find(b""xml"")
        if has_xml > 0:
            semicolon = accepts.find(b"";"")
            if semicolon < 0 or has_xml < semicolon:
                return True
    return False
",if semicolon < 0 or has_xml < semicolon :,86
"def _accept_with(cls, orm, target):
    if target is orm.mapper:
        return mapperlib.Mapper
    elif isinstance(target, type):
        if issubclass(target, mapperlib.Mapper):
            return target
        else:
            mapper = _mapper_or_none(target)
            if mapper is not None:
                return mapper
            else:
                return _MapperEventsHold(target)
    else:
        return target
","if issubclass ( target , mapperlib . Mapper ) :",123
"def _get_font_afm(self, prop):
    key = hash(prop)
    font = self.afmfontd.get(key)
    if font is None:
        fname = findfont(prop, fontext=""afm"")
        font = self.afmfontd.get(fname)
        if font is None:
            font = AFM(file(findfont(prop, fontext=""afm"")))
            self.afmfontd[fname] = font
        self.afmfontd[key] = font
    return font
",if font is None :,142
"def __call__(self, groupby):
    normalize_reduction_funcs(self, ndim=groupby.ndim)
    df = groupby
    while df.op.output_types[0] not in (OutputType.dataframe, OutputType.series):
        df = df.inputs[0]
    if self.raw_func == ""size"":
        self.output_types = [OutputType.series]
    else:
        self.output_types = (
            [OutputType.dataframe]
            if groupby.op.output_types[0] == OutputType.dataframe_groupby
            else [OutputType.series]
        )
    if self.output_types[0] == OutputType.dataframe:
        return self._call_dataframe(groupby, df)
    else:
        return self._call_series(groupby, df)
",if groupby . op . output_types [ 0 ] == OutputType . dataframe_groupby,197
"def save(self):
    if self.preferences.get(ENCRYPT_ON_DISK, False):
        if self.encryption_password is not None:
            return self.storage.write(
                self.to_dict(encrypt_password=self.encryption_password)
            )
        elif not self.is_locked:
            log.warning(
                ""Disk encryption requested but no password available for encryption. ""
                ""Resetting encryption preferences and saving wallet in an unencrypted state.""
            )
            self.preferences[ENCRYPT_ON_DISK] = False
    return self.storage.write(self.to_dict())
",if self . encryption_password is not None :,164
"def isValidDateString(config_param_name, value, valid_value):
    try:
        if value == ""DD-MM-YYYY"":
            return value
        day, month, year = value.split(""-"")
        if int(day) < 1 or int(day) > 31:
            raise DateStringValueError(config_param_name, value)
        if int(month) < 1 or int(month) > 12:
            raise DateStringValueError(config_param_name, value)
        if int(year) < 1900 or int(year) > 2013:
            raise DateStringValueError(config_param_name, value)
        return value
    except Exception:
        raise DateStringValueError(config_param_name, value)
",if int ( month ) < 1 or int ( month ) > 12 :,187
"def _capture(self, call_name, data=None, **kwargs):
    if data is None:
        data = self.get_default_context()
    else:
        default_context = self.get_default_context()
        if isinstance(data, dict):
            default_context.update(data)
        else:
            default_context[""extra""][""extra_data""] = data
        data = default_context
    client = self.get_sentry_client()
    return getattr(client, call_name)(data=data, **kwargs)
","if isinstance ( data , dict ) :",139
"def check(input, expected_output=None, expected_ffi_error=False):
    import _cffi_backend
    ffi = _cffi_backend.FFI()
    if not expected_ffi_error:
        ct = ffi.typeof(input)
        assert isinstance(ct, ffi.CType)
        assert ct.cname == (expected_output or input)
    else:
        e = py.test.raises(ffi.error, ffi.typeof, input)
        if isinstance(expected_ffi_error, str):
            assert str(e.value) == expected_ffi_error
","if isinstance ( expected_ffi_error , str ) :",157
"def run(self):
    """"""Process queries from task queue, stop if processor is None.""""""
    while True:
        try:
            processor, iprot, oprot, otrans, callback = self.queue.get()
            if processor is None:
                break
            processor.process(iprot, oprot)
            callback(True, otrans.getvalue())
        except Exception:
            logging.exception(""Exception while processing request"")
            callback(False, """")
",if processor is None :,128
"def search(self, query):
    query = query.strip().lower()
    results = []
    for provider in SidebarItemProvider.all(self.context):
        for item in provider.provide():
            if ""url"" in item:
                search_source = ""$"".join(
                    [item.get(""id"", """"), item.get(""name"", """")]
                ).lower()
                if query in search_source:
                    results.append(
                        {
                            ""title"": item[""name""],
                            ""icon"": item[""icon""],
                            ""url"": item[""url""],
                        }
                    )
    return results
",if query in search_source :,197
"def handle(self) -> None:
    """"""Handles a request ignoring dropped connections.""""""
    try:
        BaseHTTPRequestHandler.handle(self)
    except (ConnectionError, socket.timeout) as e:
        self.connection_dropped(e)
    except Exception as e:
        if self.server.ssl_context is not None and is_ssl_error(e):
            self.log_error(""SSL error occurred: %s"", e)
        else:
            raise
    if self.server.shutdown_signal:
        self.initiate_shutdown()
",if self . server . ssl_context is not None and is_ssl_error ( e ) :,137
"def cdn_url_handler(error, endpoint, kwargs):
    if endpoint == ""cdn"":
        path = kwargs.pop(""path"")
        # cdn = app.config.get('cdn', 'http://cdn.staticfile.org/')
        # cdn = app.config.get('cdn', '//cdnjs.cloudflare.com/ajax/libs/')
        cdn = app.config.get(""cdn"", ""//cdnjscn.b0.upaiyun.com/libs/"")
        return urljoin(cdn, path)
    else:
        exc_type, exc_value, tb = sys.exc_info()
        if exc_value is error:
            reraise(exc_type, exc_value, tb)
        else:
            raise error
",if exc_value is error :,186
"def pairs(self):
    for path in os.listdir(""src""):
        if path == "".svn"":
            continue
        dep = join(""src"", path)
        if isdir(dep):
            continue
        yield dep, join(build_dir, path)
",if isdir ( dep ) :,69
"def get_condition(self):
    """"""Return the condition element's name.""""""
    for child in self.xml:
        if ""{%s}"" % self.namespace in child.tag:
            cond = child.tag.split(""}"", 1)[-1]
            if cond in self.conditions:
                return cond
    return ""not-authorized""
","if ""{%s}"" % self . namespace in child . tag :",85
"def end(self, tag):
    # call the appropriate end tag handler
    try:
        f = self.dispatch[tag]
    except KeyError:
        if "":"" not in tag:
            return  # unknown tag ?
        try:
            f = self.dispatch[tag.split("":"")[-1]]
        except KeyError:
            return  # unknown tag ?
    return f(self, """".join(self._data))
","if "":"" not in tag :",107
"def checkIfSessionCodeExists(self, sessionCode):
    if self.emrtFile:
        sessionsForExperiment = (
            self.emrtFile.root.data_collection.session_meta_data.where(
                ""experiment_id == %d"" % (self.active_experiment_id,)
            )
        )
        sessionCodeMatch = [
            sess for sess in sessionsForExperiment if sess[""code""] == sessionCode
        ]
        if len(sessionCodeMatch) > 0:
            return True
        return False
",if len ( sessionCodeMatch ) > 0 :,145
"def save_bytearray(self, obj):
    if self.proto < 5:
        if not obj:  # bytearray is empty
            self.save_reduce(bytearray, (), obj=obj)
        else:
            self.save_reduce(bytearray, (bytes(obj),), obj=obj)
        return
    n = len(obj)
    if n >= self.framer._FRAME_SIZE_TARGET:
        self._write_large_bytes(BYTEARRAY8 + pack(""<Q"", n), obj)
    else:
        self.write(BYTEARRAY8 + pack(""<Q"", n) + obj)
",if not obj :,151
"def _restore_freeze(self, new):
    size_change = []
    for k, v in six.iteritems(self._freeze_backup):
        newv = new.get(k, [])
        if len(v) != len(newv):
            size_change.append((self._key_name(k), len(v), len(newv)))
    if size_change:
        logger.info(
            ""These collections were modified but restored in {}: {}"".format(
                self._name,
                "", "".join(map(lambda t: ""({}: {}->{})"".format(*t), size_change)),
            )
        )
    restore_collection(self._freeze_backup)
",if len ( v ) != len ( newv ) :,176
"def check_options(self, expr, evaluation, options):
    for key in options:
        if key != ""System`SameTest"":
            if expr is None:
                evaluation.message(""ContainsOnly"", ""optx"", Symbol(key))
            else:
                return evaluation.message(""ContainsOnly"", ""optx"", Symbol(key), expr)
    return None
",if expr is None :,91
"def bundle_directory(self, dirpath):
    """"""Bundle all modules/packages in the given directory.""""""
    dirpath = os.path.abspath(dirpath)
    for nm in os.listdir(dirpath):
        nm = _u(nm)
        if nm.startswith("".""):
            continue
        itempath = os.path.join(dirpath, nm)
        if os.path.isdir(itempath):
            if os.path.exists(os.path.join(itempath, ""__init__.py"")):
                self.bundle_package(itempath)
        elif nm.endswith("".py""):
            self.bundle_module(itempath)
","if os . path . exists ( os . path . join ( itempath , ""__init__.py"" ) ) :",160
"def _read_block(self, size):
    if self._file_end is not None:
        max_size = self._file_end - self._file.tell()
        if size == -1:
            size = max_size
        size = max(min(size, max_size), 0)
    return self._file.read(size)
",if size == - 1 :,88
"def question_mark(self):
    """"""Shows help for this command and it's sub-commands.""""""
    ret = []
    if self.param_help_msg or len(self.subcommands) == 0:
        ret.append(self._quick_help())
    if len(self.subcommands) > 0:
        for k, _ in sorted(self.subcommands.items()):
            command_path, param_help, cmd_help = self._instantiate_subcommand(
                k
            )._quick_help(nested=True)
            if command_path or param_help or cmd_help:
                ret.append((command_path, param_help, cmd_help))
    return (CommandsResponse(STATUS_OK, self.help_formatter(ret)), self.__class__)
",if command_path or param_help or cmd_help :,193
"def list_domains(self, r53, **kwargs):
    marker = None
    domains = []
    while True:
        if marker:
            response = self.wrap_aws_rate_limited_call(r53.list_domains(Marker=marker))
        else:
            response = self.wrap_aws_rate_limited_call(r53.list_domains)
        for domain in response.get(""Domains""):
            domains.append(domain)
        if response.get(""NextPageMarker""):
            marker = response.get(""NextPageMarker"")
        else:
            break
    return domains
","if response . get ( ""NextPageMarker"" ) :",157
"def writer(stream, items):
    sep = """"
    for item in items:
        stream.write(sep)
        sep = "" ""
        if not isinstance(item, str):
            item = str(item)
        if not PY3K:
            if not isinstance(item, unicode):
                item = str(item)
        stream.write(item)
    stream.write(""\n"")
","if not isinstance ( item , unicode ) :",106
"def f(view, s):
    if mode == modes.INTERNAL_NORMAL:
        view.run_command(""toggle_comment"")
        if utils.row_at(self.view, s.a) != utils.row_at(self.view, self.view.size()):
            pt = utils.next_non_white_space_char(view, s.a, white_space="" \t"")
        else:
            pt = utils.next_non_white_space_char(
                view, self.view.line(s.a).a, white_space="" \t""
            )
        return R(pt, pt)
    return s
","if utils . row_at ( self . view , s . a ) != utils . row_at ( self . view , self . view . size ( ) ) :",166
"def _parse_timestamp(value):
    if value:
        match = _TIMESTAMP_PATTERN.match(value)
        if match:
            if match.group(2):
                format = ""%Y-%m-%d %H:%M:%S.%f""
                # use the pattern to truncate the value
                value = match.group()
            else:
                format = ""%Y-%m-%d %H:%M:%S""
            value = datetime.datetime.strptime(value, format)
        else:
            raise Exception('Cannot convert ""{}"" into a datetime'.format(value))
    else:
        value = None
    return value
",if match :,170
"def _compute_log_r(model_trace, guide_trace):
    log_r = MultiFrameTensor()
    stacks = get_plate_stacks(model_trace)
    for name, model_site in model_trace.nodes.items():
        if model_site[""type""] == ""sample"":
            log_r_term = model_site[""log_prob""]
            if not model_site[""is_observed""]:
                log_r_term = log_r_term - guide_trace.nodes[name][""log_prob""]
            log_r.add((stacks[name], log_r_term.detach()))
    return log_r
","if not model_site [ ""is_observed"" ] :",162
"def get_translationproject(self):
    """"""returns the translation project belonging to this directory.""""""
    if self.is_language() or self.is_project():
        return None
    else:
        if self.is_translationproject():
            return self.translationproject
        else:
            aux_dir = self
            while not aux_dir.is_translationproject() and aux_dir.parent is not None:
                aux_dir = aux_dir.parent
            return aux_dir.translationproject
",if self . is_translationproject ( ) :,130
"def get_hosted_content():
    try:
        scheme, rest = target.split(""://"", 1)
        prefix, host_and_port = rest.split("".interactivetool."")
        faked_host = rest
        if ""/"" in rest:
            faked_host = rest.split(""/"", 1)[0]
        url = ""%s://%s"" % (scheme, host_and_port)
        response = requests.get(url, timeout=1, headers={""Host"": faked_host})
        return response.text
    except Exception as e:
        print(e)
        return None
","if ""/"" in rest :",148
"def install(self):
    log.info(self.openssl_cli)
    if not self.has_openssl or self.args.force:
        if not self.has_src:
            self._download_src()
        else:
            log.debug(""Already has src {}"".format(self.src_file))
        self._unpack_src()
        self._build_src()
        self._make_install()
    else:
        log.info(""Already has installation {}"".format(self.install_dir))
    # validate installation
    version = self.openssl_version
    if self.version not in version:
        raise ValueError(version)
",if not self . has_src :,162
"def format(self, formatstr):
    pieces = []
    for i, piece in enumerate(re_formatchars.split(force_text(formatstr))):
        if i % 2:
            pieces.append(force_text(getattr(self, piece)()))
        elif piece:
            pieces.append(re_escaped.sub(r""\1"", piece))
    return """".join(pieces)
",elif piece :,99
"def get_current_events_users(calendar):
    now = timezone.make_aware(datetime.now(), timezone.get_current_timezone())
    result = []
    day = Day(calendar.events.all(), now)
    for o in day.get_occurrences():
        if o.start <= now <= o.end:
            usernames = o.event.title.split("","")
            for username in usernames:
                result.append(User.objects.get(username=username.strip()))
    return result
",if o . start <= now <= o . end :,128
"def from_cfn_params(self, cfn_params):
    """"""Initialize param value by parsing CFN input only if the scheduler is awsbatch.""""""
    cfn_converter = self.definition.get(""cfn_param_mapping"", None)
    if cfn_converter and cfn_params:
        if get_cfn_param(cfn_params, ""Scheduler"") == ""awsbatch"":
            # we have the same CFN input parameters for both spot_price and spot_bid_percentage
            # so the CFN input could be a float
            self.value = int(float(get_cfn_param(cfn_params, cfn_converter)))
    return self
","if get_cfn_param ( cfn_params , ""Scheduler"" ) == ""awsbatch"" :",165
"def onCompletion(self, text):
    res = []
    for l in text.split(""\n""):
        if not l:
            continue
        l = l.split("":"")
        if len(l) != 2:
            continue
        res.append([l[0].strip(), l[1].strip()])
    self.panel.setChapters(res)
",if not l :,93
"def update_ranges(l, i):
    for _range in l:
        # most common case: extend a range
        if i == _range[0] - 1:
            _range[0] = i
            merge_ranges(l)
            return
        elif i == _range[1] + 1:
            _range[1] = i
            merge_ranges(l)
            return
    # somewhere outside of range proximity
    l.append([i, i])
    l.sort(key=lambda x: x[0])
",elif i == _range [ 1 ] + 1 :,145
"def process_dollar(token, state, command_line):
    if not state.is_range_start_line_parsed:
        if command_line.line_range.start:
            raise ValueError(""bad range: {0}"".format(state.scanner.state.source))
        command_line.line_range.start.append(token)
    else:
        if command_line.line_range.end:
            raise ValueError(""bad range: {0}"".format(state.scanner.state.source))
        command_line.line_range.end.append(token)
    return parse_line_ref, command_line
",if command_line . line_range . start :,156
"def _parse_description(self, text: str):
    result = dict(links=[], versions=[])
    for line in text.splitlines():
        clean = REX_TAG.sub("""", line.strip())
        if clean.startswith(""Severity:""):
            result[""severity""] = clean.split()[1]
            continue
        if clean.startswith(""Affects:""):
            result[""name""] = clean.split()[1]
            continue
        if "" or higher"" in clean:
            result[""versions""] = self._get_versions(clean)
        result[""links""].extend(REX_LINK.findall(line))
    return result
","if clean . startswith ( ""Severity:"" ) :",162
"def apply(self, chart, grammar):
    for prod in grammar.productions(empty=True):
        for index in compat.xrange(chart.num_leaves() + 1):
            new_edge = TreeEdge.from_production(prod, index)
            if chart.insert(new_edge, ()):
                yield new_edge
","if chart . insert ( new_edge , ( ) ) :",83
"def calc(self, arg):
    op = arg[""op""]
    if op == ""C"":
        self.clear()
        return str(self.current)
    num = decimal.Decimal(arg[""num""])
    if self.op:
        if self.op == ""+"":
            self.current += num
        elif self.op == ""-"":
            self.current -= num
        elif self.op == ""*"":
            self.current *= num
        elif self.op == ""/"":
            self.current /= num
        self.op = op
    else:
        self.op = op
        self.current = num
    res = str(self.current)
    if op == ""="":
        self.clear()
    return res
","if self . op == ""+"" :",187
"def cascade(self, event=None):
    """"""Cascade all Leo windows.""""""
    x, y, delta = 50, 50, 50
    for frame in g.app.windowList:
        w = frame and frame.top
        if w:
            r = w.geometry()  # a Qt.Rect
            # 2011/10/26: Fix bug 823601: cascade-windows fails.
            w.setGeometry(QtCore.QRect(x, y, r.width(), r.height()))
            # Compute the new offsets.
            x += 30
            y += 30
            if x > 200:
                x = 10 + delta
                y = 40 + delta
                delta += 10
",if x > 200 :,190
"def redirect(self):
    c = self.c
    if c.config.getBool(""eval-redirect""):
        self.old_stderr = g.stdErrIsRedirected()
        self.old_stdout = g.stdOutIsRedirected()
        if not self.old_stderr:
            g.redirectStderr()
        if not self.old_stdout:
            g.redirectStdout()
",if not self . old_stderr :,103
"def on_event(self, c, button, data):
    if self.rvGestureGrab.get_reveal_child():
        if button == ""A"" and data[0] == 0:
            self.use()
        elif button == ""Y"" and data[0] == 0:
            self.start_over()
","if button == ""A"" and data [ 0 ] == 0 :",83
"def __init__(self, in_feats, out_feats, norm=""both"", bias=True, activation=None):
    super(DenseGraphConv, self).__init__()
    self._in_feats = in_feats
    self._out_feats = out_feats
    self._norm = norm
    with self.name_scope():
        self.weight = self.params.get(
            ""weight"",
            shape=(in_feats, out_feats),
            init=mx.init.Xavier(magnitude=math.sqrt(2.0)),
        )
        if bias:
            self.bias = self.params.get(""bias"", shape=(out_feats,), init=mx.init.Zero())
        else:
            self.bias = None
        self._activation = activation
",if bias :,195
"def _import_top_module(self, name):
    # scan sys.path looking for a location in the filesystem that contains
    # the module, or an Importer object that can import the module.
    for item in sys.path:
        if isinstance(item, _StringType):
            module = self.fs_imp.import_from_dir(item, name)
        else:
            module = item.import_top(name)
        if module:
            return module
    return None
",if module :,124
"def resolver(schemas, f):
    if not callable(f):
        return
    if not hasattr(f, ""accepts""):
        return
    new_params = []
    for p in f.accepts:
        if isinstance(p, (Patch, Ref, Attribute)):
            new_params.append(p.resolve(schemas))
        else:
            raise ResolverError(""Invalid parameter definition {0}"".format(p))
    # FIXME: for some reason assigning params (f.accepts = new_params) does not work
    f.accepts.clear()
    f.accepts.extend(new_params)
","if isinstance ( p , ( Patch , Ref , Attribute ) ) :",153
"def get_files(d):
    res = []
    for p in glob.glob(os.path.join(d, ""*"")):
        if not p:
            continue
        (pth, fname) = os.path.split(p)
        if fname == ""output"":
            continue
        if fname == ""PureMVC_Python_1_0"":
            continue
        if fname[-4:] == "".pyc"":  # ehmm.. no.
            continue
        if os.path.isdir(p):
            get_dir(p)
        else:
            res.append(p)
    return res
","if fname == ""PureMVC_Python_1_0"" :",162
"def _addRightnames(groups, kerning, leftname, rightnames, includeAll=True):
    if leftname in kerning:
        for rightname in kerning[leftname]:
            if rightname[0] == ""@"":
                for rightname2 in groups[rightname]:
                    rightnames.add(rightname2)
                    if not includeAll:
                        # TODO: in this case, pick the one rightname that has the highest
                        # ranking in glyphorder
                        break
            else:
                rightnames.add(rightname)
",if not includeAll :,161
"def migrate_Stats(self):
    for old_obj in self.session_old.query(self.model_from[""Stats""]):
        if not old_obj.summary:
            self.entries_count[""Stats""] -= 1
            continue
        new_obj = self.model_to[""Stats""]()
        for key in new_obj.__table__.columns._data.keys():
            if key not in old_obj.__table__.columns:
                continue
            setattr(new_obj, key, getattr(old_obj, key))
        self.session_new.add(new_obj)
",if not old_obj . summary :,152
"def _readenv(var, msg):
    match = _ENV_VAR_PAT.match(var)
    if match and match.groups():
        envvar = match.groups()[0]
        if envvar in os.environ:
            value = os.environ[envvar]
            if six.PY2:
                value = value.decode(""utf8"")
            return value
        else:
            raise InvalidConfigException(
                ""{} - environment variable '{}' not set"".format(msg, var)
            )
    else:
        raise InvalidConfigException(
            ""{} - environment variable name '{}' does not match pattern '{}'"".format(
                msg, var, _ENV_VAR_PAT_STR
            )
        )
",if envvar in os . environ :,190
"def __next__(self):
    self._parse_reset()
    while True:
        try:
            line = next(self.input_iter)
        except StopIteration:
            # End of input OR exception
            if len(self.field) > 0:
                raise Error(""newline inside string"")
            raise
        self.line_num += 1
        if ""\0"" in line:
            raise Error(""line contains NULL byte"")
        pos = 0
        while pos < len(line):
            pos = self._parse_process_char(line, pos)
        self._parse_eol()
        if self.state == self.START_RECORD:
            break
    fields = self.fields
    self.fields = []
    return fields
",if len ( self . field ) > 0 :,198
"def createFields(self):
    while self.current_size < self.size:
        pos = self.stream.searchBytes(
            ""\0\0\1"", self.current_size, self.current_size + 1024 * 1024 * 8
        )  # seek forward by at most 1MB
        if pos is not None:
            padsize = pos - self.current_size
            if padsize:
                yield PaddingBytes(self, ""pad[]"", padsize // 8)
        chunk = Chunk(self, ""chunk[]"")
        try:
            # force chunk to be processed, so that CustomFragments are complete
            chunk[""content/data""]
        except:
            pass
        yield chunk
",if pos is not None :,184
"def spew():
    seenUID = False
    start()
    for part in query:
        if part.type == ""uid"":
            seenUID = True
        if part.type == ""body"":
            yield self.spew_body(part, id, msg, write, flush)
        else:
            f = getattr(self, ""spew_"" + part.type)
            yield f(id, msg, write, flush)
        if part is not query[-1]:
            space()
    if uid and not seenUID:
        space()
        yield self.spew_uid(id, msg, write, flush)
    finish()
    flush()
","if part . type == ""body"" :",174
"def _limit_value(key, value, config):
    if config[key].get(""upper_limit""):
        limit = config[key][""upper_limit""]
        # auto handle datetime
        if isinstance(value, datetime) and isinstance(limit, timedelta):
            if config[key][""inverse""] is True:
                if (datetime.now() - limit) > value:
                    value = datetime.now() - limit
            else:
                if (datetime.now() + limit) < value:
                    value = datetime.now() + limit
        elif value > limit:
            value = limit
    return value
",if ( datetime . now ( ) + limit ) < value :,164
"def _fix_var_naming(operators, names, mod=""input""):
    new_names = []
    map = {}
    for op in operators:
        if mod == ""input"":
            iter = op.inputs
        else:
            iter = op.outputs
        for i in iter:
            for name in names:
                if i.raw_name == name and name not in map:
                    map[i.raw_name] = i.full_name
        if len(map) == len(names):
            break
    for name in names:
        new_names.append(map[name])
    return new_names
",if len ( map ) == len ( names ) :,168
"def traverse(tree):
    """"""Generator dropping comment nodes""""""
    for entry in tree:
        # key, values = entry
        spaceless = [e for e in entry if not nginxparser.spacey(e)]
        if spaceless:
            key = spaceless[0]
            values = spaceless[1] if len(spaceless) > 1 else None
        else:
            key = values = """"
        if isinstance(key, list):
            new = copy.deepcopy(entry)
            new[1] = filter_comments(values)
            yield new
        else:
            if key != ""#"" and spaceless:
                yield spaceless
","if key != ""#"" and spaceless :",173
"def mergeCombiners(self, x, y):
    for item in y:
        if len(x) < self.heap_limit:
            self.heap.push(x, item)
        else:
            self.heap.push_pop(x, item)
    return x
",if len ( x ) < self . heap_limit :,73
"def test_scatter(self, harness: primitive_harness.Harness):
    f_name = harness.params[""f_lax""].__name__
    dtype = harness.params[""dtype""]
    if jtu.device_under_test() == ""tpu"":
        if dtype is np.complex64 and f_name in [""scatter_min"", ""scatter_max""]:
            raise unittest.SkipTest(f""TODO: complex {f_name} on TPU fails in JAX"")
    self.ConvertAndCompare(harness.dyn_fun, *harness.dyn_args_maker(self.rng()))
","if dtype is np . complex64 and f_name in [ ""scatter_min"" , ""scatter_max"" ] :",148
"def TryMerge(self, decoder):
    while decoder.avail() > 0:
        tag = decoder.getVarInt32()
        if tag == TAG_BEGIN_ITEM_GROUP:
            (type_id, message) = Item.Decode(decoder)
            if type_id in self.items:
                self.items[type_id].MergeFrom(Item(message))
            else:
                self.items[type_id] = Item(message)
            continue
        if tag == 0:
            raise ProtocolBuffer.ProtocolBufferDecodeError
        decoder.skipData(tag)
",if type_id in self . items :,154
"def process_continuations(lines):
    global continuation_pattern
    olines = []
    while len(lines) != 0:
        line = no_comments(lines[0])
        line = line.strip()
        lines.pop(0)
        if line == """":
            continue
        if continuation_pattern.search(line):
            # combine this line with the next line if the next line exists
            line = continuation_pattern.sub("""", line)
            if len(lines) >= 1:
                combined_lines = [line + lines[0]]
                lines.pop(0)
                lines = combined_lines + lines
                continue
        olines.append(line)
    del lines
    return olines
",if continuation_pattern . search ( line ) :,193
"def _getListNextPackagesReadyToBuild():
    for pkg in Scheduler.listOfPackagesToBuild:
        if pkg in Scheduler.listOfPackagesCurrentlyBuilding:
            continue
        if constants.rpmCheck or Scheduler._checkNextPackageIsReadyToBuild(pkg):
            Scheduler.listOfPackagesNextToBuild.put((-Scheduler._getPriority(pkg), pkg))
            Scheduler.logger.debug(""Adding "" + pkg + "" to the schedule list"")
",if constants . rpmCheck or Scheduler . _checkNextPackageIsReadyToBuild ( pkg ) :,113
"def process_signature(app, what, name, obj, options, signature, return_annotation):
    if signature:
        # replace Mock function names
        signature = re.sub(""<Mock name='([^']+)'.*>"", ""\g<1>"", signature)
        signature = re.sub(""tensorflow"", ""tf"", signature)
        # add scope name to layer signatures:
        if hasattr(obj, ""use_scope""):
            if obj.use_scope:
                signature = signature[0] + ""variable_scope_name, "" + signature[1:]
            elif obj.use_scope is None:
                signature = signature[0] + ""[variable_scope_name,] "" + signature[1:]
    # signature: arg list
    return signature, return_annotation
",elif obj . use_scope is None :,188
"def find_distribution_modules(name=__name__, file=__file__):
    current_dist_depth = len(name.split(""."")) - 1
    current_dist = os.path.join(
        os.path.dirname(file), *([os.pardir] * current_dist_depth)
    )
    abs = os.path.abspath(current_dist)
    dist_name = os.path.basename(abs)
    for dirpath, dirnames, filenames in os.walk(abs):
        package = (dist_name + dirpath[len(abs) :]).replace(""/"", ""."")
        if ""__init__.py"" in filenames:
            yield package
            for filename in filenames:
                if filename.endswith("".py"") and filename != ""__init__.py"":
                    yield ""."".join([package, filename])[:-3]
","if ""__init__.py"" in filenames :",198
"def transform_value(i, v, *args):
    if i not in converter_functions:
        # no converter defined on this field, return value as-is
        return v
    else:
        try:
            return converter_functions[i](v, *args)
        except Exception as e:
            if failonerror == ""inline"":
                return e
            elif failonerror:
                raise e
            else:
                return errorvalue
",elif failonerror :,126
"def _get_file(self):
    if self._file is None:
        self._file = SpooledTemporaryFile(
            max_size=self._storage.max_memory_size,
            suffix="".S3Boto3StorageFile"",
            dir=setting(""FILE_UPLOAD_TEMP_DIR""),
        )
        if ""r"" in self._mode:
            self._is_dirty = False
            self.obj.download_fileobj(self._file)
            self._file.seek(0)
        if self._storage.gzip and self.obj.content_encoding == ""gzip"":
            self._file = GzipFile(mode=self._mode, fileobj=self._file, mtime=0.0)
    return self._file
","if ""r"" in self . _mode :",184
"def connect(self, host, port, timeout):
    fp = Telnet()
    for i in range(50):
        try:
            fp.sock = socket.create_connection(
                (host, int(port)), timeout=int(timeout), source_address=("""", 1023 - i)
            )
            break
        except socket.error as e:
            if (e.errno, e.strerror) != (98, ""Address already in use""):
                raise e
    self.need_handshake = True
    return TCP_Connection(fp)
","if ( e . errno , e . strerror ) != ( 98 , ""Address already in use"" ) :",145
"def filtercomments(source):
    """"""NOT USED: strips trailing comments and put them at the top.""""""
    trailing_comments = []
    comment = True
    while comment:
        if re.search(r""^\s*\/\*"", source):
            comment = source[0, source.index(""*/"") + 2]
        elif re.search(r""^\s*\/\/"", source):
            comment = re.search(r""^\s*\/\/"", source).group(0)
        else:
            comment = None
        if comment:
            source = re.sub(r""^\s+"", """", source[len(comment) :])
            trailing_comments.append(comment)
    return ""\n"".join(trailing_comments) + source
","elif re . search ( r""^\s*\/\/"" , source ) :",179
"def yview(self, mode=None, value=None, units=None):
    if type(value) == str:
        value = float(value)
    if mode is None:
        return self.vsb.get()
    elif mode == ""moveto"":
        frameHeight = self.innerframe.winfo_reqheight()
        self._startY = value * float(frameHeight)
    else:  # mode == 'scroll'
        clipperHeight = self._clipper.winfo_height()
        if units == ""units"":
            jump = int(clipperHeight * self._jfraction)
        else:
            jump = clipperHeight
        self._startY = self._startY + value * jump
    self.reposition()
","if units == ""units"" :",181
"def visit(stmt):
    """"""Collect information about VTCM buffers and their alignments.""""""
    if isinstance(stmt, tvm.tir.AttrStmt):
        if stmt.attr_key == ""storage_scope"" and stmt.value == ""local.vtcm"":
            vtcm_buffers.append(stmt.node)
        elif stmt.attr_key == ""storage_alignment"":
            if not stmt.node in alignments:
                alignments[stmt.node] = []
            alignments[stmt.node].append(stmt.value)
","elif stmt . attr_key == ""storage_alignment"" :",133
"def cost(P):
    # wda loss
    loss_b = 0
    loss_w = 0
    for i, xi in enumerate(xc):
        xi = np.dot(xi, P)
        for j, xj in enumerate(xc[i:]):
            xj = np.dot(xj, P)
            M = dist(xi, xj)
            G = sinkhorn(wc[i], wc[j + i], M, reg, k)
            if j == 0:
                loss_w += np.sum(G * M)
            else:
                loss_b += np.sum(G * M)
    # loss inversed because minimization
    return loss_w / loss_b
",if j == 0 :,187
"def __init__(self, comm, in_channels, out_channels, ksize, pad=1):
    super(Block, self).__init__()
    with self.init_scope():
        if comm.size <= in_channels:
            self.conv = ParallelConvolution2D(
                comm, in_channels, out_channels, ksize, pad=pad, nobias=True
            )
        else:
            self.conv = chainer.links.Convolution2D(
                in_channels, out_channels, ksize, pad=pad, nobias=True
            )
        self.bn = L.BatchNormalization(out_channels)
",if comm . size <= in_channels :,164
"def halfMultipartScore(nzb_name):
    try:
        wrong_found = 0
        for nr in [1, 2, 3, 4, 5, ""i"", ""ii"", ""iii"", ""iv"", ""v"", ""a"", ""b"", ""c"", ""d"", ""e""]:
            for wrong in [""cd"", ""part"", ""dis"", ""disc"", ""dvd""]:
                if ""%s%s"" % (wrong, nr) in nzb_name.lower():
                    wrong_found += 1
        if wrong_found == 1:
            return -30
        return 0
    except:
        log.error(""Failed doing halfMultipartScore: %s"", traceback.format_exc())
    return 0
","if ""%s%s"" % ( wrong , nr ) in nzb_name . lower ( ) :",183
"def should_include(service):
    for f in filt:
        if f == ""status"":
            state = filt[f]
            containers = project.containers([service.name], stopped=True)
            if not has_container_with_state(containers, state):
                return False
        elif f == ""source"":
            source = filt[f]
            if source == ""image"" or source == ""build"":
                if source not in service.options:
                    return False
            else:
                raise UserError(""Invalid value for source filter: %s"" % source)
        else:
            raise UserError(""Invalid filter: %s"" % f)
    return True
","if f == ""status"" :",184
"def get_blob_type_declaration_sql(self, column):
    length = column.get(""length"")
    if length:
        if length <= self.LENGTH_LIMIT_TINYBLOB:
            return ""TINYBLOB""
        if length <= self.LENGTH_LIMIT_BLOB:
            return ""BLOB""
        if length <= self.LENGTH_LIMIT_MEDIUMBLOB:
            return ""MEDIUMBLOB""
    return ""LONGBLOB""
",if length <= self . LENGTH_LIMIT_BLOB :,115
"def click_outside(event):
    if event not in d:
        x, y, z = self.blockFaceUnderCursor[0]
        if y == 0:
            y = 64
        y += 3
        gotoPanel.X, gotoPanel.Y, gotoPanel.Z = x, y, z
        if event.num_clicks == 2:
            d.dismiss(""Goto"")
",if y == 0 :,100
"def check_related_active_jobs(self, obj):
    active_jobs = obj.get_active_jobs()
    if len(active_jobs) > 0:
        raise ActiveJobConflict(active_jobs)
    time_cutoff = now() - dateutil.relativedelta.relativedelta(minutes=1)
    recent_jobs = obj._get_related_jobs().filter(finished__gte=time_cutoff)
    for unified_job in recent_jobs.get_real_instances():
        if not unified_job.event_processing_finished:
            raise PermissionDenied(
                _(""Related job {} is still processing events."").format(
                    unified_job.log_format
                )
            )
",if not unified_job . event_processing_finished :,176
"def run(self):
    self.alive = True
    if _log.isEnabledFor(_DEBUG):
        _log.debug(""started"")
    while self.alive:
        task = self.queue.get()
        if task:
            function, args, kwargs = task
            assert function
            try:
                function(*args, **kwargs)
            except:
                _log.exception(""calling %s"", function)
    if _log.isEnabledFor(_DEBUG):
        _log.debug(""stopped"")
",if task :,134
"def update_sysconfig_file(fn, adjustments, allow_empty=False):
    if not adjustments:
        return
    (exists, contents) = read_sysconfig_file(fn)
    updated_am = 0
    for (k, v) in adjustments.items():
        if v is None:
            continue
        v = str(v)
        if len(v) == 0 and not allow_empty:
            continue
        contents[k] = v
        updated_am += 1
    if updated_am:
        lines = [
            str(contents),
        ]
        if not exists:
            lines.insert(0, util.make_header())
        util.write_file(fn, ""\n"".join(lines) + ""\n"", 0o644)
",if len ( v ) == 0 and not allow_empty :,198
"def wrapper(  # type: ignore
    self: RequestHandler, *args, **kwargs
) -> Optional[Awaitable[None]]:
    if self.request.path.endswith(""/""):
        if self.request.method in (""GET"", ""HEAD""):
            uri = self.request.path.rstrip(""/"")
            if uri:  # don't try to redirect '/' to ''
                if self.request.query:
                    uri += ""?"" + self.request.query
                self.redirect(uri, permanent=True)
                return None
        else:
            raise HTTPError(404)
    return method(self, *args, **kwargs)
",if uri :,163
"def output_handles_from_execution_plan(execution_plan):
    output_handles_for_current_run = set()
    for step_level in execution_plan.execution_step_levels():
        for step in step_level:
            for step_input in step.step_inputs:
                if step_input.source_handles:
                    output_handles_for_current_run.update(step_input.source_handles)
    return output_handles_for_current_run
",if step_input . source_handles :,124
"def _read_value(self, item):
    item = _normalize_path(item)
    if item in self._store:
        if item in self._expire_time and self._expire_time[item] < datetime.now():
            del self._store[item]
            raise KeyError(item)
        return PathResult(item, value=self._store[item])
    elif item in self._children:
        return PathResult(item, dir=True)
    else:
        raise KeyError(item)
",if item in self . _expire_time and self . _expire_time [ item ] < datetime . now ( ) :,125
"def _line_ranges(statements, lines):
    """"""Produce a list of ranges for `format_lines`.""""""
    statements = sorted(statements)
    lines = sorted(lines)
    pairs = []
    start = None
    lidx = 0
    for stmt in statements:
        if lidx >= len(lines):
            break
        if stmt == lines[lidx]:
            lidx += 1
            if not start:
                start = stmt
            end = stmt
        elif start:
            pairs.append((start, end))
            start = None
    if start:
        pairs.append((start, end))
    return pairs
",if not start :,167
"def _update_help_obj_params(help_obj, data_params, params_equal, attr_key_tups):
    loaded_params = []
    for param_obj in help_obj.parameters:
        loaded_param = next(
            (n for n in data_params if params_equal(param_obj, n)), None
        )
        if loaded_param:
            BaseHelpLoader._update_obj_from_data_dict(
                param_obj, loaded_param, attr_key_tups
            )
        loaded_params.append(param_obj)
    help_obj.parameters = loaded_params
",if loaded_param :,159
"def __get_ratio(self):
    """"""Return splitter ratio of the main splitter.""""""
    c = self.c
    free_layout = c.free_layout
    if free_layout:
        w = free_layout.get_main_splitter()
        if w:
            aList = w.sizes()
            if len(aList) == 2:
                n1, n2 = aList
                # 2017/06/07: guard against division by zero.
                ratio = 0.5 if n1 + n2 == 0 else float(n1) / float(n1 + n2)
                return ratio
    return 0.5
",if w :,170
"def _check_required_env_variables(vars):
    for var in vars:
        if not os.environ.get(var):
            self.tc.logger.error(
                ""%s is not set. Did you forget to source your build environment setup script?""
                % var
            )
            raise OEQAPreRun
",if not os . environ . get ( var ) :,89
"def clean_indexes():
    for coll_name in mongo.collection_types.keys():
        coll = mongo.get_collection(coll_name)
        indexes = coll_indexes[coll_name]
        try:
            for index in coll.list_indexes():
                name = index[""name""]
                if name == ""_id"" or name == ""_id_"" or name in indexes:
                    continue
                coll.drop_index(name)
        except pymongo.errors.OperationFailure:
            pass
","if name == ""_id"" or name == ""_id_"" or name in indexes :",140
"def _compare_dirs(self, dir1, dir2):
    # check that dir1 and dir2 are equivalent,
    # return the diff
    diff = []
    for root, dirs, files in os.walk(dir1):
        for file_ in files:
            path = os.path.join(root, file_)
            target_path = os.path.join(dir2, os.path.split(path)[-1])
            if not os.path.exists(target_path):
                diff.append(file_)
    return diff
",if not os . path . exists ( target_path ) :,138
"def load_state_dict(self, state_dict, strict=True):
    """"""Customized load.""""""
    self.language_model.load_state_dict(
        state_dict[self._language_model_key], strict=strict
    )
    if mpu.is_pipeline_last_stage():
        if self._multichoice_head_key in state_dict:
            self.multichoice_head.load_state_dict(
                state_dict[self._multichoice_head_key], strict=strict
            )
        else:
            print_rank_last(
                ""***WARNING*** could not find {} in the checkpoint, ""
                ""initializing to random"".format(self._multichoice_head_key)
            )
",if self . _multichoice_head_key in state_dict :,197
"def _parse_timedelta(self, value):
    try:
        sum = datetime.timedelta()
        start = 0
        while start < len(value):
            m = self._TIMEDELTA_PATTERN.match(value, start)
            if not m:
                raise Exception()
            num = float(m.group(1))
            units = m.group(2) or ""seconds""
            units = self._TIMEDELTA_ABBREV_DICT.get(units, units)
            sum += datetime.timedelta(**{units: num})
            start = m.end()
        return sum
    except:
        raise
",if not m :,165
"def SetChildMenuBar(self, pChild):
    if not pChild:
        # No Child, set Our menu bar back.
        if self._pMyMenuBar:
            self.SetMenuBar(self._pMyMenuBar)
        else:
            self.SetMenuBar(self.GetMenuBar())
        # Make sure we know our menu bar is in use
        self._pMyMenuBar = None
    else:
        if pChild.GetMenuBar() is None:
            return
        # Do we need to save the current bar?
        if self._pMyMenuBar is None:
            self._pMyMenuBar = self.GetMenuBar()
        self.SetMenuBar(pChild.GetMenuBar())
",if self . _pMyMenuBar is None :,188
"def init_weights(self):
    """"""Initialize weights of the head.""""""
    # retinanet_bias_init
    bias_cls = bias_init_with_prob(0.01)
    normal_init(self.conv_reg, std=0.01)
    normal_init(self.conv_centerness, std=0.01)
    normal_init(self.conv_cls, std=0.01, bias=bias_cls)
    for branch in [self.cls_convs, self.reg_convs]:
        for module in branch.modules():
            if isinstance(module, ConvModule) and isinstance(module.conv, nn.Conv2d):
                caffe2_xavier_init(module.conv)
","if isinstance ( module , ConvModule ) and isinstance ( module . conv , nn . Conv2d ) :",178
"def handle_exception(self, e, result):
    for k in sorted(result.thrift_spec):
        if result.thrift_spec[k][1] == ""success"":
            continue
        _, exc_name, exc_cls, _ = result.thrift_spec[k]
        if isinstance(e, exc_cls):
            setattr(result, exc_name, e)
            break
    else:
        raise
","if isinstance ( e , exc_cls ) :",115
"def scripts(self):
    application_root = current_app.config.get(""APPLICATION_ROOT"")
    subdir = application_root != ""/""
    scripts = []
    for script in get_registered_scripts():
        if script.startswith(""http""):
            scripts.append(f'<script defer src=""{script}""></script>')
        elif subdir:
            scripts.append(f'<script defer src=""{application_root}/{script}""></script>')
        else:
            scripts.append(f'<script defer src=""{script}""></script>')
    return markup(""\n"".join(scripts))
","if script . startswith ( ""http"" ) :",146
"def test_related_objects_local(self):
    result_key = ""get_all_related_objects_with_model_local""
    for model, expected in TEST_RESULTS[result_key].items():
        objects = [
            (field, self._model(model, field))
            for field in model._meta.get_fields(include_parents=False)
            if field.auto_created and not field.concrete
        ]
        self.assertEqual(
            sorted(self._map_related_query_names(objects), key=self.key_name),
            sorted(expected, key=self.key_name),
        )
",if field . auto_created and not field . concrete,164
"def setTestOutcome(self, event):
    """"""Update outcome, exc_info and reason based on configured mappings""""""
    if event.exc_info:
        ec, ev, tb = event.exc_info
        classname = ec.__name__
        if classname in self.treatAsFail:
            short, long_ = self.labels(classname)
            self._setOutcome(event, ""failed"", short, long_)
        elif classname in self.treatAsSkip:
            short, long_ = self.labels(classname, upper=False)
            self._setOutcome(event, ""skipped"", short, ""%s: '%s'"" % (long_, ev), str(ev))
",elif classname in self . treatAsSkip :,169
"def small_count(v):
    if not v:
        return 0
    z = [
        (1000000000, _(""b"")),
        (1000000, _(""m"")),
        (1000, _(""k"")),
    ]
    v = int(v)
    for x, y in z:
        o, p = divmod(v, x)
        if o:
            if len(str(o)) > 2 or not p:
                return ""%d%s"" % (o, y)
            return ""%.1f%s"" % (v / float(x), y)
    return v
",if len ( str ( o ) ) > 2 or not p :,149
"def __read(self, n):
    if self._read_watcher is None:
        raise UnsupportedOperation(""read"")
    while 1:
        try:
            return _read(self._fileno, n)
        except (IOError, OSError) as ex:
            if ex.args[0] not in ignored_errors:
                raise
        wait_on_watcher(self._read_watcher, None, None, self.hub)
",if ex . args [ 0 ] not in ignored_errors :,111
"def locked(self):
    inputfiles = set(self.all_inputfiles())
    outputfiles = set(self.all_outputfiles())
    if os.path.exists(self._lockdir):
        for lockfile in self._locks(""input""):
            with open(lockfile) as lock:
                for f in lock:
                    f = f.strip()
                    if f in outputfiles:
                        return True
        for lockfile in self._locks(""output""):
            with open(lockfile) as lock:
                for f in lock:
                    f = f.strip()
                    if f in outputfiles or f in inputfiles:
                        return True
    return False
",if f in outputfiles or f in inputfiles :,195
"def _flags_to_int(flags):
    # Note, that order does not matter, libev has its own predefined order
    if not flags:
        return 0
    if isinstance(flags, integer_types):
        return flags
    result = 0
    try:
        if isinstance(flags, basestring):
            flags = flags.split("","")
        for value in flags:
            value = value.strip().lower()
            if value:
                result |= _flags_str2int[value]
    except KeyError as ex:
        raise ValueError(
            ""Invalid backend or flag: %s\nPossible values: %s""
            % (ex, "", "".join(sorted(_flags_str2int.keys())))
        )
    return result
","if isinstance ( flags , basestring ) :",191
"def setFg(self, colour, override=False):
    if not self.ttkFlag:
        self.containerStack[-1][""fg""] = colour
        gui.SET_WIDGET_FG(self._getContainerProperty(""container""), colour, override)
        for child in self._getContainerProperty(""container"").winfo_children():
            if not self._isWidgetContainer(child):
                gui.SET_WIDGET_FG(child, colour, override)
    else:
        gui.trace(""In ttk mode - trying to set FG to %s"", colour)
        self.ttkStyle.configure(""TLabel"", foreground=colour)
        self.ttkStyle.configure(""TFrame"", foreground=colour)
",if not self . _isWidgetContainer ( child ) :,178
"def find_scintilla_constants(f):
    lexers = []
    states = []
    for name in f.order:
        v = f.features[name]
        if v[""Category""] != ""Deprecated"":
            if v[""FeatureType""] == ""val"":
                if name.startswith(""SCE_""):
                    states.append((name, v[""Value""]))
                elif name.startswith(""SCLEX_""):
                    lexers.append((name, v[""Value""]))
    return (lexers, states)
","elif name . startswith ( ""SCLEX_"" ) :",137
"def extract_error_message(response: requests.Response):
    if response.content:
        try:
            content = json.loads(response.content)
            if ""message"" in content:
                return content[""message""]
        except:
            logging.debug(f""Failed to parse the response content: {response.content}"")
    return response.reason
","if ""message"" in content :",94
"def canvas_size(self):
    """"""Return the width and height for this sprite canvas""""""
    width = height = 0
    for image in self.images:
        x = image.x + image.absolute_width
        y = image.y + image.absolute_height
        if width < x:
            width = x
        if height < y:
            height = y
    return round_up(width), round_up(height)
",if width < x :,110
"def _load_widgets(self):
    logger.info(""Loading plugins preferences widgets"")
    # Collect the preferences widget for each active plugin
    for plugin in self.plugin_manager.get_active_plugins():
        plugin_name = plugin.metadata.get(""name"")
        try:
            preferences_widget = plugin.get_preferences_widget()
            if preferences_widget:
                self._tabs.addTab(preferences_widget, plugin_name)
        except Exception as reason:
            logger.error(
                ""Unable to add the preferences widget (%s): %s"", plugin_name, reason
            )
            continue
",if preferences_widget :,162
"def clean_objects(string, common_attributes):
    """"""Return object and attribute lists""""""
    string = clean_string(string)
    words = string.split()
    if len(words) > 1:
        prefix_words_are_adj = True
        for att in words[:-1]:
            if att not in common_attributes:
                prefix_words_are_adj = False
        if prefix_words_are_adj:
            return words[-1:], words[:-1]
        else:
            return [string], []
    else:
        return [string], []
",if prefix_words_are_adj :,148
"def _reader():
    if shuffle:
        random.shuffle(file_list)
    while True:
        for fn in file_list:
            for line in open(fn, ""r""):
                yield self._process_line(line)
        if not cycle:
            break
",if not cycle :,76
"def load(weights, model, K, fsz, dil):
    index = 0
    layers = model.layers
    for layer in layers._layers:
        if hasattr(layer, ""W""):
            if layer.W.shape == weights[index].shape:
                layer.W[:] = weights[index]
            else:
                layer.W[:] = dilate(weights[index], K, fsz, dil)
            index += 1
","if hasattr ( layer , ""W"" ) :",115
"def upgrade(migrate_engine):
    print(__doc__)
    metadata.bind = migrate_engine
    liftoverjobs = dict()
    jobs = context.query(DeferredJob).filter_by(plugin=""LiftOverTransferPlugin"").all()
    for job in jobs:
        if job.params[""parentjob""] not in liftoverjobs:
            liftoverjobs[job.params[""parentjob""]] = []
        liftoverjobs[job.params[""parentjob""]].append(job.id)
    for parent in liftoverjobs:
        lifts = liftoverjobs[parent]
        deferred = context.query(DeferredJob).filter_by(id=parent).first()
        deferred.params[""liftover""] = lifts
    context.flush()
","if job . params [ ""parentjob"" ] not in liftoverjobs :",182
"def get_refs(self, recursive=False):
    """""":see: AbstractExpression.get_refs()""""""
    if recursive:
        conds_refs = self.refs + sum((c.get_refs(True) for c in self.conds), [])
        if self.consequent:
            conds_refs.extend(self.consequent.get_refs(True))
        return conds_refs
    else:
        return self.refs
",if self . consequent :,105
"def _parse(self, engine):
    """"""Parse the layer.""""""
    if isinstance(self.args, dict):
        if ""axis"" in self.args:
            self.axis = engine.evaluate(self.args[""axis""], recursive=True)
            if not isinstance(self.axis, int):
                raise ParsingError('""axis"" must be an integer.')
        if ""momentum"" in self.args:
            self.momentum = engine.evaluate(self.args[""momentum""], recursive=True)
            if not isinstance(self.momentum, (int, float)):
                raise ParsingError('""momentum"" must be numeric.')
","if ""axis"" in self . args :",157
"def CountMatches(pat, predicate):
    num_matches = 0
    for i in xrange(256):
        b = chr(i)
        m = pat.match(b)
        left = bool(m)
        right = predicate(i)
        if left != right:
            self.fail(""i = %d, b = %r, match: %s, predicate: %s"" % (i, b, left, right))
        if m:
            num_matches += 1
    return num_matches
",if m :,131
"def __new__(cls, *args, **kwargs):
    if len(args) == 1:
        if len(kwargs):
            raise ValueError(
                ""You can either use {} with one positional argument or with keyword arguments, not both."".format(
                    cls.__name__
                )
            )
        if not args[0]:
            return super().__new__(cls)
        if isinstance(args[0], cls):
            return cls
    return super().__new__(cls, *args, **kwargs)
","if isinstance ( args [ 0 ] , cls ) :",137
"def concatenateCharacterTokens(tokens):
    pendingCharacters = []
    for token in tokens:
        type = token[""type""]
        if type in (""Characters"", ""SpaceCharacters""):
            pendingCharacters.append(token[""data""])
        else:
            if pendingCharacters:
                yield {""type"": ""Characters"", ""data"": """".join(pendingCharacters)}
                pendingCharacters = []
            yield token
    if pendingCharacters:
        yield {""type"": ""Characters"", ""data"": """".join(pendingCharacters)}
",if pendingCharacters :,130
"def get_ranges_from_func_set(support_set):
    pos_start = 0
    pos_end = 0
    ranges = []
    for pos, func in enumerate(network.function):
        if func.type in support_set:
            pos_end = pos
        else:
            if pos_end >= pos_start:
                ranges.append((pos_start, pos_end))
            pos_start = pos + 1
    if pos_end >= pos_start:
        ranges.append((pos_start, pos_end))
    return ranges
",if pos_end >= pos_start :,145
"def _visit(self, func):
    fname = func[0]
    if fname in self._flags:
        if self._flags[fname] == 1:
            logger.critical(""Fatal error! network ins not Dag."")
            import sys
            sys.exit(-1)
        else:
            return
    else:
        if fname not in self._flags:
            self._flags[fname] = 1
        for output in func[3]:
            for f in self._orig:
                for input in f[2]:
                    if output == input:
                        self._visit(f)
    self._flags[fname] = 2
    self._sorted.insert(0, func)
",if self . _flags [ fname ] == 1 :,188
"def graph_merge_softmax_with_crossentropy_softmax(node):
    if node.op == softmax_with_bias:
        x, b = node.inputs
        for x_client in x.clients:
            if x_client[0].op == crossentropy_softmax_argmax_1hot_with_bias:
                big_client = x_client[0]
                if big_client in [b_client[0] for b_client in b.clients]:
                    xx, bb, ll = big_client.inputs
                    mergeable_client = big_client.op(x, b, ll)
                    copy_stack_trace(node.outputs[0], mergeable_client[1])
                    return [mergeable_client[1]]
",if x_client [ 0 ] . op == crossentropy_softmax_argmax_1hot_with_bias :,199
"def confidence(self):
    if self.bbox:
        # Units are measured in Kilometers
        distance = Distance(self.northeast, self.southwest, units=""km"")
        for score, maximum in [
            (10, 0.25),
            (9, 0.5),
            (8, 1),
            (7, 5),
            (6, 7.5),
            (5, 10),
            (4, 15),
            (3, 20),
            (2, 25),
        ]:
            if distance < maximum:
                return score
            if distance >= 25:
                return 1
    # Cannot determine score
    return 0
",if distance >= 25 :,192
"def OnListEndLabelEdit(self, std, extra):
    item = extra[0]
    text = item[4]
    if text is None:
        return
    item_id = self.GetItem(item[0])[6]
    from bdb import Breakpoint
    for bplist in Breakpoint.bplist.itervalues():
        for bp in bplist:
            if id(bp) == item_id:
                if text.strip().lower() == ""none"":
                    text = None
                bp.cond = text
                break
    self.RespondDebuggerData()
","if text . strip ( ) . lower ( ) == ""none"" :",151
"def _handle_autocomplete_request_for_text(text):
    if not hasattr(text, ""autocompleter""):
        if isinstance(text, (CodeViewText, ShellText)) and text.is_python_text():
            if isinstance(text, CodeViewText):
                text.autocompleter = Completer(text)
            elif isinstance(text, ShellText):
                text.autocompleter = ShellCompleter(text)
            text.bind(""<1>"", text.autocompleter.on_text_click)
        else:
            return
    text.autocompleter.handle_autocomplete_request()
","if isinstance ( text , ( CodeViewText , ShellText ) ) and text . is_python_text ( ) :",151
"def visit_Macro(self, node, frame):
    macro_frame, macro_ref = self.macro_body(node, frame)
    self.newline()
    if frame.toplevel:
        if not node.name.startswith(""_""):
            self.write(""context.exported_vars.add(%r)"" % node.name)
        ref = frame.symbols.ref(node.name)
        self.writeline(""context.vars[%r] = "" % node.name)
    self.write(""%s = "" % frame.symbols.ref(node.name))
    self.macro_def(macro_ref, macro_frame)
","if not node . name . startswith ( ""_"" ) :",150
"def execute(cls, ctx, op):
    try:
        pd.set_option(""mode.use_inf_as_na"", op.use_inf_as_na)
        if op.stage == OperandStage.map:
            return cls._execute_map(ctx, op)
        else:
            return cls._execute_combine(ctx, op)
    finally:
        pd.reset_option(""mode.use_inf_as_na"")
",if op . stage == OperandStage . map :,113
"def ranges(self, start, end):
    try:
        iterators = [i.ranges(start, end) for i in self.range_iterators]
        starts, ends, values = zip(*[next(i) for i in iterators])
        starts = list(starts)
        ends = list(ends)
        values = list(values)
        while start < end:
            min_end = min(ends)
            yield start, min_end, values
            start = min_end
            for i, iterator in enumerate(iterators):
                if ends[i] == min_end:
                    starts[i], ends[i], values[i] = next(iterator)
    except StopIteration:
        return
",if ends [ i ] == min_end :,188
"def get_explanation(self, spec):
    """"""Expand an explanation.""""""
    if spec:
        try:
            a = self.dns_txt(spec)
            if len(a) == 1:
                return str(self.expand(to_ascii(a[0]), stripdot=False))
        except PermError:
            # RFC4408 6.2/4 syntax errors cause exp= to be ignored
            if self.strict > 1:
                raise  # but report in harsh mode for record checking tools
            pass
    elif self.strict > 1:
        raise PermError(""Empty domain-spec on exp="")
    # RFC4408 6.2/4 empty domain spec is ignored
    # (unless you give precedence to the grammar).
    return None
",if len ( a ) == 1 :,200
"def iter_fields(node, *, include_meta=True, exclude_unset=False):
    exclude_meta = not include_meta
    for field_name, field in node._fields.items():
        if exclude_meta and field.meta:
            continue
        field_val = getattr(node, field_name, _marker)
        if field_val is _marker:
            continue
        if exclude_unset:
            if callable(field.default):
                default = field.default()
            else:
                default = field.default
            if field_val == default:
                continue
        yield field_name, field_val
",if exclude_unset :,171
"def __setattr__(self, name, value):
    try:
        field = self._meta.get_field(name)
        if type(field) in [models.CharField, models.TextField] and type(value) == str:
            value = value[: field.max_length]
    except models.fields.FieldDoesNotExist:
        pass  # This happens with foreign keys.
    super.__setattr__(self, name, value)
","if type ( field ) in [ models . CharField , models . TextField ] and type ( value ) == str :",104
"def create_child(self, value=None, _id=None):
    with atomic(savepoint=False):
        child_key = self.get_next_child_key()
        if value is None:
            value = child_key
        child = self.__class__.objects.create(id=_id, key=child_key, value=value)
        return child
",if value is None :,92
"def list_tags_for_stream(self, stream_name, exclusive_start_tag_key=None, limit=None):
    stream = self.describe_stream(stream_name)
    tags = []
    result = {""HasMoreTags"": False, ""Tags"": tags}
    for key, val in sorted(stream.tags.items(), key=lambda x: x[0]):
        if limit and len(tags) >= limit:
            result[""HasMoreTags""] = True
            break
        if exclusive_start_tag_key and key < exclusive_start_tag_key:
            continue
        tags.append({""Key"": key, ""Value"": val})
    return result
",if limit and len ( tags ) >= limit :,166
"def emit(self, record):
    try:
        app = get_app()
        if app.is_running and getattr(app, ""debug"", False):
            msg = self.format(record)
            debug_buffer = app.layout.get_buffer_by_name(""debug_buffer"")
            current_document = debug_buffer.document.text
            if current_document:
                msg = ""\n"".join([current_document, msg])
            debug_buffer.set_document(Document(text=msg), bypass_readonly=True)
        else:
            super().emit(record)
    except:
        self.handleError(record)
","if app . is_running and getattr ( app , ""debug"" , False ) :",170
"def worker():
    global error
    while True:
        (num, q) = pq.get()
        if q is None or error is not None:
            pq.task_done()
            break
        try:
            process_one(q)
        except Exception as e:
            error = e
        finally:
            pq.task_done()
",if q is None or error is not None :,99
"def transceiver(self, data):
    out = []
    for t in range(8):
        if data[t] == 0:
            continue
        value = data[t]
        for b in range(8):
            if value & 0x80:
                if len(TRANSCEIVER[t]) < b + 1:
                    out.append(""(unknown)"")
                else:
                    out.append(TRANSCEIVER[t][b])
            value <<= 1
    self.annotate(""Transceiver compliance"", "", "".join(out))
",if value & 0x80 :,155
"def skip_to_close_match(self):
    nestedCount = 1
    while 1:
        tok = self.tokenizer.get_next_token()
        ttype = tok[""style""]
        if ttype == SCE_PL_UNUSED:
            return
        elif self.classifier.is_index_op(tok):
            tval = tok[""text""]
            if self.opHash.has_key(tval):
                if self.opHash[tval][1] == 1:
                    nestedCount += 1
                else:
                    nestedCount -= 1
                    if nestedCount <= 0:
                        break
",if ttype == SCE_PL_UNUSED :,176
"def GenerateVector(self, hits, vector, level):
    """"""Generate possible hit vectors which match the rules.""""""
    for item in hits.get(level, []):
        if vector:
            if item < vector[-1]:
                continue
            if item > self.max_separation + vector[-1]:
                break
        new_vector = vector + [item]
        if level + 1 == len(hits):
            yield new_vector
        elif level + 1 < len(hits):
            for result in self.GenerateVector(hits, new_vector, level + 1):
                yield result
",if item < vector [ - 1 ] :,157
"def __setattr__(self, name, value):
    if name == ""path"":
        if value and value != """":
            if value[0] != ""/"":
                raise ValueError(
                    'The page path should always start with a slash (""/"").'
                )
    elif name == ""load_time"":
        if value and not isinstance(value, int):
            raise ValueError(
                ""Page load time must be specified in integer milliseconds.""
            )
    object.__setattr__(self, name, value)
","if value and value != """" :",136
"def awaitTermination(self, timeout=None):
    if self.scheduler is None:
        raise RuntimeError(""StreamimgContext not started"")
    try:
        deadline = time.time() + timeout if timeout is not None else None
        while True:
            is_terminated = self._runOnce()
            if is_terminated or (deadline is not None and time.time() > deadline):
                break
            if self.batchCallback:
                self.batchCallback()
    except KeyboardInterrupt:
        pass
    finally:
        self.sc.stop()
        logger.info(""StreamingContext stopped successfully"")
",if is_terminated or ( deadline is not None and time . time ( ) > deadline ) :,157
"def stopbutton(self):
    if GPIOcontrol:
        while mediastopbutton:
            time.sleep(0.25)
            if not GPIO.input(stoppushbutton):
                print(""Stopped"")
                stop()
",if not GPIO . input ( stoppushbutton ) :,64
"def test_create_connection_timeout(self):
    # Issue #9792: create_connection() should not recast timeout errors
    # as generic socket errors.
    with self.mocked_socket_module():
        try:
            socket.create_connection((HOST, 1234))
        except socket.timeout:
            pass
        except OSError as exc:
            if support.IPV6_ENABLED or exc.errno != errno.EAFNOSUPPORT:
                raise
        else:
            self.fail(""socket.timeout not raised"")
",if support . IPV6_ENABLED or exc . errno != errno . EAFNOSUPPORT :,135
"def handle_exception_and_die(e):
    if hasattr(e, ""kind""):
        if e.kind == ""die"":
            sys.stderr.write(""ABORT: "" + e.msg + ""\n"")
            sys.exit(e.value)
        elif e.kind == ""exit"":
            sys.stderr.write(""EXITING\n"")
            sys.exit(e.value)
    else:
        print(str(e))
        sys.exit(1)
","if e . kind == ""die"" :",128
"def gets(self, key):
    with self.client_pool.get_and_release(destroy_on_fail=True) as client:
        try:
            return client.gets(key)
        except Exception:
            if self.ignore_exc:
                return (None, None)
            else:
                raise
",if self . ignore_exc :,90
"def _execute(self, options, args):
    if len(args) < 3:
        raise CommandError(_(""Not enough arguments""))
    tag = fsn2text(args[0])
    value = fsn2text(args[1])
    paths = args[2:]
    songs = []
    for path in paths:
        song = self.load_song(path)
        if not song.can_change(tag):
            raise CommandError(_(""Can not set %r"") % tag)
        self.log(""Add %r to %r"" % (value, tag))
        song.add(tag, value)
        songs.append(song)
    self.save_songs(songs)
",if not song . can_change ( tag ) :,169
"def get_place_name(self, place_handle):
    """"""Obtain a place name""""""
    text = """"
    if place_handle:
        place = self.dbstate.db.get_place_from_handle(place_handle)
        if place:
            place_title = place_displayer.display(self.dbstate.db, place)
            if place_title != """":
                if len(place_title) > 25:
                    text = place_title[:24] + ""...""
                else:
                    text = place_title
    return text
","if place_title != """" :",153
"def _Determine_Do(self):
    self.applicable = 1
    self.value = os.environ.get(self.name, None)
    if self.value is None and black.configure.items.has_key(""buildType""):
        buildType = black.configure.items[""buildType""].Get()
        if buildType == ""debug"":
            self.value = ""warn""
        else:
            self.value = None
    self.determined = 1
","if buildType == ""debug"" :",115
"def bundle_directory(self, dirpath):
    """"""Bundle all modules/packages in the given directory.""""""
    dirpath = os.path.abspath(dirpath)
    for nm in os.listdir(dirpath):
        nm = _u(nm)
        if nm.startswith("".""):
            continue
        itempath = os.path.join(dirpath, nm)
        if os.path.isdir(itempath):
            if os.path.exists(os.path.join(itempath, ""__init__.py"")):
                self.bundle_package(itempath)
        elif nm.endswith("".py""):
            self.bundle_module(itempath)
","if nm . startswith ( ""."" ) :",160
"def header_fields(self, fields):
    headers = dict(self.conn.response.getheaders())
    ret = {}
    for field in fields:
        if not headers.has_key(field[1]):
            raise ValueError(""%s was not found in response header"" % (field[1]))
        try:
            ret[field[0]] = int(headers[field[1]])
        except ValueError:
            ret[field[0]] = headers[field[1]]
    return ret
",if not headers . has_key ( field [ 1 ] ) :,124
"def caesar_cipher(s, k):
    result = """"
    for char in s:
        n = ord(char)
        if 64 < n < 91:
            n = ((n - 65 + k) % 26) + 65
        if 96 < n < 123:
            n = ((n - 97 + k) % 26) + 97
        result = result + chr(n)
    return result
",if 64 < n < 91 :,104
"def qtTypeIdent(conn, *args):
    # We're not using the conn object at the moment, but - we will
    # modify the
    # logic to use the server version specific keywords later.
    res = None
    value = None
    for val in args:
        # DataType doesn't have len function then convert it to string
        if not hasattr(val, ""__len__""):
            val = str(val)
        if len(val) == 0:
            continue
        value = val
        if Driver.needsQuoting(val, True):
            value = value.replace('""', '""""')
            value = '""' + value + '""'
        res = ((res and res + ""."") or """") + value
    return res
",if len ( val ) == 0 :,181
"def _parse_timezone(
    value: Optional[str], error: Type[Exception]
) -> Union[None, int, timezone]:
    if value == ""Z"":
        return timezone.utc
    elif value is not None:
        offset_mins = int(value[-2:]) if len(value) > 3 else 0
        offset = 60 * int(value[1:3]) + offset_mins
        if value[0] == ""-"":
            offset = -offset
        try:
            return timezone(timedelta(minutes=offset))
        except ValueError:
            raise error()
    else:
        return None
","if value [ 0 ] == ""-"" :",153
"def indent(elem, level=0):
    i = ""\n"" + level * ""  ""
    if len(elem):
        if not elem.text or not elem.text.strip():
            elem.text = i + ""  ""
        if not elem.tail or not elem.tail.strip():
            elem.tail = i
        for elem in elem:
            indent(elem, level + 1)
        if not elem.tail or not elem.tail.strip():
            elem.tail = i
    else:
        if level and (not elem.tail or not elem.tail.strip()):
            elem.tail = i
",if not elem . tail or not elem . tail . strip ( ) :,161
"def _make_slices(
    shape: tp.Tuple[int, ...],
    axes: tp.Tuple[int, ...],
    size: int,
    rng: np.random.RandomState,
) -> tp.List[slice]:
    slices = []
    for a, s in enumerate(shape):
        if a in axes:
            if s <= 1:
                raise ValueError(""Cannot crossover on axis with size 1"")
            start = rng.randint(s - size)
            slices.append(slice(start, start + size))
        else:
            slices.append(slice(None))
    return slices
",if s <= 1 :,154
"def _loadTestsFromTestCase(self, event, testCaseClass):
    evt = events.LoadFromTestCaseEvent(event.loader, testCaseClass)
    result = self.session.hooks.loadTestsFromTestCase(evt)
    if evt.handled:
        loaded_suite = result or event.loader.suiteClass()
    else:
        names = self._getTestCaseNames(event, testCaseClass)
        if not names and hasattr(testCaseClass, ""runTest""):
            names = [""runTest""]
        # FIXME return failure test case if name not in testcase class
        loaded_suite = event.loader.suiteClass(map(testCaseClass, names))
    if evt.extraTests:
        loaded_suite.addTests(evt.extraTests)
    return loaded_suite
","if not names and hasattr ( testCaseClass , ""runTest"" ) :",185
"def check_settings(self):
    if self.settings_dict[""TIME_ZONE""] is not None:
        if not settings.USE_TZ:
            raise ImproperlyConfigured(
                ""Connection '%s' cannot set TIME_ZONE because USE_TZ is ""
                ""False."" % self.alias
            )
        elif self.features.supports_timezones:
            raise ImproperlyConfigured(
                ""Connection '%s' cannot set TIME_ZONE because its engine ""
                ""handles time zones conversions natively."" % self.alias
            )
",elif self . features . supports_timezones :,140
"def collect_conflicting_diffs(path, decisions):
    local_conflict_diffs = []
    remote_conflict_diffs = []
    for d in decisions:
        if d.conflict:
            ld = adjust_patch_level(path, d.common_path, d.local_diff)
            rd = adjust_patch_level(path, d.common_path, d.remote_diff)
            local_conflict_diffs.extend(ld)
            remote_conflict_diffs.extend(rd)
    return local_conflict_diffs, remote_conflict_diffs
",if d . conflict :,140
"def short_repr(obj):
    if isinstance(
        obj,
        (type, types.ModuleType, types.BuiltinMethodType, types.BuiltinFunctionType),
    ):
        return obj.__name__
    if isinstance(obj, types.MethodType):
        if obj.im_self is not None:
            return obj.im_func.__name__ + "" (bound)""
        else:
            return obj.im_func.__name__
    if isinstance(obj, (tuple, list, dict, set)):
        return ""%d items"" % len(obj)
    if isinstance(obj, weakref.ref):
        return ""all_weakrefs_are_one""
    return repr(obj)[:40]
",if obj . im_self is not None :,172
"def _massage_uri(uri):
    if uri:
        if uri.startswith(""hdfs:///""):
            uri = uri.replace(""hdfs://"", get_defaultfs())
        elif uri.startswith(""/""):
            uri = get_defaultfs() + uri
    return uri
","if uri . startswith ( ""hdfs:///"" ) :",68
"def chsub(self, msg, chatid):
    (cmd, evt, params) = self.tokenize(msg, 3)
    if cmd == ""/sub"":
        sql = ""replace into telegram_subscriptions(uid, event_type, parameters) values (?, ?, ?)""
    else:
        if evt == ""everything"":
            sql = ""delete from telegram_subscriptions where uid = ? and (event_type = ? or parameters = ? or 1 = 1)""  # does not look very elegant, but makes unsub'ing everythign possible
        else:
            sql = ""delete from telegram_subscriptions where uid = ? and event_type = ? and parameters = ?""
    with self.bot.database as conn:
        conn.execute(sql, [chatid, evt, params])
        conn.commit()
    return
","if evt == ""everything"" :",196
"def undefined_symbols(self):
    result = []
    for p in self.Productions:
        if not p:
            continue
        for s in p.prod:
            if not s in self.Prodnames and not s in self.Terminals and s != ""error"":
                result.append((s, p))
    return result
",if not p :,88
"def renumber(self, x1, y1, x2, y2, dx, dy):
    out = []
    for part in re.split(""(\w+)"", self.formula):
        m = re.match(""^([A-Z]+)([1-9][0-9]*)$"", part)
        if m is not None:
            sx, sy = m.groups()
            x = colname2num(sx)
            y = int(sy)
            if x1 <= x <= x2 and y1 <= y <= y2:
                part = cellname(x + dx, y + dy)
        out.append(part)
    return FormulaCell("""".join(out), self.fmt, self.alignment)
",if x1 <= x <= x2 and y1 <= y <= y2 :,179
"def modify_column(self, column: List[Optional[""Cell""]]):
    for i in range(len(column)):
        gate = column[i]
        if gate is self:
            continue
        elif isinstance(gate, ParityControlCell):
            # The first parity control to modify the column must merge all
            # of the other parity controls into itself.
            column[i] = None
            self._basis_change += gate._basis_change
            self.qubits += gate.qubits
        elif gate is not None:
            column[i] = gate.controlled_by(self.qubits[0])
","elif isinstance ( gate , ParityControlCell ) :",164
"def update_neighbor(neigh_ip_address, changes):
    rets = []
    for k, v in changes.items():
        if k == neighbors.MULTI_EXIT_DISC:
            rets.append(_update_med(neigh_ip_address, v))
        if k == neighbors.ENABLED:
            rets.append(update_neighbor_enabled(neigh_ip_address, v))
        if k == neighbors.CONNECT_MODE:
            rets.append(_update_connect_mode(neigh_ip_address, v))
    return all(rets)
",if k == neighbors . MULTI_EXIT_DISC :,138
"def writexml(
    self,
    stream,
    indent="""",
    addindent="""",
    newl="""",
    strip=0,
    nsprefixes={},
    namespace="""",
):
    w = _streamWriteWrapper(stream)
    if self.raw:
        val = self.nodeValue
        if not isinstance(val, str):
            val = str(self.nodeValue)
    else:
        v = self.nodeValue
        if not isinstance(v, str):
            v = str(v)
        if strip:
            v = "" "".join(v.split())
        val = escape(v)
    w(val)
","if not isinstance ( v , str ) :",164
"def _condition(ct):
    for qobj in args:
        if qobj.connector == ""AND"" and not qobj.negated:
            # normal kwargs are an AND anyway, so just use those for now
            for child in qobj.children:
                kwargs.update(dict([child]))
        else:
            raise NotImplementedError(""Unsupported Q object"")
    for attr, val in kwargs.items():
        if getattr(ct, attr) != val:
            return False
    return True
","if qobj . connector == ""AND"" and not qobj . negated :",127
"def results_iter(self):
    if self.connection.ops.oracle:
        from django.db.models.fields import DateTimeField
        fields = [DateTimeField()]
    else:
        needs_string_cast = self.connection.features.needs_datetime_string_cast
    offset = len(self.query.extra_select)
    for rows in self.execute_sql(MULTI):
        for row in rows:
            date = row[offset]
            if self.connection.ops.oracle:
                date = self.resolve_columns(row, fields)[offset]
            elif needs_string_cast:
                date = typecast_timestamp(str(date))
            yield date
",if self . connection . ops . oracle :,182
"def get_job_type(self):
    if int(self.job_runtime_conf.get(""dsl_version"", 1)) == 2:
        job_type = (
            self.job_runtime_conf[""job_parameters""].get(""common"", {}).get(""job_type"")
        )
        if not job_type:
            job_type = self.job_runtime_conf[""job_parameters""].get(""job_type"", ""train"")
    else:
        job_type = self.job_runtime_conf[""job_parameters""].get(""job_type"", ""train"")
    return job_type
",if not job_type :,150
"def validate_assessment_criteria(self):
    if self.assessment_criteria:
        total_weightage = 0
        for criteria in self.assessment_criteria:
            total_weightage += criteria.weightage or 0
        if total_weightage != 100:
            frappe.throw(_(""Total Weightage of all Assessment Criteria must be 100%""))
",if total_weightage != 100 :,97
"def get_list_of_strings_to_mongo_objects(self, notifications_list=None):
    result = []
    if len(notifications_list) > 0:
        for x in notifications_list:
            split_provider_id = x.split("":"")  # email:id
            if len(split_provider_id) == 2:
                _id = split_provider_id[1]
                cursor = self.get_by_id(_id)
                if cursor:  # Append if exists
                    result.append(cursor)
    return result
",if len ( split_provider_id ) == 2 :,148
"def dump_predictions_to_database(relation, predictions):
    judge = ""iepy-run on {}"".format(datetime.now().strftime(""%Y-%m-%d %H:%M""))
    for evidence, relation_is_present in predictions.items():
        label = (
            EvidenceLabel.YESRELATION
            if relation_is_present
            else EvidenceLabel.NORELATION
        )
        evidence.set_label(relation, label, judge, labeled_by_machine=True)
",if relation_is_present,129
"def __init__(self, **kwargs):
    # We hard-code the `to` argument for ForeignKey.__init__
    dfl = get_model_label(self.default_model_class)
    if ""to"" in kwargs.keys():  # pragma: no cover
        old_to = get_model_label(kwargs.pop(""to""))
        if old_to.lower() != dfl.lower():
            msg = ""%s can only be a ForeignKey to %s; %s passed"" % (
                self.__class__.__name__,
                dfl,
                old_to,
            )
            warnings.warn(msg, SyntaxWarning)
    kwargs[""to""] = dfl
    super().__init__(**kwargs)
",if old_to . lower ( ) != dfl . lower ( ) :,182
"def reverse(self):
    """"""Reverse *IN PLACE*.""""""
    li = self.leftindex
    lb = self.leftblock
    ri = self.rightindex
    rb = self.rightblock
    for i in range(self.len >> 1):
        lb.data[li], rb.data[ri] = rb.data[ri], lb.data[li]
        li += 1
        if li >= BLOCKLEN:
            lb = lb.rightlink
            li = 0
        ri -= 1
        if ri < 0:
            rb = rb.leftlink
            ri = BLOCKLEN - 1
",if ri < 0 :,156
"def get_api(user, url):
    global API_CACHE
    if API_CACHE is None or API_CACHE.get(url) is None:
        API_CACHE_LOCK.acquire()
        try:
            if API_CACHE is None:
                API_CACHE = {}
            if API_CACHE.get(url) is None:
                API_CACHE[url] = ImpalaDaemonApi(url)
        finally:
            API_CACHE_LOCK.release()
    api = API_CACHE[url]
    api.set_user(user)
    return api
",if API_CACHE . get ( url ) is None :,148
"def invert_index(cls, index, length):
    if np.isscalar(index):
        return length - index
    elif isinstance(index, slice):
        start, stop = index.start, index.stop
        new_start, new_stop = None, None
        if start is not None:
            new_stop = length - start
        if stop is not None:
            new_start = length - stop
        return slice(new_start - 1, new_stop - 1)
    elif isinstance(index, Iterable):
        new_index = []
        for ind in index:
            new_index.append(length - ind)
    return new_index
",if stop is not None :,168
"def infer_returned_object(pyfunction, args):
    """"""Infer the `PyObject` this `PyFunction` returns after calling""""""
    object_info = pyfunction.pycore.object_info
    result = object_info.get_exact_returned(pyfunction, args)
    if result is not None:
        return result
    result = _infer_returned(pyfunction, args)
    if result is not None:
        if args and pyfunction.get_module().get_resource() is not None:
            params = args.get_arguments(pyfunction.get_param_names(special_args=False))
            object_info.function_called(pyfunction, params, result)
        return result
    return object_info.get_returned(pyfunction, args)
",if args and pyfunction . get_module ( ) . get_resource ( ) is not None :,187
"def _check_imports(lib):
    # Make sure no conflicting libraries have been imported.
    libs = [""PyQt4"", ""PyQt5"", ""PySide""]
    libs.remove(lib)
    for lib2 in libs:
        lib2 += "".QtCore""
        if lib2 in sys.modules:
            raise RuntimeError(
                ""Refusing to import %s because %s is already "" ""imported."" % (lib, lib2)
            )
",if lib2 in sys . modules :,116
"def _poll(fds, timeout):
    if timeout is not None:
        timeout = int(timeout * 1000)  # timeout is in milliseconds
    fd_map = {}
    pollster = select.poll()
    for fd in fds:
        pollster.register(fd, select.POLLIN)
        if hasattr(fd, ""fileno""):
            fd_map[fd.fileno()] = fd
        else:
            fd_map[fd] = fd
    ls = []
    for fd, event in pollster.poll(timeout):
        if event & select.POLLNVAL:
            raise ValueError(""invalid file descriptor %i"" % fd)
        ls.append(fd_map[fd])
    return ls
","if hasattr ( fd , ""fileno"" ) :",180
"def default(cls, connection=None):
    """"""show the default connection, or make CONNECTION the default""""""
    if connection is not None:
        target = cls._get_config_filename(connection)
        if os.path.exists(target):
            if os.path.exists(cls._default_symlink):
                os.remove(cls._default_symlink)
            os.symlink(target, cls._default_symlink)
        else:
            cls._no_config_file_error(target)
    if os.path.exists(cls._default_symlink):
        print(""Default connection is "" + cls._default_connection())
    else:
        print(""There is no default connection set"")
",if os . path . exists ( target ) :,175
"def process(self, fuzzresult):
    base_url = urljoin(fuzzresult.url, "".."")
    for line in fuzzresult.history.content.splitlines():
        record = line.split(""/"")
        if len(record) == 6 and record[1]:
            self.queue_url(urljoin(base_url, record[1]))
            # Directory
            if record[0] == ""D"":
                self.queue_url(urljoin(base_url, record[1]))
                self.queue_url(urljoin(base_url, ""%s/CVS/Entries"" % (record[1])))
",if len ( record ) == 6 and record [ 1 ] :,153
"def _GetCSVRow(self, value):
    row = []
    for type_info in value.__class__.type_infos:
        if isinstance(type_info, rdf_structs.ProtoEmbedded):
            row.extend(self._GetCSVRow(value.Get(type_info.name)))
        elif isinstance(type_info, rdf_structs.ProtoBinary):
            row.append(text.Asciify(value.Get(type_info.name)))
        else:
            row.append(str(value.Get(type_info.name)))
    return row
","if isinstance ( type_info , rdf_structs . ProtoEmbedded ) :",143
"def get_history(self, state, dict_, passive=PASSIVE_OFF):
    if self.key in dict_:
        return History.from_scalar_attribute(self, state, dict_[self.key])
    else:
        if passive & INIT_OK:
            passive ^= INIT_OK
        current = self.get(state, dict_, passive=passive)
        if current is PASSIVE_NO_RESULT:
            return HISTORY_BLANK
        else:
            return History.from_scalar_attribute(self, state, current)
",if passive & INIT_OK :,143
"def _iterate_self_and_parents(self, upto=None):
    current = self
    result = ()
    while current:
        result += (current,)
        if current._parent is upto:
            break
        elif current._parent is None:
            raise sa_exc.InvalidRequestError(
                ""Transaction %s is not on the active transaction list"" % (upto)
            )
        else:
            current = current._parent
    return result
",if current . _parent is upto :,126
"def get_by_uri(self, uri: str) -> bytes:
    userId, bucket, key = self._parse_uri(uri)
    try:
        with db.session_scope() as dbsession:
            result = db_archivedocument.get(userId, bucket, key, session=dbsession)
        if result:
            return utils.ensure_bytes(self._decode(result))
        else:
            raise ObjectKeyNotFoundError(userId, bucket, key, caused_by=None)
    except Exception as err:
        logger.debug(""cannot get data: exception - "" + str(err))
        raise err
",if result :,158
"def app(scope, receive, send):
    while True:
        message = await receive()
        if message[""type""] == ""websocket.connect"":
            await send({""type"": ""websocket.accept""})
        elif message[""type""] == ""websocket.receive"":
            pass
        elif message[""type""] == ""websocket.disconnect"":
            break
","elif message [ ""type"" ] == ""websocket.disconnect"" :",93
"def recv_some(p, t=0.1, e=1, tr=5, stderr=0):
    if tr < 1:
        tr = 1
    x = time.time() + t
    y = []
    r = """"
    if stderr:
        pr = p.recv_err
    else:
        pr = p.recv
    while time.time() < x or r:
        r = pr()
        if r is None:
            break
        elif r:
            y.append(r)
        else:
            time.sleep(max((x - time.time()) / tr, 0))
    return """".join(y)
",elif r :,168
"def mouse_down(self, event):
    if event.button == 1:
        if self.scrolling:
            p = event.local
            if self.scroll_up_rect().collidepoint(p):
                self.scroll_up()
                return
            elif self.scroll_down_rect().collidepoint(p):
                self.scroll_down()
                return
    if event.button == 4:
        self.scroll_up()
    if event.button == 5:
        self.scroll_down()
    GridView.mouse_down(self, event)
",if self . scrolling :,160
"def copy_from(self, other):
    if self is other:
        return  # Myself!
    self.strictness = other.strictness  # sets behaviors in bulk
    for name in self.all_behaviors:
        self.set_behavior(name, other.get_behavior(name))
    for name in self._plain_attrs:
        val = getattr(other, name)
        if isinstance(val, set):
            val = val.copy()
        elif decimal and isinstance(val, decimal.Decimal):
            val = val.copy()
        setattr(self, name, val)
","elif decimal and isinstance ( val , decimal . Decimal ) :",148
"def __array_wrap__(self, out_arr, context=None):
    if self.dim is None:
        return out_arr
    else:
        this = self[:]
        if isinstance(this, Quantity):
            return Quantity.__array_wrap__(self[:], out_arr, context=context)
        else:
            return out_arr
","if isinstance ( this , Quantity ) :",90
"def _ArgumentListHasDictionaryEntry(self, token):
    """"""Check if the function argument list has a dictionary as an arg.""""""
    if _IsArgumentToFunction(token):
        while token:
            if token.value == ""{"":
                length = token.matching_bracket.total_length - token.total_length
                return length + self.stack[-2].indent > self.column_limit
            if token.ClosesScope():
                break
            if token.OpensScope():
                token = token.matching_bracket
            token = token.next_token
    return False
","if token . value == ""{"" :",153
"def save_all_changed_extensions(self):
    """"""Save configuration changes to the user config file.""""""
    has_changes = False
    for ext_name in self.extensions:
        options = self.extensions[ext_name]
        for opt in options:
            if self.set_extension_value(ext_name, opt):
                has_changes = True
    if has_changes:
        self.ext_userCfg.Save()
","if self . set_extension_value ( ext_name , opt ) :",111
"def to_dict(self):
    out = {}
    for key in ACTIVITY_KEYS:
        attr = getattr(self, key)
        if isinstance(attr, (datetime.timedelta, datetime.datetime)):
            out[key] = str(attr)
        else:
            out[key] = attr
    if self.streak:
        out[""streak""] = self.streak
    return out
","if isinstance ( attr , ( datetime . timedelta , datetime . datetime ) ) :",102
"def clean_publication_date(cls, cleaned_input):
    for add_channel in cleaned_input.get(""add_channels"", []):
        is_published = add_channel.get(""is_published"")
        publication_date = add_channel.get(""publication_date"")
        if is_published and not publication_date:
            add_channel[""publication_date""] = datetime.date.today()
",if is_published and not publication_date :,98
"def _random_blur(self, batch, sigma_max):
    for i in range(len(batch)):
        if bool(random.getrandbits(1)):
            # Random sigma
            sigma = random.uniform(0.0, sigma_max)
            batch[i] = scipy.ndimage.filters.gaussian_filter(batch[i], sigma)
    return batch
",if bool ( random . getrandbits ( 1 ) ) :,92
"def conninfo_parse(dsn):
    ret = {}
    length = len(dsn)
    i = 0
    while i < length:
        if dsn[i].isspace():
            i += 1
            continue
        param_match = PARAMETER_RE.match(dsn[i:])
        if not param_match:
            return
        param = param_match.group(1)
        i += param_match.end()
        if i >= length:
            return
        value, end = read_param_value(dsn[i:])
        if value is None:
            return
        i += end
        ret[param] = value
    return ret
",if i >= length :,175
"def set_environment_vars(env, source_env):
    """"""Copy allowed environment variables from |source_env|.""""""
    if not source_env:
        return
    for name, value in six.iteritems(source_env):
        if is_forwarded_environment_variable(name):
            # Avoid creating circular dependencies from importing environment by
            # using os.getenv.
            if os.getenv(""TRUSTED_HOST"") and should_rebase_environment_value(name):
                value = file_host.rebase_to_worker_root(value)
            env[name] = value
","if os . getenv ( ""TRUSTED_HOST"" ) and should_rebase_environment_value ( name ) :",152
"def toterminal(self, tw):
    # the entries might have different styles
    last_style = None
    for i, entry in enumerate(self.reprentries):
        if entry.style == ""long"":
            tw.line("""")
        entry.toterminal(tw)
        if i < len(self.reprentries) - 1:
            next_entry = self.reprentries[i + 1]
            if (
                entry.style == ""long""
                or entry.style == ""short""
                and next_entry.style == ""long""
            ):
                tw.sep(self.entrysep)
    if self.extraline:
        tw.line(self.extraline)
",if i < len ( self . reprentries ) - 1 :,198
"def __init__(self, loc, tabs=None):
    if os.path.isdir(loc):
        for item in os.listdir(loc):
            if item[0] == ""."":
                continue
            path = os.path.join(loc, item)
            self.append(CronTab(user=False, tabfile=path))
    elif os.path.isfile(loc):
        self.append(CronTab(user=False, tabfile=loc))
","if item [ 0 ] == ""."" :",121
"def import_data(self, fname):
    """"""Import data in current namespace""""""
    if self.count():
        nsb = self.currentWidget()
        nsb.refresh_table()
        nsb.import_data(fname)
        if self.dockwidget and not self.ismaximized:
            self.dockwidget.setVisible(True)
            self.dockwidget.raise_()
",if self . dockwidget and not self . ismaximized :,103
"def get_menu_items(node):
    aList = []
    for child in node.children:
        for tag in (""@menu"", ""@item""):
            if child.h.startswith(tag):
                name = child.h[len(tag) + 1 :].strip()
                if tag == ""@menu"":
                    aList.append((""%s %s"" % (tag, name), get_menu_items(child), None))
                else:
                    b = g.splitLines("""".join(child.b))
                    aList.append((tag, name, b[0] if b else """"))
                break
    return aList
",if child . h . startswith ( tag ) :,173
"def __init__(self, *args, **kw):
    if len(args) > 1:
        raise TypeError(""MultiDict can only be called with one positional "" ""argument"")
    if args:
        if hasattr(args[0], ""iteritems""):
            items = list(args[0].iteritems())
        elif hasattr(args[0], ""items""):
            items = list(args[0].items())
        else:
            items = list(args[0])
        self._items = items
    else:
        self._items = []
    if kw:
        self._items.extend(kw.items())
","elif hasattr ( args [ 0 ] , ""items"" ) :",156
"def open(self) -> ""KeyValueDb"":
    """"""Create a new data base or open existing one""""""
    if os.path.exists(self._name):
        if not os.path.isfile(self._name):
            raise IOError(""%s exists and is not a file"" % self._name)
        if os.path.getsize(self._name) == 0:
            # ignore empty files
            return self
        with open(self._name, ""rb"") as _in:  # binary mode
            self.set_records(pickle.load(_in))
    else:
        # make sure path exists
        mkpath(os.path.dirname(self._name))
        self.commit()
    return self
",if os . path . getsize ( self . _name ) == 0 :,180
"def sortModules(self):
    super(NeuronDecomposableNetwork, self).sortModules()
    self._constructParameterInfo()
    # contains a list of lists of indices
    self.decompositionIndices = {}
    for neuron in self._neuronIterator():
        self.decompositionIndices[neuron] = []
    for w in range(self.paramdim):
        inneuron, outneuron = self.paramInfo[w]
        if self.espStyleDecomposition and outneuron[0] in self.outmodules:
            self.decompositionIndices[inneuron].append(w)
        else:
            self.decompositionIndices[outneuron].append(w)
",if self . espStyleDecomposition and outneuron [ 0 ] in self . outmodules :,158
"def visit_Options(self, node: qlast.Options) -> None:
    for i, opt in enumerate(node.options.values()):
        if i > 0:
            self.write("" "")
        self.write(opt.name)
        if not isinstance(opt, qlast.Flag):
            self.write(f"" {opt.val}"")
",if i > 0 :,90
"def is_child_of(self, item_hash, possible_child_hash):
    if self.get_last(item_hash) != self.get_last(possible_child_hash):
        return None
    while True:
        if possible_child_hash == item_hash:
            return True
        if possible_child_hash not in self.items:
            return False
        possible_child_hash = self.items[possible_child_hash].previous_hash
",if possible_child_hash not in self . items :,119
"def __call__(self, text, **kargs):
    words = jieba.tokenize(text, mode=""search"")
    token = Token()
    for (w, start_pos, stop_pos) in words:
        if not accepted_chars.match(w) and len(w) <= 1:
            continue
        token.original = token.text = w
        token.pos = start_pos
        token.startchar = start_pos
        token.endchar = stop_pos
        yield token
",if not accepted_chars . match ( w ) and len ( w ) <= 1 :,123
"def test_analysis_jobs_cypher_syntax(neo4j_session):
    parameters = {
        ""AWS_ID"": None,
        ""UPDATE_TAG"": None,
        ""OKTA_ORG_ID"": None,
    }
    for job_name in contents(""cartography.data.jobs.analysis""):
        if not job_name.endswith("".json""):
            continue
        try:
            cartography.util.run_analysis_job(job_name, neo4j_session, parameters)
        except Exception as e:
            pytest.fail(
                f""run_analysis_job failed for analysis job '{job_name}' with exception: {e}""
            )
","if not job_name . endswith ( "".json"" ) :",180
"def _interleave_dataset_results_and_tensors(dataset_results, flat_run_tensors):
    flattened_results = []
    for idx in range(len(dataset_results) + len(flat_run_tensors)):
        if dataset_results.get(idx):
            flattened_results.append(dataset_results[idx])
        else:
            flattened_results.append(flat_run_tensors.pop(0))
    return flattened_results
",if dataset_results . get ( idx ) :,111
"def test_k_is_stochastic_parameter(self):
    # k as stochastic parameter
    aug = iaa.MedianBlur(k=iap.Choice([3, 5]))
    seen = [False, False]
    for i in sm.xrange(100):
        observed = aug.augment_image(self.base_img)
        if np.array_equal(observed, self.blur3x3):
            seen[0] += True
        elif np.array_equal(observed, self.blur5x5):
            seen[1] += True
        else:
            raise Exception(""Unexpected result in MedianBlur@2"")
        if all(seen):
            break
    assert np.all(seen)
","if np . array_equal ( observed , self . blur3x3 ) :",176
"def pickPath(self, color):
    self.path[color] = ()
    currentPos = self.starts[color]
    while True:
        minDist = None
        minGuide = None
        for guide in self.guides[color]:
            guideDist = dist(currentPos, guide)
            if minDist == None or guideDist < minDist:
                minDist = guideDist
                minGuide = guide
        if dist(currentPos, self.ends[color]) == 1:
            return
        if minGuide == None:
            return
        self.path[color] = self.path[color] + (minGuide,)
        currentPos = minGuide
        self.guides[color].remove(minGuide)
",if minGuide == None :,192
"def UpdateRepository(self):
    if hasattr(self, ""commit_update""):
        if self.commit_update[""Updates""] != []:
            if not path.isdir("".git/""):
                self.gitZipRepo()
            call([""git"", ""reset"", ""--hard"", ""origin/{}"".format(self.getBranch)])
            self.ProcessCall_([""git"", ""pull"", ""origin"", self.getBranch])
            self.ProcessCall_([""pip"", ""install"", ""-r"", ""requirements.txt""])
","if self . commit_update [ ""Updates"" ] != [ ] :",126
"def callback(result=Cr.NS_OK, message=None, success=None):
    if success is None:
        if Cr.NS_SUCCEEDED(result):
            success = Ci.koIAsyncCallback.RESULT_SUCCESSFUL
        else:
            success = Ci.koIAsyncCallback.RESULT_ERROR
    data = Namespace(result=result, message=message, _com_interfaces_=[Ci.koIErrorInfo])
    self._invoke_activate_callbacks(success, data)
",if Cr . NS_SUCCEEDED ( result ) :,121
"def get_location(device):
    location = []
    node = device
    while node:
        position = node.get_position() or """"
        if position:
            position = "" [%s]"" % position
        location.append(node.name + position)
        node = node.parent
    return "" / "".join(reversed(location))
",if position :,87
"def load_checkpoint(path, model, optimizer, reset_optimizer):
    global global_step
    global global_epoch
    print(""Load checkpoint from: {}"".format(path))
    checkpoint = _load(path)
    model.load_state_dict(checkpoint[""state_dict""])
    if not reset_optimizer:
        optimizer_state = checkpoint[""optimizer""]
        if optimizer_state is not None:
            print(""Load optimizer state from {}"".format(path))
            optimizer.load_state_dict(checkpoint[""optimizer""])
    global_step = checkpoint[""global_step""]
    global_epoch = checkpoint[""global_epoch""]
    return model
",if optimizer_state is not None :,155
"def run_command(self, command: str, data: Dict[str, object]) -> Dict[str, object]:
    """"""Run a specific command from the registry.""""""
    key = ""cmd_"" + command
    method = getattr(self.__class__, key, None)
    if method is None:
        return {""error"": ""Unrecognized command '%s'"" % command}
    else:
        if command not in {""check"", ""recheck"", ""run""}:
            # Only the above commands use some error formatting.
            del data[""is_tty""]
            del data[""terminal_width""]
        return method(self, **data)
","if command not in { ""check"" , ""recheck"" , ""run"" } :",151
"def call_init(self, node, instance):
    # Call __init__ on each binding.
    for b in instance.bindings:
        if b.data in self._initialized_instances:
            continue
        self._initialized_instances.add(b.data)
        node = self._call_init_on_binding(node, b)
    return node
",if b . data in self . _initialized_instances :,89
"def get_request_headers() -> Dict:
    url = urlparse(uri)
    candidates = [
        ""%s://%s"" % (url.scheme, url.netloc),
        ""%s://%s/"" % (url.scheme, url.netloc),
        uri,
        ""*"",
    ]
    for u in candidates:
        if u in self.config.linkcheck_request_headers:
            headers = dict(DEFAULT_REQUEST_HEADERS)
            headers.update(self.config.linkcheck_request_headers[u])
            return headers
    return {}
",if u in self . config . linkcheck_request_headers :,142
"def get_next_video_frame(self, skip_empty_frame=True):
    if not self.video_format:
        return
    while True:
        # We skip video packets which are not video frames
        # This happens in mkv files for the first few frames.
        video_packet = self._get_video_packet()
        if video_packet.image == 0:
            self._decode_video_packet(video_packet)
        if video_packet.image is not None or not skip_empty_frame:
            break
    if _debug:
        print(""Returning"", video_packet)
    return video_packet.image
",if video_packet . image is not None or not skip_empty_frame :,162
"def convert_path(ctx, tpath):
    for points, code in tpath.iter_segments():
        if code == Path.MOVETO:
            ctx.move_to(*points)
        elif code == Path.LINETO:
            ctx.line_to(*points)
        elif code == Path.CURVE3:
            ctx.curve_to(
                points[0], points[1], points[0], points[1], points[2], points[3]
            )
        elif code == Path.CURVE4:
            ctx.curve_to(*points)
        elif code == Path.CLOSEPOLY:
            ctx.close_path()
",elif code == Path . CURVE3 :,172
"def __init__(
    self, layout, value=None, string=None, *, dtype: np.dtype = np.float64
) -> None:
    """"""Constructor.""""""
    self.layout = layout
    if value is None:
        if string is None:
            self.value = np.zeros((self.layout.gaDims,), dtype=dtype)
        else:
            self.value = layout.parse_multivector(string).value
    else:
        self.value = np.array(value)
        if self.value.shape != (self.layout.gaDims,):
            raise ValueError(
                ""value must be a sequence of length %s"" % self.layout.gaDims
            )
","if self . value . shape != ( self . layout . gaDims , ) :",180
"def to_dict(self):
    contexts_ = {}
    for k, data in self.contexts.items():
        data_ = data.copy()
        if ""context"" in data_:
            del data_[""context""]
        if ""loaded"" in data_:
            del data_[""loaded""]
        contexts_[k] = data_
    return dict(contexts=contexts_)
","if ""loaded"" in data_ :",94
"def include_module(module):
    if not include_these:
        return True
    result = False
    for check in include_these:
        if ""/*"" in check:
            if check[:-1] in module:
                result = True
        else:
            if (os.getcwd() + ""/"" + check + "".py"") == module:
                result = True
    if result:
        print_status(""Including module: "" + module)
    return result
",if check [ : - 1 ] in module :,122
"def extract_from(msg_body, content_type=""text/plain""):
    try:
        if content_type == ""text/plain"":
            return extract_from_plain(msg_body)
        elif content_type == ""text/html"":
            return extract_from_html(msg_body)
    except Exception:
        log.exception(""ERROR extracting message"")
    return msg_body
","elif content_type == ""text/html"" :",100
"def test_list(self):
    self._create_locations()
    response = self.client.get(self.geojson_boxedlocation_list_url)
    self.assertEqual(response.status_code, 200)
    self.assertEqual(len(response.data[""features""]), 2)
    for feature in response.data[""features""]:
        self.assertIn(""bbox"", feature)
        fid = feature[""id""]
        if fid == 1:
            self.assertEqual(feature[""bbox""], self.bl1.bbox_geometry.extent)
        elif fid == 2:
            self.assertEqual(feature[""bbox""], self.bl2.bbox_geometry.extent)
        else:
            self.fail(""Unexpected id: {0}"".format(fid))
    BoxedLocation.objects.all().delete()
",if fid == 1 :,196
"def overrideCommand(self, commandName, func):
    # Override entries in c.k.masterBindingsDict
    k = self
    d = k.masterBindingsDict
    for key in d:
        d2 = d.get(key)
        for key2 in d2:
            bi = d2.get(key2)
            if bi.commandName == commandName:
                bi.func = func
                d2[key2] = bi
",if bi . commandName == commandName :,118
"def _lookup(components, specs, provided, name, i, l):
    if i < l:
        for spec in specs[i].__sro__:
            comps = components.get(spec)
            if comps:
                r = _lookup(comps, specs, provided, name, i + 1, l)
                if r is not None:
                    return r
    else:
        for iface in provided:
            comps = components.get(iface)
            if comps:
                r = comps.get(name)
                if r is not None:
                    return r
    return None
",if comps :,166
"def to_representation(self, value):
    old_social_string_fields = [""twitter"", ""github"", ""linkedIn""]
    request = self.context.get(""request"")
    show_old_format = (
        request
        and is_deprecated(request.version, self.min_version)
        and request.method == ""GET""
    )
    if show_old_format:
        social = value.copy()
        for key in old_social_string_fields:
            if social.get(key):
                social[key] = value[key][0]
            elif social.get(key) == []:
                social[key] = """"
        value = social
    return super(SocialField, self).to_representation(value)
",elif social . get ( key ) == [ ] :,200
"def process_ref_attribute(self, node, array_type=None):
    ref = qname_attr(node, ""ref"")
    if ref:
        ref = self._create_qname(ref)
        # Some wsdl's reference to xs:schema, we ignore that for now. It
        # might be better in the future to process the actual schema file
        # so that it is handled correctly
        if ref.namespace == ""http://www.w3.org/2001/XMLSchema"":
            return
        return xsd_elements.RefAttribute(
            node.tag, ref, self.schema, array_type=array_type
        )
","if ref . namespace == ""http://www.w3.org/2001/XMLSchema"" :",161
"def unescape(text):
    """"""Removes '\\' escaping from 'text'.""""""
    rv = """"
    i = 0
    while i < len(text):
        if i + 1 < len(text) and text[i] == ""\\"":
            rv += text[i + 1]
            i += 1
        else:
            rv += text[i]
        i += 1
    return rv
","if i + 1 < len ( text ) and text [ i ] == ""\\"" :",99
"def wait_child_process(signum, frame):
    try:
        while True:
            child_pid, status = os.waitpid(-1, os.WNOHANG)
            if child_pid == 0:
                stat_logger.info(""no child process was immediately available"")
                break
            exitcode = status >> 8
            stat_logger.info(
                ""child process %s exit with exitcode %s"", child_pid, exitcode
            )
    except OSError as e:
        if e.errno == errno.ECHILD:
            stat_logger.warning(
                ""current process has no existing unwaited-for child processes.""
            )
        else:
            raise
",if e . errno == errno . ECHILD :,190
"def translate_from_sortname(name, sortname):
    """"""'Translate' the artist name by reversing the sortname.""""""
    for c in name:
        ctg = unicodedata.category(c)
        if ctg[0] == ""L"" and unicodedata.name(c).find(""LATIN"") == -1:
            for separator in ("" & "", ""; "", "" and "", "" vs. "", "" with "", "" y ""):
                if separator in sortname:
                    parts = sortname.split(separator)
                    break
            else:
                parts = [sortname]
                separator = """"
            return separator.join(map(_reverse_sortname, parts))
    return name
",if separator in sortname :,181
"def python_value(self, value):
    if value:
        if isinstance(value, basestring):
            pp = lambda x: x.time()
            return format_date_time(value, self.formats, pp)
        elif isinstance(value, datetime.datetime):
            return value.time()
    if value is not None and isinstance(value, datetime.timedelta):
        return (datetime.datetime.min + value).time()
    return value
","if isinstance ( value , basestring ) :",113
"def __init__(self, fileobj, info):
    pages = []
    complete = False
    while not complete:
        page = OggPage(fileobj)
        if page.serial == info.serial:
            pages.append(page)
            complete = page.complete or (len(page.packets) > 1)
    data = OggPage.to_packets(pages)[0][7:]
    super(OggTheoraCommentDict, self).__init__(data, framing=False)
    self._padding = len(data) - self._size
",if page . serial == info . serial :,133
"def configure(self):
    # hack to configure 'from_' and 'to' and avoid exception
    if ""from_"" in self.wmeta.properties:
        from_ = float(self.wmeta.properties[""from_""])
        to = float(self.wmeta.properties.get(""to"", 0))
        if from_ > to:
            to = from_ + 1
            self.wmeta.properties[""to""] = str(to)
    super(TKSpinbox, self).configure()
",if from_ > to :,123
"def get_error_diagnostics(self):
    diagnostics = []
    if self.stdout is not None:
        with open(self.stdout.name) as fds:
            contents = fds.read().strip()
            if contents.strip():
                diagnostics.append(""ab STDOUT:\n"" + contents)
    if self.stderr is not None:
        with open(self.stderr.name) as fds:
            contents = fds.read().strip()
            if contents.strip():
                diagnostics.append(""ab STDERR:\n"" + contents)
    return diagnostics
",if contents . strip ( ) :,156
"def set_environment_vars(env, source_env):
    """"""Copy allowed environment variables from |source_env|.""""""
    if not source_env:
        return
    for name, value in six.iteritems(source_env):
        if is_forwarded_environment_variable(name):
            # Avoid creating circular dependencies from importing environment by
            # using os.getenv.
            if os.getenv(""TRUSTED_HOST"") and should_rebase_environment_value(name):
                value = file_host.rebase_to_worker_root(value)
            env[name] = value
",if is_forwarded_environment_variable ( name ) :,152
"def update_content(self, more_content: StringList) -> None:
    if isinstance(self.object, TypeVar):
        attrs = [repr(self.object.__name__)]
        for constraint in self.object.__constraints__:
            attrs.append(stringify_typehint(constraint))
        if self.object.__covariant__:
            attrs.append(""covariant=True"")
        if self.object.__contravariant__:
            attrs.append(""contravariant=True"")
        more_content.append(_(""alias of TypeVar(%s)"") % "", "".join(attrs), """")
        more_content.append("""", """")
    super().update_content(more_content)
",if self . object . __contravariant__ :,160
"def after(self, event, state):
    group = event.group
    for plugin in self.get_plugins():
        if not safe_execute(plugin.should_notify, group=group, event=event):
            continue
        metrics.incr(""notifications.sent"", instance=plugin.slug)
        yield self.future(plugin.rule_notify)
","if not safe_execute ( plugin . should_notify , group = group , event = event ) :",87
"def distinct(expr, *on):
    fields = frozenset(expr.fields)
    _on = []
    append = _on.append
    for n in on:
        if isinstance(n, Field):
            if n._child.isidentical(expr):
                n = n._name
            else:
                raise ValueError(""{0} is not a field of {1}"".format(n, expr))
        if not isinstance(n, _strtypes):
            raise TypeError(""on must be a name or field, not: {0}"".format(n))
        elif n not in fields:
            raise ValueError(""{0} is not a field of {1}"".format(n, expr))
        append(n)
    return Distinct(expr, tuple(_on))
",if n . _child . isidentical ( expr ) :,192
"def build_filter(arg):
    filt = {}
    if arg is not None:
        if ""="" not in arg:
            raise UserError(""Arguments to --filter should be in form KEY=VAL"")
        key, val = arg.split(""="", 1)
        filt[key] = val
    return filt
","if ""="" not in arg :",75
"def pickline(file, key, casefold=1):
    try:
        f = open(file, ""r"")
    except IOError:
        return None
    pat = re.escape(key) + "":""
    prog = re.compile(pat, casefold and re.IGNORECASE)
    while 1:
        line = f.readline()
        if not line:
            break
        if prog.match(line):
            text = line[len(key) + 1 :]
            while 1:
                line = f.readline()
                if not line or not line[0].isspace():
                    break
                text = text + line
            return text.strip()
    return None
",if prog . match ( line ) :,182
"def delete_doc(elastic_document_id, node, index=None, category=None):
    index = index or INDEX
    if not category:
        if isinstance(node, Preprint):
            category = ""preprint""
        elif node.is_registration:
            category = ""registration""
        else:
            category = node.project_or_component
    client().delete(
        index=index,
        doc_type=category,
        id=elastic_document_id,
        refresh=True,
        ignore=[404],
    )
","if isinstance ( node , Preprint ) :",143
"def update(self, preds, labels):
    if not _is_numpy_(labels):
        raise ValueError(""The 'labels' must be a numpy ndarray."")
    if not _is_numpy_(preds):
        raise ValueError(""The 'predictions' must be a numpy ndarray."")
    for i, lbl in enumerate(labels):
        value = preds[i, 1]
        bin_idx = int(value * self._num_thresholds)
        assert bin_idx <= self._num_thresholds
        if lbl:
            self._stat_pos[bin_idx] += 1.0
        else:
            self._stat_neg[bin_idx] += 1.0
",if lbl :,163
"def checkStatusClient(self):
    if str(self.comboxBoxIPAddress.currentText()) != """":
        if self.ClientsLogged[str(self.comboxBoxIPAddress.currentText())][""Status""]:
            self.btnEnable.setEnabled(False)
            self.btncancel.setEnabled(True)
            return None
        self.btnEnable.setEnabled(True)
        self.btncancel.setEnabled(False)
","if self . ClientsLogged [ str ( self . comboxBoxIPAddress . currentText ( ) ) ] [ ""Status"" ] :",111
"def colorizeDiffs(sheet, col, row, cellval):
    if not row or not col:
        return None
    vcolidx = sheet.visibleCols.index(col)
    rowidx = sheet.rows.index(row)
    if vcolidx < len(othersheet.visibleCols) and rowidx < len(othersheet.rows):
        otherval = othersheet.visibleCols[vcolidx].getDisplayValue(
            othersheet.rows[rowidx]
        )
        if cellval.display != otherval:
            return ""color_diff""
    else:
        return ""color_diff_add""
",if cellval . display != otherval :,162
"def identwaf(self, findall=False):
    detected = list()
    try:
        self.attackres = self.performCheck(self.centralAttack)
    except RequestBlocked:
        return detected
    for wafvendor in self.checklist:
        self.log.info(""Checking for %s"" % wafvendor)
        if self.wafdetections[wafvendor](self):
            detected.append(wafvendor)
            if not findall:
                break
    self.knowledge[""wafname""] = detected
    return detected
",if self . wafdetections [ wafvendor ] ( self ) :,143
"def get_repository_metadata_by_repository_id_changeset_revision(
    app, id, changeset_revision, metadata_only=False
):
    """"""Get a specified metadata record for a specified repository in the tool shed.""""""
    if metadata_only:
        repository_metadata = get_repository_metadata_by_changeset_revision(
            app, id, changeset_revision
        )
        if repository_metadata and repository_metadata.metadata:
            return repository_metadata.metadata
        return None
    return get_repository_metadata_by_changeset_revision(app, id, changeset_revision)
",if repository_metadata and repository_metadata . metadata :,145
"def getmultiline(self):
    line = self.getline()
    if line[3:4] == ""-"":
        code = line[:3]
        while 1:
            nextline = self.getline()
            line = line + (""\n"" + nextline)
            if nextline[:3] == code and nextline[3:4] != ""-"":
                break
    return line
","if nextline [ : 3 ] == code and nextline [ 3 : 4 ] != ""-"" :",100
"def _validate_reports(value, *args, **kwargs):
    from osf.models import OSFUser
    for key, val in value.items():
        if not OSFUser.load(key):
            raise ValidationValueError(""Keys must be user IDs"")
        if not isinstance(val, dict):
            raise ValidationTypeError(""Values must be dictionaries"")
        if (
            ""category"" not in val
            or ""text"" not in val
            or ""date"" not in val
            or ""retracted"" not in val
        ):
            raise ValidationValueError(
                (""Values must include `date`, `category`, "", ""`text`, `retracted` keys"")
            )
","if not isinstance ( val , dict ) :",179
"def deselectItem(self, item):
    if self.isSelected(item):
        if self.multiSelect:
            listItem = self._getListItem(item)
            selections = self.getSelectedItems()
            selections.remove(self.loadHandler.getSelection(listItem))
            self.setSelections(selections)
        else:
            self.deselectAll()
",if self . multiSelect :,101
"def __init__(self, **kwargs):
    if self.name is None:
        raise RuntimeError(""RenderPrimitive cannot be used directly"")
    self.option_values = {}
    for key, val in kwargs.items():
        if not key in self.options:
            raise ValueError(
                ""primitive `{0}' has no option `{1}'"".format(self.name, key)
            )
        self.option_values[key] = val
    # set up defaults
    for name, (description, default) in self.options.items():
        if not name in self.option_values:
            self.option_values[name] = default
",if not name in self . option_values :,162
"def setup_smart_indent(self, view, lang):
    # Configure a ""per-view"" instance
    if type(view) == gedit.View:
        if getattr(view, ""smart_indent_instance"", False) == False:
            setattr(view, ""smart_indent_instance"", SmartIndent())
            handler_id = view.connect(
                ""key-press-event"", view.smart_indent_instance.key_press_handler
            )
            self.handler_ids.append((handler_id, view))
        view.smart_indent_instance.set_language(lang, view)
","if getattr ( view , ""smart_indent_instance"" , False ) == False :",157
"def get_strings_of_set(word, char_set, threshold=20):
    count = 0
    letters = """"
    strings = []
    for char in word:
        if char in char_set:
            letters += char
            count += 1
        else:
            if count > threshold:
                strings.append(letters)
            letters = """"
            count = 0
    if count > threshold:
        strings.append(letters)
    return strings
",if char in char_set :,125
"def TryMerge(self, d):
    while d.avail() > 0:
        tt = d.getVarInt32()
        if tt == 10:
            self.set_logout_url(d.getPrefixedString())
            continue
        if tt == 0:
            raise ProtocolBuffer.ProtocolBufferDecodeError
        d.skipData(tt)
",if tt == 0 :,92
"def __create_table(self):
    for i in range(256):
        crcreg = i
        for j in range(8):
            if (crcreg & 1) != 0:
                crcreg = self.__CRCPOLYNOMIAL ^ (crcreg >> 1)
            else:
                crcreg >>= 1
        self.__crctable[i] = crcreg
",if ( crcreg & 1 ) != 0 :,100
"def destroy(self):
    """"""Flush all entries and empty cache""""""
    # Note: this method is currently also used for dropping the cache
    for i in range(len(self.cached_rows)):
        id_ = self.cached_rows[i]
        self.cached_rows[i] = None
        if id_ is not None:
            try:
                inode = self.attrs[id_]
            except KeyError:
                # We may have deleted that inode
                pass
            else:
                del self.attrs[id_]
                self.setattr(inode)
    assert len(self.attrs) == 0
",if id_ is not None :,167
"def set_config(self):
    """"""Set configuration options for QTextEdit.""""""
    c = self.c
    w = self.widget
    w.setWordWrapMode(QtGui.QTextOption.NoWrap)
    if 0:  # This only works when there is no style sheet.
        n = c.config.getInt(""qt-rich-text-zoom-in"")
        if n not in (None, 0):
            w.zoomIn(n)
            w.updateMicroFocus()
    # tab stop in pixels - no config for this (yet)
    w.setTabStopWidth(24)
","if n not in ( None , 0 ) :",154
"def mouseDragEvent(self, ev):
    if self.movable and ev.button() == QtCore.Qt.LeftButton:
        if ev.isStart():
            self.moving = True
            self.cursorOffset = self.pos() - self.mapToParent(ev.buttonDownPos())
            self.startPosition = self.pos()
        ev.accept()
        if not self.moving:
            return
        self.setPos(self.cursorOffset + self.mapToParent(ev.pos()))
        self.sigDragged.emit(self)
        if ev.isFinish():
            self.moving = False
            self.sigPositionChangeFinished.emit(self)
",if ev . isFinish ( ) :,178
"def reparentChildren(self, newParent):
    if newParent.childNodes:
        newParent.childNodes[-1]._element.tail += self._element.text
    else:
        if not newParent._element.text:
            newParent._element.text = """"
        if self._element.text is not None:
            newParent._element.text += self._element.text
    self._element.text = """"
    base.Node.reparentChildren(self, newParent)
",if self . _element . text is not None :,121
"def _no_sp_or_bp(self, bl):
    for s in bl.vex.statements:
        for e in chain([s], s.expressions):
            if e.tag == ""Iex_Get"":
                reg = self.get_reg_name(self.project.arch, e.offset)
                if reg == ""ebp"" or reg == ""esp"":
                    return False
            elif e.tag == ""Ist_Put"":
                reg = self.get_reg_name(self.project.arch, e.offset)
                if reg == ""ebp"" or reg == ""esp"":
                    return False
    return True
","if e . tag == ""Iex_Get"" :",176
"def _get_import_chain(self, *, until=None):
    stack = inspect.stack()[2:]
    try:
        for frameinfo in stack:
            try:
                if not frameinfo.code_context:
                    continue
                data = dedent("""".join(frameinfo.code_context))
                if data.strip() == until:
                    raise StopIteration
                yield frameinfo.filename, frameinfo.lineno, data.strip()
                del data
            finally:
                del frameinfo
    finally:
        del stack
",if not frameinfo . code_context :,155
"def stream_docker_log(log_stream):
    async for line in log_stream:
        if ""stream"" in line and line[""stream""].strip():
            logger.debug(line[""stream""].strip())
        elif ""status"" in line:
            logger.debug(line[""status""].strip())
        elif ""error"" in line:
            logger.error(line[""error""].strip())
            raise DockerBuildError
","elif ""error"" in line :",108
"def get_cycle_path(self, curr_node, goal_node_index):
    for dep in curr_node[""deps""]:
        if dep == goal_node_index:
            return [curr_node[""address""]]
    for dep in curr_node[""deps""]:
        path = self.get_cycle_path(
            self.get_by_address(dep), goal_node_index
        )  # self.nodelist[dep], goal_node_index)
        if len(path) > 0:
            path.insert(0, curr_node[""address""])
            return path
    return []
",if len ( path ) > 0 :,153
"def prompt(default=None):
    editor = ""nano""
    with tempfile.NamedTemporaryFile(mode=""r+"") as tmpfile:
        if default:
            tmpfile.write(default)
            tmpfile.flush()
        child_pid = os.fork()
        is_child = child_pid == 0
        if is_child:
            os.execvp(editor, [editor, tmpfile.name])
        else:
            os.waitpid(child_pid, 0)
            tmpfile.seek(0)
            return tmpfile.read().strip()
",if default :,143
"def _get_annotated_template(self, template):
    changed = False
    if template.get(""version"", ""0.12.0"") >= ""0.13.0"":
        using_js = self.spider._filter_js_urls(template[""url""])
        body = ""rendered_body"" if using_js else ""original_body""
        if template.get(""body"") != body:
            template[""body""] = body
            changed = True
    if changed or not template.get(""annotated""):
        _build_sample(template)
    return template
","if template . get ( ""body"" ) != body :",139
"def collect(self, paths):
    for path in paths or ():
        relpath = os.path.relpath(path, self._artifact_root)
        dst = os.path.join(self._directory, relpath)
        safe_mkdir(os.path.dirname(dst))
        if os.path.isdir(path):
            shutil.copytree(path, dst)
        else:
            shutil.copy(path, dst)
        self._relpaths.add(relpath)
",if os . path . isdir ( path ) :,120
"def dependencies(context=None):
    """"""Return all dependencies detected by knowit.""""""
    deps = OrderedDict([])
    try:
        initialize(context)
        for name, provider_cls in _provider_map.items():
            if name in available_providers:
                deps[name] = available_providers[name].version
            else:
                deps[name] = {}
    except Exception:
        pass
    return deps
",if name in available_providers :,111
"def _getaddrinfo(self, host_bytes, port, family, socktype, proto, flags):
    while True:
        ares = self.cares
        try:
            return self.__getaddrinfo(host_bytes, port, family, socktype, proto, flags)
        except gaierror:
            if ares is self.cares:
                raise
",if ares is self . cares :,96
"def write_entries(cmd, basename, filename):
    ep = cmd.distribution.entry_points
    if isinstance(ep, basestring) or ep is None:
        data = ep
    elif ep is not None:
        data = []
        for section, contents in ep.items():
            if not isinstance(contents, basestring):
                contents = EntryPoint.parse_group(section, contents)
                contents = ""\n"".join(map(str, contents.values()))
            data.append(""[%s]\n%s\n\n"" % (section, contents))
        data = """".join(data)
    cmd.write_or_delete_file(""entry points"", filename, data, True)
","if not isinstance ( contents , basestring ) :",174
"def _highlight_do(self):
    new_hl_text = self.highlight_text.text()
    if new_hl_text != self.hl_text:
        self.hl_text = new_hl_text
        if self.hl is not None:
            self.hl.setDocument(None)
            self.hl = None
        if self.hl_text:
            self.hl = Highlighter(self.hl_text, parent=self.doc)
        self.clear_highlight_button.setEnabled(bool(self.hl))
",if self . hl_text :,151
"def traverse(node, functions=[]):
    if hasattr(node, ""grad_fn""):
        node = node.grad_fn
    if hasattr(node, ""variable""):
        node = graph.nodes_by_id.get(id(node.variable))
        if node:
            node.functions = list(functions)
            del functions[:]
    if hasattr(node, ""next_functions""):
        functions.append(type(node).__name__)
        for f in node.next_functions:
            if f[0]:
                functions.append(type(f[0]).__name__)
                traverse(f[0], functions)
    if hasattr(node, ""saved_tensors""):
        for t in node.saved_tensors:
            traverse(t)
",if node :,195
"def compress(self, data_list):
    if data_list:
        page_id = data_list[1]
        if page_id in EMPTY_VALUES:
            if not self.required:
                return None
            raise forms.ValidationError(self.error_messages[""invalid_page""])
        return Page.objects.get(pk=page_id)
    return None
",if not self . required :,98
"def test_field_attr_existence(self):
    for name, item in ast.__dict__.items():
        if self._is_ast_node(name, item):
            if name == ""Index"":
                # Index(value) just returns value now.
                # The argument is required.
                continue
            x = item()
            if isinstance(x, ast.AST):
                self.assertEqual(type(x._fields), tuple)
","if isinstance ( x , ast . AST ) :",122
"def handle_starttag(self, tag, attrs):
    if tag == ""base"":
        self.base_url = dict(attrs).get(""href"")
    if self.scan_tag(tag):
        for attr, value in attrs:
            if self.scan_attr(attr):
                if self.strip:
                    value = strip_html5_whitespace(value)
                url = self.process_attr(value)
                link = Link(url=url)
                self.links.append(link)
                self.current_link = link
",if self . strip :,151
"def _initialize_asset_map(cls):
    # Generating a list of acceptable asset files reduces the possibility of
    # path attacks.
    cls._asset_name_to_path = {}
    assets = os.listdir(ASSETS_PATH)
    for asset in assets:
        path = os.path.join(ASSETS_PATH, asset)
        if os.path.isfile(path):
            cls._asset_name_to_path[os.path.basename(path)] = path
",if os . path . isfile ( path ) :,120
"def dataReceived(self, data):
    self.buf += data
    if self._paused:
        log.startLogging(sys.stderr)
        log.msg(""dataReceived while transport paused!"")
        self.transport.loseConnection()
    else:
        self.transport.write(data)
        if self.buf.endswith(b""\n0\n""):
            self.transport.loseConnection()
        else:
            self.pause()
","if self . buf . endswith ( b""\n0\n"" ) :",114
"def test_case_sensitive(self):
    with support.EnvironmentVarGuard() as env:
        env.unset(""PYTHONCASEOK"")
        if b""PYTHONCASEOK"" in _bootstrap_external._os.environ:
            self.skipTest(""os.environ changes not reflected in "" ""_os.environ"")
        loader = self.find_module()
        self.assertIsNone(loader)
","if b""PYTHONCASEOK"" in _bootstrap_external . _os . environ :",92
"def manifest(self):
    """"""The current manifest dictionary.""""""
    if self.reload:
        if not self.exists(self.manifest_path):
            return {}
        mtime = self.getmtime(self.manifest_path)
        if self._mtime is None or mtime > self._mtime:
            self._manifest = self.get_manifest()
            self._mtime = mtime
    return self._manifest
",if self . _mtime is None or mtime > self . _mtime :,102
"def test_named_parameters_and_constraints(self):
    likelihood = gpytorch.likelihoods.GaussianLikelihood()
    model = ExactGPModel(None, None, likelihood)
    for name, _param, constraint in model.named_parameters_and_constraints():
        if name == ""likelihood.noise_covar.raw_noise"":
            self.assertIsInstance(constraint, gpytorch.constraints.GreaterThan)
        elif name == ""mean_module.constant"":
            self.assertIsNone(constraint)
        elif name == ""covar_module.raw_outputscale"":
            self.assertIsInstance(constraint, gpytorch.constraints.Positive)
        elif name == ""covar_module.base_kernel.raw_lengthscale"":
            self.assertIsInstance(constraint, gpytorch.constraints.Positive)
","if name == ""likelihood.noise_covar.raw_noise"" :",192
"def process_plugin_result(name, result):
    if result:
        try:
            jsonify(test=result)
        except Exception:
            logger.exception(
                ""Error while jsonifying settings from plugin {}, please contact the plugin author about this"".format(
                    name
                )
            )
            raise
        else:
            if ""__enabled"" in result:
                del result[""__enabled""]
            data[name] = result
","if ""__enabled"" in result :",129
"def benchmarking(net, ctx, num_iteration, datashape=300, batch_size=64):
    input_shape = (batch_size, 3) + (datashape, datashape)
    data = mx.random.uniform(-1.0, 1.0, shape=input_shape, ctx=ctx, dtype=""float32"")
    dryrun = 5
    for i in range(dryrun + num_iteration):
        if i == dryrun:
            tic = time.time()
        ids, scores, bboxes = net(data)
        ids.asnumpy()
        scores.asnumpy()
        bboxes.asnumpy()
    toc = time.time() - tic
    return toc
",if i == dryrun :,165
"def merge_weekdays(base_wd, icu_wd):
    result = []
    for left, right in zip(base_wd, icu_wd):
        if left == right:
            result.append(left)
            continue
        left = set(left.split(""|""))
        right = set(right.split(""|""))
        result.append(""|"".join(left | right))
    return result
",if left == right :,104
"def create_key(self, request):
    if self._ignored_parameters:
        url, body = self._remove_ignored_parameters(request)
    else:
        url, body = request.url, request.body
    key = hashlib.sha256()
    key.update(_to_bytes(request.method.upper()))
    key.update(_to_bytes(url))
    if request.body:
        key.update(_to_bytes(body))
    else:
        if self._include_get_headers and request.headers != _DEFAULT_HEADERS:
            for name, value in sorted(request.headers.items()):
                key.update(_to_bytes(name))
                key.update(_to_bytes(value))
    return key.hexdigest()
",if self . _include_get_headers and request . headers != _DEFAULT_HEADERS :,190
"def test_invalid_mountinfo(self):
    line = (
        ""20 1 252:1 / / rw,relatime - ext4 /dev/mapper/vg0-root""
        ""rw,errors=remount-ro,data=ordered""
    )
    elements = line.split()
    for i in range(len(elements) + 1):
        lines = ["" "".join(elements[0:i])]
        if i < 10:
            expected = None
        else:
            expected = (""/dev/mapper/vg0-root"", ""ext4"", ""/"")
        self.assertEqual(expected, util.parse_mount_info(""/"", lines))
",if i < 10 :,161
"def nested_filter(self, items, mask):
    keep_current = self.current_mask(mask)
    keep_nested_lookup = self.nested_masks(mask)
    for k, v in items:
        keep_nested = keep_nested_lookup.get(k)
        if k in keep_current:
            if keep_nested is not None:
                if isinstance(v, dict):
                    yield k, dict(self.nested_filter(v.items(), keep_nested))
            else:
                yield k, v
","if isinstance ( v , dict ) :",142
"def traverse_trees(node_pos, sample, trees: List[HeteroDecisionTreeGuest]):
    if node_pos[""reach_leaf_node""].all():
        return node_pos
    for t_idx, tree in enumerate(trees):
        cur_node_idx = node_pos[""node_pos""][t_idx]
        # reach leaf
        if cur_node_idx == -1:
            continue
        rs, reach_leaf = HeteroSecureBoostingTreeGuest.traverse_a_tree(
            tree, sample, cur_node_idx
        )
        if reach_leaf:
            node_pos[""reach_leaf_node""][t_idx] = True
        node_pos[""node_pos""][t_idx] = rs
    return node_pos
",if reach_leaf :,196
"def _pop_waiting_trial_id(self) -> Optional[int]:
    # TODO(c-bata): Reduce database query counts for extracting waiting trials.
    for trial in self._storage.get_all_trials(self._study_id, deepcopy=False):
        if trial.state != TrialState.WAITING:
            continue
        if not self._storage.set_trial_state(trial._trial_id, TrialState.RUNNING):
            continue
        _logger.debug(""Trial {} popped from the trial queue."".format(trial.number))
        return trial._trial_id
    return None
",if trial . state != TrialState . WAITING :,150
"def get_step_best(self, step_models):
    best_score = None
    best_model = """"
    for model in step_models:
        model_info = self.models_trained[model]
        score = model_info.get_score()
        if score is None:
            continue
        if best_score is None or score < best_score:
            best_score = score
            best_model = model
    LOGGER.info(f""step {self.n_step}, best model {best_model}"")
    return best_model
",if score is None :,142
"def iter_filters(filters, block_end=False):
    queue = deque(filters)
    while queue:
        f = queue.popleft()
        if f is not None and f.type in (""or"", ""and"", ""not""):
            if block_end:
                queue.appendleft(None)
            for gf in f.filters:
                queue.appendleft(gf)
        yield f
",if block_end :,105
"def _buffer_decode(self, input, errors, final):
    if self.decoder is None:
        (output, consumed, byteorder) = codecs.utf_16_ex_decode(input, errors, 0, final)
        if byteorder == -1:
            self.decoder = codecs.utf_16_le_decode
        elif byteorder == 1:
            self.decoder = codecs.utf_16_be_decode
        elif consumed >= 2:
            raise UnicodeError(""UTF-16 stream does not start with BOM"")
        return (output, consumed)
    return self.decoder(input, self.errors, final)
",elif byteorder == 1 :,156
"def _load_db(self):
    try:
        with open(self.db) as db:
            content = db.read(8)
            db.seek(0)
            if content == (""Salted__""):
                data = StringIO()
                if self.encryptor:
                    self.encryptor.decrypt(db, data)
                else:
                    raise EncryptionError(
                        ""Encrpyted credential storage: {}"".format(self.db)
                    )
                return json.loads(data.getvalue())
            else:
                return json.load(db)
    except:
        return {""creds"": []}
","if content == ( ""Salted__"" ) :",187
"def _getbytes(self, start, l=1):
    out = []
    for ad in range(l):
        offset = ad + start + self.base_address
        if not is_mapped(offset):
            raise IOError(""not enough bytes"")
        out.append(int_to_byte(Byte(offset)))
    return b"""".join(out)
",if not is_mapped ( offset ) :,90
"def cache_sqs_queues_across_accounts() -> bool:
    function: str = f""{__name__}.{sys._getframe().f_code.co_name}""
    # First, get list of accounts
    accounts_d: list = async_to_sync(get_account_id_to_name_mapping)()
    # Second, call tasks to enumerate all the roles across all accounts
    for account_id in accounts_d.keys():
        if config.get(""environment"") == ""prod"":
            cache_sqs_queues_for_account.delay(account_id)
        else:
            if account_id in config.get(""celery.test_account_ids"", []):
                cache_sqs_queues_for_account.delay(account_id)
    stats.count(f""{function}.success"")
    return True
","if account_id in config . get ( ""celery.test_account_ids"" , [ ] ) :",200
"def insertLine(self, refnum, linenum, line):
    i = -1
    for i, row in enumerate(self.rows):
        if row[0] == linenum:
            if row[refnum + 1] is None:
                row[refnum + 1] = line
                return
            # else keep looking
        elif row[0] > linenum:
            break
    self.rows.insert(i, self.newRow(linenum, refnum, line))
",if row [ refnum + 1 ] is None :,125
"def __setattr__(self, name, val):
    if self.__dict__.get(name, ""hamster_graphics_no_value_really"") == val:
        return
    Sprite.__setattr__(self, name, val)
    if name == ""image_data"":
        self._surface = None
        if self.image_data:
            self.__dict__[""width""] = self.image_data.get_width()
            self.__dict__[""height""] = self.image_data.get_height()
",if self . image_data :,126
"def process_signature(app, what, name, obj, options, signature, return_annotation):
    if signature:
        # replace Mock function names
        signature = re.sub(""<Mock name='([^']+)'.*>"", ""\g<1>"", signature)
        signature = re.sub(""tensorflow"", ""tf"", signature)
        # add scope name to layer signatures:
        if hasattr(obj, ""use_scope""):
            if obj.use_scope:
                signature = signature[0] + ""variable_scope_name, "" + signature[1:]
            elif obj.use_scope is None:
                signature = signature[0] + ""[variable_scope_name,] "" + signature[1:]
    # signature: arg list
    return signature, return_annotation
",if obj . use_scope :,188
"def L_op(self, inputs, outputs, gout):
    (x,) = inputs
    (gz,) = gout
    if x.type in complex_types:
        raise NotImplementedError()
    if outputs[0].type in discrete_types:
        if x.type in discrete_types:
            return [x.zeros_like(dtype=theano.config.floatX)]
        else:
            return [x.zeros_like()]
    return (gz * (1 - sqr(tanh(x))),)
",if x . type in discrete_types :,124
"def confirm_on_console(topic, msg):
    done = False
    print(topic)
    while not done:
        output = raw_input(msg + "":[y/n]"")
        if output.lower() == ""y"":
            return True
        if output.lower() == ""n"":
            return False
","if output . lower ( ) == ""y"" :",82
"def replace_documentation_for_matching_shape(self, event_name, section, **kwargs):
    if self._shape_name == section.context.get(""shape""):
        self._replace_documentation(event_name, section)
    for section_name in section.available_sections:
        sub_section = section.get_section(section_name)
        if self._shape_name == sub_section.context.get(""shape""):
            self._replace_documentation(event_name, sub_section)
        else:
            self.replace_documentation_for_matching_shape(event_name, sub_section)
","if self . _shape_name == sub_section . context . get ( ""shape"" ) :",152
"def confirm_on_console(topic, msg):
    done = False
    print(topic)
    while not done:
        output = raw_input(msg + "":[y/n]"")
        if output.lower() == ""y"":
            return True
        if output.lower() == ""n"":
            return False
","if output . lower ( ) == ""n"" :",82
"def __getitem__(self, index):
    if self._check():
        if isinstance(index, int):
            if index < 0 or index >= len(self.features):
                raise IndexError(index)
            if self.features[index] is None:
                feature = self.device.feature_request(FEATURE.FEATURE_SET, 0x10, index)
                if feature:
                    (feature,) = _unpack(""!H"", feature[:2])
                    self.features[index] = FEATURE[feature]
            return self.features[index]
        elif isinstance(index, slice):
            indices = index.indices(len(self.features))
            return [self.__getitem__(i) for i in range(*indices)]
",if index < 0 or index >= len ( self . features ) :,195
"def _parse_locator(self, locator):
    prefix = None
    criteria = locator
    if not locator.startswith(""//""):
        locator_parts = locator.partition(""="")
        if len(locator_parts[1]) > 0:
            prefix = locator_parts[0]
            criteria = locator_parts[2].strip()
    return (prefix, criteria)
",if len ( locator_parts [ 1 ] ) > 0 :,89
"def trakt_episode_data_generate(self, data):
    # Find how many unique season we have
    uniqueSeasons = []
    for season, episode in data:
        if season not in uniqueSeasons:
            uniqueSeasons.append(season)
    # build the query
    seasonsList = []
    for searchedSeason in uniqueSeasons:
        episodesList = []
        for season, episode in data:
            if season == searchedSeason:
                episodesList.append({""number"": episode})
        seasonsList.append({""number"": searchedSeason, ""episodes"": episodesList})
    post_data = {""seasons"": seasonsList}
    return post_data
",if season not in uniqueSeasons :,189
"def __init__(self, data, n_bins):
    bin_width = span / n_bins
    bins = [0] * n_bins
    for x in data:
        b = int(mpfloor((x - minimum) / bin_width))
        if b < 0:
            b = 0
        elif b >= n_bins:
            b = n_bins - 1
        bins[b] += 1
    self.bins = bins
    self.bin_width = bin_width
",if b < 0 :,124
"def infer_context(typ, context=""http://schema.org""):
    parsed_context = urlparse(typ)
    if parsed_context.netloc:
        base = """".join([parsed_context.scheme, ""://"", parsed_context.netloc])
        if parsed_context.path and parsed_context.fragment:
            context = urljoin(base, parsed_context.path)
            typ = parsed_context.fragment.strip(""/"")
        elif parsed_context.path:
            context = base
            typ = parsed_context.path.strip(""/"")
    return context, typ
",if parsed_context . path and parsed_context . fragment :,140
"def parse(self, items):
    for index, item in enumerate(items):
        keys = self.build_key(item)
        if keys is None:
            continue
        # Update `items`
        self.items[tuple(keys)] = (index, item)
        # Update `table`
        if not self.path_set(self.table, keys, (index, item)):
            log.info(""Unable to update table (keys: %r)"", keys)
","if not self . path_set ( self . table , keys , ( index , item ) ) :",120
"def dict_to_XML(tag, dictionary, **kwargs):
    """"""Return XML element converting dicts recursively.""""""
    elem = Element(tag, **kwargs)
    for key, val in dictionary.items():
        if tag == ""layers"":
            child = dict_to_XML(""layer"", val, name=key)
        elif isinstance(val, MutableMapping):
            child = dict_to_XML(key, val)
        else:
            if tag == ""config"":
                child = Element(""variable"", name=key)
            else:
                child = Element(key)
            child.text = str(val)
        elem.append(child)
    return elem
","if tag == ""config"" :",175
"def _get_config_value(self, section, key):
    if section:
        if section not in self.config:
            self.log.error(""Error: Config section '%s' not found"", section)
            return None
        return self.config[section].get(key, self.config[key])
    else:
        return self.config[key]
",if section not in self . config :,93
"def h_line_down(self, input):
    end_this_line = self.value.find(""\n"", self.cursor_position)
    if end_this_line == -1:
        if self.scroll_exit:
            self.h_exit_down(None)
        else:
            self.cursor_position = len(self.value)
    else:
        self.cursor_position = end_this_line + 1
        for x in range(self.cursorx):
            if self.cursor_position > len(self.value) - 1:
                break
            elif self.value[self.cursor_position] == ""\n"":
                break
            else:
                self.cursor_position += 1
",if self . cursor_position > len ( self . value ) - 1 :,193
"def printsumfp(fp, filename, out=sys.stdout):
    m = md5()
    try:
        while 1:
            data = fp.read(bufsize)
            if not data:
                break
            if isinstance(data, str):
                data = data.encode(fp.encoding)
            m.update(data)
    except IOError as msg:
        sys.stderr.write(""%s: I/O error: %s\n"" % (filename, msg))
        return 1
    out.write(""%s %s\n"" % (m.hexdigest(), filename))
    return 0
",if not data :,159
"def main(input):
    logging.info(""Running Azure Cloud Custodian Policy %s"", input)
    context = {
        ""config_file"": join(function_directory, ""config.json""),
        ""auth_file"": join(function_directory, ""auth.json""),
    }
    event = None
    subscription_id = None
    if isinstance(input, QueueMessage):
        if input.dequeue_count > max_dequeue_count:
            return
        event = input.get_json()
        subscription_id = ResourceIdParser.get_subscription_id(event[""subject""])
    handler.run(event, context, subscription_id)
",if input . dequeue_count > max_dequeue_count :,166
"def maybeExtractTarball(self):
    if self.tarball:
        tar = self.computeTarballOptions() + [""-xvf"", self.tarball]
        res = yield self._Cmd(tar, abandonOnFailure=False)
        if res:  # error with tarball.. erase repo dir and tarball
            yield self._Cmd([""rm"", ""-f"", self.tarball], abandonOnFailure=False)
            yield self.runRmdir(self.repoDir(), abandonOnFailure=False)
",if res :,124
"def execute(self, arbiter, props):
    watcher = self._get_watcher(arbiter, props.pop(""name""))
    action = 0
    for key, val in props.get(""options"", {}).items():
        if key == ""hooks"":
            new_action = 0
            for name, _val in val.items():
                action = watcher.set_opt(""hooks.%s"" % name, _val)
                if action == 1:
                    new_action = 1
        else:
            new_action = watcher.set_opt(key, val)
        if new_action == 1:
            action = 1
    # trigger needed action
    return watcher.do_action(action)
","if key == ""hooks"" :",186
"def _import_playlists(self, fns, library):
    added = 0
    for filename in fns:
        name = _name_for(filename)
        with open(filename, ""rb"") as f:
            if filename.endswith("".m3u"") or filename.endswith("".m3u8""):
                playlist = parse_m3u(f, name, library=library)
            elif filename.endswith("".pls""):
                playlist = parse_pls(f, name, library=library)
            else:
                print_w(""Unsupported playlist type for '%s'"" % filename)
                continue
        self.changed(playlist)
        library.add(playlist)
        added += 1
    return added
","if filename . endswith ( "".m3u"" ) or filename . endswith ( "".m3u8"" ) :",186
"def unwrap_term_buckets(self, timestamp, term_buckets):
    for term_data in term_buckets:
        if ""interval_aggs"" in term_data:
            self.unwrap_interval_buckets(
                timestamp, term_data[""key""], term_data[""interval_aggs""][""buckets""]
            )
        else:
            self.check_matches(timestamp, term_data[""key""], term_data)
","if ""interval_aggs"" in term_data :",108
"def _get_exception(flags, timeout_ms, payload_size):
    if flags & FLAG_ERROR:
        if flags & FLAG_TIMEOUT:
            return SpicommTimeoutError(timeout_ms / 1000.0)
        if flags & FLAG_OVERFLOW:
            return SpicommOverflowError(payload_size)
        return SpicommError()
    return None
",if flags & FLAG_OVERFLOW :,99
"def _get_pattern(self, pattern_id):
    """"""Get pattern item by id.""""""
    for key in (Tag.PATTERNS1, Tag.PATTERNS2, Tag.PATTERNS3):
        if key in self.tagged_blocks:
            data = self.tagged_blocks.get_data(key)
            for pattern in data:
                if pattern.pattern_id == pattern_id:
                    return pattern
    return None
",if pattern . pattern_id == pattern_id :,110
"def print_quiet(self, context, *args, **kwargs):
    for index, (key, value) in enumerate(
        itertools.chain(enumerate(args), kwargs.items())
    ):
        if self.filter(index, key, value):
            print(
                self.format_quiet(index, key, value, fields=context.get_input_fields())
            )
","if self . filter ( index , key , value ) :",99
"def complete(self, block):
    with self._condition:
        if not self._final:
            return False
        if self._complete():
            self._calculate_state_root_if_not_already_done()
            return True
        if block:
            self._condition.wait_for(self._complete)
            self._calculate_state_root_if_not_already_done()
            return True
        return False
",if block :,117
"def compression_rotator(source, dest):
    with open(source, ""rb"") as sf:
        with gzip.open(dest, ""wb"") as wf:
            while True:
                data = sf.read(CHUNK_SIZE)
                if not data:
                    break
                wf.write(data)
    os.remove(source)
",if not data :,100
"def mockup(self, records):
    provider = TransipProvider("""", """", """")
    _dns_entries = []
    for record in records:
        if record._type in provider.SUPPORTS:
            entries_for = getattr(provider, ""_entries_for_{}"".format(record._type))
            # Root records have '@' as name
            name = record.name
            if name == """":
                name = provider.ROOT_RECORD
            _dns_entries.extend(entries_for(name, record))
            # NS is not supported as a DNS Entry,
            # so it should cover the if statement
            _dns_entries.append(DnsEntry(""@"", ""3600"", ""NS"", ""ns01.transip.nl.""))
    self.mockupEntries = _dns_entries
",if record . _type in provider . SUPPORTS :,196
"def parse_known_args(self, args=None, namespace=None):
    entrypoint = self.prog.split("" "")[0]
    try:
        defs = get_defaults_for_argparse(entrypoint)
        ignore = defs.pop(""Ignore"", None)
        self.set_defaults(**defs)
        if ignore:
            set_notebook_diff_ignores(ignore)
    except ValueError:
        pass
    return super(ConfigBackedParser, self).parse_known_args(
        args=args, namespace=namespace
    )
",if ignore :,134
"def _maybeRebuildAtlas(self, threshold=4, minlen=1000):
    n = len(self.fragmentAtlas)
    if (n > minlen) and (n > threshold * len(self.data)):
        self.fragmentAtlas.rebuild(
            list(zip(*self._style([""symbol"", ""size"", ""pen"", ""brush""])))
        )
        self.data[""sourceRect""] = 0
        if _USE_QRECT:
            self._sourceQRect.clear()
        self.updateSpots()
",if _USE_QRECT :,137
"def dispatch_return(self, frame, arg):
    if self.stop_here(frame) or frame == self.returnframe:
        # Ignore return events in generator except when stepping.
        if self.stopframe and frame.f_code.co_flags & CO_GENERATOR:
            return self.trace_dispatch
        try:
            self.frame_returning = frame
            self.user_return(frame, arg)
        finally:
            self.frame_returning = None
        if self.quitting:
            raise BdbQuit
        # The user issued a 'next' or 'until' command.
        if self.stopframe is frame and self.stoplineno != -1:
            self._set_stopinfo(None, None)
    return self.trace_dispatch
",if self . stopframe and frame . f_code . co_flags & CO_GENERATOR :,199
"def tearDown(self):
    if not self.is_playback():
        try:
            if self.hosted_service_name is not None:
                self.sms.delete_hosted_service(self.hosted_service_name)
        except:
            pass
        try:
            if self.storage_account_name is not None:
                self.sms.delete_storage_account(self.storage_account_name)
        except:
            pass
        try:
            self.sms.delete_affinity_group(self.affinity_group_name)
        except:
            pass
    return super(LegacyMgmtAffinityGroupTest, self).tearDown()
",if self . storage_account_name is not None :,180
"def make_log_msg(self, msg, *other_messages):
    MAX_MESSAGE_LENGTH = 1000
    if not other_messages:
        # assume that msg is a single string
        return msg[-MAX_MESSAGE_LENGTH:]
    else:
        if len(msg):
            msg += ""\n...\n""
            NEXT_MESSAGE_OFFSET = MAX_MESSAGE_LENGTH - len(msg)
        else:
            NEXT_MESSAGE_OFFSET = MAX_MESSAGE_LENGTH
        if NEXT_MESSAGE_OFFSET > 0:
            msg += other_messages[0][-NEXT_MESSAGE_OFFSET:]
            return self.make_log_msg(msg, *other_messages[1:])
        else:
            return self.make_log_msg(msg)
",if NEXT_MESSAGE_OFFSET > 0 :,196
"def wrapper(  # type: ignore
    self: RequestHandler, *args, **kwargs
) -> Optional[Awaitable[None]]:
    if self.request.path.endswith(""/""):
        if self.request.method in (""GET"", ""HEAD""):
            uri = self.request.path.rstrip(""/"")
            if uri:  # don't try to redirect '/' to ''
                if self.request.query:
                    uri += ""?"" + self.request.query
                self.redirect(uri, permanent=True)
                return None
        else:
            raise HTTPError(404)
    return method(self, *args, **kwargs)
",if self . request . query :,163
"def process_lib(vars_, coreval):
    for d in vars_:
        var = d.upper()
        if var == ""QTCORE"":
            continue
        value = env[""LIBPATH_"" + var]
        if value:
            core = env[coreval]
            accu = []
            for lib in value:
                if lib in core:
                    continue
                accu.append(lib)
            env[""LIBPATH_"" + var] = accu
",if value :,133
"def _attach_children(self, other, exclude_worldbody, dry_run=False):
    for other_child in other.all_children():
        if not other_child.spec.repeated:
            self_child = self.get_children(other_child.spec.name)
            self_child._attach(
                other_child, exclude_worldbody, dry_run
            )  # pylint: disable=protected-access
",if not other_child . spec . repeated :,115
"def getDictFromTree(tree):
    ret_dict = {}
    for child in tree.getchildren():
        if child.getchildren():
            ## Complex-type child. Recurse
            content = getDictFromTree(child)
        else:
            content = child.text
        if ret_dict.has_key(child.tag):
            if not type(ret_dict[child.tag]) == list:
                ret_dict[child.tag] = [ret_dict[child.tag]]
            ret_dict[child.tag].append(content or """")
        else:
            ret_dict[child.tag] = content or """"
    return ret_dict
",if ret_dict . has_key ( child . tag ) :,175
"def nsUriMatch(self, value, wanted, strict=0, tt=type(())):
    """"""Return a true value if two namespace uri values match.""""""
    if value == wanted or (type(wanted) is tt) and value in wanted:
        return 1
    if not strict and value is not None:
        wanted = type(wanted) is tt and wanted or (wanted,)
        value = value[-1:] != ""/"" and value or value[:-1]
        for item in wanted:
            if item == value or item[:-1] == value:
                return 1
    return 0
",if item == value or item [ : - 1 ] == value :,141
"def update_repository(self, ignore_issues=False, force=False):
    """"""Update.""""""
    if not await self.common_update(ignore_issues, force):
        return
    # Get appdaemon objects.
    if self.repository_manifest:
        if self.data.content_in_root:
            self.content.path.remote = """"
    if self.content.path.remote == ""apps"":
        self.data.domain = get_first_directory_in_directory(
            self.tree, self.content.path.remote
        )
        self.content.path.remote = f""apps/{self.data.name}""
    # Set local path
    self.content.path.local = self.localpath
",if self . data . content_in_root :,181
"def addOutput(self, data, isAsync=None, **kwargs):
    isAsync = _get_async_param(isAsync, **kwargs)
    if isAsync:
        self.terminal.eraseLine()
        self.terminal.cursorBackward(len(self.lineBuffer) + len(self.ps[self.pn]))
    self.terminal.write(data)
    if isAsync:
        if self._needsNewline():
            self.terminal.nextLine()
        self.terminal.write(self.ps[self.pn])
        if self.lineBuffer:
            oldBuffer = self.lineBuffer
            self.lineBuffer = []
            self.lineBufferIndex = 0
            self._deliverBuffer(oldBuffer)
",if self . _needsNewline ( ) :,188
"def is_installed(self, dlc_title="""") -> bool:
    installed = False
    if dlc_title:
        dlc_version = self.get_dlc_info(""version"", dlc_title)
        installed = True if dlc_version else False
        # Start: Code for compatibility with minigalaxy 1.0
        if not installed:
            status = self.legacy_get_dlc_status(dlc_title)
            installed = True if status in [""installed"", ""updatable""] else False
        # End: Code for compatibility with minigalaxy 1.0
    else:
        if self.install_dir and os.path.exists(self.install_dir):
            installed = True
    return installed
",if not installed :,178
"def close(self):
    self.selector.close()
    if self.sock:
        sockname = None
        try:
            sockname = self.sock.getsockname()
        except (socket.error, OSError):
            pass
        self.sock.close()
        if type(sockname) is str:
            # it was a Unix domain socket, remove it from the filesystem
            if os.path.exists(sockname):
                os.remove(sockname)
    self.sock = None
",if type ( sockname ) is str :,128
"def post_file(self, file_path, graph_type=""edges"", file_type=""csv""):
    dataset_id = self.dataset_id
    tok = self.token
    base_path = self.server_base_path
    with open(file_path, ""rb"") as file:
        out = requests.post(
            f""{base_path}/api/v2/upload/datasets/{dataset_id}/{graph_type}/{file_type}"",
            verify=self.certificate_validation,
            headers={""Authorization"": f""Bearer {tok}""},
            data=file.read(),
        ).json()
        if not out[""success""]:
            raise Exception(out)
        return out
","if not out [ ""success"" ] :",176
"def _get_vqa_v2_image_raw_dataset(directory, image_root_url, image_urls):
    """"""Extract the VQA V2 image data set to directory unless it's there.""""""
    for url in image_urls:
        filename = os.path.basename(url)
        download_url = os.path.join(image_root_url, url)
        path = generator_utils.maybe_download(directory, filename, download_url)
        unzip_dir = os.path.join(directory, filename.strip("".zip""))
        if not tf.gfile.Exists(unzip_dir):
            zipfile.ZipFile(path, ""r"").extractall(directory)
",if not tf . gfile . Exists ( unzip_dir ) :,165
"def __call__(self, environ, start_response):
    for key in ""REQUEST_URL"", ""REQUEST_URI"", ""UNENCODED_URL"":
        if key not in environ:
            continue
        request_uri = unquote(environ[key])
        script_name = unquote(environ.get(""SCRIPT_NAME"", """"))
        if request_uri.startswith(script_name):
            environ[""PATH_INFO""] = request_uri[len(script_name) :].split(""?"", 1)[0]
            break
    return self.app(environ, start_response)
",if key not in environ :,140
"def _instrument_model(self, model):
    for key, value in list(
        model.__dict__.items()
    ):  # avoid ""dictionary keys changed during iteration""
        if isinstance(value, tf.keras.layers.Layer):
            new_layer = self._instrument(value)
            if new_layer is not value:
                setattr(model, key, new_layer)
        elif isinstance(value, list):
            for i, item in enumerate(value):
                if isinstance(item, tf.keras.layers.Layer):
                    value[i] = self._instrument(item)
    return model
","elif isinstance ( value , list ) :",164
"def __init__(self, parent, dir, mask, with_dirs=True):
    filelist = []
    dirlist = [""..""]
    self.dir = dir
    self.file = """"
    mask = mask.upper()
    pattern = self.MakeRegex(mask)
    for i in os.listdir(dir):
        if i == ""."" or i == "".."":
            continue
        path = os.path.join(dir, i)
        if os.path.isdir(path):
            dirlist.append(i)
            continue
        path = path.upper()
        value = i.upper()
        if pattern.match(value) is not None:
            filelist.append(i)
    self.files = filelist
    if with_dirs:
        self.dirs = dirlist
",if pattern . match ( value ) is not None :,199
"def get_text(self, nodelist):
    """"""Return a string representation of the motif's properties listed on nodelist .""""""
    retlist = []
    for node in nodelist:
        if node.nodeType == Node.TEXT_NODE:
            retlist.append(node.wholeText)
        elif node.hasChildNodes:
            retlist.append(self.get_text(node.childNodes))
    return re.sub(r""\s+"", "" "", """".join(retlist))
",elif node . hasChildNodes :,119
"def _persist_metadata(self, dirname, filename):
    metadata_path = ""{0}/{1}.json"".format(dirname, filename)
    if self.media_metadata or self.comments or self.include_location:
        if self.posts:
            if self.latest:
                self.merge_json({""GraphImages"": self.posts}, metadata_path)
            else:
                self.save_json({""GraphImages"": self.posts}, metadata_path)
        if self.stories:
            if self.latest:
                self.merge_json({""GraphStories"": self.stories}, metadata_path)
            else:
                self.save_json({""GraphStories"": self.stories}, metadata_path)
",if self . stories :,190
"def _get_python_wrapper_content(self, job_class, args):
    job = job_class([""-r"", ""hadoop""] + list(args))
    job.sandbox()
    with job.make_runner() as runner:
        runner._create_setup_wrapper_scripts()
        if runner._spark_python_wrapper_path:
            with open(runner._spark_python_wrapper_path) as f:
                return f.read()
        else:
            return None
",if runner . _spark_python_wrapper_path :,125
"def computeLeadingWhitespaceWidth(s, tab_width):
    w = 0
    for ch in s:
        if ch == "" "":
            w += 1
        elif ch == ""\t"":
            w += abs(tab_width) - (w % abs(tab_width))
        else:
            break
    return w
","if ch == "" "" :",87
"def run(self):
    # if the i3status process dies we want to restart it.
    # We give up restarting if we have died too often
    for _ in range(10):
        if not self.py3_wrapper.running:
            break
        self.spawn_i3status()
        # check if we never worked properly and if so quit now
        if not self.ready:
            break
        # limit restart rate
        self.lock.wait(5)
",if not self . py3_wrapper . running :,122
"def translate_len(
    builder: IRBuilder, expr: CallExpr, callee: RefExpr
) -> Optional[Value]:
    # Special case builtins.len
    if len(expr.args) == 1 and expr.arg_kinds == [ARG_POS]:
        expr_rtype = builder.node_type(expr.args[0])
        if isinstance(expr_rtype, RTuple):
            # len() of fixed-length tuple can be trivially determined statically,
            # though we still need to evaluate it.
            builder.accept(expr.args[0])
            return Integer(len(expr_rtype.types))
        else:
            obj = builder.accept(expr.args[0])
            return builder.builtin_len(obj, -1)
    return None
","if isinstance ( expr_rtype , RTuple ) :",188
"def parse_auth(val):
    if val is not None:
        authtype, params = val.split("" "", 1)
        if authtype in known_auth_schemes:
            if authtype == ""Basic"" and '""' not in params:
                # this is the ""Authentication: Basic XXXXX=="" case
                pass
            else:
                params = parse_auth_params(params)
        return authtype, params
    return val
","if authtype == ""Basic"" and '""' not in params :",117
"def toxml(self):
    text = self.value
    self.parent.setBidi(getBidiType(text))
    if not text.startswith(HTML_PLACEHOLDER_PREFIX):
        if self.parent.nodeName == ""p"":
            text = text.replace(""\n"", ""\n   "")
        elif self.parent.nodeName == ""li"" and self.parent.childNodes[0] == self:
            text = ""\n     "" + text.replace(""\n"", ""\n     "")
    text = self.doc.normalizeEntities(text)
    return text
","elif self . parent . nodeName == ""li"" and self . parent . childNodes [ 0 ] == self :",148
"def get_all_related_many_to_many_objects(self):
    try:  # Try the cache first.
        return self._all_related_many_to_many_objects
    except AttributeError:
        rel_objs = []
        for klass in get_models():
            for f in klass._meta.many_to_many:
                if f.rel and self == f.rel.to._meta:
                    rel_objs.append(RelatedObject(f.rel.to, klass, f))
        self._all_related_many_to_many_objects = rel_objs
        return rel_objs
",if f . rel and self == f . rel . to . _meta :,158
"def state_highstate(self, state, dirpath):
    opts = copy.copy(self.config)
    opts[""file_roots""] = dict(base=[dirpath])
    HIGHSTATE = HighState(opts)
    HIGHSTATE.push_active()
    try:
        high, errors = HIGHSTATE.render_highstate(state)
        if errors:
            import pprint
            pprint.pprint(""\n"".join(errors))
            pprint.pprint(high)
        out = HIGHSTATE.state.call_high(high)
        # pprint.pprint(out)
    finally:
        HIGHSTATE.pop_active()
",if errors :,156
"def _update_target_host(self, target, target_host):
    """"""Update target host.""""""
    target_host = None if target_host == """" else target_host
    if not target_host:
        for device_type, tgt in target.items():
            if device_type.value == tvm.nd.cpu(0).device_type:
                target_host = tgt
                break
    if not target_host:
        target_host = ""llvm"" if tvm.runtime.enabled(""llvm"") else ""stackvm""
    if isinstance(target_host, str):
        target_host = tvm.target.Target(target_host)
    return target_host
",if device_type . value == tvm . nd . cpu ( 0 ) . device_type :,171
"def __console_writer(self):
    while True:
        self.__writer_event.wait()
        self.__writer_event.clear()
        if self.__console_view:
            if not self.short_only:
                self.log.debug(""Writing console view to STDOUT"")
                sys.stdout.write(self.console_markup.clear)
                sys.stdout.write(self.__console_view)
                sys.stdout.write(self.console_markup.TOTAL_RESET)
",if not self . short_only :,134
"def goToPrevMarkedHeadline(self, event=None):
    """"""Select the next marked node.""""""
    c = self
    p = c.p
    if not p:
        return
    p.moveToThreadBack()
    wrapped = False
    while 1:
        if p and p.isMarked():
            break
        elif p:
            p.moveToThreadBack()
        elif wrapped:
            break
        else:
            wrapped = True
            p = c.rootPosition()
    if not p:
        g.blue(""done"")
    c.treeSelectHelper(p)  # Sets focus.
",elif wrapped :,164
"def delete_map(self, query=None):
    query_map = self.interpolated_map(query=query)
    for alias, drivers in six.iteritems(query_map.copy()):
        for driver, vms in six.iteritems(drivers.copy()):
            for vm_name, vm_details in six.iteritems(vms.copy()):
                if vm_details == ""Absent"":
                    query_map[alias][driver].pop(vm_name)
            if not query_map[alias][driver]:
                query_map[alias].pop(driver)
        if not query_map[alias]:
            query_map.pop(alias)
    return query_map
",if not query_map [ alias ] :,177
"def get_shadows_zip(filename):
    import zipfile
    shadow_pkgs = set()
    with zipfile.ZipFile(filename) as lib_zip:
        already_test = []
        for fname in lib_zip.namelist():
            pname, fname = os.path.split(fname)
            if fname or (pname and fname):
                continue
            if pname not in already_test and ""/"" not in pname:
                already_test.append(pname)
                if is_shadowing(pname):
                    shadow_pkgs.add(pname)
    return shadow_pkgs
",if fname or ( pname and fname ) :,159
"def make_chains(chains_info):
    chains = [[] for _ in chains_info[0][1]]
    for i, num_ids in enumerate(chains_info[:-1]):
        num, ids = num_ids
        for j, ident in enumerate(ids):
            if ident != """":
                next_chain_info = chains_info[i + 1]
                previous = next_chain_info[1][j]
                block = SimpleBlock(num, ident, previous)
                chains[j].append(block)
    chains = {i: make_generator(chain) for i, chain in enumerate(chains)}
    return chains
","if ident != """" :",164
"def filter_input(mindate, maxdate, files):
    mindate = parse(mindate) if mindate is not None else datetime.datetime.min
    maxdate = parse(maxdate) if maxdate is not None else datetime.datetime.max
    for line in fileinput.input(files):
        tweet = json.loads(line)
        created_at = parse(tweet[""created_at""])
        created_at = created_at.replace(tzinfo=None)
        if mindate < created_at and maxdate > created_at:
            print(json.dumps(tweet))
",if mindate < created_at and maxdate > created_at :,149
"def get(self):
    """"""If a value/an exception is stored, return/raise it. Otherwise until switch() or throw() is called.""""""
    if self._exception is not _NONE:
        if self._exception is None:
            return self.value
        getcurrent().throw(*self._exception)  # pylint:disable=undefined-variable
    else:
        if self.greenlet is not None:
            raise ConcurrentObjectUseError(
                ""This Waiter is already used by %r"" % (self.greenlet,)
            )
        self.greenlet = getcurrent()  # pylint:disable=undefined-variable
        try:
            return self.hub.switch()
        finally:
            self.greenlet = None
",if self . greenlet is not None :,188
"def default_loader(href, parse, encoding=None):
    with open(href) as file:
        if parse == ""xml"":
            data = ElementTree.parse(file).getroot()
        else:
            data = file.read()
            if encoding:
                data = data.decode(encoding)
    return data
","if parse == ""xml"" :",86
"def is_all_qud(world):
    m = True
    for obj in world:
        if obj.blond:
            if obj.nice:
                m = m and True
            else:
                m = m and False
        else:
            m = m and True
    return m
",if obj . blond :,85
"def run(self, edit):
    if not self.has_selection():
        region = sublime.Region(0, self.view.size())
        originalBuffer = self.view.substr(region)
        prefixed = self.prefix(originalBuffer)
        if prefixed:
            self.view.replace(edit, region, prefixed)
        return
    for region in self.view.sel():
        if region.empty():
            continue
        originalBuffer = self.view.substr(region)
        prefixed = self.prefix(originalBuffer)
        if prefixed:
            self.view.replace(edit, region, prefixed)
",if region . empty ( ) :,163
"def add_fields(self, params):
    for (key, val) in params.iteritems():
        if isinstance(val, dict):
            new_params = {}
            for k in val:
                new_params[""%s__%s"" % (key, k)] = val[k]
            self.add_fields(new_params)
        else:
            self.add_field(key, val)
","if isinstance ( val , dict ) :",108
"def find_magic(self, f, pos, magic):
    f.seek(pos)
    block = f.read(32 * 1024)
    if len(block) < len(magic):
        return -1
    p = block.find(magic)
    while p < 0:
        pos += len(block) - len(magic) + 1
        block = block[1 - len(magic) :] + f.read(32 << 10)
        if len(block) == len(magic) - 1:
            return -1
        p = block.find(magic)
    return pos + p
",if len ( block ) == len ( magic ) - 1 :,148
"def check_strings(self):
    """"""Check that all strings have been consumed.""""""
    for i, aList in enumerate(self.string_tokens):
        if aList:
            g.trace(""warning: line %s. unused strings"" % i)
            for z in aList:
                print(self.dump_token(z))
",if aList :,87
"def get_tokens_unprocessed(self, text):
    from pygments.lexers._cocoa_builtins import (
        COCOA_INTERFACES,
        COCOA_PROTOCOLS,
        COCOA_PRIMITIVES,
    )
    for index, token, value in RegexLexer.get_tokens_unprocessed(self, text):
        if token is Name or token is Name.Class:
            if (
                value in COCOA_INTERFACES
                or value in COCOA_PROTOCOLS
                or value in COCOA_PRIMITIVES
            ):
                token = Name.Builtin.Pseudo
        yield index, token, value
",if token is Name or token is Name . Class :,176
"def key_from_key_value_dict(key_info):
    res = []
    if not ""key_value"" in key_info:
        return res
    for value in key_info[""key_value""]:
        if ""rsa_key_value"" in value:
            e = base64_to_long(value[""rsa_key_value""][""exponent""])
            m = base64_to_long(value[""rsa_key_value""][""modulus""])
            key = RSA.construct((m, e))
            res.append(key)
    return res
","if ""rsa_key_value"" in value :",141
"def run(self, edit):
    if not self.has_selection():
        region = sublime.Region(0, self.view.size())
        originalBuffer = self.view.substr(region)
        prefixed = self.prefix(originalBuffer)
        if prefixed:
            self.view.replace(edit, region, prefixed)
        return
    for region in self.view.sel():
        if region.empty():
            continue
        originalBuffer = self.view.substr(region)
        prefixed = self.prefix(originalBuffer)
        if prefixed:
            self.view.replace(edit, region, prefixed)
",if prefixed :,163
"def finalize(self):
    if self.ct < 1:
        return
    elif self.ct == 1:
        return 0
    total = ct = 0
    dtp = None
    while self.heap:
        if total == 0:
            if dtp is None:
                dtp = heapq.heappop(self.heap)
                continue
        dt = heapq.heappop(self.heap)
        diff = dt - dtp
        ct += 1
        total += total_seconds(diff)
        dtp = dt
    return float(total) / ct
",if total == 0 :,150
"def _test_configuration(self):
    config_path = self._write_config()
    try:
        self._log.debug(""testing configuration"")
        verboseflag = ""-Q""
        if self._log.isEnabledFor(logging.DEBUG):
            verboseflag = ""-v""
        p = subprocess.Popen([self.PATH_SLAPTEST, verboseflag, ""-f"", config_path])
        if p.wait() != 0:
            raise RuntimeError(""configuration test failed"")
        self._log.debug(""configuration seems ok"")
    finally:
        os.remove(config_path)
",if self . _log . isEnabledFor ( logging . DEBUG ) :,147
"def exe(self, ret):
    if not ret:
        self.assertEqual(ret, """")
    else:
        assert os.path.isabs(ret), ret
        # Note: os.stat() may return False even if the file is there
        # hence we skip the test, see:
        # http://stackoverflow.com/questions/3112546/os-path-exists-lies
        if POSIX:
            assert os.path.isfile(ret), ret
            if hasattr(os, ""access"") and hasattr(os, ""X_OK""):
                # XXX may fail on OSX
                self.assertTrue(os.access(ret, os.X_OK))
",if POSIX :,171
"def _do_cleanup(sg_name, device_id):
    masking_view_list = self.rest.get_masking_views_from_storage_group(array, sg_name)
    for masking_view in masking_view_list:
        if ""STG-"" in masking_view:
            self.rest.delete_masking_view(array, masking_view)
            self.rest.remove_vol_from_sg(array, sg_name, device_id, extra_specs)
            self.rest.delete_volume(array, device_id)
            self.rest.delete_storage_group(array, sg_name)
","if ""STG-"" in masking_view :",159
"def hide_tooltip_if_necessary(self, key):
    """"""Hide calltip when necessary""""""
    try:
        calltip_char = self.get_character(self.calltip_position)
        before = self.is_cursor_before(self.calltip_position, char_offset=1)
        other = key in (Qt.Key_ParenRight, Qt.Key_Period, Qt.Key_Tab)
        if calltip_char not in (""?"", ""("") or before or other:
            QToolTip.hideText()
    except (IndexError, TypeError):
        QToolTip.hideText()
","if calltip_char not in ( ""?"" , ""("" ) or before or other :",148
"def list_tags_for_stream(self, stream_name, exclusive_start_tag_key=None, limit=None):
    stream = self.describe_stream(stream_name)
    tags = []
    result = {""HasMoreTags"": False, ""Tags"": tags}
    for key, val in sorted(stream.tags.items(), key=lambda x: x[0]):
        if limit and len(tags) >= limit:
            result[""HasMoreTags""] = True
            break
        if exclusive_start_tag_key and key < exclusive_start_tag_key:
            continue
        tags.append({""Key"": key, ""Value"": val})
    return result
",if exclusive_start_tag_key and key < exclusive_start_tag_key :,166
"def parametrize_function_name(request, function_name):
    suffixes = []
    if ""parametrize"" in request.keywords:
        argnames = request.keywords[""parametrize""].args[::2]
        argnames = [x.strip() for names in argnames for x in names.split("","")]
        for name in argnames:
            value = request.getfuncargvalue(name)
            if inspect.isclass(value):
                value = value.__name__
            suffixes.append(""{}={}"".format(name, value))
    return ""+"".join([function_name] + suffixes)
",if inspect . isclass ( value ) :,145
"def add_entities(self, positions):
    e1 = EntityFactory()
    for p in positions:
        if isinstance(p, tuple):
            start, length = p
        else:
            start, length = p, 1
        EntityOccurrenceFactory(
            document=self.doc,
            entity=e1,
            offset=start,
            offset_end=start + length,
            alias=""AB"",
        )
","if isinstance ( p , tuple ) :",119
"def transform_value(value):
    if isinstance(value, collections.MutableMapping):
        if ""_id"" in value and ""_ns"" in value:
            return DBRef(value[""_ns""], transform_value(value[""_id""]))
        else:
            return transform_dict(SON(value))
    elif isinstance(value, list):
        return [transform_value(v) for v in value]
    return value
","if ""_id"" in value and ""_ns"" in value :",103
"def remove(self, items):
    """"""Remove messages from lease management.""""""
    with self._add_remove_lock:
        # Remove the ack ID from lease management, and decrement the
        # byte counter.
        for item in items:
            if self._leased_messages.pop(item.ack_id, None) is not None:
                self._bytes -= item.byte_size
            else:
                _LOGGER.debug(""Item %s was not managed."", item.ack_id)
        if self._bytes < 0:
            _LOGGER.debug(""Bytes was unexpectedly negative: %d"", self._bytes)
            self._bytes = 0
",if self . _bytes < 0 :,172
"def parse_hgsub(lines):
    """"""Fills OrderedDict with hgsub file content passed as list of lines""""""
    rv = OrderedDict()
    for l in lines:
        ls = l.strip()
        if not ls or ls[0] == ""#"":
            continue
        name, value = l.split(""="", 1)
        rv[name.strip()] = value.strip()
    return rv
","if not ls or ls [ 0 ] == ""#"" :",98
"def del_(self, key):
    initial_hash = hash_ = self.hash(key)
    while True:
        if self._keys[hash_] is self._empty:
            # That key was never assigned
            return None
        elif self._keys[hash_] == key:
            # key found, assign with deleted sentinel
            self._keys[hash_] = self._deleted
            self._values[hash_] = self._deleted
            self._len -= 1
            return
        hash_ = self._rehash(hash_)
        if initial_hash == hash_:
            # table is full and wrapped around
            return None
",if initial_hash == hash_ :,166
"def atom(token, no_symbol=False):
    try:
        return int(token)
    except ValueError:
        try:
            return float(token)
        except ValueError:
            if token.startswith(""'"") or token.startswith('""'):
                return token[1:-1]
            elif no_symbol:
                return token
            else:
                return Symbol(token)
","if token . startswith ( ""'"" ) or token . startswith ( '""' ) :",107
"def __Suffix_Noun_Step1b(self, token):
    for suffix in self.__suffix_noun_step1b:
        if token.endswith(suffix) and len(token) > 5:
            token = token[:-1]
            self.suffixe_noun_step1b_success = True
            break
    return token
",if token . endswith ( suffix ) and len ( token ) > 5 :,87
"def _guardAgainstUnicode(self, data):
    # Only accept byte strings or ascii unicode values, otherwise
    # there is no way to correctly decode the data into bytes.
    if _pythonMajorVersion < 3:
        if isinstance(data, unicode):
            data = data.encode(""utf8"")
    else:
        if isinstance(data, str):
            # Only accept ascii unicode values.
            try:
                return data.encode(""ascii"")
            except UnicodeEncodeError:
                pass
            raise ValueError(""pyDes can only work with encoded strings, not Unicode."")
    return data
","if isinstance ( data , unicode ) :",154
"def populate_resource_parameters(self, tool_source):
    root = getattr(tool_source, ""root"", None)
    if (
        root is not None
        and hasattr(self.app, ""job_config"")
        and hasattr(self.app.job_config, ""get_tool_resource_xml"")
    ):
        resource_xml = self.app.job_config.get_tool_resource_xml(
            root.get(""id""), self.tool_type
        )
        if resource_xml is not None:
            inputs = root.find(""inputs"")
            if inputs is None:
                inputs = parse_xml_string(""<inputs/>"")
                root.append(inputs)
            inputs.append(resource_xml)
",if inputs is None :,194
"def test_arguments_regex(self):
    argument_matches = (
        (""pip=1.1"", (""pip"", ""1.1"")),
        (""pip==1.1"", None),
        (""pip=1.2=1"", (""pip"", ""1.2=1"")),
    )
    for argument, match in argument_matches:
        if match is None:
            self.assertIsNone(salt.utils.args.KWARG_REGEX.match(argument))
        else:
            self.assertEqual(
                salt.utils.args.KWARG_REGEX.match(argument).groups(), match
            )
",if match is None :,157
"def _get_sidebar_selected(self):
    sidebar_selected = None
    if self.businessline_id:
        sidebar_selected = ""bl_%s"" % self.businessline_id
        if self.service_id:
            sidebar_selected += ""_s_%s"" % self.service_id
            if self.environment_id:
                sidebar_selected += ""_env_%s"" % self.environment_id
    return sidebar_selected
",if self . environment_id :,113
"def get_ip_info(ipaddress):
    """"""Returns device information by IP address""""""
    result = {}
    try:
        ip = IPAddress.objects.select_related().get(address=ipaddress)
    except IPAddress.DoesNotExist:
        pass
    else:
        if ip.venture is not None:
            result[""venture_id""] = ip.venture.id
        if ip.device is not None:
            result[""device_id""] = ip.device.id
            if ip.device.venture is not None:
                result[""venture_id""] = ip.device.venture.id
    return result
",if ip . device is not None :,162
"def apply(self, db, person):
    for family_handle in person.get_family_handle_list():
        family = db.get_family_from_handle(family_handle)
        if family:
            for event_ref in family.get_event_ref_list():
                if event_ref:
                    event = db.get_event_from_handle(event_ref.ref)
                    if not event.get_place_handle():
                        return True
                    if not event.get_date_object():
                        return True
    return False
",if not event . get_date_object ( ) :,159
"def killIfDead():
    if not self._isalive:
        self.log.debug(
            ""WampLongPoll: killing inactive WAMP session with transport '{0}'"".format(
                self._transport_id
            )
        )
        self.onClose(False, 5000, ""session inactive"")
        self._receive._kill()
        if self._transport_id in self._parent._transports:
            del self._parent._transports[self._transport_id]
    else:
        self.log.debug(
            ""WampLongPoll: transport '{0}' is still alive"".format(self._transport_id)
        )
        self._isalive = False
        self.reactor.callLater(killAfter, killIfDead)
",if self . _transport_id in self . _parent . _transports :,196
"def offsets(self):
    offsets = {}
    offset_so_far = 0
    for name, ty in self.fields.items():
        if isinstance(ty, SimTypeBottom):
            l.warning(
                ""Found a bottom field in struct %s. Ignore and increment the offset using the default ""
                ""element size."",
                self.name,
            )
            continue
        if not self._pack:
            align = ty.alignment
            if offset_so_far % align != 0:
                offset_so_far += align - offset_so_far % align
        offsets[name] = offset_so_far
        offset_so_far += ty.size // self._arch.byte_width
    return offsets
","if isinstance ( ty , SimTypeBottom ) :",196
"def get_override_css(self):
    """"""handls allow_css_overrides setting.""""""
    if self.settings.get(""allow_css_overrides""):
        filename = self.view.file_name()
        filetypes = self.settings.get(""markdown_filetypes"")
        if filename and filetypes:
            for filetype in filetypes:
                if filename.endswith(filetype):
                    css_filename = filename.rpartition(filetype)[0] + "".css""
                    if os.path.isfile(css_filename):
                        return u""<style>%s</style>"" % load_utf8(css_filename)
    return """"
",if filename and filetypes :,165
"def setFullCSSSource(self, fullsrc, inline=False):
    self.fullsrc = fullsrc
    if type(self.fullsrc) == six.binary_type:
        self.fullsrc = six.text_type(self.fullsrc, ""utf-8"")
    if inline:
        self.inline = inline
    if self.fullsrc:
        self.srcFullIdx = self.fullsrc.find(self.src)
        if self.srcFullIdx < 0:
            del self.srcFullIdx
        self.ctxsrcFullIdx = self.fullsrc.find(self.ctxsrc)
        if self.ctxsrcFullIdx < 0:
            del self.ctxsrcFullIdx
",if self . ctxsrcFullIdx < 0 :,175
"def title(self):
    ret = theme[""title""]
    if isinstance(self.name, six.string_types):
        width = self.statwidth()
        return (
            ret + self.name[0:width].center(width).replace("" "", ""-"") + theme[""default""]
        )
    for i, name in enumerate(self.name):
        width = self.colwidth()
        ret = ret + name[0:width].center(width).replace("" "", ""-"")
        if i + 1 != len(self.vars):
            if op.color:
                ret = ret + theme[""frame""] + char[""dash""] + theme[""title""]
            else:
                ret = ret + char[""space""]
    return ret
",if op . color :,188
"def _get_requested_databases(self):
    """"""Returns a list of databases requested, not including ignored dbs""""""
    requested_databases = []
    if (self._requested_namespaces is not None) and (self._requested_namespaces != []):
        for requested_namespace in self._requested_namespaces:
            if requested_namespace[0] is ""*"":
                return []
            elif requested_namespace[0] not in IGNORE_DBS:
                requested_databases.append(requested_namespace[0])
    return requested_databases
","if requested_namespace [ 0 ] is ""*"" :",131
"def add_channels(cls, voucher, add_channels):
    for add_channel in add_channels:
        channel = add_channel[""channel""]
        defaults = {""currency"": channel.currency_code}
        if ""discount_value"" in add_channel.keys():
            defaults[""discount_value""] = add_channel.get(""discount_value"")
        if ""min_amount_spent"" in add_channel.keys():
            defaults[""min_spent_amount""] = add_channel.get(""min_amount_spent"", None)
        models.VoucherChannelListing.objects.update_or_create(
            voucher=voucher,
            channel=channel,
            defaults=defaults,
        )
","if ""discount_value"" in add_channel . keys ( ) :",176
"def read_xml(path):
    with tf.gfile.GFile(path) as f:
        root = etree.fromstring(f.read())
    annotations = {}
    for node in root.getchildren():
        key, val = node2dict(node)
        # If `key` is object, it's actually a list.
        if key == ""object"":
            annotations.setdefault(key, []).append(val)
        else:
            annotations[key] = val
    return annotations
","if key == ""object"" :",123
"def get_ip_info(ipaddress):
    """"""Returns device information by IP address""""""
    result = {}
    try:
        ip = IPAddress.objects.select_related().get(address=ipaddress)
    except IPAddress.DoesNotExist:
        pass
    else:
        if ip.venture is not None:
            result[""venture_id""] = ip.venture.id
        if ip.device is not None:
            result[""device_id""] = ip.device.id
            if ip.device.venture is not None:
                result[""venture_id""] = ip.device.venture.id
    return result
",if ip . venture is not None :,162
"def test_large_headers(self):
    with ExpectLog(gen_log, ""Unsatisfiable read"", required=False):
        try:
            self.fetch(""/"", headers={""X-Filler"": ""a"" * 1000}, raise_error=True)
            self.fail(""did not raise expected exception"")
        except HTTPError as e:
            # 431 is ""Request Header Fields Too Large"", defined in RFC
            # 6585. However, many implementations just close the
            # connection in this case, resulting in a missing response.
            if e.response is not None:
                self.assertIn(e.response.code, (431, 599))
",if e . response is not None :,167
"def validate_reserved_serial_no_consumption(self):
    for item in self.items:
        if item.s_warehouse and not item.t_warehouse and item.serial_no:
            for sr in get_serial_nos(item.serial_no):
                sales_order = frappe.db.get_value(""Serial No"", sr, ""sales_order"")
                if sales_order:
                    msg = _(
                        ""(Serial No: {0}) cannot be consumed as it's reserverd to fullfill Sales Order {1}.""
                    ).format(sr, sales_order)
                    frappe.throw(_(""Item {0} {1}"").format(item.item_code, msg))
",if sales_order :,190
"def force_decode(string, encoding):
    if isinstance(string, str):
        if encoding:
            string = string.decode(encoding)
        else:
            try:
                # try decoding with utf-8, should only work for real UTF-8
                string = string.decode(""utf-8"")
            except UnicodeError:
                # last resort -- can't fail
                string = string.decode(""latin1"")
    return string
",if encoding :,121
"def _add_cs(master_cs, sub_cs, prefix, delimiter=""."", parent_hp=None):
    new_parameters = []
    for hp in sub_cs.get_hyperparameters():
        new_parameter = copy.deepcopy(hp)
        # Allow for an empty top-level parameter
        if new_parameter.name == """":
            new_parameter.name = prefix
        elif not prefix == """":
            new_parameter.name = ""{}{}{}"".format(prefix, SPLITTER, new_parameter.name)
        new_parameters.append(new_parameter)
    for hp in new_parameters:
        _add_hp(master_cs, hp)
","elif not prefix == """" :",162
"def __call__(self, *args, **kwargs):
    if self.log_file is not None:
        kwargs[""file""] = self.log_file
        print(*args, **kwargs)
        if hasattr(self.log_file, ""flush""):
            # get immediate feedback
            self.log_file.flush()
    elif self.log_func is not None:
        self.log_func(*args, **kwargs)
","if hasattr ( self . log_file , ""flush"" ) :",109
"def df_index_expr(self, length_expr=None, as_range=False):
    """"""Generate expression to get or create index of DF""""""
    if isinstance(self.index, types.NoneType):
        if length_expr is None:
            length_expr = df_length_expr(self)
        if as_range:
            return f""range({length_expr})""
        else:
            return f""numpy.arange({length_expr})""
    return ""self._index""
",if length_expr is None :,123
"def _setWeight(self, value):
    if value is None:
        self._fontWeight = None
    else:
        if value.lower() not in (""normal"", ""bold""):
            raise TextFormatException(f""Not a supported fontWeight: {value}"")
        self._fontWeight = value.lower()
","if value . lower ( ) not in ( ""normal"" , ""bold"" ) :",78
"def _test_configuration(self):
    config_path = self._write_config()
    try:
        self._log.debug(""testing configuration"")
        verboseflag = ""-Q""
        if self._log.isEnabledFor(logging.DEBUG):
            verboseflag = ""-v""
        p = subprocess.Popen([self.PATH_SLAPTEST, verboseflag, ""-f"", config_path])
        if p.wait() != 0:
            raise RuntimeError(""configuration test failed"")
        self._log.debug(""configuration seems ok"")
    finally:
        os.remove(config_path)
",if p . wait ( ) != 0 :,147
"def filter_queryset(self, request, queryset, view):
    kwargs = {}
    for field in view.filterset_fields:
        value = request.GET.get(field)
        if not value:
            continue
        if field == ""node_id"":
            value = get_object_or_none(Node, pk=value)
            kwargs[""node""] = value
            continue
        elif field == ""asset_id"":
            field = ""asset""
        kwargs[field] = value
    if kwargs:
        queryset = queryset.filter(**kwargs)
    logger.debug(""Filter {}"".format(kwargs))
    return queryset
","elif field == ""asset_id"" :",162
"def _find_closing_brace(string, start_pos):
    """"""Finds the corresponding closing brace after start_pos.""""""
    bracks_open = 1
    for idx, char in enumerate(string[start_pos:]):
        if char == ""("":
            if string[idx + start_pos - 1] != ""\\"":
                bracks_open += 1
        elif char == "")"":
            if string[idx + start_pos - 1] != ""\\"":
                bracks_open -= 1
            if not bracks_open:
                return start_pos + idx + 1
","if string [ idx + start_pos - 1 ] != ""\\"" :",145
"def _set_hostport(self, host, port):
    if port is None:
        i = host.rfind("":"")
        j = host.rfind(""]"")  # ipv6 addresses have [...]
        if i > j:
            try:
                port = int(host[i + 1 :])
            except ValueError:
                raise InvalidURL(""nonnumeric port: '%s'"" % host[i + 1 :])
            host = host[:i]
        else:
            port = self.default_port
        if host and host[0] == ""["" and host[-1] == ""]"":
            host = host[1:-1]
    self.host = host
    self.port = port
","if host and host [ 0 ] == ""["" and host [ - 1 ] == ""]"" :",176
"def __getstate__(self):
    state = {}
    for cls in type(self).mro():
        cls_slots = getattr(cls, ""__slots__"", ())
        for slot in cls_slots:
            if slot != ""__weakref__"":
                if hasattr(self, slot):
                    state[slot] = getattr(self, slot)
    state[""_cookiejar_cookies""] = list(self.cookiejar)
    del state[""cookiejar""]
    return state
","if hasattr ( self , slot ) :",113
"def _evp_pkey_from_der_traditional_key(self, bio_data, password):
    key = self._lib.d2i_PrivateKey_bio(bio_data.bio, self._ffi.NULL)
    if key != self._ffi.NULL:
        key = self._ffi.gc(key, self._lib.EVP_PKEY_free)
        if password is not None:
            raise TypeError(""Password was given but private key is not encrypted."")
        return key
    else:
        self._consume_errors()
        return None
",if password is not None :,142
"def is_special(s, i, directive):
    """"""Return True if the body text contains the @ directive.""""""
    # j = skip_line(s,i) ; trace(s[i:j],':',directive)
    assert directive and directive[0] == ""@""
    # 10/23/02: all directives except @others must start the line.
    skip_flag = directive in (""@others"", ""@all"")
    while i < len(s):
        if match_word(s, i, directive):
            return True, i
        else:
            i = skip_line(s, i)
            if skip_flag:
                i = skip_ws(s, i)
    return False, -1
","if match_word ( s , i , directive ) :",178
"def _decorator(coro_func):
    fut = asyncio.ensure_future(coro_func())
    self._tests.append((coro_func.__name__, fut))
    if timeout_sec is not None:
        timeout_at = self._loop.time() + timeout_sec
        handle = self.MASTER_LOOP.call_at(
            timeout_at, self._set_exception_if_not_done, fut, asyncio.TimeoutError()
        )
        fut.add_done_callback(lambda *args: handle.cancel())
        if timeout_at > self._global_timeout_at:
            self._global_timeout_at = timeout_at
    return coro_func
",if timeout_at > self . _global_timeout_at :,171
"def _load(self, db, owner):
    self.__init(owner)
    db_result = db(
        ""SELECT ship_id, state_id FROM ai_combat_ship WHERE owner_id = ?"",
        self.owner.worldid,
    )
    for (
        ship_id,
        state_id,
    ) in db_result:
        ship = WorldObject.get_object_by_id(ship_id)
        state = self.shipStates[state_id]
        # add move callbacks corresponding to given state
        if state == self.shipStates.moving:
            ship.add_move_callback(Callback(BehaviorMoveCallback._arrived, ship))
        self.add_new_unit(ship, state)
",if state == self . shipStates . moving :,188
"def addError(self, test, err):
    if err[0] is SkipTest:
        if self.showAll:
            self.stream.writeln(str(err[1]))
        elif self.dots:
            self.stream.write(""s"")
            self.stream.flush()
        return
    _org_AddError(self, test, err)
",if self . showAll :,93
"def _construct(self, node):
    self.flatten_mapping(node)
    ret = self.construct_pairs(node)
    keys = [d[0] for d in ret]
    keys_sorted = sorted(keys, key=_natsort_key)
    for key in keys:
        expected = keys_sorted.pop(0)
        if key != expected:
            raise ConstructorError(
                None,
                None,
                ""keys out of order: ""
                ""expected {} got {} at {}"".format(expected, key, node.start_mark),
            )
    return dict(ret)
",if key != expected :,159
"def sample_pos_items_for_u(u, num):
    # sample num pos items for u-th user
    pos_items = self.train_items[u]
    n_pos_items = len(pos_items)
    pos_batch = []
    while True:
        if len(pos_batch) == num:
            break
        pos_id = np.random.randint(low=0, high=n_pos_items, size=1)[0]
        pos_i_id = pos_items[pos_id]
        if pos_i_id not in pos_batch:
            pos_batch.append(pos_i_id)
    return pos_batch
",if pos_i_id not in pos_batch :,171
"def _get_id(self, type, id):
    fields = id.split("":"")
    if len(fields) >= 3:
        if type != fields[-2]:
            logger.warning(
                ""Expected id of type %s but found type %s %s"", type, fields[-2], id
            )
        return fields[-1]
    fields = id.split(""/"")
    if len(fields) >= 3:
        itype = fields[-2]
        if type != itype:
            logger.warning(
                ""Expected id of type %s but found type %s %s"", type, itype, id
            )
        return fields[-1].split(""?"")[0]
    return id
",if type != fields [ - 2 ] :,178
"def uninstall_environments(self, environments):
    environments = [
        env
        if not env.startswith(self.conda_context.envs_path)
        else os.path.basename(env)
        for env in environments
    ]
    return_codes = [self.conda_context.exec_remove([env]) for env in environments]
    final_return_code = 0
    for env, return_code in zip(environments, return_codes):
        if return_code == 0:
            log.debug(""Conda environment '%s' successfully removed."" % env)
        else:
            log.debug(""Conda environment '%s' could not be removed."" % env)
            final_return_code = return_code
    return final_return_code
",if return_code == 0 :,191
"def _add_hit_offset(self, context_list, string_name, original_offset, value):
    for context in context_list:
        hits_by_context_dict = self.hits_by_context.setdefault(context, {})
        if string_name not in hits_by_context_dict:
            hits_by_context_dict[string_name] = (
                original_offset,
                value.encode(""base64""),
            )
",if string_name not in hits_by_context_dict :,119
"def detab(self, text, length=None):
    """"""Remove a tab from the front of each line of the given text.""""""
    if length is None:
        length = self.tab_length
    newtext = []
    lines = text.split(""\n"")
    for line in lines:
        if line.startswith("" "" * length):
            newtext.append(line[length:])
        elif not line.strip():
            newtext.append("""")
        else:
            break
    return ""\n"".join(newtext), ""\n"".join(lines[len(newtext) :])
",elif not line . strip ( ) :,147
"def dump(self):
    print(self.package_name)
    for package, value in self.entries:
        print(str(package.version))
        if value is None:
            print(""    [FILTERED]"")
        elif isinstance(value, list):
            variants = value
            for variant in variants:
                print(""    %s"" % str(variant))
        else:
            print(""    %s"" % str(package))
",if value is None :,125
"def __lexical_scope(*args, **kwargs):
    try:
        scope = Scope(quasi)
        if quasi:
            binding_name_set_stack[-1].add_child(scope)
        binding_name_set_stack.append(scope)
        return func(*args, **kwargs)
    finally:
        if binding_name_set_stack[-1] is scope:
            binding_name_set_stack.pop()
",if quasi :,114
"def getnotes(self, origin=None):
    if origin is None:
        result = self.translator_comments
        if self.developer_comments:
            if result:
                result += ""\n"" + self.developer_comments
            else:
                result = self.developer_comments
        return result
    elif origin == ""translator"":
        return self.translator_comments
    elif origin in (""programmer"", ""developer"", ""source code""):
        return self.developer_comments
    else:
        raise ValueError(""Comment type not valid"")
",if self . developer_comments :,141
"def fix_datetime_fields(data: TableData, table: TableName) -> None:
    for item in data[table]:
        for field_name in DATE_FIELDS[table]:
            if item[field_name] is not None:
                item[field_name] = datetime.datetime.fromtimestamp(
                    item[field_name], tz=datetime.timezone.utc
                )
",if item [ field_name ] is not None :,102
"def _check_for_cart_error(cart):
    if cart._safe_get_element(""Cart.Request.Errors"") is not None:
        error = cart._safe_get_element(""Cart.Request.Errors.Error.Code"").text
        if error == ""AWS.ECommerceService.CartInfoMismatch"":
            raise CartInfoMismatchException(
                ""CartGet failed: AWS.ECommerceService.CartInfoMismatch ""
                ""make sure AssociateTag, CartId and HMAC are correct ""
                ""(dont use URLEncodedHMAC!!!)""
            )
        raise CartException(""CartGet failed: "" + error)
","if error == ""AWS.ECommerceService.CartInfoMismatch"" :",165
"def check_bounds(geometry):
    if isinstance(geometry[0], (list, tuple)):
        return list(map(check_bounds, geometry))
    else:
        if geometry[0] > 180 or geometry[0] < -180:
            raise ValueError(
                ""Longitude is out of bounds, check your JSON format or data""
            )
        if geometry[1] > 90 or geometry[1] < -90:
            raise ValueError(
                ""Latitude is out of bounds, check your JSON format or data""
            )
",if geometry [ 0 ] > 180 or geometry [ 0 ] < - 180 :,143
"def _mapper_output_protocol(self, step_num, step_map):
    map_key = self._step_key(step_num, ""mapper"")
    if map_key in step_map:
        if step_map[map_key] >= (len(step_map) - 1):
            return self.output_protocol()
        else:
            return self.internal_protocol()
    else:
        # mapper is not a script substep, so protocols don't apply at all
        return RawValueProtocol()
",if step_map [ map_key ] >= ( len ( step_map ) - 1 ) :,130
"def asset(*paths):
    for path in paths:
        fspath = www_root + ""/assets/"" + path
        etag = """"
        try:
            if env.cache_static:
                etag = asset_etag(fspath)
            else:
                os.stat(fspath)
        except FileNotFoundError as e:
            if path == paths[-1]:
                if not os.path.exists(fspath + "".spt""):
                    tell_sentry(e, {})
            else:
                continue
        except Exception as e:
            tell_sentry(e, {})
        return asset_url + path + (etag and ""?etag="" + etag)
",if path == paths [ - 1 ] :,182
"def ping(self, payload: Union[str, bytes] = """") -> None:
    if self.trace_enabled and self.ping_pong_trace_enabled:
        if isinstance(payload, bytes):
            payload = payload.decode(""utf-8"")
        self.logger.debug(
            ""Sending a ping data frame ""
            f""(session id: {self.session_id}, payload: {payload})""
        )
    data = _build_data_frame_for_sending(payload, FrameHeader.OPCODE_PING)
    with self.sock_send_lock:
        self.sock.send(data)
","if isinstance ( payload , bytes ) :",155
"def is_ac_power_connected():
    for power_source_path in Path(""/sys/class/power_supply/"").iterdir():
        try:
            with open(power_source_path / ""type"", ""r"") as f:
                if f.read().strip() != ""Mains"":
                    continue
            with open(power_source_path / ""online"", ""r"") as f:
                if f.read(1) == ""1"":
                    return True
        except IOError:
            continue
    return False
","if f . read ( 1 ) == ""1"" :",144
"def handle_noargs(self, **options):
    self.style = color_style()
    print(""Running Django's own validation:"")
    self.validate(display_num_errors=True)
    for model in loading.get_models():
        if hasattr(model, ""_create_content_base""):
            self.validate_base_model(model)
        if hasattr(model, ""_feincms_content_models""):
            self.validate_content_type(model)
","if hasattr ( model , ""_create_content_base"" ) :",117
"def _init_weights(self, module):
    if isinstance(module, nn.Linear):
        module.weight.data.normal_(mean=0.0, std=self.config.init_std)
        if module.bias is not None:
            module.bias.data.zero_()
    elif isinstance(module, nn.Embedding):
        module.weight.data.normal_(mean=0.0, std=self.config.init_std)
        if module.padding_idx is not None:
            module.weight.data[module.padding_idx].zero_()
",if module . bias is not None :,141
"def walk(msg, callback, data):
    partnum = 0
    for part in msg.walk():
        # multipart/* are just containers
        if part.get_content_maintype() == ""multipart"":
            continue
        ctype = part.get_content_type()
        if ctype is None:
            ctype = OCTET_TYPE
        filename = part.get_filename()
        if not filename:
            filename = PART_FN_TPL % (partnum)
        headers = dict(part)
        LOG.debug(headers)
        headers[""Content-Type""] = ctype
        payload = util.fully_decoded_payload(part)
        callback(data, filename, payload, headers)
        partnum = partnum + 1
",if ctype is None :,190
"def _mark_lcs(mask, dirs, m, n):
    while m != 0 and n != 0:
        if dirs[m, n] == ""|"":
            m -= 1
            n -= 1
            mask[m] = 1
        elif dirs[m, n] == ""^"":
            m -= 1
        elif dirs[m, n] == ""<"":
            n -= 1
        else:
            raise UnboundLocalError(""Illegal move"")
    return mask
","elif dirs [ m , n ] == ""^"" :",122
"def valid_localparts(strip_delimiters=False):
    for line in ABRIDGED_LOCALPART_VALID_TESTS.split(""\n""):
        # strip line, skip over empty lines
        line = line.strip()
        if line == """":
            continue
        # skip over comments or empty lines
        match = COMMENT.match(line)
        if match:
            continue
        # skip over localparts with delimiters
        if strip_delimiters:
            if "","" in line or "";"" in line:
                continue
        yield line
","if line == """" :",145
"def fetch(self, *tileables, **kw):
    ret_list = False
    if len(tileables) == 1 and isinstance(tileables[0], (tuple, list)):
        ret_list = True
        tileables = tileables[0]
    elif len(tileables) > 1:
        ret_list = True
    result = self._sess.fetch(*tileables, **kw)
    ret = []
    for r, t in zip(result, tileables):
        if hasattr(t, ""isscalar"") and t.isscalar() and getattr(r, ""size"", None) == 1:
            ret.append(r.item())
        else:
            ret.append(r)
    if ret_list:
        return ret
    return ret[0]
","if hasattr ( t , ""isscalar"" ) and t . isscalar ( ) and getattr ( r , ""size"" , None ) == 1 :",183
"def _convert(container):
    if _value_marker in container:
        force_list = False
        values = container.pop(_value_marker)
        if container.pop(_list_marker, False):
            force_list = True
            values.extend(_convert(x[1]) for x in sorted(container.items()))
        if not force_list and len(values) == 1:
            values = values[0]
        if not container:
            return values
        return _convert(container)
    elif container.pop(_list_marker, False):
        return [_convert(x[1]) for x in sorted(container.items())]
    return dict_cls((k, _convert(v)) for k, v in iteritems(container))
",if not force_list and len ( values ) == 1 :,188
"def _transform_init_kwargs(cls, kwargs):
    transformed = []
    for field in list(kwargs.keys()):
        prop = getattr(cls, field, None)
        if isinstance(prop, MoneyProperty):
            value = kwargs.pop(field)
            _transform_single_init_kwarg(prop, field, value, kwargs)
            transformed.append((field, value))
    return transformed
","if isinstance ( prop , MoneyProperty ) :",102
"def haslayer(self, cls):
    """"""true if self has a layer that is an instance of cls. Superseded by ""cls in self"" syntax.""""""
    if self.__class__ == cls or self.__class__.__name__ == cls:
        return 1
    for f in self.packetfields:
        fvalue_gen = self.getfieldval(f.name)
        if fvalue_gen is None:
            continue
        if not f.islist:
            fvalue_gen = SetGen(fvalue_gen, _iterpacket=0)
        for fvalue in fvalue_gen:
            if isinstance(fvalue, Packet):
                ret = fvalue.haslayer(cls)
                if ret:
                    return ret
    return self.payload.haslayer(cls)
",if ret :,199
"def insert_broken_add_sometimes(node):
    if node.op == theano.tensor.add:
        last_time_replaced[0] = not last_time_replaced[0]
        if last_time_replaced[0]:
            return [off_by_half(*node.inputs)]
    return False
",if last_time_replaced [ 0 ] :,78
"def testReadChunk10(self):
    # ""Test BZ2File.read() in chunks of 10 bytes""
    self.createTempFile()
    with BZ2File(self.filename) as bz2f:
        text = """"
        while 1:
            str = bz2f.read(10)
            if not str:
                break
            text += str
        self.assertEqual(text, self.TEXT)
",if not str :,111
"def generate_sv_faces(dcel_mesh, point_index, only_select=False, del_flag=None):
    # This part of function creates faces in SV format
    # It ignores  boundless super face
    sv_faces = []
    for i, face in enumerate(dcel_mesh.faces):
        if face.inners and face.outer:
            ""Face ({}) has inner components! Sverchok cant show polygons with holes."".format(
                i
            )
        if not face.outer or del_flag in face.flags:
            continue
        if only_select and not face.select:
            continue
        sv_faces.append([point_index[hedge.origin] for hedge in face.outer.loop_hedges])
    return sv_faces
",if only_select and not face . select :,198
"def __check_dict_contains(dct, dict_name, keys, comment="""", result=True):
    for key in keys:
        if key not in six.iterkeys(dct):
            result = False
            comment = __append_comment(
                ""Missing {0} in {1}"".format(key, dict_name), comment
            )
    return result, comment
",if key not in six . iterkeys ( dct ) :,95
"def _dump_arg_defaults(kwargs):
    """"""Inject default arguments for dump functions.""""""
    if current_app:
        kwargs.setdefault(""cls"", current_app.json_encoder)
        if not current_app.config[""JSON_AS_ASCII""]:
            kwargs.setdefault(""ensure_ascii"", False)
        kwargs.setdefault(""sort_keys"", current_app.config[""JSON_SORT_KEYS""])
    else:
        kwargs.setdefault(""sort_keys"", True)
        kwargs.setdefault(""cls"", JSONEncoder)
","if not current_app . config [ ""JSON_AS_ASCII"" ] :",129
"def _on_change(self):
    changed = False
    self.save()
    for key, value in self.data.items():
        if isinstance(value, bool):
            if value:
                changed = True
                break
        if isinstance(value, int):
            if value != 1:
                changed = True
                break
        elif value is None:
            continue
        elif len(value) != 0:
            changed = True
            break
    self._reset_button.disabled = not changed
",elif len ( value ) != 0 :,145
"def parse_win_proxy(val):
    proxies = []
    for p in val.split("";""):
        if ""="" in p:
            tab = p.split(""="", 1)
            if tab[0] == ""socks"":
                tab[0] = ""SOCKS4""
            proxies.append(
                (tab[0].upper(), tab[1], None, None)
            )  # type, addr:port, username, password
        else:
            proxies.append((""HTTP"", p, None, None))
    return proxies
","if ""="" in p :",142
"def predict(collect_dir, keys):
    run_all = len(keys) == 0
    validate_keys(keys)
    for exp_cfg in cfg:
        if run_all or exp_cfg[""key""] in keys:
            key = exp_cfg[""key""]
            _predict(key, exp_cfg[""sample_img""], collect_dir)
","if run_all or exp_cfg [ ""key"" ] in keys :",89
"def convert_port_bindings(port_bindings):
    result = {}
    for k, v in six.iteritems(port_bindings):
        key = str(k)
        if ""/"" not in key:
            key += ""/tcp""
        if isinstance(v, list):
            result[key] = [_convert_port_binding(binding) for binding in v]
        else:
            result[key] = [_convert_port_binding(v)]
    return result
","if ""/"" not in key :",119
"def assert_conll_writer_output(
    dataset: InternalBioNerDataset,
    expected_output: List[str],
    sentence_splitter: SentenceSplitter = None,
):
    outfile_path = tempfile.mkstemp()[1]
    try:
        sentence_splitter = (
            sentence_splitter
            if sentence_splitter
            else NoSentenceSplitter(tokenizer=SpaceTokenizer())
        )
        writer = CoNLLWriter(sentence_splitter=sentence_splitter)
        writer.write_to_conll(dataset, Path(outfile_path))
        contents = [l.strip() for l in open(outfile_path).readlines() if l.strip()]
    finally:
        os.remove(outfile_path)
    assert contents == expected_output
",if sentence_splitter,175
"def post(self, request, *args, **kwargs):
    self.comment_obj = get_object_or_404(Comment, id=request.POST.get(""commentid""))
    if request.user == self.comment_obj.commented_by:
        form = LeadCommentForm(request.POST, instance=self.comment_obj)
        if form.is_valid():
            return self.form_valid(form)
        return self.form_invalid(form)
    data = {""error"": ""You don't have permission to edit this comment.""}
    return JsonResponse(data)
",if form . is_valid ( ) :,142
"def trivia_list(self, ctx: commands.Context):
    """"""List available trivia categories.""""""
    lists = set(p.stem for p in self._all_lists())
    if await ctx.embed_requested():
        await ctx.send(
            embed=discord.Embed(
                title=_(""Available trivia lists""),
                colour=await ctx.embed_colour(),
                description="", "".join(sorted(lists)),
            )
        )
    else:
        msg = box(bold(_(""Available trivia lists"")) + ""\n\n"" + "", "".join(sorted(lists)))
        if len(msg) > 1000:
            await ctx.author.send(msg)
        else:
            await ctx.send(msg)
",if len ( msg ) > 1000 :,193
"def validate(self):
    result = validators.SUCCESS
    msgs = []
    for validator in self._validators:
        res, err = validator.validate()
        if res == validators.ERROR:
            result = res
        elif res == validators.WARNING and result != validators.ERROR:
            result = res
        if len(err) > 0:
            msgs.append(err)
    return result, ""\n"".join(msgs)
",if len ( err ) > 0 :,111
"def get_code(self, fullname=None):
    fullname = self._fix_name(fullname)
    if self.code is None:
        mod_type = self.etc[2]
        if mod_type == imp.PY_SOURCE:
            source = self.get_source(fullname)
            self.code = compile(source, self.filename, ""exec"")
        elif mod_type == imp.PY_COMPILED:
            self._reopen()
            try:
                self.code = read_code(self.file)
            finally:
                self.file.close()
        elif mod_type == imp.PKG_DIRECTORY:
            self.code = self._get_delegate().get_code()
    return self.code
",elif mod_type == imp . PY_COMPILED :,196
"def flush_file(self, key, f):
    f.flush()
    if self.compress:
        f.compress = zlib.compressobj(
            9, zlib.DEFLATED, -zlib.MAX_WBITS, zlib.DEF_MEM_LEVEL, 0
        )
    if len(self.files) > self.MAX_OPEN_FILES:
        if self.compress:
            open_files = sum(1 for f in self.files.values() if f.fileobj is not None)
            if open_files > self.MAX_OPEN_FILES:
                f.fileobj.close()
                f.fileobj = None
        else:
            f.close()
            self.files.pop(key)
",if open_files > self . MAX_OPEN_FILES :,183
"def TryMerge(self, d):
    while d.avail() > 0:
        tt = d.getVarInt32()
        if tt == 10:
            self.add_version(d.getPrefixedString())
            continue
        if tt == 0:
            raise ProtocolBuffer.ProtocolBufferDecodeError
        d.skipData(tt)
",if tt == 10 :,90
"def init_author_file(self):
    self.author_map = {}
    if self.ui.config(""git"", ""authors""):
        f = open(self.repo.wjoin(self.ui.config(""git"", ""authors"")))
        try:
            for line in f:
                line = line.strip()
                if not line or line.startswith(""#""):
                    continue
                from_, to = RE_AUTHOR_FILE.split(line, 2)
                self.author_map[from_] = to
        finally:
            f.close()
","if not line or line . startswith ( ""#"" ) :",152
"def decode_imsi(self, imsi):
    new_imsi = """"
    for a in imsi:
        c = hex(a)
        if len(c) == 4:
            new_imsi += str(c[3]) + str(c[2])
        else:
            new_imsi += str(c[2]) + ""0""
    mcc = new_imsi[1:4]
    mnc = new_imsi[4:6]
    return new_imsi, mcc, mnc
",if len ( c ) == 4 :,136
"def _get_infoset(self, prefname):
    """"""Return methods with the name starting with prefname.""""""
    infoset = []
    excludes = (""%sinfoset"" % prefname,)
    preflen = len(prefname)
    for name in dir(self.__class__):
        if name.startswith(prefname) and name not in excludes:
            member = getattr(self.__class__, name)
            if isinstance(member, MethodType):
                infoset.append(name[preflen:].replace(""_"", "" ""))
    return infoset
","if isinstance ( member , MethodType ) :",133
"def skip_to_close_match(self):
    nestedCount = 1
    while 1:
        tok = self.tokenizer.get_next_token()
        ttype = tok[""style""]
        if ttype == SCE_PL_UNUSED:
            return
        elif self.classifier.is_index_op(tok):
            tval = tok[""text""]
            if self.opHash.has_key(tval):
                if self.opHash[tval][1] == 1:
                    nestedCount += 1
                else:
                    nestedCount -= 1
                    if nestedCount <= 0:
                        break
",if nestedCount <= 0 :,176
"def findMarkForUnitTestNodes(self):
    """"""return the position of *all* non-ignored @mark-for-unit-test nodes.""""""
    c = self.c
    p, result, seen = c.rootPosition(), [], []
    while p:
        if p.v in seen:
            p.moveToNodeAfterTree()
        else:
            seen.append(p.v)
            if g.match_word(p.h, 0, ""@ignore""):
                p.moveToNodeAfterTree()
            elif p.h.startswith(""@mark-for-unit-tests""):
                result.append(p.copy())
                p.moveToNodeAfterTree()
            else:
                p.moveToThreadNext()
    return result
",if p . v in seen :,200
"def assert_parts_cleaned(self, earlier_parts, current_parts, expected_parts, hint):
    cleaned_parts = []
    for earlier in earlier_parts:
        earlier_part = earlier[""part""]
        earlier_step = earlier[""step""]
        found = False
        for current in current_parts:
            if earlier_part == current[""part""] and earlier_step == current[""step""]:
                found = True
                break
        if not found:
            cleaned_parts.append(dict(part=earlier_part, step=earlier_step))
    self.assertThat(cleaned_parts, HasLength(len(expected_parts)), hint)
    for expected in expected_parts:
        self.assertThat(cleaned_parts, Contains(expected), hint)
",if not found :,194
"def unmark_first_parents(event=None):
    """"""Mark the node and all its parents.""""""
    c = event.get(""c"")
    if not c:
        return
    changed = []
    for parent in c.p.self_and_parents():
        if parent.isMarked():
            parent.v.clearMarked()
            parent.setAllAncestorAtFileNodesDirty()
            changed.append(parent.copy())
    if changed:
        # g.es(""unmarked: "" + ', '.join([z.h for z in changed]))
        c.setChanged()
        c.redraw()
    return changed
",if parent . isMarked ( ) :,164
"def stop(self):
    self._log(""Monitor stop"")
    self._stop_requested = True
    try:
        if os.path.exists(self.fifo_path):
            fd = os.open(self.fifo_path, os.O_WRONLY)
            os.write(fd, b""X"")
            os.close(fd)
    except Exception as e:
        self._log(""err while closing: {0}"".format(str(e)))
    if self._thread:
        self._thread.join()
        self._thread = None
",if os . path . exists ( self . fifo_path ) :,141
"def DeleteEmptyCols(self):
    cols2delete = []
    for c in range(0, self.GetCols()):
        f = True
        for r in range(0, self.GetRows()):
            if self.FindItemAtPosition((r, c)) is not None:
                f = False
        if f:
            cols2delete.append(c)
    for i in range(0, len(cols2delete)):
        self.ShiftColsLeft(cols2delete[i] + 1)
        cols2delete = [x - 1 for x in cols2delete]
","if self . FindItemAtPosition ( ( r , c ) ) is not None :",150
"def _load_objects(self, obj_id_zset, limit, chunk_size=1000):
    ct = i = 0
    while True:
        id_chunk = obj_id_zset[i : i + chunk_size]
        if not id_chunk:
            return
        i += chunk_size
        for raw_data in self._data[id_chunk]:
            if not raw_data:
                continue
            if self._use_json:
                yield json.loads(decode(raw_data))
            else:
                yield raw_data
            ct += 1
            if limit and ct == limit:
                return
",if not id_chunk :,178
"def _convert_example(example, use_bfloat16):
    """"""Cast int64 into int32 and float32 to bfloat16 if use_bfloat16.""""""
    for key in list(example.keys()):
        val = example[key]
        if tf.keras.backend.is_sparse(val):
            val = tf.sparse.to_dense(val)
        if val.dtype == tf.int64:
            val = tf.cast(val, tf.int32)
        if use_bfloat16 and val.dtype == tf.float32:
            val = tf.cast(val, tf.bfloat16)
        example[key] = val
",if val . dtype == tf . int64 :,166
"def print_callees(self, *amount):
    width, list = self.get_print_list(amount)
    if list:
        self.calc_callees()
        self.print_call_heading(width, ""called..."")
        for func in list:
            if func in self.all_callees:
                self.print_call_line(width, func, self.all_callees[func])
            else:
                self.print_call_line(width, func, {})
        print >>self.stream
        print >>self.stream
    return self
",if func in self . all_callees :,147
"def on_task_input(self, task, config):
    if config is False:
        return
    for entry in task.entries:
        if ""&amp;"" in entry[""url""]:
            log_once(
                ""Corrected `%s` url (replaced &amp; with &)"" % entry[""title""],
                logger=log,
            )
            entry[""url""] = entry[""url""].replace(""&amp;"", ""&"")
","if ""&amp;"" in entry [ ""url"" ] :",112
"def function(self, inputs, outputs, ignore_empty=False):
    f = function(inputs, outputs, mode=self.mode)
    if self.mode is not None or theano.config.mode != ""FAST_COMPILE"":
        topo = f.maker.fgraph.toposort()
        topo_ = [node for node in topo if not isinstance(node.op, self.ignore_topo)]
        if ignore_empty:
            assert len(topo_) <= 1, topo_
        else:
            assert len(topo_) == 1, topo_
        if len(topo_) > 0:
            assert type(topo_[0].op) is self.op
    return f
",if len ( topo_ ) > 0 :,165
"def _get_env_command(self) -> Sequence[str]:
    """"""Get command sequence for `env` with configured flags.""""""
    env_list = [""env""]
    # Pass through configurable environment variables.
    for key in [""http_proxy"", ""https_proxy""]:
        value = self.build_provider_flags.get(key)
        if not value:
            continue
        # Ensure item is treated as string and append it.
        value = str(value)
        env_list.append(f""{key}={value}"")
    return env_list
",if not value :,136
"def _compare_single_run(self, compares_done):
    try:
        compare_id, redo = self.in_queue.get(
            timeout=float(self.config[""ExpertSettings""][""block_delay""])
        )
    except Empty:
        pass
    else:
        if self._decide_whether_to_process(compare_id, redo, compares_done):
            if redo:
                self.db_interface.delete_old_compare_result(compare_id)
            compares_done.add(compare_id)
            self._process_compare(compare_id)
            if self.callback:
                self.callback()
",if self . callback :,177
"def clean(self):
    # TODO: check for clashes if the random code is already taken
    if not self.code:
        self.code = u""static-%s"" % uuid.uuid4()
    if not self.site:
        placeholders = StaticPlaceholder.objects.filter(
            code=self.code, site__isnull=True
        )
        if self.pk:
            placeholders = placeholders.exclude(pk=self.pk)
        if placeholders.exists():
            raise ValidationError(
                _(""A static placeholder with the same site and code already exists"")
            )
",if placeholders . exists ( ) :,149
"def load_parser(self):
    result = OrderedDict()
    for name, flags in self.filenames:
        filename = self.get_filename(name)
        for match in sorted(glob(filename), key=self.file_key):
            # Needed to allow overlapping globs, more specific first
            if match in result:
                continue
            result[match] = TextParser(match, os.path.relpath(match, self.base), flags)
    return result
",if match in result :,119
"def __init__(self, selectable, name=None):
    baseselectable = selectable
    while isinstance(baseselectable, Alias):
        baseselectable = baseselectable.element
    self.original = baseselectable
    self.supports_execution = baseselectable.supports_execution
    if self.supports_execution:
        self._execution_options = baseselectable._execution_options
    self.element = selectable
    if name is None:
        if self.original.named_with_column:
            name = getattr(self.original, ""name"", None)
        name = _anonymous_label(""%%(%d %s)s"" % (id(self), name or ""anon""))
    self.name = name
",if self . original . named_with_column :,165
"def load_tour(self, tour_id):
    for tour_dir in self.tour_directories:
        tour_path = os.path.join(tour_dir, tour_id + "".yaml"")
        if not os.path.exists(tour_path):
            tour_path = os.path.join(tour_dir, tour_id + "".yml"")
        if os.path.exists(tour_path):
            return self._load_tour_from_path(tour_path)
",if os . path . exists ( tour_path ) :,122
"def _get_md_bg_color_down(self):
    t = self.theme_cls
    c = self.md_bg_color  # Default to no change on touch
    # Material design specifies using darker hue when on Dark theme
    if t.theme_style == ""Dark"":
        if self.md_bg_color == t.primary_color:
            c = t.primary_dark
        elif self.md_bg_color == t.accent_color:
            c = t.accent_dark
    return c
",if self . md_bg_color == t . primary_color :,135
"def get_data(self, state=None, request=None):
    if self.load_in_memory:
        data, shapes = self._in_memory_get_data(state, request)
    else:
        data, shapes = self._out_of_memory_get_data(state, request)
    for i in range(len(data)):
        if shapes[i] is not None:
            if isinstance(request, numbers.Integral):
                data[i] = data[i].reshape(shapes[i])
            else:
                for j in range(len(data[i])):
                    data[i][j] = data[i][j].reshape(shapes[i][j])
    return tuple(data)
","if isinstance ( request , numbers . Integral ) :",187
"def onClicked(event):
    if not self.path:
        if not os.path.exists(mh.getPath(""render"")):
            os.makedirs(mh.getPath(""render""))
        self.path = mh.getPath(""render"")
    filename, ftype = mh.getSaveFileName(
        os.path.splitext(self.path)[0],
        ""PNG Image (*.png);;JPEG Image (*.jpg);;Thumbnail (*.thumb);;All files (*.*)"",
    )
    if filename:
        if ""Thumbnail"" in ftype:
            self.image.save(filename, iformat=""PNG"")
        else:
            self.image.save(filename)
        self.path = os.path.dirname(filename)
","if not os . path . exists ( mh . getPath ( ""render"" ) ) :",187
"def _build_dom(cls, content, mode):
    assert mode in (""html"", ""xml"")
    if mode == ""html"":
        if not hasattr(THREAD_STORAGE, ""html_parser""):
            THREAD_STORAGE.html_parser = HTMLParser()
        dom = defusedxml.lxml.parse(
            StringIO(content), parser=THREAD_STORAGE.html_parser
        )
        return dom.getroot()
    else:
        if not hasattr(THREAD_STORAGE, ""xml_parser""):
            THREAD_STORAGE.xml_parser = XMLParser()
        dom = defusedxml.lxml.parse(BytesIO(content), parser=THREAD_STORAGE.xml_parser)
        return dom.getroot()
","if not hasattr ( THREAD_STORAGE , ""html_parser"" ) :",175
"def convert_path(ctx, tpath):
    for points, code in tpath.iter_segments():
        if code == Path.MOVETO:
            ctx.move_to(*points)
        elif code == Path.LINETO:
            ctx.line_to(*points)
        elif code == Path.CURVE3:
            ctx.curve_to(
                points[0], points[1], points[0], points[1], points[2], points[3]
            )
        elif code == Path.CURVE4:
            ctx.curve_to(*points)
        elif code == Path.CLOSEPOLY:
            ctx.close_path()
",if code == Path . MOVETO :,172
"def _targets(self, sigmaparser):
    # build list of matching target mappings
    targets = set()
    for condfield in self.conditions:
        if condfield in sigmaparser.values:
            rulefieldvalues = sigmaparser.values[condfield]
            for condvalue in self.conditions[condfield]:
                if condvalue in rulefieldvalues:
                    targets.update(self.conditions[condfield][condvalue])
    return targets
",if condvalue in rulefieldvalues :,115
"def create_image_upload():
    if request.method == ""POST"":
        image = request.form[""image""]
        if image:
            image_file = uploaded_file(file_content=image)
            image_url = upload_local(
                image_file, UPLOAD_PATHS[""temp""][""image""].format(uuid=uuid4())
            )
            return jsonify({""status"": ""ok"", ""image_url"": image_url})
        else:
            return jsonify({""status"": ""no_image""})
",if image :,135
"def lookup_actions(self, resp):
    actions = {}
    for action, conditions in self.actions.items():
        for condition, opts in conditions:
            for key, val in condition:
                if key[-1] == ""!"":
                    if resp.match(key[:-1], val):
                        break
                else:
                    if not resp.match(key, val):
                        break
            else:
                actions[action] = opts
    return actions
","if resp . match ( key [ : - 1 ] , val ) :",138
"def accept_quality(accept, default=1):
    """"""Separates out the quality score from the accepted content_type""""""
    quality = default
    if accept and "";"" in accept:
        accept, rest = accept.split("";"", 1)
        accept_quality = RE_ACCEPT_QUALITY.search(rest)
        if accept_quality:
            quality = float(accept_quality.groupdict().get(""quality"", quality).strip())
    return (quality, accept.strip())
",if accept_quality :,113
"def save(self, session=None, to=None, pickler=None):
    if to and pickler:
        self._save_to = (pickler, to)
    if self._save_to and len(self) > 0:
        with self._lock:
            pickler, fn = self._save_to
            if session:
                session.ui.mark(_(""Saving %s state to %s"") % (self, fn))
            pickler(self, fn)
",if session :,123
"def get_safe_settings():
    ""Returns a dictionary of the settings module, with sensitive settings blurred out.""
    settings_dict = {}
    for k in dir(settings):
        if k.isupper():
            if HIDDEN_SETTINGS.search(k):
                settings_dict[k] = ""********************""
            else:
                settings_dict[k] = getattr(settings, k)
    return settings_dict
",if HIDDEN_SETTINGS . search ( k ) :,109
"def _init_table_h():
    _table_h = []
    for i in range(256):
        part_l = i
        part_h = 0
        for j in range(8):
            rflag = part_l & 1
            part_l >>= 1
            if part_h & 1:
                part_l |= 1 << 31
            part_h >>= 1
            if rflag:
                part_h ^= 0xD8000000
        _table_h.append(part_h)
    return _table_h
",if rflag :,147
"def dns_query(server, timeout, protocol, qname, qtype, qclass):
    request = dns.message.make_query(qname, qtype, qclass)
    if protocol == ""tcp"":
        response = dns.query.tcp(
            request, server, timeout=timeout, one_rr_per_rrset=True
        )
    else:
        response = dns.query.udp(
            request, server, timeout=timeout, one_rr_per_rrset=True
        )
        if response.flags & dns.flags.TC:
            response = dns.query.tcp(
                request, server, timeout=timeout, one_rr_per_rrset=True
            )
    return response
",if response . flags & dns . flags . TC :,184
"def sum_and_divide(self, losses):
    if self.total_divisor != 0:
        output = torch.sum(losses) / self.total_divisor
        if torch.is_tensor(self.total_divisor):
            # remove from autograd graph if necessary
            self.total_divisor = self.total_divisor.item()
        return output
    return torch.sum(losses * 0)
",if torch . is_tensor ( self . total_divisor ) :,102
"def __iter__(self):
    for chunk in self.source:
        if chunk is not None:
            self.wait_counter = 0
            yield chunk
        elif self.wait_counter < self.wait_cntr_max:
            self.wait_counter += 1
        else:
            logger.warning(
                ""Data poller has been receiving no data for {} seconds.\n""
                ""Closing data poller"".format(self.wait_cntr_max * self.poll_period)
            )
            break
        time.sleep(self.poll_period)
",elif self . wait_counter < self . wait_cntr_max :,156
"def test_find_directive_from_block(self):
    blocks = self.config.parser_root.find_blocks(""virtualhost"")
    found = False
    for vh in blocks:
        if vh.filepath.endswith(""sites-enabled/certbot.conf""):
            servername = vh.find_directives(""servername"")
            self.assertEqual(servername[0].parameters[0], ""certbot.demo"")
            found = True
    self.assertTrue(found)
","if vh . filepath . endswith ( ""sites-enabled/certbot.conf"" ) :",118
"def assign_products(request, discount_id):
    """"""Assign products to given property group with given property_group_id.""""""
    discount = lfs_get_object_or_404(Discount, pk=discount_id)
    for temp_id in request.POST.keys():
        if temp_id.startswith(""product""):
            temp_id = temp_id.split(""-"")[1]
            product = Product.objects.get(pk=temp_id)
            discount.products.add(product)
    html = [[""#products-inline"", products_inline(request, discount_id, as_string=True)]]
    result = json.dumps(
        {""html"": html, ""message"": _(u""Products have been assigned."")}, cls=LazyEncoder
    )
    return HttpResponse(result, content_type=""application/json"")
","if temp_id . startswith ( ""product"" ) :",199
"def ChangeStyle(self, combos):
    style = 0
    for combo in combos:
        if combo.GetValue() == 1:
            if combo.GetLabel() == ""TR_VIRTUAL"":
                style = style | HTL.TR_VIRTUAL
            else:
                try:
                    style = style | eval(""wx."" + combo.GetLabel())
                except:
                    style = style | eval(""HTL."" + combo.GetLabel())
    if self.GetAGWWindowStyleFlag() != style:
        self.SetAGWWindowStyleFlag(style)
","if combo . GetLabel ( ) == ""TR_VIRTUAL"" :",153
"def _set_autocomplete(self, notebook):
    if notebook:
        try:
            if isinstance(notebook, str):
                notebook = NotebookInfo(notebook)
            obj, x = build_notebook(notebook)
            self.form.widgets[""namespace""].notebook = obj
            self.form.widgets[""page""].notebook = obj
            logger.debug(""Notebook for autocomplete: %s (%s)"", obj, notebook)
        except:
            logger.exception(""Could not set notebook: %s"", notebook)
    else:
        self.form.widgets[""namespace""].notebook = None
        self.form.widgets[""page""].notebook = None
        logger.debug(""Notebook for autocomplete unset"")
","if isinstance ( notebook , str ) :",178
"def emitSubDomainData(self, subDomainData, event):
    self.emitRawRirData(subDomainData, event)
    for subDomainElem in subDomainData:
        if self.checkForStop():
            return None
        subDomain = subDomainElem.get(""subdomain"", """").strip()
        if subDomain:
            self.emitHostname(subDomain, event)
",if self . checkForStop ( ) :,99
"def get_all_subnets(self, subnet_ids=None, filters=None):
    # Extract a list of all subnets
    matches = itertools.chain(*[x.values() for x in self.subnets.values()])
    if subnet_ids:
        matches = [sn for sn in matches if sn.id in subnet_ids]
        if len(subnet_ids) > len(matches):
            unknown_ids = set(subnet_ids) - set(matches)
            raise InvalidSubnetIdError(unknown_ids)
    if filters:
        matches = generic_filter(filters, matches)
    return matches
",if len ( subnet_ids ) > len ( matches ) :,152
"def _compat_map(self, avs):
    apps = {}
    for av in avs:
        av.version = self
        app_id = av.application
        if app_id in amo.APP_IDS:
            apps[amo.APP_IDS[app_id]] = av
    return apps
",if app_id in amo . APP_IDS :,80
"def generator(self, data):
    if self._config.SILENT:
        silent_vars = self._get_silent_vars()
    for task in data:
        for var, val in task.environment_variables():
            if self._config.SILENT:
                if var in silent_vars:
                    continue
            yield (
                0,
                [
                    int(task.UniqueProcessId),
                    str(task.ImageFileName),
                    Address(task.Peb.ProcessParameters.Environment),
                    str(var),
                    str(val),
                ],
            )
",if var in silent_vars :,182
"def warn_if_repeatable_read(self):
    if ""mysql"" in self.current_engine().lower():
        cursor = self.connection_for_read().cursor()
        if cursor.execute(""SELECT @@tx_isolation""):
            isolation = cursor.fetchone()[0]
            if isolation == ""REPEATABLE-READ"":
                warnings.warn(
                    TxIsolationWarning(
                        ""Polling results with transaction isolation level ""
                        ""repeatable-read within the same transaction ""
                        ""may give outdated results. Be sure to commit the ""
                        ""transaction for each poll iteration.""
                    )
                )
","if isolation == ""REPEATABLE-READ"" :",182
"def filter_by_level(record, level_per_module):
    name = record[""name""]
    level = 0
    if name in level_per_module:
        level = level_per_module[name]
    elif name is not None:
        lookup = """"
        if """" in level_per_module:
            level = level_per_module[""""]
        for n in name.split("".""):
            lookup += n
            if lookup in level_per_module:
                level = level_per_module[lookup]
            lookup += "".""
    if level is False:
        return False
    return record[""level""].no >= level
","if """" in level_per_module :",166
"def _readStream(self, handle: str, path: str) -> None:
    eof = False
    file = Path(path)
    with file.open(""w"") as f:
        while not eof:
            response = await self._client.send(""IO.read"", {""handle"": handle})
            eof = response.get(""eof"", False)
            if path:
                f.write(response.get(""data"", """"))
    await self._client.send(""IO.close"", {""handle"": handle})
",if path :,128
"def sendall(self, data, flags=0):
    if self._sslobj:
        if flags != 0:
            raise ValueError(
                ""non-zero flags not allowed in calls to sendall() on %s""
                % self.__class__
            )
        amount = len(data)
        count = 0
        while count < amount:
            v = self.send(data[count:])
            count += v
        return amount
    else:
        return socket.sendall(self, data, flags)
",if flags != 0 :,141
"def run(self):
    utils.assert_main_thread()
    # As a convenience, we'll set up the connection
    # if there isn't one. So F5 (etc) can be hit
    # to get started.
    if not channel:
        if not chrome_launched():
            SwiDebugStartChromeCommand.run(self)
        else:
            self.window.run_command(""swi_debug_start"")
    elif paused:
        logger.info(""Resuming..."")
        channel.send(webkit.Debugger.resume())
    else:
        logger.info(""Pausing..."")
        channel.send(webkit.Debugger.setSkipAllPauses(False))
        channel.send(webkit.Debugger.pause())
",if not chrome_launched ( ) :,190
"def TryMerge(self, d):
    while d.avail() > 0:
        tt = d.getVarInt32()
        if tt == 10:
            length = d.getVarInt32()
            tmp = ProtocolBuffer.Decoder(d.buffer(), d.pos(), d.pos() + length)
            d.skip(length)
            self.add_presence_response().TryMerge(tmp)
            continue
        if tt == 0:
            raise ProtocolBuffer.ProtocolBufferDecodeError
        d.skipData(tt)
",if tt == 0 :,140
"def _replace_home(x):
    if xp.ON_WINDOWS:
        home = (
            builtins.__xonsh__.env[""HOMEDRIVE""] + builtins.__xonsh__.env[""HOMEPATH""][0]
        )
        if x.startswith(home):
            x = x.replace(home, ""~"", 1)
        if builtins.__xonsh__.env.get(""FORCE_POSIX_PATHS""):
            x = x.replace(os.sep, os.altsep)
        return x
    else:
        home = builtins.__xonsh__.env[""HOME""]
        if x.startswith(home):
            x = x.replace(home, ""~"", 1)
        return x
","if builtins . __xonsh__ . env . get ( ""FORCE_POSIX_PATHS"" ) :",176
"def semanticTags(self, semanticTags):
    if semanticTags is None:
        self.__semanticTags = OrderedDict()
    # check
    for key, value in list(semanticTags.items()):
        if not isinstance(key, int):
            raise TypeError(""At least one key is not a valid int position"")
        if not isinstance(value, list):
            raise TypeError(
                ""At least one value of the provided dict is not a list of string""
            )
        for x in value:
            if not isinstance(x, str):
                raise TypeError(
                    ""At least one value of the provided dict is not a list of string""
                )
    self.__semanticTags = semanticTags
","if not isinstance ( key , int ) :",184
"def _recv():
    try:
        return sock.recv(bufsize)
    except SSLWantReadError:
        pass
    except socket.error as exc:
        error_code = extract_error_code(exc)
        if error_code is None:
            raise
        if error_code != errno.EAGAIN or error_code != errno.EWOULDBLOCK:
            raise
    r, w, e = select.select((sock,), (), (), sock.gettimeout())
    if r:
        return sock.recv(bufsize)
",if error_code != errno . EAGAIN or error_code != errno . EWOULDBLOCK :,139
"def _authenticate(self):
    oauth_token = self.options.get(""oauth_token"")
    if oauth_token and not self.api.oauth_token:
        self.logger.info(""Attempting to authenticate using OAuth token"")
        self.api.oauth_token = oauth_token
        user = self.api.user(schema=_user_schema)
        if user:
            self.logger.info(""Successfully logged in as {0}"", user)
        else:
            self.logger.error(
                ""Failed to authenticate, the access token "" ""is not valid""
            )
    else:
        return JustinTVPluginBase._authenticate(self)
",if user :,168
"def reverse(self, *args):
    assert self._path is not None, ""Cannot reverse url regex "" + self.regex.pattern
    assert len(args) == self._group_count, ""required number of arguments "" ""not found""
    if not len(args):
        return self._path
    converted_args = []
    for a in args:
        if not isinstance(a, (unicode_type, bytes)):
            a = str(a)
        converted_args.append(escape.url_escape(utf8(a), plus=False))
    return self._path % tuple(converted_args)
","if not isinstance ( a , ( unicode_type , bytes ) ) :",147
"def determine_block_hints(self, text):
    hints = """"
    if text:
        if text[0] in "" \n\x85\u2028\u2029"":
            hints += str(self.best_indent)
        if text[-1] not in ""\n\x85\u2028\u2029"":
            hints += ""-""
        elif len(text) == 1 or text[-2] in ""\n\x85\u2028\u2029"":
            hints += ""+""
    return hints
","if text [ 0 ] in "" \n\x85\u2028\u2029"" :",132
"def find_package_modules(package, mask):
    import fnmatch
    if hasattr(package, ""__loader__"") and hasattr(package.__loader__, ""_files""):
        path = package.__name__.replace(""."", os.path.sep)
        mask = os.path.join(path, mask)
        for fnm in package.__loader__._files.iterkeys():
            if fnmatch.fnmatchcase(fnm, mask):
                yield os.path.splitext(fnm)[0].replace(os.path.sep, ""."")
    else:
        path = package.__path__[0]
        for fnm in os.listdir(path):
            if fnmatch.fnmatchcase(fnm, mask):
                yield ""%s.%s"" % (package.__name__, os.path.splitext(fnm)[0])
","if fnmatch . fnmatchcase ( fnm , mask ) :",191
"def _condition(ct):
    for qobj in args:
        if qobj.connector == ""AND"" and not qobj.negated:
            # normal kwargs are an AND anyway, so just use those for now
            for child in qobj.children:
                kwargs.update(dict([child]))
        else:
            raise NotImplementedError(""Unsupported Q object"")
    for attr, val in kwargs.items():
        if getattr(ct, attr) != val:
            return False
    return True
","if getattr ( ct , attr ) != val :",127
"def process(self, resources):
    session = local_session(self.manager.session_factory)
    client = session.client(""logs"")
    state = self.data.get(""state"", True)
    key = self.resolve_key(self.data.get(""kms-key""))
    for r in resources:
        try:
            if state:
                client.associate_kms_key(logGroupName=r[""logGroupName""], kmsKeyId=key)
            else:
                client.disassociate_kms_key(logGroupName=r[""logGroupName""])
        except client.exceptions.ResourceNotFoundException:
            continue
",if state :,153
"def get_xmm(env, ii):
    if is_gather(ii):
        if ii.space == ""evex"":
            return gen_reg_simd_unified(env, ""xmm_evex"", True)
        return gen_reg_simd_unified(env, ""xmm"", False)
    if ii.space == ""evex"":
        return gen_reg(env, ""xmm_evex"")
    return gen_reg(env, ""xmm"")
","if ii . space == ""evex"" :",119
"def parent(self):
    """"""Return the parent device.""""""
    if self._has_parent is None:
        _parent = self._ctx.backend.get_parent(self._ctx.dev)
        self._has_parent = _parent is not None
        if self._has_parent:
            self._parent = Device(_parent, self._ctx.backend)
        else:
            self._parent = None
    return self._parent
",if self . _has_parent :,109
"def cascade(self, event=None):
    """"""Cascade all Leo windows.""""""
    x, y, delta = 50, 50, 50
    for frame in g.app.windowList:
        w = frame and frame.top
        if w:
            r = w.geometry()  # a Qt.Rect
            # 2011/10/26: Fix bug 823601: cascade-windows fails.
            w.setGeometry(QtCore.QRect(x, y, r.width(), r.height()))
            # Compute the new offsets.
            x += 30
            y += 30
            if x > 200:
                x = 10 + delta
                y = 40 + delta
                delta += 10
",if w :,190
"def _GetGoodDispatchAndUserName(IDispatch, userName, clsctx):
    # Get a dispatch object, and a 'user name' (ie, the name as
    # displayed to the user in repr() etc.
    if userName is None:
        if isinstance(IDispatch, str):
            userName = IDispatch
        elif isinstance(IDispatch, unicode):
            # We always want the displayed name to be a real string
            userName = IDispatch.encode(""ascii"", ""replace"")
    elif type(userName) == unicode:
        # As above - always a string...
        userName = userName.encode(""ascii"", ""replace"")
    else:
        userName = str(userName)
    return (_GetGoodDispatch(IDispatch, clsctx), userName)
","elif isinstance ( IDispatch , unicode ) :",200
"def _infer_return_type(*args):
    """"""Look at the type of all args and divine their implied return type.""""""
    return_type = None
    for arg in args:
        if arg is None:
            continue
        if isinstance(arg, bytes):
            if return_type is str:
                raise TypeError(""Can't mix bytes and non-bytes in "" ""path components."")
            return_type = bytes
        else:
            if return_type is bytes:
                raise TypeError(""Can't mix bytes and non-bytes in "" ""path components."")
            return_type = str
    if return_type is None:
        return str  # tempfile APIs return a str by default.
    return return_type
",if return_type is bytes :,186
"def test_ESPnetDataset_h5file_1(h5file_1):
    dataset = IterableESPnetDataset(
        path_name_type_list=[(h5file_1, ""data4"", ""hdf5"")],
        preprocess=preprocess,
    )
    for key, data in dataset:
        if key == ""a"":
            assert data[""data4""].shape == (
                100,
                80,
            )
        if key == ""b"":
            assert data[""data4""].shape == (
                150,
                80,
            )
","if key == ""b"" :",157
"def iter_fields(node, *, include_meta=True, exclude_unset=False):
    exclude_meta = not include_meta
    for field_name, field in node._fields.items():
        if exclude_meta and field.meta:
            continue
        field_val = getattr(node, field_name, _marker)
        if field_val is _marker:
            continue
        if exclude_unset:
            if callable(field.default):
                default = field.default()
            else:
                default = field.default
            if field_val == default:
                continue
        yield field_name, field_val
",if field_val is _marker :,171
"def then(self, matches, when_response, context):
    if is_iterable(when_response):
        ret = []
        when_response = list(when_response)
        for match in when_response:
            if match not in matches:
                if self.match_name:
                    match.name = self.match_name
                matches.append(match)
                ret.append(match)
        return ret
    if self.match_name:
        when_response.name = self.match_name
    if when_response not in matches:
        matches.append(when_response)
        return when_response
",if match not in matches :,169
"def _set_chat_ids(self, chat_id: SLT[int]) -> None:
    with self.__lock:
        if chat_id and self._usernames:
            raise RuntimeError(
                f""Can't set {self.chat_id_name} in conjunction with (already set) ""
                f""{self.username_name}s.""
            )
        self._chat_ids = self._parse_chat_id(chat_id)
",if chat_id and self . _usernames :,118
"def discover(self, *objlist):
    ret = []
    for l in self.splitlines():
        if len(l) < 5:
            continue
        if l[0] == ""Filename"":
            continue
        try:
            int(l[2])
            int(l[3])
        except:
            continue
        #           ret.append(improve(l[0]))
        ret.append(l[0])
    ret.sort()
    for item in objlist:
        ret.append(item)
    return ret
",if len ( l ) < 5 :,154
"def get_changed_module(self):
    source = self.resource.read()
    change_collector = codeanalyze.ChangeCollector(source)
    if self.replacement is not None:
        change_collector.add_change(self.skip_start, self.skip_end, self.replacement)
    for occurrence in self.occurrence_finder.find_occurrences(self.resource):
        start, end = occurrence.get_primary_range()
        if self.skip_start <= start < self.skip_end:
            self.handle.occurred_inside_skip(change_collector, occurrence)
        else:
            self.handle.occurred_outside_skip(change_collector, occurrence)
    result = change_collector.get_changed()
    if result is not None and result != source:
        return result
",if self . skip_start <= start < self . skip_end :,198
"def hpat_pandas_series_var_impl(
    self, axis=None, skipna=None, level=None, ddof=1, numeric_only=None
):
    if skipna is None:
        skipna = True
    if skipna:
        valuable_length = len(self._data) - numpy.sum(numpy.isnan(self._data))
        if valuable_length <= ddof:
            return numpy.nan
        return (
            numpy_like.nanvar(self._data) * valuable_length / (valuable_length - ddof)
        )
    if len(self._data) <= ddof:
        return numpy.nan
    return self._data.var() * len(self._data) / (len(self._data) - ddof)
",if valuable_length <= ddof :,188
"def to_dict(self, validate=True, ignore=(), context=None):
    context = context or {}
    condition = getattr(self, ""condition"", Undefined)
    copy = self  # don't copy unless we need to
    if condition is not Undefined:
        if isinstance(condition, core.SchemaBase):
            pass
        elif ""field"" in condition and ""type"" not in condition:
            kwds = parse_shorthand(condition[""field""], context.get(""data"", None))
            copy = self.copy(deep=[""condition""])
            copy.condition.update(kwds)
    return super(ValueChannelMixin, copy).to_dict(
        validate=validate, ignore=ignore, context=context
    )
","if isinstance ( condition , core . SchemaBase ) :",175
"def get_field_result(self, result, field_name):
    if isinstance(result.field, models.ImageField):
        if result.value:
            img = getattr(result.obj, field_name)
            result.text = mark_safe(
                '<a href=""%s"" target=""_blank"" title=""%s"" data-gallery=""gallery""><img src=""%s"" class=""field_img""/></a>'
                % (img.url, result.label, img.url)
            )
            self.include_image = True
    return result
",if result . value :,148
"def run(self):
    try:
        while True:
            dp = self.queue_get_stoppable(self.inq)
            if self.stopped():
                return
            # cannot ignore None here. will lead to unsynced send/recv
            obj = self.func(dp)
            self.queue_put_stoppable(self.outq, obj)
    except Exception:
        if self.stopped():
            pass  # skip duplicated error messages
        else:
            raise
    finally:
        self.stop()
",if self . stopped ( ) :,148
"def _evaluate_local_single(self, iterator):
    for batch in iterator:
        in_arrays = convert._call_converter(self.converter, batch, self.device)
        with function.no_backprop_mode():
            if isinstance(in_arrays, tuple):
                results = self.calc_local(*in_arrays)
            elif isinstance(in_arrays, dict):
                results = self.calc_local(**in_arrays)
            else:
                results = self.calc_local(in_arrays)
        if self._progress_hook:
            self._progress_hook(batch)
        yield results
",if self . _progress_hook :,166
"def merge(self, other):
    d = self._name2ft
    for name, (f, t) in other._name2ft.items():
        if name in d:
            # Don't print here by default, since doing
            #     so breaks some of the buildbots
            # print ""*** DocTestRunner.merge: '"" + name + ""' in both"" \
            #    "" testers; summing outcomes.""
            f2, t2 = d[name]
            f = f + f2
            t = t + t2
        d[name] = f, t
",if name in d :,157
"def _addSettingsToPanels(self, category, left, right):
    count = len(profile.getSubCategoriesFor(category)) + len(
        profile.getSettingsForCategory(category)
    )
    p = left
    n = 0
    for title in profile.getSubCategoriesFor(category):
        n += 1 + len(profile.getSettingsForCategory(category, title))
        if n > count / 2:
            p = right
        configBase.TitleRow(p, _(title))
        for s in profile.getSettingsForCategory(category, title):
            configBase.SettingRow(p, s.getName())
",if n > count / 2 :,159
"def __init__(self, parent, dir, mask, with_dirs=True):
    filelist = []
    dirlist = [""..""]
    self.dir = dir
    self.file = """"
    mask = mask.upper()
    pattern = self.MakeRegex(mask)
    for i in os.listdir(dir):
        if i == ""."" or i == "".."":
            continue
        path = os.path.join(dir, i)
        if os.path.isdir(path):
            dirlist.append(i)
            continue
        path = path.upper()
        value = i.upper()
        if pattern.match(value) is not None:
            filelist.append(i)
    self.files = filelist
    if with_dirs:
        self.dirs = dirlist
","if i == ""."" or i == "".."" :",199
"def check_network_private(test_network):
    test_net = ipaddress.IPNetwork(test_network)
    test_start = test_net.network
    test_end = test_net.broadcast
    for network in settings.vpn.safe_priv_subnets:
        network = ipaddress.IPNetwork(network)
        net_start = network.network
        net_end = network.broadcast
        if test_start >= net_start and test_end <= net_end:
            return True
    return False
",if test_start >= net_start and test_end <= net_end :,128
"def _end_description(self):
    if self._summaryKey == ""content"":
        self._end_content()
    else:
        value = self.popContent(""description"")
        context = self._getContext()
        if self.intextinput:
            context[""textinput""][""description""] = value
        elif self.inimage:
            context[""image""][""description""] = value
    self._summaryKey = None
",if self . intextinput :,107
"def compute_nullable_nonterminals(self):
    nullable = {}
    num_nullable = 0
    while 1:
        for p in self.grammar.Productions[1:]:
            if p.len == 0:
                nullable[p.name] = 1
                continue
            for t in p.prod:
                if not t in nullable:
                    break
            else:
                nullable[p.name] = 1
        if len(nullable) == num_nullable:
            break
        num_nullable = len(nullable)
    return nullable
",if p . len == 0 :,153
"def process_bind_param(self, value, dialect):
    if value is not None:
        if MAX_METADATA_VALUE_SIZE is not None:
            for k, v in list(value.items()):
                sz = total_size(v)
                if sz > MAX_METADATA_VALUE_SIZE:
                    del value[k]
                    log.warning(
                        ""Refusing to bind metadata key {} due to size ({})"".format(
                            k, sz
                        )
                    )
        value = json_encoder.encode(value).encode()
    return value
",if sz > MAX_METADATA_VALUE_SIZE :,168
"def process_input_line(self, line, store_history=True):
    """"""process the input, capturing stdout""""""
    stdout = sys.stdout
    splitter = self.IP.input_splitter
    try:
        sys.stdout = self.cout
        splitter.push(line)
        more = splitter.push_accepts_more()
        if not more:
            try:
                source_raw = splitter.source_raw_reset()[1]
            except:
                # recent ipython #4504
                source_raw = splitter.raw_reset()
            self.IP.run_cell(source_raw, store_history=store_history)
    finally:
        sys.stdout = stdout
",if not more :,186
"def _dump_section(self, name, values, f):
    doc = ""__doc__""
    if doc in values:
        print(""# %s"" % values[doc], file=f)
    print(""%s("" % name, file=f)
    for k, v in values.items():
        if k.endswith(""__doc__""):
            continue
        doc = k + ""__doc__""
        if doc in values:
            print(""    # %s"" % values[doc], file=f)
        print(""    %s = %s,"" % (k, pprint.pformat(v, indent=8)), file=f)
    print("")\n"", file=f)
",if doc in values :,168
"def open_session(self, app, request):
    sid = request.cookies.get(app.session_cookie_name)
    if sid:
        stored_session = self.cls.objects(sid=sid).first()
        if stored_session:
            expiration = stored_session.expiration
            if not expiration.tzinfo:
                expiration = expiration.replace(tzinfo=utc)
            if expiration > datetime.datetime.utcnow().replace(tzinfo=utc):
                return MongoEngineSession(
                    initial=stored_session.data, sid=stored_session.sid
                )
    return MongoEngineSession(sid=str(uuid.uuid4()))
",if not expiration . tzinfo :,174
"def table_entry(mode1, bind_type1, mode2, bind_type2):
    with sock(mode1) as sock1:
        bind(sock1, bind_type1)
        try:
            with sock(mode2) as sock2:
                bind(sock2, bind_type2)
        except OSError as exc:
            if exc.winerror == errno.WSAEADDRINUSE:
                return ""INUSE""
            elif exc.winerror == errno.WSAEACCES:
                return ""ACCESS""
            raise
        else:
            return ""Success""
",if exc . winerror == errno . WSAEADDRINUSE :,160
"def __init__(self, ruleset):
    # Organize rules by path
    self.ruleset = ruleset
    self.rules = {}
    for filename in self.ruleset.rules:
        for rule in self.ruleset.rules[filename]:
            if not rule.enabled:
                continue
            manage_dictionary(self.rules, rule.path, [])
            self.rules[rule.path].append(rule)
",if not rule . enabled :,111
"def talk(self, words):
    if self.writeSentence(words) == 0:
        return
    r = []
    while 1:
        i = self.readSentence()
        if len(i) == 0:
            continue
        reply = i[0]
        attrs = {}
        for w in i[1:]:
            j = w.find(""="", 1)
            if j == -1:
                attrs[w] = """"
            else:
                attrs[w[:j]] = w[j + 1 :]
        r.append((reply, attrs))
        if reply == ""!done"":
            return r
",if j == - 1 :,169
"def _check_decorator_overload(name: str, old: str, new: str) -> int:
    """"""Conditions for a decorator to overload an existing one.""""""
    properties = _property_decorators(name)
    if old == new:
        return _MERGE
    elif old in properties and new in properties:
        p_old, p_new = properties[old].precedence, properties[new].precedence
        if p_old > p_new:
            return _DISCARD
        elif p_old == p_new:
            return _MERGE
        else:
            return _REPLACE
    raise OverloadedDecoratorError(name, """")
",if p_old > p_new :,158
"def validate_pk(self):
    try:
        self._key = serialization.load_pem_private_key(
            self.key, password=None, backend=default_backend()
        )
        if self._key.key_size > 2048:
            AWSValidationException(
                ""The private key length is not supported. Only 1024-bit and 2048-bit are allowed.""
            )
    except Exception as err:
        if isinstance(err, AWSValidationException):
            raise
        raise AWSValidationException(
            ""The private key is not PEM-encoded or is not valid.""
        )
","if isinstance ( err , AWSValidationException ) :",157
"def _add_custom_statement(self, custom_statements):
    if custom_statements is None:
        return
    self.resource_policy[""Version""] = ""2012-10-17""
    if self.resource_policy.get(""Statement"") is None:
        self.resource_policy[""Statement""] = custom_statements
    else:
        if not isinstance(custom_statements, list):
            custom_statements = [custom_statements]
        statement = self.resource_policy[""Statement""]
        if not isinstance(statement, list):
            statement = [statement]
        for s in custom_statements:
            if s not in statement:
                statement.append(s)
        self.resource_policy[""Statement""] = statement
","if not isinstance ( custom_statements , list ) :",184
"def load(self, repn):
    for key in repn:
        tmp = self._convert(key)
        if tmp not in self:
            self.declare(tmp)
        item = dict.__getitem__(self, tmp)
        item._active = True
        item.load(repn[key])
",if tmp not in self :,80
"def on_press_release(x):
    """"""Keyboard callback function.""""""
    global is_recording, enable_trigger_record
    press = keyboard.KeyboardEvent(""down"", 28, ""space"")
    release = keyboard.KeyboardEvent(""up"", 28, ""space"")
    if x.event_type == ""down"" and x.name == press.name:
        if (not is_recording) and enable_trigger_record:
            sys.stdout.write(""Start Recording ... "")
            sys.stdout.flush()
            is_recording = True
    if x.event_type == ""up"" and x.name == release.name:
        if is_recording == True:
            is_recording = False
",if is_recording == True :,173
"def apply_mask(self, mask, data_t, data_f):
    ind_t, ind_f = 0, 0
    out = []
    for m in cycle(mask):
        if m:
            if ind_t == len(data_t):
                return out
            out.append(data_t[ind_t])
            ind_t += 1
        else:
            if ind_f == len(data_f):
                return out
            out.append(data_f[ind_f])
            ind_f += 1
    return out
",if ind_f == len ( data_f ) :,154
"def oo_contains_rule(source, apiGroups, resources, verbs):
    """"""Return true if the specified rule is contained within the provided source""""""
    rules = source[""rules""]
    if rules:
        for rule in rules:
            if set(rule[""apiGroups""]) == set(apiGroups):
                if set(rule[""resources""]) == set(resources):
                    if set(rule[""verbs""]) == set(verbs):
                        return True
    return False
","if set ( rule [ ""verbs"" ] ) == set ( verbs ) :",118
"def _maybe_commit_artifact(self, artifact_id):
    artifact_status = self._artifacts[artifact_id]
    if artifact_status[""pending_count""] == 0 and artifact_status[""commit_requested""]:
        for callback in artifact_status[""pre_commit_callbacks""]:
            callback()
        if artifact_status[""finalize""]:
            self._api.commit_artifact(artifact_id)
        for callback in artifact_status[""post_commit_callbacks""]:
            callback()
","if artifact_status [ ""finalize"" ] :",121
"def shuffler(iterator, pool_size=10 ** 5, refill_threshold=0.9):
    yields_between_refills = round(pool_size * (1 - refill_threshold))
    # initialize pool; this step may or may not exhaust the iterator.
    pool = take_n(pool_size, iterator)
    while True:
        random.shuffle(pool)
        for i in range(yields_between_refills):
            yield pool.pop()
        next_batch = take_n(yields_between_refills, iterator)
        if not next_batch:
            break
        pool.extend(next_batch)
    # finish consuming whatever's left - no need for further randomization.
    yield from pool
",if not next_batch :,186
"def __getitem__(self, key, _get_mode=False):
    if not _get_mode:
        if isinstance(key, (int, long)):
            return self._list[key]
        elif isinstance(key, slice):
            return self.__class__(self._list[key])
    ikey = key.lower()
    for k, v in self._list:
        if k.lower() == ikey:
            return v
    # micro optimization: if we are in get mode we will catch that
    # exception one stack level down so we can raise a standard
    # key error instead of our special one.
    if _get_mode:
        raise KeyError()
    raise BadRequestKeyError(key)
","elif isinstance ( key , slice ) :",176
"def find(self, path):
    if os.path.isfile(path) or os.path.islink(path):
        self.num_files = self.num_files + 1
        if self.match_function(path):
            self.files.append(path)
    elif os.path.isdir(path):
        for content in os.listdir(path):
            file = os.path.join(path, content)
            if os.path.isfile(file) or os.path.islink(file):
                self.num_files = self.num_files + 1
                if self.match_function(file):
                    self.files.append(file)
            else:
                self.find(file)
",if self . match_function ( file ) :,192
"def validate_nb(self, nb):
    super(MetadataValidatorV3, self).validate_nb(nb)
    ids = set([])
    for cell in nb.cells:
        if ""nbgrader"" not in cell.metadata:
            continue
        grade = cell.metadata[""nbgrader""][""grade""]
        solution = cell.metadata[""nbgrader""][""solution""]
        locked = cell.metadata[""nbgrader""][""locked""]
        if not grade and not solution and not locked:
            continue
        grade_id = cell.metadata[""nbgrader""][""grade_id""]
        if grade_id in ids:
            raise ValidationError(""Duplicate grade id: {}"".format(grade_id))
        ids.add(grade_id)
","if ""nbgrader"" not in cell . metadata :",186
"def _skip_start(self):
    start, stop = self.start, self.stop
    for chunk in self.app_iter:
        self._pos += len(chunk)
        if self._pos < start:
            continue
        elif self._pos == start:
            return b""""
        else:
            chunk = chunk[start - self._pos :]
            if stop is not None and self._pos > stop:
                chunk = chunk[: stop - self._pos]
                assert len(chunk) == stop - start
            return chunk
    else:
        raise StopIteration()
",if stop is not None and self . _pos > stop :,156
"def _SetUser(self, users):
    for user in users.items():
        username = user[0]
        settings = user[1]
        room = settings[""room""][""name""] if ""room"" in settings else None
        file_ = settings[""file""] if ""file"" in settings else None
        if ""event"" in settings:
            if ""joined"" in settings[""event""]:
                self._client.userlist.addUser(username, room, file_)
            elif ""left"" in settings[""event""]:
                self._client.removeUser(username)
        else:
            self._client.userlist.modUser(username, room, file_)
","elif ""left"" in settings [ ""event"" ] :",170
"def run_tests():
    # type: () -> None
    x = 5
    with switch(x) as case:
        if case(0):
            print(""zero"")
            print(""zero"")
        elif case(1, 2):
            print(""one or two"")
        elif case(3, 4):
            print(""three or four"")
        else:
            print(""default"")
            print(""another"")
","elif case ( 1 , 2 ) :",114
"def _populate():
    for fname in glob.glob(os.path.join(os.path.dirname(__file__), ""data"", ""*.json"")):
        with open(fname) as inf:
            data = json.load(inf)
            data = data[list(data.keys())[0]]
            data = data[list(data.keys())[0]]
            for item in data:
                if item[""key""] in TABLE:
                    LOGGER.warning(""Repeated emoji {}"".format(item[""key""]))
                else:
                    TABLE[item[""key""]] = item[""value""]
","if item [ ""key"" ] in TABLE :",157
"def slot_to_material(bobject: bpy.types.Object, slot: bpy.types.MaterialSlot):
    mat = slot.material
    # Pick up backed material if present
    if mat is not None:
        baked_mat = mat.name + ""_"" + bobject.name + ""_baked""
        if baked_mat in bpy.data.materials:
            mat = bpy.data.materials[baked_mat]
    return mat
",if baked_mat in bpy . data . materials :,111
"def __keyPress(self, widget, event):
    if event.key == ""G"" and event.modifiers & event.Modifiers.Control:
        if not all(hasattr(p, ""isGanged"") for p in self.getPlugs()):
            return False
        if all(p.isGanged() for p in self.getPlugs()):
            self.__ungang()
        else:
            self.__gang()
        return True
    return False
",if all ( p . isGanged ( ) for p in self . getPlugs ( ) ) :,123
"def check_expected(result, expected, contains=False):
    if sys.version_info[0] >= 3:
        if isinstance(result, str):
            result = result.encode(""ascii"")
        if isinstance(expected, str):
            expected = expected.encode(""ascii"")
    resultlines = result.splitlines()
    expectedlines = expected.splitlines()
    if len(resultlines) != len(expectedlines):
        return False
    for rline, eline in zip(resultlines, expectedlines):
        if contains:
            if eline not in rline:
                return False
        else:
            if not rline.endswith(eline):
                return False
    return True
","if isinstance ( expected , str ) :",181
"def hosts_to_domains(self, hosts, exclusions=[]):
    domains = []
    for host in hosts:
        elements = host.split(""."")
        # recursively walk through the elements
        # extracting all possible (sub)domains
        while len(elements) >= 2:
            # account for domains stored as hosts
            if len(elements) == 2:
                domain = ""."".join(elements)
            else:
                # drop the host element
                domain = ""."".join(elements[1:])
            if domain not in domains + exclusions:
                domains.append(domain)
            del elements[0]
    return domains
",if domain not in domains + exclusions :,167
"def hsconn_sender(self):
    while not self.stop_event.is_set():
        try:
            # Block, but timeout, so that we can exit the loop gracefully
            request = self.send_queue.get(True, 6.0)
            if self.socket is not None:
                # Socket got closed and set to None in another thread...
                self.socket.sendall(request)
            if self.send_queue is not None:
                self.send_queue.task_done()
        except queue.Empty:
            pass
        except OSError:
            self.stop_event.set()
",if self . socket is not None :,168
"def get_url_args(self, item):
    if self.url_args:
        if hasattr(self.url_args, ""__call__""):
            url_args = self.url_args(item)
        else:
            url_args = dict(self.url_args)
        url_args[""id""] = item.id
        return url_args
    else:
        return dict(operation=self.label, id=item.id)
","if hasattr ( self . url_args , ""__call__"" ) :",115
"def list_projects(self):
    projects = []
    page = 1
    while True:
        repos = self._client.get(
            ""/user/repos"", {""sort"": ""full_name"", ""page"": page, ""per_page"": 100}
        )
        page += 1
        for repo in repos:
            projects.append(
                {
                    ""id"": repo[""full_name""],
                    ""name"": repo[""full_name""],
                    ""description"": repo[""description""],
                    ""is_private"": repo[""private""],
                }
            )
        if len(repos) < 100:
            break
    return projects
",if len ( repos ) < 100 :,185
"def scripts(self):
    application_root = current_app.config.get(""APPLICATION_ROOT"")
    subdir = application_root != ""/""
    scripts = []
    for script in get_registered_scripts():
        if script.startswith(""http""):
            scripts.append(f'<script defer src=""{script}""></script>')
        elif subdir:
            scripts.append(f'<script defer src=""{application_root}/{script}""></script>')
        else:
            scripts.append(f'<script defer src=""{script}""></script>')
    return markup(""\n"".join(scripts))
",elif subdir :,146
"def print_map(node, l):
    if node.title not in l:
        l[node.title] = []
    for n in node.children:
        if len(n.children) > 0:
            w = {n.title: []}
            l[node.title].append(w)
            print_map(n, w)
        else:
            l[node.title].append(n.title)
",if len ( n . children ) > 0 :,112
"def _validate_distinct_on_different_types_and_field_orders(
    self, collection, query, expected_results, get_mock_result
):
    self.count = 0
    self.get_mock_result = get_mock_result
    query_iterable = collection.query_items(query, enable_cross_partition_query=True)
    results = list(query_iterable)
    for i in range(len(expected_results)):
        if isinstance(results[i], dict):
            self.assertDictEqual(results[i], expected_results[i])
        elif isinstance(results[i], list):
            self.assertListEqual(results[i], expected_results[i])
        else:
            self.assertEqual(results[i], expected_results[i])
    self.count = 0
","elif isinstance ( results [ i ] , list ) :",196
"def run(self):
    for k, v in iteritems(self.objs):
        if k.startswith(""_""):
            continue
        if (
            v[""_class""] == ""Question""
            or v[""_class""] == ""Message""
            or v[""_class""] == ""Announcement""
        ):
            v[""admin""] = None
    return self.objs
","if k . startswith ( ""_"" ) :",99
"def qvec(self):
    #        if self.polrep != 'stokes':
    #            raise Exception(""qvec is not defined unless self.polrep=='stokes'"")
    qvec = np.array([])
    if self.polrep == ""stokes"":
        qvec = self._imdict[""Q""]
    elif self.polrep == ""circ"":
        if len(self.rlvec) != 0 and len(self.lrvec) != 0:
            qvec = np.real(0.5 * (self.lrvec + self.rlvec))
    return qvec
",if len ( self . rlvec ) != 0 and len ( self . lrvec ) != 0 :,159
"def display_value(self, key, w):
    if key == ""vdevices"":
        # Very special case
        nids = [n[""deviceID""] for n in self.get_value(""devices"")]
        for device in self.app.devices.values():
            if device[""id""] != self.app.daemon.get_my_id():
                b = Gtk.CheckButton(device.get_title(), False)
                b.set_tooltip_text(device[""id""])
                self[""vdevices""].pack_start(b, False, False, 0)
                b.set_active(device[""id""] in nids)
        self[""vdevices""].show_all()
    else:
        EditorDialog.display_value(self, key, w)
","if device [ ""id"" ] != self . app . daemon . get_my_id ( ) :",197
"def _set_xflux_setting(self, **kwargs):
    for key, value in kwargs.items():
        if key in self._settings_map:
            if key == ""color"":
                self._set_xflux_screen_color(value)
                self._current_color = str(value)
                # hackish - changing the current color unpauses xflux,
                # must reflect that with state change
                if self.state == self.states[""PAUSED""]:
                    self.state = self.states[""RUNNING""]
            else:
                self._xflux.sendline(self._settings_map[key] + str(value))
            self._c()
","if key == ""color"" :",187
"def apply_acceleration(self, veh_ids, acc):
    """"""See parent class.""""""
    # to hand the case of a single vehicle
    if type(veh_ids) == str:
        veh_ids = [veh_ids]
        acc = [acc]
    for i, vid in enumerate(veh_ids):
        if acc[i] is not None and vid in self.get_ids():
            this_vel = self.get_speed(vid)
            next_vel = max([this_vel + acc[i] * self.sim_step, 0])
            self.kernel_api.vehicle.slowDown(vid, next_vel, 1e-3)
",if acc [ i ] is not None and vid in self . get_ids ( ) :,172
"def largest_factor_relatively_prime(a, b):
    """"""Return the largest factor of a relatively prime to b.""""""
    while 1:
        d = gcd(a, b)
        if d <= 1:
            break
        b = d
        while 1:
            q, r = divmod(a, d)
            if r > 0:
                break
            a = q
    return a
",if d <= 1 :,111
"def check_status(self):
    try:
        du = psutil.disk_usage(""/"")
        if DISK_USAGE_MAX and du.percent >= DISK_USAGE_MAX:
            raise ServiceWarning(
                ""{host} {percent}% disk usage exceeds {disk_usage}%"".format(
                    host=host, percent=du.percent, disk_usage=DISK_USAGE_MAX
                )
            )
    except ValueError as e:
        self.add_error(ServiceReturnedUnexpectedResult(""ValueError""), e)
",if DISK_USAGE_MAX and du . percent >= DISK_USAGE_MAX :,136
"def build_reply(self, msg, text=None, private=False, threaded=False):
    response = self.build_message(text)
    if msg.is_group:
        if private:
            response.frm = self.bot_identifier
            response.to = IRCPerson(str(msg.frm))
        else:
            response.frm = IRCRoomOccupant(str(self.bot_identifier), msg.frm.room)
            response.to = msg.frm.room
    else:
        response.frm = self.bot_identifier
        response.to = msg.frm
    return response
",if private :,159
"def _dict_refs(obj, named):
    """"""Return key and value objects of a dict/proxy.""""""
    try:
        if named:
            for k, v in _items(obj):
                s = str(k)
                yield _NamedRef(""[K] "" + s, k)
                yield _NamedRef(""[V] "" + s + "": "" + _repr(v), v)
        else:
            for k, v in _items(obj):
                yield k
                yield v
    except (KeyError, ReferenceError, TypeError) as x:
        warnings.warn(""Iterating '%s': %r"" % (_classof(obj), x))
",if named :,172
"def fetch_images():
    images = []
    marker = None
    while True:
        batch = image_service.detail(
            context,
            filters=filters,
            marker=marker,
            sort_key=""created_at"",
            sort_dir=""desc"",
        )
        if not batch:
            break
        images += batch
        marker = batch[-1][""id""]
    return images
",if not batch :,113
"def compress(self, data_list):
    warn_untested()
    if data_list:
        if data_list[1] in forms.fields.EMPTY_VALUES:
            error = self.error_messages[""invalid_year""]
            raise forms.ValidationError(error)
        if data_list[0] in forms.fields.EMPTY_VALUES:
            error = self.error_messages[""invalid_month""]
            raise forms.ValidationError(error)
        year = int(data_list[1])
        month = int(data_list[0])
        # find last day of the month
        day = monthrange(year, month)[1]
        return date(year, month, day)
    return None
",if data_list [ 0 ] in forms . fields . EMPTY_VALUES :,181
"def _diff_dict(self, old, new):
    diff = {}
    removed = []
    added = []
    for key, value in old.items():
        if key not in new:
            removed.append(key)
        elif old[key] != new[key]:
            # modified is indicated by a remove and add
            removed.append(key)
            added.append(key)
    for key, value in new.items():
        if key not in old:
            added.append(key)
    if removed:
        diff[""removed""] = sorted(removed)
    if added:
        diff[""added""] = sorted(added)
    return diff
",elif old [ key ] != new [ key ] :,172
"def add_filters(self, function):
    try:
        subscription = self.exists(function)
        if subscription:
            response = self._sns.call(
                ""set_subscription_attributes"",
                SubscriptionArn=subscription[""SubscriptionArn""],
                AttributeName=""FilterPolicy"",
                AttributeValue=json.dumps(self.filters),
            )
            kappa.event_source.sns.LOG.debug(response)
    except Exception:
        kappa.event_source.sns.LOG.exception(
            ""Unable to add filters for SNS topic %s"", self.arn
        )
",if subscription :,165
"def init_weights(self, pretrained=None):
    if isinstance(pretrained, str):
        logger = logging.getLogger()
        load_checkpoint(self, pretrained, strict=False, logger=logger)
    elif pretrained is None:
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                kaiming_init(m)
            elif isinstance(m, (_BatchNorm, nn.GroupNorm)):
                constant_init(m, 1)
    else:
        raise TypeError(""pretrained must be a str or None"")
","elif isinstance ( m , ( _BatchNorm , nn . GroupNorm ) ) :",141
"def test_is_native_login(self):
    for campaign in self.campaign_lists:
        native = campaigns.is_native_login(campaign)
        if campaign == ""prereg"" or campaign == ""erpc"":
            assert_true(native)
        else:
            assert_false(native)
    native = campaigns.is_proxy_login(self.invalid_campaign)
    assert_true(native is None)
","if campaign == ""prereg"" or campaign == ""erpc"" :",132
"def _process_filter(self, query, host_state):
    """"""Recursively parse the query structure.""""""
    if not query:
        return True
    cmd = query[0]
    method = self.commands[cmd]
    cooked_args = []
    for arg in query[1:]:
        if isinstance(arg, list):
            arg = self._process_filter(arg, host_state)
        elif isinstance(arg, basestring):
            arg = self._parse_string(arg, host_state)
        if arg is not None:
            cooked_args.append(arg)
    result = method(self, cooked_args)
    return result
","elif isinstance ( arg , basestring ) :",163
"def find_go_files_mtime(app_files):
    files, mtime = [], 0
    for f, mt in app_files.items():
        if not f.endswith("".go""):
            continue
        if APP_CONFIG.nobuild_files.match(f):
            continue
        files.append(f)
        mtime = max(mtime, mt)
    return files, mtime
","if not f . endswith ( "".go"" ) :",100
"def ExcludePath(self, path):
    """"""Check to see if this is a service url and matches inbound_services.""""""
    skip = False
    for reserved_path in self.reserved_paths.keys():
        if path.startswith(reserved_path):
            if (
                not self.inbound_services
                or self.reserved_paths[reserved_path] not in self.inbound_services
            ):
                return (True, self.reserved_paths[reserved_path])
    return (False, None)
",if path . startswith ( reserved_path ) :,132
"def param_cov(self) -> DataFrame:
    """"""Parameter covariance""""""
    if self._param_cov is not None:
        param_cov = self._param_cov
    else:
        params = np.asarray(self.params)
        if self.cov_type == ""robust"":
            param_cov = self.model.compute_param_cov(params)
        else:
            param_cov = self.model.compute_param_cov(params, robust=False)
    return DataFrame(param_cov, columns=self._names, index=self._names)
","if self . cov_type == ""robust"" :",141
"def test_calculate_all_attentions(module, atype):
    m = importlib.import_module(module)
    args = make_arg(atype=atype)
    if ""pytorch"" in module:
        batch = prepare_inputs(""pytorch"")
    else:
        raise NotImplementedError
    model = m.E2E(6, 5, args)
    with chainer.no_backprop_mode():
        if ""pytorch"" in module:
            att_ws = model.calculate_all_attentions(*batch)[0]
        else:
            raise NotImplementedError
        print(att_ws.shape)
","if ""pytorch"" in module :",149
"def __eq__(self, other):
    try:
        if self.type != other.type:
            return False
        if self.type == ""ASK"":
            return self.askAnswer == other.askAnswer
        elif self.type == ""SELECT"":
            return self.vars == other.vars and self.bindings == other.bindings
        else:
            return self.graph == other.graph
    except:
        return False
","if self . type == ""ASK"" :",116
"def validate_memory(self, value):
    for k, v in value.viewitems():
        if v is None:  # use NoneType to unset a value
            continue
        if not re.match(PROCTYPE_MATCH, k):
            raise serializers.ValidationError(""Process types can only contain [a-z]"")
        if not re.match(MEMLIMIT_MATCH, str(v)):
            raise serializers.ValidationError(
                ""Limit format: <number><unit>, where unit = B, K, M or G""
            )
    return value
","if not re . match ( MEMLIMIT_MATCH , str ( v ) ) :",141
"def get_connections(data_about):
    data = data_about.find(""h3"", text=""Connections"").findNext()
    connections = {}
    for row in data.find_all(""tr""):
        key = row.find_all(""td"")[0].text
        value = row.find_all(""td"")[1]
        if ""Teams"" in key:
            connections[key] = get_all_links(value)
        else:
            connections[key] = value.text
    return connections
","if ""Teams"" in key :",129
"def _compute_map(self, first_byte, second_byte=None):
    if first_byte != 0x0F:
        return ""XED_ILD_MAP0""
    else:
        if second_byte == None:
            return ""XED_ILD_MAP1""
        if second_byte == 0x38:
            return ""XED_ILD_MAP2""
        if second_byte == 0x3A:
            return ""XED_ILD_MAP3""
        if second_byte == 0x0F and self.amd_enabled:
            return ""XED_ILD_MAPAMD""
    die(""Unhandled escape {} / map {} bytes"".format(first_byte, second_byte))
",if second_byte == 0x0F and self . amd_enabled :,181
"def compress(self, data_list):
    if data_list:
        page_id = data_list[1]
        if page_id in EMPTY_VALUES:
            if not self.required:
                return None
            raise forms.ValidationError(self.error_messages[""invalid_page""])
        return Page.objects.get(pk=page_id)
    return None
",if page_id in EMPTY_VALUES :,98
"def find_module(self, fullname, path=None):
    path = path or self.path_entry
    # print('looking for ""%s"" in %s ...' % (fullname, path))
    for _ext in [""js"", ""pyj"", ""py""]:
        _filepath = os.path.join(self.path_entry, ""%s.%s"" % (fullname, _ext))
        if _filepath in VFS:
            print(""module found at %s:%s"" % (_filepath, fullname))
            return VFSModuleLoader(_filepath, fullname)
    print(""module %s not found"" % fullname)
    raise ImportError()
    return None
",if _filepath in VFS :,158
"def __decToBin(self, myDec):
    n = 0
    binOfDec = """"
    while myDec > 2 ** n:
        n = n + 1
    if (myDec < 2 ** n) & (myDec != 0):
        n = n - 1
    while n >= 0:
        if myDec >= 2 ** n:
            myDec = myDec - 2 ** n
            binOfDec = binOfDec + ""1""
        else:
            binOfDec = binOfDec + ""0""
        n = n - 1
    return binOfDec
",if myDec >= 2 ** n :,148
"def __str__(self):
    try:
        if self.value not in NVMLError._errcode_to_string:
            NVMLError._errcode_to_string[self.value] = str(nvmlErrorString(self.value))
        return NVMLError._errcode_to_string[self.value]
    except NVMLError_Uninitialized:
        return ""NVML Error with code %d"" % self.value
",if self . value not in NVMLError . _errcode_to_string :,101
"def abspath(pathdir: str) -> str:
    if Path is not None and isinstance(pathdir, Path):
        return pathdir.abspath()
    else:
        pathdir = path.abspath(pathdir)
        if isinstance(pathdir, bytes):
            try:
                pathdir = pathdir.decode(fs_encoding)
            except UnicodeDecodeError as exc:
                raise UnicodeDecodeError(
                    ""multibyte filename not supported on ""
                    ""this filesystem encoding ""
                    ""(%r)"" % fs_encoding
                ) from exc
        return pathdir
","if isinstance ( pathdir , bytes ) :",156
"def _get_vtkjs(self):
    if self._vtkjs is None and self.object is not None:
        if isinstance(self.object, string_types) and self.object.endswith("".vtkjs""):
            if isfile(self.object):
                with open(self.object, ""rb"") as f:
                    vtkjs = f.read()
            else:
                data_url = urlopen(self.object)
                vtkjs = data_url.read()
        elif hasattr(self.object, ""read""):
            vtkjs = self.object.read()
        self._vtkjs = vtkjs
    return self._vtkjs
","if isinstance ( self . object , string_types ) and self . object . endswith ( "".vtkjs"" ) :",180
"def _set_uid(self, val):
    if val is not None:
        if pwd is None:
            self.bus.log(""pwd module not available; ignoring uid."", level=30)
            val = None
        elif isinstance(val, text_or_bytes):
            val = pwd.getpwnam(val)[2]
    self._uid = val
","elif isinstance ( val , text_or_bytes ) :",92
"def get_attached_nodes(self, external_account):
    for node in self.get_nodes_with_oauth_grants(external_account):
        if node is None:
            continue
        node_settings = node.get_addon(self.oauth_provider.short_name)
        if node_settings is None:
            continue
        if node_settings.external_account == external_account:
            yield node
",if node is None :,110
"def from_obj(cls, py_obj):
    if not isinstance(py_obj, Image):
        raise TypeError(""py_obj must be a wandb.Image"")
    else:
        if hasattr(py_obj, ""_boxes"") and py_obj._boxes:
            box_keys = list(py_obj._boxes.keys())
        else:
            box_keys = []
        if hasattr(py_obj, ""masks"") and py_obj.masks:
            mask_keys = list(py_obj.masks.keys())
        else:
            mask_keys = []
        return cls(box_keys, mask_keys)
","if hasattr ( py_obj , ""masks"" ) and py_obj . masks :",164
"def write(self, *bits):
    for bit in bits:
        if not self.bytestream:
            self.bytestream.append(0)
        byte = self.bytestream[self.bytenum]
        if self.bitnum == 8:
            if self.bytenum == len(self.bytestream) - 1:
                byte = 0
                self.bytestream += bytes([byte])
            self.bytenum += 1
            self.bitnum = 0
        mask = 2 ** self.bitnum
        if bit:
            byte |= mask
        else:
            byte &= ~mask
        self.bytestream[self.bytenum] = byte
        self.bitnum += 1
",if self . bytenum == len ( self . bytestream ) - 1 :,186
"def destroy(self, wipe=False):
    if self.state == self.UP:
        image = self.image()
        if image:
            return self.confirm_destroy(image, self.full_name, abort=False)
        else:
            self.warn(""tried to destroy {0} which didn't exist"".format(self.full_name))
    return True
",if image :,95
"def get_host_metadata(self):
    meta = {}
    if self.agent_url:
        try:
            resp = requests.get(
                self.agent_url + ECS_AGENT_METADATA_PATH, timeout=1
            ).json()
            if ""Version"" in resp:
                match = AGENT_VERSION_EXP.search(resp.get(""Version""))
                if match is not None and len(match.groups()) == 1:
                    meta[""ecs_version""] = match.group(1)
        except Exception as e:
            self.log.debug(""Error getting ECS version: %s"" % str(e))
    return meta
","if ""Version"" in resp :",176
"def _path_type(st, lst):
    parts = []
    if st:
        if stat.S_ISREG(st.st_mode):
            parts.append(""file"")
        elif stat.S_ISDIR(st.st_mode):
            parts.append(""dir"")
        else:
            parts.append(""other"")
    if lst:
        if stat.S_ISLNK(lst.st_mode):
            parts.append(""link"")
    return "" "".join(parts)
",if stat . S_ISREG ( st . st_mode ) :,130
"def changed(self, action):
    # Something was changed in the 'files' list
    if len(action.key) >= 1 and action.key[0].lower() == ""files"":
        # Refresh project files model
        if action.type == ""insert"":
            # Don't clear the existing items if only inserting new things
            self.update_model(clear=False)
        else:
            # Clear existing items
            self.update_model(clear=True)
","if action . type == ""insert"" :",120
"def process(self, resources, event=None):
    client = local_session(self.manager.session_factory).client(""es"")
    for r in resources:
        if self.policy_attribute not in r:
            result = self.manager.retry(
                client.describe_elasticsearch_domain_config,
                DomainName=r[""DomainName""],
                ignore_err_codes=(""ResourceNotFoundException"",),
            )
            if result:
                r[self.policy_attribute] = json.loads(
                    result.get(""DomainConfig"").get(""AccessPolicies"").get(""Options"")
                )
    return super().process(resources)
",if self . policy_attribute not in r :,175
"def line_items(self):
    line_items = []
    for line in self.lines_str:
        line = line.split(""|"")
        line = line[1:-1]  # del first and last empty item (consequence of split)
        items = []
        for item in line:
            i = re.search(r""(\S+([ \t]+\S+)*)+"", item)
            if i:
                items.append(i.group())
            else:
                items.append("" "")
        line_items.append(items)
    return line_items
",if i :,151
"def on_data(res):
    if terminate.is_set():
        return
    if args.strings and not args.no_content:
        if type(res) == tuple:
            f, v = res
            if type(f) == unicode:
                f = f.encode(""utf-8"")
            if type(v) == unicode:
                v = v.encode(""utf-8"")
            self.success(""{}: {}"".format(f, v))
        elif not args.content_only:
            self.success(res)
    else:
        self.success(res)
",elif not args . content_only :,158
"def get_servers(self, detail=True, search_opts=None):
    rel_url = ""/servers/detail"" if detail else ""/servers""
    if search_opts is not None:
        qparams = {}
        for opt, val in search_opts.iteritems():
            qparams[opt] = val
        if qparams:
            query_string = ""?%s"" % urllib.urlencode(qparams)
            rel_url += query_string
    return self.api_get(rel_url)[""servers""]
",if qparams :,130
"def run(self):
    while not self.__exit__:
        if len(self.playlist) == 0:
            sleep(10)
            continue
        o = self.playlist[0]
        self.playlist.remove(o)
        obj = json.loads(o)
        if not ""args"" in obj:
            obj[""args""] = {""ua"": """", ""header"": """", ""title"": """", ""referer"": """"}
        obj[""play""] = False
        self.handle = launch_player(obj[""urls""], obj[""ext""], **obj[""args""])
        self.handle.wait()
",if len ( self . playlist ) == 0 :,151
"def get_to_download_runs_ids(session, headers):
    last_date = 0
    result = []
    while 1:
        r = session.get(RUN_DATA_API.format(last_date=last_date), headers=headers)
        if r.ok:
            run_logs = r.json()[""data""][""records""]
            result.extend([i[""logs""][0][""stats""][""id""] for i in run_logs])
            last_date = r.json()[""data""][""lastTimestamp""]
            since_time = datetime.utcfromtimestamp(last_date / 1000)
            print(f""pares keep ids data since {since_time}"")
            time.sleep(1)  # spider rule
            if not last_date:
                break
    return result
",if r . ok :,199
"def __saveWork(self, work, results):
    """"""Stores the resulting last log line to the cache with the proxy key""""""
    del work
    # pylint: disable=broad-except
    try:
        if results:
            __cached = self.__cache[results[0]]
            __cached[self.__TIME] = time.time()
            __cached[self.__LINE] = results[1]
            __cached[self.__LLU] = results[2]
    except KeyError as e:
        # Could happen while switching jobs with work in the queue
        pass
    except Exception as e:
        list(map(logger.warning, cuegui.Utils.exceptionOutput(e)))
",if results :,170
"def read_notes(rec):
    found = []
    for tag in range(500, 595):
        if tag in (505, 520):
            continue
        fields = rec.get_fields(str(tag))
        if not fields:
            continue
        for f in fields:
            x = f.get_lower_subfields()
            if x:
                found.append("" "".join(x).strip("" ""))
    if found:
        return ""\n\n"".join(found)
",if not fields :,134
"def serialize_to(self, stream, alternate_script=None):
    stream.write(self.txo_ref.tx_ref.hash)
    stream.write_uint32(self.txo_ref.position)
    if alternate_script is not None:
        stream.write_string(alternate_script)
    else:
        if self.is_coinbase:
            stream.write_string(self.coinbase)
        else:
            stream.write_string(self.script.source)
    stream.write_uint32(self.sequence)
",if self . is_coinbase :,142
"def func_named(self, arg):
    result = None
    target = ""do_"" + arg
    if target in dir(self):
        result = target
    else:
        if self.abbrev:  # accept shortened versions of commands
            funcs = [fname for fname in self.keywords if fname.startswith(arg)]
            if len(funcs) == 1:
                result = ""do_"" + funcs[0]
    return result
",if self . abbrev :,110
"def static_login(self, token, *, bot):
    # Necessary to get aiohttp to stop complaining about session creation
    self.__session = aiohttp.ClientSession(
        connector=self.connector, ws_response_class=DiscordClientWebSocketResponse
    )
    old_token, old_bot = self.token, self.bot_token
    self._token(token, bot=bot)
    try:
        data = await self.request(Route(""GET"", ""/users/@me""))
    except HTTPException as exc:
        self._token(old_token, bot=old_bot)
        if exc.response.status == 401:
            raise LoginFailure(""Improper token has been passed."") from exc
        raise
    return data
",if exc . response . status == 401 :,179
"def render_buttons(self):
    for x, button in enumerate(self.button_list):
        gcolor = Gdk.color_parse(self.color_list[x])
        if util.get_hls_val(self.color_list[x], ""light"") < 99:
            fgcolor = Gdk.color_parse(""#FFFFFF"")
        else:
            fgcolor = Gdk.color_parse(""#000000"")
        button.set_label(self.color_list[x])
        button.set_sensitive(True)
        button.modify_bg(Gtk.StateType.NORMAL, gcolor)
        button.modify_fg(Gtk.StateType.NORMAL, fgcolor)
","if util . get_hls_val ( self . color_list [ x ] , ""light"" ) < 99 :",170
"def _set_text(self, data):
    lines = []
    for key, value in data.items():
        lines.append("""")
        txt = yaml.dump({key: value}, default_flow_style=False)
        title = self.titles.get(key)
        if title:
            lines.append(""# %s"" % title)
        lines.append(txt.rstrip())
    txt = ""\n"".join(lines) + ""\n""
    txt = txt.lstrip()
    self.edit.setPlainText(txt)
",if title :,134
"def build_path(self):
    for variable in re_path_template.findall(self.path):
        name = variable.strip(""{}"")
        if name == ""user"" and ""user"" not in self.session.params and self.api.auth:
            # No 'user' parameter provided, fetch it from Auth instead.
            value = self.api.auth.get_username()
        else:
            try:
                value = quote(self.session.params[name])
            except KeyError:
                raise TweepError(
                    ""No parameter value found for path variable: %s"" % name
                )
            del self.session.params[name]
        self.path = self.path.replace(variable, value)
","if name == ""user"" and ""user"" not in self . session . params and self . api . auth :",197
"def _calculate_writes_for_built_in_indices(self, entity):
    writes = 0
    for prop_name in entity.keys():
        if not prop_name in entity.unindexed_properties():
            prop_vals = entity[prop_name]
            if isinstance(prop_vals, (list)):
                num_prop_vals = len(prop_vals)
            else:
                num_prop_vals = 1
            writes += 2 * num_prop_vals
    return writes
",if not prop_name in entity . unindexed_properties ( ) :,131
"def create_connection(self, address, protocol_factory=None, **kw):
    """"""Helper method for creating a connection to an ``address``.""""""
    protocol_factory = protocol_factory or self.create_protocol
    if isinstance(address, tuple):
        host, port = address
        if self.debug:
            self.logger.debug(""Create connection %s:%s"", host, port)
        _, protocol = await self._loop.create_connection(
            protocol_factory, host, port, **kw
        )
        await protocol.event(""connection_made"")
    else:
        raise NotImplementedError(""Could not connect to %s"" % str(address))
    return protocol
",if self . debug :,165
"def _increment_bracket_num(self):
    self._current_bracket -= 1
    if self._current_bracket < 0:
        self._current_bracket = self._get_num_brackets() - 1
        self._current_iteration += 1
        if self._current_iteration > self.hyperband_iterations:
            self._current_bracket = 0
",if self . _current_iteration > self . hyperband_iterations :,88
"def get_cycle_path(self, curr_node, goal_node_index):
    for dep in curr_node[""deps""]:
        if dep == goal_node_index:
            return [curr_node[""address""]]
    for dep in curr_node[""deps""]:
        path = self.get_cycle_path(
            self.get_by_address(dep), goal_node_index
        )  # self.nodelist[dep], goal_node_index)
        if len(path) > 0:
            path.insert(0, curr_node[""address""])
            return path
    return []
",if dep == goal_node_index :,153
"def as_dict(path="""", version=""latest"", section=""meta-data""):
    result = {}
    dirs = dir(path, version, section)
    if not dirs:
        return None
    for item in dirs:
        if item.endswith(""/""):
            records = as_dict(path + item, version, section)
            if records:
                result[item[:-1]] = records
        elif is_dict.match(item):
            idx, name = is_dict.match(item).groups()
            records = as_dict(path + idx + ""/"", version, section)
            if records:
                result[name] = records
        else:
            result[item] = valueconv(get(path + item, version, section))
    return result
",if records :,197
"def preprocess_raw_enwik9(input_filename, output_filename):
    with open(input_filename, ""r"") as f1:
        with open(output_filename, ""w"") as f2:
            while True:
                line = f1.readline()
                if not line:
                    break
                line = list(enwik9_norm_transform([line]))[0]
                if line != "" "" and line != """":
                    if line[0] == "" "":
                        line = line[1:]
                    f2.writelines(line + ""\n"")
","if line [ 0 ] == "" "" :",164
"def _handle_unsubscribe(self, web_sock):
    index = None
    with await self._subscriber_lock:
        for i, (subscriber_web_sock, _) in enumerate(self._subscribers):
            if subscriber_web_sock == web_sock:
                index = i
                break
        if index is not None:
            del self._subscribers[index]
        if not self._subscribers:
            asyncio.ensure_future(self._unregister_subscriptions())
",if subscriber_web_sock == web_sock :,124
"def formatmonthname(self, theyear, themonth, withyear=True):
    with TimeEncoding(self.locale) as encoding:
        s = month_name[themonth]
        if encoding is not None:
            s = s.decode(encoding)
        if withyear:
            s = ""%s %s"" % (s, theyear)
        return '<tr><th colspan=""7"" class=""month"">%s</th></tr>' % s
",if withyear :,115
"def generate_sitemaps(filename):
    rows = (line.strip().split(""\t"") for line in open(filename))
    for sortkey, chunk in itertools.groupby(rows, lambda row: row[0]):
        things = []
        _chunk = list(chunk)
        for segment in _chunk:
            sortkey = segment.pop(0)
            last_modified = segment.pop(-1)
            path = """".join(segment)
            things.append(web.storage(path=path, last_modified=last_modified))
        if things:
            write(""sitemaps/sitemap_%s.xml.gz"" % sortkey, sitemap(things))
",if things :,165
"def use_index(
    self, term: Union[str, Index], *terms: Union[str, Index]
) -> ""QueryBuilder"":
    for t in (term, *terms):
        if isinstance(t, Index):
            self._use_indexes.append(t)
        elif isinstance(t, str):
            self._use_indexes.append(Index(t))
","if isinstance ( t , Index ) :",94
"def get_changed(self):
    if self._is_expression():
        result = self._get_node_text(self.ast)
        if result == self.source:
            return None
        return result
    else:
        collector = codeanalyze.ChangeCollector(self.source)
        last_end = -1
        for match in self.matches:
            start, end = match.get_region()
            if start < last_end:
                if not self._is_expression():
                    continue
            last_end = end
            replacement = self._get_matched_text(match)
            collector.add_change(start, end, replacement)
        return collector.get_changed()
",if result == self . source :,189
"def quiet_f(*args):
    vars = {arg_name: Real(arg) for arg_name, arg in zip(arg_names, args)}
    value = dynamic_scoping(quiet_expr.evaluate, vars, evaluation)
    if expect_list:
        if value.has_form(""List"", None):
            value = [extract_pyreal(item) for item in value.leaves]
            if any(item is None for item in value):
                return None
            return value
        else:
            return None
    else:
        value = extract_pyreal(value)
        if value is None or isinf(value) or isnan(value):
            return None
        return value
",if any ( item is None for item in value ) :,177
"def _reemit_nested_event(self, event: Event):
    source_index = self.index(event.source)
    for attr in (""index"", ""new_index""):
        if hasattr(event, attr):
            src_index = ensure_tuple_index(event.index)
            setattr(event, attr, (source_index,) + src_index)
    if not hasattr(event, ""index""):
        setattr(event, ""index"", source_index)
    # reemit with this object's EventEmitter of the same type if present
    # otherwise just emit with the EmitterGroup itself
    getattr(self.events, event.type, self.events)(event)
","if hasattr ( event , attr ) :",163
"def check(self):
    """"""Perform required checks to conclude if it's safe to operate""""""
    if self.interpreter.manual is None:
        if not self.process.healthy:
            self.error = self.process.error
            self.tip = self.process.tip
            return False
    start = time.time()
    while not self._status():
        if time.time() - start >= 2:  # 2s
            self.error = ""can't connect to the minserver on {}:{}"".format(
                self.interpreter.host, self.interpreter.port
            )
            self.tip = ""check your vagrant machine is running""
            return False
        time.sleep(0.1)
    return True
",if not self . process . healthy :,189
"def apply(self):
    new_block = self.block.copy()
    new_block.clear()
    for inst in self.block.body:
        if isinstance(inst, Assign) and inst.value in self.getattrs:
            const_assign = self._assign_const(inst)
            new_block.append(const_assign)
            inst = self._assign_getitem(inst, index=const_assign.target)
        new_block.append(inst)
    return new_block
","if isinstance ( inst , Assign ) and inst . value in self . getattrs :",126
"def _get_orientation(self):
    if self.state:
        rotation = [0] * 9
        inclination = [0] * 9
        gravity = []
        geomagnetic = []
        gravity = self.listener_a.values
        geomagnetic = self.listener_m.values
        if gravity[0] is not None and geomagnetic[0] is not None:
            ff_state = SensorManager.getRotationMatrix(
                rotation, inclination, gravity, geomagnetic
            )
            if ff_state:
                values = [0, 0, 0]
                values = SensorManager.getOrientation(rotation, values)
            return values
",if gravity [ 0 ] is not None and geomagnetic [ 0 ] is not None :,187
"def getFirstSubGraph(graph):
    if len(graph) == 0:
        return None
    subg = {}
    todo = [graph.keys()[0]]
    while len(todo) > 0:
        if todo[0] in graph.keys():
            subg[todo[0]] = graph[todo[0]]
            todo.extend(graph[todo[0]])
            del graph[todo[0]]
        del todo[0]
    return subg
",if todo [ 0 ] in graph . keys ( ) :,120
"def decorated_function(*args, **kwargs):
    rv = f(*args, **kwargs)
    if ""Last-Modified"" not in rv.headers:
        try:
            result = date
            if callable(result):
                result = result(rv)
            if not isinstance(result, basestring):
                from werkzeug.http import http_date
                result = http_date(result)
            if result:
                rv.headers[""Last-Modified""] = result
        except Exception:
            logging.getLogger(__name__).exception(
                ""Error while calculating the lastmodified value for response {!r}"".format(
                    rv
                )
            )
    return rv
","if not isinstance ( result , basestring ) :",189
"def set_invoice_details(self, row):
    invoice_details = self.invoice_details.get(row.voucher_no, {})
    if row.due_date:
        invoice_details.pop(""due_date"", None)
    row.update(invoice_details)
    if row.voucher_type == ""Sales Invoice"":
        if self.filters.show_delivery_notes:
            self.set_delivery_notes(row)
        if self.filters.show_sales_person and row.sales_team:
            row.sales_person = "", "".join(row.sales_team)
            del row[""sales_team""]
",if self . filters . show_delivery_notes :,160
"def process(output):
    modules = {}
    for line in output:
        name, size, instances, depends, state, _ = line.split("" "", 5)
        instances = int(instances)
        module = {
            ""size"": size,
            ""instances"": instances,
            ""state"": state,
        }
        if depends != ""-"":
            module[""depends""] = [value for value in depends.split("","") if value]
        modules[name] = module
    return modules
","if depends != ""-"" :",127
"def _get_host_from_zc_service_info(service_info: zeroconf.ServiceInfo):
    """"""Get hostname or IP + port from zeroconf service_info.""""""
    host = None
    port = None
    if (
        service_info
        and service_info.port
        and (service_info.server or len(service_info.addresses) > 0)
    ):
        if len(service_info.addresses) > 0:
            host = socket.inet_ntoa(service_info.addresses[0])
        else:
            host = service_info.server.lower()
        port = service_info.port
    return (host, port)
",if len ( service_info . addresses ) > 0 :,170
"def _init_weights(self, module):
    if isinstance(module, nn.Linear):
        module.weight.data.normal_(mean=0.0, std=self.config.init_std)
        if module.bias is not None:
            module.bias.data.zero_()
    elif isinstance(module, nn.Embedding):
        module.weight.data.normal_(mean=0.0, std=self.config.init_std)
        if module.padding_idx is not None:
            module.weight.data[module.padding_idx].zero_()
",if module . padding_idx is not None :,141
"def visitFromImport(self, import_stmt, import_info):
    new_pairs = []
    if not import_info.is_star_import():
        for name, alias in import_info.names_and_aliases:
            try:
                pyname = self.pymodule[alias or name]
                if occurrences.same_pyname(self.pyname, pyname):
                    continue
            except exceptions.AttributeNotFoundError:
                pass
            new_pairs.append((name, alias))
    return importinfo.FromImport(import_info.module_name, import_info.level, new_pairs)
","if occurrences . same_pyname ( self . pyname , pyname ) :",162
"def _apply_patches(self):
    try:
        s = Subprocess(
            log=self.logfile, cwd=self.build_dir, verbose=self.options.verbose
        )
        for patch in self.patches:
            if type(patch) is dict:
                for ed, source in patch.items():
                    s.shell(""ed - %s < %s"" % (source, ed))
            else:
                s.shell(""patch -p0 < %s"" % patch)
    except:
        logger.error(""Failed to patch `%s`.\n%s"" % (self.build_dir, sys.exc_info()[1]))
        sys.exit(1)
",if type ( patch ) is dict :,183
"def __init__(self, parent, dir, mask, with_dirs=True):
    filelist = []
    dirlist = [""..""]
    self.dir = dir
    self.file = """"
    mask = mask.upper()
    pattern = self.MakeRegex(mask)
    for i in os.listdir(dir):
        if i == ""."" or i == "".."":
            continue
        path = os.path.join(dir, i)
        if os.path.isdir(path):
            dirlist.append(i)
            continue
        path = path.upper()
        value = i.upper()
        if pattern.match(value) is not None:
            filelist.append(i)
    self.files = filelist
    if with_dirs:
        self.dirs = dirlist
",if os . path . isdir ( path ) :,199
"def remove_invalid_dirs(paths, bp_dir, module_name):
    ret = []
    for path in paths:
        if os.path.isdir(os.path.join(bp_dir, path)):
            ret.append(path)
        else:
            logging.warning('Dir ""%s"" of module ""%s"" does not exist', path, module_name)
    return ret
","if os . path . isdir ( os . path . join ( bp_dir , path ) ) :",98
"def update_sockets(self):
    inputs = self.inputs
    inputs_n = ""ABabcd""
    penta_sockets = pentagon_dict[self.grid_type].input_sockets
    for socket in inputs_n:
        if socket in penta_sockets:
            if inputs[socket].hide_safe:
                inputs[socket].hide_safe = False
        else:
            inputs[socket].hide_safe = True
",if inputs [ socket ] . hide_safe :,113
"def __cut(sentence):
    global emit_P
    prob, pos_list = viterbi(sentence, ""BMES"", start_P, trans_P, emit_P)
    begin, nexti = 0, 0
    # print pos_list, sentence
    for i, char in enumerate(sentence):
        pos = pos_list[i]
        if pos == ""B"":
            begin = i
        elif pos == ""E"":
            yield sentence[begin : i + 1]
            nexti = i + 1
        elif pos == ""S"":
            yield char
            nexti = i + 1
    if nexti < len(sentence):
        yield sentence[nexti:]
","elif pos == ""S"" :",174
"def validate(self):
    if self.data.get(""encrypted"", True):
        key = self.data.get(""target_key"")
        if not key:
            raise PolicyValidationError(
                ""Encrypted snapshot copy requires kms key on %s"" % (self.manager.data,)
            )
    return self
",if not key :,82
"def __init__(self, patch_files, patch_directories):
    files = []
    files_data = {}
    for filename_data in patch_files:
        if isinstance(filename_data, list):
            filename, data = filename_data
        else:
            filename = filename_data
            data = None
        if not filename.startswith(os.sep):
            filename = ""{0}{1}"".format(FakeState.deploy_dir, filename)
        files.append(filename)
        if data:
            files_data[filename] = data
    self.files = files
    self.files_data = files_data
    self.directories = patch_directories
",if not filename . startswith ( os . sep ) :,171
"def validate_name_and_description(body, check_length=True):
    for attribute in [""name"", ""description"", ""display_name"", ""display_description""]:
        value = body.get(attribute)
        if value is not None:
            if isinstance(value, six.string_types):
                body[attribute] = value.strip()
            if check_length:
                try:
                    utils.check_string_length(
                        body[attribute], attribute, min_length=0, max_length=255
                    )
                except exception.InvalidInput as error:
                    raise webob.exc.HTTPBadRequest(explanation=error.msg)
",if check_length :,184
"def pick(items, sel):
    for x, s in zip(items, sel):
        if match(s):
            yield x
        elif not x.is_atom() and not s.is_atom():
            yield x.restructure(x.head, pick(x.leaves, s.leaves), evaluation)
",elif not x . is_atom ( ) and not s . is_atom ( ) :,79
"def wait_or_kill(self):
    """"""Wait for the program to terminate, or kill it after 5s.""""""
    if self.instance.poll() is None:
        # We try one more time to kill gracefully using Ctrl-C.
        logger.info(""Interrupting %s and waiting..."", self.coord)
        self.instance.send_signal(signal.SIGINT)
        # FIXME on py3 this becomes self.instance.wait(timeout=5)
        t = monotonic_time()
        while monotonic_time() - t < 5:
            if self.instance.poll() is not None:
                logger.info(""Terminated %s."", self.coord)
                break
            time.sleep(0.1)
        else:
            self.kill()
",if self . instance . poll ( ) is not None :,194
"def sort_collection(self, models, many):
    ordering = self.ordering
    if not many or not ordering:
        return models
    for key in reversed(ordering):
        reverse = key[0] == ""-""
        if reverse:
            key = key[1:]
        models = sorted(models, key=partial(deep_getattr, key=key), reverse=reverse)
    return models
",if reverse :,98
"def get_palette_for_custom_classes(self, class_names, palette=None):
    if self.label_map is not None:
        # return subset of palette
        palette = []
        for old_id, new_id in sorted(self.label_map.items(), key=lambda x: x[1]):
            if new_id != -1:
                palette.append(self.PALETTE[old_id])
        palette = type(self.PALETTE)(palette)
    elif palette is None:
        if self.PALETTE is None:
            palette = np.random.randint(0, 255, size=(len(class_names), 3))
        else:
            palette = self.PALETTE
    return palette
",if new_id != - 1 :,194
"def _find_tcl_dir():
    lib_dirs = [os.path.dirname(_x) for _x in sys.path if _x.lower().endswith(""lib"")]
    for lib_dir in lib_dirs:
        base_dir = os.path.join(lib_dir, TclLibrary.FOLDER)
        if os.path.exists(base_dir):
            for root, _, files in os.walk(base_dir):
                if TclLibrary.INIT_TCL in files:
                    return root
",if os . path . exists ( base_dir ) :,133
"def __next__(self):
    """"""Special paging functionality""""""
    if self.iter is None:
        self.iter = iter(self.objs)
    try:
        return next(self.iter)
    except StopIteration:
        self.iter = None
        self.objs = []
        if int(self.page) < int(self.total_pages):
            self.page += 1
            self._connection.get_response(self.action, self.params, self.page, self)
            return next(self)
        else:
            raise
",if int ( self . page ) < int ( self . total_pages ) :,144
"def parse(cls, api, json):
    lst = List(api)
    setattr(lst, ""_json"", json)
    for k, v in json.items():
        if k == ""user"":
            setattr(lst, k, User.parse(api, v))
        elif k == ""created_at"":
            setattr(lst, k, parse_datetime(v))
        else:
            setattr(lst, k, v)
    return lst
","elif k == ""created_at"" :",115
"def real_type(self):
    # Find the real type representation by updating it as required
    real_type = self.type
    if self.flag_indicator:
        real_type = ""#""
    if self.is_vector:
        if self.use_vector_id:
            real_type = ""Vector<{}>"".format(real_type)
        else:
            real_type = ""vector<{}>"".format(real_type)
    if self.is_generic:
        real_type = ""!{}"".format(real_type)
    if self.is_flag:
        real_type = ""flags.{}?{}"".format(self.flag_index, real_type)
    return real_type
",if self . use_vector_id :,171
"def check_fs(path):
    with open(path, ""rb"") as f:
        code = python_bytes_to_unicode(f.read(), errors=""replace"")
        if name in code:
            module = _load_module(evaluator, path, code)
            module_name = sys_path.dotted_path_in_sys_path(
                evaluator.project.sys_path, path
            )
            if module_name is not None:
                add_module(evaluator, module_name, module)
            return module
",if name in code :,143
"def infoCalendar(users):
    calendarId = normalizeCalendarId(sys.argv[5], checkPrimary=True)
    i = 0
    count = len(users)
    for user in users:
        i += 1
        user, cal = buildCalendarGAPIObject(user)
        if not cal:
            continue
        result = gapi.call(
            cal.calendarList(), ""get"", soft_errors=True, calendarId=calendarId
        )
        if result:
            print(f""User: {user}, Calendar:{display.current_count(i, count)}"")
            _showCalendar(result, 1, 1)
",if result :,163
"def set_hidestate_input_sockets_to_cope_with_switchnum(self):
    tndict = get_indices_that_should_be_visible(self.node_state)
    for key, value in tndict.items():
        socket = self.inputs[key]
        desired_hide_state = not (value)
        if not socket.hide == desired_hide_state:
            socket.hide_safe = desired_hide_state
",if not socket . hide == desired_hide_state :,111
"def get_class_name(item):
    class_name, module_name = None, None
    for parent in reversed(item.listchain()):
        if isinstance(parent, pytest.Class):
            class_name = parent.name
        elif isinstance(parent, pytest.Module):
            module_name = parent.module.__name__
            break
    # heuristic:
    # - better to group gpu and task tests, since tests from those modules
    #   are likely to share caching more
    # - split up the rest by class name because slow tests tend to be in
    #   the same module
    if class_name and "".tasks."" not in module_name:
        return ""{}.{}"".format(module_name, class_name)
    else:
        return module_name
","elif isinstance ( parent , pytest . Module ) :",190
"def run(self):
    versions = versioneer.get_versions()
    tempdir = tempfile.mkdtemp()
    generated = os.path.join(tempdir, ""rundemo"")
    with open(generated, ""wb"") as f:
        for line in open(""src/rundemo-template"", ""rb""):
            if line.strip().decode(""ascii"") == ""#versions"":
                f.write((""versions = %r\n"" % (versions,)).encode(""ascii""))
            else:
                f.write(line)
    self.scripts = [generated]
    rc = build_scripts.run(self)
    os.unlink(generated)
    os.rmdir(tempdir)
    return rc
","if line . strip ( ) . decode ( ""ascii"" ) == ""#versions"" :",172
"def get_user_context(request, escape=False):
    if isinstance(request, HttpRequest):
        user = getattr(request, ""user"", None)
        result = {""ip_address"": request.META[""REMOTE_ADDR""]}
        if user and user.is_authenticated():
            result.update(
                {
                    ""email"": user.email,
                    ""id"": user.id,
                }
            )
            if user.name:
                result[""name""] = user.name
    else:
        result = {}
    return mark_safe(json.dumps(result))
",if user and user . is_authenticated ( ) :,163
"def tokens_to_spans() -> Iterable[Tuple[str, Optional[Style]]]:
    """"""Convert tokens to spans.""""""
    tokens = iter(line_tokenize())
    line_no = 0
    _line_start = line_start - 1
    # Skip over tokens until line start
    while line_no < _line_start:
        _token_type, token = next(tokens)
        yield (token, None)
        if token.endswith(""\n""):
            line_no += 1
    # Generate spans until line end
    for token_type, token in tokens:
        yield (token, _get_theme_style(token_type))
        if token.endswith(""\n""):
            line_no += 1
            if line_no >= line_end:
                break
","if token . endswith ( ""\n"" ) :",194
"def encode(self, encodeFun, value, defMode, maxChunkSize):
    substrate, isConstructed = self.encodeValue(encodeFun, value, defMode, maxChunkSize)
    tagSet = value.getTagSet()
    if tagSet:
        if not isConstructed:  # primitive form implies definite mode
            defMode = 1
        return (
            self.encodeTag(tagSet[-1], isConstructed)
            + self.encodeLength(len(substrate), defMode)
            + substrate
            + self._encodeEndOfOctets(encodeFun, defMode)
        )
    else:
        return substrate  # untagged value
",if not isConstructed :,175
"def _run(self):
    while True:
        request = self._requests.get()
        if request is None:
            self.shutdown()
            break
        self.process(request)
        self._requests.task_done()
",if request is None :,64
"def _decode_payload(self, payload):
    # we need to decrypt it
    if payload[""enc""] == ""aes"":
        try:
            payload[""load""] = self.crypticle.loads(payload[""load""])
        except salt.crypt.AuthenticationError:
            if not self._update_aes():
                raise
            payload[""load""] = self.crypticle.loads(payload[""load""])
    return payload
",if not self . _update_aes ( ) :,107
"def test_row(self, row):
    for idx, test in self.patterns.items():
        try:
            value = row[idx]
        except IndexError:
            value = """"
        result = test(value)
        if self.any_match:
            if result:
                return not self.inverse  # True
        else:
            if not result:
                return self.inverse  # False
    if self.any_match:
        return self.inverse  # False
    else:
        return not self.inverse  # True
",if result :,149
"def setup_parameter_node(self, param_node):
    if param_node.bl_idname == ""SvNumberNode"":
        if self.use_prop or self.get_prop_name():
            value = self.sv_get()[0][0]
            print(""V"", value)
            if isinstance(value, int):
                param_node.selected_mode = ""int""
                param_node.int_ = value
            elif isinstance(value, float):
                param_node.selected_mode = ""float""
                param_node.float_ = value
","elif isinstance ( value , float ) :",156
"def iter_modules(self, by_clients=False, clients_filter=None):
    """"""iterate over all modules""""""
    clients = None
    if by_clients:
        clients = self.get_clients(clients_filter)
        if not clients:
            return
    self._refresh_modules()
    for module_name in self.modules:
        try:
            module = self.get_module(module_name)
        except PupyModuleDisabled:
            continue
        if clients is not None:
            for client in clients:
                if module.is_compatible_with(client):
                    yield module
                    break
        else:
            yield module
",if clients is not None :,181
"def filter_pricing_rule_based_on_condition(pricing_rules, doc=None):
    filtered_pricing_rules = []
    if doc:
        for pricing_rule in pricing_rules:
            if pricing_rule.condition:
                try:
                    if frappe.safe_eval(pricing_rule.condition, None, doc.as_dict()):
                        filtered_pricing_rules.append(pricing_rule)
                except:
                    pass
            else:
                filtered_pricing_rules.append(pricing_rule)
    else:
        filtered_pricing_rules = pricing_rules
    return filtered_pricing_rules
","if frappe . safe_eval ( pricing_rule . condition , None , doc . as_dict ( ) ) :",179
"def build_query_string(kv_data, ignore_none=True):
    # {""a"": 1, ""b"": ""test""} -> ""?a=1&b=test""
    query_string = """"
    for k, v in kv_data.iteritems():
        if ignore_none is True and kv_data[k] is None:
            continue
        if query_string != """":
            query_string += ""&""
        else:
            query_string = ""?""
        query_string += k + ""="" + str(v)
    return query_string
",if ignore_none is True and kv_data [ k ] is None :,140
"def sample(self, **config):
    """"""Sample a configuration from this search space.""""""
    ret = {}
    ret.update(self.data)
    kwspaces = self.kwspaces
    kwspaces.update(config)
    striped_keys = [k.split(SPLITTER)[0] for k in config.keys()]
    for k, v in kwspaces.items():
        if k in striped_keys:
            if isinstance(v, NestedSpace):
                sub_config = _strip_config_space(config, prefix=k)
                ret[k] = v.sample(**sub_config)
            else:
                ret[k] = v
    return ret
","if isinstance ( v , NestedSpace ) :",172
"def task_failed(self, task_id, hostname, reason):
    logger.debug(""task %d failed with message %s"", task_id, str(reason))
    if hostname in self.host_dict:
        host_status = self.host_dict[hostname]
        host_status.task_failed(task_id)
        if task_id not in self.task_host_failed_dict:
            self.task_host_failed_dict[task_id] = set()
        self.task_host_failed_dict[task_id].add(hostname)
",if task_id not in self . task_host_failed_dict :,141
"def match(path):
    for pat, _type, _property, default_title in patterns:
        m = web.re_compile(""^"" + pat).match(path)
        if m:
            prefix = m.group()
            extra = web.lstrips(path, prefix)
            tokens = extra.split(""/"", 2)
            # `extra` starts with ""/"". So first token is always empty.
            middle = web.listget(tokens, 1, """")
            suffix = web.listget(tokens, 2, """")
            if suffix:
                suffix = ""/"" + suffix
            return _type, _property, default_title, prefix, middle, suffix
    return None, None, None, None, None, None
",if m :,187
"def _get_cached_resources(self, ids):
    key = self.get_cache_key(None)
    if self._cache.load():
        resources = self._cache.get(key)
        if resources is not None:
            self.log.debug(""Using cached results for get_resources"")
            m = self.get_model()
            id_set = set(ids)
            return [r for r in resources if r[m.id] in id_set]
    return None
",if resources is not None :,127
"def has_api_behaviour(self, protocol):
    config = get_config()
    try:
        r = self.session.get(
            f""{protocol}://{self.event.host}:{self.event.port}"",
            timeout=config.network_timeout,
        )
        if (""k8s"" in r.text) or ('""code""' in r.text and r.status_code != 200):
            return True
    except requests.exceptions.SSLError:
        logger.debug(
            f""{[protocol]} protocol not accepted on {self.event.host}:{self.event.port}""
        )
    except Exception:
        logger.debug(
            f""Failed probing {self.event.host}:{self.event.port}"", exc_info=True
        )
","if ( ""k8s"" in r . text ) or ( '""code""' in r . text and r . status_code != 200 ) :",200
"def get_file_type(self, context, parent_context=None):
    file_type = context.get(self.file_type_name, None)
    if file_type == """":
        if parent_context:
            file_type = parent_context.get(self.file_type_name, self.default_file_type)
        else:
            file_type = self.default_file_type
    return file_type
",if parent_context :,110
"def selectionToChunks(self, remove=False, add=False):
    box = self.selectionBox()
    if box:
        if box == self.level.bounds:
            self.selectedChunks = set(self.level.allChunks)
            return
        selectedChunks = self.selectedChunks
        boxedChunks = set(box.chunkPositions)
        if boxedChunks.issubset(selectedChunks):
            remove = True
        if remove and not add:
            selectedChunks.difference_update(boxedChunks)
        else:
            selectedChunks.update(boxedChunks)
    self.selectionTool.selectNone()
",if boxedChunks . issubset ( selectedChunks ) :,158
"def _run_split_on_punc(self, text, never_split=None):
    """"""Splits punctuation on a piece of text.""""""
    if never_split is not None and text in never_split:
        return [text]
    chars = list(text)
    i = 0
    start_new_word = True
    output = []
    while i < len(chars):
        char = chars[i]
        if _is_punctuation(char):
            output.append([char])
            start_new_word = True
        else:
            if start_new_word:
                output.append([])
            start_new_word = False
            output[-1].append(char)
        i += 1
    return ["""".join(x) for x in output]
",if _is_punctuation ( char ) :,199
"def _save_images(notebook):
    if os.getenv(""NB_NO_IMAGES"") == ""1"":
        return
    logged = False
    for filename, img_bytes in _iter_notebook_images(notebook):
        if not logged:
            log.info(""Saving images"")
            logged = True
        with open(filename, ""wb"") as f:
            f.write(img_bytes)
",if not logged :,104
"def pickPath(self, color):
    self.path[color] = ()
    currentPos = self.starts[color]
    while True:
        minDist = None
        minGuide = None
        for guide in self.guides[color]:
            guideDist = dist(currentPos, guide)
            if minDist == None or guideDist < minDist:
                minDist = guideDist
                minGuide = guide
        if dist(currentPos, self.ends[color]) == 1:
            return
        if minGuide == None:
            return
        self.path[color] = self.path[color] + (minGuide,)
        currentPos = minGuide
        self.guides[color].remove(minGuide)
",if minDist == None or guideDist < minDist :,192
"def _terminal_messenger(tp=""write"", msg="""", out=sys.stdout):
    try:
        if tp == ""write"":
            out.write(msg)
        elif tp == ""flush"":
            out.flush()
        elif tp == ""write_flush"":
            out.write(msg)
            out.flush()
        elif tp == ""print"":
            print(msg, file=out)
        else:
            raise ValueError(""Unsupported type: "" + tp)
    except IOError as e:
        logger.critical(""{}: {}"".format(type(e).__name__, ucd(e)))
        pass
","if tp == ""write"" :",160
"def __new__(mcs, name, bases, attrs):
    include_profile = include_trace = include_garbage = True
    bases = list(bases)
    if name == ""SaltLoggingClass"":
        for base in bases:
            if hasattr(base, ""trace""):
                include_trace = False
            if hasattr(base, ""garbage""):
                include_garbage = False
    if include_profile:
        bases.append(LoggingProfileMixin)
    if include_trace:
        bases.append(LoggingTraceMixin)
    if include_garbage:
        bases.append(LoggingGarbageMixin)
    return super(LoggingMixinMeta, mcs).__new__(mcs, name, tuple(bases), attrs)
","if hasattr ( base , ""trace"" ) :",176
"def generatePidEncryptionTable():
    table = []
    for counter1 in range(0, 0x100):
        value = counter1
        for counter2 in range(0, 8):
            if value & 1 == 0:
                value = value >> 1
            else:
                value = value >> 1
                value = value ^ 0xEDB88320
        table.append(value)
    return table
",if value & 1 == 0 :,109
"def pytest_collection_modifyitems(items):
    for item in items:
        if item.nodeid.startswith(""tests/params""):
            if ""stage"" not in item.keywords:
                item.add_marker(pytest.mark.stage(""unit""))
            if ""init"" not in item.keywords:
                item.add_marker(pytest.mark.init(rng_seed=123))
","if ""stage"" not in item . keywords :",102
"def python_value(self, value):
    if value:
        if isinstance(value, basestring):
            pp = lambda x: x.time()
            return format_date_time(value, self.formats, pp)
        elif isinstance(value, datetime.datetime):
            return value.time()
    if value is not None and isinstance(value, datetime.timedelta):
        return (datetime.datetime.min + value).time()
    return value
","elif isinstance ( value , datetime . datetime ) :",113
"def list_interesting_hosts(self):
    hosts = []
    targets = self.target[""other""]
    for target in targets:
        if self.is_interesting(target) and target.status and target.status != 400:
            hosts.append(
                {""ip"": target.ip, ""description"": target.domain + "" / "" + target.name}
            )
    return hosts
",if self . is_interesting ( target ) and target . status and target . status != 400 :,99
"def TryMerge(self, d):
    while d.avail() > 0:
        tt = d.getVarInt32()
        if tt == 10:
            length = d.getVarInt32()
            tmp = ProtocolBuffer.Decoder(d.buffer(), d.pos(), d.pos() + length)
            d.skip(length)
            self.mutable_cost().TryMerge(tmp)
            continue
        if tt == 24:
            self.add_version(d.getVarInt64())
            continue
        if tt == 0:
            raise ProtocolBuffer.ProtocolBufferDecodeError
        d.skipData(tt)
",if tt == 24 :,167
"def _wait_for_finish(self) -> PollExitResponse:
    while True:
        if self._backend:
            poll_exit_resp = self._backend.interface.communicate_poll_exit()
        logger.info(""got exit ret: %s"", poll_exit_resp)
        if poll_exit_resp:
            done = poll_exit_resp.done
            pusher_stats = poll_exit_resp.pusher_stats
            if pusher_stats:
                self._on_finish_progress(pusher_stats, done)
            if done:
                return poll_exit_resp
        time.sleep(2)
",if pusher_stats :,170
"def listing_items(method):
    marker = None
    once = True
    items = []
    while once or items:
        for i in items:
            yield i
        if once or marker:
            if marker:
                items = method(parms={""marker"": marker})
            else:
                items = method()
            if len(items) == 10000:
                marker = items[-1]
            else:
                marker = None
            once = False
        else:
            items = []
",if marker :,145
"def call(monad, *args):
    for arg, name in izip(args, (""hour"", ""minute"", ""second"", ""microsecond"")):
        if not isinstance(arg, NumericMixin) or arg.type is not int:
            throw(
                TypeError,
                ""'%s' argument of time(...) function must be of 'int' type. Got: %r""
                % (name, type2str(arg.type)),
            )
        if not isinstance(arg, ConstMonad):
            throw(NotImplementedError)
    return ConstMonad.new(time(*tuple(arg.value for arg in args)))
","if not isinstance ( arg , ConstMonad ) :",157
"def group_by_sign(seq, slop=sin(pi / 18), key=lambda x: x):
    sign = None
    subseq = []
    for i in seq:
        ki = key(i)
        if sign is None:
            subseq.append(i)
            if ki != 0:
                sign = ki / abs(ki)
        else:
            subseq.append(i)
            if sign * ki < -slop:
                sign = ki / abs(ki)
                yield subseq
                subseq = [i]
    if subseq:
        yield subseq
",if sign * ki < - slop :,167
"def walk_links(self):
    link_info_list = []
    for item in self.content:
        if isinstance(item, Link):
            link_info = LinkInfo(link=item, name=item.name, sections=())
            link_info_list.append(link_info)
        else:
            link_info_list.extend(item.walk_links())
    return link_info_list
","if isinstance ( item , Link ) :",106
"def get_subkeys(self, key):
    # TODO: once we revamp the registry emulation,
    # make this better
    parent_path = key.get_path()
    subkeys = []
    for k in self.keys:
        test_path = k.get_path()
        if test_path.lower().startswith(parent_path.lower()):
            sub = test_path[len(parent_path) :]
            if sub.startswith(""\\""):
                sub = sub[1:]
            end_slash = sub.find(""\\"")
            if end_slash >= 0:
                sub = sub[:end_slash]
            if not sub:
                continue
            subkeys.append(sub)
    return subkeys
",if end_slash >= 0 :,192
"def load_dict(dict_path, reverse=False):
    word_dict = {}
    with open(dict_path, ""rb"") as fdict:
        for idx, line in enumerate(fdict):
            line = cpt.to_text(line)
            if reverse:
                word_dict[idx] = line.strip(""\n"")
            else:
                word_dict[line.strip(""\n"")] = idx
    return word_dict
",if reverse :,118
"def test_network(coords, feats, model, batch_sizes, forward_only=True):
    for batch_size in batch_sizes:
        bcoords = batched_coordinates([coords for i in range(batch_size)])
        bfeats = torch.cat([feats for i in range(batch_size)], 0)
        if forward_only:
            with torch.no_grad():
                time, length = forward(bcoords, bfeats, model)
        else:
            time, length = train(bcoords, bfeats, model)
        print(f""{net.__name__}\t{voxel_size}\t{batch_size}\t{length}\t{time}"")
        torch.cuda.empty_cache()
",if forward_only :,181
"def markUVs(self, indices=None):
    if isinstance(indices, tuple):
        indices = indices[0]
    ntexco = len(self.texco)
    if indices is None:
        self.utexc = True
    else:
        if self.utexc is False:
            self.utexc = np.zeros(ntexco, dtype=bool)
        if self.utexc is not True:
            self.utexc[indices] = True
",if self . utexc is False :,120
"def has_module(self, module, version):
    has_module = False
    for directory in self.directories:
        module_directory = join(directory, module)
        has_module_directory = isdir(module_directory)
        if not version:
            has_module = has_module_directory or exists(
                module_directory
            )  # could be a bare modulefile
        else:
            modulefile = join(module_directory, version)
            has_modulefile = exists(modulefile)
            has_module = has_module_directory and has_modulefile
        if has_module:
            break
    return has_module
",if not version :,171
"def get_editops(self):
    if not self._editops:
        if self._opcodes:
            self._editops = editops(self._opcodes, self._str1, self._str2)
        else:
            self._editops = editops(self._str1, self._str2)
    return self._editops
",if self . _opcodes :,86
"def to_representation(self, data):
    value = super(CredentialTypeSerializer, self).to_representation(data)
    # translate labels and help_text for credential fields ""managed by Tower""
    if value.get(""managed_by_tower""):
        value[""name""] = _(value[""name""])
        for field in value.get(""inputs"", {}).get(""fields"", []):
            field[""label""] = _(field[""label""])
            if ""help_text"" in field:
                field[""help_text""] = _(field[""help_text""])
    return value
","if ""help_text"" in field :",140
"def sort_nested_dictionary_lists(d):
    for k, v in d.items():
        if isinstance(v, list):
            for i in range(0, len(v)):
                if isinstance(v[i], dict):
                    v[i] = await sort_nested_dictionary_lists(v[i])
                d[k] = sorted(v)
        if isinstance(v, dict):
            d[k] = await sort_nested_dictionary_lists(v)
    return d
","if isinstance ( v [ i ] , dict ) :",134
"def messageSourceStamps(self, source_stamps):
    text = """"
    for ss in source_stamps:
        source = """"
        if ss[""branch""]:
            source += ""[branch %s] "" % ss[""branch""]
        if ss[""revision""]:
            source += str(ss[""revision""])
        else:
            source += ""HEAD""
        if ss[""patch""] is not None:
            source += "" (plus patch)""
        discriminator = """"
        if ss[""codebase""]:
            discriminator = "" '%s'"" % ss[""codebase""]
        text += ""Build Source Stamp%s: %s\n"" % (discriminator, source)
    return text
","if ss [ ""patch"" ] is not None :",176
"def fit_one(self, x):
    for i, xi in x.items():
        if self.with_centering:
            self.median[i].update(xi)
        if self.with_scaling:
            self.iqr[i].update(xi)
    return self
",if self . with_centering :,75
"def start_response(self, status, headers, exc_info=None):
    if exc_info:
        try:
            if self.started:
                six.reraise(exc_info[0], exc_info[1], exc_info[2])
        finally:
            exc_info = None
    self.request.status = int(status[:3])
    for key, val in headers:
        if key.lower() == ""content-length"":
            self.request.set_content_length(int(val))
        elif key.lower() == ""content-type"":
            self.request.content_type = val
        else:
            self.request.headers_out.add(key, val)
    return self.write
","if key . lower ( ) == ""content-length"" :",191
"def _osp2ec(self, bytes):
    compressed = self._from_bytes(bytes)
    y = compressed >> self._bits
    x = compressed & (1 << self._bits) - 1
    if x == 0:
        y = self._curve.b
    else:
        result = self.sqrtp(
            x ** 3 + self._curve.a * x + self._curve.b, self._curve.field.p
        )
        if len(result) == 1:
            y = result[0]
        elif len(result) == 2:
            y1, y2 = result
            y = y1 if (y1 & 1 == y) else y2
        else:
            return None
    return ec.Point(self._curve, x, y)
",elif len ( result ) == 2 :,200
"def trace(self, ee, rname):
    print(type(self))
    self.traceIndent()
    guess = """"
    if self.inputState.guessing > 0:
        guess = "" [guessing]""
    print((ee + rname + guess))
    for i in xrange(1, self.k + 1):
        if i != 1:
            print("", "")
        if self.LT(i):
            v = self.LT(i).getText()
        else:
            v = ""null""
        print(""LA(%s) == %s"" % (i, v))
    print(""\n"")
",if self . LT ( i ) :,158
"def _table_schema(self, table):
    rows = self.db.execute_sql(""PRAGMA table_info('%s')"" % table).fetchall()
    # Build list of fields from table information
    result = {}
    for _, name, data_type, not_null, _, primary_key in rows:
        parts = [data_type]
        if primary_key:
            parts.append(""PRIMARY KEY"")
        if not_null:
            parts.append(""NOT NULL"")
        result[name] = "" "".join(parts)
    return result
",if not_null :,137
"def _parse_csrf(self, response):
    for d in response:
        if d.startswith(""Set-Cookie:""):
            for c in d.split("":"", 1)[1].split("";""):
                if c.strip().startswith(""CSRF-Token-""):
                    self._CSRFtoken = c.strip("" \r\n"")
                    log.verbose(""Got new cookie: %s"", self._CSRFtoken)
                    break
            if self._CSRFtoken != None:
                break
",if self . _CSRFtoken != None :,135
"def _update_from_item(self, row, download_item):
    progress_stats = download_item.progress_stats
    for key in self.columns:
        column = self.columns[key][0]
        if key == ""status"" and progress_stats[""playlist_index""]:
            # Not the best place but we build the playlist status here
            status = ""{0} {1}/{2}"".format(
                progress_stats[""status""],
                progress_stats[""playlist_index""],
                progress_stats[""playlist_size""],
            )
            self.SetStringItem(row, column, status)
        else:
            self.SetStringItem(row, column, progress_stats[key])
","if key == ""status"" and progress_stats [ ""playlist_index"" ] :",183
"def unmarshal_package_repositories(cls, data: Any) -> List[""PackageRepository""]:
    repositories = list()
    if data is not None:
        if not isinstance(data, list):
            raise RuntimeError(f""invalid package-repositories: {data!r}"")
        for repository in data:
            package_repo = cls.unmarshal(repository)
            repositories.append(package_repo)
    return repositories
","if not isinstance ( data , list ) :",114
"def remove_message(e=None):
    itop = scanbox.nearest(0)
    sel = scanbox.curselection()
    if not sel:
        dialog(
            root,
            ""No Message To Remove"",
            ""Please select a message to remove"",
            """",
            0,
            ""OK"",
        )
        return
    todo = []
    for i in sel:
        line = scanbox.get(i)
        if scanparser.match(line) >= 0:
            todo.append(string.atoi(scanparser.group(1)))
    mhf.removemessages(todo)
    rescan()
    fixfocus(min(todo), itop)
",if scanparser . match ( line ) >= 0 :,182
"def test_patches():
    print(
        ""Botocore version: {} aiohttp version: {}"".format(
            botocore.__version__, aiohttp.__version__
        )
    )
    success = True
    for obj, digests in chain(_AIOHTTP_DIGESTS.items(), _API_DIGESTS.items()):
        digest = hashlib.sha1(getsource(obj).encode(""utf-8"")).hexdigest()
        if digest not in digests:
            print(
                ""Digest of {}:{} not found in: {}"".format(
                    obj.__qualname__, digest, digests
                )
            )
            success = False
    assert success
",if digest not in digests :,167
"def sample_admin_user():
    """"""List of iris messages""""""
    with iris_ctl.db_from_config(sample_db_config) as (conn, cursor):
        cursor.execute(
            ""SELECT `name` FROM `target` JOIN `user` on `target`.`id` = `user`.`target_id` WHERE `user`.`admin` = TRUE LIMIT 1""
        )
        result = cursor.fetchone()
        if result:
            return result[0]
",if result :,123
"def _addRightnames(groups, kerning, leftname, rightnames, includeAll=True):
    if leftname in kerning:
        for rightname in kerning[leftname]:
            if rightname[0] == ""@"":
                for rightname2 in groups[rightname]:
                    rightnames.add(rightname2)
                    if not includeAll:
                        # TODO: in this case, pick the one rightname that has the highest
                        # ranking in glyphorder
                        break
            else:
                rightnames.add(rightname)
","if rightname [ 0 ] == ""@"" :",161
"def build(self, input_shape):
    if isinstance(input_shape, list) and len(input_shape) == 2:
        self.data_mode = ""disjoint""
        self.F = input_shape[0][-1]
    else:
        if len(input_shape) == 2:
            self.data_mode = ""single""
        else:
            self.data_mode = ""batch""
        self.F = input_shape[-1]
",if len ( input_shape ) == 2 :,120
"def update_ranges(l, i):
    for _range in l:
        # most common case: extend a range
        if i == _range[0] - 1:
            _range[0] = i
            merge_ranges(l)
            return
        elif i == _range[1] + 1:
            _range[1] = i
            merge_ranges(l)
            return
    # somewhere outside of range proximity
    l.append([i, i])
    l.sort(key=lambda x: x[0])
",if i == _range [ 0 ] - 1 :,145
"def transform(a, cmds):
    buf = a.split(""\n"")
    for cmd in cmds:
        ctype, line, col, char = cmd
        if ctype == ""D"":
            if char != ""\n"":
                buf[line] = buf[line][:col] + buf[line][col + len(char) :]
            else:
                buf[line] = buf[line] + buf[line + 1]
                del buf[line + 1]
        elif ctype == ""I"":
            buf[line] = buf[line][:col] + char + buf[line][col:]
        buf = ""\n"".join(buf).split(""\n"")
    return ""\n"".join(buf)
","if ctype == ""D"" :",182
"def _media_files_drag_received(widget, context, x, y, data, info, timestamp):
    uris = data.get_uris()
    files = []
    for uri in uris:
        try:
            uri_tuple = GLib.filename_from_uri(uri)
        except:
            continue
        uri, unused = uri_tuple
        if os.path.exists(uri) == True:
            if utils.is_media_file(uri) == True:
                files.append(uri)
    if len(files) == 0:
        return
    open_dropped_files(files)
",if utils . is_media_file ( uri ) == True :,159
"def __walk_proceed_remote_dir_act(self, r, args):
    dirjs, filejs = args
    j = r.json()
    if ""list"" not in j:
        self.pd(
            ""Key 'list' not found in the response of directory listing request:\n{}"".format(
                j
            )
        )
        return const.ERequestFailed
    paths = j[""list""]
    for path in paths:
        if path[""isdir""]:
            dirjs.append(path)
        else:
            filejs.append(path)
    return const.ENoError
","if path [ ""isdir"" ] :",159
"def TaskUpdatesVerbose(task, progress):
    if isinstance(task.info.progress, int):
        info = task.info
        if not isinstance(progress, str):
            progress = ""%d%% (%s)"" % (info.progress, info.state)
        print(
            ""Task %s (key:%s, desc:%s) - %s""
            % (info.name.info.name, info.key, info.description, progress)
        )
","if not isinstance ( progress , str ) :",118
"def dump_constants(header):
    output = StringIO.StringIO()
    output.write(header)
    for attribute in dir(FSEvents):
        value = getattr(FSEvents, attribute)
        if attribute.startswith(""k"") and isinstance(value, int):
            output.write(""    %s = %s\n"" % (attribute, hex(value)))
    content = output.getvalue()
    output.close()
    return content
","if attribute . startswith ( ""k"" ) and isinstance ( value , int ) :",112
"def _ensure_data_is_loaded(
    self,
    sql_object,
    input_params,
    stdin_file,
    stdin_filename=""-"",
    stop_after_analysis=False,
):
    data_loads = []
    # Get each ""table name"" which is actually the file name
    for filename in sql_object.qtable_names:
        data_load = self._load_data(
            filename,
            input_params,
            stdin_file=stdin_file,
            stdin_filename=stdin_filename,
            stop_after_analysis=stop_after_analysis,
        )
        if data_load is not None:
            data_loads.append(data_load)
    return data_loads
",if data_load is not None :,189
"def _get_instantiation(self):
    if self._data is None:
        f, l, c, o = c_object_p(), c_uint(), c_uint(), c_uint()
        SourceLocation_loc(self, byref(f), byref(l), byref(c), byref(o))
        if f:
            f = File(f)
        else:
            f = None
        self._data = (f, int(l.value), int(c.value), int(c.value))
    return self._data
",if f :,136
"def _get_all_info_lines(data):
    infos = []
    for row in data:
        splitrow = row.split()
        if len(splitrow) > 0:
            if splitrow[0] == ""INFO:"":
                infos.append("" "".join(splitrow[1:]))
    return infos
",if len ( splitrow ) > 0 :,82
"def _brush_modified_cb(self, settings):
    """"""Updates the brush's base setting adjustments on brush changes""""""
    for cname in settings:
        adj = self.brush_adjustment.get(cname, None)
        if adj is None:
            continue
        value = self.brush.get_base_value(cname)
        adj.set_value(value)
",if adj is None :,92
"def migrate_node_facts(facts):
    """"""Migrate facts from various roles into node""""""
    params = {
        ""common"": (""dns_ip""),
    }
    if ""node"" not in facts:
        facts[""node""] = {}
    # pylint: disable=consider-iterating-dictionary
    for role in params.keys():
        if role in facts:
            for param in params[role]:
                if param in facts[role]:
                    facts[""node""][param] = facts[role].pop(param)
    return facts
",if param in facts [ role ] :,136
"def serialize_content_range(value):
    if isinstance(value, (tuple, list)):
        if len(value) not in (2, 3):
            raise ValueError(
                ""When setting content_range to a list/tuple, it must ""
                ""be length 2 or 3 (not %r)"" % value
            )
        if len(value) == 2:
            begin, end = value
            length = None
        else:
            begin, end, length = value
        value = ContentRange(begin, end, length)
    value = str(value).strip()
    if not value:
        return None
    return value
","if len ( value ) not in ( 2 , 3 ) :",169
"def clean(self):
    data = super().clean()
    if data.get(""expires""):
        if isinstance(data[""expires""], date):
            data[""expires""] = make_aware(
                datetime.combine(data[""expires""], time(hour=23, minute=59, second=59)),
                self.instance.event.timezone,
            )
        else:
            data[""expires""] = data[""expires""].replace(hour=23, minute=59, second=59)
        if data[""expires""] < now():
            raise ValidationError(_(""The new expiry date needs to be in the future.""))
    return data
","if isinstance ( data [ ""expires"" ] , date ) :",158
"def _build(self, obj, stream, context):
    if self.include_name:
        name, obj = obj
        for sc in self.subcons:
            if sc.name == name:
                sc._build(obj, stream, context)
                return
    else:
        for sc in self.subcons:
            stream2 = BytesIO()
            context2 = context.__copy__()
            try:
                sc._build(obj, stream2, context2)
            except Exception:
                pass
            else:
                context.__update__(context2)
                stream.write(stream2.getvalue())
                return
    raise SelectError(""no subconstruct matched"", obj)
",if sc . name == name :,195
"def records(account_id):
    """"""Fetch locks data""""""
    s = boto3.Session()
    table = s.resource(""dynamodb"").Table(""Sphere11.Dev.ResourceLocks"")
    results = table.scan()
    for r in results[""Items""]:
        if ""LockDate"" in r:
            r[""LockDate""] = datetime.fromtimestamp(r[""LockDate""])
        if ""RevisionDate"" in r:
            r[""RevisionDate""] = datetime.fromtimestamp(r[""RevisionDate""])
    print(tabulate.tabulate(results[""Items""], headers=""keys"", tablefmt=""fancy_grid""))
","if ""LockDate"" in r :",149
"def visitIf(self, node, scope):
    for test, body in node.tests:
        if isinstance(test, ast.Const):
            if type(test.value) in self._const_types:
                if not test.value:
                    continue
        self.visit(test, scope)
        self.visit(body, scope)
    if node.else_:
        self.visit(node.else_, scope)
",if type ( test . value ) in self . _const_types :,112
"def validate_max_discount(self):
    if self.rate_or_discount == ""Discount Percentage"" and self.get(""items""):
        for d in self.items:
            max_discount = frappe.get_cached_value(""Item"", d.item_code, ""max_discount"")
            if max_discount and flt(self.discount_percentage) > flt(max_discount):
                throw(
                    _(""Max discount allowed for item: {0} is {1}%"").format(
                        self.item_code, max_discount
                    )
                )
",if max_discount and flt ( self . discount_percentage ) > flt ( max_discount ) :,158
"def has_invalid_cce(yaml_file, product_yaml=None):
    rule = yaml.open_and_macro_expand(yaml_file, product_yaml)
    if ""identifiers"" in rule and rule[""identifiers""] is not None:
        for i_type, i_value in rule[""identifiers""].items():
            if i_type[0:3] == ""cce"":
                if not checks.is_cce_value_valid(""CCE-"" + str(i_value)):
                    return True
    return False
","if not checks . is_cce_value_valid ( ""CCE-"" + str ( i_value ) ) :",134
"def parse_calendar_eras(data, calendar):
    eras = data.setdefault(""eras"", {})
    for width in calendar.findall(""eras/*""):
        width_type = NAME_MAP[width.tag]
        widths = eras.setdefault(width_type, {})
        for elem in width.getiterator():
            if elem.tag == ""era"":
                _import_type_text(widths, elem, type=int(elem.attrib.get(""type"")))
            elif elem.tag == ""alias"":
                eras[width_type] = Alias(
                    _translate_alias([""eras"", width_type], elem.attrib[""path""])
                )
","elif elem . tag == ""alias"" :",170
"def validate_grammar() -> None:
    for fn in _NONTERMINAL_CONVERSIONS_SEQUENCE:
        fn_productions = get_productions(fn)
        if all(p.name == fn_productions[0].name for p in fn_productions):
            # all the production names are the same, ensure that the `convert_` function
            # is named correctly
            production_name = fn_productions[0].name
            expected_name = f""convert_{production_name}""
            if fn.__name__ != expected_name:
                raise Exception(
                    f""The conversion function for '{production_name}' ""
                    + f""must be called '{expected_name}', not '{fn.__name__}'.""
                )
",if fn . __name__ != expected_name :,189
"def split_ratio(row):
    if float(row[""Numerator""]) > 0:
        if "":"" in row[""Splitratio""]:
            n, m = row[""Splitratio""].split("":"")
            return float(m) / float(n)
        else:
            return eval(row[""Splitratio""])
    else:
        return 1
","if "":"" in row [ ""Splitratio"" ] :",87
"def _handle_def_errors(testdef):
    # If the test generation had an error, raise
    if testdef.error:
        if testdef.exception:
            if isinstance(testdef.exception, Exception):
                raise testdef.exception
            else:
                raise Exception(testdef.exception)
        else:
            raise Exception(""Test parse failure"")
","if isinstance ( testdef . exception , Exception ) :",100
"def _get_quota_availability(self):
    quotas_ok = defaultdict(int)
    qa = QuotaAvailability()
    qa.queue(*[k for k, v in self._quota_diff.items() if v > 0])
    qa.compute(now_dt=self.now_dt)
    for quota, count in self._quota_diff.items():
        if count <= 0:
            quotas_ok[quota] = 0
            break
        avail = qa.results[quota]
        if avail[1] is not None and avail[1] < count:
            quotas_ok[quota] = min(count, avail[1])
        else:
            quotas_ok[quota] = count
    return quotas_ok
",if avail [ 1 ] is not None and avail [ 1 ] < count :,182
"def reverse(self):
    """"""Reverse *IN PLACE*.""""""
    li = self.leftindex
    lb = self.leftblock
    ri = self.rightindex
    rb = self.rightblock
    for i in range(self.len >> 1):
        lb.data[li], rb.data[ri] = rb.data[ri], lb.data[li]
        li += 1
        if li >= BLOCKLEN:
            lb = lb.rightlink
            li = 0
        ri -= 1
        if ri < 0:
            rb = rb.leftlink
            ri = BLOCKLEN - 1
",if li >= BLOCKLEN :,156
"def __manipulate_item(self, item):
    if self._Cursor__manipulate:
        db = self._Cursor__collection.database
        son = db._fix_outgoing(item, self._Cursor__collection)
    else:
        son = item
    if self.__wrap is not None:
        if self.__wrap.type_field in son:
            return getattr(self._Cursor__collection, son[self.__wrap.type_field])(son)
        return self.__wrap(son, collection=self._Cursor__collection)
    else:
        return son
",if self . __wrap . type_field in son :,140
"def apply_transforms(self):
    """"""Apply all of the stored transforms, in priority order.""""""
    self.document.reporter.attach_observer(self.document.note_transform_message)
    while self.transforms:
        if not self.sorted:
            # Unsorted initially, and whenever a transform is added.
            self.transforms.sort()
            self.transforms.reverse()
            self.sorted = 1
        priority, transform_class, pending, kwargs = self.transforms.pop()
        transform = transform_class(self.document, startnode=pending)
        transform.apply(**kwargs)
        self.applied.append((priority, transform_class, pending, kwargs))
",if not self . sorted :,169
"def format_sql(sql, params):
    rv = []
    if isinstance(params, dict):
        # convert sql with named parameters to sql with unnamed parameters
        conv = _FormatConverter(params)
        if params:
            sql = sql_to_string(sql)
            sql = sql % conv
            params = conv.params
        else:
            params = ()
    for param in params or ():
        if param is None:
            rv.append(""NULL"")
        param = safe_repr(param)
        rv.append(param)
    return sql, rv
",if params :,151
"def on_execution_item(self, cpath, execution):
    if not isinstance(execution, dict):
        return
    if ""executor"" in execution and execution.get(""executor"") != ""jmeter"":
        return
    scenario = execution.get(""scenario"", None)
    if not scenario:
        return
    if isinstance(scenario, str):
        scenario_name = scenario
        scenario = self.get_named_scenario(scenario_name)
        if not scenario:
            scenario = None
        scenario_path = Path(""scenarios"", scenario_name)
    else:
        scenario_path = cpath.copy()
        scenario_path.add_component(""scenario"")
    if scenario is not None:
        self.check_jmeter_scenario(scenario_path, scenario)
",if not scenario :,192
"def _poll_ipc_requests(self) -> None:
    try:
        if self._ipc_requests.empty():
            return
        while not self._ipc_requests.empty():
            args = self._ipc_requests.get()
            try:
                for filename in args:
                    if os.path.isfile(filename):
                        self.get_editor_notebook().show_file(filename)
            except Exception as e:
                logger.exception(""Problem processing ipc request"", exc_info=e)
        self.become_active_window()
    finally:
        self.after(50, self._poll_ipc_requests)
",if os . path . isfile ( filename ) :,180
"def get_scroll_distance_to_element(driver, element):
    try:
        scroll_position = driver.execute_script(""return window.scrollY;"")
        element_location = None
        element_location = element.location[""y""]
        element_location = element_location - 130
        if element_location < 0:
            element_location = 0
        distance = element_location - scroll_position
        return distance
    except Exception:
        return 0
",if element_location < 0 :,118
"def TryMerge(self, d):
    while d.avail() > 0:
        tt = d.getVarInt32()
        if tt == 10:
            self.set_access_token(d.getPrefixedString())
            continue
        if tt == 16:
            self.set_expiration_time(d.getVarInt64())
            continue
        if tt == 0:
            raise ProtocolBuffer.ProtocolBufferDecodeError
        d.skipData(tt)
",if tt == 16 :,124
"def _validate_and_define(params, key, value):
    (key, force_generic) = _validate_key(_unescape(key))
    if key in params:
        raise SyntaxError(f'duplicate key ""{key}""')
    cls = _class_for_key.get(key, GenericParam)
    emptiness = cls.emptiness()
    if value is None:
        if emptiness == Emptiness.NEVER:
            raise SyntaxError(""value cannot be empty"")
        value = cls.from_value(value)
    else:
        if force_generic:
            value = cls.from_wire_parser(dns.wire.Parser(_unescape(value)))
        else:
            value = cls.from_value(value)
    params[key] = value
",if force_generic :,194
"def iter_fields(node, *, include_meta=True, exclude_unset=False):
    exclude_meta = not include_meta
    for field_name, field in node._fields.items():
        if exclude_meta and field.meta:
            continue
        field_val = getattr(node, field_name, _marker)
        if field_val is _marker:
            continue
        if exclude_unset:
            if callable(field.default):
                default = field.default()
            else:
                default = field.default
            if field_val == default:
                continue
        yield field_name, field_val
",if exclude_meta and field . meta :,171
"def tearDown(self):
    """"""Shutdown the server.""""""
    try:
        if self.server:
            self.server.stop()
        if self.sl_hdlr:
            self.root_logger.removeHandler(self.sl_hdlr)
            self.sl_hdlr.close()
    finally:
        BaseTest.tearDown(self)
",if self . sl_hdlr :,92
"def _wait_for_async_copy(self, share_name, file_path):
    count = 0
    share_client = self.fsc.get_share_client(share_name)
    file_client = share_client.get_file_client(file_path)
    properties = file_client.get_file_properties()
    while properties.copy.status != ""success"":
        count = count + 1
        if count > 10:
            self.fail(""Timed out waiting for async copy to complete."")
        self.sleep(6)
        properties = file_client.get_file_properties()
    self.assertEqual(properties.copy.status, ""success"")
",if count > 10 :,165
"def __new__(
    cls,
    message_type: OrderBookMessageType,
    content: Dict[str, any],
    timestamp: Optional[float] = None,
    *args,
    **kwargs,
):
    if timestamp is None:
        if message_type is OrderBookMessageType.SNAPSHOT:
            raise ValueError(
                ""timestamp must not be None when initializing snapshot messages.""
            )
        timestamp = int(time.time())
    return super(KucoinOrderBookMessage, cls).__new__(
        cls, message_type, content, timestamp=timestamp, *args, **kwargs
    )
",if message_type is OrderBookMessageType . SNAPSHOT :,152
"def _drop_unique_features(
    X: DataFrame, feature_metadata: FeatureMetadata, max_unique_ratio
) -> list:
    features_to_drop = []
    X_len = len(X)
    max_unique_value_count = X_len * max_unique_ratio
    for column in X:
        unique_value_count = len(X[column].unique())
        if unique_value_count == 1:
            features_to_drop.append(column)
        elif feature_metadata.get_feature_type_raw(column) in [
            R_CATEGORY,
            R_OBJECT,
        ] and (unique_value_count > max_unique_value_count):
            features_to_drop.append(column)
    return features_to_drop
",if unique_value_count == 1 :,197
"def get_src_findex_by_pad(s, S, padding_mode, align_corners):
    if padding_mode == ""zero"":
        return get_src_findex_with_zero_pad(s, S)
    elif padding_mode == ""reflect"":
        if align_corners:
            return get_src_findex_with_reflect_pad(s, S, True)
        else:
            sf = get_src_findex_with_reflect_pad(s, S, False)
            return get_src_findex_with_repeat_pad(sf, S)
    elif padding_mode == ""repeat"":
        return get_src_findex_with_repeat_pad(s, S)
",if align_corners :,175
"def _iterate_self_and_parents(self, upto=None):
    current = self
    result = ()
    while current:
        result += (current,)
        if current._parent is upto:
            break
        elif current._parent is None:
            raise sa_exc.InvalidRequestError(
                ""Transaction %s is not on the active transaction list"" % (upto)
            )
        else:
            current = current._parent
    return result
",elif current . _parent is None :,126
"def __setattr__(self, name: str, val: Any):
    if name.startswith(""COMPUTED_""):
        if name in self:
            old_val = self[name]
            if old_val == val:
                return
            raise KeyError(
                ""Computed attributed '{}' already exists ""
                ""with a different value! old={}, new={}."".format(name, old_val, val)
            )
        self[name] = val
    else:
        super().__setattr__(name, val)
",if name in self :,137
"def get_fnlist(bbhandler, pkg_pn, preferred):
    """"""Get all recipe file names""""""
    if preferred:
        (latest_versions, preferred_versions) = bb.providers.findProviders(
            bbhandler.config_data, bbhandler.cooker.recipecaches[""""], pkg_pn
        )
    fn_list = []
    for pn in sorted(pkg_pn):
        if preferred:
            fn_list.append(preferred_versions[pn][1])
        else:
            fn_list.extend(pkg_pn[pn])
    return fn_list
",if preferred :,150
"def links_extracted(self, _, links):
    links_deduped = {}
    for link in links:
        link_fingerprint = link.meta[FIELD_FINGERPRINT]
        if link_fingerprint in links_deduped:
            continue
        links_deduped[link_fingerprint] = link
    [
        self._redis_pipeline.hmset(fingerprint, self._create_link_extracted(link))
        for (fingerprint, link) in links_deduped.items()
    ]
    self._redis_pipeline.execute()
",if link_fingerprint in links_deduped :,138
"def __call__(self, name, rawtext, text, lineno, inliner, options=None, content=None):
    options = options or {}
    content = content or []
    issue_nos = [each.strip() for each in utils.unescape(text).split("","")]
    config = inliner.document.settings.env.app.config
    ret = []
    for i, issue_no in enumerate(issue_nos):
        node = self.make_node(name, issue_no, config, options=options)
        ret.append(node)
        if i != len(issue_nos) - 1:
            sep = nodes.raw(text="", "", format=""html"")
            ret.append(sep)
    return ret, []
",if i != len ( issue_nos ) - 1 :,176
"def init_messengers(messengers):
    for messenger in messengers:
        if ""."" in messenger[""type""]:
            module_path = messenger[""type""]
            messenger[""type""] = messenger[""type""].split(""."")[-1]
        else:
            module_path = ""oncall.messengers."" + messenger[""type""]
        instance = getattr(importlib.import_module(module_path), messenger[""type""])(
            messenger
        )
        for transport in instance.supports:
            _active_messengers[transport].append(instance)
","if ""."" in messenger [ ""type"" ] :",141
"def _process_enum_definition(self, tok):
    fields = []
    for field in tok.fields:
        if field.expression:
            expression = self.expression_parser.parse(field.expression)
        else:
            expression = None
        fields.append(c_ast.CEnumField(name=field.name.first, value=expression))
    name = tok.enum_name
    if name:
        name = ""enum %s"" % tok.enum_name.first
    else:
        name = self._make_anonymous_type(""enum"")
    return c_ast.CTypeDefinition(
        name=name,
        type_definition=c_ast.CEnum(
            attributes=tok.attributes, fields=fields, name=name
        ),
    )
",if field . expression :,199
"def result_iterator():
    try:
        # reverse to keep finishing order
        fs.reverse()
        while fs:
            # Careful not to keep a reference to the popped future
            if timeout is None:
                yield fs.pop().result()
            else:
                yield fs.pop().result(end_time - time.time())
    finally:
        for future in fs:
            future.cancel()
",if timeout is None :,117
"def has_encrypted_ssh_key_data(self):
    try:
        ssh_key_data = self.get_input(""ssh_key_data"")
    except AttributeError:
        return False
    try:
        pem_objects = validate_ssh_private_key(ssh_key_data)
        for pem_object in pem_objects:
            if pem_object.get(""key_enc"", False):
                return True
    except ValidationError:
        pass
    return False
","if pem_object . get ( ""key_enc"" , False ) :",123
"def test_seq_object_transcription_method(self):
    for nucleotide_seq in test_seqs:
        if isinstance(nucleotide_seq, Seq.Seq):
            self.assertEqual(
                repr(Seq.transcribe(nucleotide_seq)),
                repr(nucleotide_seq.transcribe()),
            )
","if isinstance ( nucleotide_seq , Seq . Seq ) :",96
"def max_elevation(self):
    max_el = None
    for y in xrange(self.height):
        for x in xrange(self.width):
            el = self.elevation[""data""][y][x]
            if max_el is None or el > max_el:
                max_el = el
    return max_el
",if max_el is None or el > max_el :,91
"def stress(mapping, index):
    for count in range(OPERATIONS):
        function = random.choice(functions)
        function(mapping, index)
        if count % 1000 == 0:
            print(""\r"", len(mapping), "" "" * 7, end="""")
    print()
",if count % 1000 == 0 :,72
"def sync_terminology(self):
    if self.is_source:
        return
    store = self.store
    missing = []
    for source in self.component.get_all_sources():
        if ""terminology"" not in source.all_flags:
            continue
        try:
            _unit, add = store.find_unit(source.context, source.source)
        except UnitNotFound:
            add = True
        # Unit is already present
        if not add:
            continue
        missing.append((source.context, source.source, """"))
    if missing:
        self.add_units(None, missing)
",if not add :,166
"def get_generators(self):
    """"""Get a dict with all registered generators, indexed by name""""""
    generators = {}
    for core in self.db.find():
        if hasattr(core, ""get_generators""):
            _generators = core.get_generators({})
            if _generators:
                generators[str(core.name)] = _generators
    return generators
","if hasattr ( core , ""get_generators"" ) :",93
"def act(self, state):
    if self.body.env.clock.frame < self.training_start_step:
        return policy_util.random(state, self, self.body).cpu().squeeze().numpy()
    else:
        action = self.action_policy(state, self, self.body)
        if not self.body.is_discrete:
            action = self.scale_action(torch.tanh(action))  # continuous action bound
        return action.cpu().squeeze().numpy()
",if not self . body . is_discrete :,124
"def try_open_completions_event(self, event=None):
    ""(./) Open completion list after pause with no movement.""
    lastchar = self.text.get(""insert-1c"")
    if lastchar in TRIGGERS:
        args = TRY_A if lastchar == ""."" else TRY_F
        self._delayed_completion_index = self.text.index(""insert"")
        if self._delayed_completion_id is not None:
            self.text.after_cancel(self._delayed_completion_id)
        self._delayed_completion_id = self.text.after(
            self.popupwait, self._delayed_open_completions, args
        )
",if self . _delayed_completion_id is not None :,167
"def token_is_available(self):
    if self.token:
        try:
            resp = requests.get(
                ""https://api.shodan.io/account/profile?key={0}"".format(self.token)
            )
            if resp and resp.status_code == 200 and ""member"" in resp.json():
                return True
        except Exception as ex:
            logger.error(str(ex))
    return False
","if resp and resp . status_code == 200 and ""member"" in resp . json ( ) :",121
"def next_bar_(self, event):
    bars = event.bar_dict
    self._current_minute = self._minutes_since_midnight(
        self.ucontext.now.hour, self.ucontext.now.minute
    )
    for day_rule, time_rule, func in self._registry:
        if day_rule() and time_rule():
            with ExecutionContext(EXECUTION_PHASE.SCHEDULED):
                with ModifyExceptionFromType(EXC_TYPE.USER_EXC):
                    func(self.ucontext, bars)
    self._last_minute = self._current_minute
",if day_rule ( ) and time_rule ( ) :,156
"def decoder(s):
    r = []
    decode = []
    for c in s:
        if c == ""&"" and not decode:
            decode.append(""&"")
        elif c == ""-"" and decode:
            if len(decode) == 1:
                r.append(""&"")
            else:
                r.append(modified_unbase64("""".join(decode[1:])))
            decode = []
        elif decode:
            decode.append(c)
        else:
            r.append(c)
    if decode:
        r.append(modified_unbase64("""".join(decode[1:])))
    bin_str = """".join(r)
    return (bin_str, len(s))
","elif c == ""-"" and decode :",188
"def admin_audit_get(admin_id):
    if settings.app.demo_mode:
        resp = utils.demo_get_cache()
        if resp:
            return utils.jsonify(resp)
    if not flask.g.administrator.super_user:
        return utils.jsonify(
            {
                ""error"": REQUIRES_SUPER_USER,
                ""error_msg"": REQUIRES_SUPER_USER_MSG,
            },
            400,
        )
    admin = auth.get_by_id(admin_id)
    resp = admin.get_audit_events()
    if settings.app.demo_mode:
        utils.demo_set_cache(resp)
    return utils.jsonify(resp)
",if resp :,190
"def vjp(self, argnum, outgrad, ans, vs, gvs, args, kwargs):
    try:
        return self.vjps[argnum](outgrad, ans, vs, gvs, *args, **kwargs)
    except KeyError:
        if self.vjps == {}:
            errstr = ""Gradient of {0} not yet implemented.""
        else:
            errstr = ""Gradient of {0} w.r.t. arg number {1} not yet implemented.""
        raise NotImplementedError(errstr.format(self.fun.__name__, argnum))
",if self . vjps == { } :,143
"def update(self, *args, **kwargs):
    assert not self.readonly
    longest_key = 0
    _dict = self._dict
    reverse = self.reverse
    casereverse = self.casereverse
    for iterable in args + (kwargs,):
        if isinstance(iterable, (dict, StenoDictionary)):
            iterable = iterable.items()
        for key, value in iterable:
            longest_key = max(longest_key, len(key))
            _dict[key] = value
            reverse[value].append(key)
            casereverse[value.lower()][value] += 1
    self._longest_key = max(self._longest_key, longest_key)
","if isinstance ( iterable , ( dict , StenoDictionary ) ) :",172
"def update_ui(self, window):
    view = window.get_active_view()
    self.set_status(view)
    lang = ""plain_text""
    if view:
        buf = view.get_buffer()
        language = buf.get_language()
        if language:
            lang = language.get_id()
        self.setup_smart_indent(view, lang)
",if language :,101
"def number_operators(self, a, b, skip=[]):
    dict = {""a"": a, ""b"": b}
    for name, expr in self.binops.items():
        if name not in skip:
            name = ""__%s__"" % name
            if hasattr(a, name):
                res = eval(expr, dict)
                self.binop_test(a, b, res, expr, name)
    for name, expr in self.unops.items():
        if name not in skip:
            name = ""__%s__"" % name
            if hasattr(a, name):
                res = eval(expr, dict)
                self.unop_test(a, res, expr, name)
","if hasattr ( a , name ) :",187
"def _getItemHeight(self, item, ctrl=None):
    """"""Returns the full height of the item to be inserted in the form""""""
    if type(ctrl) == psychopy.visual.TextBox2:
        return ctrl.size[1]
    if type(ctrl) == psychopy.visual.Slider:
        # Set radio button layout
        if item[""layout""] == ""horiz"":
            return 0.03 + ctrl.labelHeight * 3
        elif item[""layout""] == ""vert"":
            # for vertical take into account the nOptions
            return ctrl.labelHeight * len(item[""options""])
","if item [ ""layout"" ] == ""horiz"" :",155
"def test_cleanup_params(self, body, rpc_mock):
    res = self._get_resp_post(body)
    self.assertEqual(http_client.ACCEPTED, res.status_code)
    rpc_mock.assert_called_once_with(self.context, mock.ANY)
    cleanup_request = rpc_mock.call_args[0][1]
    for key, value in body.items():
        if key in (""disabled"", ""is_up""):
            if value is not None:
                value = value == ""true""
        self.assertEqual(value, getattr(cleanup_request, key))
    self.assertEqual(self._expected_services(*SERVICES), res.json)
",if value is not None :,177
"def _read_json_content(self, body_is_optional=False):
    if ""content-length"" not in self.headers:
        return self.send_error(411) if not body_is_optional else {}
    try:
        content_length = int(self.headers.get(""content-length""))
        if content_length == 0 and body_is_optional:
            return {}
        request = json.loads(self.rfile.read(content_length).decode(""utf-8""))
        if isinstance(request, dict) and (request or body_is_optional):
            return request
    except Exception:
        logger.exception(""Bad request"")
    self.send_error(400)
","if isinstance ( request , dict ) and ( request or body_is_optional ) :",176
"def env_purge_doc(app: Sphinx, env: BuildEnvironment, docname: str) -> None:
    modules = getattr(env, ""_viewcode_modules"", {})
    for modname, entry in list(modules.items()):
        if entry is False:
            continue
        code, tags, used, refname = entry
        for fullname in list(used):
            if used[fullname] == docname:
                used.pop(fullname)
        if len(used) == 0:
            modules.pop(modname)
",if entry is False :,133
"def frames(self):
    """"""an array of all the frames (including iframes) in the current window""""""
    from thug.DOM.W3C.HTML.HTMLCollection import HTMLCollection
    frames = set()
    for frame in self._findAll([""frame"", ""iframe""]):
        if not getattr(frame, ""_node"", None):
            from thug.DOM.W3C.Core.DOMImplementation import DOMImplementation
            DOMImplementation.createHTMLElement(self.window.doc, frame)
        frames.add(frame._node)
    return HTMLCollection(self.doc, list(frames))
","if not getattr ( frame , ""_node"" , None ) :",145
"def check(self, **kw):
    if not kw:
        return exists(self.strpath)
    if len(kw) == 1:
        if ""dir"" in kw:
            return not kw[""dir""] ^ isdir(self.strpath)
        if ""file"" in kw:
            return not kw[""file""] ^ isfile(self.strpath)
    return super(LocalPath, self).check(**kw)
","if ""dir"" in kw :",106
"def __init__(self, folders):
    self.folders = folders
    self.duplicates = {}
    for folder, path in folders.items():
        duplicates = []
        for other_folder, other_path in folders.items():
            if other_folder == folder:
                continue
            if other_path == path:
                duplicates.append(other_folder)
        if len(duplicates):
            self.duplicates[folder] = duplicates
",if other_folder == folder :,117
"def next(self, buf, pos):
    if pos >= len(buf):
        return EOF, """", pos
    mo = self.tokens_re.match(buf, pos)
    if mo:
        text = mo.group()
        type, regexp, test_lit = self.tokens[mo.lastindex - 1]
        pos = mo.end()
        if test_lit:
            type = self.literals.get(text, type)
        return type, text, pos
    else:
        c = buf[pos]
        return self.symbols.get(c, None), c, pos + 1
",if test_lit :,153
"def step(self, action):
    """"""Repeat action, sum reward, and max over last observations.""""""
    total_reward = 0.0
    done = None
    for i in range(self._skip):
        obs, reward, done, info = self.env.step(action)
        if i == self._skip - 2:
            self._obs_buffer[0] = obs
        if i == self._skip - 1:
            self._obs_buffer[1] = obs
        total_reward += reward
        if done:
            break
    # Note that the observation on the done=True frame
    # doesn't matter
    max_frame = self._obs_buffer.max(axis=0)
    return max_frame, total_reward, done, info
",if i == self . _skip - 2 :,189
"def convert(self, ctx, argument):
    arg = argument.replace(""0x"", """").lower()
    if arg[0] == ""#"":
        arg = arg[1:]
    try:
        value = int(arg, base=16)
        if not (0 <= value <= 0xFFFFFF):
            raise BadColourArgument(arg)
        return discord.Colour(value=value)
    except ValueError:
        arg = arg.replace("" "", ""_"")
        method = getattr(discord.Colour, arg, None)
        if arg.startswith(""from_"") or method is None or not inspect.ismethod(method):
            raise BadColourArgument(arg)
        return method()
","if arg . startswith ( ""from_"" ) or method is None or not inspect . ismethod ( method ) :",172
"def run(self, **inputs):
    if self.inputs.copy_inputs:
        self.inputs.subjects_dir = os.getcwd()
        if ""subjects_dir"" in inputs:
            inputs[""subjects_dir""] = self.inputs.subjects_dir
        for originalfile in [self.inputs.in_file, self.inputs.in_norm]:
            copy2subjdir(self, originalfile, folder=""mri"")
    return super(SegmentCC, self).run(**inputs)
","if ""subjects_dir"" in inputs :",122
"def get_queryset(self):
    if not hasattr(self, ""_queryset""):
        if self.queryset is not None:
            qs = self.queryset
        else:
            qs = self.model._default_manager.get_queryset()
        # If the queryset isn't already ordered we need to add an
        # artificial ordering here to make sure that all formsets
        # constructed from this queryset have the same form order.
        if not qs.ordered:
            qs = qs.order_by(self.model._meta.pk.name)
        # Removed queryset limiting here. As per discussion re: #13023
        # on django-dev, max_num should not prevent existing
        # related objects/inlines from being displayed.
        self._queryset = qs
    return self._queryset
",if self . queryset is not None :,197
"def visit_simple_stmt(self, node: Node) -> Iterator[Line]:
    """"""Visit a statement without nested statements.""""""
    is_suite_like = node.parent and node.parent.type in STATEMENT
    if is_suite_like:
        if self.is_pyi and is_stub_body(node):
            yield from self.visit_default(node)
        else:
            yield from self.line(+1)
            yield from self.visit_default(node)
            yield from self.line(-1)
    else:
        if not self.is_pyi or not node.parent or not is_stub_suite(node.parent):
            yield from self.line()
        yield from self.visit_default(node)
",if self . is_pyi and is_stub_body ( node ) :,189
"def rawDataReceived(self, data):
    if self.timeout > 0:
        self.resetTimeout()
    self._pendingSize -= len(data)
    if self._pendingSize > 0:
        self._pendingBuffer.write(data)
    else:
        passon = b""""
        if self._pendingSize < 0:
            data, passon = data[: self._pendingSize], data[self._pendingSize :]
        self._pendingBuffer.write(data)
        rest = self._pendingBuffer
        self._pendingBuffer = None
        self._pendingSize = None
        rest.seek(0, 0)
        self._parts.append(rest.read())
        self.setLineMode(passon.lstrip(b""\r\n""))
",if self . _pendingSize < 0 :,189
"def handle(self, *args, **options):
    app_name = options.get(""app_name"")
    job_name = options.get(""job_name"")
    # hack since we are using job_name nargs='?' for -l to work
    if app_name and not job_name:
        job_name = app_name
        app_name = None
    if options.get(""list_jobs""):
        print_jobs(only_scheduled=False, show_when=True, show_appname=True)
    else:
        if not job_name:
            print(""Run a single maintenance job. Please specify the name of the job."")
            return
        self.runjob(app_name, job_name, options)
",if not job_name :,184
"def _exportReceived(self, content, error=False, server=None, context={}, **kwargs):
    if error:
        if content:
            self.error.emit(content[""message""], True)
        else:
            self.error.emit(""Can't export the project from the server"", True)
        self.finished.emit()
        return
    self.finished.emit()
",if content :,97
"def __iter__(self):
    n = self.n
    k = self.k
    j = int(np.ceil(n / k))
    for i in range(k):
        test_index = np.zeros(n, dtype=bool)
        if i < k - 1:
            test_index[i * j : (i + 1) * j] = True
        else:
            test_index[i * j :] = True
        train_index = np.logical_not(test_index)
        yield train_index, test_index
",if i < k - 1 :,140
"def addType(self, graphene_type):
    meta = get_meta(graphene_type)
    if meta:
        if not graphene_type in self._typeMap:
            self._typeMap[meta.name] = graphene_type
        else:
            raise Exception(
                ""Type {typeName} already exists in the registry."".format(
                    typeName=meta.name
                )
            )
    else:
        raise Exception(""Cannot add unnamed type or a non-type to registry."")
",if not graphene_type in self . _typeMap :,135
"def test_len(self):
    eq = self.assertEqual
    eq(base64MIME.base64_len(""hello""), len(base64MIME.encode(""hello"", eol="""")))
    for size in range(15):
        if size == 0:
            bsize = 0
        elif size <= 3:
            bsize = 4
        elif size <= 6:
            bsize = 8
        elif size <= 9:
            bsize = 12
        elif size <= 12:
            bsize = 16
        else:
            bsize = 20
        eq(base64MIME.base64_len(""x"" * size), bsize)
",elif size <= 6 :,160
"def _asStringList(self, sep=""""):
    out = []
    for item in self._toklist:
        if out and sep:
            out.append(sep)
        if isinstance(item, ParseResults):
            out += item._asStringList()
        else:
            out.append(str(item))
    return out
",if out and sep :,88
"def open_file_input(cli_parsed):
    files = glob.glob(os.path.join(cli_parsed.d, ""*report.html""))
    if len(files) > 0:
        print(""\n[*] Done! Report written in the "" + cli_parsed.d + "" folder!"")
        print(""Would you like to open the report now? [Y/n]"")
        while True:
            try:
                response = input().lower()
                if response == """":
                    return True
                else:
                    return strtobool(response)
            except ValueError:
                print(""Please respond with y or n"")
    else:
        print(""[*] No report files found to open, perhaps no hosts were successful"")
        return False
","if response == """" :",200
"def init_values(self):
    config = self._raw_config
    for valname, value in self.overrides.iteritems():
        if ""."" in valname:
            realvalname, key = valname.split(""."", 1)
            config.setdefault(realvalname, {})[key] = value
        else:
            config[valname] = value
    for name in config:
        if name in self.values:
            self.__dict__[name] = config[name]
    del self._raw_config
","if ""."" in valname :",131
"def get_result(self):
    result_list = []
    exc_info = None
    for f in self.children:
        try:
            result_list.append(f.get_result())
        except Exception as e:
            if exc_info is None:
                exc_info = sys.exc_info()
            else:
                if not isinstance(e, self.quiet_exceptions):
                    app_log.error(""Multiple exceptions in yield list"", exc_info=True)
    if exc_info is not None:
        raise_exc_info(exc_info)
    if self.keys is not None:
        return dict(zip(self.keys, result_list))
    else:
        return list(result_list)
",if exc_info is None :,196
"def test01e_json(self):
    ""Testing GeoJSON input/output.""
    if not GEOJSON:
        return
    for g in self.geometries.json_geoms:
        geom = OGRGeometry(g.wkt)
        if not hasattr(g, ""not_equal""):
            self.assertEqual(g.json, geom.json)
            self.assertEqual(g.json, geom.geojson)
        self.assertEqual(OGRGeometry(g.wkt), OGRGeometry(geom.json))
","if not hasattr ( g , ""not_equal"" ) :",133
"def __init__(self, hub=None):  # pylint: disable=unused-argument
    if resolver._resolver is None:
        _resolver = resolver._resolver = _DualResolver()
        if config.resolver_nameservers:
            _resolver.network_resolver.nameservers[:] = config.resolver_nameservers
        if config.resolver_timeout:
            _resolver.network_resolver.lifetime = config.resolver_timeout
    # Different hubs in different threads could be sharing the same
    # resolver.
    assert isinstance(resolver._resolver, _DualResolver)
    self._resolver = resolver._resolver
",if config . resolver_nameservers :,147
"def __iadd__(self, term):
    if isinstance(term, (int, long)):
        if 0 <= term < 65536:
            _gmp.mpz_add_ui(self._mpz_p, self._mpz_p, c_ulong(term))
            return self
        if -65535 < term < 0:
            _gmp.mpz_sub_ui(self._mpz_p, self._mpz_p, c_ulong(-term))
            return self
        term = Integer(term)
    _gmp.mpz_add(self._mpz_p, self._mpz_p, term._mpz_p)
    return self
",if 0 <= term < 65536 :,170
"def copy(dst, src):
    for (k, v) in src.iteritems():
        if isinstance(v, dict):
            d = {}
            dst[k] = d
            copy(d, v)
        else:
            dst[k] = v
","if isinstance ( v , dict ) :",73
"def generator(self, data):
    self.procs = OrderedDict()
    for task in data:
        self.recurse_task(task, 0, 0, self.procs)
    for offset, name, level, pid, ppid, uid, euid, gid in self.procs.values():
        if offset:
            yield (
                0,
                [
                    Address(offset),
                    str(name),
                    str(level),
                    int(pid),
                    int(ppid),
                    int(uid),
                    int(gid),
                    int(euid),
                ],
            )
",if offset :,186
"def apply(self, db, person):
    families = person.get_parent_family_handle_list()
    if families == []:
        return True
    for family_handle in person.get_parent_family_handle_list():
        family = db.get_family_from_handle(family_handle)
        if family:
            father_handle = family.get_father_handle()
            mother_handle = family.get_mother_handle()
            if not father_handle:
                return True
            if not mother_handle:
                return True
    return False
",if family :,157
"def _arctic_task_exec(request):
    request.start_time = time.time()
    logging.debug(
        ""Executing asynchronous request for {}/{}"".format(
            request.library, request.symbol
        )
    )
    result = None
    try:
        request.is_running = True
        if request.mongo_retry:
            result = mongo_retry(request.fun)(*request.args, **request.kwargs)
        else:
            result = request.fun(*request.args, **request.kwargs)
    except Exception as e:
        request.exception = e
    finally:
        request.data = result
        request.end_time = time.time()
        request.is_running = False
    return result
",if request . mongo_retry :,191
"def _setup_styles(self):
    for ttype, ndef in self.style:
        escape = EscapeSequence()
        if ndef[""color""]:
            escape.fg = self._color_index(ndef[""color""])
        if ndef[""bgcolor""]:
            escape.bg = self._color_index(ndef[""bgcolor""])
        if self.usebold and ndef[""bold""]:
            escape.bold = True
        if self.useunderline and ndef[""underline""]:
            escape.underline = True
        self.style_string[str(ttype)] = (escape.color_string(), escape.reset_string())
","if ndef [ ""color"" ] :",156
"def process_string(self, remove_repetitions, sequence):
    string = """"
    for i, char in enumerate(sequence):
        if char != self.int_to_char[self.blank_index]:
            # if this char is a repetition and remove_repetitions=true,
            # skip.
            if remove_repetitions and i != 0 and char == sequence[i - 1]:
                pass
            elif char == self.labels[self.space_index]:
                string += "" ""
            else:
                string = string + char
    return string
",elif char == self . labels [ self . space_index ] :,152
"def arith_expr(self, nodelist):
    node = self.com_node(nodelist[0])
    for i in range(2, len(nodelist), 2):
        right = self.com_node(nodelist[i])
        if nodelist[i - 1].type == token.PLUS:
            node = Add(node, right, lineno=nodelist[1].context)
        elif nodelist[i - 1].type == token.MINUS:
            node = Sub(node, right, lineno=nodelist[1].context)
        else:
            raise ValueError(""unexpected token: %s"" % nodelist[i - 1][0])
    return node
",if nodelist [ i - 1 ] . type == token . PLUS :,160
"def invert_index(cls, index, length):
    if np.isscalar(index):
        return length - index
    elif isinstance(index, slice):
        start, stop = index.start, index.stop
        new_start, new_stop = None, None
        if start is not None:
            new_stop = length - start
        if stop is not None:
            new_start = length - stop
        return slice(new_start - 1, new_stop - 1)
    elif isinstance(index, Iterable):
        new_index = []
        for ind in index:
            new_index.append(length - ind)
    return new_index
",if start is not None :,168
"def getRoots(job):
    if job not in visited:
        visited.add(job)
        if len(job._directPredecessors) > 0:
            list(map(lambda p: getRoots(p), job._directPredecessors))
        else:
            roots.add(job)
        # The following call ensures we explore all successor edges.
        list(map(lambda c: getRoots(c), job._children + job._followOns))
",if len ( job . _directPredecessors ) > 0 :,120
"def visit_filter_projection(self, node, value):
    base = self.visit(node[""children""][0], value)
    if not isinstance(base, list):
        return None
    comparator_node = node[""children""][2]
    collected = []
    for element in base:
        if self._is_true(self.visit(comparator_node, element)):
            current = self.visit(node[""children""][1], element)
            if current is not None:
                collected.append(current)
    return collected
","if self . _is_true ( self . visit ( comparator_node , element ) ) :",132
"def func(x, y):
    try:
        if x > y:
            z = x + 2 * math.sin(y)
            return z ** 2
        elif x == y:
            return 4
        else:
            return 2 ** 3
    except ValueError:
        foo = 0
        for i in range(4):
            foo += i
        return foo
    except TypeError:
        return 42
    else:
        return 33
    finally:
        print(""finished"")
",elif x == y :,134
"def set_filter(self, dataset_opt):
    """"""This function create and set the pre_filter to the obj as attributes""""""
    self.pre_filter = None
    for key_name in dataset_opt.keys():
        if ""filter"" in key_name:
            new_name = key_name.replace(""filters"", ""filter"")
            try:
                filt = instantiate_filters(getattr(dataset_opt, key_name))
            except Exception:
                log.exception(
                    ""Error trying to create {}, {}"".format(
                        new_name, getattr(dataset_opt, key_name)
                    )
                )
                continue
            setattr(self, new_name, filt)
","if ""filter"" in key_name :",195
"def _add_states_to_lookup(
    self, trackers_as_states, trackers_as_actions, domain, online=False
):
    """"""Add states to lookup dict""""""
    for states in trackers_as_states:
        active_form = self._get_active_form_name(states[-1])
        if active_form and self._prev_action_listen_in_state(states[-1]):
            # modify the states
            states = self._modified_states(states)
            feature_key = self._create_feature_key(states)
            # even if there are two identical feature keys
            # their form will be the same
            # because of `active_form_...` feature
            self.lookup[feature_key] = active_form
",if active_form and self . _prev_action_listen_in_state ( states [ - 1 ] ) :,192
"def list_loaded_payloads(self):
    print(helpers.color(""\n [*] Available Payloads:\n""))
    lastBase = None
    x = 1
    for name in sorted(self.active_payloads.keys()):
        parts = name.split(""/"")
        if lastBase and parts[0] != lastBase:
            print()
        lastBase = parts[0]
        print(""\t%s)\t%s"" % (x, ""{0: <24}"".format(name)))
        x += 1
    print(""\n"")
    return
",if lastBase and parts [ 0 ] != lastBase :,136
"def reprSmart(vw, item):
    ptype = type(item)
    if ptype is int:
        if -1024 < item < 1024:
            return str(item)
        elif vw.isValidPointer(item):
            return vw.reprPointer(item)
        else:
            return hex(item)
    elif ptype in (list, tuple):
        return reprComplex(vw, item)  # recurse
    elif ptype is dict:
        return ""{%s}"" % "","".join(
            [""%s:%s"" % (reprSmart(vw, k), reprSmart(vw, v)) for k, v in item.items()]
        )
    else:
        return repr(item)
",elif vw . isValidPointer ( item ) :,183
"def ConfigSectionMap(section):
    config = ConfigParser.RawConfigParser()
    configurations = config_manager()  # Class from mkchromecast.config
    configf = configurations.configf
    config.read(configf)
    dict1 = {}
    options = config.options(section)
    for option in options:
        try:
            dict1[option] = config.get(section, option)
            if dict1[option] == -1:
                DebugPrint(""skip: %s"" % option)
        except:
            print(""Exception on %s!"" % option)
            dict1[option] = None
    return dict1
",if dict1 [ option ] == - 1 :,162
"def on_success(result):
    subtasks = {}
    if result:
        subtasks = {
            self.nodes_keys.inverse[s[""node_id""]]: s.get(""subtask_id"")
            for s in result
            if s.get(""status"") == ""Failure""
        }
    if subtasks:
        print(""subtask finished"")
        self.next()
    else:
        print(""waiting for a subtask to finish"")
        time.sleep(10)
","if s . get ( ""status"" ) == ""Failure""",129
"def redirect_aware_commmunicate(p, sys=_sys):
    """"""Variant of process.communicate that works with in process I/O redirection.""""""
    assert sys is not None
    out, err = p.communicate()
    if redirecting_io(sys=sys):
        if out:
            # We don't unicodify in Python2 because sys.stdout may be a
            # cStringIO.StringIO object, which does not accept Unicode strings
            out = unicodify(out)
            sys.stdout.write(out)
            out = None
        if err:
            err = unicodify(err)
            sys.stderr.write(err)
            err = None
    return out, err
",if err :,177
"def __exit__(self, *args, **kwargs):
    self._samples_cache = {}
    if is_validation_enabled() and isinstance(self.prior, dict):
        extra = set(self.prior) - self._param_hits
        if extra:
            warnings.warn(
                ""pyro.module prior did not find params ['{}']. ""
                ""Did you instead mean one of ['{}']?"".format(
                    ""', '"".join(extra), ""', '"".join(self._param_misses)
                )
            )
    return super().__exit__(*args, **kwargs)
",if extra :,156
"def __download_thread(self):
    while True:
        if not self.__queue.empty():
            self.__current_download = self.__queue.get()
            self.__download_file(self.__current_download)
        time.sleep(0.1)
",if not self . __queue . empty ( ) :,68
"def plot_timer_command(args):
    import nnabla.monitor as M
    format_unit = dict(
        s=""seconds"",
        m=""minutes"",
        h=""hours"",
        d=""days"",
    )
    if not args.ylabel:
        if args.elapsed:
            args.ylabel = ""Total elapsed time [{}]"".format(format_unit[args.time_unit])
        else:
            args.ylabel = ""Elapsed time [{}/iter]"".format(format_unit[args.time_unit])
    plot_any_command(
        args, M.plot_time_elapsed, dict(elapsed=args.elapsed, unit=args.time_unit)
    )
    return True
",if args . elapsed :,177
"def resolve_page(root: ChannelContext[models.MenuItem], info, **kwargs):
    if root.node.page_id:
        requestor = get_user_or_app_from_context(info.context)
        requestor_has_access_to_all = requestor.is_active and requestor.has_perm(
            PagePermissions.MANAGE_PAGES
        )
        return (
            PageByIdLoader(info.context)
            .load(root.node.page_id)
            .then(
                lambda page: page
                if requestor_has_access_to_all or page.is_visible
                else None
            )
        )
    return None
",if requestor_has_access_to_all or page . is_visible,177
"def find(self, pattern):
    """"""Find pages in database.""""""
    results = self._search_keyword(pattern)
    pat = re.compile(""(.*?)(%s)(.*?)( \(.*\))?$"" % re.escape(pattern), re.I)
    if results:
        for name, keyword, url in results:
            if os.isatty(sys.stdout.fileno()):
                keyword = pat.sub(
                    r""\1\033[1;31m\2\033[0m\3\033[1;33m\4\033[0m"", keyword
                )
            print(""%s - %s"" % (keyword, name))
    else:
        raise RuntimeError(""%s: nothing appropriate."" % pattern)
",if os . isatty ( sys . stdout . fileno ( ) ) :,184
"def _certonly_new_request_common(self, mock_client, args=None):
    with mock.patch(
        ""certbot._internal.main._find_lineage_for_domains_and_certname""
    ) as mock_renewal:
        mock_renewal.return_value = (""newcert"", None)
        with mock.patch(""certbot._internal.main._init_le_client"") as mock_init:
            mock_init.return_value = mock_client
            if args is None:
                args = []
            args += ""-d foo.bar -a standalone certonly"".split()
            self._call(args)
",if args is None :,169
"def __init__(self, *args, **kw):
    if len(args) > 1:
        raise TypeError(""MultiDict can only be called with one positional "" ""argument"")
    if args:
        if hasattr(args[0], ""iteritems""):
            items = list(args[0].iteritems())
        elif hasattr(args[0], ""items""):
            items = list(args[0].items())
        else:
            items = list(args[0])
        self._items = items
    else:
        self._items = []
    if kw:
        self._items.extend(kw.items())
","if hasattr ( args [ 0 ] , ""iteritems"" ) :",156
"def test08_ExceptionTypes(self):
    self.assertTrue(issubclass(db.DBError, Exception))
    for i, j in db.__dict__.items():
        if i.startswith(""DB"") and i.endswith(""Error""):
            self.assertTrue(issubclass(j, db.DBError), msg=i)
            if i not in (""DBKeyEmptyError"", ""DBNotFoundError""):
                self.assertFalse(issubclass(j, KeyError), msg=i)
    # This two exceptions have two bases
    self.assertTrue(issubclass(db.DBKeyEmptyError, KeyError))
    self.assertTrue(issubclass(db.DBNotFoundError, KeyError))
","if i . startswith ( ""DB"" ) and i . endswith ( ""Error"" ) :",154
"def _delegate_to_sinks(self, value: Any) -> None:
    for sink in self._sinks:
        if isinstance(sink, AgentT):
            await sink.send(value=value)
        elif isinstance(sink, ChannelT):
            await cast(TopicT, sink).send(value=value)
        else:
            await maybe_async(cast(Callable, sink)(value))
","elif isinstance ( sink , ChannelT ) :",103
"def _select_block(str_in, start_tag, end_tag):
    """"""Select first block delimited by start_tag and end_tag""""""
    start_pos = str_in.find(start_tag)
    if start_pos < 0:
        raise ValueError(""start_tag not found"")
    depth = 0
    for pos in range(start_pos, len(str_in)):
        if str_in[pos] == start_tag:
            depth += 1
        elif str_in[pos] == end_tag:
            depth -= 1
        if depth == 0:
            break
    sel = str_in[start_pos + 1 : pos]
    return sel
",elif str_in [ pos ] == end_tag :,171
"def confirm(request):
    details = request.session.get(""reauthenticate"")
    if not details:
        return redirect(""home"")
    # Monkey patch request
    request.user = User.objects.get(pk=details[""user_pk""])
    if request.method == ""POST"":
        confirm_form = PasswordConfirmForm(request, request.POST)
        if confirm_form.is_valid():
            request.session.pop(""reauthenticate"")
            request.session[""reauthenticate_done""] = True
            return redirect(""social:complete"", backend=details[""backend""])
    else:
        confirm_form = PasswordConfirmForm(request)
    context = {""confirm_form"": confirm_form}
    context.update(details)
    return render(request, ""accounts/confirm.html"", context)
",if confirm_form . is_valid ( ) :,195
"def verify_credentials(self):
    if self.enabled:
        response = requests.get(
            ""https://api.exotel.com/v1/Accounts/{sid}"".format(sid=self.account_sid),
            auth=(self.api_key, self.api_token),
        )
        if response.status_code != 200:
            frappe.throw(_(""Invalid credentials""))
",if response . status_code != 200 :,103
"def pixbufrenderer(self, column, crp, model, it):
    tok = model.get_value(it, 0)
    if tok.type == ""class"":
        icon = ""class""
    else:
        if tok.visibility == ""private"":
            icon = ""method_priv""
        elif tok.visibility == ""protected"":
            icon = ""method_prot""
        else:
            icon = ""method""
    crp.set_property(""pixbuf"", imagelibrary.pixbufs[icon])
","if tok . visibility == ""private"" :",132
"def _omit_keywords(self, context):
    omitted_kws = 0
    for event, elem in context:
        # Teardowns aren't omitted to allow checking suite teardown status.
        omit = elem.tag == ""kw"" and elem.get(""type"") != ""teardown""
        start = event == ""start""
        if omit and start:
            omitted_kws += 1
        if not omitted_kws:
            yield event, elem
        elif not start:
            elem.clear()
        if omit and not start:
            omitted_kws -= 1
",elif not start :,144
"def on_double_click(self, event):
    # TODO: don't act when the click happens below last item
    path = self.get_selected_path()
    kind = self.get_selected_kind()
    name = self.get_selected_name()
    if kind == ""file"":
        if self.should_open_name_in_thonny(name):
            self.open_file(path)
        else:
            self.open_path_with_system_app(path)
    elif kind == ""dir"":
        self.request_focus_into(path)
    return ""break""
",if self . should_open_name_in_thonny ( name ) :,152
"def search_cve(db: DatabaseInterface, product: Product) -> dict:
    result = {}
    for query_result in db.fetch_multiple(QUERIES[""cve_lookup""]):
        cve_entry = CveDbEntry(*query_result)
        if _product_matches_cve(product, cve_entry):
            result[cve_entry.cve_id] = {
                ""score2"": cve_entry.cvss_v2_score,
                ""score3"": cve_entry.cvss_v3_score,
                ""cpe_version"": build_version_string(cve_entry),
            }
    return result
","if _product_matches_cve ( product , cve_entry ) :",174
"def find_go_files_mtime(app_files):
    files, mtime = [], 0
    for f, mt in app_files.items():
        if not f.endswith("".go""):
            continue
        if APP_CONFIG.nobuild_files.match(f):
            continue
        files.append(f)
        mtime = max(mtime, mt)
    return files, mtime
",if APP_CONFIG . nobuild_files . match ( f ) :,100
"def wrapper(filename):
    mtime = getmtime(filename)
    with lock:
        if filename in cache:
            old_mtime, result = cache.pop(filename)
            if old_mtime == mtime:
                # Move to the end
                cache[filename] = old_mtime, result
                return result
    result = function(filename)
    with lock:
        cache[filename] = mtime, result  # at the end
        if len(cache) > max_size:
            cache.popitem(last=False)
    return result
",if filename in cache :,144
"def Tokenize(s):
    # type: (str) -> Iterator[Token]
    for item in TOKEN_RE.findall(s):
        # The type checker can't know the true type of item!
        item = cast(TupleStr4, item)
        if item[0]:
            typ = ""number""
            val = item[0]
        elif item[1]:
            typ = ""name""
            val = item[1]
        elif item[2]:
            typ = item[2]
            val = item[2]
        elif item[3]:
            typ = item[3]
            val = item[3]
        yield Token(typ, val)
",if item [ 0 ] :,181
"def _show_encoders(self, *args, **kwargs):
    if issubclass(self.current_module.__class__, BasePayload):
        encoders = self.current_module.get_encoders()
        if encoders:
            headers = (""Encoder"", ""Name"", ""Description"")
            print_table(headers, *encoders, max_column_length=100)
            return
    print_error(""No encoders available"")
",if encoders :,109
"def __init__(self):
    Builder.__init__(self, commandName=""VCExpress.exe"", formatName=""msvcProject"")
    for key in [""VS90COMNTOOLS"", ""VC80COMNTOOLS"", ""VC71COMNTOOLS""]:
        if os.environ.has_key(key):
            self.programDir = os.path.join(os.environ[key], "".."", ""IDE"")
    if self.programDir is None:
        for version in [""9.0"", ""8"", "".NET 2003""]:
            msvcDir = (
                ""C:\\Program Files\\Microsoft Visual Studio %s\\Common7\\IDE"" % version
            )
            if os.path.exists(msvcDir):
                self.programDir = msvcDir
",if os . environ . has_key ( key ) :,192
"def _inner(*args, **kwargs):
    component_manager = args[0].component_manager
    for condition_name in condition_names:
        condition_result, err_msg = component_manager.evaluate_condition(condition_name)
        if not condition_result:
            raise ComponentStartConditionNotMetError(err_msg)
    if not component_manager.all_components_running(*components):
        raise ComponentsNotStartedError(
            f""the following required components have not yet started: {json.dumps(components)}""
        )
    return method(*args, **kwargs)
",if not condition_result :,144
"def _gridconvvalue(self, value):
    if isinstance(value, (str, _tkinter.Tcl_Obj)):
        try:
            svalue = str(value)
            if not svalue:
                return None
            elif ""."" in svalue:
                return self.tk.getdouble(svalue)
            else:
                return self.tk.getint(svalue)
        except (ValueError, TclError):
            pass
    return value
","elif ""."" in svalue :",129
"def check_songs():
    desc = numeric_phrase(""%d song"", ""%d songs"", len(songs))
    with Task(_(""Rescan songs""), desc) as task:
        task.copool(check_songs)
        for i, song in enumerate(songs):
            song = song._song
            if song in app.library:
                app.library.reload(song)
            task.update((float(i) + 1) / len(songs))
            yield
",if song in app . library :,121
"def initialize(self):
    nn.init.xavier_uniform_(self.linear.weight.data)
    if self.linear.bias is not None:
        self.linear.bias.data.uniform_(-1.0, 1.0)
    if self.self_layer:
        nn.init.xavier_uniform_(self.linear_self.weight.data)
        if self.linear_self.bias is not None:
            self.linear_self.bias.data.uniform_(-1.0, 1.0)
",if self . linear_self . bias is not None :,126
"def test_row(self, row):
    for idx, test in self.patterns.items():
        try:
            value = row[idx]
        except IndexError:
            value = """"
        result = test(value)
        if self.any_match:
            if result:
                return not self.inverse  # True
        else:
            if not result:
                return self.inverse  # False
    if self.any_match:
        return self.inverse  # False
    else:
        return not self.inverse  # True
",if self . any_match :,149
"def toterminal(self, tw):
    for element in self.chain:
        element[0].toterminal(tw)
        if element[2] is not None:
            tw.line("""")
            tw.line(element[2], yellow=True)
    super(ExceptionChainRepr, self).toterminal(tw)
",if element [ 2 ] is not None :,88
"def runMainLoop(self):
    """"""The curses gui main loop.""""""
    # pylint: disable=no-member
    #
    # Do NOT change g.app!
    self.curses_app = LeoApp()
    stdscr = curses.initscr()
    if 1:  # Must follow initscr.
        self.dump_keys()
    try:
        self.curses_app.run()
        # run calls CApp.main(), which calls CGui.run().
    finally:
        curses.nocbreak()
        stdscr.keypad(0)
        curses.echo()
        curses.endwin()
        if ""shutdown"" in g.app.debug:
            g.pr(""Exiting Leo..."")
","if ""shutdown"" in g . app . debug :",179
"def test_chunkcoding(self):
    for native, utf8 in zip(*[StringIO(f).readlines() for f in self.tstring]):
        u = self.decode(native)[0]
        self.assertEqual(u, utf8.decode(""utf-8""))
        if self.roundtriptest:
            self.assertEqual(native, self.encode(u)[0])
",if self . roundtriptest :,93
"def reload_sanitize_allowlist(self, explicit=True):
    self.sanitize_allowlist = []
    try:
        with open(self.sanitize_allowlist_file) as f:
            for line in f.readlines():
                if not line.startswith(""#""):
                    self.sanitize_allowlist.append(line.strip())
    except OSError:
        if explicit:
            log.warning(
                ""Sanitize log file explicitly specified as '%s' but does not exist, continuing with no tools allowlisted."",
                self.sanitize_allowlist_file,
            )
","if not line . startswith ( ""#"" ) :",149
"def get_all_extensions(subtree=None):
    if subtree is None:
        subtree = full_extension_tree()
    result = []
    if isinstance(subtree, dict):
        for value in subtree.values():
            if isinstance(value, dict):
                result += get_all_extensions(value)
            elif isinstance(value, (ContentTypeMapping, ContentTypeDetector)):
                result += value.extensions
            elif isinstance(value, (list, tuple)):
                result += value
    elif isinstance(subtree, (ContentTypeMapping, ContentTypeDetector)):
        result = subtree.extensions
    elif isinstance(subtree, (list, tuple)):
        result = subtree
    return result
","elif isinstance ( value , ( ContentTypeMapping , ContentTypeDetector ) ) :",172
"def _configuration_dict_to_commandlist(name, config_dict):
    command_list = [""config:%s"" % name]
    for key, value in config_dict.items():
        if type(value) is bool:
            if value:
                b = ""true""
            else:
                b = ""false""
            command_list.append(""%s:%s"" % (key, b))
        else:
            command_list.append(""%s:%s"" % (key, value))
    return command_list
",if type ( value ) is bool :,140
"def _RewriteModinfo(
    self,
    modinfo,
    obj_kernel_version,
    this_kernel_version,
    info_strings=None,
    to_remove=None,
):
    new_modinfo = """"
    for line in modinfo.split(""\x00""):
        if not line:
            continue
        if to_remove and line.split(""="")[0] == to_remove:
            continue
        if info_strings is not None:
            info_strings.add(line.split(""="")[0])
        if line.startswith(""vermagic""):
            line = line.replace(obj_kernel_version, this_kernel_version)
        new_modinfo += line + ""\x00""
    return new_modinfo
",if not line :,187
"def zip_random_open_test(self, f, compression):
    self.make_test_archive(f, compression)
    # Read the ZIP archive
    with zipfile.ZipFile(f, ""r"", compression) as zipfp:
        zipdata1 = []
        with zipfp.open(TESTFN) as zipopen1:
            while True:
                read_data = zipopen1.read(randint(1, 1024))
                if not read_data:
                    break
                zipdata1.append(read_data)
        testdata = """".join(zipdata1)
        self.assertEqual(len(testdata), len(self.data))
        self.assertEqual(testdata, self.data)
",if not read_data :,182
"def _memoized(*args):
    now = time.time()
    try:
        value, last_update = self.cache[args]
        age = now - last_update
        if self._call_count > self.ctl or age > self.ttl:
            self._call_count = 0
            raise AttributeError
        if self.ctl:
            self._call_count += 1
        return value
    except (KeyError, AttributeError):
        value = func(*args)
        if value:
            self.cache[args] = (value, now)
        return value
    except TypeError:
        return func(*args)
",if self . _call_count > self . ctl or age > self . ttl :,164
"def on_data(res):
    if terminate.is_set():
        return
    if args.strings and not args.no_content:
        if type(res) == tuple:
            f, v = res
            if type(f) == unicode:
                f = f.encode(""utf-8"")
            if type(v) == unicode:
                v = v.encode(""utf-8"")
            self.success(""{}: {}"".format(f, v))
        elif not args.content_only:
            self.success(res)
    else:
        self.success(res)
",if type ( f ) == unicode :,158
"def _finalize_setup_keywords(self):
    for ep in pkg_resources.iter_entry_points(""distutils.setup_keywords""):
        value = getattr(self, ep.name, None)
        if value is not None:
            ep.require(installer=self.fetch_build_egg)
            ep.load()(self, ep.name, value)
",if value is not None :,90
"def test_attributes_types(self):
    if not self.connection.strategy.pooled:
        if not self.connection.server.info:
            self.connection.refresh_server_info()
        self.assertEqual(
            type(self.connection.server.schema.attribute_types[""cn""]), AttributeTypeInfo
        )
",if not self . connection . server . info :,82
"def to_key(literal_or_identifier):
    """"""returns string representation of this object""""""
    if literal_or_identifier[""type""] == ""Identifier"":
        return literal_or_identifier[""name""]
    elif literal_or_identifier[""type""] == ""Literal"":
        k = literal_or_identifier[""value""]
        if isinstance(k, float):
            return unicode(float_repr(k))
        elif ""regex"" in literal_or_identifier:
            return compose_regex(k)
        elif isinstance(k, bool):
            return ""true"" if k else ""false""
        elif k is None:
            return ""null""
        else:
            return unicode(k)
","elif isinstance ( k , bool ) :",179
"def list2rec(x, test=False):
    if test:
        vid = ""{}_{:06d}_{:06d}"".format(x[0], int(x[1]), int(x[2]))
        label = -1  # label unknown
        return vid, label
    else:
        vid = ""{}_{:06d}_{:06d}"".format(x[1], int(x[2]), int(x[3]))
        if level == 2:
            vid = ""{}/{}"".format(convert_label(x[0]), vid)
        else:
            assert level == 1
        label = class_mapping[convert_label(x[0])]
        return vid, label
",if level == 2 :,169
"def _expand_env(self, snapcraft_yaml):
    environment_keys = [""name"", ""version""]
    for key in snapcraft_yaml:
        if any((key == env_key for env_key in environment_keys)):
            continue
        replacements = environment_to_replacements(
            get_snapcraft_global_environment(self.project)
        )
        snapcraft_yaml[key] = replace_attr(snapcraft_yaml[key], replacements)
    return snapcraft_yaml
",if any ( ( key == env_key for env_key in environment_keys ) ) :,124
"def enableCtrls(self):
    # Check if each ctrl has a requirement or an incompatibility,
    # look it up, and enable/disable if so
    for data in self.storySettingsData:
        name = data[""name""]
        if name in self.ctrls:
            if ""requires"" in data:
                set = self.getSetting(data[""requires""])
                for i in self.ctrls[name]:
                    i.Enable(set not in [""off"", ""false"", ""0""])
",if name in self . ctrls :,133
"def __init__(self, *args, **kwargs):
    super(ChallengePhaseCreateSerializer, self).__init__(*args, **kwargs)
    context = kwargs.get(""context"")
    if context:
        challenge = context.get(""challenge"")
        if challenge:
            kwargs[""data""][""challenge""] = challenge.pk
        test_annotation = context.get(""test_annotation"")
        if test_annotation:
            kwargs[""data""][""test_annotation""] = test_annotation
",if challenge :,119
"def set_inactive(self):
    for title in self.gramplet_map:
        if self.gramplet_map[title].pui:
            if self.gramplet_map[title].gstate != ""detached"":
                self.gramplet_map[title].pui.active = False
","if self . gramplet_map [ title ] . gstate != ""detached"" :",81
"def authenticate(username, password):
    try:
        u = User.objects.get(username=username)
        if check_password_hash(u.password, password):
            userLogger.info(""User logged in : %s"", username)
            return u
        else:
            userLogger.warn(""Attempt to log in to : %s"", username)
            return False
    except DoesNotExist:
        return False
","if check_password_hash ( u . password , password ) :",109
"def _check_date(self):
    if not self.value:
        return None
    if not self.allow_date_in_past:
        if self.value < self.date_or_datetime().today():
            if self.allow_todays_date:
                self.value = self.date_or_datetime().today()
            else:
                self.value = self.date_or_datetime().today() + datetime.timedelta(1)
",if self . allow_todays_date :,119
"def update(self, E=None, **F):
    if E:
        if hasattr(E, ""keys""):
            # Update with `E` dictionary
            for k in E:
                self[k] = E[k]
        else:
            # Update with `E` items
            for (k, v) in E:
                self[k] = v
    # Update with `F` dictionary
    for k in F:
        self[k] = F[k]
","if hasattr ( E , ""keys"" ) :",131
"def _get_quota_availability(self):
    quotas_ok = defaultdict(int)
    qa = QuotaAvailability()
    qa.queue(*[k for k, v in self._quota_diff.items() if v > 0])
    qa.compute(now_dt=self.now_dt)
    for quota, count in self._quota_diff.items():
        if count <= 0:
            quotas_ok[quota] = 0
            break
        avail = qa.results[quota]
        if avail[1] is not None and avail[1] < count:
            quotas_ok[quota] = min(count, avail[1])
        else:
            quotas_ok[quota] = count
    return quotas_ok
",if count <= 0 :,182
"def gen_env_vars():
    for fd_id, fd in zip(STDIO_DESCRIPTORS, (stdin, stdout, stderr)):
        is_atty = fd.isatty()
        yield (cls.TTY_ENV_TMPL.format(fd_id), cls.encode_env_var_value(int(is_atty)))
        if is_atty:
            yield (cls.TTY_PATH_ENV.format(fd_id), os.ttyname(fd.fileno()) or b"""")
",if is_atty :,123
"def _convertDict(self, d):
    r = {}
    for k, v in d.items():
        if isinstance(v, bytes):
            v = str(v, ""utf-8"")
        elif isinstance(v, list) or isinstance(v, tuple):
            v = self._convertList(v)
        elif isinstance(v, dict):
            v = self._convertDict(v)
        if isinstance(k, bytes):
            k = str(k, ""utf-8"")
        r[k] = v
    return r
","if isinstance ( k , bytes ) :",142
"def get_attribute_value(self, nodeid, attr):
    with self._lock:
        self.logger.debug(""get attr val: %s %s"", nodeid, attr)
        if nodeid not in self._nodes:
            dv = ua.DataValue()
            dv.StatusCode = ua.StatusCode(ua.StatusCodes.BadNodeIdUnknown)
            return dv
        node = self._nodes[nodeid]
        if attr not in node.attributes:
            dv = ua.DataValue()
            dv.StatusCode = ua.StatusCode(ua.StatusCodes.BadAttributeIdInvalid)
            return dv
        attval = node.attributes[attr]
        if attval.value_callback:
            return attval.value_callback()
        return attval.value
",if attr not in node . attributes :,200
"def conninfo_parse(dsn):
    ret = {}
    length = len(dsn)
    i = 0
    while i < length:
        if dsn[i].isspace():
            i += 1
            continue
        param_match = PARAMETER_RE.match(dsn[i:])
        if not param_match:
            return
        param = param_match.group(1)
        i += param_match.end()
        if i >= length:
            return
        value, end = read_param_value(dsn[i:])
        if value is None:
            return
        i += end
        ret[param] = value
    return ret
",if dsn [ i ] . isspace ( ) :,175
"def connect(self, buttons):
    for button in buttons:
        assert button is not None
        handled = False
        for handler_idx in range(0, len(self.__signal_handlers)):
            (obj_class, signal, handler, handler_id) = self.__signal_handlers[
                handler_idx
            ]
            if isinstance(button, obj_class):
                handler_id = button.connect(signal, handler)
                handled = True
            self.__signal_handlers[handler_idx] = (
                obj_class,
                signal,
                handler,
                handler_id,
            )
        assert handled
","if isinstance ( button , obj_class ) :",182
"def _parse_display(display):
    """"""Parse an X11 display value""""""
    try:
        host, dpynum = display.rsplit("":"", 1)
        if host.startswith(""["") and host.endswith(""]""):
            host = host[1:-1]
        idx = dpynum.find(""."")
        if idx >= 0:
            screen = int(dpynum[idx + 1 :])
            dpynum = dpynum[:idx]
        else:
            screen = 0
    except (ValueError, UnicodeEncodeError):
        raise ValueError(""Invalid X11 display"") from None
    return host, dpynum, screen
",if idx >= 0 :,156
"def delete_all(path):
    ppath = os.getcwd()
    os.chdir(path)
    for fn in glob.glob(""*""):
        fn_full = os.path.join(path, fn)
        if os.path.isdir(fn):
            delete_all(fn_full)
        elif fn.endswith("".png""):
            os.remove(fn_full)
        elif fn.endswith("".md""):
            os.remove(fn_full)
        elif DELETE_ALL_OLD:
            os.remove(fn_full)
    os.chdir(ppath)
    os.rmdir(path)
","elif fn . endswith ( "".png"" ) :",158
"def _sync_get(self, identifier, *args, **kw):
    self._mutex.acquire()
    try:
        try:
            if identifier in self._values:
                return self._values[identifier]
            else:
                self._values[identifier] = value = self.creator(identifier, *args, **kw)
                return value
        except KeyError:
            self._values[identifier] = value = self.creator(identifier, *args, **kw)
            return value
    finally:
        self._mutex.release()
",if identifier in self . _values :,148
"def _query_fd(self):
    if self.stream is None:
        self._last_stat = None, None
    else:
        try:
            st = os.stat(self._filename)
        except OSError:
            e = sys.exc_info()[1]
            if e.errno != errno.ENOENT:
                raise
            self._last_stat = None, None
        else:
            self._last_stat = st[stat.ST_DEV], st[stat.ST_INO]
",if e . errno != errno . ENOENT :,137
"def get_place_name(self, place_handle):
    """"""Obtain a place name""""""
    text = """"
    if place_handle:
        place = self.dbstate.db.get_place_from_handle(place_handle)
        if place:
            place_title = place_displayer.display(self.dbstate.db, place)
            if place_title != """":
                if len(place_title) > 25:
                    text = place_title[:24] + ""...""
                else:
                    text = place_title
    return text
",if place :,153
"def test_decoder_state(self):
    # Check that getstate() and setstate() handle the state properly
    u = ""abc123""
    for encoding in all_unicode_encodings:
        if encoding not in broken_unicode_with_stateful:
            self.check_state_handling_decode(encoding, u, u.encode(encoding))
            self.check_state_handling_encode(encoding, u, u.encode(encoding))
",if encoding not in broken_unicode_with_stateful :,110
"def cleanup(self):
    if os.path.exists(self.meta_gui_dir):
        for f in os.listdir(self.meta_gui_dir):
            if os.path.splitext(f)[1] == "".desktop"":
                os.remove(os.path.join(self.meta_gui_dir, f))
","if os . path . splitext ( f ) [ 1 ] == "".desktop"" :",85
"def _have_applied_incense(self):
    for applied_item in inventory.applied_items().all():
        self.logger.info(applied_item)
        if applied_item.expire_ms > 0:
            mins = format_time(applied_item.expire_ms * 1000)
            self.logger.info(
                ""Not applying incense, currently active: %s, %s minutes remaining"",
                applied_item.item.name,
                mins,
            )
            return True
        else:
            self.logger.info("""")
            return False
    return False
",if applied_item . expire_ms > 0 :,162
"def get_closest_point(self, point):
    point = to_point(point)
    cp, cd = None, None
    for p0, p1 in iter_pairs(self.pts, self.connected):
        diff = p1 - p0
        l = diff.length
        d = diff / l
        pp = p0 + d * max(0, min(l, (point - p0).dot(d)))
        dist = (point - pp).length
        if not cp or dist < cd:
            cp, cd = pp, dist
    return cp
",if not cp or dist < cd :,143
"def process_return(lines):
    for line in lines:
        m = re.fullmatch(r""(?P<param>\w+)\s+:\s+(?P<type>[\w.]+)"", line)
        if m:
            # Once this is in scanpydoc, we can use the fancy hover stuff
            yield f'**{m[""param""]}** : :class:`~{m[""type""]}`'
        else:
            yield line
",if m :,106
"def _classify(nodes_by_level):
    missing, invalid, downloads = [], [], []
    for level in nodes_by_level:
        for node in level:
            if node.binary == BINARY_MISSING:
                missing.append(node)
            elif node.binary == BINARY_INVALID:
                invalid.append(node)
            elif node.binary in (BINARY_UPDATE, BINARY_DOWNLOAD):
                downloads.append(node)
    return missing, invalid, downloads
",elif node . binary == BINARY_INVALID :,126
"def safe_parse_date(date_hdr):
    """"""Parse a Date: or Received: header into a unix timestamp.""""""
    try:
        if "";"" in date_hdr:
            date_hdr = date_hdr.split("";"")[-1].strip()
        msg_ts = long(rfc822.mktime_tz(rfc822.parsedate_tz(date_hdr)))
        if (msg_ts > (time.time() + 24 * 3600)) or (msg_ts < 1):
            return None
        else:
            return msg_ts
    except (ValueError, TypeError, OverflowError):
        return None
","if "";"" in date_hdr :",150
"def _on_change(self):
    changed = False
    self.save()
    for key, value in self.data.items():
        if isinstance(value, bool):
            if value:
                changed = True
                break
        if isinstance(value, int):
            if value != 1:
                changed = True
                break
        elif value is None:
            continue
        elif len(value) != 0:
            changed = True
            break
    self._reset_button.disabled = not changed
",if value != 1 :,145
"def _rewrite_prepend_append(self, string, prepend, append=None):
    if append is None:
        append = prepend
    if not isinstance(string, StringElem):
        string = StringElem(string)
    string.sub.insert(0, prepend)
    if unicode(string).endswith(u""\n""):
        # Try and remove the last character from the tree
        try:
            lastnode = string.flatten()[-1]
            if isinstance(lastnode.sub[-1], unicode):
                lastnode.sub[-1] = lastnode.sub[-1].rstrip(u""\n"")
        except IndexError:
            pass
        string.sub.append(append + u""\n"")
    else:
        string.sub.append(append)
    return string
","if isinstance ( lastnode . sub [ - 1 ] , unicode ) :",197
"def parse_indentless_sequence_entry(self):
    if self.check_token(BlockEntryToken):
        token = self.get_token()
        if not self.check_token(BlockEntryToken, KeyToken, ValueToken, BlockEndToken):
            self.states.append(self.parse_indentless_sequence_entry)
            return self.parse_block_node()
        else:
            self.state = self.parse_indentless_sequence_entry
            return self.process_empty_scalar(token.end_mark)
    token = self.peek_token()
    event = SequenceEndEvent(token.start_mark, token.start_mark)
    self.state = self.states.pop()
    return event
","if not self . check_token ( BlockEntryToken , KeyToken , ValueToken , BlockEndToken ) :",184
"def walk_directory(directory, verbose=False):
    """"""Iterates a directory's text files and their contents.""""""
    for dir_path, _, filenames in os.walk(directory):
        for filename in filenames:
            file_path = os.path.join(dir_path, filename)
            if os.path.isfile(file_path) and not filename.startswith("".""):
                with io.open(file_path, ""r"", encoding=""utf-8"") as file:
                    if verbose:
                        print(""Reading {}"".format(filename))
                    doc_text = file.read()
                    yield filename, doc_text
",if verbose :,166
"def set_bounds(self, x, y, width, height):
    if self.native:
        # Root level widgets may require vertical adjustment to
        # account for toolbars, etc.
        if self.interface.parent is None:
            vertical_shift = self.frame.vertical_shift
        else:
            vertical_shift = 0
        self.native.Size = Size(width, height)
        self.native.Location = Point(x, y + vertical_shift)
",if self . interface . parent is None :,122
"def _check_x11(self, command=None, *, exc=None, exit_status=None, **kwargs):
    """"""Check requesting X11 forwarding""""""
    with (yield from self.connect()) as conn:
        if exc:
            with self.assertRaises(exc):
                yield from _create_x11_process(conn, command, **kwargs)
        else:
            proc = yield from _create_x11_process(conn, command, **kwargs)
            yield from proc.wait()
            self.assertEqual(proc.exit_status, exit_status)
    yield from conn.wait_closed()
",if exc :,156
"def repr(self):
    try:
        if isinstance(self.obj, (dict, web.threadeddict)):
            from infogami.infobase.utils import prepr
            return prepr(self.obj)
        else:
            return repr(self.obj)
    except:
        return ""failed""
    return render_template(""admin/memory/object"", self.obj)
","if isinstance ( self . obj , ( dict , web . threadeddict ) ) :",100
"def add(self, tag, values):
    if tag not in self.different:
        if tag not in self:
            self[tag] = values
        elif self[tag] != values:
            self.different.add(tag)
            self[tag] = [""""]
    self.counts[tag] += 1
",elif self [ tag ] != values :,82
"def _on_geturl(self, event):
    selected = self._status_list.get_selected()
    if selected != -1:
        object_id = self._status_list.GetItemData(selected)
        download_item = self._download_list.get_item(object_id)
        url = download_item.url
        if not wx.TheClipboard.IsOpened():
            clipdata = wx.TextDataObject()
            clipdata.SetText(url)
            wx.TheClipboard.Open()
            wx.TheClipboard.SetData(clipdata)
            wx.TheClipboard.Close()
",if not wx . TheClipboard . IsOpened ( ) :,163
"def escape2null(text):
    """"""Return a string with escape-backslashes converted to nulls.""""""
    parts = []
    start = 0
    while True:
        found = text.find(""\\"", start)
        if found == -1:
            parts.append(text[start:])
            return """".join(parts)
        parts.append(text[start:found])
        parts.append(""\x00"" + text[found + 1 : found + 2])
        start = found + 2  # skip character after escape
",if found == - 1 :,129
"def _process_inner_views(self):
    for view in self.baseviews:
        for inner_class in view.get_uninit_inner_views():
            for v in self.baseviews:
                if isinstance(v, inner_class) and v not in view.get_init_inner_views():
                    view.get_init_inner_views().append(v)
","if isinstance ( v , inner_class ) and v not in view . get_init_inner_views ( ) :",99
"def TryMerge(self, d):
    while d.avail() > 0:
        tt = d.getVarInt32()
        if tt == 10:
            self.set_url(d.getPrefixedString())
            continue
        if tt == 18:
            self.set_app_version_id(d.getPrefixedString())
            continue
        if tt == 26:
            self.set_method(d.getPrefixedString())
            continue
        if tt == 34:
            self.set_queue(d.getPrefixedString())
            continue
        if tt == 0:
            raise ProtocolBuffer.ProtocolBufferDecodeError
        d.skipData(tt)
",if tt == 18 :,184
"def test_sample_output():
    comment = ""SAMPLE OUTPUT""
    skip_files = [""__init__.py""]
    errors = []
    for _file in sorted(MODULE_PATH.iterdir()):
        if _file.suffix == "".py"" and _file.name not in skip_files:
            with _file.open() as f:
                if comment not in f.read():
                    errors.append((comment, _file))
    if errors:
        line = ""Missing sample error(s) detected!\n\n""
        for error in errors:
            line += ""`{}` is not in module `{}`\n"".format(*error)
        print(line[:-1])
        assert False
",if comment not in f . read ( ) :,174
"def _get_planner(name, path, source):
    for klass in _planners:
        if klass.detect(path, source):
            LOG.debug(""%r accepted %r (filename %r)"", klass, name, path)
            return klass
        LOG.debug(""%r rejected %r"", klass, name)
    raise ansible.errors.AnsibleError(NO_METHOD_MSG + repr(invocation))
","if klass . detect ( path , source ) :",100
"def _to_string_infix(self, ostream, idx, verbose):
    if verbose:
        ostream.write("" , "")
    else:
        hasConst = not (
            self._const.__class__ in native_numeric_types and self._const == 0
        )
        if hasConst:
            idx -= 1
        _l = self._coef[id(self._args[idx])]
        _lt = _l.__class__
        if _lt is _NegationExpression or (_lt in native_numeric_types and _l < 0):
            ostream.write("" - "")
        else:
            ostream.write("" + "")
",if hasConst :,169
"def cluster_info_query(self):
    if self._major_version >= 90600:
        extra = (
            "", CASE WHEN latest_end_lsn IS NULL THEN NULL ELSE received_tli END,""
            "" slot_name, conninfo FROM pg_catalog.pg_stat_get_wal_receiver()""
        )
        if self.role == ""standby_leader"":
            extra = ""timeline_id"" + extra + "", pg_catalog.pg_control_checkpoint()""
        else:
            extra = ""0"" + extra
    else:
        extra = ""0, NULL, NULL, NULL""
    return (""SELECT "" + self.TL_LSN + "", {2}"").format(
        self.wal_name, self.lsn_name, extra
    )
","if self . role == ""standby_leader"" :",199
"def __init__(self, *args, **kwargs):
    self.country = kwargs.pop(""country"")
    self.fields_needed = kwargs.pop(""fields_needed"", [])
    super(DynamicManagedAccountForm, self).__init__(*args, **kwargs)
    # build our form using the country specific fields and falling
    # back to our default set
    for f in self.fields_needed:
        if f in FIELDS_BY_COUNTRY.get(self.country, {}):  # pragma: no branch
            field_name, field = FIELDS_BY_COUNTRY[self.country][f]
            self.fields[field_name] = field
","if f in FIELDS_BY_COUNTRY . get ( self . country , { } ) :",155
"def delete_map(self, query=None):
    query_map = self.interpolated_map(query=query)
    for alias, drivers in six.iteritems(query_map.copy()):
        for driver, vms in six.iteritems(drivers.copy()):
            for vm_name, vm_details in six.iteritems(vms.copy()):
                if vm_details == ""Absent"":
                    query_map[alias][driver].pop(vm_name)
            if not query_map[alias][driver]:
                query_map[alias].pop(driver)
        if not query_map[alias]:
            query_map.pop(alias)
    return query_map
","if vm_details == ""Absent"" :",177
"def on_strokes_edited(self):
    strokes = self._strokes()
    if strokes:
        translation = self._engine.raw_lookup(strokes)
        if translation is not None:
            fmt = _(""{strokes} maps to {translation}"")
        else:
            fmt = _(""{strokes} is not in the dictionary"")
        info = self._format_label(fmt, (strokes,), translation)
    else:
        info = """"
    self.strokes_info.setText(info)
",if translation is not None :,123
"def release(self):
    tid = _thread.get_ident()
    with self.lock:
        if self.owner != tid:
            raise RuntimeError(""cannot release un-acquired lock"")
        assert self.count > 0
        self.count -= 1
        if self.count == 0:
            self.owner = None
            if self.waiters:
                self.waiters -= 1
                self.wakeup.release()
",if self . count == 0 :,117
"def _cat_blob(self, gcs_uri):
    """""":py:meth:`cat_file`, minus decompression.""""""
    blob = self._get_blob(gcs_uri)
    if not blob:
        return  # don't cat nonexistent files
    start = 0
    while True:
        end = start + _CAT_CHUNK_SIZE
        try:
            chunk = blob.download_as_string(start=start, end=end)
        except google.api_core.exceptions.RequestRangeNotSatisfiable:
            return
        yield chunk
        if len(chunk) < _CAT_CHUNK_SIZE:
            return
        start = end
",if len ( chunk ) < _CAT_CHUNK_SIZE :,168
"def device_iter(**kwargs):
    for dev in backend.enumerate_devices():
        d = Device(dev, backend)
        tests = (val == _try_getattr(d, key) for key, val in kwargs.items())
        if _interop._all(tests) and (custom_match is None or custom_match(d)):
            yield d
",if _interop . _all ( tests ) and ( custom_match is None or custom_match ( d ) ) :,88
"def _get_vtkjs(self):
    if self._vtkjs is None and self.object is not None:
        if isinstance(self.object, string_types) and self.object.endswith("".vtkjs""):
            if isfile(self.object):
                with open(self.object, ""rb"") as f:
                    vtkjs = f.read()
            else:
                data_url = urlopen(self.object)
                vtkjs = data_url.read()
        elif hasattr(self.object, ""read""):
            vtkjs = self.object.read()
        self._vtkjs = vtkjs
    return self._vtkjs
","elif hasattr ( self . object , ""read"" ) :",180
"def _execute_with_error(command, error, message):
    try:
        cli.invocation = cli.invocation_cls(
            cli_ctx=cli,
            parser_cls=cli.parser_cls,
            commands_loader_cls=cli.commands_loader_cls,
            help_cls=cli.help_cls,
        )
        cli.invocation.execute(command.split())
    except CLIError as ex:
        if error not in str(ex):
            raise AssertionError(
                ""{}\nExpected: {}\nActual: {}"".format(message, error, ex)
            )
        return
    except Exception as ex:
        raise ex
    raise AssertionError(""exception not raised for '{0}'"".format(message))
",if error not in str ( ex ) :,193
"def ray_intersection(self, p, line):
    p = Vector(center(line.sites))
    min_r = BIG_FLOAT
    nearest = None
    for v_i, v_j in self.edges:
        bound = LineEquation2D.from_two_points(v_i, v_j)
        intersection = bound.intersect_with_line(line)
        if intersection is not None:
            r = (p - intersection).length
            # info(""INT: [%s - %s] X [%s] => %s (%s)"", v_i, v_j, line, intersection, r)
            if r < min_r:
                nearest = intersection
                min_r = r
    return nearest
",if r < min_r :,187
"def CalculateChecksum(data):
    # The checksum is just a sum of all the bytes. I swear.
    if isinstance(data, bytearray):
        total = sum(data)
    elif isinstance(data, bytes):
        if data and isinstance(data[0], bytes):
            # Python 2 bytes (str) index as single-character strings.
            total = sum(map(ord, data))
        else:
            # Python 3 bytes index as numbers (and PY2 empty strings sum() to 0)
            total = sum(data)
    else:
        # Unicode strings (should never see?)
        total = sum(map(ord, data))
    return total & 0xFFFFFFFF
","if data and isinstance ( data [ 0 ] , bytes ) :",172
"def __mul__(self, other: Union[""Tensor"", float]) -> ""Tensor"":
    if isinstance(other, Tensor):
        if self.backend.name != other.backend.name:
            errstr = (
                f""Given backens are inconsistent. Found '{self.backend.name}'""
                f""and '{other.backend.name}'""
            )
            raise ValueError(errstr)
        other = other.array
    array = self.backend.multiply(self.array, other)
    return Tensor(array, backend=self.backend)
",if self . backend . name != other . backend . name :,140
"def next_item(self, direction):
    """"""Selects next menu item, based on self._direction""""""
    start, i = -1, 0
    try:
        start = self.items.index(self._selected)
        i = start + direction
    except:
        pass
    while True:
        if i == start:
            # Cannot find valid menu item
            self.select(start)
            break
        if i >= len(self.items):
            i = 0
            continue
        if i < 0:
            i = len(self.items) - 1
            continue
        if self.select(i):
            break
        i += direction
        if start < 0:
            start = 0
",if i == start :,194
"def resolve_none(self, data):
    # replace None to '_'
    for tok_idx in range(len(data)):
        for feat_idx in range(len(data[tok_idx])):
            if data[tok_idx][feat_idx] is None:
                data[tok_idx][feat_idx] = ""_""
    return data
",if data [ tok_idx ] [ feat_idx ] is None :,87
"def distinct(expr, *on):
    fields = frozenset(expr.fields)
    _on = []
    append = _on.append
    for n in on:
        if isinstance(n, Field):
            if n._child.isidentical(expr):
                n = n._name
            else:
                raise ValueError(""{0} is not a field of {1}"".format(n, expr))
        if not isinstance(n, _strtypes):
            raise TypeError(""on must be a name or field, not: {0}"".format(n))
        elif n not in fields:
            raise ValueError(""{0} is not a field of {1}"".format(n, expr))
        append(n)
    return Distinct(expr, tuple(_on))
",elif n not in fields :,192
"def TryMerge(self, d):
    while d.avail() > 0:
        tt = d.getVarInt32()
        if tt == 10:
            length = d.getVarInt32()
            tmp = ProtocolBuffer.Decoder(d.buffer(), d.pos(), d.pos() + length)
            d.skip(length)
            self.mutable_cost().TryMerge(tmp)
            continue
        if tt == 24:
            self.add_version(d.getVarInt64())
            continue
        if tt == 0:
            raise ProtocolBuffer.ProtocolBufferDecodeError
        d.skipData(tt)
",if tt == 10 :,167
"def func_std_string(func_name):  # match what old profile produced
    if func_name[:2] == (""~"", 0):
        # special case for built-in functions
        name = func_name[2]
        if name.startswith(""<"") and name.endswith("">""):
            return ""{%s}"" % name[1:-1]
        else:
            return name
    else:
        return ""%s:%d(%s)"" % func_name
","if name . startswith ( ""<"" ) and name . endswith ( "">"" ) :",115
"def f():
    try:
        # Intra-buffer read then buffer-flushing read
        for n in cycle([1, 19]):
            s = bufio.read(n)
            if not s:
                break
            # list.append() is atomic
            results.append(s)
    except Exception as e:
        errors.append(e)
        raise
",if not s :,104
"def stop(self):
    # Try to shut the connection down, but if we get any sort of
    # errors, go ahead and ignore them.. as we're shutting down anyway
    try:
        self.rpcserver.stop()
        if self.backend_rpcserver:
            self.backend_rpcserver.stop()
        if self.cluster_rpcserver:
            self.cluster_rpcserver.stop()
    except Exception:
        pass
    if self.coordination:
        try:
            coordination.COORDINATOR.stop()
        except Exception:
            pass
    super(Service, self).stop(graceful=True)
",if self . cluster_rpcserver :,171
"def download(cls, architecture, path=""./""):
    if cls.sanity_check(architecture):
        architecture_file = download_file(
            cls.architecture_map[architecture], directory=path
        )
        if not architecture_file:
            return None
        print(""Coreml model {} is saved in [{}]"".format(architecture, path))
        return architecture_file
    else:
        return None
",if not architecture_file :,107
"def opps_output_converter(kpt_list):
    kpts = []
    mpii_keys = to_opps_converter.keys()
    for mpii_idx in range(0, 16):
        if mpii_idx in mpii_keys:
            model_idx = to_opps_converter[mpii_idx]
            x, y = kpt_list[model_idx]
            if x < 0 or y < 0:
                kpts += [0.0, 0.0, -1.0]
            else:
                kpts += [x, y, 1.0]
        else:
            kpts += [0.0, 0.0, -1.0]
    return kpts
",if mpii_idx in mpii_keys :,188
"def _get_headers(self, headers=None):
    request_headers = headers or {}
    # Auth headers if access_token is present
    if self._client.client.config:
        config = self._client.client.config
        if ""Authorization"" not in request_headers and config.token:
            request_headers.update(
                {
                    ""Authorization"": ""{} {}"".format(
                        config.authentication_type, config.token
                    )
                }
            )
        if config.header and config.header_service:
            request_headers.update({config.header: config.header_service})
    return request_headers
",if config . header and config . header_service :,176
"def get_last_traded_prices(cls, trading_pairs: List[str]) -> Dict[str, float]:
    results = dict()
    async with aiohttp.ClientSession() as client:
        resp = await client.get(f""{constants.REST_URL}/tickers"")
        resp_json = await resp.json()
        for trading_pair in trading_pairs:
            resp_record = [
                o
                for o in resp_json
                if o[""symbol""] == convert_to_exchange_trading_pair(trading_pair)
            ][0]
            results[trading_pair] = float(resp_record[""price""])
    return results
","if o [ ""symbol"" ] == convert_to_exchange_trading_pair ( trading_pair )",181
"def reset_two_factor_hotp():
    uid = request.form[""uid""]
    otp_secret = request.form.get(""otp_secret"", None)
    if otp_secret:
        user = Journalist.query.get(uid)
        if not validate_hotp_secret(user, otp_secret):
            return render_template(""admin_edit_hotp_secret.html"", uid=uid)
        db.session.commit()
        return redirect(url_for(""admin.new_user_two_factor"", uid=uid))
    else:
        return render_template(""admin_edit_hotp_secret.html"", uid=uid)
","if not validate_hotp_secret ( user , otp_secret ) :",166
"def ctx_for_video(self, vurl):
    ""Get a context dict for a given video URL""
    ctx = self.get_context_dict()
    for portal, match, context_fn in self.PORTALS:
        if match.search(vurl):
            try:
                ctx.update(context_fn(vurl))
                ctx[""portal""] = portal
                break
            except AttributeError:
                continue
    return ctx
",if match . search ( vurl ) :,122
"def get(self):
    name = request.args.get(""filename"")
    if name is not None:
        opts = dict()
        opts[""type""] = ""episode""
        result = guessit(name, options=opts)
        res = dict()
        if ""episode"" in result:
            res[""episode""] = result[""episode""]
        else:
            res[""episode""] = 0
        if ""season"" in result:
            res[""season""] = result[""season""]
        else:
            res[""season""] = 0
        if ""subtitle_language"" in result:
            res[""subtitle_language""] = str(result[""subtitle_language""])
        return jsonify(data=res)
    else:
        return """", 400
","if ""subtitle_language"" in result :",196
"def package_files(package_path, directory_name):
    paths = []
    directory_path = os.path.join(package_path, directory_name)
    for (path, directories, filenames) in os.walk(directory_path):
        relative_path = os.path.relpath(path, package_path)
        for filename in filenames:
            if filename[0] == ""."":
                continue
            paths.append(os.path.join(relative_path, filename))
    return paths
","if filename [ 0 ] == ""."" :",126
"def parse_simple(d, data):
    units = {}
    for v in data[d]:
        key = v[""name""]
        if not key:
            continue
        key_to_insert = make_key(key)
        if key_to_insert in units:
            index = 2
            tmp = f""{key_to_insert}_{index}""
            while tmp in units:
                index += 1
                tmp = f""{key_to_insert}_{index}""
            key_to_insert = tmp
        units[key_to_insert] = v[""id""]
    return units
",if key_to_insert in units :,160
"def parse_clademodelc(branch_type_no, line_floats, site_classes):
    """"""Parse results specific to the clade model C.""""""
    if not site_classes or len(line_floats) == 0:
        return
    for n in range(len(line_floats)):
        if site_classes[n].get(""branch types"") is None:
            site_classes[n][""branch types""] = {}
        site_classes[n][""branch types""][branch_type_no] = line_floats[n]
    return site_classes
","if site_classes [ n ] . get ( ""branch types"" ) is None :",134
"def track_modules(self, *modules):
    """"""Add module names to the tracked list.""""""
    already_tracked = self.session.GetParameter(""autodetect_build_local_tracked"") or []
    needed = set(modules)
    if not needed.issubset(already_tracked):
        needed.update(already_tracked)
        with self.session as session:
            session.SetParameter(""autodetect_build_local_tracked"", needed)
            for module_name in modules:
                module_obj = self.GetModuleByName(module_name)
                if module_obj:
                    # Clear the module's profile. This will force it to
                    # reload a new profile.
                    module_obj.profile = None
",if module_obj :,196
"def set_job_on_hold(self, value, blocking=True):
    trigger = False
    # don't run any locking code beyond this...
    if not self._job_on_hold.acquire(blocking=blocking):
        return False
    try:
        if value:
            self._job_on_hold.set()
        else:
            self._job_on_hold.clear()
            if self._job_on_hold.counter == 0:
                trigger = True
    finally:
        self._job_on_hold.release()
    # locking code is now safe to run again
    if trigger:
        self._continue_sending()
    return True
",if value :,172
"def moveToThreadNext(self):
    """"""Move a position to threadNext position.""""""
    p = self
    if p.v:
        if p.v.children:
            p.moveToFirstChild()
        elif p.hasNext():
            p.moveToNext()
        else:
            p.moveToParent()
            while p:
                if p.hasNext():
                    p.moveToNext()
                    break  # found
                p.moveToParent()
            # not found.
    return p
",if p . v . children :,150
"def best_image(width, height):
    # A heuristic for finding closest sized image to required size.
    image = images[0]
    for img in images:
        if img.width == width and img.height == height:
            # Exact match always used
            return img
        elif img.width >= width and img.width * img.height > image.width * image.height:
            # At least wide enough, and largest area
            image = img
    return image
",if img . width == width and img . height == height :,120
"def _check_input_types(self):
    if len(self.base_features) == 0:
        return True
    input_types = self.primitive.input_types
    if input_types is not None:
        if type(input_types[0]) != list:
            input_types = [input_types]
        for t in input_types:
            zipped = list(zip(t, self.base_features))
            if all([issubclass(f.variable_type, v) for v, f in zipped]):
                return True
    else:
        return True
    return False
",if type ( input_types [ 0 ] ) != list :,154
"def get_result(self):
    result_list = []
    exc_info = None
    for f in self.children:
        try:
            result_list.append(f.get_result())
        except Exception as e:
            if exc_info is None:
                exc_info = sys.exc_info()
            else:
                if not isinstance(e, self.quiet_exceptions):
                    app_log.error(""Multiple exceptions in yield list"", exc_info=True)
    if exc_info is not None:
        raise_exc_info(exc_info)
    if self.keys is not None:
        return dict(zip(self.keys, result_list))
    else:
        return list(result_list)
","if not isinstance ( e , self . quiet_exceptions ) :",196
"def _update_learning_params(self):
    model = self.model
    hparams = self.hparams
    fd = self.runner.feed_dict
    step_num = self.step_num
    if hparams.model_type == ""resnet_tf"":
        if step_num < hparams.lrn_step:
            lrn_rate = hparams.mom_lrn
        elif step_num < 30000:
            lrn_rate = hparams.mom_lrn / 10
        elif step_num < 35000:
            lrn_rate = hparams.mom_lrn / 100
        else:
            lrn_rate = hparams.mom_lrn / 1000
        fd[model.lrn_rate] = lrn_rate
",elif step_num < 30000 :,190
"def topic_exists(self, arn):
    response = self._conn.get_all_topics()
    topics = response[""ListTopicsResponse""][""ListTopicsResult""][""Topics""]
    current_topics = []
    if len(topics) > 0:
        for topic in topics:
            topic_arn = topic[""TopicArn""]
            current_topics.append(topic_arn)
        if arn in current_topics:
            return True
    return False
",if arn in current_topics :,115
"def assertStartsWith(self, expectedPrefix, text, msg=None):
    if not text.startswith(expectedPrefix):
        if len(expectedPrefix) + 5 < len(text):
            text = text[: len(expectedPrefix) + 5] + ""...""
        standardMsg = ""{} not found at the start of {}"".format(
            repr(expectedPrefix), repr(text)
        )
        self.fail(self._formatMessage(msg, standardMsg))
",if len ( expectedPrefix ) + 5 < len ( text ) :,112
"def validate_memory(self, value):
    for k, v in value.viewitems():
        if v is None:  # use NoneType to unset a value
            continue
        if not re.match(PROCTYPE_MATCH, k):
            raise serializers.ValidationError(""Process types can only contain [a-z]"")
        if not re.match(MEMLIMIT_MATCH, str(v)):
            raise serializers.ValidationError(
                ""Limit format: <number><unit>, where unit = B, K, M or G""
            )
    return value
",if v is None :,141
"def open(self) -> ""KeyValueJsonDb"":
    """"""Create a new data base or open existing one""""""
    if os.path.exists(self._name):
        if not os.path.isfile(self._name):
            raise IOError(""%s exists and is not a file"" % self._name)
        try:
            with open(self._name, ""r"") as _in:
                self.set_records(json.load(_in))
        except json.JSONDecodeError:
            # file corrupted, reset it.
            self.commit()
    else:
        # make sure path exists
        mkpath(os.path.dirname(self._name))
        self.commit()
    return self
",if not os . path . isfile ( self . _name ) :,180
"def _calculate(self):
    before = self.before.data
    after = self.after.data
    self.deleted = {}
    self.updated = {}
    self.created = after.copy()
    for path, f in before.items():
        if path not in after:
            self.deleted[path] = f
            continue
        del self.created[path]
        if f.mtime < after[path].mtime:
            self.updated[path] = after[path]
",if path not in after :,125
"def cache_sqs_queues_across_accounts() -> bool:
    function: str = f""{__name__}.{sys._getframe().f_code.co_name}""
    # First, get list of accounts
    accounts_d: list = async_to_sync(get_account_id_to_name_mapping)()
    # Second, call tasks to enumerate all the roles across all accounts
    for account_id in accounts_d.keys():
        if config.get(""environment"") == ""prod"":
            cache_sqs_queues_for_account.delay(account_id)
        else:
            if account_id in config.get(""celery.test_account_ids"", []):
                cache_sqs_queues_for_account.delay(account_id)
    stats.count(f""{function}.success"")
    return True
","if config . get ( ""environment"" ) == ""prod"" :",200
"def remove(self, path, config=None, error_on_path=False, defaults=None):
    if not path:
        if error_on_path:
            raise NoSuchSettingsPath()
        return
    if config is not None or defaults is not None:
        if config is None:
            config = self._config
        if defaults is None:
            defaults = dict(self._map.parents)
        chain = HierarchicalChainMap(config, defaults)
    else:
        chain = self._map
    try:
        chain.del_by_path(path)
        self._mark_dirty()
    except KeyError:
        if error_on_path:
            raise NoSuchSettingsPath()
        pass
",if config is None :,184
"def PopulateProjectId(project_id=None):
    """"""Fills in a project_id from the boto config file if one is not provided.""""""
    if not project_id:
        default_id = boto.config.get_value(""GSUtil"", ""default_project_id"")
        if not default_id:
            raise ProjectIdException(""MissingProjectId"")
        return default_id
    return project_id
",if not default_id :,101
"def set(self, name, value):
    with self._object_cache_lock:
        old_value = self._object_cache.get(name)
        ret = not old_value or int(old_value.metadata.resource_version) < int(
            value.metadata.resource_version
        )
        if ret:
            self._object_cache[name] = value
    return ret, old_value
",if ret :,106
"def remove(self, url):
    try:
        i = self.items.index(url)
    except (ValueError, IndexError):
        pass
    else:
        was_selected = i in self.selectedindices()
        self.list.delete(i)
        del self.items[i]
        if not self.items:
            self.mp.hidepanel(self.name)
        elif was_selected:
            if i >= len(self.items):
                i = len(self.items) - 1
            self.list.select_set(i)
",elif was_selected :,150
"def add_directory_csv_files(dir_path, paths=None):
    if not paths:
        paths = []
    for p in listdir(dir_path):
        path = join(dir_path, p)
        if isdir(path):
            # call recursively for each dir
            paths = add_directory_csv_files(path, paths)
        elif isfile(path) and path.endswith("".csv""):
            # add every file to the list
            paths.append(path)
    return paths
",if isdir ( path ) :,130
"def _get_client(rp_mapping, resource_provider):
    for key, value in rp_mapping.items():
        if str.lower(key) == str.lower(resource_provider):
            if isinstance(value, dict):
                return GeneralPrivateEndpointClient(
                    key,
                    value[""api_version""],
                    value[""support_list_or_not""],
                    value[""resource_get_api_version""],
                )
            return value()
    raise CLIError(
        ""Resource type must be one of {}"".format("", "".join(rp_mapping.keys()))
    )
",if str . lower ( key ) == str . lower ( resource_provider ) :,165
"def compute_rule_hash(self, rule):
    buf = ""%d-%d-%s-"" % (
        rule.get(""FromPort"", 0) or 0,
        rule.get(""ToPort"", 0) or 0,
        rule.get(""IpProtocol"", ""-1"") or ""-1"",
    )
    for a, ke in self.RULE_ATTRS:
        if a not in rule:
            continue
        ev = [e[ke] for e in rule[a]]
        ev.sort()
        for e in ev:
            buf += ""%s-"" % e
    # mask to generate the same numeric value across all Python versions
    return zlib.crc32(buf.encode(""ascii"")) & 0xFFFFFFFF
",if a not in rule :,176
"def analysis_sucess_metrics(analysis_time: float, allow_exception=False):
    try:
        anchore_engine.subsys.metrics.counter_inc(name=""anchore_analysis_success"")
        anchore_engine.subsys.metrics.histogram_observe(
            ""anchore_analysis_time_seconds"",
            analysis_time,
            buckets=ANALYSIS_TIME_SECONDS_BUCKETS,
            status=""success"",
        )
    except:
        if allow_exception:
            raise
        else:
            logger.exception(
                ""Unexpected exception during metrics update for a successful analysis. Swallowing error and continuing""
            )
",if allow_exception :,179
"def decide_file_icon(file):
    if file.state == File.ERROR:
        return FileItem.icon_error
    elif isinstance(file.parent, Track):
        if file.state == File.NORMAL:
            return FileItem.icon_saved
        elif file.state == File.PENDING:
            return FileItem.match_pending_icons[int(file.similarity * 5 + 0.5)]
        else:
            return FileItem.match_icons[int(file.similarity * 5 + 0.5)]
    elif file.state == File.PENDING:
        return FileItem.icon_file_pending
    else:
        return FileItem.icon_file
",if file . state == File . NORMAL :,169
"def deleteMenu(self, menuName):
    try:
        menu = self.getMenu(menuName)
        if menu:
            self.destroy(menu)
            self.destroyMenu(menuName)
        else:
            g.es(""can't delete menu:"", menuName)
    except Exception:
        g.es(""exception deleting"", menuName, ""menu"")
        g.es_exception()
",if menu :,106
"def parser(cls, buf):
    (type_, code, csum) = struct.unpack_from(cls._PACK_STR, buf)
    msg = cls(type_, code, csum)
    offset = cls._MIN_LEN
    if len(buf) > offset:
        cls_ = cls._ICMPV6_TYPES.get(type_, None)
        if cls_:
            msg.data = cls_.parser(buf, offset)
        else:
            msg.data = buf[offset:]
    return msg, None, None
",if cls_ :,133
"def _load_dataset_area(self, dsid, file_handlers, coords):
    """"""Get the area for *dsid*.""""""
    try:
        return self._load_area_def(dsid, file_handlers)
    except NotImplementedError:
        if any(x is None for x in coords):
            logger.warning(""Failed to load coordinates for '{}'"".format(dsid))
            return None
        area = self._make_area_from_coords(coords)
        if area is None:
            logger.debug(""No coordinates found for %s"", str(dsid))
        return area
",if area is None :,148
"def __getattr__(self, name):
    if Popen.verbose:
        sys.stdout.write(""Getattr: %s..."" % name)
    if name in Popen.__slots__:
        return object.__getattribute__(self, name)
    else:
        if self.popen is not None:
            if Popen.verbose:
                print(""from Popen"")
            return getattr(self.popen, name)
        else:
            if name == ""wait"":
                return self.emu_wait
            else:
                raise Exception(""subprocess emulation: not implemented: %s"" % name)
","if name == ""wait"" :",156
"def update(self, time_delta):
    super().update(time_delta)
    n = self.menu.selected_option
    if n == self.last:
        return
    self.last = n
    s = """"
    for i in range(len(self.files)):
        if self.files[i][0] == n:
            for l in open(self.files[i][1]):
                x = l.strip()
                if len(x) > 1 and x[0] == ""#"":
                    x = ""<b><u>"" + x[1:] + "" </u></b>""
                s += x + ""<br>""
    self.set_text(s)
",if self . files [ i ] [ 0 ] == n :,178
"def wrapper(*args, **kwargs):
    list_args, empty = _apply_defaults(func, args, kwargs)
    if len(dimensions) > len(list_args):
        raise TypeError(
            ""%s takes %i parameters, but %i dimensions were passed""
            % (func.__name__, len(list_args), len(dimensions))
        )
    for dim, value in zip(dimensions, list_args):
        if dim is None:
            continue
        if not ureg.Quantity(value).check(dim):
            val_dim = ureg.get_dimensionality(value)
            raise DimensionalityError(value, ""a quantity of"", val_dim, dim)
    return func(*args, **kwargs)
",if not ureg . Quantity ( value ) . check ( dim ) :,182
"def _check(self, name, size=None, *extra):
    func = getattr(imageop, name)
    for height in VALUES:
        for width in VALUES:
            strlen = abs(width * height)
            if size:
                strlen *= size
            if strlen < MAX_LEN:
                data = ""A"" * strlen
            else:
                data = AAAAA
            if size:
                arguments = (data, size, width, height) + extra
            else:
                arguments = (data, width, height) + extra
            try:
                func(*arguments)
            except (ValueError, imageop.error):
                pass
",if strlen < MAX_LEN :,188
"def wait_send_all_might_not_block(self) -> None:
    with self._send_conflict_detector:
        if self._fd_holder.closed:
            raise trio.ClosedResourceError(""file was already closed"")
        try:
            await trio.lowlevel.wait_writable(self._fd_holder.fd)
        except BrokenPipeError as e:
            # kqueue: raises EPIPE on wait_writable instead
            # of sending, which is annoying
            raise trio.BrokenResourceError from e
",if self . _fd_holder . closed :,135
"def parse_win_proxy(val):
    proxies = []
    for p in val.split("";""):
        if ""="" in p:
            tab = p.split(""="", 1)
            if tab[0] == ""socks"":
                tab[0] = ""SOCKS4""
            proxies.append(
                (tab[0].upper(), tab[1], None, None)
            )  # type, addr:port, username, password
        else:
            proxies.append((""HTTP"", p, None, None))
    return proxies
","if tab [ 0 ] == ""socks"" :",142
"def _super_function(args):
    passed_class, passed_self = args.get_arguments([""type"", ""self""])
    if passed_self is None:
        return passed_class
    else:
        # pyclass = passed_self.get_type()
        pyclass = passed_class
        if isinstance(pyclass, pyobjects.AbstractClass):
            supers = pyclass.get_superclasses()
            if supers:
                return pyobjects.PyObject(supers[0])
        return passed_self
",if supers :,132
"def update_output_mintime(job):
    try:
        return output_mintime[job]
    except KeyError:
        for job_ in chain([job], self.depending[job]):
            try:
                t = output_mintime[job_]
            except KeyError:
                t = job_.output_mintime
            if t is not None:
                output_mintime[job] = t
                return
        output_mintime[job] = None
",if t is not None :,136
"def get_list_of_strings_to_mongo_objects(self, notifications_list=None):
    result = []
    if len(notifications_list) > 0:
        for x in notifications_list:
            split_provider_id = x.split("":"")  # email:id
            if len(split_provider_id) == 2:
                _id = split_provider_id[1]
                cursor = self.get_by_id(_id)
                if cursor:  # Append if exists
                    result.append(cursor)
    return result
",if cursor :,148
"def stop(self):
    with self.lock:
        if not self.process:
            return
        self.task_queue.put(None)
        self.result_queue.put(None)
        process = self.process
        self.process = None
        self.task_queue = None
        self.result_queue = None
    process.join(timeout=0.1)
    if process.exitcode is None:
        os.kill(process.pid, signal.SIGKILL)
        process.join()
",if not self . process :,132
"def on_api_command(self, command, data):
    if command == ""select"":
        if not Permissions.PLUGIN_ACTION_COMMAND_PROMPT_INTERACT.can():
            return flask.abort(403, ""Insufficient permissions"")
        if self._prompt is None:
            return flask.abort(409, ""No active prompt"")
        choice = data[""choice""]
        if not isinstance(choice, int) or not self._prompt.validate_choice(choice):
            return flask.abort(
                400, ""{!r} is not a valid value for choice"".format(choice)
            )
        self._answer_prompt(choice)
",if self . _prompt is None :,164
"def application_openFiles_(self, nsapp, filenames):
    # logging.info('[osx] file open')
    # logging.info('[osx] file : %s' % (filenames))
    for filename in filenames:
        logging.info(""[osx] receiving from macOS : %s"", filename)
        if os.path.exists(filename):
            if sabnzbd.filesystem.get_ext(filename) in VALID_ARCHIVES + VALID_NZB_FILES:
                sabnzbd.add_nzbfile(filename, keep=True)
",if os . path . exists ( filename ) :,136
"def test_error_through_destructor(self):
    # Test that the exception state is not modified by a destructor,
    # even if close() fails.
    rawio = self.CloseFailureIO()
    with support.catch_unraisable_exception() as cm:
        with self.assertRaises(AttributeError):
            self.tp(rawio).xyzzy
        if not IOBASE_EMITS_UNRAISABLE:
            self.assertIsNone(cm.unraisable)
        elif cm.unraisable is not None:
            self.assertEqual(cm.unraisable.exc_type, OSError)
",if not IOBASE_EMITS_UNRAISABLE :,157
"def http_wrapper(self, url, postdata={}):
    try:
        if postdata != {}:
            f = urllib.urlopen(url, postdata)
        else:
            f = urllib.urlopen(url)
        response = f.read()
    except:
        import traceback
        import logging, sys
        cla, exc, tb = sys.exc_info()
        logging.error(url)
        if postdata:
            logging.error(""with post data"")
        else:
            logging.error(""without post data"")
        logging.error(exc.args)
        logging.error(traceback.format_tb(tb))
        response = """"
    return response
",if postdata != { } :,178
"def check_single_file(fn, fetchuri):
    """"""Determine if a single downloaded file is something we can't handle""""""
    with open(fn, ""r"", errors=""surrogateescape"") as f:
        if ""<html"" in f.read(100).lower():
            logger.error(
                'Fetching ""%s"" returned a single HTML page - check the URL is correct and functional'
                % fetchuri
            )
            sys.exit(1)
","if ""<html"" in f . read ( 100 ) . lower ( ) :",117
"def update_properties(self, update_dict):
    signed_attribute_changed = False
    for k, value in update_dict.items():
        if getattr(self, k) != value:
            setattr(self, k, value)
            signed_attribute_changed = signed_attribute_changed or (
                k in self.payload_arguments
            )
    if signed_attribute_changed:
        if self.status != NEW:
            self.status = UPDATED
        self.timestamp = clock.tick()
        self.sign()
    return self
",if self . status != NEW :,145
"def clean_items(event, items, variations):
    for item in items:
        if event != item.event:
            raise ValidationError(_(""One or more items do not belong to this event.""))
        if item.has_variations:
            if not any(var.item == item for var in variations):
                raise ValidationError(
                    _(
                        ""One or more items has variations but none of these are in the variations list.""
                    )
                )
",if not any ( var . item == item for var in variations ) :,127
"def TryMerge(self, d):
    while d.avail() > 0:
        tt = d.getVarInt32()
        if tt == 10:
            length = d.getVarInt32()
            tmp = ProtocolBuffer.Decoder(d.buffer(), d.pos(), d.pos() + length)
            d.skip(length)
            self.add_status().TryMerge(tmp)
            continue
        if tt == 18:
            self.add_doc_id(d.getPrefixedString())
            continue
        if tt == 0:
            raise ProtocolBuffer.ProtocolBufferDecodeError
        d.skipData(tt)
",if tt == 0 :,169
"def connections(self):
    # Connections look something like this:
    # socket:[102422]
    fds = self.open_files
    socket = ""socket:[""
    result = []
    functions = [pwndbg.net.tcp, pwndbg.net.unix, pwndbg.net.netlink]
    for fd, path in fds.items():
        if socket not in path:
            continue
        inode = path[len(socket) : -1]
        inode = int(inode)
        for func in functions:
            for x in func():
                if x.inode == inode:
                    x.fd = fd
                    result.append(x)
    return tuple(result)
",if x . inode == inode :,184
"def _movement_finished(self):
    if self.in_ship_map:
        # if the movement somehow stops, the position sticks, and the unit isn't at next_target any more
        if self._next_target is not None:
            ship = self.session.world.ship_map.get(self._next_target.to_tuple())
            if ship is not None and ship() is self:
                del self.session.world.ship_map[self._next_target.to_tuple()]
    super()._movement_finished()
",if self . _next_target is not None :,136
"def print_addresses(self):
    p = 3
    tmp_str = ""[""
    if self.get_len() >= 7:  # at least one complete IP address
        while 1:
            if p + 1 == self.get_ptr():
                tmp_str += ""#""
            tmp_str += self.get_ip_address(p)
            p += 4
            if p >= self.get_len():
                break
            else:
                tmp_str += "", ""
    tmp_str += ""] ""
    if self.get_ptr() % 4:  # ptr field should be a multiple of 4
        tmp_str += ""nonsense ptr field: %d "" % self.get_ptr()
    return tmp_str
",if p >= self . get_len ( ) :,191
"def source_shapes(self):
    """"""Prints debug information about the sources in this provider.""""""
    if logger.isEnabledFor(logging.DEBUG):
        for i, source in enumerate(self.sources):
            if self.keys is None:
                name = ""anonymous""
            else:
                name = self.keys[i]
            try:
                shape = source.shape()
            except NotImplementedError:
                shape = ""N/A""
            logger.debug(
                'Data source ""%s"": entries=%s, shape=%s', name, len(source), shape
            )
",if self . keys is None :,161
"def swap_actions(actions):
    for mutexgroup in mutex_groups:
        mutex_actions = mutexgroup._group_actions
        if contains_actions(mutex_actions, actions):
            # make a best guess as to where we should store the group
            targetindex = actions.index(mutexgroup._group_actions[0])
            # insert the _ArgumentGroup container
            actions[targetindex] = mutexgroup
            # remove the duplicated individual actions
            actions = [action for action in actions if action not in mutex_actions]
    return actions
","if contains_actions ( mutex_actions , actions ) :",147
"def rec_deps(services, container_by_name, cnt, init_service):
    deps = cnt[""_deps""]
    for dep in deps.copy():
        dep_cnts = services.get(dep)
        if not dep_cnts:
            continue
        dep_cnt = container_by_name.get(dep_cnts[0])
        if dep_cnt:
            # TODO: avoid creating loops, A->B->A
            if init_service and init_service in dep_cnt[""_deps""]:
                continue
            new_deps = rec_deps(services, container_by_name, dep_cnt, init_service)
            deps.update(new_deps)
    return deps
","if init_service and init_service in dep_cnt [ ""_deps"" ] :",181
"def make_dump_list_by_name_list(name_list):
    info_list = []
    for info_name in name_list:
        info = next((x for x in DUMP_LIST if x.info_name == info_name), None)
        if not info:
            raise RuntimeError('Unknown info name: ""{}""'.format(info_name))
        info_list.append(info)
    return info_list
",if not info :,106
"def create(self, private=False):
    try:
        if private:
            log.info(""Creating private channel %s."", self)
            self._bot.api_call(
                ""conversations.create"", data={""name"": self.name, ""is_private"": True}
            )
        else:
            log.info(""Creating channel %s."", self)
            self._bot.api_call(""conversations.create"", data={""name"": self.name})
    except SlackAPIResponseError as e:
        if e.error == ""user_is_bot"":
            raise RoomError(f""Unable to create channel. {USER_IS_BOT_HELPTEXT}"")
        else:
            raise RoomError(e)
","if e . error == ""user_is_bot"" :",189
"def talk(self, words):
    if self.writeSentence(words) == 0:
        return
    r = []
    while 1:
        i = self.readSentence()
        if len(i) == 0:
            continue
        reply = i[0]
        attrs = {}
        for w in i[1:]:
            j = w.find(""="", 1)
            if j == -1:
                attrs[w] = """"
            else:
                attrs[w[:j]] = w[j + 1 :]
        r.append((reply, attrs))
        if reply == ""!done"":
            return r
",if len ( i ) == 0 :,169
"def _load_logfile(self, lfn):
    enc_key = self.decryption_key_func()
    with open(os.path.join(self.logdir, lfn)) as fd:
        if enc_key:
            with DecryptingStreamer(
                fd, mep_key=enc_key, name=""EventLog/DS(%s)"" % lfn
            ) as streamer:
                lines = streamer.read()
                streamer.verify(_raise=IOError)
        else:
            lines = fd.read()
        if lines:
            for line in lines.splitlines():
                event = Event.Parse(line.strip())
                self._events[event.event_id] = event
",if enc_key :,191
"def set_ok_port(self, cookie, request):
    if cookie.port_specified:
        req_port = request_port(request)
        if req_port is None:
            req_port = ""80""
        else:
            req_port = str(req_port)
        for p in cookie.port.split("",""):
            try:
                int(p)
            except ValueError:
                debug(""   bad port %s (not numeric)"", p)
                return False
            if p == req_port:
                break
        else:
            debug(""   request port (%s) not found in %s"", req_port, cookie.port)
            return False
    return True
",if p == req_port :,195
"def get_attribute_value(self, nodeid, attr):
    with self._lock:
        self.logger.debug(""get attr val: %s %s"", nodeid, attr)
        if nodeid not in self._nodes:
            dv = ua.DataValue()
            dv.StatusCode = ua.StatusCode(ua.StatusCodes.BadNodeIdUnknown)
            return dv
        node = self._nodes[nodeid]
        if attr not in node.attributes:
            dv = ua.DataValue()
            dv.StatusCode = ua.StatusCode(ua.StatusCodes.BadAttributeIdInvalid)
            return dv
        attval = node.attributes[attr]
        if attval.value_callback:
            return attval.value_callback()
        return attval.value
",if nodeid not in self . _nodes :,200
"def data_logging_status(self, trail_name, trail_details, api_client):
    for es in api_client.get_event_selectors(TrailName=trail_name)[""EventSelectors""]:
        has_wildcard = {
            u""Values"": [u""arn:aws:s3:::""],
            u""Type"": u""AWS::S3::Object"",
        } in es[""DataResources""]
        is_logging = trail_details[""IsLogging""]
        if has_wildcard and is_logging and self.is_fresh(trail_details):
            return True
    return False
",if has_wildcard and is_logging and self . is_fresh ( trail_details ) :,149
"def pytest_deselected(items):
    if sb_config.dashboard:
        sb_config.item_count -= len(items)
        for item in items:
            test_id, display_id = _get_test_ids_(item)
            if test_id in sb_config._results.keys():
                sb_config._results.pop(test_id)
",if test_id in sb_config . _results . keys ( ) :,96
"def _visit(self, func):
    fname = func[0]
    if fname in self._flags:
        if self._flags[fname] == 1:
            logger.critical(""Fatal error! network ins not Dag."")
            import sys
            sys.exit(-1)
        else:
            return
    else:
        if fname not in self._flags:
            self._flags[fname] = 1
        for output in func[3]:
            for f in self._orig:
                for input in f[2]:
                    if output == input:
                        self._visit(f)
    self._flags[fname] = 2
    self._sorted.insert(0, func)
",if output == input :,188
"def printWiki():
    firstHeading = False
    for m in protocol:
        if m[0] == """":
            if firstHeading:
                output(""|}"")
            __printWikiHeader(m[1], m[2])
            firstHeading = True
        else:
            output(""|-"")
            output(
                '| <span style=""white-space:nowrap;""><tt>'
                + m[0]
                + ""</tt></span> || || ""
                + m[1]
            )
    output(""|}"")
","if m [ 0 ] == """" :",155
"def test_getitem(self):
    n = 200
    d = deque(range(n))
    l = list(range(n))
    for i in range(n):
        d.popleft()
        l.pop(0)
        if random.random() < 0.5:
            d.append(i)
            l.append(i)
        for j in range(1 - len(l), len(l)):
            assert d[j] == l[j]
    d = deque(""superman"")
    self.assertEqual(d[0], ""s"")
    self.assertEqual(d[-1], ""n"")
    d = deque()
    self.assertRaises(IndexError, d.__getitem__, 0)
    self.assertRaises(IndexError, d.__getitem__, -1)
",if random . random ( ) < 0.5 :,193
"def get_num(line, char_ptr, num_chars):
    char_ptr = char_ptr + 1
    numstr = """"
    good = ""-.0123456789""
    while char_ptr < num_chars:
        digit = line[char_ptr]
        if good.find(digit) != -1:
            numstr = numstr + digit
            char_ptr = char_ptr + 1
        else:
            break
    return numstr
",if good . find ( digit ) != - 1 :,116
"def read_digits(source, start, first_code):
    body = source.body
    position = start
    code = first_code
    if code is not None and 48 <= code <= 57:  # 0 - 9
        while True:
            position += 1
            code = char_code_at(body, position)
            if not (code is not None and 48 <= code <= 57):
                break
        return position
    raise GraphQLSyntaxError(
        source,
        position,
        u""Invalid number, expected digit but got: {}."".format(print_char_code(code)),
    )
",if not ( code is not None and 48 <= code <= 57 ) :,155
"def get_aws_metadata(headers, provider=None):
    if not provider:
        provider = boto.provider.get_default()
    metadata_prefix = provider.metadata_prefix
    metadata = {}
    for hkey in headers.keys():
        if hkey.lower().startswith(metadata_prefix):
            val = urllib.unquote_plus(headers[hkey])
            try:
                metadata[hkey[len(metadata_prefix) :]] = unicode(val, ""utf-8"")
            except UnicodeDecodeError:
                metadata[hkey[len(metadata_prefix) :]] = val
            del headers[hkey]
    return metadata
",if hkey . lower ( ) . startswith ( metadata_prefix ) :,164
"def _process_rtdest(self):
    LOG.debug(""Processing RT NLRI destination..."")
    if self._rtdest_queue.is_empty():
        return
    else:
        processed_any = False
        while not self._rtdest_queue.is_empty():
            # We process the first destination in the queue.
            next_dest = self._rtdest_queue.pop_first()
            if next_dest:
                next_dest.process()
                processed_any = True
        if processed_any:
            # Since RT destination were updated we update RT filters
            self._core_service.update_rtfilters()
",if processed_any :,171
"def _get_header(self, requester, header_name):
    hits = sum([header_name in headers for _, headers in requester.requests])
    self.assertEquals(hits, 2 if self.revs_enabled else 1)
    for url, headers in requester.requests:
        if header_name in headers:
            if self.revs_enabled:
                self.assertTrue(url.endswith(""/latest""), msg=url)
            else:
                self.assertTrue(url.endswith(""/download_urls""), msg=url)
            return headers.get(header_name)
",if self . revs_enabled :,143
"def add_external_deps(self, deps):
    for dep in deps:
        if hasattr(dep, ""el""):
            dep = dep.el
        if not isinstance(dep, dependencies.Dependency):
            raise InvalidArguments(""Argument is not an external dependency"")
        self.external_deps.append(dep)
        if isinstance(dep, dependencies.Dependency):
            self.process_sourcelist(dep.get_sources())
","if not isinstance ( dep , dependencies . Dependency ) :",109
"def _consume_msg(self):
    ws = self._ws
    try:
        while True:
            r = await ws.recv()
            if isinstance(r, bytes):
                r = r.decode(""utf-8"")
            msg = json.loads(r)
            stream = msg.get(""stream"")
            if stream is not None:
                await self._dispatch(stream, msg)
    except websockets.WebSocketException as wse:
        logging.warn(wse)
        await self.close()
        asyncio.ensure_future(self._ensure_ws())
",if stream is not None :,158
"def generate_and_check_random():
    random_size = 256
    while True:
        random = os.urandom(random_size)
        a = int.from_bytes(random, ""big"")
        A = pow(g, a, p)
        if is_good_mod_exp_first(A, p):
            a_for_hash = big_num_for_hash(A)
            u = int.from_bytes(sha256(a_for_hash, b_for_hash), ""big"")
            if u > 0:
                return (a, a_for_hash, u)
","if is_good_mod_exp_first ( A , p ) :",158
"def write(self, datagram, address):
    """"""Write a datagram.""""""
    try:
        return self.socket.sendto(datagram, address)
    except OSError as se:
        no = se.args[0]
        if no == EINTR:
            return self.write(datagram, address)
        elif no == EMSGSIZE:
            raise error.MessageLengthError(""message too long"")
        elif no == EAGAIN:
            # oh, well, drop the data. The only difference from UDP
            # is that UDP won't ever notice.
            # TODO: add TCP-like buffering
            pass
        else:
            raise
",elif no == EAGAIN :,176
"def doDir(elem):
    for child in elem.childNodes:
        if not isinstance(child, minidom.Element):
            continue
        if child.tagName == ""Directory"":
            doDir(child)
        elif child.tagName == ""Component"":
            for grandchild in child.childNodes:
                if not isinstance(grandchild, minidom.Element):
                    continue
                if grandchild.tagName != ""File"":
                    continue
                files.add(grandchild.getAttribute(""Source"").replace(os.sep, ""/""))
","if not isinstance ( child , minidom . Element ) :",152
"def add_reversed_tensor(i, X, reversed_X):
    # Do not keep tensors that should stop the mapping.
    if X in stop_mapping_at_tensors:
        return
    if X not in reversed_tensors:
        reversed_tensors[X] = {""id"": (nid, i), ""tensor"": reversed_X}
    else:
        tmp = reversed_tensors[X]
        if ""tensor"" in tmp and ""tensors"" in tmp:
            raise Exception(""Wrong order, tensors already aggregated!"")
        if ""tensor"" in tmp:
            tmp[""tensors""] = [tmp[""tensor""], reversed_X]
            del tmp[""tensor""]
        else:
            tmp[""tensors""].append(reversed_X)
","if ""tensor"" in tmp :",183
"def walk(source, path, default, delimiter="".""):
    """"""Walk the sourch hash given the path and return the value or default if not found""""""
    if not isinstance(source, dict):
        raise RuntimeError(
            ""The source is not a walkable dict: {} path: {}"".format(source, path)
        )
    keys = path.split(delimiter)
    max_depth = len(keys)
    cur_depth = 0
    while cur_depth < max_depth:
        if keys[cur_depth] in source:
            source = source[keys[cur_depth]]
            cur_depth = cur_depth + 1
        else:
            return default
    return source
",if keys [ cur_depth ] in source :,171
"def _from_txt_get_vulns(self):
    file_vulns = []
    vuln_regex = (
        'SQL injection in a .*? was found at: ""(.*?)""'
        ', using HTTP method (.*?). The sent .*?data was: ""(.*?)""'
    )
    vuln_re = re.compile(vuln_regex)
    for line in file(self.OUTPUT_FILE):
        mo = vuln_re.search(line)
        if mo:
            v = MockVuln(""TestCase"", None, ""High"", 1, ""plugin"")
            v.set_url(URL(mo.group(1)))
            v.set_method(mo.group(2))
            file_vulns.append(v)
    return file_vulns
",if mo :,194
"def __get__(self, instance, instance_type=None):
    if instance:
        if self.att_name not in instance._obj_cache:
            rel_obj = self.get_obj(instance)
            if rel_obj:
                instance._obj_cache[self.att_name] = rel_obj
        return instance._obj_cache.get(self.att_name)
    return self
",if rel_obj :,105
"def get_ranges_from_func_set(support_set):
    pos_start = 0
    pos_end = 0
    ranges = []
    for pos, func in enumerate(network.function):
        if func.type in support_set:
            pos_end = pos
        else:
            if pos_end >= pos_start:
                ranges.append((pos_start, pos_end))
            pos_start = pos + 1
    if pos_end >= pos_start:
        ranges.append((pos_start, pos_end))
    return ranges
",if func . type in support_set :,145
"def get_all_active_plugins(self) -> List[BotPlugin]:
    """"""This returns the list of plugins in the callback ordered defined from the config.""""""
    all_plugins = []
    for name in self.plugins_callback_order:
        # None is a placeholder for any plugin not having a defined order
        if name is None:
            all_plugins += [
                plugin
                for name, plugin in self.plugins.items()
                if name not in self.plugins_callback_order and plugin.is_activated
            ]
        else:
            plugin = self.plugins[name]
            if plugin.is_activated:
                all_plugins.append(plugin)
    return all_plugins
",if name not in self . plugins_callback_order and plugin . is_activated,186
"def render_token_list(self, tokens):
    result = []
    vars = []
    for token in tokens:
        if token.token_type == TOKEN_TEXT:
            result.append(token.contents.replace(""%"", ""%%""))
        elif token.token_type == TOKEN_VAR:
            result.append(""%%(%s)s"" % token.contents)
            vars.append(token.contents)
    msg = """".join(result)
    if self.trimmed:
        msg = translation.trim_whitespace(msg)
    return msg, vars
",if token . token_type == TOKEN_TEXT :,139
"def test_build_root_config_overwrite(self):
    cfg = build_root_config(""tests.files.settings_overwrite"")
    for key, val in DEFAULT_SPIDER_GLOBAL_CONFIG.items():
        if key == ""spider_modules"":
            self.assertEqual(cfg[""global""][key], [""zzz""])
        else:
            self.assertEqual(cfg[""global""][key], val)
","if key == ""spider_modules"" :",98
"def get_limit(self, request):
    if self.limit_query_param:
        try:
            limit = int(request.query_params[self.limit_query_param])
            if limit < 0:
                raise ValueError()
            # Enforce maximum page size, if defined
            if settings.MAX_PAGE_SIZE:
                if limit == 0:
                    return settings.MAX_PAGE_SIZE
                else:
                    return min(limit, settings.MAX_PAGE_SIZE)
            return limit
        except (KeyError, ValueError):
            pass
    return self.default_limit
",if settings . MAX_PAGE_SIZE :,169
"def track_handler(handler):
    tid = handler.request.tid
    for event in events_monitored:
        if event[""handler_check""](handler):
            e = Event(event, handler.request.execution_time)
            State.tenant_state[tid].RecentEventQ.append(e)
            State.tenant_state[tid].EventQ.append(e)
            break
","if event [ ""handler_check"" ] ( handler ) :",105
"def TryMerge(self, d):
    while d.avail() > 0:
        tt = d.getVarInt32()
        if tt == 10:
            length = d.getVarInt32()
            tmp = ProtocolBuffer.Decoder(d.buffer(), d.pos(), d.pos() + length)
            d.skip(length)
            self.add_subscription().TryMerge(tmp)
            continue
        if tt == 0:
            raise ProtocolBuffer.ProtocolBufferDecodeError
        d.skipData(tt)
",if tt == 0 :,138
"def GetCreateInstanceBinder(self, info):
    with self._lock:
        if self._createInstanceBinders.ContainsKey(info):
            return self._createInstanceBinders[info]
        b = runtime.SymplCreateInstanceBinder(info)
        self._createInstanceBinders[info] = b
    return b
",if self . _createInstanceBinders . ContainsKey ( info ) :,83
"def process_task(self, body, message):
    if ""control"" in body:
        try:
            return self.control(body, message)
        except Exception:
            logger.exception(""Exception handling control message:"")
            return
    if len(self.pool):
        if ""uuid"" in body and body[""uuid""]:
            try:
                queue = UUID(body[""uuid""]).int % len(self.pool)
            except Exception:
                queue = self.total_messages % len(self.pool)
        else:
            queue = self.total_messages % len(self.pool)
    else:
        queue = 0
    self.pool.write(queue, body)
    self.total_messages += 1
    message.ack()
","if ""uuid"" in body and body [ ""uuid"" ] :",199
"def is_defined_in_base_class(self, var: Var) -> bool:
    if var.info:
        for base in var.info.mro[1:]:
            if base.get(var.name) is not None:
                return True
        if var.info.fallback_to_any:
            return True
    return False
",if var . info . fallback_to_any :,90
"def ant_map(m):
    tmp = ""rows %s\ncols %s\n"" % (len(m), len(m[0]))
    players = {}
    for row in m:
        tmp += ""m ""
        for col in row:
            if col == LAND:
                tmp += "".""
            elif col == BARRIER:
                tmp += ""%""
            elif col == FOOD:
                tmp += ""*""
            elif col == UNSEEN:
                tmp += ""?""
            else:
                players[col] = True
                tmp += chr(col + 97)
        tmp += ""\n""
    tmp = (""players %s\n"" % len(players)) + tmp
    return tmp
",elif col == BARRIER :,199
"def prompt_for_resume(config):
    logger = logging.getLogger(""changeme"")
    logger.error(
        ""A previous scan was interrupted. Type R to resume or F to start a fresh scan""
    )
    answer = """"
    while not (answer == ""R"" or answer == ""F""):
        prompt = ""(R/F)> ""
        answer = """"
        try:
            answer = raw_input(prompt)
        except NameError:
            answer = input(prompt)
        if answer.upper() == ""F"":
            logger.debug(""Forcing a fresh scan"")
        elif answer.upper() == ""R"":
            logger.debug(""Resuming previous scan"")
            config.resume = True
    return config.resume
","if answer . upper ( ) == ""F"" :",189
"def f(view, s):
    if mode == modes.INTERNAL_NORMAL:
        if count == 1:
            if view.line(s.b).size() > 0:
                eol = view.line(s.b).b
                return R(s.b, eol)
            return s
    return s
",if count == 1 :,85
"def flush(self):
    if not self.cuts:
        return
    for move, (x, y, z), cent in douglas(self.cuts, self.tolerance, self.plane):
        if cent:
            self.write(""%s X%.4f Y%.4f Z%.4f %s"" % (move, x, y, z, cent))
            self.lastgcode = None
            self.lastx = x
            self.lasty = y
            self.lastz = z
        else:
            self.move_common(x, y, z, gcode=""G1"")
    self.cuts = []
",if cent :,167
"def copy_shell(self):
    cls = self.__class__
    old_id = cls.id
    new_i = cls()  # create a new group
    new_i.id = self.id  # with the same id
    cls.id = old_id  # Reset the Class counter
    # Copy all properties
    for prop in cls.properties:
        if prop is not ""members"":
            if self.has(prop):
                val = getattr(self, prop)
                setattr(new_i, prop, val)
    # but no members
    new_i.members = []
    return new_i
","if prop is not ""members"" :",156
"def find_region_by_value(key, value):
    for region in cognitoidp_backends:
        backend = cognitoidp_backends[region]
        for user_pool in backend.user_pools.values():
            if key == ""client_id"" and value in user_pool.clients:
                return region
            if key == ""access_token"" and value in user_pool.access_tokens:
                return region
    # If we can't find the `client_id` or `access_token`, we just pass
    # back a default backend region, which will raise the appropriate
    # error message (e.g. NotAuthorized or NotFound).
    return list(cognitoidp_backends)[0]
","if key == ""access_token"" and value in user_pool . access_tokens :",184
"def __init__(
    self, fixed: MQTTFixedHeader = None, variable_header: PacketIdVariableHeader = None
):
    if fixed is None:
        header = MQTTFixedHeader(PUBREL, 0x02)  # [MQTT-3.6.1-1]
    else:
        if fixed.packet_type is not PUBREL:
            raise HBMQTTException(
                ""Invalid fixed packet type %s for PubrelPacket init"" % fixed.packet_type
            )
        header = fixed
    super().__init__(header)
    self.variable_header = variable_header
    self.payload = None
",if fixed . packet_type is not PUBREL :,163
"def _on_event_MetadataStatisticsUpdated(self, event, data):
    with self._selectedFileMutex:
        if self._selectedFile:
            self._setJobData(
                self._selectedFile[""filename""],
                self._selectedFile[""filesize""],
                self._selectedFile[""sd""],
                self._selectedFile[""user""],
            )
",if self . _selectedFile :,99
"def _validate_parameter_range(self, value_hp, parameter_range):
    """"""Placeholder docstring""""""
    for (
        parameter_range_key,
        parameter_range_value,
    ) in parameter_range.__dict__.items():
        if parameter_range_key == ""scaling_type"":
            continue
        # Categorical ranges
        if isinstance(parameter_range_value, list):
            for categorical_value in parameter_range_value:
                value_hp.validate(categorical_value)
        # Continuous, Integer ranges
        else:
            value_hp.validate(parameter_range_value)
","if parameter_range_key == ""scaling_type"" :",159
"def visit_filter_projection(self, node, value):
    base = self.visit(node[""children""][0], value)
    if not isinstance(base, list):
        return None
    comparator_node = node[""children""][2]
    collected = []
    for element in base:
        if self._is_true(self.visit(comparator_node, element)):
            current = self.visit(node[""children""][1], element)
            if current is not None:
                collected.append(current)
    return collected
",if current is not None :,132
"def _getSubstrings(self, va, size, ltyp):
    # rip through the desired memory range to populate any substrings
    subs = set()
    end = va + size
    for offs in range(va, end, 1):
        loc = self.getLocation(offs, range=True)
        if loc and loc[L_LTYPE] == LOC_STRING and loc[L_VA] > va:
            subs.add((loc[L_VA], loc[L_SIZE]))
            if loc[L_TINFO]:
                subs = subs.union(set(loc[L_TINFO]))
    return list(subs)
",if loc and loc [ L_LTYPE ] == LOC_STRING and loc [ L_VA ] > va :,161
"def run(self):
    while not self._stopped:
        try:
            try:
                test_name = next(self.pending)
            except StopIteration:
                break
            mp_result = self._runtest(test_name)
            self.output.put((False, mp_result))
            if must_stop(mp_result.result, self.ns):
                break
        except ExitThread:
            break
        except BaseException:
            self.output.put((True, traceback.format_exc()))
            break
","if must_stop ( mp_result . result , self . ns ) :",151
"def get_in_inputs(key, data):
    if isinstance(data, dict):
        for k, v in data.items():
            if k == key:
                return v
            elif isinstance(v, (list, tuple, dict)):
                out = get_in_inputs(key, v)
                if out:
                    return out
    elif isinstance(data, (list, tuple)):
        out = [get_in_inputs(key, x) for x in data]
        out = [x for x in out if x]
        if out:
            return out[0]
",if out :,160
"def act_mapping(self, items, actions, mapping):
    """"""Executes all the actions on the list of pods.""""""
    success = True
    for action in actions:
        for key, method in mapping.items():
            if key in action:
                params = action.get(key)
                ret = method(items, params)
                if not ret:
                    success = False
    return success
",if not ret :,109
"def _apply(self, plan):
    desired = plan.desired
    changes = plan.changes
    self.log.debug(""_apply: zone=%s, len(changes)=%d"", desired.name, len(changes))
    domain_name = desired.name[:-1]
    try:
        nsone_zone = self._client.loadZone(domain_name)
    except ResourceException as e:
        if e.message != self.ZONE_NOT_FOUND_MESSAGE:
            raise
        self.log.debug(""_apply:   no matching zone, creating"")
        nsone_zone = self._client.createZone(domain_name)
    for change in changes:
        class_name = change.__class__.__name__
        getattr(self, ""_apply_{}"".format(class_name))(nsone_zone, change)
",if e . message != self . ZONE_NOT_FOUND_MESSAGE :,198
"def split_artists(self, json):
    if len(json) == 0:
        ([], [])
    elif len(json) == 1:
        artist = Artist.query.filter_by(name=json[0][""name""]).first()
        return ([artist], [])
    my_artists = []
    other_artists = []
    for artist_dict in json:
        artist = Artist.query.filter_by(name=artist_dict[""name""])
        if artist.count():
            my_artists.append(artist.first())
        else:
            del artist_dict[""thumb_url""]
            other_artists.append(artist_dict)
    return (my_artists, other_artists)
",if artist . count ( ) :,176
"def update_metadata(self):
    for attrname in dir(self):
        if attrname.startswith(""__""):
            continue
        attrvalue = getattr(self, attrname, None)
        if attrvalue == 0:
            continue
        if attrname == ""salt_version"":
            attrname = ""version""
        if hasattr(self.metadata, ""set_{0}"".format(attrname)):
            getattr(self.metadata, ""set_{0}"".format(attrname))(attrvalue)
        elif hasattr(self.metadata, attrname):
            try:
                setattr(self.metadata, attrname, attrvalue)
            except AttributeError:
                pass
",if attrvalue == 0 :,173
"def close(self, code=errno.ECONNRESET):
    with self.shutdown_lock:
        if not self.closed:
            super(RemoteIPRoute, self).close(code=code)
            self.closed = True
            try:
                self._mitogen_call.get()
            except mitogen.core.ChannelError:
                pass
            if self._mitogen_broker is not None:
                self._mitogen_broker.shutdown()
                self._mitogen_broker.join()
",if not self . closed :,142
"def untokenize(self, iterable):
    for t in iterable:
        if len(t) == 2:
            self.compat(t, iterable)
            break
        tok_type, token, start, end, line = t
        self.add_whitespace(start)
        self.tokens.append(token)
        self.prev_row, self.prev_col = end
        if tok_type in (NEWLINE, NL):
            self.prev_row += 1
            self.prev_col = 0
    return """".join(self.tokens)
",if len ( t ) == 2 :,143
"def __call__(self, x, uttid=None):
    if self.utt2spk is not None:
        spk = self.utt2spk[uttid]
    else:
        spk = uttid
    if not self.reverse:
        if self.norm_means:
            x = np.add(x, self.bias[spk])
        if self.norm_vars:
            x = np.multiply(x, self.scale[spk])
    else:
        if self.norm_vars:
            x = np.divide(x, self.scale[spk])
        if self.norm_means:
            x = np.subtract(x, self.bias[spk])
    return x
",if self . norm_vars :,189
"def get_party_total(self, args):
    self.party_total = frappe._dict()
    for d in self.receivables:
        self.init_party_total(d)
        # Add all amount columns
        for k in list(self.party_total[d.party]):
            if k not in [""currency"", ""sales_person""]:
                self.party_total[d.party][k] += d.get(k, 0.0)
        # set territory, customer_group, sales person etc
        self.set_party_details(d)
","if k not in [ ""currency"" , ""sales_person"" ] :",150
"def get_databases(request):
    dbs = {}
    global_env = globals()
    for (key, value) in global_env.items():
        try:
            cond = isinstance(value, GQLDB)
        except:
            cond = isinstance(value, SQLDB)
        if cond:
            dbs[key] = value
    return dbs
",if cond :,97
"def check_twobit_file(dbkey, GALAXY_DATA_INDEX_DIR):
    twobit_file = ""%s/twobit.loc"" % GALAXY_DATA_INDEX_DIR
    twobit_path = """"
    twobits = {}
    for i, line in enumerate(open(twobit_file)):
        line = line.rstrip(""\r\n"")
        if line and not line.startswith(""#""):
            fields = line.split(""\t"")
            if len(fields) < 2:
                continue
            twobits[(fields[0])] = fields[1]
    if dbkey in twobits:
        twobit_path = twobits[(dbkey)]
    return twobit_path
",if len ( fields ) < 2 :,177
"def action(scheduler, _):
    nonlocal state
    nonlocal has_result
    nonlocal result
    nonlocal first
    nonlocal time
    if has_result:
        observer.on_next(result)
    try:
        if first:
            first = False
        else:
            state = iterate(state)
        has_result = condition(state)
        if has_result:
            result = state
            time = time_mapper(state)
    except Exception as e:  # pylint: disable=broad-except
        observer.on_error(e)
        return
    if has_result:
        mad.disposable = scheduler.schedule_relative(time, action)
    else:
        observer.on_completed()
",if has_result :,187
"def orthogonalEnd(self):
    if self.type == Segment.LINE:
        O = self.AB.orthogonal()
        O.norm()
        return O
    else:
        O = self.B - self.C
        O.norm()
        if self.type == Segment.CCW:
            return -O
        else:
            return O
",if self . type == Segment . CCW :,97
"def remove(self, values):
    if not isinstance(values, (list, tuple, set)):
        values = [values]
    for v in values:
        v = str(v)
        if isinstance(self._definition, dict):
            self._definition.pop(v, None)
        elif self._definition == ""ANY"":
            if v == ""ANY"":
                self._definition = []
        elif v in self._definition:
            self._definition.remove(v)
    if (
        self._value is not None
        and self._value not in self._definition
        and self._not_any()
    ):
        raise ConanException(bad_value_msg(self._name, self._value, self.values_range))
","if v == ""ANY"" :",192
"def __enter__(self) -> None:
    try:
        if threading.current_thread() == threading.main_thread():
            signal.signal(signal.SIGALRM, self.handle_timeout)
            signal.alarm(self.seconds)
    except ValueError as ex:
        logger.warning(""timeout can't be used in the current context"")
        logger.exception(ex)
",if threading . current_thread ( ) == threading . main_thread ( ) :,98
"def __init__(self, fixed: MQTTFixedHeader = None):
    if fixed is None:
        header = MQTTFixedHeader(PINGRESP, 0x00)
    else:
        if fixed.packet_type is not PINGRESP:
            raise HBMQTTException(
                ""Invalid fixed packet type %s for PingRespPacket init""
                % fixed.packet_type
            )
        header = fixed
    super().__init__(header)
    self.variable_header = None
    self.payload = None
",if fixed . packet_type is not PINGRESP :,140
"def _put_nowait(self, data, *, sender):
    if not self._running:
        logger.warning(""Pub/Sub listener message after stop: %r, %r"", sender, data)
        return
    self._queue.put_nowait((sender, data))
    if self._waiter is not None:
        fut, self._waiter = self._waiter, None
        if fut.done():
            assert fut.cancelled(), (""Waiting future is in wrong state"", self, fut)
            return
        fut.set_result(None)
",if fut . done ( ) :,139
"def OnAssignBuiltin(self, cmd_val):
    # type: (cmd_value__Assign) -> None
    buf = self._ShTraceBegin()
    if not buf:
        return
    for i, arg in enumerate(cmd_val.argv):
        if i != 0:
            buf.write("" "")
        buf.write(arg)
    for pair in cmd_val.pairs:
        buf.write("" "")
        buf.write(pair.var_name)
        buf.write(""="")
        if pair.rval:
            _PrintShValue(pair.rval, buf)
    buf.write(""\n"")
    self.f.write(buf.getvalue())
",if i != 0 :,169
"def convertDict(obj):
    obj = dict(obj)
    for k, v in obj.items():
        del obj[k]
        if not (isinstance(k, str) or isinstance(k, unicode)):
            k = dumps(k)
            # Keep track of which keys need to be decoded when loading.
            if Types.KEYS not in obj:
                obj[Types.KEYS] = []
            obj[Types.KEYS].append(k)
        obj[k] = convertObjects(v)
    return obj
",if Types . KEYS not in obj :,137
"def _ArgumentListHasDictionaryEntry(self, token):
    """"""Check if the function argument list has a dictionary as an arg.""""""
    if _IsArgumentToFunction(token):
        while token:
            if token.value == ""{"":
                length = token.matching_bracket.total_length - token.total_length
                return length + self.stack[-2].indent > self.column_limit
            if token.ClosesScope():
                break
            if token.OpensScope():
                token = token.matching_bracket
            token = token.next_token
    return False
",if token . OpensScope ( ) :,153
"def get_editable_dict(self):
    ret = {}
    for ref, ws_package in self._workspace_packages.items():
        path = ws_package.root_folder
        if os.path.isdir(path):
            path = os.path.join(path, CONANFILE)
        ret[ref] = {""path"": path, ""layout"": ws_package.layout}
    return ret
",if os . path . isdir ( path ) :,100
"def serialize(self, name=None):
    data = super(WebLink, self).serialize(name)
    data[""contentType""] = self.contentType
    if self.width:
        if self.width not in [100, 50, 33, 25]:
            raise InvalidWidthException(self.width)
        data[""inputOptions""] = {}
        data[""width""] = self.width
    data.update({""content"": {""url"": self.linkUrl, ""text"": self.linkText}})
    return data
","if self . width not in [ 100 , 50 , 33 , 25 ] :",122
"def callback(lexer, match, context):
    text = match.group()
    extra = """"
    if start:
        context.next_indent = len(text)
        if context.next_indent < context.indent:
            while context.next_indent < context.indent:
                context.indent = context.indent_stack.pop()
            if context.next_indent > context.indent:
                extra = text[context.indent :]
                text = text[: context.indent]
    else:
        context.next_indent += len(text)
    if text:
        yield match.start(), TokenClass, text
    if extra:
        yield match.start() + len(text), TokenClass.Error, extra
    context.pos = match.end()
",if context . next_indent < context . indent :,196
"def _handle_unsubscribe(self, web_sock):
    index = None
    with await self._subscriber_lock:
        for i, (subscriber_web_sock, _) in enumerate(self._subscribers):
            if subscriber_web_sock == web_sock:
                index = i
                break
        if index is not None:
            del self._subscribers[index]
        if not self._subscribers:
            asyncio.ensure_future(self._unregister_subscriptions())
",if index is not None :,124
"def test_missing_dict_param():
    expected_err = ""params dictionary did not contain value for placeholder""
    try:
        substitute_params(
            ""SELECT * FROM cust WHERE salesrep = %(name)s"", {""foobar"": ""John Doe""}
        )
        assert False, ""expected exception b/c dict did not contain replacement value""
    except ValueError as exc:
        if expected_err not in str(exc):
            raise
",if expected_err not in str ( exc ) :,110
"def one_gpr_reg_one_mem_scalable(ii):
    n, r = 0, 0
    for op in _gen_opnds(ii):
        if op_agen(op) or (op_mem(op) and op.oc2 in [""v""]):
            n += 1
        elif op_gprv(op):
            r += 1
        else:
            return False
    return n == 1 and r == 1
","if op_agen ( op ) or ( op_mem ( op ) and op . oc2 in [ ""v"" ] ) :",113
"def on_enter(self):
    """"""Fired when mouse enter the bbox of the widget.""""""
    if hasattr(self, ""md_bg_color"") and self.focus_behavior:
        if hasattr(self, ""theme_cls"") and not self.focus_color:
            self.md_bg_color = self.theme_cls.bg_normal
        else:
            if not self.focus_color:
                self.md_bg_color = App.get_running_app().theme_cls.bg_normal
            else:
                self.md_bg_color = self.focus_color
",if not self . focus_color :,154
"def __init__(self, *args, **kwargs):
    BaseCellExporter.__init__(self, *args, **kwargs)
    self.comment = ""#""
    for key in [""cell_marker""]:
        if key in self.unfiltered_metadata:
            self.metadata[key] = self.unfiltered_metadata[key]
    if self.fmt.get(""rst2md""):
        raise ValueError(
            ""The 'rst2md' option is a read only option. The reverse conversion is not ""
            ""implemented. Please either deactivate the option, or save to another format.""
        )  # pragma: no cover
",if key in self . unfiltered_metadata :,150
"def sendQueryQueueByAfterNate(self):
    for i in range(10):
        queryQueueByAfterNateRsp = self.session.httpClint.send(urls.get(""queryQueue""))
        if not queryQueueByAfterNateRsp.get(""status""):
            print(
                """".join(queryQueueByAfterNateRsp.get(""messages""))
                or queryQueueByAfterNateRsp.get(""validateMessages"")
            )
            time.sleep(1)
        else:
            sendEmail(ticket.WAIT_ORDER_SUCCESS)
            sendServerChan(ticket.WAIT_ORDER_SUCCESS)
            raise ticketIsExitsException(ticket.WAIT_AFTER_NATE_SUCCESS)
","if not queryQueueByAfterNateRsp . get ( ""status"" ) :",190
"def filter_errors(self, errors: List[str]) -> List[str]:
    real_errors: List[str] = list()
    current_file = __file__
    current_path = os.path.split(current_file)
    for line in errors:
        line = line.strip()
        if not line:
            continue
        fn, lno, lvl, msg = self.parse_trace_line(line)
        if fn is not None:
            _path = os.path.split(fn)
            if _path[-1] != current_path[-1]:
                continue
        real_errors.append(line)
    return real_errors
",if not line :,171
"def pretty(self, n, comment=True):
    if isinstance(n, (str, bytes, list, tuple, dict)):
        r = repr(n)
        if not comment:  # then it can be inside a comment!
            r = r.replace(""*/"", r""\x2a/"")
        return r
    if not isinstance(n, six.integer_types):
        return n
    if isinstance(n, constants.Constant):
        if comment:
            return ""%s /* %s */"" % (n, self.pretty(int(n)))
        else:
            return ""%s (%s)"" % (n, self.pretty(int(n)))
    elif abs(n) < 10:
        return str(n)
    else:
        return hex(n)
",if comment :,194
"def get_pricings(self, subscription_id: str):
    try:
        client = self.get_client(subscription_id)
        pricings_list = await run_concurrently(lambda: client.pricings.list())
        if hasattr(pricings_list, ""value""):
            return pricings_list.value
        else:
            return []
    except Exception as e:
        print_exception(f""Failed to retrieve pricings: {e}"")
        return []
","if hasattr ( pricings_list , ""value"" ) :",131
"def add_doc(target, variables, body_lines):
    if isinstance(target, ast.Name):
        # if it is a variable name add it to the doc
        name = target.id
        if name not in variables:
            doc = find_doc_for(target, body_lines)
            if doc is not None:
                variables[name] = doc
    elif isinstance(target, ast.Tuple):
        # if it is a tuple then iterate the elements
        # this can happen like this:
        # a, b = 1, 2
        for e in target.elts:
            add_doc(e, variables, body_lines)
",if doc is not None :,167
"def find_word_bounds(self, text, index, allowed_chars):
    right = left = index
    done = False
    while not done:
        if left == 0:
            done = True
        elif not self.word_boundary_char(text[left - 1]):
            left -= 1
        else:
            done = True
    done = False
    while not done:
        if right == len(text):
            done = True
        elif not self.word_boundary_char(text[right]):
            right += 1
        else:
            done = True
    return left, right
",if right == len ( text ) :,159
"def pxrun_nodes(self, *args, **kwargs):
    cell = self._px_cell
    if re.search(r""^\s*%autopx\b"", cell):
        self._disable_autopx()
        return False
    else:
        try:
            result = self.view.execute(cell, silent=False, block=False)
        except:
            self.shell.showtraceback()
            return True
        else:
            if self.view.block:
                try:
                    result.get()
                except:
                    self.shell.showtraceback()
                    return True
                else:
                    result.display_outputs()
            return False
",if self . view . block :,198
"def candidates() -> Generator[""Symbol"", None, None]:
    s = self
    if Symbol.debug_lookup:
        Symbol.debug_print(""searching in self:"")
        print(s.to_string(Symbol.debug_indent + 1), end="""")
    while True:
        if matchSelf:
            yield s
        if recurseInAnon:
            yield from s.children_recurse_anon
        else:
            yield from s._children
        if s.siblingAbove is None:
            break
        s = s.siblingAbove
        if Symbol.debug_lookup:
            Symbol.debug_print(""searching in sibling:"")
            print(s.to_string(Symbol.debug_indent + 1), end="""")
",if s . siblingAbove is None :,190
"def decTaskGen():
    cnt = intbv(0, min=-n, max=n)
    while 1:
        yield clock.posedge, reset.negedge
        if reset == ACTIVE_LOW:
            cnt[:] = 0
            count.next = 0
        else:
            # print count
            decTaskFunc(cnt, enable, reset, n)
            count.next = cnt
",if reset == ACTIVE_LOW :,107
"def __call__(self, *args, **kwargs):
    if not NET_INITTED:
        return self.raw(*args, **kwargs)
    for stack in traceback.walk_stack(None):
        if ""self"" in stack[0].f_locals:
            layer = stack[0].f_locals[""self""]
            if layer in layer_names:
                log.pytorch_layer_name = layer_names[layer]
                print(layer_names[layer])
                break
    out = self.obj(self.raw, *args, **kwargs)
    # if isinstance(out,Variable):
    #     out=[out]
    return out
","if ""self"" in stack [ 0 ] . f_locals :",173
"def to_json_dict(self):
    d = super().to_json_dict()
    d[""bullet_list""] = RenderedContent.rendered_content_list_to_json(self.bullet_list)
    if self.header is not None:
        if isinstance(self.header, RenderedContent):
            d[""header""] = self.header.to_json_dict()
        else:
            d[""header""] = self.header
    if self.subheader is not None:
        if isinstance(self.subheader, RenderedContent):
            d[""subheader""] = self.subheader.to_json_dict()
        else:
            d[""subheader""] = self.subheader
    return d
","if isinstance ( self . subheader , RenderedContent ) :",172
"def add(request):
    form_type = ""servers""
    if request.method == ""POST"":
        form = BookMarkForm(request.POST)
        if form.is_valid():
            form_type = form.save()
            messages.add_message(request, messages.INFO, ""Bookmark created"")
        else:
            messages.add_message(request, messages.INFO, form.errors)
        if form_type == ""server"":
            url = reverse(""servers"")
        else:
            url = reverse(""metrics"")
        return redirect(url)
    else:
        return redirect(reverse(""servers""))
",if form . is_valid ( ) :,164
"def fee_amount_in_quote(self, trading_pair: str, price: Decimal, order_amount: Decimal):
    fee_amount = Decimal(""0"")
    if self.percent > 0:
        fee_amount = (price * order_amount) * self.percent
    base, quote = trading_pair.split(""-"")
    for flat_fee in self.flat_fees:
        if interchangeable(flat_fee[0], base):
            fee_amount += flat_fee[1] * price
        elif interchangeable(flat_fee[0], quote):
            fee_amount += flat_fee[1]
    return fee_amount
","elif interchangeable ( flat_fee [ 0 ] , quote ) :",163
"def load_batch(fpath):
    with open(fpath, ""rb"") as f:
        if sys.version_info > (3, 0):
            # Python3
            d = pickle.load(f, encoding=""latin1"")
        else:
            # Python2
            d = pickle.load(f)
    data = d[""data""]
    labels = d[""labels""]
    return data, labels
","if sys . version_info > ( 3 , 0 ) :",106
"def clear_entries(options):
    """"""Clear pending entries""""""
    with Session() as session:
        query = session.query(db.PendingEntry).filter(db.PendingEntry.approved == False)
        if options.task_name:
            query = query.filter(db.PendingEntry.task_name == options.task_name)
        deleted = query.delete()
        console(""Successfully deleted %i pending entries"" % deleted)
",if options . task_name :,110
"def attribute_table(self, attribute):
    """"""Return a tuple (schema, table) for attribute.""""""
    dimension = attribute.dimension
    if dimension:
        schema = self.naming.dimension_schema or self.naming.schema
        if dimension.is_flat and not dimension.has_details:
            table = self.fact_name
        else:
            table = self.naming.dimension_table_name(dimension)
    else:
        table = self.fact_name
        schema = self.naming.schema
    return (schema, table)
",if dimension . is_flat and not dimension . has_details :,137
"def remove_rating(self, songs, librarian):
    count = len(songs)
    if count > 1 and config.getboolean(""browsers"", ""rating_confirm_multiple""):
        parent = qltk.get_menu_item_top_parent(self)
        dialog = ConfirmRateMultipleDialog(parent, _(""_Remove Rating""), count, None)
        if dialog.run() != Gtk.ResponseType.YES:
            return
    reset = []
    for song in songs:
        if ""~#rating"" in song:
            del song[""~#rating""]
            reset.append(song)
    librarian.changed(reset)
","if ""~#rating"" in song :",159
"def find_word_bounds(self, text, index, allowed_chars):
    right = left = index
    done = False
    while not done:
        if left == 0:
            done = True
        elif not self.word_boundary_char(text[left - 1]):
            left -= 1
        else:
            done = True
    done = False
    while not done:
        if right == len(text):
            done = True
        elif not self.word_boundary_char(text[right]):
            right += 1
        else:
            done = True
    return left, right
",elif not self . word_boundary_char ( text [ right ] ) :,159
"def handle_read(self):
    """"""Called when there is data waiting to be read.""""""
    try:
        chunk = self.recv(self.ac_in_buffer_size)
    except RetryError:
        pass
    except socket.error:
        self.handle_error()
    else:
        self.tot_bytes_received += len(chunk)
        if not chunk:
            self.transfer_finished = True
            # self.close()  # <-- asyncore.recv() already do that...
            return
        if self._data_wrapper is not None:
            chunk = self._data_wrapper(chunk)
        try:
            self.file_obj.write(chunk)
        except OSError as err:
            raise _FileReadWriteError(err)
",if self . _data_wrapper is not None :,200
"def toggle(self, event=None):
    if self.absolute:
        if self.save == self.split:
            self.save = 100
        if self.split > 20:
            self.save = self.split
            self.split = 1
        else:
            self.split = self.save
    else:
        if self.save == self.split:
            self.save = 0.3
        if self.split <= self.min or self.split >= self.max:
            self.split = self.save
        elif self.split < 0.5:
            self.split = self.min
        else:
            self.split = self.max
    self.placeChilds()
",elif self . split < 0.5 :,189
"def readAtOffset(self, offset, size, shortok=False):
    ret = b""""
    self.fd.seek(offset)
    while len(ret) != size:
        rlen = size - len(ret)
        x = self.fd.read(rlen)
        if x == b"""":
            if not shortok:
                return None
            return ret
        ret += x
    return ret
",if not shortok :,111
"def webfinger(environ, start_response, _):
    query = parse_qs(environ[""QUERY_STRING""])
    try:
        rel = query[""rel""]
        resource = query[""resource""][0]
    except KeyError:
        resp = BadRequest(""Missing parameter in request"")
    else:
        if rel != [OIC_ISSUER]:
            resp = BadRequest(""Bad issuer in request"")
        else:
            wf = WebFinger()
            resp = Response(wf.response(subject=resource, base=OAS.baseurl))
    return resp(environ, start_response)
",if rel != [ OIC_ISSUER ] :,152
"def _tokenize(self, text):
    if format_text(text) == EMPTY_TEXT:
        return [self.additional_special_tokens[0]]
    split_tokens = []
    if self.do_basic_tokenize:
        for token in self.basic_tokenizer.tokenize(
            text, never_split=self.all_special_tokens
        ):
            # If the token is part of the never_split set
            if token in self.basic_tokenizer.never_split:
                split_tokens.append(token)
            else:
                split_tokens += self.wordpiece_tokenizer.tokenize(token)
    else:
        split_tokens = self.wordpiece_tokenizer.tokenize(text)
    return split_tokens
",if token in self . basic_tokenizer . never_split :,189
"def send_packed_command(self, command, check_health=True):
    if not self._sock:
        self.connect()
    try:
        if isinstance(command, str):
            command = [command]
        for item in command:
            self._sock.sendall(item)
    except socket.error as e:
        self.disconnect()
        if len(e.args) == 1:
            _errno, errmsg = ""UNKNOWN"", e.args[0]
        else:
            _errno, errmsg = e.args
        raise ConnectionError(
            ""Error %s while writing to socket. %s."" % (_errno, errmsg)
        )
    except Exception:
        self.disconnect()
        raise
",if len ( e . args ) == 1 :,188
"def to_value(self, value):
    # Tip: 'value' is the object returned by
    #      taiga.projects.history.models.HistoryEntry.values_diff()
    ret = {}
    for key, val in value.items():
        if key in [""attachments"", ""custom_attributes"", ""description_diff""]:
            ret[key] = val
        elif key == ""points"":
            ret[key] = {k: {""from"": v[0], ""to"": v[1]} for k, v in val.items()}
        else:
            ret[key] = {""from"": val[0], ""to"": val[1]}
    return ret
","elif key == ""points"" :",169
"def to_child(cls, key=None, process=None):
    if process is not None:
        if type(process) is not dict:
            raise ValueError(
                'Invalid value provided for ""process"" parameter, expected a dictionary'
            )
        if cls.__process__:
            # Merge class `__process__` parameters with provided parameters
            result = {}
            result.update(deepcopy(cls.__process__))
            result.update(process)
            process = result
    class Child(cls):
        __key__ = key
        __process__ = process
        __root__ = False
    Child.__name__ = cls.__name__
    return Child
",if cls . __process__ :,173
"def _super_function(args):
    passed_class, passed_self = args.get_arguments([""type"", ""self""])
    if passed_self is None:
        return passed_class
    else:
        # pyclass = passed_self.get_type()
        pyclass = passed_class
        if isinstance(pyclass, pyobjects.AbstractClass):
            supers = pyclass.get_superclasses()
            if supers:
                return pyobjects.PyObject(supers[0])
        return passed_self
","if isinstance ( pyclass , pyobjects . AbstractClass ) :",132
"def get_data(row):
    data = []
    for field_name, field_xpath in fields:
        result = row.xpath(field_xpath)
        if result:
            result = "" "".join(
                text
                for text in map(
                    six.text_type.strip, map(six.text_type, map(unescape, result))
                )
                if text
            )
        else:
            result = None
        data.append(result)
    return data
",if result :,142
"def say(jarvis, s):
    """"""Reads what is typed.""""""
    if not s:
        jarvis.say(""What should I say?"")
    else:
        voice_state = jarvis.is_voice_enabled()
        jarvis.enable_voice()
        jarvis.say(s)
        if not voice_state:
            jarvis.disable_voice()
",if not voice_state :,99
"def __import__(name, globals=None, locals=None, fromlist=(), level=0):
    module = orig___import__(name, globals, locals, fromlist, level)
    if fromlist and module.__name__ in modules:
        if ""*"" in fromlist:
            fromlist = list(fromlist)
            fromlist.remove(""*"")
            fromlist.extend(getattr(module, ""__all__"", []))
        for x in fromlist:
            if isinstance(getattr(module, x, None), types.ModuleType):
                from_name = ""{}.{}"".format(module.__name__, x)
                if from_name in modules:
                    importlib.import_module(from_name)
    return module
","if ""*"" in fromlist :",175
"def _read_pricing_file(self, region=None, pricing_file=None):
    if not self.__pricing_file_cache:
        if pricing_file:
            logging.info(""Reading pricing file..."")
            with open(pricing_file) as data_file:
                self.__pricing_file_cache = json.load(data_file)
        else:
            self.__pricing_file_cache = self._download_pricing_file(region)
    return self.__pricing_file_cache
",if pricing_file :,130
