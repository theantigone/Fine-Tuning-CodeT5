Input function with masked if condition,Whether the prediction is correct,Expected if condition,Predicted if condition,CodeBLEU prediction score,BLEU-4 prediction score
"def read(self, count=True, timeout=None, ignore_non_errors=True, ignore_timeouts=True): try: return self._read(count, timeout) except usb.USBError as e: if DEBUG_COMM: log.info( ""read: e.errno=%s e.strerror=%s e.message=%s repr=%s"" % (e.errno, e.strerror, e.message, repr(e)) ) <mask>: return [] if ignore_non_errors and is_noerr(e): return [] raise",False,if ignore_timeouts and is_timeout ( e ) :,if ignore_timeouts and is_timeouts ( e ) :,0.8820016898747209,73.48889200874659
"def _cache_mem(curr_out, prev_mem, mem_len, reuse_len=None): """"""cache hidden states into memory."""""" if mem_len is None or mem_len == 0: return None else: if reuse_len is not None and reuse_len > 0: curr_out = curr_out[:reuse_len] <mask>: new_mem = curr_out[-mem_len:] else: new_mem = tf.concat([prev_mem, curr_out], 0)[-mem_len:] new_mem.stop_gradient = True return new_mem",True,if prev_mem is None :,if prev_mem is None :,0.8820016898747209,100.00000000000004
def filtered(gen): for example in gen: example_len = length_fn(example) # Checking max length boundary. if max_length is not None: <mask>: continue # Checking min length boundary. if min_length is not None: if example_len < min_length: continue # Within bounds. yield example,True,if example_len > max_length :,if example_len > max_length :,0.8820016898747209,100.00000000000004
"def search(self, query): # ""Search.ashx?query="" + query + filterVal if not query: logger.debug(""Empty search query"") return [] logger.debug('Searching TuneIn for ""%s""' % query) args = ""&query="" + query search_results = self._tunein(""Search.ashx"", args) results = [] for item in self._flatten(search_results): <mask>: # Only return stations self._stations[item[""guide_id""]] = item results.append(item) return results",False,"if item . get ( ""type"" , """" ) == ""audio"" :","if item [ ""guide_id"" ] not in self . _stations :",0.8820016898747209,5.68077839814409
"def _check_script(self, script, directive): for var in compile_script(script): <mask>: # Skip variable checks return False if var.can_contain("".""): # Yay! Our variable can contain any symbols! reason = ( 'At least variable ""${var}"" can contain untrusted user input'.format( var=var.name ) ) self.add_issue(directive=[directive] + var.providers, reason=reason) return True return False",False,"if var . must_contain ( ""/"" ) :",if not var . can_contain ( directive ) :,0.8820016898747209,19.358307479298794
"def getAllDataLinkIDs(): linkDataIDs = set() dataType = _forestData.dataTypeBySocket for socketID, linkedIDs in _forestData.linkedSockets.items(): for linkedID in linkedIDs: <mask>: # check which one is origin/target linkDataIDs.add( (socketID, linkedID, dataType[socketID], dataType[linkedID]) ) else: linkDataIDs.add( (linkedID, socketID, dataType[linkedID], dataType[socketID]) ) return linkDataIDs",False,if socketID [ 1 ] :,if linkedID == socketID :,0.8820016898747209,10.682175159905853
"def _stderr_supports_color(): try: if hasattr(sys.stderr, ""isatty"") and sys.stderr.isatty(): if curses: curses.setupterm() <mask>: return True elif colorama: if sys.stderr is getattr( colorama.initialise, ""wrapped_stderr"", object() ): return True except Exception: # Very broad exception handling because it's always better to # fall back to non-colored logs than to break at startup. pass return False",False,"if curses . tigetnum ( ""colors"" ) > 0 :",if sys . stderr is curses . getterminal ( ) :,0.8820016898747209,9.00746750211399
"def offsets(self): offsets = {} offset_so_far = 0 for name, ty in self.fields.items(): if isinstance(ty, SimTypeBottom): l.warning( ""Found a bottom field in struct %s. Ignore and increment the offset using the default "" ""element size."", self.name, ) continue if not self._pack: align = ty.alignment <mask>: offset_so_far += align - offset_so_far % align offsets[name] = offset_so_far offset_so_far += ty.size // self._arch.byte_width return offsets",False,if offset_so_far % align != 0 :,if align :,0.8820016898747209,0.0
"def Restore(self): picker, obj = self._window, self._pObject value = obj.RestoreValue(PERSIST_FILEDIRPICKER_PATH) if value is not None: if issubclass(picker.__class__, wx.FileDialog): <mask>: value = value[-1] picker.SetPath(value) return True return False",False,if type ( value ) == list :,"if value [ - 1 ] == ""/"" :",0.8820016898747209,8.516593018819643
"def dt_s_tup_to_string(dt_s_tup): dt_string = dt_s_tup[0] # string for identifying the file to parse. if dt_s_tup[1] > 0: # if there are seasons in the model <mask>: dt_string = dt_string[:2] + ""s"" + dt_string[2:] else: dt_string = ""s"" + dt_string return dt_string",False,"if ""co"" in dt_string or ""ci"" in dt_string or ""nc"" in dt_string :",if len ( dt_string ) > 2 :,0.8820016898747209,3.959981382356705
"def writer(stream, items): sep = """" for item in items: stream.write(sep) sep = "" "" <mask>: item = str(item) if not PY3K: if not isinstance(item, unicode): item = str(item) stream.write(item) stream.write(""\n"")",False,"if not isinstance ( item , str ) :",if not PY3K :,0.8820016898747209,10.129474235115733
"def _get_result_keys(self, config): result_key = config.get(""result_key"") if result_key is not None: <mask>: result_key = [result_key] result_key = [jmespath.compile(rk) for rk in result_key] return result_key",True,"if not isinstance ( result_key , list ) :","if not isinstance ( result_key , list ) :",0.8820016898747209,100.00000000000004
"def _download_build_artifacts(self, build: Dict[str, Any]) -> None: arch = build[""arch_tag""] snap_build = self._lp_load_url(build[""self_link""]) urls = snap_build.getFileUrls() if not urls: logger.error(f""Snap file not available for arch {arch!r}."") return for url in urls: file_name = _get_url_basename(url) self._download_file(url=url, dst=file_name) <mask>: logger.info(f""Snapped {file_name}"") else: logger.info(f""Fetched {file_name}"")",False,"if file_name . endswith ( "".snap"" ) :","if arch == ""snap"" :",0.8820016898747209,7.433761660133445
"def _add_custom_statement(self, custom_statements): if custom_statements is None: return self.resource_policy[""Version""] = ""2012-10-17"" if self.resource_policy.get(""Statement"") is None: self.resource_policy[""Statement""] = custom_statements else: if not isinstance(custom_statements, list): custom_statements = [custom_statements] statement = self.resource_policy[""Statement""] if not isinstance(statement, list): statement = [statement] for s in custom_statements: <mask>: statement.append(s) self.resource_policy[""Statement""] = statement",True,if s not in statement :,if s not in statement :,0.8820016898747209,100.00000000000004
"def display_failures_for_single_test(result: TestResult) -> None: """"""Display a failure for a single method / endpoint."""""" display_subsection(result) checks = _get_unique_failures(result.checks) for idx, check in enumerate(checks, 1): message: Optional[str] <mask>: message = f""{idx}. {check.message}"" else: message = None example = cast(Case, check.example) # filtered in `_get_unique_failures` display_example(example, check.name, message, result.seed) # Display every time except the last check if idx != len(checks): click.echo(""\n"")",True,if check . message :,if check . message :,0.8820016898747209,100.00000000000004
"def build(opt): dpath = os.path.join(opt[""datapath""], ""qangaroo"") version = ""v1.1"" if not build_data.built(dpath, version_string=version): print(""[building data: "" + dpath + ""]"") <mask>: # An older version exists, so remove these outdated files. build_data.remove_dir(dpath) build_data.make_dir(dpath) # Download the data. for downloadable_file in RESOURCES: downloadable_file.download_file(dpath) # Mark the data as built. build_data.mark_done(dpath, version_string=version)",True,if build_data . built ( dpath ) :,if build_data . built ( dpath ) :,0.8820016898747209,100.00000000000004
"def call(self, step_input, states): new_states = [] for i in range(self.num_layers): out, new_state = self.lstm_cells[i](step_input, states[i]) step_input = ( layers.dropout( out, self.dropout_prob, dropout_implementation=""upscale_in_train"" ) <mask>: else out ) new_states.append(new_state) return step_input, new_states",False,if self . dropout_prob > 0.0,if i == self . num_layers - 1,0.8820016898747209,8.913765521398126
"def jupyter_progress_bar(min=0, max=1.0): """"""Returns an ipywidget progress bar or None if we can't import it"""""" widgets = wandb.util.get_module(""ipywidgets"") try: <mask>: # TODO: this currently works in iPython but it's deprecated since 4.0 from IPython.html import widgets # type: ignore assert hasattr(widgets, ""VBox"") assert hasattr(widgets, ""Label"") assert hasattr(widgets, ""FloatProgress"") return ProgressWidget(widgets, min=min, max=max) except (ImportError, AssertionError): return None",False,if widgets is None :,"if hasattr ( widgets , ""ProgressWidget"" ) :",0.8820016898747209,5.522397783539471
"def _record_event(self, path, fsevent_handle, filename, events, error): with self.lock: self.events[path].append(events) <mask>: if not os.path.exists(path): self.watches.pop(path).close()",False,if events | pyuv . fs . UV_RENAME :,if not self . watches [ path ] :,0.8820016898747209,5.0243511979240845
"def _get_v1_id_from_tags(self, tags_obj, tag): """"""Get image id from array of tags"""""" if isinstance(tags_obj, dict): try: return tags_obj[tag] except KeyError: pass elif isinstance(tags_obj, []): try: for tag_dict in tags_obj: <mask>: return tag_dict[""layer""] except KeyError: pass return """"",False,"if tag_dict [ ""name"" ] == tag :","if tag_dict [ ""tag"" ] == tag :",0.8820016898747209,76.11606003349888
"def query_lister(domain, query="""", max_items=None, attr_names=None): more_results = True num_results = 0 next_token = None while more_results: rs = domain.connection.query_with_attributes( domain, query, attr_names, next_token=next_token ) for item in rs: <mask>: if num_results == max_items: raise StopIteration yield item num_results += 1 next_token = rs.next_token more_results = next_token != None",False,if max_items :,if item . next_token == next_token :,0.8820016898747209,4.456882760699063
"def filter(this, args): array = to_object(this, args.space) callbackfn = get_arg(args, 0) arr_len = js_arr_length(array) if not is_callable(callbackfn): raise MakeError(""TypeError"", ""callbackfn must be a function"") _this = get_arg(args, 1) k = 0 res = [] while k < arr_len: <mask>: kValue = array.get(unicode(k)) if to_boolean(callbackfn.call(_this, (kValue, float(k), array))): res.append(kValue) k += 1 return args.space.ConstructArray(res)",False,if array . has_property ( unicode ( k ) ) :,if array . get ( unicode ( k ) ) :,0.8820016898747209,58.50343668259105
"def every_one_is(self, dst): msg = ""all members of %r should be %r, but the %dth is %r"" for index, item in enumerate(self._src): if self._range: if index < self._range[0] or index > self._range[1]: continue error = msg % (self._src, dst, index, item) <mask>: raise AssertionError(error) return True",False,if item != dst :,if error :,0.8820016898747209,0.0
"def schedule_logger(job_id=None, delete=False): if not job_id: return getLogger(""fate_flow_schedule"") else: if delete: with LoggerFactory.lock: try: for key in LoggerFactory.schedule_logger_dict.keys(): <mask>: del LoggerFactory.schedule_logger_dict[key] except: pass return True key = job_id + ""schedule"" if key in LoggerFactory.schedule_logger_dict: return LoggerFactory.schedule_logger_dict[key] return LoggerFactory.get_schedule_logger(job_id)",False,if job_id in key :,if key in LoggerFactory . schedule_logger_dict [ key ] :,0.8820016898747209,4.246549372656572
"def Tokenize(s): # type: (str) -> Iterator[Token] for item in TOKEN_RE.findall(s): # The type checker can't know the true type of item! item = cast(TupleStr4, item) if item[0]: typ = ""number"" val = item[0] elif item[1]: typ = ""name"" val = item[1] <mask>: typ = item[2] val = item[2] elif item[3]: typ = item[3] val = item[3] yield Token(typ, val)",True,elif item [ 2 ] :,elif item [ 2 ] :,0.8820016898747209,100.00000000000004
"def _read_data_from_all_categories(self, directory, config, categories): lines = [] for category in categories: data_file = os.path.join(directory, _DATASET_VERSION, category, config) <mask>: with open(data_file) as f: ls = f.read().split(""\n"") for l in ls[::-1]: if not l: ls.remove(l) lines.extend(ls) return lines",True,if os . path . exists ( data_file ) :,if os . path . exists ( data_file ) :,0.8820016898747209,100.00000000000004
"def find_handlers(self, forms): handlers = {} for form in forms.itervalues(): for action_name, _action_label in form.actions: <mask>: handlers[action_name] = form else: raise HandlerError( ""More than one form defines the handler %s"" % action_name ) return handlers",False,if action_name not in handlers :,if action_name in self . handlers :,0.8820016898747209,36.88939732334405
"def get_story_task_completed_body(payload: Dict[str, Any]) -> Optional[str]: action = get_action_with_primary_id(payload) kwargs = { ""task_description"": action[""description""], } story_id = action[""story_id""] for ref in payload[""references""]: <mask>: kwargs[""name_template""] = STORY_NAME_TEMPLATE.format( name=ref[""name""], app_url=ref[""app_url""], ) if action[""changes""][""complete""][""new""]: return STORY_TASK_COMPLETED_TEMPLATE.format(**kwargs) else: return None",False,"if ref [ ""id"" ] == story_id :","if ref [ ""story_id"" ] == story_id :",0.8820016898747209,72.41577342575832
"def _create_valid_graph(graph): nodes = graph.nodes() for i in range(len(nodes)): for j in range(len(nodes)): <mask>: continue edge = (nodes[i], nodes[j]) if graph.has_edge(edge): graph.del_edge(edge) graph.add_edge(edge, 1)",True,if i == j :,if i == j :,0.8820016898747209,100.00000000000004
"def _post_order(op): if isinstance(op, tvm.tir.Allocate): lift_stmt[-1].append(op) return op.body if isinstance(op, tvm.tir.AttrStmt): <mask>: lift_stmt[-1].append(op) return op.body if op.attr_key == ""virtual_thread"": return _merge_block(lift_stmt.pop() + [op], op.body) return op if isinstance(op, tvm.tir.For): return _merge_block(lift_stmt.pop() + [op], op.body) raise RuntimeError(""not reached"")",False,"if op . attr_key == ""storage_scope"" :","if op . attr_key == ""virtual_thread"" :",0.8820016898747209,65.91844162499147
"def format_lazy_import(names): """"""Formats lazy import lines"""""" lines = """" for _, name, asname in names: pkg, _, _ = name.partition(""."") <mask>: line = ""{pkg} = _LazyModule.load({pkg!r}, {mod!r})\n"" else: line = ""{asname} = _LazyModule.load({pkg!r}, {mod!r}, {asname!r})\n"" lines += line.format(pkg=pkg, mod=name, asname=asname) return lines",False,if asname is None :,"if pkg == ""lazy"" :",0.8820016898747209,6.567274736060395
"def evaluateWord(self, argument): wildcard_count = argument[0].count(""*"") if wildcard_count > 0: if wildcard_count == 1 and argument[0].startswith(""*""): return self.GetWordWildcard(argument[0][1:], method=""endswith"") if wildcard_count == 1 and argument[0].endswith(""*""): return self.GetWordWildcard(argument[0][:-1], method=""startswith"") else: _regex = argument[0].replace(""*"", "".+"") matched = False for w in self.words: matched = bool(re.search(_regex, w)) <mask>: break return matched return self.GetWord(argument[0])",True,if matched :,if matched :,0.8820016898747209,0.0
"def setup(self, ir: ""IR"", aconf: Config) -> bool: if self.kind == ""ConsulResolver"": self.resolve_with = ""consul"" <mask>: self.post_error(""ConsulResolver is required to have a datacenter"") return False elif self.kind == ""KubernetesServiceResolver"": self.resolve_with = ""k8s"" elif self.kind == ""KubernetesEndpointResolver"": self.resolve_with = ""k8s"" else: self.post_error(f""Resolver kind {self.kind} unknown"") return False return True",False,"if not self . get ( ""datacenter"" ) :","elif self . kind == ""datacenter"" :",0.8820016898747209,18.60045401920258
"def get_success_url(self): """"""Continue to the flow index or redirect according `?back` parameter."""""" if ""back"" in self.request.GET: back_url = self.request.GET[""back""] <mask>: back_url = ""/"" return back_url return reverse(self.success_url)",False,"if not is_safe_url ( url = back_url , allowed_hosts = { self . request . get_host ( ) } ) :","if not back_url . startswith ( ""/"" ) :",0.8820016898747209,4.506693101226436
"def download_main( download, download_playlist, urls, playlist, output_dir, merge, info_only ): for url in urls: if url.startswith(""https://""): url = url[8:] <mask>: url = ""http://"" + url if playlist: download_playlist( url, output_dir=output_dir, merge=merge, info_only=info_only ) else: download(url, output_dir=output_dir, merge=merge, info_only=info_only)",False,"if not url . startswith ( ""http://"" ) :","elif url . startswith ( ""http://"" ) :",0.8820016898747209,84.46319809857219
"def __str__(self): buf = [""""] if self.fileName: buf.append(self.fileName + "":"") if self.line != -1: <mask>: buf.append(""line "") buf.append(str(self.line)) if self.column != -1: buf.append("":"" + str(self.column)) buf.append("":"") buf.append("" "") return str("""").join(buf)",False,if not self . fileName :,if self . line != - 1 :,0.8820016898747209,11.339582221952005
"def parse_bash_set_output(output): """"""Parse Bash-like 'set' output"""""" if not sys.platform.startswith(""win""): # Replace ""\""-continued lines in *Linux* environment dumps. # Cannot do this on Windows because a ""\"" at the end of the # line does not imply a continuation. output = output.replace(""\\\n"", """") environ = {} for line in output.splitlines(0): line = line.rstrip() if not line: continue # skip black lines item = _ParseBashEnvStr(line) <mask>: environ[item[0]] = item[1] return environ",True,if item :,if item :,0.8820016898747209,0.0
"def remove_selected(self): """"""Removes selected items from list."""""" to_delete = [] for i in range(len(self)): if self[i].selected: to_delete.append(i) to_delete.reverse() for i in to_delete: self.pop(i) if len(to_delete) > 0: first_to_delete = to_delete[-1] <mask>: self[0].selected = True elif first_to_delete > 0: self[first_to_delete - 1].selected = True",False,if first_to_delete == 0 and len ( self ) > 0 :,if first_to_delete == 0 :,0.8820016898747209,46.21246966669783
"def update(self, update_tracks=True): self.enable_update_metadata_images(False) old_album_title = self.metadata[""album""] self.metadata[""album""] = config.setting[""nat_name""] for track in self.tracks: <mask>: track.metadata[""album""] = self.metadata[""album""] for file in track.linked_files: track.update_file_metadata(file) self.enable_update_metadata_images(True) super().update(update_tracks)",False,"if old_album_title == track . metadata [ ""album"" ] :","if track . metadata [ ""album"" ] != old_album_title :",0.8820016898747209,66.3767913214782
"def on_input(self, target, message): if message.strip() == """": self.panel(""No commit message provided"") return if target: command = [""git"", ""add""] <mask>: command.append(""--all"") else: command.extend((""--"", target)) self.run_command(command, functools.partial(self.add_done, message)) else: self.add_done(message, """")",False,"if target == ""*"" :","if target == ""all"" :",0.8820016898747209,59.4603557501361
"def go_to_last_edit_location(self): if self.last_edit_cursor_pos is not None: filename, position = self.last_edit_cursor_pos <mask>: self.last_edit_cursor_pos = None return else: self.load(filename) editor = self.get_current_editor() if position < editor.document().characterCount(): editor.set_cursor_position(position)",False,if not osp . isfile ( filename ) :,if filename is None :,0.8820016898747209,6.316906128202129
"def returnByType(self, results): new_results = {} for r in results: type_name = r.get(""type"", ""movie"") + ""s"" <mask>: new_results[type_name] = [] new_results[type_name].append(r) # Combine movies, needs a cleaner way.. if ""movies"" in new_results: new_results[""movies""] = self.combineOnIMDB(new_results[""movies""]) return new_results",True,if type_name not in new_results :,if type_name not in new_results :,0.8820016898747209,100.00000000000004
"def cache_sns_topics_across_accounts() -> bool: function: str = f""{__name__}.{sys._getframe().f_code.co_name}"" # First, get list of accounts accounts_d: list = async_to_sync(get_account_id_to_name_mapping)() for account_id in accounts_d.keys(): if config.get(""environment"") == ""prod"": cache_sns_topics_for_account.delay(account_id) else: <mask>: cache_sns_topics_for_account.delay(account_id) stats.count(f""{function}.success"") return True",False,"if account_id in config . get ( ""celery.test_account_ids"" , [ ] ) :",if account_id in list :,0.8820016898747209,6.5440664409673674
"def get(self, subject, topic): """"""Handles GET requests."""""" if subject in feconf.AVAILABLE_LANDING_PAGES: <mask>: self.render_template(""topic-landing-page.mainpage.html"") else: raise self.PageNotFoundException else: raise self.PageNotFoundException",False,if topic in feconf . AVAILABLE_LANDING_PAGES [ subject ] :,if topic in feconf . AVAILABLE_LANDING_PAGES :,0.8820016898747209,69.63547789070398
"def callback(compiled): <mask>: logger.show_tabulated( ""Compiled"", showpath(codepath), ""without writing to file."" ) else: with univ_open(destpath, ""w"") as opened: writefile(opened, compiled) logger.show_tabulated(""Compiled to"", showpath(destpath), ""."") if self.show: print(compiled) if run: if destpath is None: self.execute(compiled, path=codepath, allow_show=False) else: self.execute_file(destpath)",True,if destpath is None :,if destpath is None :,0.8820016898747209,100.00000000000004
"def _find_start_index(self, string, start, end): while True: index = string.find(""{"", start, end) - 1 if index < 0: return -1 <mask>: return index start = index + 2",False,"if self . _start_index_is_ok ( string , index ) :","if string . startswith ( ""{"" , index ) :",0.8820016898747209,16.62073269874415
"def _get_nlu_target_format(export_path: Text) -> Text: guessed_format = loading.guess_format(export_path) if guessed_format not in {MARKDOWN, RASA, RASA_YAML}: if rasa.shared.data.is_likely_json_file(export_path): guessed_format = RASA elif rasa.shared.data.is_likely_markdown_file(export_path): guessed_format = MARKDOWN <mask>: guessed_format = RASA_YAML return guessed_format",True,elif rasa . shared . data . is_likely_yaml_file ( export_path ) :,elif rasa . shared . data . is_likely_yaml_file ( export_path ) :,0.8820016898747209,100.00000000000004
"def moveToThreadNext(self): """"""Move a position to threadNext position."""""" p = self if p.v: if p.v.children: p.moveToFirstChild() el<mask>: p.moveToNext() else: p.moveToParent() while p: if p.hasNext(): p.moveToNext() break # found p.moveToParent() # not found. return p",False,if p . hasNext ( ) :,el = p . v . children [ 0 ],0.8820016898747209,8.392229812593097
"def copy_attributes(info_add, obj, name_fmt, attributes, formatter=None): for attr in attributes: value = getattr(obj, attr, None) <mask>: continue name = name_fmt % attr if formatter is not None: value = formatter(attr, value) info_add(name, value)",True,if value is None :,if value is None :,0.8820016898747209,100.00000000000004
"def getElement(self, aboutUri, namespace, name): for desc in self.rdfRoot.getElementsByTagNameNS(RDF_NAMESPACE, ""Description""): <mask>: attr = desc.getAttributeNodeNS(namespace, name) if attr != None: yield attr for element in desc.getElementsByTagNameNS(namespace, name): yield element",False,"if desc . getAttributeNS ( RDF_NAMESPACE , ""about"" ) == aboutUri :",if aboutUri == desc . getQName ( ) :,0.8820016898747209,7.218248978775658
def run(self): while not self.completed: if self.block: time.sleep(self.period) else: self._completed.wait(self.period) self.counter += 1 try: self.callback(self.counter) except Exception: self.stop() <mask>: dt = time.time() - self._start_time if dt > self.timeout: self.stop() if self.counter == self.count: self.stop(),False,if self . timeout is not None :,if self . counter == self . count :,0.8820016898747209,16.784459625186194
"def _parse_fixits(message, titer, line): """"""Parses fixit messages."""""" while ( OutputParser.message_line_re.match(line) is None and OutputParser.note_line_re.match(line) is None ): message_text = line.strip() <mask>: message.fixits.append( Note( message.path, message.line, line.find(message_text) + 1, message_text, ) ) line = next(titer) return line",False,"if message_text != """" :",if message_text :,0.8820016898747209,31.772355751081438
"def _connect_db(self, force_reconnect=False): thread_id = thread.get_ident() if force_reconnect and thread_id in ENGINES: del ENGINES[thread_id] conn = None try: engine = ENGINES[thread_id] conn = engine.connect() _test = conn.execute(""SELECT 1"") _test.fetchall() except (KeyError, MySQLdb.OperationalError): <mask>: conn.close() engine = sqla.create_engine(self.db_url, pool_recycle=3600) ENGINES[thread_id] = engine conn = engine.connect() return conn",True,if conn :,if conn :,0.8820016898747209,0.0
"def read(self, n): if self.current_frame: data = self.current_frame.read(n) <mask>: self.current_frame = None return self.file_read(n) if len(data) < n: raise UnpicklingError(""pickle exhausted before end of frame"") return data else: return self.file_read(n)",False,if not data and n != 0 :,if not data :,0.8820016898747209,18.306026428729766
"def __setLoadCmd(self): base = self.__rawLoadCmd for _ in range(self.__machHeader.ncmds): command = LOAD_COMMAND.from_buffer_copy(base) <mask>: segment = SEGMENT_COMMAND.from_buffer_copy(base) self.__setSections(segment, base[56:], 32) elif command.cmd == MACHOFlags.LC_SEGMENT_64: segment = SEGMENT_COMMAND64.from_buffer_copy(base) self.__setSections(segment, base[72:], 64) base = base[command.cmdsize :]",False,if command . cmd == MACHOFlags . LC_SEGMENT :,if command . cmd == MACHOFlags . LC_SEGMENT_32 :,0.8820016898747209,77.4403141014203
"def emit_post_sync_signal(created_models, verbosity, interactive, db): # Emit the post_sync signal for every application. for app in models.get_apps(): app_name = app.__name__.split(""."")[-2] <mask>: print(""Running post-sync handlers for application %s"" % app_name) models.signals.post_syncdb.send( sender=app, app=app, created_models=created_models, verbosity=verbosity, interactive=interactive, db=db, )",False,if verbosity >= 2 :,if app_name in models . signals . post_syncdb . get_apps ( ) :,0.8820016898747209,2.4074859035470344
"def git_pull(args): if len(args) <= 1: repo = _get_repo() _confirm_dangerous() url = args[0] if len(args) == 1 else repo.remotes.get(""origin"", """") if url in repo.remotes: origin = url url = repo.remotes.get(origin) <mask>: repo.pull(origin_uri=url) else: print(""No pull URL."") else: print(command_help[""git pull""])",True,if url :,if url :,0.8820016898747209,0.0
"def version(self): try: return self._version except AttributeError: for line in self._get_metadata(self.PKG_INFO): <mask>: self._version = safe_version(line.split("":"", 1)[1].strip()) return self._version else: tmpl = ""Missing 'Version:' header and/or %s file"" raise ValueError(tmpl % self.PKG_INFO, self)",False,"if line . lower ( ) . startswith ( ""version:"" ) :","if line . startswith ( ""Version:"" ) :",0.8820016898747209,39.189158241349624
"def increment(self, metric, labels, delta): """"""Increment a value by |delta|."""""" with self._lock: key = self._get_key(metric.name, labels) <mask>: start_time = self._store[key].start_time value = self._store[key].value + delta else: start_time = time.time() value = metric.default_value + delta self._store[key] = _StoreValue(metric, labels, start_time, value)",True,if key in self . _store :,if key in self . _store :,0.8820016898747209,100.00000000000004
"def get_current_connections(session): """"""Retrieves open connections using the the given session"""""" # Use Show process list to count the open sesions. res = session.sql(""SHOW PROCESSLIST"").execute() rows = res.fetch_all() connections = {} for row in rows: <mask>: connections[row.get_string(""User"")] = [row.get_string(""Host"")] else: connections[row.get_string(""User"")].append(row.get_string(""Host"")) return connections",True,"if row . get_string ( ""User"" ) not in connections :","if row . get_string ( ""User"" ) not in connections :",0.8820016898747209,100.00000000000004
"def asset(*paths): for path in paths: fspath = www_root + ""/assets/"" + path etag = """" try: <mask>: etag = asset_etag(fspath) else: os.stat(fspath) except FileNotFoundError as e: if path == paths[-1]: if not os.path.exists(fspath + "".spt""): tell_sentry(e, {}) else: continue except Exception as e: tell_sentry(e, {}) return asset_url + path + (etag and ""?etag="" + etag)",False,if env . cache_static :,"if os . path . exists ( fspath + "".spt"" ) :",0.8820016898747209,3.4585921141027356
def thread_loop(self) -> None: while not self.stop_event.is_set(): time.sleep(1) new_trials = self.study.trials with self.lock: need_to_add_callback = self.new_trials is None self.new_trials = new_trials <mask>: self.doc.add_next_tick_callback(self.update_callback),True,if need_to_add_callback :,if need_to_add_callback :,0.8820016898747209,100.00000000000004
"def _cache_db_tables_iterator(tables, cache_alias, db_alias): no_tables = not tables cache_aliases = settings.CACHES if cache_alias is None else (cache_alias,) db_aliases = settings.DATABASES if db_alias is None else (db_alias,) for db_alias in db_aliases: if no_tables: tables = connections[db_alias].introspection.table_names() <mask>: for cache_alias in cache_aliases: yield cache_alias, db_alias, tables",True,if tables :,if tables :,0.8820016898747209,0.0
"def remove_subscriber(self, topic, subscriber): if subscriber in self.subscribers[topic]: if hasattr(subscriber, ""_pyroRelease""): subscriber._pyroRelease() <mask>: try: proxy = self.proxy_cache[subscriber._pyroUri] proxy._pyroRelease() del self.proxy_cache[subscriber._pyroUri] except KeyError: pass self.subscribers[topic].discard(subscriber)",False,"if hasattr ( subscriber , ""_pyroUri"" ) :",if subscriber . _pyroUri in self . proxy_cache :,0.8820016898747209,8.516593018819643
"def test_constructor(job_id): with patch(""apscheduler.job.Job._modify"") as _modify: scheduler_mock = MagicMock(BaseScheduler) job = Job(scheduler_mock, id=job_id) assert job._scheduler is scheduler_mock assert job._jobstore_alias is None modify_kwargs = _modify.call_args[1] <mask>: assert len(modify_kwargs[""id""]) == 32 else: assert modify_kwargs[""id""] == job_id",False,if job_id is None :,"if ""id"" in modify_kwargs :",0.8820016898747209,6.742555929751843
"def get_connection(self): if self.config.proxy_host != """": return httplib.HTTPConnection(self.config.proxy_host, self.config.proxy_port) else: <mask>: return httplib.HTTPSConnection(self.config.simpledb_host) else: return httplib.HTTPConnection(self.config.simpledb_host)",False,if self . config . use_https :,if self . config . ssl_enabled :,0.8820016898747209,48.54917717073236
"def notify_login(self, ipaddress=""""): if app.NOTIFY_ON_LOGIN: update_text = common.notifyStrings[common.NOTIFY_LOGIN_TEXT] title = common.notifyStrings[common.NOTIFY_LOGIN] <mask>: self._notify_pht(title, update_text.format(ipaddress))",False,if update_text and title and ipaddress :,if title :,0.8820016898747209,0.0
"def _getItemHeight(self, item, ctrl=None): """"""Returns the full height of the item to be inserted in the form"""""" if type(ctrl) == psychopy.visual.TextBox2: return ctrl.size[1] if type(ctrl) == psychopy.visual.Slider: # Set radio button layout if item[""layout""] == ""horiz"": return 0.03 + ctrl.labelHeight * 3 <mask>: # for vertical take into account the nOptions return ctrl.labelHeight * len(item[""options""])",False,"elif item [ ""layout"" ] == ""vert"" :",elif type ( ctrl ) == psychopy . visual . TextBox :,0.8820016898747209,7.347053125977879
"def _get_errors_lines(self): """"""Return the number of lines that contains errors to highlight."""""" errors_lines = [] block = self.document().begin() while block.isValid(): user_data = get_user_data(block) <mask>: errors_lines.append(block.blockNumber()) block = block.next() return errors_lines",False,if user_data . error :,"if user_data . get ( ""errors"" ) :",0.8820016898747209,33.18077402843942
"def set_pbar_fraction(self, frac, progress, stage=None): gtk.gdk.threads_enter() try: self.is_pulsing = False self.set_stage_text(stage or _(""Processing..."")) self.pbar.set_text(progress) if frac > 1: frac = 1.0 <mask>: frac = 0 self.pbar.set_fraction(frac) finally: gtk.gdk.threads_leave()",False,if frac < 0 :,elif frac < 0 :,0.8820016898747209,66.87403049764218
"def list_files(basedir): """"""List files in the directory rooted at |basedir|."""""" if not os.path.isdir(basedir): raise NoSuchDirectory(basedir) directories = [""""] while directories: d = directories.pop() for basename in os.listdir(os.path.join(basedir, d)): filename = os.path.join(d, basename) if os.path.isdir(os.path.join(basedir, filename)): directories.append(filename) <mask>: yield filename",False,"elif os . path . exists ( os . path . join ( basedir , filename ) ) :","if os . path . isfile ( os . path . join ( d , filename ) ) :",0.8820016898747209,62.89868866690353
"def assistive(self): """"""Detects if item can be used as assistance"""""" # Make sure we cache results if self.__assistive is None: assistive = False # Go through all effects and find first assistive for effect in self.effects.values(): <mask>: # If we find one, stop and mark item as assistive assistive = True break self.__assistive = assistive return self.__assistive",False,if effect . isAssistance is True :,if effect . assistive :,0.8820016898747209,28.641904579795423
"def closest_unseen(self, row1, col1, filter=None): # find the closest unseen from this row/col min_dist = maxint closest_unseen = None for row in range(self.height): for col in range(self.width): if filter is None or (row, col) not in filter: if self.map[row][col] == UNSEEN: dist = self.distance(row1, col1, row, col) <mask>: min_dist = dist closest_unseen = (row, col) return closest_unseen",True,if dist < min_dist :,if dist < min_dist :,0.8820016898747209,100.00000000000004
"def _maybe_has_default_route(self): for route in self.iter_routes(): <mask>: return True for iface in self.iter_interfaces(): for subnet in iface.get(""subnets"", []): for route in subnet.get(""routes"", []): if self._is_default_route(route): return True return False",True,if self . _is_default_route ( route ) :,if self . _is_default_route ( route ) :,0.8820016898747209,100.00000000000004
"def data(self, data): if data is None: raise Exception(""Data cannot be None"") val = [] for d in data: if isinstance(d, str): val.append(bytes(d, ""utf-8"")) <mask>: val.append(d) else: raise Exception( ""Invalid type, data can only be an str or a bytes not {}: {}"".format( type(data), d ) ) self.__data = val",True,"elif isinstance ( d , bytes ) :","elif isinstance ( d , bytes ) :",0.8820016898747209,100.00000000000004
"def get_one_segment_function(data, context, echoerr): ext = data[""ext""] function_name = context[-2][1].get(""function"") if function_name: module, function_name = get_function_strings(function_name, context, ext) func = import_segment(function_name, data, context, echoerr, module=module) <mask>: yield func",True,if func :,if func :,0.8820016898747209,0.0
"def generic_visit(self, node, parents=None): parents = (parents or []) + [node] for field, value in iter_fields(node): if isinstance(value, list): for item in value: <mask>: self.visit(item, parents) elif isinstance(value, AST): self.visit(value, parents)",True,"if isinstance ( item , AST ) :","if isinstance ( item , AST ) :",0.8820016898747209,100.00000000000004
"def find_scintilla_constants(f): lexers = [] states = [] for name in f.order: v = f.features[name] <mask>: if v[""FeatureType""] == ""val"": if name.startswith(""SCE_""): states.append((name, v[""Value""])) elif name.startswith(""SCLEX_""): lexers.append((name, v[""Value""])) return (lexers, states)",False,"if v [ ""Category"" ] != ""Deprecated"" :","if v [ ""FeatureType"" ] == ""lex"" :",0.8820016898747209,28.917849332325716
"def things(self, query): limit = query.pop(""limit"", 100) offset = query.pop(""offset"", 0) keys = set(self.docs) for k, v in query.items(): <mask>: # query keys need to be flattened properly, # this corrects any nested keys that have been included # in values. flat = common.flatten_dict(v)[0] k += ""."" + web.rstrips(flat[0], "".key"") v = flat[1] keys = set(k for k in self.filter_index(self.index, k, v) if k in keys) keys = sorted(keys) return keys[offset : offset + limit]",True,"if isinstance ( v , dict ) :","if isinstance ( v , dict ) :",0.8820016898747209,100.00000000000004
"def del_(self, key): initial_hash = hash_ = self.hash(key) while True: if self._keys[hash_] is self._empty: # That key was never assigned return None <mask>: # key found, assign with deleted sentinel self._keys[hash_] = self._deleted self._values[hash_] = self._deleted self._len -= 1 return hash_ = self._rehash(hash_) if initial_hash == hash_: # table is full and wrapped around return None",False,elif self . _keys [ hash_ ] == key :,if self . _keys [ hash_ ] == self . _deleted :,0.8820016898747209,59.687741756345
"def test_204_invalid_content_length(self): # 204 status with non-zero content length is malformed with ExpectLog(gen_log, "".*Response with code 204 should not have body""): response = self.fetch(""/?error=1"") <mask>: self.skipTest(""requires HTTP/1.x"") if self.http_client.configured_class != SimpleAsyncHTTPClient: self.skipTest(""curl client accepts invalid headers"") self.assertEqual(response.code, 599)",False,if not self . http1 :,if response . code == 204 :,0.8820016898747209,7.267884212102741
"def __str__(self) -> str: text = ""\n"" for k, r in self.result.items(): text += ""{}\n"".format(""#"" * 40) <mask>: text += ""# {} (failed)\n"".format(k) else: text += ""# {} (succeeded)\n"".format(k) text += ""{}\n"".format(""#"" * 40) for sub_r in r: text += ""**** {}\n"".format(sub_r.name) text += ""{}\n"".format(sub_r) return text",False,if r . failed :,if r is None :,0.8820016898747209,23.643540225079384
"def DeleteTask(): oid = request.form.get(""oid"", """") if oid: result = Mongo.coll[""Task""].delete_one({""_id"": ObjectId(oid)}) <mask>: result = Mongo.coll[""Result""].delete_many({""task_id"": ObjectId(oid)}) if result: return ""success"" return ""fail""",False,if result . deleted_count > 0 :,if result :,0.8820016898747209,0.0
"def _replace_vars(self, line, extracted, env_variables): for e in extracted: <mask>: value = env_variables.get(e) if isinstance(value, dict) or isinstance(value, list): value = pprint.pformat(value) decorated = self._decorate_var(e) line = line.replace(decorated, str(value)) return line",True,if e in env_variables :,if e in env_variables :,0.8820016898747209,100.00000000000004
"def should_include(service): for f in filt: if f == ""status"": state = filt[f] containers = project.containers([service.name], stopped=True) if not has_container_with_state(containers, state): return False elif f == ""source"": source = filt[f] if source == ""image"" or source == ""build"": <mask>: return False else: raise UserError(""Invalid value for source filter: %s"" % source) else: raise UserError(""Invalid filter: %s"" % f) return True",False,if source not in service . options :,"if not has_container_with_state ( containers , state ) :",0.8820016898747209,3.4585921141027356
def state_callback_loop(): if usercallback: when = 1 while ( when and not self.future_removed.done() and not self.session.shutdownstarttime ): result = usercallback(self.get_state()) when = (await result) if iscoroutine(result) else result <mask>: await sleep(when),False,if when > 0.0 and not self . session . shutdownstarttime :,if when :,0.8820016898747209,0.0
"def __get_new_timeout(self, timeout): """"""When using --timeout_multiplier=#.#"""""" self.__check_scope() try: timeout_multiplier = float(self.timeout_multiplier) <mask>: timeout_multiplier = 0.5 timeout = int(math.ceil(timeout_multiplier * timeout)) return timeout except Exception: # Wrong data type for timeout_multiplier (expecting int or float) return timeout",False,if timeout_multiplier <= 0.5 :,if timeout_multiplier < 0.5 :,0.8820016898747209,61.29752413741059
"def readexactly(self, n): buf = b"""" while n: yield IORead(self.s) res = self.s.read(n) assert res is not None <mask>: yield IOReadDone(self.s) break buf += res n -= len(res) return buf",False,if not res :,if len ( res ) == 0 :,0.8820016898747209,6.27465531099474
"def contract_rendering_pane(event): """"""Expand the rendering pane."""""" c = event.get(""c"") if c: vr = c.frame.top.findChild(QtWidgets.QWidget, ""viewrendered_pane"") <mask>: vr.contract() else: # Just open the pane. viewrendered(event)",True,if vr :,if vr :,0.8820016898747209,0.0
"def translate_headers(self, environ): """"""Translate CGI-environ header names to HTTP header names."""""" for cgiName in environ: # We assume all incoming header keys are uppercase already. <mask>: yield self.headerNames[cgiName], environ[cgiName] elif cgiName[:5] == ""HTTP_"": # Hackish attempt at recovering original header names. translatedHeader = cgiName[5:].replace(""_"", ""-"") yield translatedHeader, environ[cgiName]",False,if cgiName in self . headerNames :,"if cgiName . startswith ( ""HTTP_"" ) :",0.8820016898747209,8.913765521398126
"def get_value_from_string(self, string_value): """"""Return internal representation starting from CFN/user-input value."""""" param_value = self.get_default_value() try: <mask>: string_value = str(string_value).strip() if string_value != ""NONE"": param_value = int(string_value) except ValueError: self.pcluster_config.warn( ""Unable to convert the value '{0}' to an Integer. "" ""Using default value for parameter '{1}'"".format(string_value, self.key) ) return param_value",False,if string_value is not None :,"if isinstance ( string_value , str ) :",0.8820016898747209,17.747405280050266
"def monitor_filter(self): """"""Return filtered service objects list"""""" services = self.client.services.list(filters={""label"": ""com.ouroboros.enable""}) monitored_services = [] for service in services: ouro_label = service.attrs[""Spec""][""Labels""].get(""com.ouroboros.enable"") <mask>: monitored_services.append(service) self.data_manager.monitored_containers[self.socket] = len(monitored_services) self.data_manager.set(self.socket) return monitored_services",False,"if not self . config . label_enable or ouro_label . lower ( ) in [ ""true"" , ""yes"" ] :",if ouro_label :,0.8820016898747209,0.4541429459403983
"def nextEditable(self): """"""Moves focus of the cursor to the next editable window"""""" if self.currentEditable is None: if len(self._editableChildren): self._currentEditableRef = self._editableChildren[0] else: for ref in weakref.getweakrefs(self.currentEditable): if ref in self._editableChildren: cei = self._editableChildren.index(ref) nei = cei + 1 <mask>: nei = 0 self._currentEditableRef = self._editableChildren[nei] return self.currentEditable",False,if nei >= len ( self . _editableChildren ) :,if nei == len ( self . _editableChildren ) :,0.8820016898747209,76.91605673134588
"def linkify_cm_by_tp(self, timeperiods): for rm in self: mtp_name = rm.modulation_period.strip() # The new member list, in id mtp = timeperiods.find_by_name(mtp_name) <mask>: err = ( ""Error: the business impact modulation '%s' got an unknown "" ""modulation_period '%s'"" % (rm.get_name(), mtp_name) ) rm.configuration_errors.append(err) rm.modulation_period = mtp",False,"if mtp_name != """" and mtp is None :",if mtp is None :,0.8820016898747209,15.340817918113808
def close_open_fds(keep=None): # noqa keep = [maybe_fileno(f) for f in (keep or []) if maybe_fileno(f) is not None] for fd in reversed(range(get_fdmax(default=2048))): <mask>: try: os.close(fd) except OSError as exc: if exc.errno != errno.EBADF: raise,True,if fd not in keep :,if fd not in keep :,0.8820016898747209,100.00000000000004
"def _append_child_from_unparsed_xml(father_node, unparsed_xml): """"""Append child xml nodes to a node."""""" dom_tree = parseString(unparsed_xml) if dom_tree.hasChildNodes(): first_child = dom_tree.childNodes[0] <mask>: child_nodes = first_child.childNodes for _ in range(len(child_nodes)): childNode = child_nodes.item(0) father_node.appendChild(childNode) return raise DistutilsInternalError( ""Could not Append append elements to "" ""the Windows msi descriptor."" )",False,if first_child . hasChildNodes ( ) :,if first_child . nodeType == Node . ELEMENT_NODE :,0.8820016898747209,27.824623288353134
"def process_request(self, request): for old, new in self.names_name: request.uri = request.uri.replace(old, new) <mask>: body = six.ensure_str(request.body) if old in body: request.body = body.replace(old, new) return request",False,if is_text_payload ( request ) and request . body :,if request . body :,0.8820016898747209,11.688396478408103
"def __init__(self, **options): self.func_name_highlighting = get_bool_opt(options, ""func_name_highlighting"", True) self.disabled_modules = get_list_opt(options, ""disabled_modules"", []) self._functions = set() if self.func_name_highlighting: from pygments.lexers._luabuiltins import MODULES for mod, func in MODULES.iteritems(): <mask>: self._functions.update(func) RegexLexer.__init__(self, **options)",False,if mod not in self . disabled_modules :,if func . __name__ in self . disabled_modules :,0.8820016898747209,41.374412020518825
"def GetBestSizeForParentSize(self, parentSize): """"""Finds the best width and height given the parent's width and height."""""" if len(self.GetChildren()) == 1: win = self.GetChildren()[0] <mask>: temp_dc = wx.ClientDC(self) childSize = win.GetBestSizeForParentSize(parentSize) clientParentSize = self._art.GetPanelClientSize( temp_dc, self, wx.Size(*parentSize), None ) overallSize = self._art.GetPanelSize( temp_dc, self, wx.Size(*clientParentSize), None ) return overallSize return self.GetSize()",False,"if isinstance ( win , RibbonControl ) :",if win :,0.8820016898747209,0.0
"def pid_from_name(name): processes = [] for pid in os.listdir(""/proc""): try: pid = int(pid) pname, cmdline = SunProcess._name_args(pid) <mask>: return pid if name in cmdline.split("" "", 1)[0]: return pid except: pass raise ProcessException(""No process with such name: %s"" % name)",False,if name in pname :,if pname == name :,0.8820016898747209,11.478744233307168
"def __get_file_by_num(self, num, file_list, idx=0): for element in file_list: if idx == num: return element <mask>: i = self.__get_file_by_num(num, element[3], idx + 1) if not isinstance(i, int): return i idx = i else: idx += 1 return idx",False,if element [ 3 ] and element [ 4 ] :,"elif element [ 3 ] == ""file"" :",0.8820016898747209,24.808415001701817
"def scan_block_scalar_indentation(self): # See the specification for details. chunks = [] max_indent = 0 end_mark = self.get_mark() while self.peek() in "" \r\n\x85\u2028\u2029"": if self.peek() != "" "": chunks.append(self.scan_line_break()) end_mark = self.get_mark() else: self.forward() <mask>: max_indent = self.column return chunks, max_indent, end_mark",True,if self . column > max_indent :,if self . column > max_indent :,0.8820016898747209,100.00000000000004
"def ant_map(m): tmp = ""rows %s\ncols %s\n"" % (len(m), len(m[0])) players = {} for row in m: tmp += ""m "" for col in row: if col == LAND: tmp += ""."" elif col == BARRIER: tmp += ""%"" <mask>: tmp += ""*"" elif col == UNSEEN: tmp += ""?"" else: players[col] = True tmp += chr(col + 97) tmp += ""\n"" tmp = (""players %s\n"" % len(players)) + tmp return tmp",False,elif col == FOOD :,elif col == SAND :,0.8820016898747209,53.7284965911771
"def prepare_data(entry): branch_wise_entries = {} gross_pay = 0 for d in entry: gross_pay += d.gross_pay <mask>: branch_wise_entries[d.branch][d.mode_of_payment] = d.net_pay else: branch_wise_entries.setdefault(d.branch, {}).setdefault( d.mode_of_payment, d.net_pay ) return branch_wise_entries, gross_pay",False,if branch_wise_entries . get ( d . branch ) :,if d . branch in branch_wise_entries :,0.8820016898747209,37.773311868264216
"def __init__(self, uuid=None, cluster_state=None, children=None, **kwargs): self.uuid = uuid self.cluster_state = cluster_state if self.cluster_state is not None: self.children = WeakSet( self.cluster_state.tasks.get(task_id) for task_id in children or () <mask>: ) else: self.children = WeakSet() self._serializer_handlers = { ""children"": self._serializable_children, ""root"": self._serializable_root, ""parent"": self._serializable_parent, } if kwargs: self.__dict__.update(kwargs)",False,if task_id in self . cluster_state . tasks,if task_id,0.8820016898747209,13.533528323661276
"def listdir(self, d): try: return [ p for p in os.listdir(d) <mask>: ] except OSError: return []",False,"if os . path . basename ( p ) != ""CVS"" and os . path . isdir ( os . path . join ( d , p ) )",if p . startswith ( self . prefix ) and p . endswith ( self . suffix ),0.8820016898747209,1.8364645691443648
"def send_packed_command(self, command, check_health=True): if not self._sock: self.connect() try: <mask>: command = [command] for item in command: self._sock.sendall(item) except socket.error as e: self.disconnect() if len(e.args) == 1: _errno, errmsg = ""UNKNOWN"", e.args[0] else: _errno, errmsg = e.args raise ConnectionError( ""Error %s while writing to socket. %s."" % (_errno, errmsg) ) except Exception: self.disconnect() raise",False,"if isinstance ( command , str ) :",if check_health :,0.8820016898747209,6.9717291216921975
"def run(self): """"""Start the scanner"""""" logging.info(""Dirscanner starting up"") self.shutdown = False while not self.shutdown: # Wait to be woken up or triggered with self.loop_condition: self.loop_condition.wait(self.dirscan_speed) <mask>: self.scan()",False,if self . dirscan_speed and not self . shutdown :,if self . loop_condition . is_set ( ) :,0.8820016898747209,13.674406678232565
"def __aexit__( self, exc_type: type, exc_value: BaseException, tb: TracebackType ) -> None: if exc_type is not None: await self.close() await self._task while not self._receive_queue.empty(): data = await self._receive_queue.get() if isinstance(data, bytes): self.response_data.extend(data) <mask>: raise data",False,"elif not isinstance ( data , HTTPDisconnect ) :",elif exc_type is not None :,0.8820016898747209,6.413885305524152
"def f(msg): text = extractor(msg) for px in prefix: <mask>: chunks = text[len(px) :].split(separator) return chunks[0], (chunks[1:],) if pass_args else () return ((None,),) # to distinguish with `None`",False,if text . startswith ( px ) :,if px in text :,0.8820016898747209,8.290829875388036
"def _flatten(*args): ahs = set() if len(args) > 0: for item in args: if type(item) is ActionHandle: ahs.add(item) <mask>: for ah in item: if type(ah) is not ActionHandle: # pragma:nocover raise ActionManagerError(""Bad argument type %s"" % str(ah)) ahs.add(ah) else: # pragma:nocover raise ActionManagerError(""Bad argument type %s"" % str(item)) return ahs",False,"elif type ( item ) in ( list , tuple , dict , set ) :","elif isinstance ( item , list ) :",0.8820016898747209,6.60902979597904
"def find_class(self, module, name): # Subclasses may override this. sys.audit(""pickle.find_class"", module, name) if self.proto < 3 and self.fix_imports: if (module, name) in _compat_pickle.NAME_MAPPING: module, name = _compat_pickle.NAME_MAPPING[(module, name)] <mask>: module = _compat_pickle.IMPORT_MAPPING[module] __import__(module, level=0) if self.proto >= 4: return _getattribute(sys.modules[module], name)[0] else: return getattr(sys.modules[module], name)",False,elif module in _compat_pickle . IMPORT_MAPPING :,if module in _compat_pickle . IMPORT_MAPPING :,0.8820016898747209,90.36020036098445
"def _send_until_done(self, data): while True: try: return self.connection.send(data) except OpenSSL.SSL.WantWriteError: wr = util.wait_for_write(self.socket, self.socket.gettimeout()) <mask>: raise timeout() continue except OpenSSL.SSL.SysCallError as e: raise SocketError(str(e))",False,if not wr :,if wr is None :,0.8820016898747209,14.058533129758727
"def __new__(cls, *args, **kwargs): """"""Hack to ensure method defined as async are implemented as such."""""" coroutines = inspect.getmembers(BaseManager, predicate=inspect.iscoroutinefunction) for coroutine in coroutines: implemented_method = getattr(cls, coroutine[0]) <mask>: raise RuntimeError(""The method %s must be a coroutine"" % implemented_method) return super().__new__(cls, *args, **kwargs)",False,if not inspect . iscoroutinefunction ( implemented_method ) :,if not implemented_method :,0.8820016898747209,17.28116170001394
"def add_directive(self, name, obj, content=None, arguments=None, **options): if isinstance(obj, clstypes) and issubclass(obj, Directive): <mask>: raise ExtensionError( ""when adding directive classes, no "" ""additional arguments may be given"" ) directives.register_directive(name, directive_dwim(obj)) else: obj.content = content obj.arguments = arguments obj.options = options directives.register_directive(name, obj)",False,if content or arguments or options :,if not arguments :,0.8820016898747209,9.930283522141846
"def create(self, w): if w.use_eventloop: # does not use dedicated timer thread. w.timer = _Timer(max_interval=10.0) else: <mask>: # Default Timer is set by the pool, as for example, the # eventlet pool needs a custom timer implementation. w.timer_cls = w.pool_cls.Timer w.timer = self.instantiate( w.timer_cls, max_interval=w.timer_precision, on_error=self.on_timer_error, on_tick=self.on_timer_tick, )",False,if not w . timer_cls :,if w . timer_cls is None :,0.8820016898747209,48.54917717073236
"def _config(_molecule_file, request): with open(_molecule_file) as f: d = util.safe_load(f) if hasattr(request, ""param""): <mask>: d2 = util.safe_load(request.getfixturevalue(request.param)) else: d2 = request.getfixturevalue(request.param) # print(100, d) # print(200, d2) d = util.merge_dicts(d, d2) # print(300, d) return d",False,"if isinstance ( request . getfixturevalue ( request . param ) , str ) :","if isinstance ( request . param , str ) :",0.8820016898747209,47.65082587109519
"def _instrument_model(self, model): for key, value in list( model.__dict__.items() ): # avoid ""dictionary keys changed during iteration"" <mask>: new_layer = self._instrument(value) if new_layer is not value: setattr(model, key, new_layer) elif isinstance(value, list): for i, item in enumerate(value): if isinstance(item, tf.keras.layers.Layer): value[i] = self._instrument(item) return model",False,"if isinstance ( value , tf . keras . layers . Layer ) :","if isinstance ( value , dict ) :",0.8820016898747209,28.08708327044616
"def is_accepted_drag_event(self, event): if event.source() == self.table: return True mime = event.mimeData() if mime.hasUrls(): for url in mime.urls(): # Only support local files. <mask>: break # And only allow supported extensions. filename = url.toLocalFile() extension = os.path.splitext(filename)[1].lower()[1:] if extension not in _dictionary_formats(): break else: return True return False",False,if not url . isLocalFile ( ) :,if url . isLocalFile ( ) :,0.8820016898747209,72.89545183625967
"def explain(self, other, depth=0): exp = super(UnionType, self).explain(other, depth) for ndx, subtype in enumerate(self.params[""allowed_types""]): <mask>: exp += ""\n{}and"".format("""".join([""\t""] * depth)) exp += ""\n"" + subtype.explain(other, depth=depth + 1) return exp",False,if ndx > 0 :,if ndx == 0 :,0.8820016898747209,22.957488466614336
"def test_k_is_stochastic_parameter(self): # k as stochastic parameter aug = iaa.MedianBlur(k=iap.Choice([3, 5])) seen = [False, False] for i in sm.xrange(100): observed = aug.augment_image(self.base_img) if np.array_equal(observed, self.blur3x3): seen[0] += True <mask>: seen[1] += True else: raise Exception(""Unexpected result in MedianBlur@2"") if all(seen): break assert np.all(seen)",False,"elif np . array_equal ( observed , self . blur5x5 ) :","elif np . array_equal ( observed , self . blur2x3 ) :",0.8820016898747209,80.91067115702207
"def test_get_message(self): async with self.chat_client: await self._create_thread() async with self.chat_thread_client: message_id = await self._send_message() message = await self.chat_thread_client.get_message(message_id) assert message.id == message_id assert message.type == ChatMessageType.TEXT assert message.content.message == ""hello world"" # delete chat threads <mask>: await self.chat_client.delete_chat_thread(self.thread_id)",False,if not self . is_playback ( ) :,if self . thread_id :,0.8820016898747209,10.759051250985632
"def do_write_property(self, device, callback=None): try: iocb = ( device <mask>: else self.form_iocb(device, request_type=""writeProperty"") ) deferred(self.request_io, iocb) self.requests_in_progress.update({iocb: {""callback"": callback}}) iocb.add_callback(self.__general_cb) except Exception as error: log.exception(""exception: %r"", error)",False,"if isinstance ( device , IOCB )",if callback is None,0.8820016898747209,7.545383788761362
"def fit(self, dataset, force_retrain): if force_retrain: self.sub_unit_1[""fitted""] = True self.sub_unit_1[""calls""] += 1 self.sub_unit_2[""fitted""] = True self.sub_unit_2[""calls""] += 1 else: if not self.sub_unit_1[""fitted""]: self.sub_unit_1[""fitted""] = True self.sub_unit_1[""calls""] += 1 <mask>: self.sub_unit_2[""fitted""] = True self.sub_unit_2[""calls""] += 1 return self",True,"if not self . sub_unit_2 [ ""fitted"" ] :","if not self . sub_unit_2 [ ""fitted"" ] :",0.8820016898747209,100.00000000000004
"def _insert_with_loop(self): id_list = [] last_id = None return_id_list = self._return_id_list for row in self._rows: last_id = InsertQuery(self.model_class, row).upsert(self._upsert).execute() <mask>: id_list.append(last_id) if return_id_list: return id_list else: return last_id",False,if return_id_list :,if last_id :,0.8820016898747209,17.030578356760866
"def merge_block(self): """"""merges a block in the map"""""" for i in range(self.block.x): for j in range(self.block.x): c = self.block.get(i, j) <mask>: self.map[(i + self.block.pos.x, j + self.block.pos.y)] = c",False,if c :,if c is not None :,0.8820016898747209,17.965205598154213
"def configure_plex(config): core.PLEX_SSL = int(config[""Plex""][""plex_ssl""]) core.PLEX_HOST = config[""Plex""][""plex_host""] core.PLEX_PORT = config[""Plex""][""plex_port""] core.PLEX_TOKEN = config[""Plex""][""plex_token""] plex_section = config[""Plex""][""plex_sections""] or [] if plex_section: <mask>: plex_section = "","".join(plex_section) # fix in case this imported as list. plex_section = [tuple(item.split("","")) for item in plex_section.split(""|"")] core.PLEX_SECTION = plex_section",False,"if isinstance ( plex_section , list ) :","if "","" in plex_section :",0.8820016898747209,18.88588859215946
"def select(self): e = xlib.XEvent() while xlib.XPending(self._display): xlib.XNextEvent(self._display, e) # Key events are filtered by the xlib window event # handler so they get a shot at the prefiltered event. <mask>: if xlib.XFilterEvent(e, e.xany.window): continue try: dispatch = self._window_map[e.xany.window] except KeyError: continue dispatch(e)",False,"if e . xany . type not in ( xlib . KeyPress , xlib . KeyRelease ) :",if e . xany . window in self . _window_map :,0.8820016898747209,22.4687979920349
"def format_message(self): bits = [self.message] if self.possibilities: <mask>: bits.append(""Did you mean %s?"" % self.possibilities[0]) else: possibilities = sorted(self.possibilities) bits.append(""(Possible options: %s)"" % "", "".join(possibilities)) return "" "".join(bits)",True,if len ( self . possibilities ) == 1 :,if len ( self . possibilities ) == 1 :,0.8820016898747209,100.00000000000004
"def _collect_logs(model): page_token = None all_logs = [] while True: paginated_logs = model.lookup_logs(now, later, page_token=page_token) page_token = paginated_logs.next_page_token all_logs.extend(paginated_logs.logs) <mask>: break return all_logs",False,if page_token is None :,if not paginated_logs . is_valid :,0.8820016898747209,5.934202609760488
"def run(self): while True: context_id_list_tuple = self._inflated_addresses.get(block=True) <mask>: break c_id, inflated_address_list = context_id_list_tuple inflated_value_map = dict(inflated_address_list) if c_id in self._contexts: self._contexts[c_id].set_from_tree(inflated_value_map)",False,if context_id_list_tuple is _SHUTDOWN_SENTINEL :,if context_id_list_tuple is None :,0.8820016898747209,61.44118374261937
"def _setup_prefix(self): # we assume here that our metadata may be nested inside a ""basket"" # of multiple eggs; that's why we use module_path instead of .archive path = self.module_path old = None while path != old: <mask>: self.egg_name = os.path.basename(path) self.egg_info = os.path.join(path, ""EGG-INFO"") self.egg_root = path break old = path path, base = os.path.split(path)",False,"if path . lower ( ) . endswith ( "".egg"" ) :",if os . path . isdir ( path ) :,0.8820016898747209,8.27951003977077
"def get_filename(self, prompt): okay = False val = """" while not okay: val = raw_input(""%s: %s"" % (prompt, val)) val = os.path.expanduser(val) if os.path.isfile(val): okay = True <mask>: path = val val = self.choose_from_list(os.listdir(path)) if val: val = os.path.join(path, val) okay = True else: val = """" else: print(""Invalid value: %s"" % val) val = """" return val",True,elif os . path . isdir ( val ) :,elif os . path . isdir ( val ) :,0.8820016898747209,100.00000000000004
"def versions(self, sitename, data): # handle the query of type {""query"": '{""key"": ""/books/ia:foo00bar"", ...}} if ""query"" in data: q = json.loads(data[""query""]) itemid = self._get_itemid(q.get(""key"")) <mask>: key = q[""key""] return json.dumps([self.dummy_edit(key)]) # if not just go the default way return ConnectionMiddleware.versions(self, sitename, data)",False,if itemid :,"if itemid == ""ia"" :",0.8820016898747209,12.22307556087252
"def read_stanza(self): while True: try: stanza_end = self._buffer.index(b""\n"") stanza = self.decoder.decode(self._buffer[:stanza_end]) self._buffer = self._buffer[stanza_end + 1 :] colon = stanza.index("":"") return stanza[:colon], stanza[colon + 1 :] except ValueError: bytes = self.read_bytes() <mask>: return None else: self._buffer += bytes",False,if not bytes :,if bytes is None :,0.8820016898747209,14.058533129758727
def decodeattrs(attrs): names = [] for bit in range(16): mask = 1 << bit <mask>: if attrnames.has_key(mask): names.append(attrnames[mask]) else: names.append(hex(mask)) return names,False,if attrs & mask :,if mask in attrs :,0.8820016898747209,15.106876986783844
"def _set_http_cookie(): if conf.cookie: <mask>: conf.http_headers[HTTP_HEADER.COOKIE] = ""; "".join( map(lambda x: ""="".join(x), conf.cookie.items()) ) else: conf.http_headers[HTTP_HEADER.COOKIE] = conf.cookie",True,"if isinstance ( conf . cookie , dict ) :","if isinstance ( conf . cookie , dict ) :",0.8820016898747209,100.00000000000004
"def __ne__(self, other): if isinstance(other, WeakMethod): <mask>: return self is not other return weakref.ref.__ne__(self, other) or self._func_ref != other._func_ref return True",False,if not self . _alive or not other . _alive :,if self . _func_ref is other . _func_ref :,0.8820016898747209,17.678748653651848
"def update_unread(self, order_id, reset=False): conn = Database.connect_database(self.PATH) with conn: cursor = conn.cursor() <mask>: cursor.execute( """"""UPDATE sales SET unread = unread + 1 WHERE id=?;"""""", (order_id,) ) else: cursor.execute(""""""UPDATE sales SET unread=0 WHERE id=?;"""""", (order_id,)) conn.commit() conn.close()",False,if reset is False :,if reset :,0.8820016898747209,0.0
"def _get_field_value(self, test, key, match): if test.ver == ofproto_v1_0.OFP_VERSION: members = inspect.getmembers(match) for member in members: if member[0] == key: field_value = member[1] elif member[0] == ""wildcards"": wildcards = member[1] if key == ""nw_src"": field_value = test.nw_src_to_str(wildcards, field_value) <mask>: field_value = test.nw_dst_to_str(wildcards, field_value) else: field_value = match[key] return field_value",True,"elif key == ""nw_dst"" :","elif key == ""nw_dst"" :",0.8820016898747209,100.00000000000004
"def nested_filter(self, items, mask): keep_current = self.current_mask(mask) keep_nested_lookup = self.nested_masks(mask) for k, v in items: keep_nested = keep_nested_lookup.get(k) <mask>: if keep_nested is not None: if isinstance(v, dict): yield k, dict(self.nested_filter(v.items(), keep_nested)) else: yield k, v",False,if k in keep_current :,if keep_current is not None and keep_current != v :,0.8820016898747209,11.114924776032012
"def goToPrevMarkedHeadline(self, event=None): """"""Select the next marked node."""""" c = self p = c.p if not p: return p.moveToThreadBack() wrapped = False while 1: <mask>: break elif p: p.moveToThreadBack() elif wrapped: break else: wrapped = True p = c.rootPosition() if not p: g.blue(""done"") c.treeSelectHelper(p) # Sets focus.",False,if p and p . isMarked ( ) :,if c . treeSelectHelper ( p ) :,0.8820016898747209,12.827770611048305
"def sample(self, **config): """"""Sample a configuration from this search space."""""" ret = {} ret.update(self.data) kwspaces = self.kwspaces kwspaces.update(config) striped_keys = [k.split(SPLITTER)[0] for k in config.keys()] for k, v in kwspaces.items(): <mask>: if isinstance(v, NestedSpace): sub_config = _strip_config_space(config, prefix=k) ret[k] = v.sample(**sub_config) else: ret[k] = v return ret",False,if k in striped_keys :,if k not in striped_keys :,0.8820016898747209,59.4603557501361
"def update_gradients_full(self, dL_dK, X, X2=None): if self.ARD: phi1 = self.phi(X) <mask>: self.variance.gradient = np.einsum(""ij,iq,jq->q"", dL_dK, phi1, phi1) else: phi2 = self.phi(X2) self.variance.gradient = np.einsum(""ij,iq,jq->q"", dL_dK, phi1, phi2) else: self.variance.gradient = np.einsum(""ij,ij"", dL_dK, self._K(X, X2)) * self.beta",False,if X2 is None or X is X2 :,if X2 is None :,0.8820016898747209,31.772355751081438
"def post(self): host_json = json.loads(request.data) host_os = host_json.get(""os"") if host_os: result = get_monkey_executable(host_os.get(""type""), host_os.get(""machine"")) if result: # change resulting from new base path executable_filename = result[""filename""] real_path = MonkeyDownload.get_executable_full_path(executable_filename) <mask>: result[""size""] = os.path.getsize(real_path) return result return {}",False,if os . path . isfile ( real_path ) :,if real_path :,0.8820016898747209,11.141275535087015
"def _encode_data( self, data, content_type, ): if content_type is MULTIPART_CONTENT: return encode_multipart(BOUNDARY, data) else: # Encode the content so that the byte representation is correct. match = CONTENT_TYPE_RE.match(content_type) <mask>: charset = match.group(1) else: charset = settings.DEFAULT_CHARSET return force_bytes(data, encoding=charset)",True,if match :,if match :,0.8820016898747209,0.0
"def _merge_scientific_float_tokens(tokens: Iterable[str]) -> List[str]: tokens = list(tokens) i = 0 while ""e"" in tokens[i + 1 :]: i = tokens.index(""e"", i + 1) s = i - 1 e = i + 1 if not re.match(""[0-9]"", str(tokens[s])): continue if re.match(""[+-]"", str(tokens[e])): e += 1 <mask>: e += 1 tokens[s:e] = ["""".join(tokens[s:e])] i -= 1 return tokens",False,"if re . match ( ""[0-9]"" , str ( tokens [ e ] ) ) :","if re . match ( ""[+-]]"" , str ( tokens [ s : e ] ) ) :",0.8820016898747209,63.55183125619188
"def convert_with_key(self, key, value, replace=True): result = self.configurator.convert(value) # If the converted value is different, save for next time if value is not result: <mask>: self[key] = result if type(result) in (ConvertingDict, ConvertingList, ConvertingTuple): result.parent = self result.key = key return result",True,if replace :,if replace :,0.8820016898747209,0.0
"def OnListEndLabelEdit(self, std, extra): item = extra[0] text = item[4] if text is None: return item_id = self.GetItem(item[0])[6] from bdb import Breakpoint for bplist in Breakpoint.bplist.itervalues(): for bp in bplist: <mask>: if text.strip().lower() == ""none"": text = None bp.cond = text break self.RespondDebuggerData()",False,if id ( bp ) == item_id :,if bp . id == item_id :,0.8820016898747209,51.7679965241078
"def add(self, url: str, future_nzo: NzbObject, when: Optional[int] = None): """"""Add an URL to the URLGrabber queue, 'when' is seconds from now"""""" if future_nzo and when: # Always increase counter future_nzo.url_tries += 1 # Too many tries? Cancel <mask>: self.fail_to_history(future_nzo, url, T(""Maximum retries"")) return future_nzo.url_wait = time.time() + when self.queue.put((url, future_nzo))",False,if future_nzo . url_tries > cfg . max_url_retries ( ) :,if future_nzo . url_tries > self . max_retries :,0.8820016898747209,52.78890511109627
def _is_datetime_string(series): if series.dtype == object: not_numeric = False try: pd.to_numeric(series) except Exception as e: not_numeric = True datetime_col = None <mask>: try: datetime_col = pd.to_datetime(series) except Exception as e: return False if datetime_col is not None: return True return False,True,if not_numeric :,if not_numeric :,0.8820016898747209,100.00000000000004
"def _getEventAndObservers(self, event): if isinstance(event, xpath.XPathQuery): # Treat as xpath observers = self._xpathObservers else: <mask>: # Treat as event observers = self._eventObservers else: # Treat as xpath event = xpath.internQuery(event) observers = self._xpathObservers return event, observers",False,if self . prefix == event [ : len ( self . prefix ) ] :,"if isinstance ( event , ( str , unicode ) ) :",0.8820016898747209,3.3383922484634225
"def test_wildcard_import(): bonobo = __import__(""bonobo"") assert bonobo.__version__ for name in dir(bonobo): # ignore attributes starting by underscores if name.startswith(""_""): continue attr = getattr(bonobo, name) <mask>: continue assert name in bonobo.__all__",False,if inspect . ismodule ( attr ) :,if not attr . is_attribute :,0.8820016898747209,7.809849842300637
"def relint_views(wid=None): windows = [sublime.Window(wid)] if wid else sublime.windows() for window in windows: for view in window.views(): <mask>: hit(view, ""relint_views"")",False,if view . buffer_id ( ) in persist . assigned_linters and view . is_primary ( ) :,if view . is_visible ( ) :,0.8820016898747209,10.595236773288903
def _check_for_unknown_gender(self): if self.obj.get_gender() == Person.UNKNOWN: d = GenderDialog(parent=self.window) gender = d.run() d.destroy() <mask>: self.obj.set_gender(gender),False,if gender >= 0 :,if gender is not None :,0.8820016898747209,17.965205598154213
"def add_to_path(self, fnames): """"""Add fnames to path"""""" indexes = [] for path in fnames: project = self.get_source_project(path) <mask>: self.parent_widget.emit(SIGNAL(""pythonpath_changed()"")) indexes.append(self.get_index(path)) if indexes: self.reset_icon_provider() for index in indexes: self.update(index)",False,if project . add_to_pythonpath ( path ) :,if project . is_pythonpath ( ) :,0.8820016898747209,24.299913285893492
"def validate(self, value): if value.grid_id is not None: if not isinstance(value, self.proxy_class): self.error(""FileField only accepts GridFSProxy values"") <mask>: self.error(""Invalid GridFSProxy value"")",False,"if not isinstance ( value . grid_id , ObjectId ) :",if value . grid_id not in self . proxy_class . _gridfs_proxies :,0.8820016898747209,21.31456897111116
"def shortcut(self, input, ch_out, stride, name, if_first=False): ch_in = input.shape[1] if ch_in != ch_out or stride != 1: <mask>: return self.conv_bn_layer(input, ch_out, 1, stride, name=name) else: return self.conv_bn_layer_new(input, ch_out, 1, stride, name=name) else: return input",True,if if_first :,if if_first :,0.8820016898747209,100.00000000000004
"def convert_path(ctx, tpath): for points, code in tpath.iter_segments(): if code == Path.MOVETO: ctx.move_to(*points) elif code == Path.LINETO: ctx.line_to(*points) elif code == Path.CURVE3: ctx.curve_to( points[0], points[1], points[0], points[1], points[2], points[3] ) <mask>: ctx.curve_to(*points) elif code == Path.CLOSEPOLY: ctx.close_path()",True,elif code == Path . CURVE4 :,elif code == Path . CURVE4 :,0.8820016898747209,100.00000000000004
"def _get_build_status(self, job_name, build_number): try: build_info = self.server.get_build_info(job_name, build_number) <mask>: return ""building"" else: return ""built"" except jenkins.NotFoundException: return ""not found""",False,"if build_info [ ""building"" ] :","if build_info . status == ""building"" :",0.8820016898747209,31.702331385234313
"def _parse_param_value(name, datatype, default): if datatype == ""bool"": if default.lower() == ""true"": return True elif default.lower() == ""false"": return False else: _s = ""{}: Invalid default value '{}' for bool parameter {}"" raise SyntaxError(_s.format(self.name, default, p)) elif datatype == ""int"": if type(default) == int: return default else: return int(default, 0) elif datatype == ""real"": <mask>: return default else: return float(default) else: return str(default)",True,if type ( default ) == float :,if type ( default ) == float :,0.8820016898747209,100.00000000000004
"def get_fills(self, exchange_order_id): async with aiohttp.ClientSession() as client: response: aiohttp.ClientResponse = await client.get( f""{BASE_URL}{FILLS_ROUTE}"", params={""orderId"": exchange_order_id, ""limit"": 100}, ) <mask>: try: msg = await response.json() except ValueError: msg = await response.text() raise DydxAsyncAPIError(response.status, msg) return await response.json()",False,if response . status >= 300 :,if response . status != 200 :,0.8820016898747209,38.260294162784454
"def semanticTags(self, semanticTags): if semanticTags is None: self.__semanticTags = OrderedDict() # check for key, value in list(semanticTags.items()): if not isinstance(key, int): raise TypeError(""At least one key is not a valid int position"") if not isinstance(value, list): raise TypeError( ""At least one value of the provided dict is not a list of string"" ) for x in value: <mask>: raise TypeError( ""At least one value of the provided dict is not a list of string"" ) self.__semanticTags = semanticTags",True,"if not isinstance ( x , str ) :","if not isinstance ( x , str ) :",0.8820016898747209,100.00000000000004
"def start_cutting_tool(self, event, axis, direction): toggle = event.EventObject self.cutting = toggle.Value if toggle.Value: # Disable the other toggles for child in self.cutsizer.Children: child = child.Window <mask>: child.Value = False self.cutting_axis = axis self.cutting_direction = direction else: self.cutting_axis = None self.cutting_direction = None self.cutting_dist = None",False,if child != toggle :,if child . Value :,0.8820016898747209,19.3576934939088
"def decoration_helper(self, patched, args, keywargs): extra_args = [] with contextlib.ExitStack() as exit_stack: for patching in patched.patchings: arg = exit_stack.enter_context(patching) if patching.attribute_name is not None: keywargs.update(arg) <mask>: extra_args.append(arg) args += tuple(extra_args) yield (args, keywargs)",False,elif patching . new is DEFAULT :,elif patching . attribute_name is not None :,0.8820016898747209,17.747405280050266
def decodeattrs(attrs): names = [] for bit in range(16): mask = 1 << bit if attrs & mask: <mask>: names.append(attrnames[mask]) else: names.append(hex(mask)) return names,False,if attrnames . has_key ( mask ) :,if mask in attrnames :,0.8820016898747209,5.557509463743763
"def pytest_collection_modifyitems(items): for item in items: if item.nodeid.startswith(""tests/params""): if ""stage"" not in item.keywords: item.add_marker(pytest.mark.stage(""unit"")) <mask>: item.add_marker(pytest.mark.init(rng_seed=123))",False,"if ""init"" not in item . keywords :","elif ""init"" not in item . keywords :",0.8820016898747209,88.01117367933934
"def handle_socket(self, request): conn = request.connection while True: chunk = conn.recv(4) <mask>: break slen = struct.unpack("">L"", chunk)[0] chunk = conn.recv(slen) while len(chunk) < slen: chunk = chunk + conn.recv(slen - len(chunk)) obj = pickle.loads(chunk) record = logging.makeLogRecord(obj) self.log_output += record.msg + ""\n"" self.handled.release()",False,if len ( chunk ) < 4 :,if not chunk :,0.8820016898747209,7.733712583165139
"def on_source_foreach(self, model, path, iter, id): m_id = model.get_value(iter, self.COLUMN_ID) if m_id == id: if self._foreach_mode == ""get"": self._foreach_take = model.get_value(iter, self.COLUMN_ENABLED) <mask>: self._foreach_take = iter",True,"elif self . _foreach_mode == ""set"" :","elif self . _foreach_mode == ""set"" :",0.8820016898747209,100.00000000000004
"def parts(): for l in lists.leaves: head_name = l.get_head_name() if head_name == ""System`List"": yield l.leaves <mask>: raise MessageException(""Catenate"", ""invrp"", l)",False,"elif head_name != ""System`Missing"" :","elif head_name == ""System`List"" :",0.8820016898747209,46.59538415189962
"def __fill_counter_values(self, command: str): result = [] regex = r""(item[0-9]+\.counter_value)"" for token in re.split(regex, command): <mask>: try: result.append(str(self.simulator_config.item_dict[token].value)) except (KeyError, ValueError, AttributeError): logger.error(""Could not get counter value for "" + token) else: result.append(token) return """".join(result)",False,"if re . match ( regex , token ) is not None :",if token in self . simulator_config . item_dict :,0.8820016898747209,4.368583925857938
"def IMPORTFROM(self, node): <mask>: if not self.futuresAllowed: self.report(messages.LateFutureImport, node, [n.name for n in node.names]) else: self.futuresAllowed = False for alias in node.names: if alias.name == ""*"": self.scope.importStarred = True self.report(messages.ImportStarUsed, node, node.module) continue name = alias.asname or alias.name importation = Importation(name, node) if node.module == ""__future__"": importation.used = (self.scope, node) self.addBinding(node, importation)",True,"if node . module == ""__future__"" :","if node . module == ""__future__"" :",0.8820016898747209,100.00000000000004
"def _split_batch_list(args, batch_list): new_list = [] for batch in batch_list.batches: new_list.append(batch) <mask>: yield batch_pb2.BatchList(batches=new_list) new_list = [] if new_list: yield batch_pb2.BatchList(batches=new_list)",False,if len ( new_list ) == args . batch_size_limit :,if len ( new_list ) == 1 :,0.8820016898747209,46.7751969423698
"def get_branch_or_use_upstream(branch_name, arg, repo): if not branch_name: # use upstream branch current_b = repo.current_branch upstream_b = current_b.upstream <mask>: raise ValueError( ""No {0} branch specified and the current branch has no upstream "" ""branch set"".format(arg) ) ret = current_b.upstream else: ret = get_branch(branch_name, repo) return ret",False,if not upstream_b :,if upstream_b is None :,0.8820016898747209,27.77619034011791
"def __init__(self, **settings): default_settings = self.get_default_settings() for name, value in default_settings.items(): <mask>: setattr(self, name, value) for name, value in settings.items(): if name not in default_settings: raise ImproperlyConfigured( ""Invalid setting '{}' for {}"".format( name, self.__class__.__name__, ) ) setattr(self, name, value)",False,"if not hasattr ( self , name ) :",if name not in settings :,0.8820016898747209,6.962210312500384
"def _declare(self, name, obj, included=False, quals=0): if name in self._declarations: prevobj, prevquals = self._declarations[name] if prevobj is obj and prevquals == quals: return <mask>: raise api.FFIError( ""multiple declarations of %s (for interactive usage, "" ""try cdef(xx, override=True))"" % (name,) ) assert ""__dotdotdot__"" not in name.split() self._declarations[name] = (obj, quals) if included: self._included_declarations.add(obj)",False,if not self . _override :,if len ( self . _declarations ) > 1 :,0.8820016898747209,15.851165692617148
"def include_file(name, fdir=tmp_dir, b64=False): try: if fdir is None: fdir = """" <mask>: with io.open(os.path.join(fdir, name), ""rb"") as f: return base64.b64encode(f.read()).decode(""utf-8"") else: with io.open(os.path.join(fdir, name), ""r"", encoding=""utf-8"") as f: return f.read() except (OSError, IOError) as e: logger.error(""Could not include file '{}': {}"".format(name, e))",True,if b64 :,if b64 :,0.8820016898747209,0.0
"def to_raw_json(self): parts = {} for p in self.parts: <mask>: parts[p[0]] = [] parts[p[0]].append({""value"": p[2], ""parameters"": p[1]}) children = [x.to_raw_json() for x in self.children] return { ""type"": self.__class__.__name__, ""children"": children, ""parts"": parts, }",True,if p [ 0 ] not in parts :,if p [ 0 ] not in parts :,0.8820016898747209,100.00000000000004
"def process_output( output: str, filename: str, start_line: int ) -> Tuple[Optional[str], bool]: error_found = False for line in output.splitlines(): t = get_revealed_type(line, filename, start_line) <mask>: return t, error_found elif ""error:"" in line: error_found = True return None, True # finding no reveal_type is an error",False,if t :,if t is not None :,0.8820016898747209,17.965205598154213
"def __init__( self, resize_keyboard=None, one_time_keyboard=None, selective=None, row_width=3 ): if row_width > self.max_row_keys: # Todo: Will be replaced with Exception in future releases <mask>: logger.error( ""Telegram does not support reply keyboard row width over %d."" % self.max_row_keys ) row_width = self.max_row_keys self.resize_keyboard = resize_keyboard self.one_time_keyboard = one_time_keyboard self.selective = selective self.row_width = row_width self.keyboard = []",False,if not DISABLE_KEYLEN_ERROR :,if self . max_row_keys > self . max_row_keys :,0.8820016898747209,3.2342452920962157
"def realizeElementExpressions(innerElement): elementHasBeenRealized = False for exp in innerElement.expressions: if not hasattr(exp, ""realize""): continue # else: before, during, after = exp.realize(innerElement) elementHasBeenRealized = True for n in before: newStream.append(n) <mask>: newStream.append(during) for n in after: newStream.append(n) if elementHasBeenRealized is False: newStream.append(innerElement)",True,if during is not None :,if during is not None :,0.8820016898747209,100.00000000000004
"def lex_number(self, pos): # numeric literal start = pos found_dot = False while pos < len(self.string) and ( self.string[pos].isdigit() or self.string[pos] == ""."" ): <mask>: if found_dot is True: raise ValueError(""Invalid number. Found multiple '.'"") found_dot = True # technically we allow more than one ""."" and let float()'s parsing # complain later pos += 1 val = self.string[start:pos] return Token(TokenType.LNUM, val, len(val))",True,"if self . string [ pos ] == ""."" :","if self . string [ pos ] == ""."" :",0.8820016898747209,100.00000000000004
"def rename(src, dst): # Try atomic or pseudo-atomic rename if _rename(src, dst): return # Fall back to ""move away and replace"" try: os.rename(src, dst) except OSError as e: <mask>: raise old = ""%s-%08x"" % (dst, random.randint(0, sys.maxsize)) os.rename(dst, old) os.rename(src, dst) try: os.unlink(old) except Exception: pass",True,if e . errno != errno . EEXIST :,if e . errno != errno . EEXIST :,0.8820016898747209,100.00000000000004
"def _the_callback(widget, event_id): point = widget.GetCenter() index = widget.WIDGET_INDEX if hasattr(callback, ""__call__""): if num > 1: args = [point, index] else: args = [point] <mask>: args.append(widget) try_callback(callback, *args) return",False,if pass_widget :,if event_id == widget . WIDGET_EVENT :,0.8820016898747209,4.789232204309912
"def run(self): for _ in range(self.n): error = True try: self.collection.insert_one({""test"": ""insert""}) error = False except: <mask>: raise if self.expect_exception: assert error",False,if not self . expect_exception :,if self . expect_exception :,0.8820016898747209,72.89545183625967
"def handle(self, *args: Any, **options: Any) -> None: realm = self.get_realm(options) if options[""all""]: <mask>: raise CommandError( ""You must specify a realm if you choose the --all option."" ) self.fix_all_users(realm) return self.fix_emails(realm, options[""emails""])",False,if realm is None :,if not realm :,0.8820016898747209,16.37226966703825
"def recv_tdi(self, nbits, pos): bits = 0 for n in range(nbits * 2): yield from self._wait_for_tck() <mask>: bits = (bits << 1) | (yield self.tdi.o) return bits",False,if ( yield self . tck . o ) == pos :,if n == pos :,0.8820016898747209,16.731227054577023
"def _split_head(self): if not hasattr(self, ""_severed_head""): <mask>: tree = self._tree.copy() head = tree.get_heading_text() tree.remove_heading() self._severed_head = (head, tree) else: self._severed_head = (None, None) return self._severed_head",True,if self . _tree :,if self . _tree :,0.8820016898747209,100.00000000000004
"def buildSearchTrie(self, choices): searchtrie = trie.Trie() for choice in choices: for token in self.tokenizeChoice(choice): <mask>: searchtrie[token] = [] searchtrie[token].append(choice) return searchtrie",False,if not searchtrie . has_key ( token ) :,if token not in searchtrie :,0.8820016898747209,5.274846355723257
"def format_sql(sql, params): rv = [] if isinstance(params, dict): # convert sql with named parameters to sql with unnamed parameters conv = _FormatConverter(params) if params: sql = sql_to_string(sql) sql = sql % conv params = conv.params else: params = () for param in params or (): <mask>: rv.append(""NULL"") param = safe_repr(param) rv.append(param) return sql, rv",True,if param is None :,if param is None :,0.8820016898747209,100.00000000000004
def on_completed2(): doner[0] = True if not qr: if len(ql) > 0: observer.on_next(False) observer.on_completed() <mask>: observer.on_next(True) observer.on_completed(),False,elif donel [ 0 ] :,elif len ( qr ) > 0 :,0.8820016898747209,7.267884212102741
"def notify_digest(self, frequency, changes): notifications = defaultdict(list) users = {} for change in changes: for user in self.get_users(frequency, change): <mask>: notifications[user.pk].append(change) users[user.pk] = user for user in users.values(): self.send_digest( user.profile.language, user.email, notifications[user.pk], subscription=user.current_subscription, )",False,if change . project is None or user . can_access_project ( change . project ) :,if user . profile . language == self . profile . language :,0.8820016898747209,4.869426103311578
"def _any_listener_using(self, target_group_arn): for load_balancer in self.load_balancers.values(): for listener in load_balancer.listeners.values(): for rule in listener.rules: for action in rule.actions: <mask>: return True return False",False,"if action . data . get ( ""target_group_arn"" ) == target_group_arn :",if action . target_group_arn == target_group_arn :,0.8820016898747209,49.05867907682778
"def train_dict(self, triples): """"""Train a dict lemmatizer given training (word, pos, lemma) triples."""""" # accumulate counter ctr = Counter() ctr.update([(p[0], p[1], p[2]) for p in triples]) # find the most frequent mappings for p, _ in ctr.most_common(): w, pos, l = p if (w, pos) not in self.composite_dict: self.composite_dict[(w, pos)] = l <mask>: self.word_dict[w] = l return",True,if w not in self . word_dict :,if w not in self . word_dict :,0.8820016898747209,100.00000000000004
"def parse_git_config(path): """"""Parse git config file."""""" config = dict() section = None with open(os.path.join(path, ""config""), ""r"") as f: for line in f: line = line.strip() <mask>: section = line[1:-1].strip() config[section] = dict() elif section: key, value = line.replace("" "", """").split(""="") config[section][key] = value return config",False,"if line . startswith ( ""["" ) :",if section is None :,0.8820016898747209,4.673289785800722
"def send_signal(self, pid, signum): if pid in self.processes: process = self.processes[pid] hook_result = self.call_hook(""before_signal"", pid=pid, signum=signum) <mask>: logger.debug( ""before_signal hook didn't return True "" ""=> signal %i is not sent to %i"" % (signum, pid) ) else: process.send_signal(signum) self.call_hook(""after_signal"", pid=pid, signum=signum) else: logger.debug(""process %s does not exist"" % pid)",False,if signum != signal . SIGKILL and not hook_result :,if hook_result is False :,0.8820016898747209,11.787460936700446
"def validate_pos_return(self): if self.is_pos and self.is_return: total_amount_in_payments = 0 for payment in self.payments: total_amount_in_payments += payment.amount invoice_total = self.rounded_total or self.grand_total <mask>: frappe.throw( _(""Total payments amount can't be greater than {}"").format( -invoice_total ) )",False,if total_amount_in_payments < invoice_total :,if total_amount_in_payments > - invoice_total :,0.8820016898747209,69.97522298221911
"def delete(key, inner_key=None): if inner_key is not None: try: del cache[key][inner_key] del use_count[key][inner_key] <mask>: del cache[key] del use_count[key] wrapper.cache_size -= 1 except KeyError: return False else: return True else: try: wrapper.cache_size -= len(cache[key]) del cache[key] del use_count[key] except KeyError: return False else: return True",False,if not cache [ key ] :,if len ( cache [ key ] ) == 0 :,0.8820016898747209,23.462350320527996
"def insertionsort(array): size = array.getsize() array.reset(""Insertion sort"") for i in range(1, size): j = i - 1 while j >= 0: <mask>: break array.swap(j, j + 1) j = j - 1 array.message(""Sorted"")",False,"if array . compare ( j , j + 1 ) <= 0 :",if array . get ( j ) == array . get ( j + 1 ) :,0.8820016898747209,22.786788980326644
"def publish_state(cls, payload, state): try: if isinstance(payload, LiveActionDB): <mask>: cls.process(payload) else: worker.get_worker().process(payload) except Exception: traceback.print_exc() print(payload)",False,if state == action_constants . LIVEACTION_STATUS_REQUESTED :,"if state == ""running"" :",0.8820016898747209,17.267606045625936
"def change_opacity_function(self, new_f): self.opacity_function = new_f dr = self.radius / self.num_levels sectors = [] for submob in self.submobjects: if type(submob) == AnnularSector: sectors.append(submob) for (r, submob) in zip(np.arange(0, self.radius, dr), sectors): <mask>: # it's the shadow, don't dim it continue alpha = self.opacity_function(r) submob.set_fill(opacity=alpha)",False,if type ( submob ) != AnnularSector :,if r == 0 :,0.8820016898747209,6.4790667469036025
"def is_suppressed_warning( type: str, subtype: str, suppress_warnings: List[str] ) -> bool: """"""Check the warning is suppressed or not."""""" if type is None: return False for warning_type in suppress_warnings: <mask>: target, subtarget = warning_type.split(""."", 1) else: target, subtarget = warning_type, None if target == type: if ( subtype is None or subtarget is None or subtarget == subtype or subtarget == ""*"" ): return True return False",True,"if ""."" in warning_type :","if ""."" in warning_type :",0.8820016898747209,100.00000000000004
"def set_many(self, mapping, timeout=None): timeout = self._normalize_timeout(timeout) # Use transaction=False to batch without calling redis MULTI # which is not supported by twemproxy pipe = self._client.pipeline(transaction=False) for key, value in _items(mapping): dump = self.dump_object(value) <mask>: pipe.set(name=self.key_prefix + key, value=dump) else: pipe.setex(name=self.key_prefix + key, value=dump, time=timeout) return pipe.execute()",False,if timeout == - 1 :,if timeout is None :,0.8820016898747209,15.848738972120703
"def maybe_relative_path(path): if not os.path.isabs(path): return path # already relative dir = path names = [] while True: prevdir = dir dir, name = os.path.split(prevdir) if dir == prevdir or not dir: return path # failed to make it relative names.append(name) try: <mask>: names.reverse() return os.path.join(*names) except OSError: pass",False,"if samefile ( dir , os . curdir ) :",if os . path . isdir ( name ) :,0.8820016898747209,13.134549472120794
"def word_range(word): for ind in range(len(word)): temp = word[ind] for c in [chr(x) for x in range(ord(""a""), ord(""z"") + 1)]: <mask>: yield word[:ind] + c + word[ind + 1 :]",False,if c != temp :,if c in temp :,0.8820016898747209,24.736929544091932
"def validate(self): self.update_soil_edit(""sand_composition"") for soil_type in self.soil_types: <mask>: frappe.throw(_(""{0} should be a value between 0 and 100"").format(soil_type)) if sum(self.get(soil_type) for soil_type in self.soil_types) != 100: frappe.throw(_(""Soil compositions do not add up to 100""))",False,if self . get ( soil_type ) > 100 or self . get ( soil_type ) < 0 :,if not self . get ( soil_type ) :,0.8820016898747209,24.10447793829011
"def on_click(self, event): run = self._is_running() if event[""button""] == self.button_activate: self.py3.command_run([""xscreensaver-command"", ""-activate""]) if event[""button""] == self.button_toggle: <mask>: self.py3.command_run([""xscreensaver-command"", ""-exit""]) else: # Because we want xscreensaver to continue running after # exit, we instead use preexec_fn=setpgrp here. Popen( [""xscreensaver"", ""-no-splash"", ""-no-capture-stderr""], stdout=PIPE, stderr=PIPE, preexec_fn=setpgrp, )",True,if run :,if run :,0.8820016898747209,0.0
"def maybe_relative_path(path): if not os.path.isabs(path): return path # already relative dir = path names = [] while True: prevdir = dir dir, name = os.path.split(prevdir) <mask>: return path # failed to make it relative names.append(name) try: if samefile(dir, os.curdir): names.reverse() return os.path.join(*names) except OSError: pass",False,if dir == prevdir or not dir :,if not os . path . islink ( dir ) :,0.8820016898747209,5.300156689756295
"def _format_micros(self, datestring): parts = datestring[:-1].split(""."") if len(parts) == 1: <mask>: return datestring[:-1] + "".000000Z"" else: return datestring + "".000000Z"" else: micros = parts[-1][:6] if len(parts[-1]) > 6 else parts[-1] return ""."".join(parts[:-1] + [""{:06d}"".format(int(micros))]) + ""Z""",False,"if datestring . endswith ( ""Z"" ) :",if len ( parts ) > 1 :,0.8820016898747209,6.082317172853824
"def preprocess_raw_enwik9(input_filename, output_filename): with open(input_filename, ""r"") as f1: with open(output_filename, ""w"") as f2: while True: line = f1.readline() if not line: break line = list(enwik9_norm_transform([line]))[0] <mask>: if line[0] == "" "": line = line[1:] f2.writelines(line + ""\n"")",False,"if line != "" "" and line != """" :",if line :,0.8820016898747209,0.0
"def set(self, item, data): if not type(item) is slice: item = slice(item, item + len(data), None) virt_item = self.item2virtitem(item) if not virt_item: return off = 0 for s, n_item in virt_item: <mask>: i = slice(off, n_item.stop + off - n_item.start, n_item.step) data_slice = data.__getitem__(i) s.content.__setitem__(n_item, data_slice) off = i.stop else: raise ValueError(""TODO XXX"") return",False,"if isinstance ( s , ProgBits ) :",if n_item . start < off :,0.8820016898747209,5.669791110976001
"def walk(msg, callback, data): partnum = 0 for part in msg.walk(): # multipart/* are just containers if part.get_content_maintype() == ""multipart"": continue ctype = part.get_content_type() if ctype is None: ctype = OCTET_TYPE filename = part.get_filename() <mask>: filename = PART_FN_TPL % (partnum) headers = dict(part) LOG.debug(headers) headers[""Content-Type""] = ctype payload = util.fully_decoded_payload(part) callback(data, filename, payload, headers) partnum = partnum + 1",False,if not filename :,if filename is None :,0.8820016898747209,14.058533129758727
"def _run_wes(args): """"""Run CWL using a Workflow Execution Service (WES) endpoint"""""" main_file, json_file, project_name = _get_main_and_json(args.directory) main_file = _pack_cwl(main_file) if args.host and ""stratus"" in args.host: _run_wes_stratus(args, main_file, json_file) else: opts = [""--no-wait""] <mask>: opts += [""--host"", args.host] if args.auth: opts += [""--auth"", args.auth] cmd = [""wes-client""] + opts + [main_file, json_file] _run_tool(cmd)",True,if args . host :,if args . host :,0.8820016898747209,100.00000000000004
"def insertTestData(self, rows): for row in rows: if isinstance(row, Worker): self.workers[row.id] = dict( id=row.id, name=row.name, paused=0, graceful=0, info=row.info ) <mask>: row.id = row.buildermasterid * 10000 + row.workerid self.configured[row.id] = dict( buildermasterid=row.buildermasterid, workerid=row.workerid ) elif isinstance(row, ConnectedWorker): self.connected[row.id] = dict(masterid=row.masterid, workerid=row.workerid)",True,"elif isinstance ( row , ConfiguredWorker ) :","elif isinstance ( row , ConfiguredWorker ) :",0.8820016898747209,100.00000000000004
"def local_shape_to_shape_i(node): if node.op == T.shape: # This optimization needs ShapeOpt and fgraph.shape_feature <mask>: return shape_feature = node.fgraph.shape_feature ret = shape_feature.make_vector_shape(node.inputs[0]) # We need to copy over stack trace from input to output copy_stack_trace(node.outputs[0], ret) return [ret]",False,"if not hasattr ( node . fgraph , ""shape_feature"" ) :",if node . fgraph . shape_feature is None :,0.8820016898747209,17.52592436173078
"def get_config(): """"""Get INI parser with version.ini data."""""" # TODO(hanuszczak): See comment in `setup.py` for `grr-response-proto`. ini_path = os.path.join(THIS_DIRECTORY, ""version.ini"") <mask>: ini_path = os.path.join(THIS_DIRECTORY, ""../../version.ini"") if not os.path.exists(ini_path): raise RuntimeError(""Couldn't find version.ini"") config = configparser.ConfigParser() config.read(ini_path) return config",True,if not os . path . exists ( ini_path ) :,if not os . path . exists ( ini_path ) :,0.8820016898747209,100.00000000000004
"def init_weights(self, pretrained=None): if isinstance(pretrained, str): logger = logging.getLogger() load_checkpoint(self, pretrained, strict=False, logger=logger) elif pretrained is None: for m in self.modules(): <mask>: kaiming_init(m) elif isinstance(m, (_BatchNorm, nn.GroupNorm)): constant_init(m, 1) else: raise TypeError(""pretrained must be a str or None"")",True,"if isinstance ( m , nn . Conv2d ) :","if isinstance ( m , nn . Conv2d ) :",0.8820016898747209,100.00000000000004
"def isValidDateString(config_param_name, value, valid_value): try: <mask>: return value day, month, year = value.split(""-"") if int(day) < 1 or int(day) > 31: raise DateStringValueError(config_param_name, value) if int(month) < 1 or int(month) > 12: raise DateStringValueError(config_param_name, value) if int(year) < 1900 or int(year) > 2013: raise DateStringValueError(config_param_name, value) return value except Exception: raise DateStringValueError(config_param_name, value)",False,"if value == ""DD-MM-YYYY"" :",if valid_value :,0.8820016898747209,7.715486568024961
"def from_obj(cls, py_obj): if not isinstance(py_obj, Image): raise TypeError(""py_obj must be a wandb.Image"") else: <mask>: box_keys = list(py_obj._boxes.keys()) else: box_keys = [] if hasattr(py_obj, ""masks"") and py_obj.masks: mask_keys = list(py_obj.masks.keys()) else: mask_keys = [] return cls(box_keys, mask_keys)",True,"if hasattr ( py_obj , ""_boxes"" ) and py_obj . _boxes :","if hasattr ( py_obj , ""_boxes"" ) and py_obj . _boxes :",0.8820016898747209,100.00000000000004
"def _path_type(st, lst): parts = [] if st: if stat.S_ISREG(st.st_mode): parts.append(""file"") <mask>: parts.append(""dir"") else: parts.append(""other"") if lst: if stat.S_ISLNK(lst.st_mode): parts.append(""link"") return "" "".join(parts)",True,elif stat . S_ISDIR ( st . st_mode ) :,elif stat . S_ISDIR ( st . st_mode ) :,0.8820016898747209,100.00000000000004
"def is_destructive(queries): """"""Returns if any of the queries in *queries* is destructive."""""" keywords = (""drop"", ""shutdown"", ""delete"", ""truncate"", ""alter"") for query in sqlparse.split(queries): if query: <mask>: return True elif query_starts_with( query, [""update""] ) is True and not query_has_where_clause(query): return True return False",False,"if query_starts_with ( query , keywords ) is True :",if query in keywords :,0.8820016898747209,4.199688916946863
"def _store_gsuite_membership_post(self): """"""Flush storing gsuite memberships."""""" if not self.member_cache: return self.session.flush() # session.execute automatically flushes if self.membership_items: <mask>: # SQLite doesn't support bulk insert for item in self.membership_items: stmt = self.dao.TBL_MEMBERSHIP.insert(item) self.session.execute(stmt) else: stmt = self.dao.TBL_MEMBERSHIP.insert(self.membership_items) self.session.execute(stmt)",False,"if get_sql_dialect ( self . session ) == ""sqlite"" :","if self . db . db_type == ""sqlite"" :",0.8820016898747209,33.58216499387018
"def forward(self, inputs: paddle.Tensor): outputs = [] blocks = self.block(inputs) route = None for i, block in enumerate(blocks): <mask>: block = paddle.concat([route, block], axis=1) route, tip = self.yolo_blocks[i](block) block_out = self.block_outputs[i](tip) outputs.append(block_out) if i < 2: route = self.route_blocks_2[i](route) route = self.upsample(route) return outputs",False,if i > 0 :,if i < 1 :,0.8820016898747209,23.643540225079384
"def deep_dict(self, root=None): if root is None: root = self result = {} for key, value in root.items(): <mask>: result[key] = self.deep_dict(root=self.__class__._get_next(key, root)) else: result[key] = value return result",True,"if isinstance ( value , dict ) :","if isinstance ( value , dict ) :",0.8820016898747209,100.00000000000004
"def _parse_param_list(self, content): r = Reader(content) params = [] while not r.eof(): header = r.read().strip() <mask>: arg_name, arg_type = header.split("" : "")[:2] else: arg_name, arg_type = header, """" desc = r.read_to_next_unindented_line() desc = dedent_lines(desc) params.append((arg_name, arg_type, desc)) return params",False,"if "" : "" in header :","if "":"" in header :",0.8820016898747209,100.00000000000004
"def _ungroup(sequence, groups=None): for v in sequence: <mask>: if groups is not None: groups.append(list(_ungroup(v, groups=None))) for v in _ungroup(v, groups): yield v else: yield v",False,"if isinstance ( v , ( list , tuple ) ) :","if isinstance ( v , list ) :",0.8820016898747209,37.28878639930421
"def _add_resource_group(obj): if isinstance(obj, list): for array_item in obj: _add_resource_group(array_item) elif isinstance(obj, dict): try: if ""resourcegroup"" not in [x.lower() for x in obj.keys()]: if obj[""id""]: obj[""resourceGroup""] = _parse_id(obj[""id""])[""resource-group""] except (KeyError, IndexError, TypeError): pass for item_key in obj: <mask>: _add_resource_group(obj[item_key])",False,"if item_key != ""sourceVault"" :","if item_key . lower ( ) in [ ""resourceGroup"" , ""id"" ] :",0.8820016898747209,14.576846149722611
"def haslayer(self, cls): """"""true if self has a layer that is an instance of cls. Superseded by ""cls in self"" syntax."""""" if self.__class__ == cls or self.__class__.__name__ == cls: return 1 for f in self.packetfields: fvalue_gen = self.getfieldval(f.name) if fvalue_gen is None: continue if not f.islist: fvalue_gen = SetGen(fvalue_gen, _iterpacket=0) for fvalue in fvalue_gen: <mask>: ret = fvalue.haslayer(cls) if ret: return ret return self.payload.haslayer(cls)",False,"if isinstance ( fvalue , Packet ) :","if isinstance ( fvalue , cls ) :",0.8820016898747209,59.4603557501361
"def _post_attachment(self, message, channel, color, sub_fields=None): if channel is None: message_channels = self.channels else: message_channels = [channel] for message_channel in message_channels: attachment = { ""fallback"": message, ""text"": message, ""color"": color, } <mask>: attachment[""fields""] = sub_fields self.slack_client.api_call( ""chat.postMessage"", channel=message_channel, attachments=[attachment], as_user=True, )",False,if sub_fields is not None :,if sub_fields :,0.8820016898747209,38.80684294761701
"def create(cls, repository, args): key = cls() passphrase = os.environ.get(""ATTIC_PASSPHRASE"") if passphrase is not None: passphrase2 = passphrase else: passphrase, passphrase2 = 1, 2 while passphrase != passphrase2: passphrase = getpass(""Enter passphrase: "") <mask>: print(""Passphrase must not be blank"") continue passphrase2 = getpass(""Enter same passphrase again: "") if passphrase != passphrase2: print(""Passphrases do not match"") key.init(repository, passphrase) if passphrase: print(""Remember your passphrase. Your data will be inaccessible without it."") return key",True,if not passphrase :,if not passphrase :,0.8820016898747209,100.00000000000004
"def _generate_create_date(self): if self.timezone is not None: # First, assume correct capitalization tzinfo = tz.gettz(self.timezone) <mask>: # Fall back to uppercase tzinfo = tz.gettz(self.timezone.upper()) if tzinfo is None: raise util.CommandError(""Can't locate timezone: %s"" % self.timezone) create_date = ( datetime.datetime.utcnow().replace(tzinfo=tz.tzutc()).astimezone(tzinfo) ) else: create_date = datetime.datetime.now() return create_date",True,if tzinfo is None :,if tzinfo is None :,0.8820016898747209,100.00000000000004
"def _read_header_lines(fp): """"""Read lines with headers until the start of body"""""" lines = deque() for line in fp: if is_empty(line): break # tricky case if it's not a header and not an empty line # usually means that user forgot to separate the body and newlines # so ""unread"" this line here, what means to treat it like a body <mask>: fp.seek(fp.tell() - len(line)) break lines.append(line) return lines",False,if not _RE_HEADER . match ( line ) :,if not is_header ( line ) :,0.8820016898747209,26.432408210372945
"def _media_files_drag_received(widget, context, x, y, data, info, timestamp): uris = data.get_uris() files = [] for uri in uris: try: uri_tuple = GLib.filename_from_uri(uri) except: continue uri, unused = uri_tuple <mask>: if utils.is_media_file(uri) == True: files.append(uri) if len(files) == 0: return open_dropped_files(files)",False,if os . path . exists ( uri ) == True :,if uri :,0.8820016898747209,0.0
"def remove_importlib(frame, options): if frame is None: return None for child in frame.children: remove_importlib(child, options=options) <mask>: # remove this node, moving the self_time and children up to the parent frame.self_time += child.self_time frame.add_children(child.children, after=child) child.remove_from_parent() return frame",False,"if ""<frozen importlib._bootstrap"" in child . file_path :",if child . self_time :,0.8820016898747209,4.565849196164922
"def __call__(self, graph): for layer_name, data in self.params: <mask>: node = graph.get_node(layer_name) node.data = self.adjust_parameters(node, data) else: print_stderr(""Ignoring parameters for non-existent layer: %s"" % layer_name) return graph",False,if layer_name in graph :,if layer_name in graph . get_layers ( ) :,0.8820016898747209,39.553325358771794
"def test_with_three_points(self): cba = ia.Polygon([(1, 2), (3, 4), (5, 5)]) for i, xy in enumerate(cba): assert i in [0, 1, 2] if i == 0: assert np.allclose(xy, (1, 2)) <mask>: assert np.allclose(xy, (3, 4)) elif i == 2: assert np.allclose(xy, (5, 5)) assert i == 2",True,elif i == 1 :,elif i == 1 :,0.8820016898747209,100.00000000000004
"def _serve(self): self._conn = self.manager.request(REQUEST_DNS_LISTENER, self.domain) conn = MsgPackMessages(self._conn) while self.active: request = conn.recv() if not request: logger.warning(""DNS: Recieved empty request. Shutdown"") self.stop() break now = time.time() response = self.handler.process(request) if not response: response = [] used = time.time() - now <mask>: logger.warning(""DNS: Slow processing speed (%s)s"", used) conn.send(response)",False,if used > 1 :,if used > self . delay :,0.8820016898747209,26.269098944241588
"def read(cls, fp, **kwargs): major_version, minor_version, count = read_fmt(""2HI"", fp) items = [] for _ in range(count): length = read_fmt(""I"", fp)[0] - 4 <mask>: with io.BytesIO(fp.read(length)) as f: items.append(Annotation.read(f)) return cls(major_version=major_version, minor_version=minor_version, items=items)",True,if length > 0 :,if length > 0 :,0.8820016898747209,100.00000000000004
"def save_uploaded_files(): files = [] unzip = bool(request.form.get(""unzip"") in [""true"", ""on""]) for uploaded_file in request.files.getlist(""files""): <mask>: with zipfile.ZipFile(uploaded_file, ""r"") as zf: for info in zf.infolist(): name = info.filename size = info.file_size data = zf.read(name) if size > 0: files.append(save_file(data, filename=name.split(""/"")[-1])) else: files.append(save_file(uploaded_file)) return files",False,if unzip and zipfile . is_zipfile ( uploaded_file ) :,if unzip :,0.8820016898747209,0.0
"def analyze_string_content(self, string, line_num, filename): output = {} if self.keyword_exclude and self.keyword_exclude.search(string): return output for identifier in self.secret_generator( string, filetype=determine_file_type(filename), ): <mask>: continue secret = PotentialSecret( self.secret_type, filename, identifier, line_num, ) output[secret] = secret return output",False,if self . is_secret_false_positive ( identifier ) :,if identifier is None :,0.8820016898747209,2.497149970415641
"def _validate_and_set_default_hyperparameters(self): """"""Placeholder docstring"""""" # Check if all the required hyperparameters are set. If there is a default value # for one, set it. for name, definition in self.hyperparameter_definitions.items(): if name not in self.hyperparam_dict: spec = definition[""spec""] if ""DefaultValue"" in spec: self.hyperparam_dict[name] = spec[""DefaultValue""] <mask>: raise ValueError(""Required hyperparameter: %s is not set"" % name)",False,"elif ""IsRequired"" in spec and spec [ ""IsRequired"" ] :",if name not in self . hyperparam_dict :,0.8820016898747209,3.3449303459224256
"def get_code(self, fullname=None): fullname = self._fix_name(fullname) if self.code is None: mod_type = self.etc[2] if mod_type == imp.PY_SOURCE: source = self.get_source(fullname) self.code = compile(source, self.filename, ""exec"") elif mod_type == imp.PY_COMPILED: self._reopen() try: self.code = read_code(self.file) finally: self.file.close() <mask>: self.code = self._get_delegate().get_code() return self.code",False,elif mod_type == imp . PKG_DIRECTORY :,if self . _get_delegate :,0.8820016898747209,4.736913377107212
"def eigh_abstract_eval(operand, lower): if isinstance(operand, ShapedArray): <mask>: raise ValueError( ""Argument to symmetric eigendecomposition must have shape [..., n, n],"" ""got shape {}"".format(operand.shape) ) batch_dims = operand.shape[:-2] n = operand.shape[-1] v = ShapedArray(batch_dims + (n, n), operand.dtype) w = ShapedArray(batch_dims + (n,), lax.lax._complex_basetype(operand.dtype)) else: v, w = operand, operand return v, w",False,if operand . ndim < 2 or operand . shape [ - 2 ] != operand . shape [ - 1 ] :,if lower :,0.8820016898747209,0.0
"def conninfo_parse(dsn): ret = {} length = len(dsn) i = 0 while i < length: if dsn[i].isspace(): i += 1 continue param_match = PARAMETER_RE.match(dsn[i:]) <mask>: return param = param_match.group(1) i += param_match.end() if i >= length: return value, end = read_param_value(dsn[i:]) if value is None: return i += end ret[param] = value return ret",False,if not param_match :,if param_match is None :,0.8820016898747209,27.77619034011791
"def load_weights_from_unsupervised(self, unsupervised_model): update_state_dict = copy.deepcopy(self.network.state_dict()) for param, weights in unsupervised_model.network.state_dict().items(): if param.startswith(""encoder""): # Convert encoder's layers name to match new_param = ""tabnet."" + param else: new_param = param <mask>: # update only common layers update_state_dict[new_param] = weights self.network.load_state_dict(update_state_dict)",False,if self . network . state_dict ( ) . get ( new_param ) is not None :,if new_param in self . common_layers :,0.8820016898747209,7.948786838905727
"def viewer_setup(self): for key, value in DEFAULT_CAMERA_CONFIG.items(): <mask>: getattr(self.viewer.cam, key)[:] = value else: setattr(self.viewer.cam, key, value)",False,"if isinstance ( value , np . ndarray ) :","if isinstance ( value , list ) :",0.8820016898747209,46.307771619910305
"def colormap_changed(change): if change[""new""]: cmap_colors = [ color[1:] for color in cmap.step.__dict__[""_schemes""][colormap.value] ] palette.value = "", "".join(cmap_colors) colorbar = getattr(cmap.step, colormap.value) colorbar_output = self.colorbar_widget with colorbar_output: colorbar_output.clear_output() display(colorbar) <mask>: labels = [f""Class {i+1}"" for i in range(len(palette.value.split("","")))] legend_labels.value = "", "".join(labels)",False,"if len ( palette . value ) > 0 and "","" in palette . value :",if colormap . value :,0.8820016898747209,3.173613488953928
"def invalidate(self, layers=None): if layers is None: layers = Layer.AllLayers if layers: layers = set(layers) self.invalidLayers.update(layers) blockRenderers = [ br for br in self.blockRenderers if br.layer is Layer.Blocks or br.layer not in layers ] <mask>: self.forgetDisplayLists() self.blockRenderers = blockRenderers if self.renderer.showRedraw and Layer.Blocks in layers: self.needsRedisplay = True",False,if len ( blockRenderers ) < len ( self . blockRenderers ) :,if len ( blockRenderers ) > 0 :,0.8820016898747209,28.96204682801084
"def fromstring(cls, input): productions = [] for linenum, line in enumerate(input.split(""\n"")): line = line.strip() <mask>: continue try: productions += _read_dependency_production(line) except ValueError: raise ValueError(""Unable to parse line %s: %s"" % (linenum, line)) if len(productions) == 0: raise ValueError(""No productions found!"") return DependencyGrammar(productions)",False,"if line . startswith ( ""#"" ) or line == """" :",if not line :,0.8820016898747209,1.0466441829132096
"def repl(m, base_path, rel_path=None): if m.group(""comments""): tag = m.group(""comments"") else: tag = m.group(""open"") <mask>: tag += RE_TAG_LINK_ATTR.sub( lambda m2: repl_absolute(m2, base_path), m.group(""attr"") ) else: tag += RE_TAG_LINK_ATTR.sub( lambda m2: repl_relative(m2, base_path, rel_path), m.group(""attr"") ) tag += m.group(""close"") return tag",True,if rel_path is None :,if rel_path is None :,0.8820016898747209,100.00000000000004
"def encode(path): if isinstance(path, str_cls): try: path = path.encode(fs_encoding, ""strict"") except UnicodeEncodeError: <mask>: raise path = path.encode(fs_fallback_encoding, ""strict"") return path",False,if not platform . is_linux ( ) :,if not path . endswith ( fs_fallback_encoding ) :,0.8820016898747209,10.04916995660316
"def __iter__(self): base_iterator = super(ProcessIterable, self).__iter__() if getattr(self.queryset, ""_coerced"", False): for process in base_iterator: <mask>: process = coerce_to_related_instance( process, process.flow_class.process_class ) yield process else: for process in base_iterator: yield process",False,"if isinstance ( process , self . queryset . model ) :","if isinstance ( process , Process ) :",0.8820016898747209,36.06452879987793
"def footnotes_under(n: Element) -> Iterator[nodes.footnote]: if isinstance(n, nodes.footnote): yield n else: for c in n.children: <mask>: continue elif isinstance(c, nodes.Element): yield from footnotes_under(c)",False,"if isinstance ( c , addnodes . start_of_file ) :","if c . tag == ""footnote"" :",0.8820016898747209,3.9778149665594618
"def _process_submissions(self) -> None: """"""Process all submissions which have not been processed yet."""""" while self._to_be_processed: job = self._to_be_processed[0] job.process() # trigger computation <mask>: heapq.heappush( self._steady_priority_queue, OrderedJobs(job.release_time, self._order, job), ) self._to_be_processed.popleft() # remove right after it is added to the heap queue self._order += 1",False,if not self . batch_mode :,if job . release_time :,0.8820016898747209,8.051153633013374
"def valid_localparts(strip_delimiters=False): for line in ABRIDGED_LOCALPART_VALID_TESTS.split(""\n""): # strip line, skip over empty lines line = line.strip() if line == """": continue # skip over comments or empty lines match = COMMENT.match(line) <mask>: continue # skip over localparts with delimiters if strip_delimiters: if "","" in line or "";"" in line: continue yield line",False,if match :,if match is None :,0.8820016898747209,23.643540225079384
"def _get_payload_hash(self, method, data=None): if method in (""POST"", ""PUT""): if data: <mask>: # File upload; don't try to read the entire payload return UNSIGNED_PAYLOAD return _hash(data) else: return UNSIGNED_PAYLOAD else: return _hash("""")",False,"if hasattr ( data , ""next"" ) or hasattr ( data , ""__next__"" ) :","if data [ 0 ] == ""file"" :",0.8820016898747209,1.8825235322509422
"def get_download_info(self): try: download_info = self.api.get_download_info(self.game) result = True except NoDownloadLinkFound as e: print(e) <mask>: Config.unset(""current_download"") GLib.idle_add( self.parent.parent.show_error, _(""Download error""), _( ""There was an error when trying to fetch the download link!\n{}"".format( e ) ), ) download_info = False result = False return result, download_info",False,"if Config . get ( ""current_download"" ) == self . game . id :",if e . status == 404 :,0.8820016898747209,3.5114603692955577
"def find_id(self, doc_id): self._lock.acquire() try: doc = self._docs.get(doc_id) <mask>: doc = copy.deepcopy(doc) doc[""id""] = doc_id return doc finally: self._lock.release()",True,if doc :,if doc :,0.8820016898747209,0.0
"def assign_art(self, session, task): """"""Place the discovered art in the filesystem."""""" if task in self.art_candidates: candidate = self.art_candidates.pop(task) self._set_art(task.album, candidate, not self.src_removed) <mask>: task.prune(candidate.path)",False,if self . src_removed :,if candidate . path :,0.8820016898747209,9.423716574733431
"def _replace_named(self, named, replace_scalar): for item in named: for name, value in self._get_replaced_named(item, replace_scalar): <mask>: raise DataError(""Argument names must be strings."") yield name, value",False,if not is_string ( name ) :,"if not isinstance ( name , str ) :",0.8820016898747209,16.51582159006904
"def qtTypeIdent(conn, *args): # We're not using the conn object at the moment, but - we will # modify the # logic to use the server version specific keywords later. res = None value = None for val in args: # DataType doesn't have len function then convert it to string if not hasattr(val, ""__len__""): val = str(val) if len(val) == 0: continue value = val <mask>: value = value.replace('""', '""""') value = '""' + value + '""' res = ((res and res + ""."") or """") + value return res",False,"if Driver . needsQuoting ( val , True ) :","if ""'"" in value :",0.8820016898747209,5.08764122072739
"def _update_tileable_and_chunk_shape(self, tileable_graph, chunk_result, failed_ops): for n in tileable_graph: <mask>: continue tiled_n = get_tiled(n) if has_unknown_shape(tiled_n): if any(c.key not in chunk_result for c in tiled_n.chunks): # some of the chunks has been fused continue new_nsplits = self.get_tileable_nsplits(n, chunk_result=chunk_result) for node in (n, tiled_n): node._update_shape(tuple(sum(nsplit) for nsplit in new_nsplits)) tiled_n._nsplits = new_nsplits",False,if n . op in failed_ops :,if n . _nsplits :,0.8820016898747209,20.82186541080652
"def _read_filter(self, data): if data: <mask>: self.inner_sha.update(data) if self.expected_inner_md5sum: self.inner_md5.update(data) return data",False,if self . expected_inner_sha256 :,if self . expected_inner_shasum :,0.8820016898747209,75.06238537503395
"def find_previous_editable(self, *args): if self.editw == 0: if self._active_page > 0: self.switch_page(self._active_page - 1) if not self.editw == 0: # remember that xrange does not return the 'last' value, # so go to -1, not 0! (fence post error in reverse) for n in range(self.editw - 1, -1, -1): <mask>: self.editw = n break",False,if self . _widgets__ [ n ] . editable and not self . _widgets__ [ n ] . hidden :,if self . _edit_range [ n ] == 0 :,0.8820016898747209,11.619025350807155
"def _get_event_for_message(self, message_id): with self.event_lock: <mask>: raise RuntimeError( ""Event for message[{}] should have been created before accessing"".format( message_id ) ) return self._events[message_id]",False,if message_id not in self . _events :,if not self . _events :,0.8820016898747209,37.764976913718144
"def _get_deepest(self, t): if isinstance(t, list): if len(t) == 1: return t[0] else: for part in t: res = self._get_deepest(part) <mask>: return res return None return None",False,if res :,if res is not None :,0.8820016898747209,17.965205598154213
"def _get_notify(self, action_node): if action_node.name not in self._skip_notify_tasks: <mask>: task_notify = NotificationsHelper.to_model(action_node.notify) return task_notify elif self._chain_notify: return self._chain_notify return None",True,if action_node . notify :,if action_node . notify :,0.8820016898747209,100.00000000000004
"def __init__(self, centered=None, shape_params=()): assert centered is None or isinstance(centered, (float, torch.Tensor)) assert isinstance(shape_params, (tuple, list)) assert all(isinstance(name, str) for name in shape_params) if is_validation_enabled(): if isinstance(centered, float): assert 0 <= centered and centered <= 1 <mask>: assert (0 <= centered).all() assert (centered <= 1).all() else: assert centered is None self.centered = centered self.shape_params = shape_params",False,"elif isinstance ( centered , torch . Tensor ) :","elif isinstance ( centered , tuple ) :",0.8820016898747209,46.307771619910305
"def collect(self): for nickname in self.squid_hosts.keys(): squid_host = self.squid_hosts[nickname] fulldata = self._getData(squid_host[""host""], squid_host[""port""]) if fulldata is not None: fulldata = fulldata.splitlines() for data in fulldata: matches = self.stat_pattern.match(data) <mask>: self.publish_counter( ""%s.%s"" % (nickname, matches.group(1)), float(matches.group(2)) )",True,if matches :,if matches :,0.8820016898747209,0.0
"def test_len(self): eq = self.assertEqual eq(base64MIME.base64_len(""hello""), len(base64MIME.encode(""hello"", eol=""""))) for size in range(15): if size == 0: bsize = 0 elif size <= 3: bsize = 4 elif size <= 6: bsize = 8 <mask>: bsize = 12 elif size <= 12: bsize = 16 else: bsize = 20 eq(base64MIME.base64_len(""x"" * size), bsize)",False,elif size <= 9 :,elif size <= 10 :,0.8820016898747209,53.7284965911771
"def wait_for_initial_conf(self, timeout=1.0): logger.info(""Waiting for initial configuration"") cur_timeout = timeout # Arbiter do not already set our have_conf param while not self.new_conf and not self.interrupted: elapsed, _, _ = self.handleRequests(cur_timeout) if elapsed: cur_timeout -= elapsed <mask>: continue cur_timeout = timeout sys.stdout.write(""."") sys.stdout.flush()",False,if cur_timeout > 0 :,if cur_timeout <= 0 :,0.8820016898747209,41.11336169005198
"def __init__(self, querylist=None): self.query_id = -1 if querylist is None: self.querylist = [] else: self.querylist = querylist for query in self.querylist: if self.query_id == -1: self.query_id = query.query_id else: <mask>: raise ValueError(""query in list must be same query_id"")",True,if self . query_id != query . query_id :,if self . query_id != query . query_id :,0.8820016898747209,100.00000000000004
"def candidates() -> Generator[""Symbol"", None, None]: s = self if Symbol.debug_lookup: Symbol.debug_print(""searching in self:"") print(s.to_string(Symbol.debug_indent + 1), end="""") while True: if matchSelf: yield s <mask>: yield from s.children_recurse_anon else: yield from s._children if s.siblingAbove is None: break s = s.siblingAbove if Symbol.debug_lookup: Symbol.debug_print(""searching in sibling:"") print(s.to_string(Symbol.debug_indent + 1), end="""")",False,if recurseInAnon :,elif matchAnon :,0.8820016898747209,0.0
"def get_default_params(problem_type: str, penalty: str): # TODO: get seed from seeds provider if problem_type == REGRESSION: default_params = {""C"": None, ""random_state"": 0, ""fit_intercept"": True} <mask>: default_params[""solver""] = ""auto"" else: default_params = { ""C"": None, ""random_state"": 0, ""solver"": _get_solver(problem_type), ""n_jobs"": -1, ""fit_intercept"": True, } model_params = list(default_params.keys()) return model_params, default_params",False,if penalty == L2 :,"if penalty == ""auto"" :",0.8820016898747209,36.55552228545123
"def _UploadDirectory(local_dir: str, gcs_bucket: storage.Bucket, gcs_dir: str): """"""Upload the contents of a local directory to a GCS Bucket."""""" for file_name in os.listdir(local_dir): path = os.path.join(local_dir, file_name) <mask>: logging.info(""Skipping %s as it's not a file."", path) continue logging.info(""Uploading: %s"", path) gcs_blob = gcs_bucket.blob(f""{gcs_dir}/{file_name}"") gcs_blob.upload_from_filename(path)",True,if not os . path . isfile ( path ) :,if not os . path . isfile ( path ) :,0.8820016898747209,100.00000000000004
"def decode_query_ids(self, trans, conditional): if conditional.operator == ""and"": self.decode_query_ids(trans, conditional.left) self.decode_query_ids(trans, conditional.right) else: left_base = conditional.left.split(""."")[0] if left_base in self.FIELDS: field = self.FIELDS[left_base] <mask>: conditional.right = trans.security.decode_id(conditional.right)",False,if field . id_decode :,if field . is_id ( ) :,0.8820016898747209,21.10534063187263
"def data_dir(self) -> Path: try: from appdirs import user_data_dir except ImportError: # linux path = Path.home() / "".local"" / ""share"" <mask>: return path / ""dephell"" # mac os path = Path.home() / ""Library"" / ""Application Support"" if path.exists(): return path / ""dephell"" self.pip_main([""install"", ""appdirs""]) from appdirs import user_data_dir return Path(user_data_dir(""dephell""))",True,if path . exists ( ) :,if path . exists ( ) :,0.8820016898747209,100.00000000000004
"def setGameCard(self, isGameCard=False): if isGameCard: targetValue = 1 else: targetValue = 0 for nca in self: if isinstance(nca, Nca): <mask>: continue Print.info(""writing isGameCard for %s, %d"" % (str(nca._path), targetValue)) nca.header.setIsGameCard(targetValue)",False,if nca . header . getIsGameCard ( ) == targetValue :,if nca . header . isGameCard ( ) :,0.8820016898747209,38.03141958086991
"def check_apns_certificate(ss): mode = ""start"" for s in ss.split(""\n""): if mode == ""start"": if ""BEGIN RSA PRIVATE KEY"" in s or ""BEGIN PRIVATE KEY"" in s: mode = ""key"" <mask>: if ""END RSA PRIVATE KEY"" in s or ""END PRIVATE KEY"" in s: mode = ""end"" break elif s.startswith(""Proc-Type"") and ""ENCRYPTED"" in s: raise ImproperlyConfigured( ""Encrypted APNS private keys are not supported"" ) if mode != ""end"": raise ImproperlyConfigured(""The APNS certificate doesn't contain a private key"")",False,"elif mode == ""key"" :","elif s . startswith ( ""Proc-Type"" ) :",0.8820016898747209,5.934202609760488
"def register_aggregate_groups(conn, *groups): seen = set() for group in groups: klasses = AGGREGATE_COLLECTION[group] for klass in klasses: name = getattr(klass, ""name"", klass.__name__) <mask>: seen.add(name) conn.create_aggregate(name, -1, klass)",True,if name not in seen :,if name not in seen :,0.8820016898747209,100.00000000000004
"def _impl(inputs, input_types): data = inputs[0] axis = None keepdims = False if len(inputs) > 2: # default, torch have only data, axis=None, keepdims=False if isinstance(inputs[1], int): axis = int(inputs[1]) <mask>: axis = inputs[1] else: axis = list(_infer_shape(inputs[1])) keepdims = bool(inputs[2]) return get_relay_op(name)(data, axis=axis, keepdims=keepdims)",False,elif _is_int_seq ( inputs [ 1 ] ) :,"elif isinstance ( inputs [ 1 ] , list ) :",0.8820016898747209,31.763442542283983
"def walks_generator(): if filelist is not None: bucket = [] for filename in filelist: with io.open(filename) as inf: for line in inf: walk = [int(x) for x in line.strip(""\n"").split("" "")] bucket.append(walk) if len(bucket) == batch_size: yield bucket bucket = [] <mask>: yield bucket else: for _ in range(epoch): for nodes in graph.node_batch_iter(batch_size): walks = graph.random_walk(nodes, walk_len) yield walks",False,if len ( bucket ) :,if epoch == 0 :,0.8820016898747209,9.652434877402245
"def _calculate_runtimes(states): results = {""runtime"": 0.00, ""num_failed_states"": 0, ""num_passed_states"": 0} for state, resultset in states.items(): <mask>: # Count the pass vs failures if resultset[""result""]: results[""num_passed_states""] += 1 else: results[""num_failed_states""] += 1 # Count durations results[""runtime""] += resultset[""duration""] log.debug(""Parsed state metrics: {}"".format(results)) return results",False,"if isinstance ( resultset , dict ) and ""duration"" in resultset :","if state in resultset [ ""states"" ] :",0.8820016898747209,7.40354787297858
"def _replicator_primary_device() -> snt_replicator.Replicator: # NOTE: The explicit device list is required since currently Replicator # only considers CPU and GPU devices. This means on TPU by default we only # mirror on the local CPU. for device_type in (""TPU"", ""GPU"", ""CPU""): devices = tf.config.experimental.list_logical_devices(device_type=device_type) <mask>: devices = [d.name for d in devices] logging.info(""Replicating over %s"", devices) return snt_replicator.Replicator(devices=devices) assert False, ""No TPU/GPU or CPU found""",True,if devices :,if devices :,0.8820016898747209,0.0
"def get_tag_values(self, event): http = event.interfaces.get(""sentry.interfaces.Http"") if not http: return [] if not http.headers: return [] headers = http.headers # XXX: transitional support for workers if isinstance(headers, dict): headers = headers.items() output = [] for key, value in headers: <mask>: continue ua = Parse(value) if not ua: continue result = self.get_tag_from_ua(ua) if result: output.append(result) return output",False,"if key != ""User-Agent"" :","if key . startswith ( ""HTTP_"" ) :",0.8820016898747209,9.425159511373677
"def general(metadata, value): if metadata.get(""commands"") and value: <mask>: v = quote(value) else: v = value return u""{0} {1}"".format(metadata[""commands""][0], v) else: if not value: return None elif not metadata.get(""nargs""): return quote(value) else: return value",False,"if not metadata . get ( ""nargs"" ) :","if metadata [ ""commands"" ] [ 0 ] == ""all"" :",0.8820016898747209,3.6570159134143823
"def _actions_read(self, c): self.action_input.handle_read(c) if c in [curses.KEY_ENTER, util.KEY_ENTER2]: # take action if self.action_input.selected_index == 0: # Cancel self.back_to_parent() elif self.action_input.selected_index == 1: # Apply self._apply_prefs() client.core.get_config().addCallback(self._update_preferences) <mask>: # OK self._apply_prefs() self.back_to_parent()",True,elif self . action_input . selected_index == 2 :,elif self . action_input . selected_index == 2 :,0.8820016898747209,100.00000000000004
def logic(): if reset == 1: lfsr.next = 1 else: <mask>: # lfsr.next[24:1] = lfsr[23:0] lfsr.next = lfsr << 1 lfsr.next[0] = lfsr[23] ^ lfsr[22] ^ lfsr[21] ^ lfsr[16],False,if enable :,if reset == 2 :,0.8820016898747209,9.652434877402245
"def action_delete(self, request, attachments): deleted_attachments = [] desynced_posts = [] for attachment in attachments: <mask>: deleted_attachments.append(attachment.pk) desynced_posts.append(attachment.post_id) if desynced_posts: with transaction.atomic(): for post in Post.objects.filter(id__in=desynced_posts): self.delete_from_cache(post, deleted_attachments) for attachment in attachments: attachment.delete() message = _(""Selected attachments have been deleted."") messages.success(request, message)",False,if attachment . post :,if attachment . pk :,0.8820016898747209,42.72870063962342
"def __getitem__(self, index): if self._check(): if isinstance(index, int): if index < 0 or index >= len(self.features): raise IndexError(index) if self.features[index] is None: feature = self.device.feature_request(FEATURE.FEATURE_SET, 0x10, index) if feature: (feature,) = _unpack(""!H"", feature[:2]) self.features[index] = FEATURE[feature] return self.features[index] <mask>: indices = index.indices(len(self.features)) return [self.__getitem__(i) for i in range(*indices)]",False,"elif isinstance ( index , slice ) :",if index . indices ( len ( self . features ) ) > 0 :,0.8820016898747209,3.716499092256817
"def _skip_start(self): start, stop = self.start, self.stop for chunk in self.app_iter: self._pos += len(chunk) <mask>: continue elif self._pos == start: return b"""" else: chunk = chunk[start - self._pos :] if stop is not None and self._pos > stop: chunk = chunk[: stop - self._pos] assert len(chunk) == stop - start return chunk else: raise StopIteration()",False,if self . _pos < start :,if self . _pos >= stop :,0.8820016898747209,46.713797772819994
"def get_files(d): f = [] for root, dirs, files in os.walk(d): for name in files: <mask>: continue if ""qemux86copy-"" in root or ""qemux86-"" in root: continue if ""do_build"" not in name and ""do_populate_sdk"" not in name: f.append(os.path.join(root, name)) return f",False,"if ""meta-environment"" in root or ""cross-canadian"" in root :","if ""qemux86"" in root or ""qemux86"" in root :",0.8820016898747209,52.664038784792666
"def _load_windows_store_certs(self, storename, purpose): certs = bytearray() try: for cert, encoding, trust in enum_certificates(storename): # CA certs are never PKCS#7 encoded <mask>: if trust is True or purpose.oid in trust: certs.extend(cert) except PermissionError: warnings.warn(""unable to enumerate Windows certificate store"") if certs: self.load_verify_locations(cadata=certs) return certs",False,"if encoding == ""x509_asn"" :","if encoding == ""PKCS#7"" :",0.8820016898747209,45.180100180492246
"def test_tokenizer_identifier_with_correct_config(self): for tokenizer_class in [BertTokenizer, BertTokenizerFast, AutoTokenizer]: tokenizer = tokenizer_class.from_pretrained(""wietsedv/bert-base-dutch-cased"") self.assertIsInstance(tokenizer, (BertTokenizer, BertTokenizerFast)) <mask>: self.assertEqual(tokenizer.basic_tokenizer.do_lower_case, False) else: self.assertEqual(tokenizer.do_lower_case, False) self.assertEqual(tokenizer.model_max_length, 512)",False,"if isinstance ( tokenizer , BertTokenizer ) :",if tokenizer . basic_tokenizer :,0.8820016898747209,7.492442692259767
"def run(self): global WAITING_BEFORE_START time.sleep(WAITING_BEFORE_START) while self.keep_alive: path_id, module, resolve = self.queue_receive.get() if path_id is None: continue self.lock.acquire() self.modules[path_id] = module self.lock.release() <mask>: resolution = self._resolve_with_other_modules(resolve) self._relations[path_id] = [] for package in resolution: self._relations[path_id].append(resolution[package]) self.queue_send.put((path_id, module, False, resolution))",True,if resolve :,if resolve :,0.8820016898747209,0.0
"def __new__(mcs, name, bases, attrs): include_profile = include_trace = include_garbage = True bases = list(bases) if name == ""SaltLoggingClass"": for base in bases: if hasattr(base, ""trace""): include_trace = False <mask>: include_garbage = False if include_profile: bases.append(LoggingProfileMixin) if include_trace: bases.append(LoggingTraceMixin) if include_garbage: bases.append(LoggingGarbageMixin) return super(LoggingMixinMeta, mcs).__new__(mcs, name, tuple(bases), attrs)",False,"if hasattr ( base , ""garbage"" ) :","elif hasattr ( base , ""garbage"" ) :",0.8820016898747209,88.01117367933934
"def __str__(self, prefix="""", printElemNumber=0): res = """" if self.has_owner_: res += prefix + (""owner: %s\n"" % self.DebugFormatString(self.owner_)) cnt = 0 for e in self.entries_: elm = """" <mask>: elm = ""(%d)"" % cnt res += prefix + (""entries%s <\n"" % elm) res += e.__str__(prefix + "" "", printElemNumber) res += prefix + "">\n"" cnt += 1 return res",True,if printElemNumber :,if printElemNumber :,0.8820016898747209,0.0
"def parse_tag(self): buf = [] escaped = False for c in self.get_next_chars(): if escaped: buf.append(c) elif c == ""\\"": escaped = True <mask>: return """".join(buf) else: buf.append(c) raise Exception(""Unclosed tag "" + """".join(buf))",False,"elif c == "">"" :","elif c == ""\\"" :",0.8820016898747209,51.33450480401705
"def get_batches(train_nodes, train_labels, batch_size=64, shuffle=True): if shuffle: random.shuffle(train_nodes) total = train_nodes.shape[0] for i in range(0, total, batch_size): <mask>: cur_nodes = train_nodes[i : i + batch_size] cur_labels = train_labels[cur_nodes] yield cur_nodes, cur_labels",False,if i + batch_size <= total :,if i + batch_size < total :,0.8820016898747209,71.89393375176813
"def _get_all_info_lines(data): infos = [] for row in data: splitrow = row.split() if len(splitrow) > 0: <mask>: infos.append("" "".join(splitrow[1:])) return infos",False,"if splitrow [ 0 ] == ""INFO:"" :","if splitrow [ 0 ] == ""info"" :",0.8820016898747209,67.74702029865007
"def _validate_client_public_key(self, username, key_data): """"""Validate a client public key for the specified user"""""" try: key = decode_ssh_public_key(key_data) except KeyImportError: return None options = None if self._client_keys: options = self._client_keys.validate(key, self._peer_addr) if options is None: result = self._owner.validate_public_key(username, key) if asyncio.iscoroutine(result): result = yield from result <mask>: return None options = {} self._key_options = options return key",True,if not result :,if not result :,0.8820016898747209,100.00000000000004
"def attach_related_versions(addons, addon_dict=None): if addon_dict is None: addon_dict = {addon.id: addon for addon in addons} all_ids = set(filter(None, (addon._current_version_id for addon in addons))) versions = list(Version.objects.filter(id__in=all_ids).order_by()) for version in versions: try: addon = addon_dict[version.addon_id] except KeyError: log.info(""Version %s has an invalid add-on id."" % version.id) continue <mask>: addon._current_version = version version.addon = addon",False,if addon . _current_version_id == version . id :,if addon is not None :,0.8820016898747209,4.008579202215618
"def move_view(obj, evt): position = obj.GetCurrentCursorPosition() for other_axis, axis_number in self._axis_names.iteritems(): <mask>: continue ipw3d = getattr(self, ""ipw_3d_%s"" % other_axis) ipw3d.ipw.slice_position = position[axis_number]",False,if other_axis == axis_name :,if axis_number not in position :,0.8820016898747209,10.229197414177778
"def func_wrapper(*args, **kwargs): warnings.simplefilter(""always"", DeprecationWarning) # turn off filter for old, new in arg_mapping.items(): <mask>: warnings.warn( f""Keyword argument '{old}' has been "" f""deprecated in favour of '{new}'. "" f""'{old}' will be removed in a future version."", category=DeprecationWarning, stacklevel=2, ) val = kwargs.pop(old) kwargs[new] = val # reset filter warnings.simplefilter(""default"", DeprecationWarning) return func(*args, **kwargs)",True,if old in kwargs :,if old in kwargs :,0.8820016898747209,100.00000000000004
"def inner_connection_checker(self, *args, **kwargs): LOG.debug(""in _connection_checker"") for attempts in range(5): try: return func(self, *args, **kwargs) except exception.VolumeBackendAPIException as e: pattern = re.compile(r"".*Session id expired$"") matches = pattern.match(six.text_type(e)) if matches: <mask>: LOG.debug(""Session might have expired."" "" Trying to relogin"") self._login() continue LOG.error(""Re-throwing Exception %s"", e) raise",False,if attempts < 4 :,if attempts % 10 == 0 :,0.8820016898747209,12.22307556087252
"def set(self, pcount): """"""Set channel prefetch_count setting."""""" if pcount != self.prev: new_value = pcount <mask>: logger.warning( ""QoS: Disabled: prefetch_count exceeds %r"", PREFETCH_COUNT_MAX ) new_value = 0 logger.debug(""basic.qos: prefetch_count->%s"", new_value) self.callback(prefetch_count=new_value) self.prev = pcount return pcount",False,if pcount > PREFETCH_COUNT_MAX :,if new_value > PREFETCH_COUNT_MAX :,0.8820016898747209,59.00468726392806
"def _build_gcs_object_key(self, key): if self.platform_specific_separator: <mask>: gcs_object_key = os.path.join( self.prefix, self._convert_key_to_filepath(key) ) else: gcs_object_key = self._convert_key_to_filepath(key) else: if self.prefix: gcs_object_key = ""/"".join((self.prefix, self._convert_key_to_filepath(key))) else: gcs_object_key = self._convert_key_to_filepath(key) return gcs_object_key",True,if self . prefix :,if self . prefix :,0.8820016898747209,100.00000000000004
"def number_operators(self, a, b, skip=[]): dict = {""a"": a, ""b"": b} for name, expr in self.binops.items(): <mask>: name = ""__%s__"" % name if hasattr(a, name): res = eval(expr, dict) self.binop_test(a, b, res, expr, name) for name, expr in self.unops.items(): if name not in skip: name = ""__%s__"" % name if hasattr(a, name): res = eval(expr, dict) self.unop_test(a, res, expr, name)",True,if name not in skip :,if name not in skip :,0.8820016898747209,100.00000000000004
def isCurveMonotonic(set_): for i in range(len(set_) - 1): # ==== added by zli ======= <mask>: return False # ==== added by zli ======= # ==== added by zli ======= # if set_[i][1] > set_[i + 1][1]: if set_[i][1] >= set_[i + 1][1]: # ==== added by zli ======= return False return True,False,if set_ [ i ] [ 0 ] >= set_ [ i + 1 ] [ 0 ] :,if set_ [ i ] [ 0 ] < set_ [ i + 1 ] [ 0 ] :,0.8820016898747209,82.5349877279405
"def show_topics(): """"""prints all available miscellaneous help topics."""""" print(_stash.text_color(""Miscellaneous Topics:"", ""yellow"")) for pp in PAGEPATHS: <mask>: continue content = os.listdir(pp) for pn in content: if ""."" in pn: name = pn[: pn.index(""."")] else: name = pn print(name)",False,if not os . path . isdir ( pp ) :,"if pp == ""help"" :",0.8820016898747209,4.995138898472386
"def test_send_error(self): allow_transfer_encoding_codes = (205, 304) for code in (101, 102, 204, 205, 304): self.con.request(""SEND_ERROR"", ""/{}"".format(code)) res = self.con.getresponse() self.assertEqual(code, res.status) self.assertEqual(None, res.getheader(""Content-Length"")) self.assertEqual(None, res.getheader(""Content-Type"")) <mask>: self.assertEqual(None, res.getheader(""Transfer-Encoding"")) data = res.read() self.assertEqual(b"""", data)",False,if code not in allow_transfer_encoding_codes :,if code in allow_transfer_encoding_codes :,0.8820016898747209,77.72460244048297
"def _length_hint(obj): """"""Returns the length hint of an object."""""" try: return len(obj) except (AttributeError, TypeError): try: get_hint = type(obj).__length_hint__ except AttributeError: return None try: hint = get_hint(obj) except TypeError: return None <mask>: return None return hint",False,"if hint is NotImplemented or not isinstance ( hint , int_types ) or hint < 0 :",if hint is None :,0.8820016898747209,2.5983349617896914
"def _rmtree(self, path): # Essentially a stripped down version of shutil.rmtree. We can't # use globals because they may be None'ed out at shutdown. for name in self._listdir(path): fullname = self._path_join(path, name) try: isdir = self._isdir(fullname) except self._os_error: isdir = False <mask>: self._rmtree(fullname) else: try: self._remove(fullname) except self._os_error: pass try: self._rmdir(path) except self._os_error: pass",True,if isdir :,if isdir :,0.8820016898747209,0.0
"def get_sources(self, sources=None): """"""Returns all sources from this provider."""""" self._load() if sources is None: sources = list(self.data.keys()) elif not isinstance(sources, (list, tuple)): sources = [sources] for source in sources: <mask>: raise KeyError( ""Invalid data key: {}. Valid keys are: {}"".format( source, "", "".join(str(k) for k in self.data) ) ) return {k: self.data[k] for k in sources}",False,if source not in self . data :,if not self . data . has_key ( source ) :,0.8820016898747209,14.211672443220438
"def do_shorts( opts: List[Tuple[str, str]], optstring: str, shortopts: str, args: List[str] ) -> Tuple[List[Tuple[str, str]], List[str]]: while optstring != """": opt, optstring = optstring[0], optstring[1:] if short_has_arg(opt, shortopts): <mask>: if not args: raise GetoptError(""option -%s requires argument"" % opt, opt) optstring, args = args[0], args[1:] optarg, optstring = optstring, """" else: optarg = """" opts.append((""-"" + opt, optarg)) return opts, args",False,"if optstring == """" :",if opt in shortopts :,0.8820016898747209,8.51528917838043
"def _sanitize_dict(self, config_dict, allow_val_change=None, ignore_keys: set = None): sanitized = {} for k, v in six.iteritems(config_dict): <mask>: continue k, v = self._sanitize(k, v, allow_val_change) sanitized[k] = v return sanitized",False,if ignore_keys and k in ignore_keys :,if k in ignore_keys :,0.8820016898747209,47.486944442513455
def x(data): count = 0 while count < 10: data.start_example(SOME_LABEL) b = data.draw_bits(1) <mask>: count += 1 data.stop_example(discard=not b) data.mark_interesting(),True,if b :,if b :,0.8820016898747209,0.0
"def prompt_for_resume(config): logger = logging.getLogger(""changeme"") logger.error( ""A previous scan was interrupted. Type R to resume or F to start a fresh scan"" ) answer = """" while not (answer == ""R"" or answer == ""F""): prompt = ""(R/F)> "" answer = """" try: answer = raw_input(prompt) except NameError: answer = input(prompt) if answer.upper() == ""F"": logger.debug(""Forcing a fresh scan"") <mask>: logger.debug(""Resuming previous scan"") config.resume = True return config.resume",True,"elif answer . upper ( ) == ""R"" :","elif answer . upper ( ) == ""R"" :",0.8820016898747209,100.00000000000004
"def _evaluate_local_single(self, iterator): for batch in iterator: in_arrays = convert._call_converter(self.converter, batch, self.device) with function.no_backprop_mode(): <mask>: results = self.calc_local(*in_arrays) elif isinstance(in_arrays, dict): results = self.calc_local(**in_arrays) else: results = self.calc_local(in_arrays) if self._progress_hook: self._progress_hook(batch) yield results",False,"if isinstance ( in_arrays , tuple ) :","if isinstance ( in_arrays , list ) :",0.8820016898747209,70.71067811865478
"def _send_until_done(self, data): while True: try: return self.connection.send(data) except OpenSSL.SSL.WantWriteError: <mask>: raise timeout() continue except OpenSSL.SSL.SysCallError as e: raise SocketError(str(e))",False,"if not util . wait_for_write ( self . socket , self . socket . gettimeout ( ) ) :",if self . timeout :,0.8820016898747209,0.6942039088478356
"def _read_jtl_chunk(self, jtl): data = jtl.read(1024 * 1024 * 10) if data: parts = data.rsplit(""\n"", 1) <mask>: ready_chunk = self.buffer + parts[0] + ""\n"" self.buffer = parts[1] df = string_to_df(ready_chunk) self.stat_queue.put(df) return df else: self.buffer += parts[0] else: if self.jmeter_finished: self.agg_finished = True jtl.readline() return None",False,if len ( parts ) > 1 :,if len ( parts ) == 2 :,0.8820016898747209,46.713797772819994
"def __new__(mcl, classname, bases, dictionary): slots = list(dictionary.get(""__slots__"", [])) for getter_name in [key for key in dictionary if key.startswith(""get_"")]: name = getter_name slots.append(""__"" + name) getter = dictionary.pop(getter_name) setter = dictionary.get(setter_name, None) <mask>: del dictionary[setter_name] dictionary[name] = property(getter.setter) dictionary[""__slots__""] = tuple(slots) return super().__new__(mcl, classname, bases, dictionary)",False,"if setter is not None and isinstance ( setter , collections . Callable ) :",if getter is None or setter is None :,0.8820016898747209,6.155947438501932
"def tex_coords(self): """"""Array of texture coordinate data."""""" if ""multi_tex_coords"" not in self.domain.attribute_names: <mask>: domain = self.domain attribute = domain.attribute_names[""tex_coords""] self._tex_coords_cache = attribute.get_region( attribute.buffer, self.start, self.count ) self._tex_coords_cache_version = domain._version region = self._tex_coords_cache region.invalidate() return region.array else: return None",False,if self . _tex_coords_cache_version != self . domain . _version :,"if ""tex_coords"" in self . domain . attribute_names :",0.8820016898747209,18.638974500698723
"def index(self, sub, start=0): """"""Returns the index of the closing bracket"""""" br = ""([{<""["")]}>"".index(sub)] count = 0 for i in range(start, len(self.string)): char = self.string[i] <mask>: count += 1 elif char == sub: if count > 0: count -= 1 else: return i err = ""Closing bracket {!r} missing in string {!r}"".format( sub, """".join(self.original) ) raise ParseError(err)",False,if char == br :,if char in br :,0.8820016898747209,24.736929544091932
"def test_createFile(self): text = ""This is a test!"" path = tempfile.mktemp() try: koDoc = self._koDocFromPath(path, load=False) koDoc.buffer = text koDoc.save(0) del koDoc koDoc2 = self._koDocFromPath(path) assert koDoc2.buffer == text finally: <mask>: os.unlink(path) # clean up",True,if os . path . exists ( path ) :,if os . path . exists ( path ) :,0.8820016898747209,100.00000000000004
"def __editScopeHasEdit(self, attributeHistory): with attributeHistory.context: tweak = GafferScene.EditScopeAlgo.acquireParameterEdit( attributeHistory.scene.node(), attributeHistory.context[""scene:path""], attributeHistory.attributeName, IECoreScene.ShaderNetwork.Parameter("""", self.__parameter), createIfNecessary=False, ) <mask>: return False return tweak[""enabled""].getValue()",True,if tweak is None :,if tweak is None :,0.8820016898747209,100.00000000000004
"def mail_migrator(app, schema_editor): Event_SettingsStore = app.get_model(""pretixbase"", ""Event_SettingsStore"") for ss in Event_SettingsStore.objects.filter( key__in=[ ""mail_text_order_approved"", ""mail_text_order_placed"", ""mail_text_order_placed_require_approval"", ] ): chgd = ss.value.replace(""{date}"", ""{expire_date}"") <mask>: ss.value = chgd ss.save() cache.delete(""hierarkey_{}_{}"".format(""event"", ss.object_id))",False,if chgd != ss . value :,if chgd :,0.8820016898747209,0.0
"def __get_limits(self): dimension = len(self.__tree.get_root().data) nodes = self.__get_all_nodes() max, min = [float(""-inf"")] * dimension, [float(""+inf"")] * dimension for node in nodes: for d in range(dimension): if max[d] < node.data[d]: max[d] = node.data[d] <mask>: min[d] = node.data[d] return min, max",False,if min [ d ] > node . data [ d ] :,elif min [ d ] > node . data [ d ] :,0.8820016898747209,91.21679090703874
"def get_complete_position(self, context: UserContext) -> int: # Check member prefix pattern. for prefix_pattern in convert2list( self.get_filetype_var(context[""filetype""], ""prefix_patterns"") ): m = re.search(self._object_pattern + prefix_pattern + r""\w*$"", context[""input""]) <mask>: continue self._prefix = re.sub(r""\w*$"", """", m.group(0)) m = re.search(r""\w*$"", context[""input""]) if m: return m.start() return -1",False,"if m is None or prefix_pattern == """" :",if not m :,0.8820016898747209,2.215745752614824
"def _stderr_supports_color(): try: if hasattr(sys.stderr, ""isatty"") and sys.stderr.isatty(): if curses: curses.setupterm() if curses.tigetnum(""colors"") > 0: return True <mask>: if sys.stderr is getattr( colorama.initialise, ""wrapped_stderr"", object() ): return True except Exception: # Very broad exception handling because it's always better to # fall back to non-colored logs than to break at startup. pass return False",False,elif colorama :,if colorama :,0.8820016898747209,0.0
"def setLabelColumnWidth(self, panel, width): for child in panel.GetChildren(): <mask>: size = child.GetSize() size[0] = width child.SetBestSize(size)",False,"if isinstance ( child , wx . lib . stattext . GenStaticText ) :","if child . GetType ( ) == ""Label"" :",0.8820016898747209,4.486485776935436
"def update(self, other): if other.M is None: <mask>: self.items.update(other.items) else: for i in other.items: self.add(i) return if self.M is None: self.convert() self.M = array.array(""B"", list(map(max, list(zip(self.M, other.M)))))",False,if self . M is None :,if self . items is not None :,0.8820016898747209,27.054113452696992
"def on_end_epoch(self, state): if self.write_epoch_metrics: <mask>: self.writer.add_text( ""epoch"", ""<h4>Epoch {}</h4>"".format(state[torchbearer.EPOCH]) + self.table_formatter(str(state[torchbearer.METRICS])), 1, ) else: self.writer.add_text( ""epoch"", self.table_formatter(str(state[torchbearer.METRICS])), state[torchbearer.EPOCH], )",False,if self . visdom :,if self . write_epoch_metrics :,0.8820016898747209,19.070828081828378
"def is_listening_for_message(conversation_id: Text, endpoint: EndpointConfig) -> bool: """"""Check if the conversation is in need for a user message."""""" tracker = await retrieve_tracker(endpoint, conversation_id, EventVerbosity.APPLIED) for i, e in enumerate(reversed(tracker.get(""events"", []))): <mask>: return False elif e.get(""event"") == ActionExecuted.type_name: return e.get(""name"") == ACTION_LISTEN_NAME return False",False,"if e . get ( ""event"" ) == UserUttered . type_name :",if i == 0 :,0.8820016898747209,3.086457674499703
"def filter_ports(self, dpid, in_port, nw_id, allow_nw_id_external=None): assert nw_id != self.nw_id_unknown ret = [] for port in self.get_ports(dpid): nw_id_ = port.network_id <mask>: continue if nw_id_ == nw_id: ret.append(port.port_no) elif allow_nw_id_external is not None and nw_id_ == allow_nw_id_external: ret.append(port.port_no) return ret",False,if port . port_no == in_port :,if in_port is not None and in_port == nw_id_ :,0.8820016898747209,11.64394847706997
"def next_month(billing_cycle_anchor: datetime, dt: datetime) -> datetime: estimated_months = round((dt - billing_cycle_anchor).days * 12.0 / 365) for months in range(max(estimated_months - 1, 0), estimated_months + 2): proposed_next_month = add_months(billing_cycle_anchor, months) <mask>: return proposed_next_month raise AssertionError( ""Something wrong in next_month calculation with "" f""billing_cycle_anchor: {billing_cycle_anchor}, dt: {dt}"" )",False,if 20 < ( proposed_next_month - dt ) . days < 40 :,if proposed_next_month :,0.8820016898747209,16.02643071979889
"def wait_complete(self): """"""Wait for futures complete done."""""" for future in concurrent.futures.as_completed(self._futures.keys()): try: error = future.exception() except concurrent.futures.CancelledError: break name = self._futures[future] <mask>: err_msg = 'Extracting ""{0}"", got: {1}'.format(name, error) logger.error(err_msg)",False,if error is not None :,if error :,0.8820016898747209,0.0
"def _accept_with(cls, orm, target): if target is orm.mapper: return mapperlib.Mapper elif isinstance(target, type): if issubclass(target, mapperlib.Mapper): return target else: mapper = _mapper_or_none(target) <mask>: return mapper else: return _MapperEventsHold(target) else: return target",True,if mapper is not None :,if mapper is not None :,0.8820016898747209,100.00000000000004
"def gvariant_args(args: List[Any]) -> str: """"""Convert args into gvariant."""""" gvariant = """" for arg in args: <mask>: gvariant += "" {}"".format(str(arg).lower()) elif isinstance(arg, (int, float)): gvariant += f"" {arg}"" elif isinstance(arg, str): gvariant += f' ""{arg}""' else: gvariant += f"" {arg!s}"" return gvariant.lstrip()",False,"if isinstance ( arg , bool ) :","if isinstance ( arg , str ) :",0.8820016898747209,59.4603557501361
"def _list_cases(suite): for test in suite: if isinstance(test, unittest.TestSuite): _list_cases(test) <mask>: if support.match_test(test): print(test.id())",False,"elif isinstance ( test , unittest . TestCase ) :",if test . id :,0.8820016898747209,5.171845311465849
def get_and_set_all_disambiguation(self): all_disambiguations = [] for page in self.pages: <mask>: all_disambiguations.extend(page.relations.disambiguation_links_norm) if page.relations.disambiguation_links is not None: all_disambiguations.extend(page.relations.disambiguation_links) return set(all_disambiguations),True,if page . relations . disambiguation_links_norm is not None :,if page . relations . disambiguation_links_norm is not None :,0.8820016898747209,100.00000000000004
"def test_decode_invalid(self): testcases = [ (b""xn--w&"", ""strict"", UnicodeError()), (b""xn--w&"", ""ignore"", ""xn-""), ] for puny, errors, expected in testcases: with self.subTest(puny=puny, errors=errors): <mask>: self.assertRaises(UnicodeError, puny.decode, ""punycode"", errors) else: self.assertEqual(puny.decode(""punycode"", errors), expected)",False,"if isinstance ( expected , Exception ) :","if isinstance ( puny , UnicodeError ) :",0.8820016898747209,27.054113452696992
"def find_globs(walker, patterns, matches): for root, dirs, files in walker: for d in dirs: d = join(root, d) for pattern in patterns: for p in Path(d).glob(pattern): matches.add(str(p)) sub_files = set() for p in matches: <mask>: for f in files: sub_files.add(join(root, f)) matches.update(sub_files)",False,if root . startswith ( p ) :,if p . startswith ( root ) :,0.8820016898747209,29.071536848410968
"def parse_stack_trace(self, it, line): """"""Iterate over lines and parse stack traces."""""" events = [] stack_traces = [] while self.stack_trace_re.match(line): event = self.parse_stack_trace_line(line) <mask>: events.append(event) stack_traces.append(line) line = get_next(it) events.reverse() return stack_traces, events, line",True,if event :,if event :,0.8820016898747209,0.0
"def process(self): """"""Do processing necessary, storing result in feature."""""" summation = 0 # count of all histo = self.data[""flat.notes.quarterLengthHistogram""] if not histo: raise NativeFeatureException(""input lacks notes"") maxKey = 0 # max found for any one key for key in histo: # all defined keys should be greater than zero, but just in case if histo[key] > 0: summation += histo[key] <mask>: maxKey = histo[key] self.feature.vector[0] = maxKey / summation",False,if histo [ key ] >= maxKey :,if histo [ key ] > maxKey :,0.8820016898747209,67.5291821812656
"def load_resource(name): """"""return file contents for files within the package root folder"""""" try: <mask>: return sublime.load_resource(""Packages/Markdown Preview/{0}"".format(name)) else: filename = os.path.join( sublime.packages_path(), INSTALLED_DIRECTORY, os.path.normpath(name) ) return load_utf8(filename) except: print(""Error while load_resource('%s')"" % name) traceback.print_exc() return """"",False,if is_ST3 ( ) :,"if name . endswith ( "".md"" ) :",0.8820016898747209,8.913765521398126
"def get_password(self, service, repo_url): if self.is_unlocked: asyncio.set_event_loop(asyncio.new_event_loop()) collection = secretstorage.get_default_collection(self.connection) attributes = {""application"": ""Vorta"", ""service"": service, ""repo_url"": repo_url} items = list(collection.search_items(attributes)) logger.debug(""Found %i passwords matching repo URL."", len(items)) <mask>: return items[0].get_secret().decode(""utf-8"") return None",False,if len ( items ) > 0 :,if len ( items ) == 1 :,0.8820016898747209,46.713797772819994
"def get_files(d): res = [] for p in glob.glob(os.path.join(d, ""*"")): if not p: continue (pth, fname) = os.path.split(p) if fname == ""output"": continue if fname == ""PureMVC_Python_1_0"": continue if fname[-4:] == "".pyc"": # ehmm.. no. continue <mask>: get_dir(p) else: res.append(p) return res",False,if os . path . isdir ( p ) :,"if fname == ""PureMVC_Python_1_0"" :",0.8820016898747209,3.377156414337854
"def test_nic_names(self): p = subprocess.Popen([""ipconfig"", ""/all""], stdout=subprocess.PIPE) out = p.communicate()[0] if PY3: out = str(out, sys.stdout.encoding) nics = psutil.net_io_counters(pernic=True).keys() for nic in nics: <mask>: continue if nic not in out: self.fail(""%r nic wasn't found in 'ipconfig /all' output"" % nic)",False,"if ""pseudo-interface"" in nic . replace ( "" "" , ""-"" ) . lower ( ) :",if nic in out :,0.8820016898747209,0.6157896012113694
"def vexop_to_simop(op, extended=True, fp=True): res = operations.get(op) if res is None and extended: attrs = op_attrs(op) <mask>: raise UnsupportedIROpError(""Operation not implemented"") res = SimIROp(op, **attrs) if res is None: raise UnsupportedIROpError(""Operation not implemented"") if res._float and not fp: raise UnsupportedIROpError(""Floating point support disabled"") return res",True,if attrs is None :,if attrs is None :,0.8820016898747209,100.00000000000004
"def rule_builder_add_value(self, value, screenshot_name=None): rule_builder = self.components.rule_builder rule_builder.menu_button_column.wait_for_and_click() with self.rule_builder_rule_editor(""add-column-value"") as editor_element: filter_input = editor_element.find_element_by_css_selector(""input[type='text']"") filter_input.clear() filter_input.send_keys(value) <mask>: self.screenshot(screenshot_name)",True,if screenshot_name :,if screenshot_name :,0.8820016898747209,100.00000000000004
"def make_open_socket(self): s = socket.socket() try: s.bind(DEFAULT_BIND_ADDR_TUPLE) <mask>: # Windows and linux (with psutil) doesn't show as open until # we call listen (linux with lsof accepts either) s.listen(1) self.assert_open(s, s.fileno()) except: s.close() s = None raise return s",False,if WIN or greentest . LINUX :,"if sys . platform == ""win32"" :",0.8820016898747209,5.522397783539471
"def handle_ray_task_error(e): for s in e.traceback_str.split(""\n"")[::-1]: <mask>: try: raise getattr(builtins, s.split("":"")[0])("""".join(s.split("":"")[1:])) except AttributeError as att_err: if ""module"" in str(att_err) and builtins.__name__ in str(att_err): pass else: raise att_err raise e",False,"if ""Error"" in s or ""Exception"" in s :","if s . startswith ( ""Error"" ) :",0.8820016898747209,13.76074141597786
"def compare_multiple_events(i, expected_results, actual_results): events_in_a_row = [] j = i while j < len(expected_results) and isinstance( actual_results[j], actual_results[i].__class__ ): events_in_a_row.append(actual_results[j]) j += 1 message = """" for event in events_in_a_row: for k in range(i, j): passed, message = compare_events(expected_results[k], event) <mask>: expected_results[k] = None break else: return i, False, message return j, True, """"",True,if passed :,if passed :,0.8820016898747209,0.0
"def ListSubscriptions(self, params): queryreturn = sqlQuery(""""""SELECT label, address, enabled FROM subscriptions"""""") data = '{""subscriptions"":[' for row in queryreturn: label, address, enabled = row label = shared.fixPotentiallyInvalidUTF8Data(label) <mask>: data += "","" data += json.dumps( { ""label"": label.encode(""base64""), ""address"": address, ""enabled"": enabled == 1, }, indent=4, separators=("","", "": ""), ) data += ""]}"" return data",False,if len ( data ) > 20 :,if len ( data ) > 0 :,0.8820016898747209,70.71067811865478
"def compile(self, args): compiled_args = {} for key, value in six.iteritems(args): <mask>: compiled_args[key] = str(value) else: compiled_args[key] = sjson_dumps(value) return self._minified_code % compiled_args",False,if key in self . clean_args :,"if isinstance ( value , six . string_types ) :",0.8820016898747209,4.789232204309912
"def insert(self, pack_id, data): if (pack_id not in self.queue) and pack_id > self.begin_id: self.queue[pack_id] = PacketInfo(data) if self.end_id == pack_id: self.end_id = pack_id + 1 <mask>: eid = self.end_id while eid < pack_id: self.miss_queue.add(eid) eid += 1 self.end_id = pack_id + 1 else: self.miss_queue.remove(pack_id)",False,elif self . end_id < pack_id :,elif self . end_id < self . begin_id :,0.8820016898747209,57.83569866465144
"def _target_generator(self): # since we do not have predictions yet, so we ignore sampling here if self._internal_target_generator is None: <mask>: return None from ....model_zoo.ssd.target import SSDTargetGenerator self._internal_target_generator = SSDTargetGenerator( iou_thresh=self._iou_thresh, stds=self._box_norm, negative_mining_ratio=-1, **self._kwargs ) return self._internal_target_generator else: return self._internal_target_generator",False,if self . _anchors_none :,if self . _box_norm is None :,0.8820016898747209,29.071536848410968
"def test_heapsort(self): # Exercise everything with repeated heapsort checks for trial in range(100): size = random.randrange(50) data = [random.randrange(25) for i in range(size)] <mask>: # Half of the time, use heapify heap = data[:] self.module.heapify(heap) else: # The rest of the time, use heappush heap = [] for item in data: self.module.heappush(heap, item) heap_sorted = [self.module.heappop(heap) for i in range(size)] self.assertEqual(heap_sorted, sorted(data))",False,if trial & 1 :,if trial == 0 :,0.8820016898747209,17.965205598154213
"def wait(self, timeout=None): if self.returncode is None: if timeout is None: msecs = _subprocess.INFINITE else: msecs = max(0, int(timeout * 1000 + 0.5)) res = _subprocess.WaitForSingleObject(int(self._handle), msecs) <mask>: code = _subprocess.GetExitCodeProcess(self._handle) if code == TERMINATE: code = -signal.SIGTERM self.returncode = code return self.returncode",False,if res == _subprocess . WAIT_OBJECT_0 :,if res == 0 :,0.8820016898747209,18.817320787862926
"def _on_change(self): changed = False self.save() for key, value in self.data.items(): if isinstance(value, bool): if value: changed = True break if isinstance(value, int): if value != 1: changed = True break <mask>: continue elif len(value) != 0: changed = True break self._reset_button.disabled = not changed",False,elif value is None :,if value == 0 :,0.8820016898747209,9.652434877402245
"def isnotsurplus(self, item: T) -> bool: if not self.matchers: <mask>: self.mismatch_description.append_text( ""not matched: "" ).append_description_of(item) return False return True",True,if self . mismatch_description :,if self . mismatch_description :,0.8820016898747209,100.00000000000004
"def resolve_env_secrets(config, environ): """"""Create copy that recursively replaces {""$env"": ""NAME""} with values from environ"""""" if isinstance(config, dict): <mask>: return environ.get(list(config.values())[0]) elif list(config.keys()) == [""$file""]: return open(list(config.values())[0]).read() else: return { key: resolve_env_secrets(value, environ) for key, value in config.items() } elif isinstance(config, list): return [resolve_env_secrets(value, environ) for value in config] else: return config",True,"if list ( config . keys ( ) ) == [ ""$env"" ] :","if list ( config . keys ( ) ) == [ ""$env"" ] :",0.8820016898747209,100.00000000000004
"def __open__(filename, *args, **kwargs): if os.path.isfile(filename): return __realopen__(filename, *args, **kwargs) if not os.path.isabs(filename): datafilename = __papplet__.dataPath(filename) <mask>: return __realopen__(datafilename, *args, **kwargs) sketchfilename = __papplet__.sketchPath(filename) if os.path.isfile(sketchfilename): return __realopen__(sketchfilename, *args, **kwargs) # Fail naturally return __realopen__(filename, *args, **kwargs)",True,if os . path . isfile ( datafilename ) :,if os . path . isfile ( datafilename ) :,0.8820016898747209,100.00000000000004
def run(self): while not self.completed: <mask>: time.sleep(self.period) else: self._completed.wait(self.period) self.counter += 1 try: self.callback(self.counter) except Exception: self.stop() if self.timeout is not None: dt = time.time() - self._start_time if dt > self.timeout: self.stop() if self.counter == self.count: self.stop(),False,if self . block :,if self . counter == self . count :,0.8820016898747209,16.784459625186194
"def remove(self, path, config=None, error_on_path=False, defaults=None): if not path: <mask>: raise NoSuchSettingsPath() return if config is not None or defaults is not None: if config is None: config = self._config if defaults is None: defaults = dict(self._map.parents) chain = HierarchicalChainMap(config, defaults) else: chain = self._map try: chain.del_by_path(path) self._mark_dirty() except KeyError: if error_on_path: raise NoSuchSettingsPath() pass",True,if error_on_path :,if error_on_path :,0.8820016898747209,100.00000000000004
"def structured_dot_grad(sparse_A, dense_B, ga): if sparse_A.type.format in (""csc"", ""csr""): <mask>: sdgcsx = sdg_csc CSx = CSC else: sdgcsx = sdg_csr CSx = CSR g_A_data = sdgcsx(csm_indices(sparse_A), csm_indptr(sparse_A), dense_B, ga) return CSx( g_A_data, csm_indices(sparse_A), csm_indptr(sparse_A), csm_shape(sparse_A) ) else: raise NotImplementedError()",True,"if sparse_A . type . format == ""csc"" :","if sparse_A . type . format == ""csc"" :",0.8820016898747209,100.00000000000004
"def step_async(self, actions): listify = True try: <mask>: listify = False except TypeError: pass if not listify: self.actions = actions else: assert ( self.num_envs == 1 ), f""actions {actions} is either not a list or has a wrong size - cannot match to {self.num_envs} environments"" self.actions = [actions]",False,if len ( actions ) == self . num_envs :,if actions is not None :,0.8820016898747209,3.3264637832151163
"def tempFailureRetry(func, *args, **kwargs): while True: try: return func(*args, **kwargs) except (os.error, IOError) as ex: <mask>: continue else: raise",True,if ex . errno == errno . EINTR :,if ex . errno == errno . EINTR :,0.8820016898747209,100.00000000000004
"def test_learning_always_changes_generation(chars, order): learner = LStar(lambda s: len(s) == 1 and s[0] in chars) for c in order: prev = learner.generation s = bytes([c]) <mask>: learner.learn(s) assert learner.generation > prev",False,if learner . dfa . matches ( s ) != learner . member ( s ) :,if len ( s ) > 0 :,0.8820016898747209,6.691863570734902
"def test_costs_5D_noisy_names(signal_bkps_5D_noisy, cost_name): signal, bkps = signal_bkps_5D_noisy cost = cost_factory(cost_name) cost.fit(signal) cost.error(0, 100) cost.error(100, signal.shape[0]) cost.error(10, 50) cost.sum_of_costs(bkps) with pytest.raises(NotEnoughPoints): <mask>: cost.min_size = 4 cost.error(1, 2) else: cost.error(1, 2)",False,"if cost_name == ""cosine"" :",if cost . min_size is None :,0.8820016898747209,10.147104008451905
"def remove_empty_dirs(dirname): logger.debug(""remove_empty_dirs '%s'"" % (dirname)) try: <mask>: dirname = dirname.encode(""utf-8"") os.removedirs(dirname) logger.debug(""remove_empty_dirs '%s' done"" % (dirname)) except OSError as exc: # Python >2.5 if exc.errno == errno.ENOTEMPTY: logger.debug(""remove_empty_dirs '%s' not empty"" % (dirname)) pass else: raise except Exception as e: logger.exception(e) logger.error(""remove_empty_dirs exception: "" + dirname) raise e",False,"if not isinstance ( dirname , str ) :",if not os . path . exists ( dirname ) :,0.8820016898747209,12.982679446701692
"def get_unique_attribute(self, name: str): feat = None for f in self.features: <mask>: if feat is not None: raise RuntimeError(""The attribute was not unique."") feat = f if feat is None: raise RuntimeError(""The attribute did not exist"") return getattr(feat, name)",False,"if self . _return_feature ( f ) and hasattr ( f , name ) :",if f . name == name :,0.8820016898747209,2.365931054820936
"def get_allocated_address( self, config: ActorPoolConfig, allocated: allocated_type ) -> str: addresses = config.get_external_addresses(label=self.label) for addr in addresses: occupied = False for strategy, _ in allocated.get(addr, dict()).values(): if strategy == self: occupied = True break <mask>: return addr raise NoIdleSlot( f""No idle slot for creating actor "" f""with label {self.label}, mark {self.mark}"" )",False,if not occupied :,if occupied :,0.8820016898747209,0.0
"def __deepcopy__(self, memo): cls = self.__class__ result = cls.__new__(cls) memo[id(self)] = result for key, value in self.__dict__.items(): <mask>: setattr(result, key, copy.copy(value)) else: setattr(result, key, copy.deepcopy(value, memo)) return result",False,if key in cls . dynamic_methods :,"if isinstance ( value , dict ) :",0.8820016898747209,5.795599612995366
def restore_forward(model): for child in model.children(): # leaf node <mask>: child.forward = child.old_forward child.old_forward = None else: restore_forward(child),False,"if is_leaf ( child ) and hasattr ( child , ""old_forward"" ) :","if isinstance ( child , Node ) :",0.8820016898747209,6.840351110235662
"def add(self, obj, allow_duplicates=False): if allow_duplicates or obj not in self._constants: self._constant_pool.append(obj) self._constants[obj] = len(self) <mask>: self._constant_pool.append(None)",False,"if obj . __class__ in ( Double , Long ) :",if obj in self . _constant_pool :,0.8820016898747209,8.27951003977077
"def find_file_copyright_notices(fname): ret = set() f = open(fname) lines = f.readlines() for l in lines[:80]: # hmmm, assume copyright to be in first 80 lines idx = l.lower().find(""copyright"") if idx < 0: continue copyright = l[idx + 9 :].strip() if not copyright: continue copyright = sanitise(copyright) # hmm, do a quick check to see if there's a year, # if not, skip it <mask>: continue ret.add(copyright) return ret",False,"if not copyright . find ( ""200"" ) >= 0 and not copyright . find ( ""199"" ) >= 0 :","if copyright . startswith ( ""hmmm"" ) :",0.8820016898747209,2.853461783028348
"def callback(lexer, match, context): text = match.group() extra = """" if start: context.next_indent = len(text) if context.next_indent < context.indent: while context.next_indent < context.indent: context.indent = context.indent_stack.pop() <mask>: extra = text[context.indent :] text = text[: context.indent] else: context.next_indent += len(text) if text: yield match.start(), TokenClass, text if extra: yield match.start() + len(text), TokenClass.Error, extra context.pos = match.end()",False,if context . next_indent > context . indent :,if context . indent :,0.8820016898747209,25.32731330921383
"def queries(self): if DEV: cmd = ShellCommand(""docker"", ""ps"", ""-qf"", ""name=%s"" % self.path.k8s) if not cmd.check(f""docker check for {self.path.k8s}""): if not cmd.stdout.strip(): log_cmd = ShellCommand( ""docker"", ""logs"", self.path.k8s, stderr=subprocess.STDOUT ) <mask>: print(cmd.stdout) pytest.exit(f""container failed to start for {self.path.k8s}"") return ()",False,"if log_cmd . check ( f""docker logs for {self.path.k8s}"" ) :",if not log_cmd . check ( ) :,0.8820016898747209,18.035689048530983
"def nodes(self): if not self._nodes: nodes = self.cluster_group.instances() self._nodes = [] master = self.master_node nodeid = 1 for node in nodes: if node.state not in [""pending"", ""running""]: continue <mask>: self._nodes.insert(0, master) continue self._nodes.append(Node(node, self.key_location, ""node%.3d"" % nodeid)) nodeid += 1 else: for node in self._nodes: log.debug(""refreshing instance %s"" % node.id) node.update() return self._nodes",False,if node . id == master . id :,if nodeid == master . id :,0.8820016898747209,55.0695314903184
"def match(cls, agent_name, guid, uri, media=None): # Retrieve `Agent` for provided `guid` agent = Agents.get(agent_name) if agent is None: <mask>: # First occurrence of unsupported agent log.warn(""Unsupported metadata agent: %s"" % agent_name) # Mark unsupported agent as ""seen"" unsupported_agents[agent_name] = True return False # Duplicate occurrence of unsupported agent log.warn( ""Unsupported metadata agent: %s"" % agent_name, extra={""duplicate"": True} ) return False # Fill `guid` with details from agent return agent.fill(guid, uri, media)",True,if agent_name not in unsupported_agents :,if agent_name not in unsupported_agents :,0.8820016898747209,100.00000000000004
"def __createRandom(plug): node = plug.node() parentNode = node.ancestor(Gaffer.Node) with Gaffer.UndoScope(node.scriptNode()): randomNode = Gaffer.Random() parentNode.addChild(randomNode) if isinstance(plug, (Gaffer.FloatPlug, Gaffer.IntPlug)): plug.setInput(randomNode[""outFloat""]) <mask>: plug.setInput(randomNode[""outColor""]) GafferUI.NodeEditor.acquire(randomNode)",False,"elif isinstance ( plug , Gaffer . Color3fPlug ) :","elif isinstance ( plug , Gaffer . ColorPlug ) :",0.8820016898747209,70.71067811865478
"def post_arrow(self, arr: pa.Table, graph_type: str, opts: str = """"): dataset_id = self.dataset_id tok = self.token sub_path = f""api/v2/upload/datasets/{dataset_id}/{graph_type}/arrow"" try: resp = self.post_arrow_generic(sub_path, tok, arr, opts) out = resp.json() <mask>: raise Exception(""No success indicator in server response"") return out except Exception as e: logger.error(""Failed to post arrow to %s"", sub_path, exc_info=True) raise e",False,"if not ( ""success"" in out ) or not out [ ""success"" ] :",if not out :,0.8820016898747209,1.3643208082090863
"def dict_to_XML(tag, dictionary, **kwargs): """"""Return XML element converting dicts recursively."""""" elem = Element(tag, **kwargs) for key, val in dictionary.items(): <mask>: child = dict_to_XML(""layer"", val, name=key) elif isinstance(val, MutableMapping): child = dict_to_XML(key, val) else: if tag == ""config"": child = Element(""variable"", name=key) else: child = Element(key) child.text = str(val) elem.append(child) return elem",False,"if tag == ""layers"" :","if isinstance ( val , MutableLayer ) :",0.8820016898747209,6.567274736060395
"def apply_incpaths_ml(self): inc_lst = self.includes.split() lst = self.incpaths_lst for dir in inc_lst: node = self.path.find_dir(dir) <mask>: error(""node not found: "" + str(dir)) continue if not node in lst: lst.append(node) self.bld_incpaths_lst.append(node)",False,if not node :,if node is None :,0.8820016898747209,14.058533129758727
"def _table_reprfunc(self, row, col, val): if self._table.column_names[col].endswith(""Size""): if isinstance(val, compat.string_types): return "" %s"" % val elif val < 1024 ** 2: return "" %.1f KB"" % (val / 1024.0 ** 1) <mask>: return "" %.1f MB"" % (val / 1024.0 ** 2) else: return "" %.1f GB"" % (val / 1024.0 ** 3) if col in (0, """"): return str(val) else: return "" %s"" % val",True,elif val < 1024 ** 3 :,elif val < 1024 ** 3 :,0.8820016898747209,100.00000000000004
"def _cache_mem(curr_out, prev_mem, mem_len, reuse_len=None): """"""cache hidden states into memory."""""" if mem_len is None or mem_len == 0: return None else: <mask>: curr_out = curr_out[:reuse_len] if prev_mem is None: new_mem = curr_out[-mem_len:] else: new_mem = tf.concat([prev_mem, curr_out], 0)[-mem_len:] new_mem.stop_gradient = True return new_mem",False,if reuse_len is not None and reuse_len > 0 :,if reuse_len is not None :,0.8820016898747209,41.06951993704473
"def GROUP_CONCAT(builder, distinct, expr, sep=None): assert distinct in (None, True, False) result = distinct and ""GROUP_CONCAT(DISTINCT "" or ""GROUP_CONCAT("", builder(expr) if sep is not None: <mask>: result = result, "" SEPARATOR "", builder(sep) else: result = result, "", "", builder(sep) return result, "")""",False,"if builder . provider . dialect == ""MySQL"" :",if sep . isspace ( ) :,0.8820016898747209,4.231118166423695
"def __init__(self, *args, **kwargs): super().__init__(*args, **kwargs) self.custom_fields = [] self.obj_type = ContentType.objects.get_for_model(self.model) # Add all applicable CustomFields to the form custom_fields = CustomField.objects.filter(content_types=self.obj_type) for cf in custom_fields: # Annotate non-required custom fields as nullable <mask>: self.nullable_fields.append(cf.name) self.fields[cf.name] = cf.to_form_field( set_initial=False, enforce_required=False ) # Annotate this as a custom field self.custom_fields.append(cf.name)",False,if not cf . required :,if cf . required :,0.8820016898747209,57.89300674674101
"def is_child_of(self, item_hash, possible_child_hash): if self.get_last(item_hash) != self.get_last(possible_child_hash): return None while True: <mask>: return True if possible_child_hash not in self.items: return False possible_child_hash = self.items[possible_child_hash].previous_hash",False,if possible_child_hash == item_hash :,if self . items [ item_hash ] . previous_hash == possible_child_hash :,0.8820016898747209,35.448014399611814
"def validate(self): self.assertEqual(len(self.inputs), len(self.outputs)) for batch_in, batch_out in zip(self.inputs, self.outputs): self.assertEqual(len(batch_in), len(batch_out)) <mask>: self.validate_unordered_batch(batch_in, batch_out) else: for in_data, out_data in zip(batch_in, batch_out): self.assertEqual(in_data.shape, out_data.shape) if not self.use_parallel_executor: self.assertTrue((in_data == out_data).all())",False,if self . use_parallel_executor and not self . use_double_buffer :,if self . use_unordered_batch :,0.8820016898747209,17.860244166902365
"def add_cells(self, cells): for cell in cells: <mask>: id = len(self.cell_id_map) self.cell_id_map[cell] = id self.id_cell_map[id] = cell",True,if cell not in self . cell_id_map :,if cell not in self . cell_id_map :,0.8820016898747209,100.00000000000004
"def _verify_out(marker="">>""): if shared: self.assertIn(""libapp_lib.dylib"", self.client.out) else: <mask>: self.assertIn(""libapp_lib.a"", self.client.out) else: # Incremental build not the same msg self.assertIn(""Built target app_lib"", self.client.out) out = str(self.client.out).splitlines() for k, v in vals.items(): self.assertIn(""%s %s: %s"" % (marker, k, v), out)",False,"if marker == "">>"" :",if shared :,0.8820016898747209,0.0
"def Visit_expr(self, node): # pylint: disable=invalid-name # expr ::= xor_expr ('|' xor_expr)* for child in node.children: self.Visit(child) <mask>: _AppendTokenSubtype(child, format_token.Subtype.BINARY_OPERATOR)",False,"if isinstance ( child , pytree . Leaf ) and child . value == ""|"" :","if isinstance ( child , ast . Expr ) :",0.8820016898747209,17.96191510244705
"def fill_members(self): if self._get_retrieve(): after = self.after.id if self.after else None data = await self.get_members(self.guild.id, self.retrieve, after) if not data: # no data, terminate return <mask>: self.limit = 0 # terminate loop self.after = Object(id=int(data[-1][""user""][""id""])) for element in reversed(data): await self.members.put(self.create_member(element))",False,if len ( data ) < 1000 :,if len ( data ) > self . limit :,0.8820016898747209,41.11336169005198
"def assert_warns(expected): with warnings.catch_warnings(record=True) as w: warnings.simplefilter(""always"") yield # Python 2 does not raise warnings multiple times from the same stack # frame. if sys.version_info >= (3, 0): <mask>: try: exc_name = expected.__name__ except AttributeError: exc_name = str(expected) raise AssertionError(""%s not triggerred"" % exc_name)",False,"if not any ( isinstance ( m . message , expected ) for m in w ) :",if expected is not None :,0.8820016898747209,1.5534791020152603
"def __init__(self, measures): """"""Constructs a ContingencyMeasures given a NgramAssocMeasures class"""""" self.__class__.__name__ = ""Contingency"" + measures.__class__.__name__ for k in dir(measures): <mask>: continue v = getattr(measures, k) if not k.startswith(""_""): v = self._make_contingency_fn(measures, v) setattr(self, k, v)",False,"if k . startswith ( ""__"" ) :","if k . startswith ( ""_"" ) :",0.8820016898747209,80.45268749630647
"def _omit_keywords(self, context): omitted_kws = 0 for event, elem in context: # Teardowns aren't omitted to allow checking suite teardown status. omit = elem.tag == ""kw"" and elem.get(""type"") != ""teardown"" start = event == ""start"" <mask>: omitted_kws += 1 if not omitted_kws: yield event, elem elif not start: elem.clear() if omit and not start: omitted_kws -= 1",True,if omit and start :,if omit and start :,0.8820016898747209,100.00000000000004
"def read_block(buffer, i): offset = i * BLOCK_LENGTH % config.CAPTURE_BUFFER while True: if buffer[offset] == BLOCK_MARKER.END: return None while buffer[offset] == BLOCK_MARKER.WRITE: time.sleep(SHORT_SENSOR_SLEEP_TIME) buffer[offset] = BLOCK_MARKER.READ buffer.seek(offset + 1) length = struct.unpack(""=H"", buffer.read(2))[0] retval = buffer.read(length) <mask>: break buffer[offset] = BLOCK_MARKER.NOP return retval",False,if buffer [ offset ] == BLOCK_MARKER . READ :,if retval == BLOCK_MARKER . NOP :,0.8820016898747209,41.15421581016571
def _start(self): try: instance_info = self._get_instance_info() <mask>: self._multipass_cmd.start(instance_name=self.instance_name) except errors.ProviderInfoError as instance_error: # Until we have proper multipass error codes to know if this # was a communication error we should keep this error tracking # and generation here. raise errors.ProviderInstanceNotFoundError( instance_name=self.instance_name ) from instance_error,False,if not instance_info . is_running ( ) :,if instance_info :,0.8820016898747209,11.141275535087015
"def _river_driver(self): if self._cached_river_driver: return self._cached_river_driver else: <mask>: self._cached_river_driver = MsSqlDriver( self.workflow, self.wokflow_object_class, self.field_name ) else: self._cached_river_driver = OrmDriver( self.workflow, self.wokflow_object_class, self.field_name ) return self._cached_river_driver",False,if app_config . IS_MSSQL :,"if self . query_type == ""sql"" :",0.8820016898747209,4.789232204309912
"def __LazyMap__(self, attr): try: <mask>: debug_attr_print( ""%s.__LazyMap__(%s) added something"" % (self._username_, attr) ) return 1 except AttributeError: return 0",False,if self . _LazyAddAttr_ ( attr ) :,if attr in self . _lazy_map :,0.8820016898747209,19.304869754804493
"def prepare(self, data=None, user=None): """"""Prepare activation for execution."""""" super(ManagedStartViewActivation, self).prepare.original() self.task.owner = user management_form_class = self.get_management_form_class() self.management_form = management_form_class(data=data, instance=self.task) if data: <mask>: raise FlowRuntimeError( ""Activation metadata is broken {}"".format(self.management_form.errors) ) self.task = self.management_form.save(commit=False)",False,if not self . management_form . is_valid ( ) :,if self . management_form . errors :,0.8820016898747209,36.21513850221006
"def PreprocessConditionalStatement(self, IfList, ReplacedLine): while self: if self.__Token: x = 1 elif not IfList: if self <= 2: continue RegionSizeGuid = 3 <mask>: RegionLayoutLine = 5 continue RegionLayoutLine = self.CurrentLineNumber return 1",False,if not RegionSizeGuid :,if self <= 3 :,0.8820016898747209,9.652434877402245
"def _get_completion(self, document): try: completion_header = document.xpath(""//div[@id='complete_day']"")[0] completion_message = completion_header.getchildren()[0] <mask>: return False elif ""day_complete_message"" in completion_message.classes: return True except IndexError: return False # Who knows, probably not my diary.",False,"if ""day_incomplete_message"" in completion_message . classes :","if ""day_complete_header"" in completion_header . classes :",0.8820016898747209,41.374412020518825
"def run(self): DISPATCH_SYNC = components.interfaces.nsIEventTarget.DISPATCH_SYNC try: <mask>: return for match in findlib2.find_all_matches(self.regex, self.text): if self._stopped: return self.target.dispatch(lambda: self.callback(match), DISPATCH_SYNC) if self._stopped: return self.target.dispatch(lambda: self.callback(None), DISPATCH_SYNC) finally: self.callback = None self.target = None",False,if self . _stopped :,if self . callback is None :,0.8820016898747209,26.269098944241588
"def to_key(literal_or_identifier): """"""returns string representation of this object"""""" if literal_or_identifier[""type""] == ""Identifier"": return literal_or_identifier[""name""] elif literal_or_identifier[""type""] == ""Literal"": k = literal_or_identifier[""value""] if isinstance(k, float): return unicode(float_repr(k)) <mask>: return compose_regex(k) elif isinstance(k, bool): return ""true"" if k else ""false"" elif k is None: return ""null"" else: return unicode(k)",False,"elif ""regex"" in literal_or_identifier :","elif isinstance ( k , str ) :",0.8820016898747209,4.513617516969122
"def process_image_pre_creation(sender, instance: Image, **kwargs): # FIXME(winkidney): May have issue on determining if it # is created or not if instance.pk is not None: return for plugin in _plugin_instances: process_fn = getattr(plugin, ""process_image_pre_creation"", None) <mask>: continue try: process_fn( django_settings=settings, image_instance=instance, ) except Exception: logging.exception( ""Error occurs while trying to access plugin's pin_pre_save "" ""for plugin %s"" % plugin )",True,if process_fn is None :,if process_fn is None :,0.8820016898747209,100.00000000000004
"def check_screenshots(self): # If we arrive here, there have not been any failures yet if self.interactive: self._commit_screenshots() else: <mask>: self._validate_screenshots() # Always commit the screenshots here. They can be used for the next test run. # If reference screenshots were already present and there was a mismatch, it should # have failed above. self._commit_screenshots() elif self.allow_missing_screenshots: warnings.warn(""No committed reference screenshots available. Ignoring."") else: self.fail( ""No committed reference screenshots available. Run interactive first."" )",False,if self . _has_reference_screenshots ( ) :,if self . allow_missing_screenshots :,0.8820016898747209,17.39350277271197
"def on_task_abort(self, task, config): if ""abort"" in config: <mask>: return log.debug(""sending abort notification"") self.send_notification( config[""abort""][""title""], config[""abort""][""message""], config[""abort""][""via""], template_renderer=task.render, )",False,if task . silent_abort :,"if config [ ""abort"" ] [ ""title"" ] == ""abort"" :",0.8820016898747209,2.8265205879007453
"def block_users(self, user_ids): broken_items = [] self.logger.info(""Going to block %d users."" % len(user_ids)) for user_id in tqdm(user_ids): <mask>: self.error_delay() broken_items = user_ids[user_ids.index(user_id) :] break self.logger.info(""DONE: Total blocked %d users."" % self.total[""blocks""]) return broken_items",False,if not self . block ( user_id ) :,"if user_id in self . total [ ""blocks"" ] :",0.8820016898747209,14.458924666162856
"def find_widget_by_id(self, id, parent=None): """"""Recursively searches for widget with specified ID"""""" if parent == None: if id in self: return self[id] # Do things fast if possible parent = self[""editor""] for c in parent.get_children(): if hasattr(c, ""get_id""): if c.get_id() == id: return c if isinstance(c, Gtk.Container): r = self.find_widget_by_id(id, c) <mask>: return r return None",False,if not r is None :,if r :,0.8820016898747209,0.0
"def addClasses(self, name): # Result: void - None # In: name: string for n in name.split(): try: k, method = n.split(""."") except ValueError: k = n method = None self.classes[k] = 1 <mask>: self.methods.setdefault(k, {})[method] = 1",False,if method is not None :,if method :,0.8820016898747209,0.0
"def Read(self, lex_mode): while True: t = self._Read(lex_mode) self.was_line_cont = t.id == Id.Ignored_LineCont # TODO: Change to ALL IGNORED types, once you have SPACE_TOK. This means # we don't have to handle them in the VS_1/VS_2/etc. states. <mask>: break # log('Read() Returning %s', t) return t",False,if t . id != Id . Ignored_LineCont :,if self . was_line_cont :,0.8820016898747209,4.8312524369621626
"def _dir_guildfile(dir, ctx): from guild import guildfile try: return guildfile.for_dir(dir) except guildfile.NoModels: <mask>: help_suffix = "" or '%s' for help"" % click_util.cmd_help(ctx) else: help_suffix = """" cli.error( ""%s does not contain a Guild file (guild.yml)\n"" ""Try specifying a project path or package name%s."" % (cwd_desc(dir), help_suffix) ) except guildfile.GuildfileError as e: cli.error(str(e))",False,if ctx :,"if ctx . get_option ( ""help"" ) :",0.8820016898747209,7.495553473355845
"def check_response(self, response): """"""Specialized version of check_response()."""""" for line in response: # Skip blank lines: if not line.strip(): continue <mask>: return elif line.startswith(b""Benutzer/Passwort Fehler""): raise BadLogin(line) else: raise FailedPost(""Server returned '%s'"" % six.ensure_text(line))",False,"if line . startswith ( b""OK"" ) :","if line . startswith ( b""Benutzer/Passwort Fehler"" ) :",0.8820016898747209,53.16967153331756
"def ParseResponses( self, knowledge_base: rdf_client.KnowledgeBase, responses: Iterable[rdfvalue.RDFValue], ) -> Iterator[rdf_client.User]: for response in responses: if not isinstance(response, rdf_client_fs.StatEntry): raise TypeError(f""Unexpected response type: `{type(response)}`"") # TODO: `st_mode` has to be an `int`, not `StatMode`. <mask>: homedir = response.pathspec.path username = os.path.basename(homedir) if username not in self._ignore_users: yield rdf_client.User(username=username, homedir=homedir)",False,if stat . S_ISDIR ( int ( response . st_mode ) ) :,if response . st_mode == rdf_client_fs . StatMode . ST_MODE_ALL :,0.8820016898747209,18.72867462785877
"def __call__(self, x, uttid=None): if self.utt2spk is not None: spk = self.utt2spk[uttid] else: spk = uttid if not self.reverse: <mask>: x = np.add(x, self.bias[spk]) if self.norm_vars: x = np.multiply(x, self.scale[spk]) else: if self.norm_vars: x = np.divide(x, self.scale[spk]) if self.norm_means: x = np.subtract(x, self.bias[spk]) return x",True,if self . norm_means :,if self . norm_means :,0.8820016898747209,100.00000000000004
"def hasFixtures(self, ctx_callback=None): context = self.context if context is None: return False if self.implementsAnyFixture(context, ctx_callback=ctx_callback): return True # My context doesn't have any, but its ancestors might factory = self.factory if factory: ancestors = factory.context.get(self, []) for ancestor in ancestors: <mask>: return True return False",False,"if self . implementsAnyFixture ( ancestor , ctx_callback = ctx_callback ) :",if ancestor . hasFixtures ( ctx_callback = ctx_callback ) :,0.8820016898747209,56.6066245608455
def UpdateControlState(self): active = self.demoModules.GetActiveID() # Update the radio/restore buttons for moduleID in self.radioButtons: btn = self.radioButtons[moduleID] if moduleID == active: btn.SetValue(True) else: btn.SetValue(False) if self.demoModules.Exists(moduleID): btn.Enable(True) <mask>: self.btnRestore.Enable(True) else: btn.Enable(False) if moduleID == modModified: self.btnRestore.Enable(False),False,if moduleID == modModified :,if moduleID == modActive :,0.8820016898747209,53.7284965911771
"def ignore_proxy_host(self): """"""Check if self.host is in the $no_proxy ignore list."""""" if urllib.proxy_bypass(self.host): return True no_proxy = os.environ.get(""no_proxy"") if no_proxy: entries = [parse_host_port(x) for x in no_proxy.split("","")] for host, port in entries: <mask>: return True return False",False,if host . lower ( ) == self . host and port == self . port :,if self . host == host and port == self . port :,0.8820016898747209,53.70731736606218
"def run(self, _): view = self.view if not view.settings().get(""terminus_view""): return terminal = Terminal.from_id(view.id()) if terminal: terminal.close() panel_name = terminal.panel_name <mask>: window = panel_window(view) if window: window.destroy_output_panel(panel_name) else: view.close()",True,if panel_name :,if panel_name :,0.8820016898747209,100.00000000000004
"def get_docname_for_node(self, node: Node) -> str: while node: <mask>: return self.env.path2doc(node[""source""]) elif isinstance(node, addnodes.start_of_file): return node[""docname""] else: node = node.parent return None # never reached here. only for type hinting",False,"if isinstance ( node , nodes . document ) :","if isinstance ( node , addnodes . start_of_file ) :",0.8820016898747209,31.61487584488944
"def TryMerge(self, d): while d.avail() > 0: tt = d.getVarInt32() if tt == 10: self.add_version(d.getPrefixedString()) continue <mask>: raise ProtocolBuffer.ProtocolBufferDecodeError d.skipData(tt)",True,if tt == 0 :,if tt == 0 :,0.8820016898747209,100.00000000000004
"def _maybe_female(self, path_elements, female, strict): if female: if self.has_gender_differences: elements = path_elements + [""female""] try: return self._get_file(elements, "".png"", strict=strict) except ValueError: <mask>: raise elif strict: raise ValueError(""Pokemon %s has no gender differences"" % self.species_id) return self._get_file(path_elements, "".png"", strict=strict)",False,if strict :,if self . species_id not in self . species_id :,0.8820016898747209,3.377156414337854
"def OnKeyUp(self, event): if self._properties.modifiable: if event.GetKeyCode() == wx.WXK_ESCAPE: self._cancel_editing() elif event.GetKeyCode() == wx.WXK_RETURN: self._update_value() <mask>: self.SetValue("""") if event.GetKeyCode() != wx.WXK_RETURN: # Don't send skip event if enter key is pressed # On some platforms this event is sent too late and causes crash event.Skip()",False,elif event . GetKeyCode ( ) == wx . WXK_DELETE :,elif event . GetKeyCode ( ) == wx . WXK_ENTER :,0.8820016898747209,85.5526185871245
"def sync_up_to_new_location(self, worker_ip): if worker_ip != self.worker_ip: logger.debug(""Setting new worker IP to %s"", worker_ip) self.set_worker_ip(worker_ip) self.reset() <mask>: logger.warning(""Sync up to new location skipped. This should not occur."") else: logger.warning(""Sync attempted to same IP %s."", worker_ip)",False,if not self . sync_up ( ) :,if self . is_new_location ( ) :,0.8820016898747209,19.081654556856684
"def _get_download_link(self, url, download_type=""torrent""): links = { ""torrent"": """", ""magnet"": """", } try: data = self.session.get(url).text with bs4_parser(data) as html: downloads = html.find(""div"", {""class"": ""download""}) <mask>: for download in downloads.findAll(""a""): link = download[""href""] if link.startswith(""magnet""): links[""magnet""] = link else: links[""torrent""] = urljoin(self.urls[""base_url""], link) except Exception: pass return links[download_type]",True,if downloads :,if downloads :,0.8820016898747209,0.0
"def force_ipv4(self, *args): """"""only ipv4 localhost in /etc/hosts"""""" logg.debug(""checking /etc/hosts for '::1 localhost'"") lines = [] for line in open(self.etc_hosts()): if ""::1"" in line: newline = re.sub(""\\slocalhost\\s"", "" "", line) <mask>: logg.info(""/etc/hosts: '%s' => '%s'"", line.rstrip(), newline.rstrip()) line = newline lines.append(line) f = open(self.etc_hosts(), ""w"") for line in lines: f.write(line) f.close()",False,if line != newline :,if newline :,0.8820016898747209,0.0
"def prepare(self): # Maybe the brok is a old daemon one or was already prepared # if so, the data is already ok if hasattr(self, ""prepared"") and not self.prepared: self.data = SafeUnpickler.loads(self.data) <mask>: self.data[""instance_id""] = self.instance_id self.prepared = True",False,"if hasattr ( self , ""instance_id"" ) :",if self . instance_id :,0.8820016898747209,14.231728394642222
"def _test_compute_q0(self): # Stub code to search a logq space and figure out logq0 by eyeballing # results. This code does not run with the tests. Remove underscore to run. sigma = 15 order = 250 logqs = np.arange(-290, -270, 1) count = 0 for logq in logqs: count += 1 sys.stdout.write( ""\t%0.5g: %0.10g"" % (logq, pate.rdp_gaussian(logq, sigma, order)) ) sys.stdout.flush() <mask>: print("""")",False,if count % 5 == 0 :,if count > 0 :,0.8820016898747209,16.58165975077607
"def valid_fieldnames(fieldnames): """"""check if fieldnames are valid"""""" for fieldname in fieldnames: <mask>: return True elif fieldname in fieldname_map and fieldname_map[fieldname] == ""source"": return True return False",False,"if fieldname in canonical_field_names and fieldname == ""source"" :","if fieldname in fieldname_map and fieldname_map [ fieldname ] == ""source"" :",0.8820016898747209,35.98590234416894
"def ns_provide(self, id_): global controllers, layouts if id_ == ""_leo_viewrendered"": c = self.c vr = controllers.get(c.hash()) or ViewRenderedController(c) h = c.hash() controllers[h] = vr <mask>: layouts[h] = c.db.get(""viewrendered_default_layouts"", (None, None)) # return ViewRenderedController(self.c) return vr",False,if not layouts . get ( h ) :,if h not in layouts :,0.8820016898747209,7.3616411144674565
"def remove(self, path, config=None, error_on_path=False, defaults=None): if not path: if error_on_path: raise NoSuchSettingsPath() return if config is not None or defaults is not None: if config is None: config = self._config <mask>: defaults = dict(self._map.parents) chain = HierarchicalChainMap(config, defaults) else: chain = self._map try: chain.del_by_path(path) self._mark_dirty() except KeyError: if error_on_path: raise NoSuchSettingsPath() pass",True,if defaults is None :,if defaults is None :,0.8820016898747209,100.00000000000004
"def _mongo_query_and(self, queries): if len(queries) == 1: return queries[0] query = {} for q in queries: for k, v in q.items(): if k not in query: query[k] = {} <mask>: # TODO check exists of k in query, may be it should be update query[k] = v else: query[k].update(v) return query",False,"if isinstance ( v , list ) :","elif isinstance ( v , dict ) :",0.8820016898747209,41.11336169005198
"def write(self, data): self.size -= len(data) passon = None if self.size > 0: self.data.append(data) else: <mask>: data, passon = data[: self.size], data[self.size :] else: passon = b"""" if data: self.data.append(data) return passon",False,if self . size :,if len ( data ) > 0 :,0.8820016898747209,6.567274736060395
"def updateVar(name, data, mode=None): if mode: if mode == ""append"": core.config.globalVariables[name].append(data) <mask>: core.config.globalVariables[name].add(data) else: core.config.globalVariables[name] = data",True,"elif mode == ""add"" :","elif mode == ""add"" :",0.8820016898747209,100.00000000000004
"def vi_pos_back_short(line, index=0, count=1): line = vi_list(line) try: for i in range(count): index -= 1 while vi_is_space(line[index]): index -= 1 in_word = vi_is_word(line[index]) <mask>: while vi_is_word(line[index]): index -= 1 else: while not vi_is_word_or_space(line[index]): index -= 1 return index + 1 except IndexError: return 0",True,if in_word :,if in_word :,0.8820016898747209,100.00000000000004
"def _truncate_to_length(generator, len_map=None): for example in generator: example = list(example) <mask>: for key, max_len in len_map.items(): example_len = example[key].shape if example_len > max_len: example[key] = np.resize(example[key], max_len) yield tuple(example)",False,if len_map is not None :,if len_map :,0.8820016898747209,38.80684294761701
"def decorate(f): # call-signature of f is exposed via __wrapped__. # we want it to mimic Obj.__init__ f.__wrapped__ = Obj.__init__ f._uses_signature = Obj # Supplement the docstring of f with information from Obj if Obj.__doc__: doclines = Obj.__doc__.splitlines() <mask>: doc = f.__doc__ + ""\n"".join(doclines[1:]) else: doc = ""\n"".join(doclines) try: f.__doc__ = doc except AttributeError: # __doc__ is not modifiable for classes in Python < 3.3 pass return f",False,if f . __doc__ :,if len ( doclines ) > 1 :,0.8820016898747209,5.795599612995366
"def IncrementErrorCount(self, category): """"""Bumps the module's error statistic."""""" self.error_count += 1 if self.counting in (""toplevel"", ""detailed""): if self.counting != ""detailed"": category = category.split(""/"")[0] <mask>: self.errors_by_category[category] = 0 self.errors_by_category[category] += 1",True,if category not in self . errors_by_category :,if category not in self . errors_by_category :,0.8820016898747209,100.00000000000004
"def _delete_fields(self, data): data = self._del( data, [""speaker_ids"", ""track_id"", ""microlocation_id"", ""session_type_id""] ) # convert datetime fields for _ in [""start_time_tz"", ""end_time_tz""]: <mask>: data[_] = SESSION_POST[_[0:-3]].from_str(data[_]) data[_[0:-3]] = data.pop(_) return data",False,if _ in data :,if _ [ 0 : - 3 ] in SESSION_POST :,0.8820016898747209,7.347053125977879
"def get_strings_of_set(word, char_set, threshold=20): count = 0 letters = """" strings = [] for char in word: if char in char_set: letters += char count += 1 else: <mask>: strings.append(letters) letters = """" count = 0 if count > threshold: strings.append(letters) return strings",True,if count > threshold :,if count > threshold :,0.8820016898747209,100.00000000000004
"def _ArgumentListHasDictionaryEntry(self, token): """"""Check if the function argument list has a dictionary as an arg."""""" if _IsArgumentToFunction(token): while token: if token.value == ""{"": length = token.matching_bracket.total_length - token.total_length return length + self.stack[-2].indent > self.column_limit <mask>: break if token.OpensScope(): token = token.matching_bracket token = token.next_token return False",False,if token . ClosesScope ( ) :,"if token . value == ""}"" :",0.8820016898747209,16.784459625186194
"def check_apns_certificate(ss): mode = ""start"" for s in ss.split(""\n""): if mode == ""start"": if ""BEGIN RSA PRIVATE KEY"" in s or ""BEGIN PRIVATE KEY"" in s: mode = ""key"" elif mode == ""key"": if ""END RSA PRIVATE KEY"" in s or ""END PRIVATE KEY"" in s: mode = ""end"" break <mask>: raise ImproperlyConfigured( ""Encrypted APNS private keys are not supported"" ) if mode != ""end"": raise ImproperlyConfigured(""The APNS certificate doesn't contain a private key"")",False,"elif s . startswith ( ""Proc-Type"" ) and ""ENCRYPTED"" in s :","elif mode == ""key"" :",0.8820016898747209,2.8730831956184355
"def main(self): self.model.clear() self.callman.unregister_all() active_handle = self.get_active(""Person"") if active_handle: active = self.dbstate.db.get_person_from_handle(active_handle) <mask>: self.callman.register_obj(active) self.display_citations(active) else: self.set_has_data(False) else: self.set_has_data(False)",True,if active :,if active :,0.8820016898747209,0.0
"def _validate(self) -> None: # Paren validation and such super(Tuple, self)._validate() if len(self.elements) == 0: <mask>: # assumes len(lpar) == len(rpar), via superclass raise CSTValidationError( ""A zero-length tuple must be wrapped in parentheses."" )",False,if len ( self . lpar ) == 0 :,if len ( lpar ) == len ( rpar ) :,0.8820016898747209,31.702331385234313
"def _session_from_arg(self, session_obj, lock_type=None): if not isinstance(session_obj, self.ISession): vm = self._machine_from_arg(session_obj) lock_type = lock_type or self.LockType.null <mask>: return vm.create_session(lock_type) return None return session_obj",False,if vm :,if vm . is_running ( ) :,0.8820016898747209,10.552670315936318
"def _decorator(cls): for name, meth in inspect.getmembers(cls, inspect.isroutine): if name not in cls.__dict__: continue <mask>: if not private and name.startswith(""_""): continue if name in butnot: continue setattr(cls, name, decorator(meth)) return cls",False,"if name != ""__init__"" :",if meth is not None :,0.8820016898747209,3.550932348642477
"def pdb(message=""""): """"""Fall into pdb."""""" import pdb # Required: we have just defined pdb as a function! if app and not app.useIpython: # from leo.core.leoQt import QtCore # This is more portable. try: import PyQt5.QtCore as QtCore except ImportError: try: import PyQt4.QtCore as QtCore except ImportError: QtCore = None <mask>: # pylint: disable=no-member QtCore.pyqtRemoveInputHook() if message: print(message) pdb.set_trace()",True,if QtCore :,if QtCore :,0.8820016898747209,0.0
"def get_s3_bucket_locations(buckets, self_log=False): """"""return (bucket_name, prefix) for all s3 logging targets"""""" for b in buckets: if b.get(""Logging""): <mask>: if b[""Name""] != b[""Logging""][""TargetBucket""]: continue yield (b[""Logging""][""TargetBucket""], b[""Logging""][""TargetPrefix""]) if not self_log and b[""Name""].startswith(""cf-templates-""): yield (b[""Name""], """")",False,if self_log :,"if ""TargetBucket"" in b [ ""Logging"" ] :",0.8820016898747209,4.02724819242185
"def prepare_fields(self): # See clean() for k, v in self.fields.items(): v._required = v.required v.required = False v.widget.is_required = False <mask>: v._required = v.one_required v.one_required = False v.widget.enabled_locales = self.locales",False,"if isinstance ( v , I18nFormField ) :",if v . one_required :,0.8820016898747209,7.492442692259767
"def __pack__(self): new_values = [] for i in xrange(len(self.__unpacked_data_elms__)): for key in self.__keys__[i]: new_val = getattr(self, key) old_val = self.__unpacked_data_elms__[i] # In the case of Unions, when the first changed value # is picked the loop is exited <mask>: break new_values.append(new_val) return struct.pack(self.__format__, *new_values)",False,if new_val != old_val :,if old_val != new_val :,0.8820016898747209,51.33450480401705
"def run(self): pwd_found = [] if constant.user_dpapi and constant.user_dpapi.unlocked: main_vault_directory = os.path.join( constant.profile[""APPDATA""], u"".."", u""Local"", u""Microsoft"", u""Vault"" ) <mask>: for vault_directory in os.listdir(main_vault_directory): cred = constant.user_dpapi.decrypt_vault( os.path.join(main_vault_directory, vault_directory) ) if cred: pwd_found.append(cred) return pwd_found",False,if os . path . exists ( main_vault_directory ) :,if os . path . isdir ( main_vault_directory ) :,0.8820016898747209,78.25422900366432
"def on_revision_plugin_revision_pre_save(**kwargs): instance = kwargs[""instance""] if kwargs.get(""created"", False): update_previous_revision = ( not instance.previous_revision and instance.plugin and instance.plugin.current_revision and instance.plugin.current_revision != instance ) <mask>: instance.previous_revision = instance.plugin.current_revision if not instance.revision_number: try: previous_revision = instance.plugin.revision_set.latest() instance.revision_number = previous_revision.revision_number + 1 except RevisionPluginRevision.DoesNotExist: instance.revision_number = 1",True,if update_previous_revision :,if update_previous_revision :,0.8820016898747209,100.00000000000004
"def __setattr__(self, name, value): super().__setattr__(name, value) field = self._fields.get(name) if field: self.check_field_type(field, value) <mask>: raise TypeError(f""cannot set immutable {name} on {self!r}"")",False,if name in self . __ast_frozen_fields__ :,if not self . _immutable :,0.8820016898747209,8.858009236942326
"def _check_for_req_data(data): required_args = [""columns""] for arg in required_args: <mask>: return True, make_json_response( status=400, success=0, errormsg=gettext(""Could not find required parameter ({})."").format(arg), ) return False, """"",False,"if arg not in data or ( isinstance ( data [ arg ] , list ) and len ( data [ arg ] ) < 1 ) :",if arg in data :,0.8820016898747209,0.32112755670249143
"def train_dict(self, triples): """"""Train a dict lemmatizer given training (word, pos, lemma) triples."""""" # accumulate counter ctr = Counter() ctr.update([(p[0], p[1], p[2]) for p in triples]) # find the most frequent mappings for p, _ in ctr.most_common(): w, pos, l = p <mask>: self.composite_dict[(w, pos)] = l if w not in self.word_dict: self.word_dict[w] = l return",False,"if ( w , pos ) not in self . composite_dict :",if w not in self . composite_dict :,0.8820016898747209,53.85541747733198
"def render(type_, obj, context): if type_ == ""foreign_key"": return None if type_ == ""column"": if obj.name == ""y"": return None <mask>: return False else: return ""col(%s)"" % obj.name if type_ == ""type"" and isinstance(obj, MySpecialType): context.imports.add(""from mypackage import MySpecialType"") return ""MySpecialType()"" return ""render:%s"" % type_",False,"elif obj . name == ""q"" :","elif obj . name == ""x"" :",0.8820016898747209,70.71067811865478
"def test_knows_when_stepping_back_possible(self): iterator = bidirectional_iterator.BidirectionalIterator([0, 1, 2, 3]) commands = [0, 1, 0, 0, 1, 1, 0, 0, 0, 0] command_count = 0 results = [] for _ in iterator: <mask>: iterator.step_back_on_next_iteration() results.append(iterator.can_step_back()) command_count += 1 assert results == [False, True, False, True, True, True, False, True, True, True]",False,if commands [ command_count ] :,if command_count % 2 == 0 :,0.8820016898747209,17.747405280050266
"def flask_debug_true(context): if context.is_module_imported_like(""flask""): if context.call_function_name_qual.endswith("".run""): <mask>: return bandit.Issue( severity=bandit.HIGH, confidence=bandit.MEDIUM, text=""A Flask app appears to be run with debug=True, "" ""which exposes the Werkzeug debugger and allows "" ""the execution of arbitrary code."", lineno=context.get_lineno_for_call_arg(""debug""), )",False,"if context . check_call_arg_value ( ""debug"" , ""True"" ) :","if context . get_arg ( ""debug"" ) :",0.8820016898747209,21.301093618561733
"def __exit__(self, exc_type, exc_val, exc_tb): if self._should_meta_profile: end_time = timezone.now() exception_raised = exc_type is not None if exception_raised: Logger.error( ""Exception when performing meta profiling, dumping trace below"" ) traceback.print_exception(exc_type, exc_val, exc_tb) request = getattr(DataCollector().local, ""request"", None) <mask>: curr = request.meta_time or 0 request.meta_time = curr + _time_taken(self.start_time, end_time)",True,if request :,if request :,0.8820016898747209,0.0
"def get_job_offer(ja_list): ja_joff_map = {} offers = frappe.get_all( ""Job Offer"", filters=[[""job_applicant"", ""IN"", ja_list]], fields=[""name"", ""job_applicant"", ""status"", ""offer_date"", ""designation""], ) for offer in offers: <mask>: ja_joff_map[offer.job_applicant] = [offer] else: ja_joff_map[offer.job_applicant].append(offer) return ja_joff_map",False,if offer . job_applicant not in ja_joff_map . keys ( ) :,if offer . job_applicant not in ja_joff_map :,0.8820016898747209,70.37688239435663
"def _get_deepest(self, t): if isinstance(t, list): <mask>: return t[0] else: for part in t: res = self._get_deepest(part) if res: return res return None return None",True,if len ( t ) == 1 :,if len ( t ) == 1 :,0.8820016898747209,100.00000000000004
"def test_main(self): root = os.path.dirname(mutagen.__path__[0]) skip = [os.path.join(root, ""docs""), os.path.join(root, ""venv"")] for dirpath, dirnames, filenames in os.walk(root): <mask>: continue for filename in filenames: if filename.endswith("".py""): path = os.path.join(dirpath, filename) self._check_encoding(path)",False,if any ( ( dirpath . startswith ( s + os . sep ) or s == dirpath ) for s in skip ) :,if dirpath in skip :,0.8820016898747209,0.4028448990408853
"def xview(self, mode=None, value=None, units=None): if type(value) == str: value = float(value) if mode is None: return self.hsb.get() elif mode == ""moveto"": frameWidth = self.innerframe.winfo_reqwidth() self._startX = value * float(frameWidth) else: # mode == 'scroll' clipperWidth = self._clipper.winfo_width() <mask>: jump = int(clipperWidth * self._jfraction) else: jump = clipperWidth self._startX = self._startX + value * jump self.reposition()",False,"if units == ""units"" :",if self . _jfraction :,0.8820016898747209,6.916271812933183
"def test_training_script_with_max_history_set(tmpdir): train_dialogue_model( DEFAULT_DOMAIN_PATH, DEFAULT_STORIES_FILE, tmpdir.strpath, interpreter=RegexInterpreter(), policy_config=""data/test_config/max_hist_config.yml"", kwargs={}, ) agent = Agent.load(tmpdir.strpath) for policy in agent.policy_ensemble.policies: <mask>: if type(policy) == FormPolicy: assert policy.featurizer.max_history == 2 else: assert policy.featurizer.max_history == 5",False,"if hasattr ( policy . featurizer , ""max_history"" ) :",if policy . featurizer :,0.8820016898747209,7.468220329575271
"def generate_auto_complete(self, base, iterable_var): sugg = [] for entry in iterable_var: compare_entry = entry compare_base = base <mask>: compare_entry = compare_entry.lower() compare_base = compare_base.lower() if self.compare_entries(compare_entry, compare_base): if entry not in sugg: sugg.append(entry) return sugg",False,if self . settings . get ( IGNORE_CASE_SETTING ) :,"if isinstance ( compare_entry , str ) :",0.8820016898747209,7.073666451977357
"def marker_expr(remaining): if remaining and remaining[0] == ""("": result, remaining = marker(remaining[1:].lstrip()) <mask>: raise SyntaxError(""unterminated parenthesis: %s"" % remaining) remaining = remaining[1:].lstrip() else: lhs, remaining = marker_var(remaining) while remaining: m = MARKER_OP.match(remaining) if not m: break op = m.groups()[0] remaining = remaining[m.end() :] rhs, remaining = marker_var(remaining) lhs = {""op"": op, ""lhs"": lhs, ""rhs"": rhs} result = lhs return result, remaining",False,"if remaining [ 0 ] != "")"" :","if remaining [ 0 ] == "")"" :",0.8820016898747209,70.16879391277372
"def __repr__(self): """"""Dump the class data in the format of a .netrc file."""""" rep = """" for host in self.hosts.keys(): attrs = self.hosts[host] rep = rep + ""machine "" + host + ""\n\tlogin "" + repr(attrs[0]) + ""\n"" <mask>: rep = rep + ""account "" + repr(attrs[1]) rep = rep + ""\tpassword "" + repr(attrs[2]) + ""\n"" for macro in self.macros.keys(): rep = rep + ""macdef "" + macro + ""\n"" for line in self.macros[macro]: rep = rep + line rep = rep + ""\n"" return rep",True,if attrs [ 1 ] :,if attrs [ 1 ] :,0.8820016898747209,100.00000000000004
"def _parse_policies(self, policies_yaml): for item in policies_yaml: id_ = required_key(item, ""id"") controls_ids = required_key(item, ""controls"") <mask>: if controls_ids != ""all"": msg = ""Policy {id_} contains invalid controls list {controls}."".format( id_=id_, controls=str(controls_ids) ) raise ValueError(msg) self.policies[id_] = controls_ids",False,"if not isinstance ( controls_ids , list ) :",if id_ :,0.8820016898747209,3.6531471527995247
"def __set__(self, obj, value): # noqa if ( value is not None and self.field._currency_field.null and not isinstance(value, MONEY_CLASSES + (Decimal,)) ): # For nullable fields we need either both NULL amount and currency or both NOT NULL raise ValueError(""Missing currency value"") if isinstance(value, BaseExpression): <mask>: value = self.prepare_value(obj, value.value) elif not isinstance(value, Func): validate_money_expression(obj, value) prepare_expression(value) else: value = self.prepare_value(obj, value) obj.__dict__[self.field.name] = value",False,"if isinstance ( value , Value ) :","if isinstance ( value , Money ) :",0.8820016898747209,59.4603557501361
"def Children(self): """"""Returns a list of all of this object's owned (strong) children."""""" children = [] for property, attributes in self._schema.iteritems(): (is_list, property_type, is_strong) = attributes[0:3] <mask>: if not is_list: children.append(self._properties[property]) else: children.extend(self._properties[property]) return children",False,if is_strong and property in self . _properties :,if is_strong :,0.8820016898747209,17.437038542312457
"def next_item(self, direction): """"""Selects next menu item, based on self._direction"""""" start, i = -1, 0 try: start = self.items.index(self._selected) i = start + direction except: pass while True: if i == start: # Cannot find valid menu item self.select(start) break if i >= len(self.items): i = 0 continue if i < 0: i = len(self.items) - 1 continue if self.select(i): break i += direction <mask>: start = 0",False,if start < 0 :,if i == start :,0.8820016898747209,10.682175159905853
"def setup_displace(self): self.displace_mod = None self.displace_strength = 0.020 for mod in self.obj.modifiers: <mask>: self.displace_mod = mod self.displace_strength = mod.strength if not self.displace_mod: bpy.ops.object.modifier_add(type=""DISPLACE"") self.displace_mod = self.obj.modifiers[-1] self.displace_mod.show_expanded = False self.displace_mod.strength = self.displace_strength self.displace_mod.show_render = False self.displace_mod.show_viewport = False",False,"if mod . type == ""DISPLACE"" :","if isinstance ( mod , bpy . ops . object . modifier_add ) :",0.8820016898747209,3.4585921141027365
"def set_json_body(cls, request_builder): old_body = request_builder.info.pop(""data"", {}) if isinstance(old_body, abc.Mapping): body = request_builder.info.setdefault(""json"", {}) for path in old_body: <mask>: cls._sequence_path_resolver(path, old_body[path], body) else: body[path] = old_body[path] else: request_builder.info.setdefault(""json"", old_body)",False,"if isinstance ( path , tuple ) :","if isinstance ( old_body [ path ] , abc . Mapping ) :",0.8820016898747209,13.380161378318954
"def build(opt): dpath = os.path.join(opt[""datapath""], ""DBLL"") version = None if not build_data.built(dpath, version_string=version): print(""[building data: "" + dpath + ""]"") <mask>: # An older version exists, so remove these outdated files. build_data.remove_dir(dpath) build_data.make_dir(dpath) # Download the data. for downloadable_file in RESOURCES: downloadable_file.download_file(dpath) # Mark the data as built. build_data.mark_done(dpath, version_string=version)",True,if build_data . built ( dpath ) :,if build_data . built ( dpath ) :,0.8820016898747209,100.00000000000004
"def test_prefix_lm(self): num_tries = 100 original = ""This is a long test with lots of words to see if it works ok."" dataset = tf.data.Dataset.from_tensor_slices({""text"": [original] * num_tries}) dataset = prep.prefix_lm(dataset) for data in test_utils.dataset_as_text(dataset): inputs = data[""inputs""].replace(""prefix: "", """") targets = data[""targets""] reconstructed = """".join(inputs) <mask>: reconstructed += "" "" reconstructed += """".join(targets) self.assertEqual(reconstructed, original)",False,if inputs :,if targets :,0.8820016898747209,0.0
"def leading_whitespace(self, inputstring): """"""Get leading whitespace."""""" leading_ws = [] for i, c in enumerate(inputstring): if c in legal_indent_chars: leading_ws.append(c) else: break <mask>: self.indchar = c elif c != self.indchar: self.strict_err_or_warn(""found mixing of tabs and spaces"", inputstring, i) return """".join(leading_ws)",False,if self . indchar is None :,if i == 0 :,0.8820016898747209,8.170609724417774
"def __init__(self, text): self.mappings = {} self.attributes = collections.defaultdict(set) for stanza in _ParseTextProperties(text): processor_id, single_values, multiple_values = self._ParseStanza(stanza) if processor_id is None: # can be 0 continue <mask>: logging.warn(""Processor id %s seen twice in %s"", processor_id, text) continue self.mappings[processor_id] = single_values for key, value in multiple_values.items(): self.attributes[key].add(value)",True,if processor_id in self . mappings :,if processor_id in self . mappings :,0.8820016898747209,100.00000000000004
"def __iter__(self): for chunk in self.source: <mask>: self.wait_counter = 0 yield chunk elif self.wait_counter < self.wait_cntr_max: self.wait_counter += 1 else: logger.warning( ""Data poller has been receiving no data for {} seconds.\n"" ""Closing data poller"".format(self.wait_cntr_max * self.poll_period) ) break time.sleep(self.poll_period)",False,if chunk is not None :,if self . wait_counter == self . wait_cntr_max :,0.8820016898747209,2.908317710573757
"def download(self, prefetch=False): while self.running: try: <mask>: (path, start, end) = self.prefetch_queue.get( True, 1 ) # 1 second time-out else: (path, start, end) = self.download_queue.get( True, 1 ) # 1 second time-out self.download_data(path, start, end) if prefetch: self.prefetch_queue.task_done() else: self.download_queue.task_done() except Queue.Empty: pass",True,if prefetch :,if prefetch :,0.8820016898747209,0.0
"def process_messages(self, found_files, messages): for message in messages: <mask>: message.to_absolute_path(self.config.workdir) else: message.to_relative_path(self.config.workdir) if self.config.blending: messages = blender.blend(messages) filepaths = found_files.iter_module_paths(abspath=False) return postfilter.filter_messages(filepaths, self.config.workdir, messages)",False,if self . config . absolute_paths :,if self . config . absolute :,0.8820016898747209,63.191456189157286
"def set_indentation_params(self, ispythonsource, guess=1): if guess and ispythonsource: i = self.guess_indent() <mask>: self.indentwidth = i if self.indentwidth != self.tabwidth: self.usetabs = 0 self.editwin.set_tabwidth(self.tabwidth)",False,if 2 <= i <= 8 :,if i > self . tabwidth :,0.8820016898747209,6.495032985064742
"def to_tree(self, tagname=None, value=None, namespace=None): namespace = getattr(self, ""namespace"", namespace) if value is not None: <mask>: tagname = ""{%s}%s"" % (namespace, tagname) el = Element(tagname) el.text = safe_string(value) return el",False,if namespace is not None :,if tagname is not None :,0.8820016898747209,53.7284965911771
"def execute(self, argv: List) -> bool: if not argv: print(""ERROR: You must give at least one module to download."") return False for _arg in argv: result = module_server.search_module(_arg) CacheUpdater(""hub_download"", _arg).start() <mask>: url = result[0][""url""] with log.ProgressBar(""Download {}"".format(url)) as bar: for file, ds, ts in utils.download_with_progress(url): bar.update(float(ds) / ts) else: print(""ERROR: Could not find a HubModule named {}"".format(_arg)) return True",True,if result :,if result :,0.8820016898747209,0.0
"def visit_type_type(self, t: TypeType) -> ProperType: if isinstance(self.s, TypeType): typ = self.meet(t.item, self.s.item) <mask>: typ = TypeType.make_normalized(typ, line=t.line) return typ elif isinstance(self.s, Instance) and self.s.type.fullname == ""builtins.type"": return t elif isinstance(self.s, CallableType): return self.meet(t, self.s) else: return self.default(self.s)",False,"if not isinstance ( typ , NoneType ) :",if typ . is_normalized ( ) :,0.8820016898747209,11.99014838091355
"def run(self, paths=[]): items = [] for item in SideBarSelection(paths).getSelectedItems(): items.append(item.name()) if len(items) > 0: sublime.set_clipboard(""\n"".join(items)) <mask>: sublime.status_message(""Items copied"") else: sublime.status_message(""Item copied"")",False,if len ( items ) > 1 :,if len ( items ) == 1 :,0.8820016898747209,51.33450480401705
"def get_icon(self): if self.icon is not None: # Load it from an absolute filename <mask>: try: return GdkPixbuf.Pixbuf.new_from_file_at_size(self.icon, 24, 24) except GObject.GError as ge: pass # Load it from the current icon theme (icon_name, extension) = os.path.splitext(os.path.basename(self.icon)) theme = Gtk.IconTheme() if theme.has_icon(icon_name): return theme.load_icon(icon_name, 24, 0)",False,if os . path . exists ( self . icon ) :,if os . path . isabs ( self . icon ) :,0.8820016898747209,73.48889200874659
"def setup_logger(): """"""Set up logger and add stdout handler"""""" logging.setLoggerClass(IPDLogger) logger = logging.getLogger(""icloudpd"") has_stdout_handler = False for handler in logger.handlers: <mask>: has_stdout_handler = True if not has_stdout_handler: formatter = logging.Formatter( fmt=""%(asctime)s %(levelname)-8s %(message)s"", datefmt=""%Y-%m-%d %H:%M:%S"" ) stdout_handler = logging.StreamHandler(stream=sys.stdout) stdout_handler.setFormatter(formatter) stdout_handler.name = ""stdoutLogger"" logger.addHandler(stdout_handler) return logger",False,"if handler . name == ""stdoutLogger"" :",if handler . is_writable ( ) :,0.8820016898747209,17.0653267718276
"def process_extra_fields(self): if self.instance.pk is not None: if self.cleaned_data.get(""initialize"", None): self.instance.initialize() <mask>: self.instance.update_from_templates()",False,"if self . cleaned_data . get ( ""update"" , None ) or not self . instance . stores . count ( ) :","elif self . cleaned_data . get ( ""update_from_templates"" , None ) :",0.8820016898747209,42.64830988613545
"def testFunctions(self): from zim.formats.wiki import match_url, is_url for input, input_is_url, tail in self.examples: if input_is_url: <mask>: self.assertEqual(match_url(input), input[: -len(tail)]) self.assertFalse(is_url(input)) else: self.assertEqual(match_url(input), input) self.assertTrue(is_url(input)) else: self.assertEqual(match_url(input), None) self.assertFalse(is_url(input))",True,if tail :,if tail :,0.8820016898747209,0.0
"def _SetUser(self, users): for user in users.items(): username = user[0] settings = user[1] room = settings[""room""][""name""] if ""room"" in settings else None file_ = settings[""file""] if ""file"" in settings else None <mask>: if ""joined"" in settings[""event""]: self._client.userlist.addUser(username, room, file_) elif ""left"" in settings[""event""]: self._client.removeUser(username) else: self._client.userlist.modUser(username, room, file_)",False,"if ""event"" in settings :","if ""user"" in settings :",0.8820016898747209,48.892302243490086
"def restoreTerminals(self, state): for name in list(self.terminals.keys()): <mask>: self.removeTerminal(name) for name, opts in state.items(): if name in self.terminals: term = self[name] term.setOpts(**opts) continue try: opts = strDict(opts) self.addTerminal(name, **opts) except: printExc(""Error restoring terminal %s (%s):"" % (str(name), str(opts)))",False,if name not in state :,if name in state :,0.8820016898747209,40.93653765389909
"def htmlify(path, text): fname = os.path.basename(path) if any((fnmatch.fnmatchcase(fname, p) for p in _patterns)): # Get file_id, skip if not in database sql = ""SELECT files.id FROM files WHERE path = ? LIMIT 1"" row = _conn.execute(sql, (path,)).fetchone() <mask>: return ClangHtmlifier(_tree, _conn, path, text, row[0]) return None",True,if row :,if row :,0.8820016898747209,0.0
"def autoformat_filter_conv2d(fsize, in_depth, out_depth): if isinstance(fsize, int): return [fsize, fsize, in_depth, out_depth] elif isinstance(fsize, (tuple, list, tf.TensorShape)): <mask>: return [fsize[0], fsize[1], in_depth, out_depth] else: raise Exception( ""filter length error: "" + str(len(fsize)) + "", only a length of 2 is supported."" ) else: raise Exception(""filter format error: "" + str(type(fsize)))",True,if len ( fsize ) == 2 :,if len ( fsize ) == 2 :,0.8820016898747209,100.00000000000004
"def _rle_encode(string): new = b"""" count = 0 for cur in string: <mask>: count += 1 else: if count: new += b""\0"" + bytes([count]) count = 0 new += bytes([cur]) return new",False,if not cur :,"if cur == b""\0"" :",0.8820016898747209,5.522397783539471
"def is_clean(self): acceptable_statuses = {""external"", ""unversioned""} root = self._capture_output(""status"", ""--quiet"") for elem in root.findall(""./target/entry""): status = elem.find(""./wc-status"") <mask>: continue log.debug(""Path %s is %s"", elem.get(""path""), status.get(""item"")) return False return True",False,"if status . get ( ""item"" , None ) in acceptable_statuses :",if status not in acceptable_statuses :,0.8820016898747209,21.874242445215227
"def process(self, body, message): try: <mask>: raise TypeError( 'Received an unexpected type ""%s"" for payload.' % type(body) ) response = self._handler.pre_ack_process(body) self._dispatcher.dispatch(self._process_message, response) except: LOG.exception(""%s failed to process message: %s"", self.__class__.__name__, body) finally: # At this point we will always ack a message. message.ack()",False,"if not isinstance ( body , self . _handler . message_type ) :","if not isinstance ( body , Message ) :",0.8820016898747209,30.35117977459125
"def page_file(self, page): try: page = self.notebook.get_page(page) <mask>: return page.source else: return None except PageNotFoundError: return None",False,"if hasattr ( page , ""source"" ) and isinstance ( page . source , File ) :",if page . source :,0.8820016898747209,2.7474047213893553
"def _optimize(self, solutions): best_a = None best_silhouette = None best_k = None for a, silhouette, k in solutions(): <mask>: pass elif silhouette <= best_silhouette: break best_silhouette = silhouette best_a = a best_k = k return best_a, best_silhouette, best_k",False,if best_silhouette is None :,if a == best_a :,0.8820016898747209,13.134549472120788
"def _cancel_tasks_for_partitions(self, to_cancel_partitions): # type: (Iterable[str]) -> None with self._lock: _LOGGER.debug( ""EventProcessor %r tries to cancel partitions %r"", self._id, to_cancel_partitions, ) for partition_id in to_cancel_partitions: <mask>: self._consumers[partition_id].stop = True _LOGGER.info( ""EventProcessor %r has cancelled partition %r"", self._id, partition_id, )",True,if partition_id in self . _consumers :,if partition_id in self . _consumers :,0.8820016898747209,100.00000000000004
"def get_intersect_all(self, refine=False): result = None for source, parts in self._per_source.items(): <mask>: result = parts else: result.intersection_update(parts) if not result: return None elif len(result) == 1: return list(result)[0].item else: solids = [p.item for p in result] solid = solids[0].fuse(solids[1:]) if refine: solid = solid.removeSplitter() return solid",False,if result is None :,if source == self . _source :,0.8820016898747209,5.669791110976001
"def geli_detach(self, pool, clear=False): failed = 0 for ed in self.middleware.call_sync( ""datastore.query"", ""storage.encrypteddisk"", [(""encrypted_volume"", ""="", pool[""id""])], ): dev = ed[""encrypted_provider""] try: self.geli_detach_single(dev) except Exception as ee: self.logger.warn(str(ee)) failed += 1 <mask>: try: self.geli_clear(dev) except Exception as e: self.logger.warn(""Failed to clear %s: %s"", dev, e) return failed",True,if clear :,if clear :,0.8820016898747209,0.0
def compute_lengths(batch_sizes): tmp_batch_sizes = np.copy(batch_sizes) lengths = [] while True: c = np.count_nonzero(tmp_batch_sizes > 0) <mask>: break lengths.append(c) tmp_batch_sizes = np.array([b - 1 for b in tmp_batch_sizes]) return np.array(lengths),True,if c == 0 :,if c == 0 :,0.8820016898747209,100.00000000000004
"def _render_raw_list(bytes_items): flatten_items = [] for item in bytes_items: <mask>: flatten_items.append(b"""") elif isinstance(item, bytes): flatten_items.append(item) elif isinstance(item, int): flatten_items.append(str(item).encode()) elif isinstance(item, list): flatten_items.append(_render_raw_list(item)) return b""\n"".join(flatten_items)",False,if item is None :,"if isinstance ( item , str ) :",0.8820016898747209,7.267884212102741
"def update(self, new_config): jsonschema.validate(new_config, self.schema) config = {} for k, v in new_config.items(): <mask>: config[k] = self[k] else: config[k] = v self._config = config self.changed()",False,"if k in self . schema . get ( ""secret"" , [ ] ) and v == SECRET_PLACEHOLDER :","if isinstance ( v , dict ) :",0.8820016898747209,1.1697055077575267
"def _encode_numpy(values, uniques=None, encode=False, check_unknown=True): # only used in _encode below, see docstring there for details if uniques is None: if encode: uniques, encoded = np.unique(values, return_inverse=True) return uniques, encoded else: # unique sorts return np.unique(values) if encode: <mask>: diff = _encode_check_unknown(values, uniques) if diff: raise ValueError(""y contains previously unseen labels: %s"" % str(diff)) encoded = np.searchsorted(uniques, values) return uniques, encoded else: return uniques",True,if check_unknown :,if check_unknown :,0.8820016898747209,100.00000000000004
"def restore_dtype_and_merge(arr, input_dtype): if isinstance(arr, list): arr = [restore_dtype_and_merge(arr_i, input_dtype) for arr_i in arr] shapes = [arr_i.shape for arr_i in arr] <mask>: arr = np.array(arr) if ia.is_np_array(arr): arr = iadt.restore_dtypes_(arr, input_dtype) return arr",False,if len ( set ( shapes ) ) == 1 :,if shapes :,0.8820016898747209,0.0
"def proc_minute(d): if expanded[0][0] != ""*"": diff_min = nearest_diff_method(d.minute, expanded[0], 60) if diff_min is not None and diff_min != 0: <mask>: d += relativedelta(minutes=diff_min, second=59) else: d += relativedelta(minutes=diff_min, second=0) return True, d return False, d",False,if is_prev :,"if expanded [ 0 ] [ 0 ] == ""*+"" :",0.8820016898747209,3.1251907639724417
"def _populate_tree(self, element, d): """"""Populates an etree with attributes & elements, given a dict."""""" for k, v in d.iteritems(): <mask>: self._populate_dict(element, k, v) elif isinstance(v, list): self._populate_list(element, k, v) elif isinstance(v, bool): self._populate_bool(element, k, v) elif isinstance(v, basestring): self._populate_str(element, k, v) elif type(v) in [int, float, long, complex]: self._populate_number(element, k, v)",True,"if isinstance ( v , dict ) :","if isinstance ( v , dict ) :",0.8820016898747209,100.00000000000004
"def __createItemAttribute(self, item, function, preload): """"""Create the new widget, add it, and remove the old one"""""" try: self.__stack.addWidget(function(item, preload)) # Remove the widget <mask>: oldWidget = self.__stack.widget(0) self.__stack.removeWidget(oldWidget) oldWidget.setParent(QtWidgets.QWidget()) except Exception as e: list(map(logger.warning, cuegui.Utils.exceptionOutput(e)))",False,if self . __stack . count ( ) > 1 :,if self . __stack . widget ( 0 ) :,0.8820016898747209,51.86805880190238
"def download_main( download, download_playlist, urls, playlist, output_dir, merge, info_only ): for url in urls: <mask>: url = url[8:] if not url.startswith(""http://""): url = ""http://"" + url if playlist: download_playlist( url, output_dir=output_dir, merge=merge, info_only=info_only ) else: download(url, output_dir=output_dir, merge=merge, info_only=info_only)",False,"if url . startswith ( ""https://"" ) :","if url [ : 8 ] == ""https"" :",0.8820016898747209,10.135943830402923
"def add_enc_zero(obj, enc_zero): if isinstance(obj, np.ndarray): return obj + enc_zero elif isinstance(obj, Iterable): return type(obj)( EncryptModeCalculator.add_enc_zero(o, enc_zero) <mask>: else o + enc_zero for o in obj ) else: return obj + enc_zero",False,"if isinstance ( o , Iterable )","if isinstance ( o , ( list , tuple ) )",0.8820016898747209,36.72056269893591
"def ensemble(self, pairs, other_preds): """"""Ensemble the dict with statistical model predictions."""""" lemmas = [] assert len(pairs) == len(other_preds) for p, pred in zip(pairs, other_preds): w, pos = p if (w, pos) in self.composite_dict: lemma = self.composite_dict[(w, pos)] elif w in self.word_dict: lemma = self.word_dict[w] else: lemma = pred <mask>: lemma = w lemmas.append(lemma) return lemmas",False,if lemma is None :,if pred is None :,0.8820016898747209,42.72870063962342
"def replace_to_6hex(color): """"""Validate and replace 3hex colors to 6hex ones."""""" if match(r""^#(?:[0-9a-fA-F]{3}){1,2}$"", color): <mask>: color = ""#{0}{0}{1}{1}{2}{2}"".format(color[1], color[2], color[3]) return color else: exit(_(""Invalid color {}"").format(color))",False,if len ( color ) == 4 :,"if color [ 0 ] == ""#"" :",0.8820016898747209,9.425159511373677
"def computeMachineName(self): """"""Return the name of the current machine, i.e, HOSTNAME."""""" # This is prepended to leoSettings.leo or myLeoSettings.leo # to give the machine-specific setting name. # How can this be worth doing?? try: import os name = os.getenv(""HOSTNAME"") <mask>: name = os.getenv(""COMPUTERNAME"") if not name: import socket name = socket.gethostname() except Exception: name = """" return name",True,if not name :,if not name :,0.8820016898747209,100.00000000000004
"def _git_dirty_working_directory(q, include_untracked): try: cmd = [""git"", ""status"", ""--porcelain""] if include_untracked: cmd += [""--untracked-files=normal""] else: cmd += [""--untracked-files=no""] status = _run_git_cmd(cmd) <mask>: q.put(bool(status)) else: q.put(None) except (subprocess.CalledProcessError, OSError, FileNotFoundError): q.put(None)",False,if status is not None :,if status :,0.8820016898747209,0.0
"def runAndWaitWork(server, work): work.touch() thr = threading.Thread(target=workThread, args=(server, work)) thr.setDaemon(True) thr.start() # Wait around for done or timeout while True: if work.isTimedOut(): break # If the thread is done, lets get out. if not thr.isAlive(): break # If our parent, or some thread closes stdin, # time to pack up and go. <mask>: break time.sleep(2)",False,if sys . stdin . closed :,if thr . isAlive ( ) :,0.8820016898747209,8.643019616048525
"def read(self, count=True, timeout=None, ignore_non_errors=True, ignore_timeouts=True): try: return self._read(count, timeout) except usb.USBError as e: if DEBUG_COMM: log.info( ""read: e.errno=%s e.strerror=%s e.message=%s repr=%s"" % (e.errno, e.strerror, e.message, repr(e)) ) if ignore_timeouts and is_timeout(e): return [] <mask>: return [] raise",False,if ignore_non_errors and is_noerr ( e ) :,if ignore_non_errors and is_non_errors ( e ) :,0.8820016898747209,67.39047062564734
"def PrintHeader(self): # print the header array if self.draw == False: return for val in self.parent.header: self.SetPrintFont(val[""Font""]) header_indent = val[""Indent""] * self.pwidth text = val[""Text""] htype = val[""Type""] <mask>: addtext = self.GetDate() elif htype == ""Date & Time"": addtext = self.GetDateTime() else: addtext = """" self.OutTextPageWidth( text + addtext, self.pheader_margin, val[""Align""], header_indent, True )",False,"if htype == ""Date"" :","if htype == ""Date & Date"" :",0.8820016898747209,63.894310424627285
"def get_intersect_all(self, refine=False): result = None for source, parts in self._per_source.items(): if result is None: result = parts else: result.intersection_update(parts) if not result: return None elif len(result) == 1: return list(result)[0].item else: solids = [p.item for p in result] solid = solids[0].fuse(solids[1:]) <mask>: solid = solid.removeSplitter() return solid",True,if refine :,if refine :,0.8820016898747209,0.0
"def captured_updateNode(self, context): if not self.updating_name_from_pointer: font_datablock = self.get_bpy_data_from_name(self.fontname, bpy.data.fonts) <mask>: self.font_pointer = font_datablock updateNode(self, context)",True,if font_datablock :,if font_datablock :,0.8820016898747209,100.00000000000004
"def __add__(self, other): if isinstance(other, Vector2): # Vector + Vector -> Vector # Vector + Point -> Point # Point + Point -> Vector <mask>: _class = Vector2 else: _class = Point2 return _class(self.x + other.x, self.y + other.y) else: assert hasattr(other, ""__len__"") and len(other) == 2 return Vector2(self.x + other[0], self.y + other[1])",False,if self . __class__ is other . __class__ :,"if isinstance ( other , ( Vector2 , Point2 ) ) :",0.8820016898747209,2.9381581998927433
"def _flatten_settings_from_form(self, settings, form, form_values): """"""Take a nested dict and return a flat dict of setting values."""""" setting_values = {} for field in form.c: <mask>: setting_values.update( self._flatten_settings_from_form( settings, field, form_values[field._name] ) ) elif field._name in settings: setting_values[field._name] = form_values[field._name] return setting_values",False,"if isinstance ( field , _ContainerMixin ) :",if field . _name in form_values :,0.8820016898747209,5.934202609760488
"def add_include_dirs(self, args): ids = [] for a in args: # FIXME same hack, forcibly unpack from holder. if hasattr(a, ""includedirs""): a = a.includedirs <mask>: raise InvalidArguments( ""Include directory to be added is not an include directory object."" ) ids.append(a) self.include_dirs += ids",False,"if not isinstance ( a , IncludeDirs ) :","if not isinstance ( a , ( list , tuple ) ) :",0.8820016898747209,42.803206067505954
"def _clip_array(array, config): if ""threshold"" in config.keys(): threshold = config[""threshold""] else: abs_array = np.max(np.abs(array)) <mask>: return array threshold = np.percentile(np.abs(array), 99.99) return np.clip(array, -threshold, threshold)",False,if abs_array < 1.0 :,if abs_array == 0 :,0.8820016898747209,36.55552228545123
def dfs(v: str) -> Iterator[Set[str]]: index[v] = len(stack) stack.append(v) boundaries.append(index[v]) for w in edges[v]: <mask>: yield from dfs(w) elif w not in identified: while index[w] < boundaries[-1]: boundaries.pop() if boundaries[-1] == index[v]: boundaries.pop() scc = set(stack[index[v] :]) del stack[index[v] :] identified.update(scc) yield scc,False,if w not in index :,if w in identified :,0.8820016898747209,20.80119537801062
"def create_balancer( self, name, members, protocol=""http"", port=80, algorithm=DEFAULT_ALGORITHM ): balancer = self.ex_create_balancer_nowait(name, members, protocol, port, algorithm) timeout = 60 * 20 waittime = 0 interval = 2 * 15 if balancer.id is not None: return balancer else: while waittime < timeout: balancers = self.list_balancers() for i in balancers: <mask>: return i waittime += interval time.sleep(interval) raise Exception(""Failed to get id"")",False,if i . name == balancer . name and i . id is not None :,if i . id is not None :,0.8820016898747209,30.70373468463369
"def handle(self, scope: Scope, receive: Receive, send: Send) -> None: if self.methods and scope[""method""] not in self.methods: <mask>: raise HTTPException(status_code=405) else: response = PlainTextResponse(""Method Not Allowed"", status_code=405) await response(scope, receive, send) else: await self.app(scope, receive, send)",False,"if ""app"" in scope :","if self . method == ""GET"" :",0.8820016898747209,5.934202609760488
"def convert(data): result = [] for d in data: # noinspection PyCompatibility if isinstance(d, tuple) and len(d) == 2: result.append((d[0], None, d[1])) <mask>: result.append(d) return result",False,"elif isinstance ( d , basestring ) :","elif isinstance ( d , list ) :",0.8820016898747209,59.4603557501361
"def register_adapters(): global adapters_registered if adapters_registered is True: return try: import pkg_resources packageDir = pkg_resources.resource_filename(""pyamf"", ""adapters"") except: packageDir = os.path.dirname(__file__) for f in glob.glob(os.path.join(packageDir, ""*.py"")): mod = os.path.basename(f).split(os.path.extsep, 1)[0] <mask>: continue try: register_adapter(mod[1:].replace(""_"", "".""), PackageImporter(mod)) except ImportError: pass adapters_registered = True",False,"if mod == ""__init__"" or not mod . startswith ( ""_"" ) :","if mod == ""__init__.py"" :",0.8820016898747209,40.80974367765305
"def load_modules( to_load, load, attr, modules_dict, excluded_aliases, loading_message=None ): if loading_message: print(loading_message) for name in to_load: module = load(name) if module is None or not hasattr(module, attr): continue cls = getattr(module, attr) if hasattr(cls, ""initialize"") and not cls.initialize(): continue if hasattr(module, ""aliases""): for alias in module.aliases(): <mask>: modules_dict[alias] = module else: modules_dict[name] = module if loading_message: print()",True,if alias not in excluded_aliases :,if alias not in excluded_aliases :,0.8820016898747209,100.00000000000004
"def clean_items(event, items, variations): for item in items: <mask>: raise ValidationError(_(""One or more items do not belong to this event."")) if item.has_variations: if not any(var.item == item for var in variations): raise ValidationError( _( ""One or more items has variations but none of these are in the variations list."" ) )",False,if event != item . event :,if not item . has_event :,0.8820016898747209,16.515821590069027
"def __get_file_by_num(self, num, file_list, idx=0): for element in file_list: <mask>: return element if element[3] and element[4]: i = self.__get_file_by_num(num, element[3], idx + 1) if not isinstance(i, int): return i idx = i else: idx += 1 return idx",False,if idx == num :,"if not isinstance ( element , int ) :",0.8820016898747209,5.669791110976001
"def check(chip, xeddb, chipdb): all_inst = [] undoc = [] for inst in xeddb.recs: <mask>: if inst.undocumented: undoc.append(inst) else: all_inst.append(inst) return (all_inst, undoc)",False,if inst . isa_set in chipdb [ chip ] :,if inst . id == chipdb . id :,0.8820016898747209,14.530346490115708
"def get_all_topic_src_files(self): """"""Retrieves the file paths of all the topics in directory"""""" topic_full_paths = [] topic_names = os.listdir(self.topic_dir) for topic_name in topic_names: # Do not try to load hidden files. <mask>: topic_full_path = os.path.join(self.topic_dir, topic_name) # Ignore the JSON Index as it is stored with topic files. if topic_full_path != self.index_file: topic_full_paths.append(topic_full_path) return topic_full_paths",False,"if not topic_name . startswith ( ""."" ) :","if not os . path . isfile ( os . path . join ( self . topic_dir , topic_name ) ) :",0.8820016898747209,8.640609739997757
"def _get_element(dom_msi, tag_name, name=None, id_=None): """"""Get a xml element defined on Product."""""" product = dom_msi.getElementsByTagName(""Product"")[0] elements = product.getElementsByTagName(tag_name) for element in elements: <mask>: if ( element.getAttribute(""Name"") == name and element.getAttribute(""Id"") == id_ ): return element elif id_: if element.getAttribute(""Id"") == id_: return element",False,if name and id_ :,if name :,0.8820016898747209,0.0
"def __init__(self, *models): super().__init__() self.models = ModuleList(models) for m in models: <mask>: raise ValueError( ""IndependentModelList currently only supports models that have a likelihood (e.g. ExactGPs)"" ) self.likelihood = LikelihoodList(*[m.likelihood for m in models])",False,"if not hasattr ( m , ""likelihood"" ) :","if isinstance ( m , IndependentModelList ) :",0.8820016898747209,18.594002123233256
"def _sniff(filename, oxlitype): try: with open(filename, ""rb"") as fileobj: header = fileobj.read(4) <mask>: fileobj.read(1) # skip the version number ftype = fileobj.read(1) if binascii.hexlify(ftype) == oxlitype: return True return False except OSError: return False",False,"if header == b""OXLI"" :","if header == ""version"" :",0.8820016898747209,37.70794596593207
"def convert_port_bindings(port_bindings): result = {} for k, v in six.iteritems(port_bindings): key = str(k) if ""/"" not in key: key += ""/tcp"" <mask>: result[key] = [_convert_port_binding(binding) for binding in v] else: result[key] = [_convert_port_binding(v)] return result",True,"if isinstance ( v , list ) :","if isinstance ( v , list ) :",0.8820016898747209,100.00000000000004
"def input_data(self): gen = self.config.generator # don't try running the generator if we specify an output file explicitly, # otherwise generator may segfault and we end up returning the output file anyway if gen and (not self.config[""out""] or not self.config[""in""]): <mask>: self._run_generator(gen, args=self.config.generator_args) if self._generated[0]: return self._generated[0] # in file is optional return ( self._normalize(self.problem.problem_data[self.config[""in""]]) if self.config[""in""] else b"""" )",False,if self . _generated is None :,if self . _generated :,0.8820016898747209,56.98363775444274
"def __new__(cls, *tasks, **kwargs): # This forces `chain(X, Y, Z)` to work the same way as `X | Y | Z` if not kwargs and tasks: <mask>: tasks = tasks[0] if len(tasks) == 1 else tasks return reduce(operator.or_, tasks) return super(chain, cls).__new__(cls, *tasks, **kwargs)",False,if len ( tasks ) != 1 or is_list ( tasks [ 0 ] ) :,"if isinstance ( tasks , tuple ) :",0.8820016898747209,4.175853655216358
"def get_file_sources(): global _file_sources if _file_sources is None: from galaxy.files import ConfiguredFileSources file_sources = None if os.path.exists(""file_sources.json""): file_sources_as_dict = None with open(""file_sources.json"", ""r"") as f: file_sources_as_dict = json.load(f) if file_sources_as_dict is not None: file_sources = ConfiguredFileSources.from_dict(file_sources_as_dict) <mask>: ConfiguredFileSources.from_dict([]) _file_sources = file_sources return _file_sources",False,if file_sources is None :,if not file_sources :,0.8820016898747209,29.05925408079185
"def InitializeColours(self): """"""Initializes the 16 custom colours in :class:`CustomPanel`."""""" curr = self._colourData.GetColour() self._colourSelection = -1 for i in range(16): c = self._colourData.GetCustomColour(i) <mask>: self._customColours[i] = self._colourData.GetCustomColour(i) else: self._customColours[i] = wx.WHITE if c == curr: self._colourSelection = i",False,if c . IsOk ( ) :,if c == wx . BLACK :,0.8820016898747209,13.134549472120788
"def convert_obj_into_marshallable(self, obj): if isinstance(obj, self.marshalable_types): return obj if isinstance(obj, array.array): if obj.typecode == ""c"": return obj.tostring() <mask>: return obj.tounicode() return obj.tolist() return self.class_to_dict(obj)",False,"if obj . typecode == ""u"" :","elif obj . typecode == ""b"" :",0.8820016898747209,58.14307369682194
"def run(self): self.run_command(""egg_info"") from glob import glob for pattern in self.match: pattern = self.distribution.get_name() + ""*"" + pattern files = glob(os.path.join(self.dist_dir, pattern)) files = [(os.path.getmtime(f), f) for f in files] files.sort() files.reverse() log.info(""%d file(s) matching %s"", len(files), pattern) files = files[self.keep :] for (t, f) in files: log.info(""Deleting %s"", f) <mask>: os.unlink(f)",False,if not self . dry_run :,if os . path . exists ( f ) :,0.8820016898747209,5.522397783539471
"def render_token_list(self, tokens): result = [] vars = [] for token in tokens: if token.token_type == TOKEN_TEXT: result.append(token.contents.replace(""%"", ""%%"")) <mask>: result.append(""%%(%s)s"" % token.contents) vars.append(token.contents) return """".join(result), vars",False,elif token . token_type == TOKEN_VAR :,elif token . token_type == TOKEN_VARIABLE :,0.8820016898747209,82.651681837938
"def _handle_raise(self, values, is_NAs, origins): for is_NA, origin in zip(is_NAs, origins): <mask>: msg = ( ""Missing values detected. If you want rows with missing "" ""values to be automatically deleted in a list-wise "" ""manner (not recommended), please set dropna=True in "" ""the Bambi Model initialization."" ) raise PatsyError(msg, origin) return values",False,if np . any ( is_NA ) :,if is_NA :,0.8820016898747209,16.62083000646927
"def add_node_data(node_array, ntwk): node_ntwk = nx.Graph() newdata = {} for idx, data in ntwk.nodes(data=True): <mask>: newdata[""value""] = node_array[int(idx) - 1] data.update(newdata) node_ntwk.add_node(int(idx), **data) return node_ntwk",False,if not int ( idx ) == 0 :,if int ( idx ) > 0 :,0.8820016898747209,33.27714551776234
"def safe_parse_date(date_hdr): """"""Parse a Date: or Received: header into a unix timestamp."""""" try: if "";"" in date_hdr: date_hdr = date_hdr.split("";"")[-1].strip() msg_ts = long(rfc822.mktime_tz(rfc822.parsedate_tz(date_hdr))) <mask>: return None else: return msg_ts except (ValueError, TypeError, OverflowError): return None",False,if ( msg_ts > ( time . time ( ) + 24 * 3600 ) ) or ( msg_ts < 1 ) :,if msg_ts < 0 :,0.8820016898747209,2.6131505234865924
"def _route_db(self, model, **hints): chosen_db = None for router in self.routers: try: method = getattr(router, action) except AttributeError: # If the router doesn't have a method, skip to the next one. pass else: chosen_db = method(model, **hints) <mask>: return chosen_db try: return hints[""instance""]._state.db or DEFAULT_DB_ALIAS except KeyError: return DEFAULT_DB_ALIAS",True,if chosen_db :,if chosen_db :,0.8820016898747209,100.00000000000004
"def get_keys(struct, ignore_first_level=False): res = [] if isinstance(struct, dict): if not ignore_first_level: keys = [x.split(""("")[0] for x in struct.keys()] res.extend(keys) for key in struct: <mask>: logging.debug(""Ignored: %s: %s"", key, struct[key]) continue res.extend(get_keys(struct[key], key in IGNORED_FIRST_LEVEL)) elif isinstance(struct, list): for item in struct: res.extend(get_keys(item)) return res",False,if key in IGNORED_KEYS :,if key in IGNORED_FIRST_LEVEL :,0.8820016898747209,46.713797772819994
"def launch_app(self, fs_id): if fs_id in self.app_infos: row = self.get_row_by_fsid(fs_id) <mask>: return app_info = self.app_infos[fs_id] filepath = os.path.join(row[SAVEDIR_COL], row[SAVENAME_COL]) gfile = Gio.File.new_for_path(filepath) app_info.launch( [ gfile, ], None, ) self.app_infos.pop(fs_id, None)",True,if not row :,if not row :,0.8820016898747209,100.00000000000004
"def create_skipfile(files_changed, skipfile): # File is likely to contain some garbage values at start, # only the corresponding json should be parsed. json_pattern = re.compile(r""^\{.*\}"") for line in files_changed.readlines(): <mask>: for filename in json.loads(line): if ""/COMMIT_MSG"" in filename: continue skipfile.write(""+*/%s\n"" % filename) skipfile.write(""-*\n"")",False,"if re . match ( json_pattern , line ) :","if re . search ( line , json_pattern ) :",0.8820016898747209,26.084743001221455
"def zscore(self, client, request, N): check_input(request, N != 2) key = request[1] db = client.db value = db.get(key) if value is None: client.reply_bulk(None) elif not isinstance(value, self.zset_type): client.reply_wrongtype() else: score = value.score(request[2], None) <mask>: score = str(score).encode(""utf-8"") client.reply_bulk(score)",False,if score is not None :,"if isinstance ( score , bytes ) :",0.8820016898747209,7.267884212102741
"def _list_cases(suite): for test in suite: if isinstance(test, unittest.TestSuite): _list_cases(test) elif isinstance(test, unittest.TestCase): <mask>: print(test.id())",False,if support . match_test ( test ) :,if test . id ( ) :,0.8820016898747209,11.260801105802155
"def Run(self): """"""The main run method of the client."""""" for thread in self._threads.values(): thread.start() logging.info(START_STRING) while True: dead_threads = [tn for (tn, t) in self._threads.items() if not t.isAlive()] <mask>: raise FatalError( ""These threads are dead: %r. Shutting down..."" % dead_threads ) time.sleep(10)",True,if dead_threads :,if dead_threads :,0.8820016898747209,100.00000000000004
"def _slice_queryset(queryset, order_by, per_page, start): page_len = int(per_page) + 1 if start: <mask>: filter_name = ""%s__lte"" % order_by[1:] else: filter_name = ""%s__gte"" % order_by return queryset.filter(**{filter_name: start})[:page_len] return queryset[:page_len]",False,"if order_by . startswith ( ""-"" ) :","if order_by . startswith ( ""start"" ) :",0.8820016898747209,73.48889200874659
"def compute_timer_precision(timer): precision = None points = 0 timeout = timeout_timer() + 1.0 previous = timer() while timeout_timer() < timeout or points < 5: for _ in XRANGE(10): t1 = timer() t2 = timer() dt = t2 - t1 <mask>: break else: dt = t2 - previous if dt <= 0.0: continue if precision is not None: precision = min(precision, dt) else: precision = dt points += 1 previous = timer() return precision",False,if 0 < dt :,if dt <= 0.0 :,0.8820016898747209,11.478744233307168
"def findWorkingDir(): frozen = getattr(sys, ""frozen"", """") if not frozen: path = os.path.dirname(__file__) elif frozen in (""dll"", ""console_exe"", ""windows_exe"", ""macosx_app""): path = os.path.dirname( os.path.dirname(os.path.dirname(os.path.dirname(__file__))) ) elif frozen: # needed for PyInstaller <mask>: path = getattr(sys, ""_MEIPASS"", """") # --onefile else: path = os.path.dirname(sys.executable) # --onedir else: path = """" return path",False,"if getattr ( sys , ""_MEIPASS"" , """" ) is not None :","if sys . platform == ""win32"" :",0.8820016898747209,3.115901613796704
"def CreateDataType(vmodlName, wsdlName, parent, version, props): with _lazyLock: dic = [vmodlName, wsdlName, parent, version, props] names = vmodlName.split(""."") <mask>: vmodlName = ""."".join(name[0].lower() + name[1:] for name in names) _AddToDependencyMap(names) typeNs = GetWsdlNamespace(version) _dataDefMap[vmodlName] = dic _wsdlDefMap[(typeNs, wsdlName)] = dic _wsdlTypeMapNSs.add(typeNs)",False,if _allowCapitalizedNames :,if len ( names ) > 1 :,0.8820016898747209,6.567274736060395
"def ParseResponses( self, knowledge_base: rdf_client.KnowledgeBase, responses: Iterable[rdfvalue.RDFValue], ) -> Iterator[rdf_client.User]: for response in responses: if not isinstance(response, rdf_client_fs.StatEntry): raise TypeError(f""Unexpected response type: `{type(response)}`"") # TODO: `st_mode` has to be an `int`, not `StatMode`. if stat.S_ISDIR(int(response.st_mode)): homedir = response.pathspec.path username = os.path.basename(homedir) <mask>: yield rdf_client.User(username=username, homedir=homedir)",False,if username not in self . _ignore_users :,if username in knowledge_base . users :,0.8820016898747209,12.41950196698629
"def process_question(qtxt): question = """" skip = False for letter in qtxt: if letter == ""<"": skip = True if letter == "">"": skip = False if skip: continue <mask>: if letter == "" "": letter = ""_"" question += letter.lower() return question",False,"if letter . isalnum ( ) or letter == "" "" :",if not skip :,0.8820016898747209,2.002152301552759
"def process_all(self, lines, times=1): gap = False for _ in range(times): for line in lines: if gap: self.write("""") self.process(line) <mask>: gap = True return 0",False,if not is_command ( line ) :,"if line . startswith ( ""\n"" ) :",0.8820016898747209,9.425159511373677
"def _get(self, domain): with self.lock: try: record = self.cache[domain] time_now = time.time() if time_now - record[""update""] > self.ttl: record = None except KeyError: record = None <mask>: record = {""r"": ""unknown"", ""dns"": {}, ""g"": 1, ""query_count"": 0} # self.cache[domain] = record return record",True,if not record :,if not record :,0.8820016898747209,100.00000000000004
"def gen_constant_folding(cw): types = [""Int32"", ""Double"", ""BigInteger"", ""Complex""] for cur_type in types: cw.enter_block(""if (constLeft.Value.GetType() == typeof(%s))"" % (cur_type,)) cw.enter_block(""switch (_op)"") for op in ops: gen = getattr(op, ""genConstantFolding"", None) <mask>: gen(cw, cur_type) cw.exit_block() cw.exit_block()",True,if gen is not None :,if gen is not None :,0.8820016898747209,100.00000000000004
"def unreferenced_dummy(self): for g, base in zip(self.evgroups, self.evbases): for ind, j in enumerate(g): <mask>: debug_print( ""replacing unreferenced %d %s with dummy"" % ((base + ind), g[ind]) ) g[ind] = ""dummy"" self.evnum[base + ind] = ""dummy""",False,if not self . indexobj [ base + ind ] :,if j == base + ind :,0.8820016898747209,16.0529461904344
"def handle_signature(self, sig: str, signode: desc_signature) -> Tuple[str, str]: for cls in self.__class__.__mro__: <mask>: warnings.warn( ""PyDecoratorMixin is deprecated. "" ""Please check the implementation of %s"" % cls, RemovedInSphinx50Warning, stacklevel=2, ) break else: warnings.warn( ""PyDecoratorMixin is deprecated"", RemovedInSphinx50Warning, stacklevel=2 ) ret = super().handle_signature(sig, signode) # type: ignore signode.insert(0, addnodes.desc_addname(""@"", ""@"")) return ret",False,"if cls . __name__ != ""DirectiveAdapter"" :",if cls . __name__ == sig :,0.8820016898747209,54.88684910025905
"def _iter_lines(path=path, response=response, max_next=options.http_max_next): path.responses = [] n = 0 while response: path.responses.append(response) yield from response.iter_lines(decode_unicode=True) src = response.links.get(""next"", {}).get(""url"", None) <mask>: break n += 1 if n > max_next: vd.warning(f""stopping at max {max_next} pages"") break vd.status(f""fetching next page from {src}"") response = requests.get(src, stream=True)",False,if not src :,if src is None :,0.8820016898747209,14.058533129758727
"def ordered_indices(self): with data_utils.numpy_seed(self.seed, self.epoch): # Used to store the order of indices of each dataset to use indices = [ np.random.permutation(len(dataset)) for dataset in self.datasets.values() ] # Keep track of which samples we've used for each dataset counters = [0 for _ in self.datasets] sampled_indices = [ self._sample(indices, counters) for _ in range(self.total_num_instances) ] <mask>: sampled_indices.sort(key=lambda i: self.num_tokens(i)) return np.array(sampled_indices, dtype=np.int64)",False,if self . sort_indices :,if self . num_tokens ( ) > 0 :,0.8820016898747209,15.851165692617148
"def _build_columns(self): self.columns = [Column() for col in self.keys] for row in self: for (col_idx, col_val) in enumerate(row): col = self.columns[col_idx] col.append(col_val) <mask>: col.is_quantity = False for (idx, key_name) in enumerate(self.keys): self.columns[idx].name = key_name self.x = Column() self.ys = []",False,if ( col_val is not None ) and ( not is_quantity ( col_val ) ) :,if col . is_quantity :,0.8820016898747209,3.4106484601338583
"def tearDown(self): subprocess_list = self.subprocess_list processes = subprocess_list.processes self.schedule.reset() del self.schedule for proc in processes: <mask>: terminate_process(proc.pid, kill_children=True, slow_stop=True) subprocess_list.cleanup() processes = subprocess_list.processes if processes: for proc in processes: if proc.is_alive(): terminate_process(proc.pid, kill_children=True, slow_stop=False) subprocess_list.cleanup() processes = subprocess_list.processes if processes: log.warning(""Processes left running: %s"", processes)",True,if proc . is_alive ( ) :,if proc . is_alive ( ) :,0.8820016898747209,100.00000000000004
"def colorNetwork(cls, network, nodesInNetwork, nodeByID=None): for node in nodesInNetwork: node.use_custom_color = True neededCopies = sum(socket.execution.neededCopies for socket in node.outputs) <mask>: color = (0.7, 0.9, 0.7) else: color = (1.0, 0.3, 0.3) node.color = color",False,if neededCopies == 0 :,if neededCopies == 1 :,0.8820016898747209,53.7284965911771
"def _init_warmup_scheduler(self, optimizer, states): updates_so_far = states.get(""number_training_updates"", 0) if self.warmup_updates > 0 and ( updates_so_far <= self.warmup_updates or self.hard_reset ): self.warmup_scheduler = optim.lr_scheduler.LambdaLR(optimizer, self._warmup_lr) <mask>: self.warmup_scheduler.load_state_dict(states[""warmup_scheduler""]) else: self.warmup_scheduler = None",False,"if states . get ( ""warmup_scheduler"" ) :","if ""warmup_scheduler"" in states :",0.8820016898747209,35.967895947086795
"def inner(self, *iargs, **ikwargs): try: return getattr(super(VEXResilienceMixin, self), func)(*iargs, **ikwargs) except excs as e: for exc, handler in zip(excs, handlers): if isinstance(e, exc): v = getattr(self, handler)(*iargs, **ikwargs) <mask>: raise return v assert False, ""this should be unreachable if Python is working correctly""",False,if v is raiseme :,if v is None :,0.8820016898747209,42.72870063962342
"def unwrap_envelope(self, data, many): if many: if data[""items""]: <mask>: self.context[""total""] = len(data) return data else: self.context[""total""] = data[""total""] else: self.context[""total""] = 0 data = {""items"": []} return data[""items""] return data",False,"if isinstance ( data , InstrumentedList ) or isinstance ( data , list ) :",if len ( data ) > 0 :,0.8820016898747209,5.789419402078114
"def __subclasscheck__(self, cls): if self.__origin__ is not None: <mask>: raise TypeError( ""Parameterized generics cannot be used with class "" ""or instance checks"" ) return False if self is Generic: raise TypeError( ""Class %r cannot be used with class "" ""or instance checks"" % self ) return super().__subclasscheck__(cls)",False,"if sys . _getframe ( 1 ) . f_globals [ ""__name__"" ] not in [ ""abc"" , ""functools"" ] :",if self is Generic :,0.8820016898747209,0.046975141313942724
"def __init__(self, pyversions, coverage_service): build_matrix = """" for version in pyversions: build_matrix += ""\n {},"".format( version <mask>: else ""py{}"".format("""".join(version.split("".""))) ) coverage_package = """" if coverage_service: coverage_package += ""\n {}"".format(coverage_service.package) coverage_package += ""\n"" super(Tox, self).__init__( ""tox.ini"", TEMPLATE.format(build_matrix=build_matrix, coverage_package=coverage_package), )",False,"if version . startswith ( ""pypy"" )","if version . startswith ( ""py"" )",0.8820016898747209,66.06328636027612
"def _get_app(self, body=None): app = self._app if app is None: try: tasks = self.tasks.tasks # is a group except AttributeError: tasks = self.tasks if len(tasks): app = tasks[0]._app <mask>: app = body._app return app if app is not None else current_app",False,if app is None and body is not None :,elif body :,0.8820016898747209,0.0
"def logic(): for v in [True, False, None, 0, True, None, None, 1]: yield clk.posedge xd.next = v <mask>: yd.next = zd.next = None elif v: yd.next = zd.next = 11 else: yd.next = zd.next = 0",False,if v is None :,if v :,0.8820016898747209,0.0
"def run(self): eid = self.start_episode() obs = self.env.reset() while True: <mask>: action = self.env.action_space.sample() self.log_action(eid, obs, action) else: action = self.get_action(eid, obs) obs, reward, done, info = self.env.step(action) self.log_returns(eid, reward, info=info) if done: self.end_episode(eid, obs) obs = self.env.reset() eid = self.start_episode()",False,if random . random ( ) < self . off_pol_frac :,if self . env . action_space :,0.8820016898747209,6.443030905386945
"def tearDown(self): os.chdir(self.orig_working_dir) sys.argv = self.orig_argv sys.stdout = self.orig_stdout sys.stderr = self.orig_stderr for dirname in [""lv_LV"", ""ja_JP""]: locale_dir = os.path.join(self.datadir, ""project"", ""i18n"", dirname) <mask>: shutil.rmtree(locale_dir)",True,if os . path . isdir ( locale_dir ) :,if os . path . isdir ( locale_dir ) :,0.8820016898747209,100.00000000000004
"def sentry_set_scope(process_context, entity, project, email=None, url=None): # Using GLOBAL_HUB means these tags will persist between threads. # Normally there is one hub per thread. with sentry_sdk.hub.GLOBAL_HUB.configure_scope() as scope: scope.set_tag(""process_context"", process_context) scope.set_tag(""entity"", entity) scope.set_tag(""project"", project) <mask>: scope.user = {""email"": email} if url: scope.set_tag(""url"", url)",True,if email :,if email :,0.8820016898747209,0.0
"def getDataMax(self): result = -Double.MAX_VALUE nCurves = self.chart.getNCurves() for i in range(nCurves): c = self.getSystemCurve(i) <mask>: continue if c.getYAxis() == Y_AXIS: nPoints = c.getNPoints() for j in range(nPoints): result = self.maxIgnoreNaNAndMaxValue(result, c.getPoint(j).getY()) if result == -Double.MAX_VALUE: return Double.NaN return result",False,if not c . isVisible ( ) :,if c is None :,0.8820016898747209,7.715486568024961
"def handle_starttag(self, tag, attrs): if tag == ""link"" and (""rel"", ""icon"") in attrs or (""rel"", ""shortcut icon"") in attrs: href = None icon_type = None for attr, value in attrs: if attr == ""href"": href = value elif attr == ""type"": icon_type = value <mask>: try: mimetype = extension_to_mimetype(href.rpartition(""."")[2]) except KeyError: pass else: icon_type = mimetype if icon_type: self.icons.append((href, icon_type))",False,if href :,"elif attr == ""type"" :",0.8820016898747209,5.522397783539471
"def get_version(version_file=STATIC_VERSION_FILE): version_info = get_static_version_info(version_file) version = version_info[""version""] if version == ""__use_git__"": version = get_version_from_git() <mask>: version = get_version_from_git_archive(version_info) if not version: version = Version(""unknown"", None, None) return pep440_format(version) else: return version",False,if not version :,"elif version == ""__use_git_archive__"" :",0.8820016898747209,2.908317710573757
"def _Sleep(self, seconds): if threading.current_thread() is not self._worker_thread: return self._original_sleep(seconds) self._time += seconds self._budget -= seconds while self._budget < 0: self._worker_thread_turn.clear() self._owner_thread_turn.set() self._worker_thread_turn.wait() <mask>: raise FakeTimeline._WorkerThreadExit()",False,if self . _worker_thread_done :,if self . _time >= self . _max_time :,0.8820016898747209,20.448007360218387
"def validate_attributes(self): if not (self.has_variants or self.variant_of): return if not self.variant_based_on: self.variant_based_on = ""Item Attribute"" if self.variant_based_on == ""Item Attribute"": attributes = [] <mask>: frappe.throw(_(""Attribute table is mandatory"")) for d in self.attributes: if d.attribute in attributes: frappe.throw( _( ""Attribute {0} selected multiple times in Attributes Table"" ).format(d.attribute) ) else: attributes.append(d.attribute)",True,if not self . attributes :,if not self . attributes :,0.8820016898747209,100.00000000000004
"def check_digest_auth(user, passwd): """"""Check user authentication using HTTP Digest auth"""""" if request.headers.get(""Authorization""): credentails = parse_authorization_header(request.headers.get(""Authorization"")) if not credentails: return response_hash = response( credentails, passwd, dict( uri=request.script_root + request.path, body=request.data, method=request.method, ), ) <mask>: return True return False",False,"if credentails . get ( ""response"" ) == response_hash :",if response_hash == user :,0.8820016898747209,11.720937028376891
"def _get_index_type(return_index_type, ctx): if return_index_type is None: # pragma: no cover if ctx.running_mode == RunningMode.local: return_index_type = ""object"" <mask>: return_index_type = ""filename"" else: return_index_type = ""bytes"" return return_index_type",False,elif ctx . running_mode == RunningMode . local_cluster :,elif ctx . running_mode == RunningMode . local_filename :,0.8820016898747209,85.5526185871245
"def iter_event_handlers( self, resource: resources_.Resource, event: bodies.RawEvent, ) -> Iterator[handlers.ResourceWatchingHandler]: warnings.warn( ""SimpleRegistry.iter_event_handlers() is deprecated; use "" ""ResourceWatchingRegistry.iter_handlers()."", DeprecationWarning, ) cause = _create_watching_cause(resource, event) for handler in self._handlers: if not isinstance(handler, handlers.ResourceWatchingHandler): pass <mask>: yield handler",False,"elif registries . match ( handler = handler , cause = cause , ignore_fields = True ) :",if cause . is_watching ( handler ) :,0.8820016898747209,5.021776686519537
"def subprocess_post_check( completed_process: subprocess.CompletedProcess, raise_error: bool = True ) -> None: if completed_process.returncode: <mask>: print(completed_process.stdout, file=sys.stdout, end="""") if completed_process.stderr is not None: print(completed_process.stderr, file=sys.stderr, end="""") if raise_error: raise PipxError( f""{' '.join([str(x) for x in completed_process.args])!r} failed"" ) else: logger.info(f""{' '.join(completed_process.args)!r} failed"")",True,if completed_process . stdout is not None :,if completed_process . stdout is not None :,0.8820016898747209,100.00000000000004
"def __pow__(self, power): if power == 1: return self if power == -1: # HACK: break cycle from cirq.devices import line_qubit decomposed = protocols.decompose_once_with_qubits( self, qubits=line_qubit.LineQid.for_gate(self), default=None ) <mask>: return NotImplemented inverse_decomposed = protocols.inverse(decomposed, None) if inverse_decomposed is None: return NotImplemented return _InverseCompositeGate(self) return NotImplemented",True,if decomposed is None :,if decomposed is None :,0.8820016898747209,100.00000000000004
"def tearDown(self): """"""Close the application after tests"""""" # set it back to it's old position so not to annoy users :-) self.old_pos = self.dlg.rectangle # close the application self.dlg.menu_select(""File->Exit"") try: <mask>: self.app.UntitledNotepad[""Do&n't Save""].click() self.app.UntitledNotepad.wait_not(""visible"") except Exception: pass finally: self.app.kill()",False,"if self . app . UntitledNotepad [ ""Do&n't Save"" ] . exists ( ) :",if self . app . UntitledNotepad :,0.8820016898747209,15.14389796999661
"def terminate_subprocess(proc, timeout=0.1, log=None): <mask>: if log: log.info(""Sending SIGTERM to %r"", proc) proc.terminate() timeout_time = time.time() + timeout while proc.poll() is None and time.time() < timeout_time: time.sleep(0.02) if proc.poll() is None: if log: log.info(""Sending SIGKILL to %r"", proc) proc.kill() return proc.returncode",True,if proc . poll ( ) is None :,if proc . poll ( ) is None :,0.8820016898747209,100.00000000000004
"def validate(self, detection, expectation): config = SigmaConfiguration() self.basic_rule[""detection""] = detection with patch(""yaml.safe_load_all"", return_value=[self.basic_rule]): parser = SigmaCollectionParser(""any sigma io"", config, None) backend = SQLiteBackend(config, self.table) assert len(parser.parsers) == 1 for p in parser.parsers: <mask>: self.assertEqual(expectation, backend.generate(p)) elif isinstance(expectation, Exception): self.assertRaises(type(expectation), backend.generate, p)",True,"if isinstance ( expectation , str ) :","if isinstance ( expectation , str ) :",0.8820016898747209,100.00000000000004
"def makelist(d): """"""Convert d into a list if all the keys of d are integers."""""" if isinstance(d, dict): <mask>: return [makelist(d[k]) for k in sorted(d, key=int)] else: return web.storage((k, makelist(v)) for k, v in d.items()) else: return d",False,if all ( isint ( k ) for k in d ) :,"if isinstance ( d , list ) :",0.8820016898747209,7.433761660133445
"def __share_local_dir(self, lpath, rpath, fast): result = const.ENoError for walk in self.__walk_normal_file(lpath): (dirpath, dirnames, filenames) = walk for filename in filenames: rpart = os.path.relpath(dirpath, lpath) if rpart == ""."": rpart = """" subr = self.__share_local_file( joinpath(dirpath, filename), posixpath.join(rpath, rpart, filename), fast, ) <mask>: result = subr return result",False,if subr != const . ENoError :,if subr :,0.8820016898747209,0.0
"def _targets(self, sigmaparser): # build list of matching target mappings targets = set() for condfield in self.conditions: <mask>: rulefieldvalues = sigmaparser.values[condfield] for condvalue in self.conditions[condfield]: if condvalue in rulefieldvalues: targets.update(self.conditions[condfield][condvalue]) return targets",True,if condfield in sigmaparser . values :,if condfield in sigmaparser . values :,0.8820016898747209,100.00000000000004
"def _wrapped_view(request, *args, **kwargs): # based on authority/decorators.py user = request.user if user.is_authenticated(): obj = _resolve_lookup(obj_lookup, kwargs) perm_obj = _resolve_lookup(perm_obj_lookup, kwargs) granted = access.has_perm_or_owns(user, perm, obj, perm_obj, owner_attr) <mask>: return view_func(request, *args, **kwargs) # In all other cases, permission denied return HttpResponseForbidden()",False,if granted or user . has_perm ( perm ) :,if granted :,0.8820016898747209,0.0
"def assert_parts_cleaned(self, earlier_parts, current_parts, expected_parts, hint): cleaned_parts = [] for earlier in earlier_parts: earlier_part = earlier[""part""] earlier_step = earlier[""step""] found = False for current in current_parts: <mask>: found = True break if not found: cleaned_parts.append(dict(part=earlier_part, step=earlier_step)) self.assertThat(cleaned_parts, HasLength(len(expected_parts)), hint) for expected in expected_parts: self.assertThat(cleaned_parts, Contains(expected), hint)",False,"if earlier_part == current [ ""part"" ] and earlier_step == current [ ""step"" ] :",if earlier_part == current and earlier_step == current :,0.8820016898747209,41.804688830601805
"def show_image(self, wnd_name, img): if wnd_name in self.named_windows: if self.named_windows[wnd_name] == 0: self.named_windows[wnd_name] = 1 self.on_create_window(wnd_name) <mask>: self.capture_mouse(wnd_name) self.on_show_image(wnd_name, img) else: print(""show_image: named_window "", wnd_name, "" not found."")",False,if wnd_name in self . capture_mouse_windows :,elif self . named_windows [ wnd_name ] == 1 :,0.8820016898747209,14.865996369027277
"def readlines(self, hint=None): # Again, allow hint but ignore body = self._get_body() rest = body[self.position :] self.position = len(body) result = [] while 1: next = rest.find(""\r\n"") <mask>: result.append(rest) break result.append(rest[: next + 2]) rest = rest[next + 2 :] return result",True,if next == - 1 :,if next == - 1 :,0.8820016898747209,100.00000000000004
"def __lt__(self, other): olen = len(other) for i in range(olen): try: c = self[i] < other[i] except IndexError: # self must be shorter return True if c: return c <mask>: return False return len(self) < olen",False,elif other [ i ] < self [ i ] :,if len ( self ) < olen :,0.8820016898747209,4.995138898472386
"def social_user(backend, uid, user=None, *args, **kwargs): provider = backend.name social = backend.strategy.storage.user.get_social_auth(provider, uid) if social: if user and social.user != user: msg = ""This account is already in use."" raise AuthAlreadyAssociated(backend, msg) <mask>: user = social.user return { ""social"": social, ""user"": user, ""is_new"": user is None, ""new_association"": social is None, }",False,elif not user :,if user is None :,0.8820016898747209,12.703318703865365
"def markUVs(self, indices=None): if isinstance(indices, tuple): indices = indices[0] ntexco = len(self.texco) if indices is None: self.utexc = True else: if self.utexc is False: self.utexc = np.zeros(ntexco, dtype=bool) <mask>: self.utexc[indices] = True",False,if self . utexc is not True :,if indices is not None :,0.8820016898747209,13.83254362586636
"def destination(self, type, name, arglist): classname = ""ResFunction"" listname = ""functions"" if arglist: t, n, m = arglist[0] <mask>: classname = ""ResMethod"" listname = ""resmethods"" return classname, listname",False,"if t == ""Handle"" and m == ""InMode"" :","if t == ""ResMethod"" :",0.8820016898747209,24.786763988804346
"def select(self, regions, register): self.view.sel().clear() to_store = [] for r in regions: self.view.sel().add(r) if register: to_store.append(self.view.substr(self.view.full_line(r))) if register: text = """".join(to_store) <mask>: text = text + ""\n"" state = State(self.view) state.registers[register] = [text]",False,"if not text . endswith ( ""\n"" ) :",if not self . view . is_empty ( ) :,0.8820016898747209,10.600313379512592
"def _skip_start(self): start, stop = self.start, self.stop for chunk in self.app_iter: self._pos += len(chunk) if self._pos < start: continue <mask>: return b"""" else: chunk = chunk[start - self._pos :] if stop is not None and self._pos > stop: chunk = chunk[: stop - self._pos] assert len(chunk) == stop - start return chunk else: raise StopIteration()",False,elif self . _pos == start :,if start is None :,0.8820016898747209,5.70796903405875
"def start(self): self.on_config_change() self.start_config_watch() try: if self.config[""MITMf""][""DNS""][""tcp""].lower() == ""on"": self.startTCP() else: self.startUDP() except socket.error as e: <mask>: shutdown( ""\n[DNS] Unable to start DNS server on port {}: port already in use"".format( self.config[""MITMf""][""DNS""][""port""] ) )",False,"if ""Address already in use"" in e :",if e . errno == errno . EADDRINUSE :,0.8820016898747209,5.522397783539471
"def ignore(self, other): if isinstance(other, Suppress): if other not in self.ignoreExprs: super(ParseElementEnhance, self).ignore(other) <mask>: self.expr.ignore(self.ignoreExprs[-1]) else: super(ParseElementEnhance, self).ignore(other) if self.expr is not None: self.expr.ignore(self.ignoreExprs[-1]) return self",True,if self . expr is not None :,if self . expr is not None :,0.8820016898747209,100.00000000000004
"def test_relative_deploy_path_override(): s = Site(TEST_SITE_ROOT) s.load() res = s.content.resource_from_relative_path( ""blog/2010/december/merry-christmas.html"" ) res.relative_deploy_path = ""blog/2010/december/happy-holidays.html"" for page in s.content.walk_resources(): <mask>: assert page.relative_deploy_path == ""blog/2010/december/happy-holidays.html"" else: assert page.relative_deploy_path == Folder(page.relative_path)",False,if res . source_file == page . source_file :,"if page . relative_path == ""blog/2010/december/merry-christmas.html"" :",0.8820016898747209,6.439931429457922
"def _parser(cls, buf): tlvs = [] while buf: tlv_type = LLDPBasicTLV.get_type(buf) tlv = cls._tlv_parsers[tlv_type](buf) tlvs.append(tlv) offset = LLDP_TLV_SIZE + tlv.len buf = buf[offset:] <mask>: break assert len(buf) > 0 lldp_pkt = cls(tlvs) assert lldp_pkt._tlvs_len_valid() assert lldp_pkt._tlvs_valid() return lldp_pkt, None, buf",False,if tlv . tlv_type == LLDP_TLV_END :,if not tlv_type :,0.8820016898747209,9.049145405312009
"def _do_pull(self, repo, pull_kwargs, silent, ignore_pull_failures): try: output = self.client.pull(repo, **pull_kwargs) if silent: with open(os.devnull, ""w"") as devnull: yield from stream_output(output, devnull) else: yield from stream_output(output, sys.stdout) except (StreamOutputError, NotFound) as e: <mask>: raise else: log.error(str(e))",False,if not ignore_pull_failures :,if ignore_pull_failures :,0.8820016898747209,72.89545183625967
def _collect_bytecode(ordered_code): bytecode_blocks = [] stack = [ordered_code] while stack: code = stack.pop() bytecode_blocks.append(code.co_code) for const in code.co_consts: <mask>: stack.append(const) return bytecode_blocks,False,"if isinstance ( const , blocks . OrderedCode ) :",if const . co_code == ordered_code :,0.8820016898747209,4.789232204309912
"def displayhook(value): if value is None: return builtins = modules[""builtins""] # Set '_' to None to avoid recursion builtins._ = None text = repr(value) try: local_stdout = stdout except NameError as e: raise RuntimeError(""lost sys.stdout"") from e try: local_stdout.write(text) except UnicodeEncodeError: bytes = text.encode(local_stdout.encoding, ""backslashreplace"") <mask>: local_stdout.buffer.write(bytes) else: text = bytes.decode(local_stdout.encoding, ""strict"") local_stdout.write(text) local_stdout.write(""\n"") builtins._ = value",False,"if hasattr ( local_stdout , ""buffer"" ) :",if PY3 :,0.8820016898747209,0.0
"def _analyze(self): lines = open(self.log_path, ""r"").readlines() prev_line = None for line in lines: if line.startswith(""ERROR:"") and prev_line and prev_line.startswith(""=""): self.errors.append(line[len(""ERROR:"") :].strip()) <mask>: self.failures.append(line[len(""FAIL:"") :].strip()) prev_line = line",True,"elif line . startswith ( ""FAIL:"" ) and prev_line and prev_line . startswith ( ""="" ) :","elif line . startswith ( ""FAIL:"" ) and prev_line and prev_line . startswith ( ""="" ) :",0.8820016898747209,100.00000000000004
"def _flush(self): if self._data: if self._last is not None: text = """".join(self._data) <mask>: assert self._last.tail is None, ""internal error (tail)"" self._last.tail = text else: assert self._last.text is None, ""internal error (text)"" self._last.text = text self._data = []",False,if self . _tail :,if self . _tail is not None :,0.8820016898747209,46.713797772819994
"def write(self, chunk): consumer = self._current_consumer server_side = consumer.server_side if server_side: server_side.data_received(chunk) else: consumer.message += chunk assert consumer.in_parser.execute(chunk, len(chunk)) == len(chunk) <mask>: consumer.finished()",False,if consumer . in_parser . is_message_complete ( ) :,if consumer . finished :,0.8820016898747209,5.78270080339587
"def _api_change_cat(name, output, kwargs): """"""API: accepts output, value(=nzo_id), value2(=category)"""""" value = kwargs.get(""value"") value2 = kwargs.get(""value2"") if value and value2: nzo_id = value cat = value2 <mask>: cat = None result = sabnzbd.NzbQueue.change_cat(nzo_id, cat) return report(output, keyword=""status"", data=bool(result > 0)) else: return report(output, _MSG_NO_VALUE)",False,"if cat == ""None"" :","if cat == ""category"" :",0.8820016898747209,59.4603557501361
"def get_allocated_address( self, config: ActorPoolConfig, allocated: allocated_type ) -> str: addresses = config.get_external_addresses(label=self.label) for addr in addresses: occupied = False for strategy, _ in allocated.get(addr, dict()).values(): <mask>: occupied = True break if not occupied: return addr raise NoIdleSlot( f""No idle slot for creating actor "" f""with label {self.label}, mark {self.mark}"" )",False,if strategy == self :,if strategy . is_available ( ) :,0.8820016898747209,10.552670315936318
"def schedule_logger(job_id=None, delete=False): if not job_id: return getLogger(""fate_flow_schedule"") else: <mask>: with LoggerFactory.lock: try: for key in LoggerFactory.schedule_logger_dict.keys(): if job_id in key: del LoggerFactory.schedule_logger_dict[key] except: pass return True key = job_id + ""schedule"" if key in LoggerFactory.schedule_logger_dict: return LoggerFactory.schedule_logger_dict[key] return LoggerFactory.get_schedule_logger(job_id)",True,if delete :,if delete :,0.8820016898747209,0.0
"def quick_load(tool_file, async_load=True): try: tool = self.load_tool(tool_file, tool_cache_data_dir) self.__add_tool(tool, load_panel_dict, elems) # Always load the tool into the integrated_panel_dict, or it will not be included in the integrated_tool_panel.xml file. key = ""tool_%s"" % str(tool.id) integrated_elems[key] = tool <mask>: self._load_tool_panel() self._save_integrated_tool_panel() return tool.id except Exception: log.exception(""Failed to load potential tool %s."", tool_file) return None",True,if async_load :,if async_load :,0.8820016898747209,100.00000000000004
"def _get_default_ordering(self): try: ordering = super(DocumentChangeList, self)._get_default_ordering() except AttributeError: ordering = [] if self.model_admin.ordering: ordering = self.model_admin.ordering <mask>: ordering = self.lookup_opts.ordering return ordering",False,elif self . lookup_opts . ordering :,if self . lookup_opts . ordering :,0.8820016898747209,86.33400213704509
"def names(self, persistent=None): u = set() result = [] for s in [ self.__storage(None), self.__storage(self.__category), ]: for b in s: <mask>: continue if b.name.startswith(""__""): continue if b.name not in u: result.append(b.name) u.add(b.name) return result",False,if persistent is not None and b . persistent != persistent :,if b . persistent :,0.8820016898747209,10.094825899732768
"def common_check_get_messages_query( self, query_params: Dict[str, object], expected: str ) -> None: user_profile = self.example_user(""hamlet"") request = POSTRequestMock(query_params, user_profile) with queries_captured() as queries: get_messages_backend(request, user_profile) for query in queries: <mask>: sql = str(query[""sql""]).replace("" /* get_messages */"", """") self.assertEqual(sql, expected) return raise AssertionError(""get_messages query not found"")",False,"if ""/* get_messages */"" in query [ ""sql"" ] :","if query [ ""sql"" ] :",0.8820016898747209,24.909923021496894
"def _activate_only_current_top_active(): for i in range(0, len(current_sequence().tracks) - 1): <mask>: current_sequence().tracks[i].active = True else: current_sequence().tracks[i].active = False gui.tline_column.widget.queue_draw()",False,if i == current_sequence ( ) . get_first_active_track ( ) . id :,if current_sequence ( ) . tracks [ i ] . active == False :,0.8820016898747209,26.661724799024192
"def http_wrapper(self, url, postdata={}): try: if postdata != {}: f = urllib.urlopen(url, postdata) else: f = urllib.urlopen(url) response = f.read() except: import traceback import logging, sys cla, exc, tb = sys.exc_info() logging.error(url) <mask>: logging.error(""with post data"") else: logging.error(""without post data"") logging.error(exc.args) logging.error(traceback.format_tb(tb)) response = """" return response",False,if postdata :,"if exc . args [ 0 ] == ""POST"" :",0.8820016898747209,3.673526562988939
"def frequent_thread_switches(): """"""Make concurrency bugs more likely to manifest."""""" interval = None <mask>: if hasattr(sys, ""getswitchinterval""): interval = sys.getswitchinterval() sys.setswitchinterval(1e-6) else: interval = sys.getcheckinterval() sys.setcheckinterval(1) try: yield finally: if not sys.platform.startswith(""java""): if hasattr(sys, ""setswitchinterval""): sys.setswitchinterval(interval) else: sys.setcheckinterval(interval)",True,"if not sys . platform . startswith ( ""java"" ) :","if not sys . platform . startswith ( ""java"" ) :",0.8820016898747209,100.00000000000004
"def iter_filters(filters, block_end=False): queue = deque(filters) while queue: f = queue.popleft() <mask>: if block_end: queue.appendleft(None) for gf in f.filters: queue.appendleft(gf) yield f",False,"if f is not None and f . type in ( ""or"" , ""and"" , ""not"" ) :","if isinstance ( f , Filter ) :",0.8820016898747209,1.9672023367516605
"def smartsplit(code): """"""Split `code` at "" symbol, only if it is not escaped."""""" strings = [] pos = 0 while pos < len(code): <mask>: word = """" # new word pos += 1 while pos < len(code): if code[pos] == '""': break if code[pos] == ""\\"": word += ""\\"" pos += 1 word += code[pos] pos += 1 strings.append('""%s""' % word) pos += 1 return strings",True,"if code [ pos ] == '""' :","if code [ pos ] == '""' :",0.8820016898747209,100.00000000000004
"def get_folder_content(cls, name): """"""Return (folders, files) for the given folder in the root dir."""""" folders = set() files = set() for path in cls.LAYOUT: <mask>: continue parts = path.split(""/"") if len(parts) == 2: files.add(parts[1]) else: folders.add(parts[1]) folders = list(folders) folders.sort() files = list(files) files.sort() return (folders, files)",False,"if not path . startswith ( name + ""/"" ) :",if not path . startswith ( name ) :,0.8820016898747209,51.51425457345961
"def array_for(self, i): if 0 <= i < self._cnt: <mask>: return self._tail node = self._root level = self._shift while level > 0: assert isinstance(node, Node) node = node._array[(i >> level) & 0x01F] level -= 5 assert isinstance(node, Node) return node._array affirm(False, u""Index out of Range"")",False,if i >= self . tailoff ( ) :,if i >= self . _cnt :,0.8820016898747209,54.627576446464936
"def __or__(self, other) -> ""MultiVector"": r""""""``self | other``, the inner product :math:`M \cdot N`"""""" other, mv = self._checkOther(other) if mv: newValue = self.layout.imt_func(self.value, other.value) else: <mask>: obj = self.__array__() return obj | other # l * M = M * l = 0 for scalar l return self._newMV(dtype=np.result_type(self.value.dtype, other)) return self._newMV(newValue)",False,"if isinstance ( other , np . ndarray ) :",if self . __array__ ( ) :,0.8820016898747209,9.425159511373677
"def parse_bzr_stats(status): stats = RepoStats() statustype = ""changed"" for statusline in status: if statusline[:2] == "" "": setattr(stats, statustype, getattr(stats, statustype) + 1) <mask>: statustype = ""staged"" elif statusline == ""unknown:"": statustype = ""new"" else: # removed, missing, renamed, modified or kind changed statustype = ""changed"" return stats",False,"elif statusline == ""added:"" :","elif statusline == ""staged:"" :",0.8820016898747209,59.694917920196445
"def write(self, timestamps, actualValues, predictedValues, predictionStep=1): assert len(timestamps) == len(actualValues) == len(predictedValues) for index in range(len(self.names)): timestamp = timestamps[index] actual = actualValues[index] prediction = predictedValues[index] writer = self.outputWriters[index] <mask>: outputRow = [timestamp, actual, prediction] writer.writerow(outputRow) self.lineCounts[index] += 1",False,if timestamp is not None :,if predictionStep == 1 :,0.8820016898747209,9.652434877402245
"def clean(self): """"""Delete old files in ""tmp""."""""" now = time.time() for entry in os.listdir(os.path.join(self._path, ""tmp"")): path = os.path.join(self._path, ""tmp"", entry) <mask>: # 60 * 60 * 36 os.remove(path)",False,if now - os . path . getatime ( path ) > 129600 :,if now - os . path . getmtime ( path ) > self . _max_age :,0.8820016898747209,44.89771072202119
"def _get_info(self, path): info = OrderedDict() if not self._is_mac() or self._has_xcode_tools(): stdout = None try: stdout, stderr = Popen( [self._find_binary(), ""info"", os.path.realpath(path)], stdout=PIPE, stderr=PIPE, ).communicate() except OSError: pass else: <mask>: for line in stdout.splitlines(): line = u(line).split("": "", 1) if len(line) == 2: info[line[0]] = line[1] return info",True,if stdout :,if stdout :,0.8820016898747209,0.0
"def add(meta_list, info_list=None): if not info_list: info_list = meta_list if not isinstance(meta_list, (list, tuple)): meta_list = (meta_list,) if not isinstance(info_list, (list, tuple)): info_list = (info_list,) for info_f in info_list: <mask>: for meta_f in meta_list: metadata[meta_f] = info[info_f] break",False,if info . get ( info_f ) is not None :,if info_f not in metadata :,0.8820016898747209,14.481023414005424
"def _compute_log_r(model_trace, guide_trace): log_r = MultiFrameTensor() stacks = get_plate_stacks(model_trace) for name, model_site in model_trace.nodes.items(): <mask>: log_r_term = model_site[""log_prob""] if not model_site[""is_observed""]: log_r_term = log_r_term - guide_trace.nodes[name][""log_prob""] log_r.add((stacks[name], log_r_term.detach())) return log_r",False,"if model_site [ ""type"" ] == ""sample"" :","if ""log_prob"" in model_site :",0.8820016898747209,11.985825441691155
"def pickline(file, key, casefold=1): try: f = open(file, ""r"") except IOError: return None pat = re.escape(key) + "":"" prog = re.compile(pat, casefold and re.IGNORECASE) while 1: line = f.readline() if not line: break if prog.match(line): text = line[len(key) + 1 :] while 1: line = f.readline() <mask>: break text = text + line return text.strip() return None",False,if not line or not line [ 0 ] . isspace ( ) :,if not line :,0.8820016898747209,5.2447643832804935
"def build_iterator(data, infinite=True): """"""Build the iterator for inputs."""""" index = 0 size = len(data[0]) while True: if index + batch_size > size: <mask>: index = 0 else: return yield data[0][index : index + batch_size], data[1][index : index + batch_size] index += batch_size",True,if infinite :,if infinite :,0.8820016898747209,0.0
"def checkall(g, bg, dst_nodes, include_dst_in_src=True): for etype in g.etypes: ntype = g.to_canonical_etype(etype)[2] <mask>: check(g, bg, ntype, etype, dst_nodes[ntype], include_dst_in_src) else: check(g, bg, ntype, etype, None, include_dst_in_src)",False,if dst_nodes is not None and ntype in dst_nodes :,if ntype in dst_nodes :,0.8820016898747209,30.93485033266056
"def minimalBases(classes): """"""Reduce a list of base classes to its ordered minimum equivalent"""""" if not __python3: # pragma: no cover classes = [c for c in classes if c is not ClassType] candidates = [] for m in classes: for n in classes: if issubclass(n, m) and m is not n: break else: # m has no subclasses in 'classes' <mask>: candidates.remove(m) # ensure that we're later in the list candidates.append(m) return candidates",True,if m in candidates :,if m in candidates :,0.8820016898747209,100.00000000000004
"def __keep_songs_enable(self, enabled): config.set(""memory"", ""queue_keep_songs"", enabled) if enabled: self.queue.set_first_column_type(CurrentColumn) else: for col in self.queue.get_columns(): # Remove the CurrentColum if it exists <mask>: self.queue.set_first_column_type(None) break",False,"if isinstance ( col , CurrentColumn ) :",if col . type == CurrentCol :,0.8820016898747209,7.267884212102741
"def outlineView_heightOfRowByItem_(self, tree, item) -> float: default_row_height = self.rowHeight if item is self: return default_row_height heights = [default_row_height] for column in self.tableColumns: value = getattr(item.attrs[""node""], str(column.identifier)) <mask>: # if the cell value is a widget, use its height heights.append(value._impl.native.intrinsicContentSize().height) return max(heights)",False,"if isinstance ( value , toga . Widget ) :",if value is not None :,0.8820016898747209,5.484411595600381
"def condition(self): if self.__condition is None: <mask>: # Avoid an extra indirection in the common case of only one condition. self.__condition = self.flat_conditions[0] elif len(self.flat_conditions) == 0: # Possible, if unlikely, due to filter predicate rewriting self.__condition = lambda _: True else: self.__condition = lambda x: all(cond(x) for cond in self.flat_conditions) return self.__condition",True,if len ( self . flat_conditions ) == 1 :,if len ( self . flat_conditions ) == 1 :,0.8820016898747209,100.00000000000004
"def _find_delimiter(f, block_size=2 ** 16): delimiter = b""\n"" if f.tell() == 0: return 0 while True: b = f.read(block_size) if not b: return f.tell() <mask>: return f.tell() - len(b) + b.index(delimiter) + 1",False,elif delimiter in b :,if b . startswith ( delimiter ) :,0.8820016898747209,7.267884212102741
"def serialize(self, name=None): data = super(SimpleText, self).serialize(name) data[""contentType""] = self.contentType data[""content""] = self.content if self.width: <mask>: raise InvalidWidthException(self.width) data[""inputOptions""] = {} data[""width""] = self.width return data",False,"if self . width not in [ 100 , 50 , 33 , 25 ] :","if self . width not in ( ""0"" , ""1"" ) :",0.8820016898747209,32.37722713145643
"def inference(self): self.attention_weight_dim = self.input_dims[0][-1] if self.keep_dim: self.output_dim = copy.deepcopy(self.input_dims[0]) else: self.output_dim = [] for idx, dim in enumerate(self.input_dims[0]): <mask>: self.output_dim.append(dim) super( LinearAttentionConf, self ).inference() # PUT THIS LINE AT THE END OF inference()",False,if idx != len ( self . input_dims [ 0 ] ) - 2 :,if idx == self . attention_weight_dim :,0.8820016898747209,6.682025998431122
"def __delete_hook(self, rpc): try: rpc.check_success() except apiproxy_errors.Error: return None result = [] for status in rpc.response.delete_status_list(): if status == MemcacheDeleteResponse.DELETED: result.append(DELETE_SUCCESSFUL) <mask>: result.append(DELETE_ITEM_MISSING) else: result.append(DELETE_NETWORK_FAILURE) return result",False,elif status == MemcacheDeleteResponse . NOT_FOUND :,elif status == MemcacheDeleteResponse . MISSING :,0.8820016898747209,55.0695314903184
def identify_page_at_cursor(self): for region in self.view.sel(): text_on_cursor = None pos = region.begin() scope_region = self.view.extract_scope(pos) <mask>: text_on_cursor = self.view.substr(scope_region) return text_on_cursor.strip(string.punctuation) return None,False,if not scope_region . empty ( ) :,if scope_region :,0.8820016898747209,16.62083000646927
"def from_elem(cls, parent, when_elem): """"""Loads the proper when by attributes of elem"""""" when_value = when_elem.get(""value"", None) <mask>: return ValueToolOutputActionConditionalWhen(parent, when_elem, when_value) else: when_value = when_elem.get(""datatype_isinstance"", None) if when_value is not None: return DatatypeIsInstanceToolOutputActionConditionalWhen( parent, when_elem, when_value ) raise TypeError(""When type not implemented"")",True,if when_value is not None :,if when_value is not None :,0.8820016898747209,100.00000000000004
"def test_insert_entity_empty_string_rk( self, tables_cosmos_account_name, tables_primary_cosmos_account_key ): # Arrange await self._set_up(tables_cosmos_account_name, tables_primary_cosmos_account_key) try: entity = {""PartitionKey"": ""pk"", ""RowKey"": """"} # Act with pytest.raises(HttpResponseError): await self.table.create_entity(entity=entity) # Assert # assert resp is None finally: await self._tear_down() <mask>: sleep(SLEEP_DELAY)",False,if self . is_live :,if self . _is_running ( ) :,0.8820016898747209,20.556680845025987
"def provider_uris(self): login_urls = {} continue_url = self.request.get(""continue_url"") for provider in self.provider_info: <mask>: login_url = self.uri_for( ""social-login"", provider_name=provider, continue_url=continue_url ) else: login_url = self.uri_for(""social-login"", provider_name=provider) login_urls[provider] = login_url return login_urls",True,if continue_url :,if continue_url :,0.8820016898747209,100.00000000000004
"def expand_extensions(existing): for name in extension_names: ext = ( im(""lizard_ext.lizard"" + name.lower()).LizardExtension() <mask>: else name ) existing.insert( len(existing) if not hasattr(ext, ""ordering_index"") else ext.ordering_index, ext, ) return existing",False,"if isinstance ( name , str )",if name . lower ( ) in existing,0.8820016898747209,7.809849842300637
"def wrapper(self, *args, **kwargs): if not self.request.path.endswith(""/""): if self.request.method in (""GET"", ""HEAD""): uri = self.request.path + ""/"" <mask>: uri += ""?"" + self.request.query self.redirect(uri, permanent=True) return raise HTTPError(404) return method(self, *args, **kwargs)",True,if self . request . query :,if self . request . query :,0.8820016898747209,100.00000000000004
"def subword_map_by_joiner(subwords, marker=SubwordMarker.JOINER): """"""Return word id for each subword token (annotate by joiner)."""""" flags = [0] * len(subwords) for i, tok in enumerate(subwords): <mask>: flags[i] = 1 if tok.startswith(marker): assert i >= 1 and flags[i - 1] != 1, ""Sentence `{}` not correct!"".format( "" "".join(subwords) ) flags[i - 1] = 1 marker_acc = list(accumulate([0] + flags[:-1])) word_group = [(i - maker_sofar) for i, maker_sofar in enumerate(marker_acc)] return word_group",False,if tok . endswith ( marker ) :,if tok . startswith ( marker ) :,0.8820016898747209,50.000000000000014
"def next_item(self, direction): """"""Selects next menu item, based on self._direction"""""" start, i = -1, 0 try: start = self.items.index(self._selected) i = start + direction except: pass while True: if i == start: # Cannot find valid menu item self.select(start) break if i >= len(self.items): i = 0 continue if i < 0: i = len(self.items) - 1 continue <mask>: break i += direction if start < 0: start = 0",False,if self . select ( i ) :,if self . items [ i ] . selected == self . _selected :,0.8820016898747209,10.343603005129705
"def get_config(cls): # FIXME: Replace this as soon as we have a config module config = {} # Try to get iflytek_yuyin config from config profile_path = dingdangpath.config(""profile.yml"") if os.path.exists(profile_path): with open(profile_path, ""r"") as f: profile = yaml.safe_load(f) <mask>: if ""vid"" in profile[""iflytek_yuyin""]: config[""vid""] = profile[""iflytek_yuyin""][""vid""] return config",True,"if ""iflytek_yuyin"" in profile :","if ""iflytek_yuyin"" in profile :",0.8820016898747209,100.00000000000004
"def get_signed_in_user(test_case): playback = not (test_case.is_live or test_case.in_recording) if playback: return MOCKED_USER_NAME else: account_info = test_case.cmd(""account show"").get_output_in_json() <mask>: return account_info[""user""][""name""] return None",False,"if account_info [ ""user"" ] [ ""type"" ] != ""servicePrincipal"" :","if account_info [ ""user"" ] :",0.8820016898747209,33.241660012938524
"def rename_project(self, project, new_name): """"""Rename project, update the related projects if necessary"""""" old_name = project.name for proj in self.projects: relproj = proj.get_related_projects() <mask>: relproj[relproj.index(old_name)] = new_name proj.set_related_projects(relproj) project.rename(new_name) self.save()",False,if old_name in relproj :,if old_name not in relproj :,0.8820016898747209,50.000000000000014
"def test_call_extern_c_fn(self): global memcmp memcmp = cffi_support.ExternCFunction( ""memcmp"", (""int memcmp ( const uint8_t * ptr1, "" ""const uint8_t * ptr2, size_t num )""), ) @udf(BooleanVal(FunctionContext, StringVal, StringVal)) def fn(context, a, b): if a.is_null != b.is_null: return False if a is None: return True if len(a) != b.len: return False <mask>: return True return memcmp(a.ptr, b.ptr, a.len) == 0",False,if a . ptr == b . ptr :,if a . ptr != b . ptr :,0.8820016898747209,65.80370064762461
"def parse_variable(self): begin = self._pos while True: ch = self.read() <mask>: return ScriptVariable(self._text[begin : self._pos - 1]) elif ch is None: self.__raise_eof() elif not isidentif(ch) and ch != "":"": self.__raise_char(ch)",False,"if ch == ""%"" :","if ch == ""#"" :",0.8820016898747209,59.4603557501361
"def h_file(self): filename = self.abspath() st = os.stat(filename) cache = self.ctx.hashes_md5_tstamp if filename in cache and cache[filename][0] == st.st_mtime: return cache[filename][1] if STRONGEST: ret = Utils.h_file(filename) else: <mask>: raise IOError(""Not a file"") ret = Utils.md5(str((st.st_mtime, st.st_size)).encode()).digest() cache[filename] = (st.st_mtime, ret) return ret",False,if stat . S_ISDIR ( st [ stat . ST_MODE ] ) :,"if not isinstance ( st , os . path . File ) :",0.8820016898747209,7.387561680614721
"def add_widgets(self, *widgets_or_spacings): """"""Add widgets/spacing to dialog vertical layout"""""" layout = self.layout() for widget_or_spacing in widgets_or_spacings: <mask>: layout.addSpacing(widget_or_spacing) else: layout.addWidget(widget_or_spacing)",False,"if isinstance ( widget_or_spacing , int ) :",if widget_or_spacing . is_vertical ( ) :,0.8820016898747209,35.416987661440594
"def _str_index(self): idx = self[""index""] out = [] if len(idx) == 0: return out out += ["".. index:: %s"" % idx.get(""default"", """")] for section, references in idx.iteritems(): <mask>: continue elif section == ""refguide"": out += ["" single: %s"" % ("", "".join(references))] else: out += ["" %s: %s"" % (section, "","".join(references))] return out",True,"if section == ""default"" :","if section == ""default"" :",0.8820016898747209,100.00000000000004
"def dictify_CPPDEFINES(env): cppdefines = env.get(""CPPDEFINES"", {}) if cppdefines is None: return {} if SCons.Util.is_Sequence(cppdefines): result = {} for c in cppdefines: <mask>: result[c[0]] = c[1] else: result[c] = None return result if not SCons.Util.is_Dict(cppdefines): return {cppdefines: None} return cppdefines",False,if SCons . Util . is_Sequence ( c ) :,"if c [ 0 ] == ""CPPDEFINES"" :",0.8820016898747209,4.503733751056993
"def decoder(s): r = [] decode = [] for c in s: if c == ""&"" and not decode: decode.append(""&"") elif c == ""-"" and decode: if len(decode) == 1: r.append(""&"") else: r.append(modified_unbase64("""".join(decode[1:]))) decode = [] <mask>: decode.append(c) else: r.append(c) if decode: r.append(modified_unbase64("""".join(decode[1:]))) bin_str = """".join(r) return (bin_str, len(s))",False,elif decode :,"elif c == ""+"" :",0.8820016898747209,6.567274736060395
"def optimize(self, graph: Graph): MAX_TEXTURE_SIZE = config.WEBGL_MAX_TEXTURE_SIZE flag_changed = False for v in traverse.listup_variables(graph): <mask>: continue height, width = TextureShape.get(v) if height <= MAX_TEXTURE_SIZE and width <= MAX_TEXTURE_SIZE: continue if not v.has_attribute(SplitTarget): flag_changed = True v.attributes.add(SplitTarget()) return graph, flag_changed",False,if not Placeholder . check_resolved ( v . size ) :,"if not isinstance ( v , Graph ) :",0.8820016898747209,10.589620902360299
"def one_gpr_reg_one_mem_scalable(ii): n, r = 0, 0 for op in _gen_opnds(ii): if op_agen(op) or (op_mem(op) and op.oc2 in [""v""]): n += 1 <mask>: r += 1 else: return False return n == 1 and r == 1",False,elif op_gprv ( op ) :,elif op_reg ( op ) :,0.8820016898747209,50.000000000000014
"def get_genome_dir(gid, galaxy_dir, data): """"""Return standard location of genome directories."""""" if galaxy_dir: refs = genome.get_refs(gid, None, galaxy_dir, data) seq_file = tz.get_in([""fasta"", ""base""], refs) if seq_file and os.path.exists(seq_file): return os.path.dirname(os.path.dirname(seq_file)) else: gdirs = glob.glob(os.path.join(_get_data_dir(), ""genomes"", ""*"", gid)) <mask>: return gdirs[0]",False,if len ( gdirs ) == 1 and os . path . exists ( gdirs [ 0 ] ) :,if len ( gdirs ) == 1 :,0.8820016898747209,23.43746816281914
"def __modules(self): raw_output = self.__module_avail_output().decode(""utf-8"") for line in StringIO(raw_output): line = line and line.strip() if not line or line.startswith(""-""): continue line_modules = line.split() for module in line_modules: <mask>: module = module[0 : -len(self.default_indicator)].strip() module_parts = module.split(""/"") module_version = None if len(module_parts) == 2: module_version = module_parts[1] module_name = module_parts[0] yield module_name, module_version",True,if module . endswith ( self . default_indicator ) :,if module . endswith ( self . default_indicator ) :,0.8820016898747209,100.00000000000004
"def save(self): updates = self.cinder_obj_get_changes() if updates: <mask>: metadata = updates.pop(""metadata"", None) self.metadata = db.backup_metadata_update( self._context, self.id, metadata, True ) updates.pop(""parent"", None) db.backup_update(self._context, self.id, updates) self.obj_reset_changes()",True,"if ""metadata"" in updates :","if ""metadata"" in updates :",0.8820016898747209,100.00000000000004
"def test_set_tag(association_obj, sagemaker_session): tag = {""Key"": ""foo"", ""Value"": ""bar""} association_obj.set_tag(tag) while True: actual_tags = sagemaker_session.sagemaker_client.list_tags( ResourceArn=association_obj.source_arn )[""Tags""] <mask>: break time.sleep(5) # When sagemaker-client-config endpoint-url is passed as argument to hit some endpoints, # length of actual tags will be greater than 1 assert len(actual_tags) > 0 assert actual_tags[0] == tag",False,if actual_tags :,if not actual_tags :,0.8820016898747209,53.7284965911771
"def test_error_stream(environ, start_response): writer = start_response(""200 OK"", []) wsgi_errors = environ[""wsgi.errors""] error_msg = None for method in [ ""flush"", ""write"", ""writelines"", ]: if not hasattr(wsgi_errors, method): error_msg = ""wsgi.errors has no '%s' attr"" % method <mask>: error_msg = ""wsgi.errors.%s attr is not callable"" % method if error_msg: break return_msg = error_msg or ""success"" writer(return_msg) return []",False,"if not error_msg and not callable ( getattr ( wsgi_errors , method ) ) :",elif not callable ( wsgi_errors [ method ] ) :,0.8820016898747209,19.069363427734363
"def current_dict(cursor_offset, line): """"""If in dictionary completion, return the dict that should be used"""""" for m in current_dict_re.finditer(line): <mask>: return LinePart(m.start(1), m.end(1), m.group(1)) return None",False,if m . start ( 2 ) <= cursor_offset and m . end ( 2 ) >= cursor_offset :,if m . start ( 1 ) <= cursor_offset :,0.8820016898747209,30.24050824074871
"def show_file_browser(self): """"""Show/hide the file browser."""""" if self.show_file_browser_action.isChecked(): sizes = self.panel.sizes() <mask>: sizes[0] = sum(sizes) // 4 self.panel.setSizes(sizes) self.file_browser.show() else: self.file_browser.hide()",False,if sizes [ 0 ] == 0 :,if len ( sizes ) > 0 :,0.8820016898747209,11.59119922599073
"def run(self, paths=[]): items = [] for item in SideBarSelection(paths).getSelectedItems(): items.append(item.nameEncoded()) if len(items) > 0: sublime.set_clipboard(""\n"".join(items)) <mask>: sublime.status_message(""Items copied"") else: sublime.status_message(""Item copied"")",False,if len ( items ) > 1 :,if len ( items ) == 1 :,0.8820016898747209,51.33450480401705
"def prepend(self, value): """"""prepend value to nodes"""""" root, root_text = self._get_root(value) for i, tag in enumerate(self): <mask>: tag.text = """" if len(root) > 0: root[-1].tail = tag.text tag.text = root_text else: tag.text = root_text + tag.text if i > 0: root = deepcopy(list(root)) tag[:0] = root root = tag[: len(root)] return self",False,if not tag . text :,if tag . text is None :,0.8820016898747209,27.77619034011791
"def getLabel(self, address=None): if address is None: address = self.address label = address if shared.config.has_section(address): label = shared.config.get(address, ""label"") queryreturn = sqlQuery(""""""select label from addressbook where address=?"""""", address) <mask>: for row in queryreturn: (label,) = row else: queryreturn = sqlQuery( """"""select label from subscriptions where address=?"""""", address ) if queryreturn != []: for row in queryreturn: (label,) = row return label",True,if queryreturn != [ ] :,if queryreturn != [ ] :,0.8820016898747209,100.00000000000004
"def _parse(self, engine): """"""Parse the layer."""""" if isinstance(self.args, dict): if ""axis"" in self.args: self.axis = engine.evaluate(self.args[""axis""], recursive=True) if not isinstance(self.axis, int): raise ParsingError('""axis"" must be an integer.') <mask>: self.momentum = engine.evaluate(self.args[""momentum""], recursive=True) if not isinstance(self.momentum, (int, float)): raise ParsingError('""momentum"" must be numeric.')",True,"if ""momentum"" in self . args :","if ""momentum"" in self . args :",0.8820016898747209,100.00000000000004
"def urlquote(*args, **kwargs): new_kwargs = dict(kwargs) if not PY3: new_kwargs = dict(kwargs) if ""encoding"" in new_kwargs: del new_kwargs[""encoding""] <mask>: del new_kwargs[""errors""] return quote(*args, **new_kwargs)",False,"if ""errors"" in kwargs :","elif ""errors"" in new_kwargs :",0.8820016898747209,35.49481056010054
"def setNextFormPrevious(self, backup=STARTING_FORM): try: if self._THISFORM.FORM_NAME == self._FORM_VISIT_LIST[-1]: self._FORM_VISIT_LIST.pop() # Remove the current form. if it is at the end of the list <mask>: # take no action if it looks as if someone has already set the next form. self.setNextForm( self._FORM_VISIT_LIST.pop() ) # Switch to the previous form if one exists except IndexError: self.setNextForm(backup)",False,if self . _THISFORM . FORM_NAME == self . NEXT_ACTIVE_FORM :,if self . _FORM_VISIT_LIST [ - 1 ] . FORM_NAME == self . _FORM_VISIT_LIST [ - 1 ] :,0.8820016898747209,31.454352182114505
"def iter_chars_to_words(self, chars): current_word = [] for char in chars: if not self.keep_blank_chars and char[""text""].isspace(): if current_word: yield current_word current_word = [] <mask>: yield current_word current_word = [char] else: current_word.append(char) if current_word: yield current_word",False,"elif current_word and self . char_begins_new_word ( current_word , char ) :","elif self . keep_blank_chars and char [ ""text"" ] . isspace ( ) :",0.8820016898747209,6.161398495622037
"def get(self): """"""return a secret by name"""""" results = self._get(""secrets"", self.name) results[""decoded""] = {} results[""exists""] = False if results[""returncode""] == 0 and results[""results""][0]: results[""exists""] = True <mask>: if ""data"" in results[""results""][0]: for sname, value in results[""results""][0][""data""].items(): results[""decoded""][sname] = base64.b64decode(value) if results[""returncode""] != 0 and '""%s"" not found' % self.name in results[""stderr""]: results[""returncode""] = 0 return results",False,if self . decode :,"if results [ ""exists"" ] :",0.8820016898747209,6.567274736060395
"def insert_use(self, edit): if self.is_first_use(): for location in [r""^\s*namespace\s+[\w\\]+[;{]"", r""<\?php""]: inserted = self.insert_first_use(location, edit) <mask>: break else: self.insert_use_among_others(edit)",True,if inserted :,if inserted :,0.8820016898747209,0.0
"def _new_rsa_key(spec): if ""name"" not in spec: <mask>: (head, tail) = os.path.split(spec[""key""]) spec[""path""] = head spec[""name""] = tail else: spec[""name""] = spec[""key""] return rsa_init(spec)",False,"if ""/"" in spec [ ""key"" ] :","if spec [ ""key"" ] :",0.8820016898747209,52.734307450329375
"def mimeData(self, indexes): if len(indexes) == 1: index = indexes[0] model = song = index.data(Qt.UserRole) <mask>: try: model = song.album except (ProviderIOError, Exception): model = None return ModelMimeData(model)",False,if index . column ( ) == Column . album :,if model is None :,0.8820016898747209,3.1325998243558226
"def get(self, url, **kwargs): app, url = self._prepare_call(url, kwargs) if app: if url.endswith(""ping"") and self._first_ping: self._first_ping = False return EmptyCapabilitiesResponse() <mask>: return ErrorApiResponse() else: response = app.get(url, **kwargs) return TestingResponse(response) else: return requests.get(url, **kwargs)",False,"elif ""Hello0"" in url and ""1.2.1"" in url and ""v1"" in url :","elif url . endswith ( ""error"" ) :",0.8820016898747209,2.551084474780675
"def handle_noargs(self, **options): self.style = color_style() print(""Running Django's own validation:"") self.validate(display_num_errors=True) for model in loading.get_models(): if hasattr(model, ""_create_content_base""): self.validate_base_model(model) <mask>: self.validate_content_type(model)",False,"if hasattr ( model , ""_feincms_content_models"" ) :","elif hasattr ( model , ""_create_content_type"" ) :",0.8820016898747209,49.35578819979934
"def test_rules_widget(self): subreddit = self.reddit.subreddit(pytest.placeholders.test_subreddit) widgets = subreddit.widgets with self.use_cassette(""TestSubredditWidgets.fetch_widgets""): rules = None for widget in widgets.sidebar: <mask>: rules = widget break assert isinstance(rules, RulesWidget) assert rules == rules assert rules.id == rules assert rules.display assert len(rules) > 0 assert subreddit == rules.subreddit",True,"if isinstance ( widget , RulesWidget ) :","if isinstance ( widget , RulesWidget ) :",0.8820016898747209,100.00000000000004
"def __init__(self, exception): message = str(exception) with contextlib.suppress(IndexError): underlying_exception = exception.args[0] <mask>: message = ( ""maximum retries exceeded trying to reach the store.\n"" ""Check your network connection, and check the store "" ""status at {}"".format(_STORE_STATUS_URL) ) super().__init__(message=message)",False,"if isinstance ( underlying_exception , urllib3 . exceptions . MaxRetryError ) :",if underlying_exception . errno == errno . EAGAIN :,0.8820016898747209,13.188274750399428
"def wrapped(self, request): try: return self._finished except AttributeError: if self.node_ids: <mask>: log.debug( ""%s is still going to be used, not terminating it. "" ""Still in use on:\n%s"", self, pprint.pformat(list(self.node_ids)), ) return log.debug(""Finish called on %s"", self) try: return func(request) finally: self._finished = True",False,if not request . session . shouldfail and not request . session . shouldstop :,if self . _finished :,0.8820016898747209,2.383515454163372
"def get_min_vertical_scroll() -> int: # Make sure that the cursor line is not below the bottom. # (Calculate how many lines can be shown between the cursor and the .) used_height = 0 prev_lineno = ui_content.cursor_position.y for lineno in range(ui_content.cursor_position.y, -1, -1): used_height += get_line_height(lineno) <mask>: return prev_lineno else: prev_lineno = lineno return 0",False,if used_height > height - scroll_offsets_bottom :,if used_height > ui_content . scroll_height :,0.8820016898747209,36.362270465000705
"def cookies(self): # strip cookie_suffix from all cookies in the request, return result cookies = flask.Request.cookies.__get__(self) result = {} desuffixed = {} for key, value in cookies.items(): <mask>: desuffixed[key[: -len(self.cookie_suffix)]] = value else: result[key] = value result.update(desuffixed) return result",True,if key . endswith ( self . cookie_suffix ) :,if key . endswith ( self . cookie_suffix ) :,0.8820016898747209,100.00000000000004
"def update_vars(state1, state2): ops = [] for name in state1._fields: state1_vs = getattr(state1, name) <mask>: ops += [ tf.assign(_v1, _v2) for _v1, _v2 in zip(state1_vs, getattr(state2, name)) ] else: ops += [tf.assign(state1_vs, getattr(state2, name))] return tf.group(*ops)",False,"if isinstance ( state1_vs , list ) :","if isinstance ( state1_vs , tuple ) :",0.8820016898747209,70.71067811865478
"def manifest(self): """"""The current manifest dictionary."""""" if self.reload: <mask>: return {} mtime = self.getmtime(self.manifest_path) if self._mtime is None or mtime > self._mtime: self._manifest = self.get_manifest() self._mtime = mtime return self._manifest",False,if not self . exists ( self . manifest_path ) :,if self . _manifest is None :,0.8820016898747209,7.780436171361459
"def csvtitle(self): if isinstance(self.name, six.string_types): return '""' + self.name + '""' + char[""sep""] * (len(self.nick) - 1) else: ret = """" for i, name in enumerate(self.name): ret = ret + '""' + name + '""' + char[""sep""] * (len(self.nick) - 1) <mask>: ret = ret + char[""sep""] return ret",False,if i + 1 != len ( self . name ) :,if i == len ( self . nick ) - 1 :,0.8820016898747209,36.362270465000705
"def cache_dst(self): final_dst = None final_linenb = None for linenb, assignblk in enumerate(self): for dst, src in viewitems(assignblk): <mask>: if final_dst is not None: raise ValueError(""Multiple destinations!"") final_dst = src final_linenb = linenb self._dst = final_dst self._dst_linenb = final_linenb return final_dst",False,"if dst . is_id ( ""IRDst"" ) :",if dst == linenb :,0.8820016898747209,6.60902979597904
"def _ProcessName(self, name, dependencies): """"""Retrieve a module name from a node name."""""" module_name, dot, base_name = name.rpartition(""."") if dot: <mask>: if module_name in dependencies: dependencies[module_name].add(base_name) else: dependencies[module_name] = {base_name} else: # If we have a relative import that did not get qualified (usually due # to an empty package_name), don't insert module_name='' into the # dependencies; we get a better error message if we filter it out here # and fail later on. logging.warning(""Empty package name: %s"", name)",False,if module_name :,if base_name :,0.8820016898747209,42.72870063962342
"def get_aa_from_codonre(re_aa): aas = [] m = 0 for i in re_aa: if i == ""["": m = -1 aas.append("""") elif i == ""]"": m = 0 continue elif m == -1: aas[-1] = aas[-1] + i <mask>: aas.append(i) return aas",False,elif m == 0 :,elif m == 1 :,0.8820016898747209,53.7284965911771
"def logic(): count = intbv(0, min=0, max=MAXVAL + 1) while True: yield clock.posedge, reset.posedge if reset == 1: count[:] = 0 else: flag.next = 0 <mask>: flag.next = 1 count[:] = 0 else: count += 1",False,if count == MAXVAL :,elif reset == 0 :,0.8820016898747209,17.965205598154213
"def _history_define_metric( self, hkey: str ) -> Optional[wandb_internal_pb2.MetricRecord]: """"""check for hkey match in glob metrics, return defined metric."""""" # Dont define metric for internal metrics if hkey.startswith(""_""): return None for k, mglob in six.iteritems(self._metric_globs): if k.endswith(""*""): <mask>: m = wandb_internal_pb2.MetricRecord() m.CopyFrom(mglob) m.ClearField(""glob_name"") m.name = hkey return m return None",False,if hkey . startswith ( k [ : - 1 ] ) :,if mglob :,0.8820016898747209,0.0
"def optimize_models(args, use_cuda, models): """"""Optimize ensemble for generation"""""" for model in models: model.make_generation_fast_( beamable_mm_beam_size=None if args.no_beamable_mm else args.beam, need_attn=args.print_alignment, ) <mask>: model.half() if use_cuda: model.cuda()",False,if args . fp16 :,if use_half :,0.8820016898747209,12.703318703865365
"def _Dynamic_Rollback(self, transaction, transaction_response): txid = transaction.handle() self.__local_tx_lock.acquire() try: <mask>: raise apiproxy_errors.ApplicationError( datastore_pb.Error.BAD_REQUEST, ""Transaction %d not found."" % (txid,) ) txdata = self.__transactions[txid] assert ( txdata.thread_id == thread.get_ident() ), ""Transactions are single-threaded."" del self.__transactions[txid] finally: self.__local_tx_lock.release()",True,if txid not in self . __transactions :,if txid not in self . __transactions :,0.8820016898747209,100.00000000000004
"def get_job_dirs(path): regex = re.compile(""[1-9][0-9]*-"") jobdirs = [] for d in os.listdir(path): # skip directories not matching the job result dir pattern <mask>: continue d = os.path.join(options.resultsdir, d) if os.path.isdir(d) and not os.path.exists(os.path.join(d, PUBLISH_FLAGFILE)): jobdirs.append(d) return jobdirs",False,if not regex . match ( d ) :,if not regex . search ( d ) :,0.8820016898747209,59.694917920196445
"def traverse(node, functions=[]): if hasattr(node, ""grad_fn""): node = node.grad_fn if hasattr(node, ""variable""): node = graph.nodes_by_id.get(id(node.variable)) if node: node.functions = list(functions) del functions[:] if hasattr(node, ""next_functions""): functions.append(type(node).__name__) for f in node.next_functions: <mask>: functions.append(type(f[0]).__name__) traverse(f[0], functions) if hasattr(node, ""saved_tensors""): for t in node.saved_tensors: traverse(t)",False,if f [ 0 ] :,"if hasattr ( f , 0 ) :",0.8820016898747209,7.809849842300637
"def get_all_snap_points(self, forts): points = [] radius = Constants.MAX_DISTANCE_FORT_IS_REACHABLE for i in range(0, len(forts)): for j in range(i + 1, len(forts)): c1, c2 = self.get_enclosing_circles(forts[i], forts[j], radius) <mask>: points.append((c1, c2, forts[i], forts[j])) return points",False,if c1 and c2 :,if c1 != c2 :,0.8820016898747209,22.957488466614336
"def doDir(elem): for child in elem.childNodes: if not isinstance(child, minidom.Element): continue if child.tagName == ""Directory"": doDir(child) elif child.tagName == ""Component"": for grandchild in child.childNodes: <mask>: continue if grandchild.tagName != ""File"": continue files.add(grandchild.getAttribute(""Source"").replace(os.sep, ""/""))",True,"if not isinstance ( grandchild , minidom . Element ) :","if not isinstance ( grandchild , minidom . Element ) :",0.8820016898747209,100.00000000000004
"def computeLeadingWhitespaceWidth(s, tab_width): w = 0 for ch in s: if ch == "" "": w += 1 <mask>: w += abs(tab_width) - (w % abs(tab_width)) else: break return w",True,"elif ch == ""\t"" :","elif ch == ""\t"" :",0.8820016898747209,100.00000000000004
"def test_avg_group_by(self): ret = ( await Book.annotate(avg=Avg(""rating"")) .group_by(""author_id"") .values(""author_id"", ""avg"") ) for item in ret: author_id = item.get(""author_id"") avg = item.get(""avg"") <mask>: self.assertEqual(avg, 4.5) elif author_id == self.a2.pk: self.assertEqual(avg, 2.0)",True,if author_id == self . a1 . pk :,if author_id == self . a1 . pk :,0.8820016898747209,100.00000000000004
"def open_session(self, app, request): sid = request.cookies.get(app.session_cookie_name) if sid: stored_session = self.cls.objects(sid=sid).first() <mask>: expiration = stored_session.expiration if not expiration.tzinfo: expiration = expiration.replace(tzinfo=utc) if expiration > datetime.datetime.utcnow().replace(tzinfo=utc): return MongoEngineSession( initial=stored_session.data, sid=stored_session.sid ) return MongoEngineSession(sid=str(uuid.uuid4()))",True,if stored_session :,if stored_session :,0.8820016898747209,100.00000000000004
"def one_line_description(self): MAX_LINE_LENGTH = 120 desc = util.remove_html_tags(self.description or """") desc = re.sub(""\s+"", "" "", desc).strip() if not desc: return _(""No description available"") else: # Decode the description to avoid gPodder bug 1277 desc = util.convert_bytes(desc).strip() <mask>: return desc[:MAX_LINE_LENGTH] + ""..."" else: return desc",True,if len ( desc ) > MAX_LINE_LENGTH :,if len ( desc ) > MAX_LINE_LENGTH :,0.8820016898747209,100.00000000000004
"def setInnerHTML(self, html): log.HTMLClassifier.classify( log.ThugLogging.url if log.ThugOpts.local else log.last_url, html ) self.tag.clear() for node in bs4.BeautifulSoup(html, ""html.parser"").contents: self.tag.append(node) name = getattr(node, ""name"", None) if name is None: continue handler = getattr(log.DFT, ""handle_%s"" % (name,), None) <mask>: handler(node)",False,if handler :,if handler is not None :,0.8820016898747209,17.965205598154213
def get_supported_period_type_map(cls): if cls.supported_period_map is None: cls.supported_period_map = {} cls.supported_period_map.update(cls.period_type_map) try: from dateutil import relativedelta <mask>: cls.supported_period_map.update(cls.optional_period_type_map) except Exception: pass return cls.supported_period_map,False,if relativedelta is not None :,if relativedelta ( cls . period_type_map ) :,0.8820016898747209,7.495553473355845
"def _compare_single_run(self, compares_done): try: compare_id, redo = self.in_queue.get( timeout=float(self.config[""ExpertSettings""][""block_delay""]) ) except Empty: pass else: <mask>: if redo: self.db_interface.delete_old_compare_result(compare_id) compares_done.add(compare_id) self._process_compare(compare_id) if self.callback: self.callback()",False,"if self . _decide_whether_to_process ( compare_id , redo , compares_done ) :",if compare_id not in compares_done :,0.8820016898747209,7.693023055531449
"def _get_field_actual(cant_be_number, raw_string, field_names): for line in raw_string.splitlines(): for field_name in field_names: field_name = field_name.lower() if "":"" in line: left, right = line.split("":"", 1) left = left.strip().lower() right = right.strip() if left == field_name and len(right) > 0: if cant_be_number: <mask>: return right else: return right return None",False,if not right . isdigit ( ) :,if left == field_name :,0.8820016898747209,6.567274736060395
"def _p_basicstr_content(s, content=_basicstr_re): res = [] while True: res.append(s.expect_re(content).group(0)) if not s.consume(""\\""): break <mask>: pass elif s.consume_re(_short_uni_re) or s.consume_re(_long_uni_re): res.append(_chr(int(s.last().group(1), 16))) else: s.expect_re(_escapes_re) res.append(_escapes[s.last().group(0)]) return """".join(res)",False,if s . consume_re ( _newline_esc_re ) :,if s . consume_re ( _short_re ) :,0.8820016898747209,65.26220818377338
"def removedir(self, path): # type: (Text) -> None _path = self.validatepath(path) if _path == ""/"": raise errors.RemoveRootError() with ftp_errors(self, path): try: self.ftp.rmd(_encode(_path, self.ftp.encoding)) except error_perm as error: code, _ = _parse_ftp_error(error) if code == ""550"": if self.isfile(path): raise errors.DirectoryExpected(path) <mask>: raise errors.DirectoryNotEmpty(path) raise # pragma: no cover",False,if not self . isempty ( path ) :,elif self . islink ( path ) :,0.8820016898747209,36.28241434631104
"def _normalize_store_path(self, resource_store): if resource_store[""type""] == ""filesystem"": <mask>: resource_store[""base_directory""] = os.path.join( self.root_directory, resource_store[""base_directory""] ) return resource_store",False,"if not os . path . isabs ( resource_store [ ""base_directory"" ] ) :","if resource_store [ ""base_directory"" ] :",0.8820016898747209,43.367978688929774
"def _apply_nested(name, val, nested): parts = name.split(""."") cur = nested for i in range(0, len(parts) - 1): cur = cur.setdefault(parts[i], {}) <mask>: conflicts_with = ""."".join(parts[0 : i + 1]) raise ValueError( ""%r cannot be nested: conflicts with {%r: %s}"" % (name, conflicts_with, cur) ) cur[parts[-1]] = val",False,"if not isinstance ( cur , dict ) :",if cur [ parts [ i ] ] != val :,0.8820016898747209,4.456882760699063
"def build_packages(targeted_packages, distribution_directory, is_dev_build=False): # run the build and distribution for package_root in targeted_packages: service_hierarchy = os.path.join(os.path.basename(package_root)) <mask>: verify_update_package_requirement(package_root) print(""Generating Package Using Python {}"".format(sys.version)) run_check_call( [ sys.executable, build_packing_script_location, ""--dest"", os.path.join(distribution_directory, service_hierarchy), package_root, ], root_dir, )",True,if is_dev_build :,if is_dev_build :,0.8820016898747209,100.00000000000004
"def resolve_root_node_address(self, root_node): if ""["" in root_node: name, numbers = root_node.split(""["", maxsplit=1) number = numbers.split("","", maxsplit=1)[0] <mask>: number = number.split(""-"")[0] number = re.sub(""[^0-9]"", """", number) root_node = name + number return root_node",True,"if ""-"" in number :","if ""-"" in number :",0.8820016898747209,100.00000000000004
"def _map_args(maps: dict, **kwargs): # maps: key=old name, value= new name output = {} for name, val in kwargs.items(): if name in maps: assert isinstance(maps[name], str) output.update({maps[name]: val}) else: output.update({name: val}) for keys in maps.keys(): <mask>: pass return output",False,if keys not in output . keys ( ) :,if keys [ 0 ] == name :,0.8820016898747209,9.442944296079734
"def next_item(self, direction): """"""Selects next menu item, based on self._direction"""""" start, i = -1, 0 try: start = self.items.index(self._selected) i = start + direction except: pass while True: if i == start: # Cannot find valid menu item self.select(start) break <mask>: i = 0 continue if i < 0: i = len(self.items) - 1 continue if self.select(i): break i += direction if start < 0: start = 0",False,if i >= len ( self . items ) :,if i > len ( self . items ) - 1 :,0.8820016898747209,54.52469119630866
"def detect_reentrancy(self, contract): for function in contract.functions_and_modifiers_declared: <mask>: if self.KEY in function.context: continue self._explore(function.entry_point, []) function.context[self.KEY] = True",False,if function . is_implemented :,if self . KEY in function . context :,0.8820016898747209,11.339582221952005
"def load_model(self): if not os.path.exists(self.get_filename(absolute=True)): <mask>: return {}, {} error( ""Model file with pre-trained convolution layers not found. Download it here..."", ""https://github.com/alexjc/neural-enhance/releases/download/v%s/%s"" % (__version__, self.get_filename()), ) print("" - Loaded file `{}` with trained model."".format(self.get_filename())) return pickle.load(bz2.open(self.get_filename(), ""rb""))",False,if args . train :,if not self . _is_trained_convolution ( ) :,0.8820016898747209,4.065425428798724
"def get_nonexisting_check_definition_extends(definition, indexed_oval_defs): # TODO: handle multiple levels of referrals. # OVAL checks that go beyond one level of extend_definition won't be properly identified for extdefinition in definition.findall("".//{%s}extend_definition"" % oval_ns): # Verify each extend_definition in the definition extdefinitionref = extdefinition.get(""definition_ref"") # Search the OVAL tree for a definition with the referred ID referreddefinition = indexed_oval_defs.get(extdefinitionref) <mask>: # There is no oval satisfying the extend_definition referal return extdefinitionref return None",False,if referreddefinition is None :,if referreddefinition is not None :,0.8820016898747209,37.99178428257963
"def pause(self): if self.is_playing: self.state = MusicPlayerState.PAUSED <mask>: self._current_player.pause() self.emit(""pause"", player=self, entry=self.current_entry) return elif self.is_paused: return raise ValueError(""Cannot pause a MusicPlayer in state %s"" % self.state)",True,if self . _current_player :,if self . _current_player :,0.8820016898747209,100.00000000000004
"def setNextFormPrevious(self, backup=STARTING_FORM): try: <mask>: self._FORM_VISIT_LIST.pop() # Remove the current form. if it is at the end of the list if self._THISFORM.FORM_NAME == self.NEXT_ACTIVE_FORM: # take no action if it looks as if someone has already set the next form. self.setNextForm( self._FORM_VISIT_LIST.pop() ) # Switch to the previous form if one exists except IndexError: self.setNextForm(backup)",False,if self . _THISFORM . FORM_NAME == self . _FORM_VISIT_LIST [ - 1 ] :,if self . _THISFORM . FORM_NAME == self . NEXT_ACTIVE_FORM :,0.8820016898747209,54.95528514675272
"def get_expr_referrers(schema: s_schema.Schema, obj: so.Object) -> Dict[so.Object, str]: """"""Return schema referrers with refs in expressions."""""" refs = schema.get_referrers_ex(obj) result = {} for (mcls, fn), referrers in refs.items(): field = mcls.get_field(fn) <mask>: result.update({ref: fn for ref in referrers}) return result",False,"if issubclass ( field . type , ( Expression , ExpressionList ) ) :",if field . is_ref_expr ( obj ) :,0.8820016898747209,8.972971553870872
"def _fields_to_index(cls): fields = [] for field in cls._meta.sorted_fields: <mask>: continue requires_index = any( (field.index, field.unique, isinstance(field, ForeignKeyField)) ) if requires_index: fields.append(field) return fields",False,if field . primary_key :,if field . index is None :,0.8820016898747209,26.269098944241588
"def ident_values(self): value = self._ident_values if value is False: value = None # XXX: how will this interact with orig_prefix ? # not exposing attrs for now if orig_prefix is set. if not self.orig_prefix: wrapped = self.wrapped idents = getattr(wrapped, ""ident_values"", None) <mask>: value = [self._wrap_hash(ident) for ident in idents] ##else: ## ident = self.ident ## if ident is not None: ## value = [ident] self._ident_values = value return value",False,if idents :,if idents is not None :,0.8820016898747209,17.965205598154213
"def apply_incpaths_ml(self): inc_lst = self.includes.split() lst = self.incpaths_lst for dir in inc_lst: node = self.path.find_dir(dir) if not node: error(""node not found: "" + str(dir)) continue <mask>: lst.append(node) self.bld_incpaths_lst.append(node)",False,if not node in lst :,if node not in lst :,0.8820016898747209,35.930411196308434
"def application_openFiles_(self, nsapp, filenames): # logging.info('[osx] file open') # logging.info('[osx] file : %s' % (filenames)) for filename in filenames: logging.info(""[osx] receiving from macOS : %s"", filename) if os.path.exists(filename): <mask>: sabnzbd.add_nzbfile(filename, keep=True)",False,if sabnzbd . filesystem . get_ext ( filename ) in VALID_ARCHIVES + VALID_NZB_FILES :,"if nsapp == ""mac"" :",0.8820016898747209,1.1412212286076961
"def check(self, xp, nout): input = xp.asarray(self.x).astype(numpy.float32) with warnings.catch_warnings(): if self.ignore_warning: warnings.simplefilter(""ignore"", self.ignore_warning) <mask>: self.check_positive(xp, self.func, input, self.eps, nout) else: self.check_negative(xp, self.func, input, self.eps, nout)",False,if self . result :,if self . is_positive :,0.8820016898747209,26.269098944241588
"def _set_scheme(url, newscheme): scheme = _get_scheme(url) newscheme = newscheme or """" newseparator = "":"" if newscheme in COLON_SEPARATED_SCHEMES else ""://"" if scheme == """": # Protocol relative URL. url = ""%s:%s"" % (newscheme, url) elif scheme is None and url: # No scheme. url = """".join([newscheme, newseparator, url]) elif scheme: # Existing scheme. remainder = url[len(scheme) :] <mask>: remainder = remainder[3:] elif remainder.startswith("":""): remainder = remainder[1:] url = """".join([newscheme, newseparator, remainder]) return url",True,"if remainder . startswith ( ""://"" ) :","if remainder . startswith ( ""://"" ) :",0.8820016898747209,100.00000000000004
"def parquet(tables, data_directory, ignore_missing_dependency, **params): try: import pyarrow as pa # noqa: F401 import pyarrow.parquet as pq # noqa: F401 except ImportError: msg = ""PyArrow dependency is missing"" <mask>: logger.warning(""Ignored: %s"", msg) return 0 else: raise click.ClickException(msg) data_directory = Path(data_directory) for table, df in read_tables(tables, data_directory): arrow_table = pa.Table.from_pandas(df) target_path = data_directory / ""{}.parquet"".format(table) pq.write_table(arrow_table, str(target_path))",True,if ignore_missing_dependency :,if ignore_missing_dependency :,0.8820016898747209,100.00000000000004
"def h2i(self, pkt, s): t = () if type(s) is str: t = time.strptime(s) t = t[:2] + t[2:-3] else: <mask>: y, m, d, h, min, sec, rest, rest, rest = time.gmtime(time.time()) t = (y, m, d, h, min, sec) else: t = s return t",False,if not s :,if type ( s ) is int :,0.8820016898747209,7.267884212102741
"def filter_episodes(self, batch, cross_entropy): """"""Filter the episodes for the cross_entropy method"""""" accumulated_reward = [sum(rewards) for rewards in batch[""rewards""]] percentile = cross_entropy * 100 reward_bound = np.percentile(accumulated_reward, percentile) # we save the batch with reward above the bound result = {k: [] for k in self.data_keys} episode_kept = 0 for i in range(len(accumulated_reward)): <mask>: for k in self.data_keys: result[k].append(batch[k][i]) episode_kept += 1 return result",False,if accumulated_reward [ i ] >= reward_bound :,if reward_bound > episode_kept :,0.8820016898747209,14.06401411379081
"def _readenv(var, msg): match = _ENV_VAR_PAT.match(var) if match and match.groups(): envvar = match.groups()[0] if envvar in os.environ: value = os.environ[envvar] <mask>: value = value.decode(""utf8"") return value else: raise InvalidConfigException( ""{} - environment variable '{}' not set"".format(msg, var) ) else: raise InvalidConfigException( ""{} - environment variable name '{}' does not match pattern '{}'"".format( msg, var, _ENV_VAR_PAT_STR ) )",False,if six . PY2 :,"if isinstance ( value , bytes ) :",0.8820016898747209,6.567274736060395
"def _allocate_nbd(self): if not os.path.exists(""/sys/block/nbd0""): self.error = _(""nbd unavailable: module not loaded"") return None while True: if not self._DEVICES: # really want to log this info, not raise self.error = _(""No free nbd devices"") return None device = self._DEVICES.pop() <mask>: break return device",False,"if not os . path . exists ( ""/sys/block/%s/pid"" % os . path . basename ( device ) ) :",if device is None :,0.8820016898747209,0.09472565111320143
"def _expand_deps_java_generation(self): """"""Ensure that all multilingual dependencies such as proto_library generate java code."""""" queue = collections.deque(self.deps) keys = set() while queue: k = queue.popleft() <mask>: keys.add(k) dep = self.target_database[k] if ""generate_java"" in dep.attr: # Has this attribute dep.attr[""generate_java""] = True queue.extend(dep.deps)",True,if k not in keys :,if k not in keys :,0.8820016898747209,100.00000000000004
"def load_syntax(syntax): context = _create_scheme() or {} partition_scanner = PartitionScanner(syntax.get(""partitions"", [])) scanners = {} for part_name, part_scanner in list(syntax.get(""scanner"", {}).items()): scanners[part_name] = Scanner(part_scanner) formats = [] for fname, fstyle in list(syntax.get(""formats"", {}).items()): if isinstance(fstyle, basestring): <mask>: key = fstyle[2:-2] fstyle = context[key] else: fstyle = fstyle % context formats.append((fname, fstyle)) return partition_scanner, scanners, formats",False,"if fstyle . startswith ( ""%("" ) and fstyle . endswith ( "")s"" ) :","if fstyle . endswith ( "" "" ) :",0.8820016898747209,17.808624691433064
"def rollback(self): for operation, values in self.current_transaction_state[::-1]: <mask>: values.remove() elif operation == ""update"": old_value, new_value = values if new_value.full_filename != old_value.full_filename: os.unlink(new_value.full_filename) old_value.write() self._post_xact_cleanup()",False,"if operation == ""insert"" :","if operation == ""delete"" :",0.8820016898747209,59.4603557501361
"def _buildOffsets(offsetDict, localeData, indexStart): o = indexStart for key in localeData: <mask>: for k in key.split(""|""): offsetDict[k] = o else: offsetDict[key] = o o += 1",True,"if ""|"" in key :","if ""|"" in key :",0.8820016898747209,100.00000000000004
"def _check_start_pipeline_execution_errors( graphene_info, execution_params, execution_plan ): if execution_params.step_keys: for step_key in execution_params.step_keys: <mask>: raise UserFacingGraphQLError( graphene_info.schema.type_named(""InvalidStepError"")( invalid_step_key=step_key ) )",False,if not execution_plan . has_step ( step_key ) :,if execution_plan . step_keys [ step_key ] . start_step_key != step_key :,0.8820016898747209,15.980518115118317
"def __setattr__(self, option_name, option_value): if option_name in self._options: # type checking sort = self.OPTIONS[self.arch.name][option_name][0] <mask>: self._options[option_name] = option_value else: raise ValueError( 'Value for option ""%s"" must be of type %s' % (option_name, sort) ) else: super(CFGArchOptions, self).__setattr__(option_name, option_value)",False,"if sort is None or isinstance ( option_value , sort ) :","if sort == ""value"" :",0.8820016898747209,6.204321855952011
"def value(self): quote = False if self.defects: quote = True else: for x in self: <mask>: quote = True if quote: pre = post = """" if self[0].token_type == ""cfws"" or self[0][0].token_type == ""cfws"": pre = "" "" if self[-1].token_type == ""cfws"" or self[-1][-1].token_type == ""cfws"": post = "" "" return pre + quote_string(self.display_name) + post else: return super(DisplayName, self).value",False,"if x . token_type == ""quoted-string"" :",if x . value :,0.8820016898747209,10.536767850900915
"def __init__(self, patch_files, patch_directories): files = [] files_data = {} for filename_data in patch_files: <mask>: filename, data = filename_data else: filename = filename_data data = None if not filename.startswith(os.sep): filename = ""{0}{1}"".format(FakeState.deploy_dir, filename) files.append(filename) if data: files_data[filename] = data self.files = files self.files_data = files_data self.directories = patch_directories",False,"if isinstance ( filename_data , list ) :","if isinstance ( filename_data , ( tuple , list ) ) :",0.8820016898747209,54.75857123833055
"def _evaluateStack(s): op = s.pop() if op in ""+-*/@^"": op2 = _evaluateStack(s) op1 = _evaluateStack(s) result = opn[op](op1, op2) <mask>: print(result) return result else: return op",False,if debug_flag :,if result :,0.8820016898747209,0.0
"def reconnect_user(self, user_id, host_id, server_id): if host_id == settings.local.host_id: return if server_id and self.server.id != server_id: return for client in self.clients.find({""user_id"": user_id}): self.clients.update_id( client[""id""], { ""ignore_routes"": True, }, ) <mask>: self.instance.disconnect_wg(client[""id""]) else: self.instance_com.client_kill(client[""id""])",False,"if len ( client [ ""id"" ] ) > 32 :",if self . instance :,0.8820016898747209,2.564755813286796
"def _get_library(self, name, args): library_database = self._library_manager.get_new_connection_to_library_database() try: last_updated = library_database.get_library_last_updated(name, args) if last_updated: <mask>: self._library_manager.fetch_keywords( name, args, self._libraries_need_refresh_listener ) return library_database.fetch_library_keywords(name, args) return self._library_manager.get_and_insert_keywords(name, args) finally: library_database.close()",False,if time . time ( ) - last_updated > 10.0 :,if last_updated == self . _library_last_updated :,0.8820016898747209,11.633270842295033
"def get_paths(self, path, commit): """"""Return a generator of all filepaths under path at commit."""""" _check_path_is_repo_relative(path) git_path = _get_git_path(path) tree = self.gl_repo.git_repo[commit.tree[git_path].id] assert tree.type == pygit2.GIT_OBJ_TREE for tree_entry in tree: tree_entry_path = os.path.join(path, tree_entry.name) <mask>: for fp in self.get_paths(tree_entry_path, commit): yield fp else: yield tree_entry_path",False,"if tree_entry . type == ""tree"" :",if self . _is_file_path_in_tree ( tree_entry_path ) :,0.8820016898747209,8.3551771205213
"def scan_resource_conf(self, conf): if ""properties"" in conf: if ""attributes"" in conf[""properties""]: <mask>: if conf[""properties""][""attributes""][""exp""]: return CheckResult.PASSED return CheckResult.FAILED",True,"if ""exp"" in conf [ ""properties"" ] [ ""attributes"" ] :","if ""exp"" in conf [ ""properties"" ] [ ""attributes"" ] :",0.8820016898747209,100.00000000000004
"def _set_parse_context(self, tag, tag_attrs): # special case: script or style parse context if not self._wb_parse_context: if tag == ""style"": self._wb_parse_context = ""style"" elif tag == ""script"": <mask>: self._wb_parse_context = ""script""",False,if self . _allow_js_type ( tag_attrs ) :,"if tag == ""style"" :",0.8820016898747209,3.0297048914466935
"def modified(self): paths = set() dictionary_list = [] for op_list in self._operations: if not isinstance(op_list, list): op_list = (op_list,) for item in chain(*op_list): <mask>: continue dictionary = item.dictionary if dictionary.path in paths: continue paths.add(dictionary.path) dictionary_list.append(dictionary) return dictionary_list",False,if item is None :,"if not isinstance ( item , dict ) :",0.8820016898747209,6.27465531099474
def preorder(root): res = [] if not root: return res stack = [] stack.append(root) while stack: root = stack.pop() res.append(root.val) <mask>: stack.append(root.right) if root.left: stack.append(root.left) return res,True,if root . right :,if root . right :,0.8820016898747209,100.00000000000004
"def create(exported_python_target): if exported_python_target not in created: self.context.log.info( ""Creating setup.py project for {}"".format(exported_python_target) ) subject = self.derived_by_original.get( exported_python_target, exported_python_target ) setup_dir, dependencies = self.create_setup_py(subject, dist_dir) created[exported_python_target] = setup_dir if self._recursive: for dep in dependencies: <mask>: create(dep)",False,if is_exported_python_target ( dep ) :,if dep . is_dir ( ) :,0.8820016898747209,11.113458655312735
"def test_array_interface(self, data): result = np.array(data) np.testing.assert_array_equal(result[0], data[0]) result = np.array(data, dtype=object) expected = np.array(list(data), dtype=object) for a1, a2 in zip(result, expected): <mask>: assert np.isnan(a1) and np.isnan(a2) else: tm.assert_numpy_array_equal(a2, a1)",False,if np . isscalar ( a1 ) :,"if isinstance ( a1 , np . ndarray ) :",0.8820016898747209,15.106876986783844
"def valueChanged(plug): changed = plug.getInput() is not None if not changed and isinstance(plug, Gaffer.ValuePlug): <mask>: changed = not Gaffer.NodeAlgo.isSetToUserDefault(plug) else: changed = not plug.isSetToDefault() return changed",False,if Gaffer . NodeAlgo . hasUserDefault ( plug ) :,if plug . isUserDefault ( ) :,0.8820016898747209,11.260801105802155
"def process_tag(hive_name, company, company_key, tag, default_arch): with winreg.OpenKeyEx(company_key, tag) as tag_key: version = load_version_data(hive_name, company, tag, tag_key) <mask>: # if failed to get version bail major, minor, _ = version arch = load_arch_data(hive_name, company, tag, tag_key, default_arch) if arch is not None: exe_data = load_exe(hive_name, company, company_key, tag) if exe_data is not None: exe, args = exe_data return company, major, minor, arch, exe, args",True,if version is not None :,if version is not None :,0.8820016898747209,100.00000000000004
"def __iter__(self): for name, value in self.__class__.__dict__.items(): if isinstance(value, alias_flag_value): continue <mask>: yield (name, self._has_flag(value.flag))",False,"if isinstance ( value , flag_value ) :","if isinstance ( value , alias_flag_value ) :",0.8820016898747209,63.40466277046863
"def connect(self): self.sock = sockssocket() self.sock.setproxy(*proxy_args) if type(self.timeout) in (int, float): self.sock.settimeout(self.timeout) self.sock.connect((self.host, self.port)) if isinstance(self, compat_http_client.HTTPSConnection): <mask>: # Python > 2.6 self.sock = self._context.wrap_socket(self.sock, server_hostname=self.host) else: self.sock = ssl.wrap_socket(self.sock)",False,"if hasattr ( self , ""_context"" ) :",if self . _context :,0.8820016898747209,8.871198783083607
"def frequent_thread_switches(): """"""Make concurrency bugs more likely to manifest."""""" interval = None if not sys.platform.startswith(""java""): if hasattr(sys, ""getswitchinterval""): interval = sys.getswitchinterval() sys.setswitchinterval(1e-6) else: interval = sys.getcheckinterval() sys.setcheckinterval(1) try: yield finally: if not sys.platform.startswith(""java""): <mask>: sys.setswitchinterval(interval) else: sys.setcheckinterval(interval)",True,"if hasattr ( sys , ""setswitchinterval"" ) :","if hasattr ( sys , ""setswitchinterval"" ) :",0.8820016898747209,100.00000000000004
"def vars(self): ret = [] if op.intlist: varlist = op.intlist else: varlist = self.discover for name in varlist: if name in (""0"", ""1"", ""2"", ""8"", ""CPU0"", ""ERR"", ""LOC"", ""MIS"", ""NMI""): varlist.remove(name) if not op.full and len(varlist) > 3: varlist = varlist[-3:] for name in varlist: if name in self.discover: ret.append(name) <mask>: ret.append(self.intmap[name.lower()]) return ret",True,elif name . lower ( ) in self . intmap :,elif name . lower ( ) in self . intmap :,0.8820016898747209,100.00000000000004
"def deleteDuplicates(gadgets, callback=None): toReturn = [] inst = set() count = 0 added = False len_gadgets = len(gadgets) for i, gadget in enumerate(gadgets): inst.add(gadget._gadget) <mask>: count = len(inst) toReturn.append(gadget) added = True if callback: callback(gadget, added, float(i + 1) / (len_gadgets)) added = False return toReturn",False,if len ( inst ) > count :,if count == len_gadgets :,0.8820016898747209,7.809849842300637
"def ident(self): value = self._ident if value is False: value = None # XXX: how will this interact with orig_prefix ? # not exposing attrs for now if orig_prefix is set. if not self.orig_prefix: wrapped = self.wrapped ident = getattr(wrapped, ""ident"", None) <mask>: value = self._wrap_hash(ident) self._ident = value return value",True,if ident is not None :,if ident is not None :,0.8820016898747209,100.00000000000004
"def _flatten_settings_from_form(self, settings, form, form_values): """"""Take a nested dict and return a flat dict of setting values."""""" setting_values = {} for field in form.c: if isinstance(field, _ContainerMixin): setting_values.update( self._flatten_settings_from_form( settings, field, form_values[field._name] ) ) <mask>: setting_values[field._name] = form_values[field._name] return setting_values",False,elif field . _name in settings :,"elif isinstance ( field , dict ) :",0.8820016898747209,7.267884212102741
"def _decorator(cls): for name, meth in inspect.getmembers(cls, inspect.isroutine): if name not in cls.__dict__: continue if name != ""__init__"": <mask>: continue if name in butnot: continue setattr(cls, name, decorator(meth)) return cls",False,"if not private and name . startswith ( ""_"" ) :",if meth is None :,0.8820016898747209,2.564755813286796
"def _do_cmp(f1, f2): bufsize = BUFSIZE with open(f1, ""rb"") as fp1, open(f2, ""rb"") as fp2: while True: b1 = fp1.read(bufsize) b2 = fp2.read(bufsize) <mask>: return False if not b1: return True",True,if b1 != b2 :,if b1 != b2 :,0.8820016898747209,100.00000000000004
"def _memoized(*args): now = time.time() try: value, last_update = self.cache[args] age = now - last_update if self._call_count > self.ctl or age > self.ttl: self._call_count = 0 raise AttributeError <mask>: self._call_count += 1 return value except (KeyError, AttributeError): value = func(*args) if value: self.cache[args] = (value, now) return value except TypeError: return func(*args)",False,if self . ctl :,if value :,0.8820016898747209,0.0
"def check(self, hyperlinks: Dict[str, Hyperlink]) -> Generator[CheckResult, None, None]: self.invoke_threads() total_links = 0 for hyperlink in hyperlinks.values(): <mask>: yield CheckResult( hyperlink.uri, hyperlink.docname, hyperlink.lineno, ""ignored"", """", 0 ) else: self.wqueue.put(CheckRequest(CHECK_IMMEDIATELY, hyperlink), False) total_links += 1 done = 0 while done < total_links: yield self.rqueue.get() done += 1 self.shutdown_threads()",False,if self . is_ignored_uri ( hyperlink . uri ) :,if hyperlink . is_ignored :,0.8820016898747209,18.693159143202898
"def remove_subscriber(self, topic, subscriber): if subscriber in self.subscribers[topic]: <mask>: subscriber._pyroRelease() if hasattr(subscriber, ""_pyroUri""): try: proxy = self.proxy_cache[subscriber._pyroUri] proxy._pyroRelease() del self.proxy_cache[subscriber._pyroUri] except KeyError: pass self.subscribers[topic].discard(subscriber)",True,"if hasattr ( subscriber , ""_pyroRelease"" ) :","if hasattr ( subscriber , ""_pyroRelease"" ) :",0.8820016898747209,100.00000000000004
"def delete_arc(collection, document, origin, target, type): directory = collection real_dir = real_directory(directory) mods = ModificationTracker() projectconf = ProjectConfiguration(real_dir) document = path_join(real_dir, document) with TextAnnotations(document) as ann_obj: # bail as quick as possible if read-only <mask>: raise AnnotationsIsReadOnlyError(ann_obj.get_document()) _delete_arc_with_ann(origin, target, type, mods, ann_obj, projectconf) mods_json = mods.json_response() mods_json[""annotations""] = _json_from_ann(ann_obj) return mods_json",False,if ann_obj . _read_only :,if not ann_obj . get_document ( ) :,0.8820016898747209,24.384183193426086
"def _select_from(self, parent_path, is_dir, exists, listdir): if not is_dir(parent_path): return with _cached(listdir) as listdir: yielded = set() try: successor_select = self.successor._select_from for starting_point in self._iterate_directories( parent_path, is_dir, listdir ): for p in successor_select(starting_point, is_dir, exists, listdir): <mask>: yield p yielded.add(p) finally: yielded.clear()",True,if p not in yielded :,if p not in yielded :,0.8820016898747209,100.00000000000004
"def _fractional_part(self, n, expr, evaluation): n_sympy = n.to_sympy() if n_sympy.is_constant(): <mask>: positive_integer_part = ( Expression(""Floor"", n).evaluate(evaluation).to_python() ) result = n - positive_integer_part else: negative_integer_part = ( Expression(""Ceiling"", n).evaluate(evaluation).to_python() ) result = n - negative_integer_part else: return expr return from_python(result)",False,if n_sympy >= 0 :,if evaluation . is_positive ( ) :,0.8820016898747209,6.27465531099474
"def check_bounds(geometry): if isinstance(geometry[0], (list, tuple)): return list(map(check_bounds, geometry)) else: if geometry[0] > 180 or geometry[0] < -180: raise ValueError( ""Longitude is out of bounds, check your JSON format or data"" ) <mask>: raise ValueError( ""Latitude is out of bounds, check your JSON format or data"" )",False,if geometry [ 1 ] > 90 or geometry [ 1 ] < - 90 :,if geometry [ 1 ] > 180 or geometry [ 1 ] < - 180 :,0.8820016898747209,68.65065103648593
"def get_absolute_path(self, root, path): # find the first absolute path that exists self.root = self.roots[0] for root in self.roots: abspath = os.path.abspath(os.path.join(root, path)) <mask>: self.root = root # make sure all the other methods in the base class know how to find the file break return abspath",False,if os . path . exists ( abspath ) :,if abspath == self . root :,0.8820016898747209,6.082317172853824
"def do_setflow(self, l=""""): try: <mask>: l = str(self.flow_slider.GetValue()) else: l = l.lower() flow = int(l) if self.p.online: self.p.send_now(""M221 S"" + l) self.log(_(""Setting print flow factor to %d%%."") % flow) else: self.logError(_(""Printer is not online."")) except Exception as x: self.logError(_(""You must enter a flow. (%s)"") % (repr(x),))",False,"if not isinstance ( l , str ) or not len ( l ) :",if self . flow_slider :,0.8820016898747209,2.4906123264252495
"def sources(): for d in os.listdir(base): # if d.startswith('talis'): # continue <mask>: continue if d == ""indcat"": continue if not os.path.isdir(base + d): continue yield d",False,"if d . endswith ( ""old"" ) :","if d . startswith ( ""talis"" ) :",0.8820016898747209,29.84745896009822
"def create_accumulator(self) -> tf_metric_accumulators.TFCompilableMetricsAccumulator: configs = zip(self._metric_configs, self._loss_configs) padding_options = None if self._eval_config is not None: model_spec = model_util.get_model_spec(self._eval_config, self._model_name) <mask>: padding_options = model_spec.padding_options return tf_metric_accumulators.TFCompilableMetricsAccumulator( padding_options, [len(m) + len(l) for m, l in configs], desired_batch_size=self._desired_batch_size, )",False,"if model_spec is not None and model_spec . HasField ( ""padding_options"" ) :",if model_spec is not None :,0.8820016898747209,17.120323028183236
"def parseImpl(self, instring, loc, doActions=True): try: loc, tokens = self.expr._parse(instring, loc, doActions, callPreParse=False) except (ParseException, IndexError): <mask>: if self.expr.resultsName: tokens = ParseResults([self.defaultValue]) tokens[self.expr.resultsName] = self.defaultValue else: tokens = [self.defaultValue] else: tokens = [] return loc, tokens",False,if self . defaultValue is not self . __optionalNotMatched :,if self . defaultValue :,0.8820016898747209,17.437038542312457
"def handleConnection(self): # connection handshake try: <mask>: return True self.csock.close() except: ex_t, ex_v, ex_tb = sys.exc_info() tb = util.formatTraceback(ex_t, ex_v, ex_tb) log.warning(""error during connect/handshake: %s; %s"", ex_v, ""\n"".join(tb)) self.csock.close() return False",False,if self . daemon . _handshake ( self . csock ) :,if self . csock . connect ( ) :,0.8820016898747209,20.564695254383224
"def getProc(su, innerTarget): if len(su) == 1: # have a one element wedge proc = (""first"", ""last"") else: <mask>: proc = (""first"", ""last"") # same element can be first and last elif su.isFirst(innerTarget): proc = (""first"",) elif su.isLast(innerTarget): proc = (""last"",) else: proc = () return proc",False,if su . isFirst ( innerTarget ) and su . isLast ( innerTarget ) :,if su . isFirst ( innerTarget ) :,0.8820016898747209,41.686201967850856
"def get_color_dtype(data, column_names): has_color = all(column in data[""points""] for column in column_names) if has_color: color_data_types = [ data[""points""][column_name].dtype for column_name in column_names ] <mask>: raise TypeError( f""Data types of color values are inconsistent: got {color_data_types}"" ) color_data_type = color_data_types[0] else: color_data_type = None return color_data_type",False,if len ( set ( color_data_types ) ) > 1 :,if len ( color_data_types ) != 1 :,0.8820016898747209,52.30193450457853
"def close(self): children = [] for children_part, line_offset, last_line_offset_leaf in self.children_groups: <mask>: try: _update_positions(children_part, line_offset, last_line_offset_leaf) except _PositionUpdatingFinished: pass children += children_part self.tree_node.children = children # Reset the parents for node in children: node.parent = self.tree_node",False,if line_offset != 0 :,if children_part :,0.8820016898747209,7.715486568024961
"def get_multi(self, keys, index=None): with self._lmdb.begin() as txn: result = [] for key in keys: packed = txn.get(key.encode()) <mask>: result.append((key, cbor.loads(packed))) return result",True,if packed is not None :,if packed is not None :,0.8820016898747209,100.00000000000004
"def get_directory_info(prefix, pth, recursive): res = [] directory = os.listdir(pth) directory.sort() for p in directory: if p[0] != ""."": subp = os.path.join(pth, p) p = os.path.join(prefix, p) <mask>: res.append([p, get_directory_info(prefix, subp, 1)]) else: res.append([p, None]) return res",False,if recursive and os . path . isdir ( subp ) :,if recursive :,0.8820016898747209,0.0
"def __schedule(self, workflow_scheduler_id, workflow_scheduler): invocation_ids = self.__active_invocation_ids(workflow_scheduler_id) for invocation_id in invocation_ids: log.debug(""Attempting to schedule workflow invocation [%s]"", invocation_id) self.__attempt_schedule(invocation_id, workflow_scheduler) <mask>: return",False,if not self . monitor_running :,if self . __is_scheduled ( invocation_id ) :,0.8820016898747209,7.141816289329644
"def write(self, data): self.size -= len(data) passon = None if self.size > 0: self.data.append(data) else: if self.size: data, passon = data[: self.size], data[self.size :] else: passon = b"""" <mask>: self.data.append(data) return passon",True,if data :,if data :,0.8820016898747209,0.0
"def __getstate__(self): try: store_func, load_func = self.store_function, self.load_function self.store_function, self.load_function = None, None # ignore analyses. we re-initialize analyses when restoring from pickling so that we do not lose any newly # added analyses classes d = dict( (k, v) for k, v in self.__dict__.items() <mask>: not in { ""analyses"", } ) return d finally: self.store_function, self.load_function = store_func, load_func",True,if k,if k,0.8820016898747209,0.0
"def mouse_down(self, event): if event.button == 1: if self.scrolling: p = event.local <mask>: self.scroll_up() return elif self.scroll_down_rect().collidepoint(p): self.scroll_down() return if event.button == 4: self.scroll_up() if event.button == 5: self.scroll_down() GridView.mouse_down(self, event)",True,if self . scroll_up_rect ( ) . collidepoint ( p ) :,if self . scroll_up_rect ( ) . collidepoint ( p ) :,0.8820016898747209,100.00000000000004
"def on_api_command(self, command, data): if command == ""select"": if not Permissions.PLUGIN_ACTION_COMMAND_PROMPT_INTERACT.can(): return flask.abort(403, ""Insufficient permissions"") if self._prompt is None: return flask.abort(409, ""No active prompt"") choice = data[""choice""] <mask>: return flask.abort( 400, ""{!r} is not a valid value for choice"".format(choice) ) self._answer_prompt(choice)",False,"if not isinstance ( choice , int ) or not self . _prompt . validate_choice ( choice ) :","if not isinstance ( choice , str ) :",0.8820016898747209,15.58281527911091
"def register_predictors(self, model_data_arr): for integration in self._get_integrations(): <mask>: integration.register_predictors(model_data_arr) else: logger.warning( f""There is no connection to {integration.name}. predictor wouldn't be registred."" )",False,if integration . check_connection ( ) :,if integration . is_connected ( ) :,0.8820016898747209,31.02016197007
"def _pack_shears(shearData): shears = list() vidxs = list() for e_idx, entry in enumerate(shearData): # Should be 3 entries <mask>: shears.extend([float(""nan""), float(""nan"")]) vidxs.extend([0, 0]) else: vidx1, vidx2, shear1, shear2 = entry shears.extend([shear1, shear2]) vidxs.extend([vidx1, vidx2]) return (np.asarray(shears, dtype=np.float32), np.asarray(vidxs, dtype=np.uint32))",False,if entry is None :,if e_idx == 3 :,0.8820016898747209,6.567274736060395
"def aiter_cogs(cls) -> AsyncIterator[Tuple[str, str]]: yield ""Core"", ""0"" for _dir in data_manager.cog_data_path().iterdir(): fpath = _dir / ""settings.json"" if not fpath.exists(): continue with fpath.open() as f: try: data = json.load(f) except json.JSONDecodeError: continue if not isinstance(data, dict): continue cog_name = _dir.stem for cog_id, inner in data.items(): <mask>: continue yield cog_name, cog_id",False,"if not isinstance ( inner , dict ) :",if cog_name not in inner :,0.8820016898747209,6.892168295481103
"def subFeaName(m, newNames, state): try: int(m[3], 16) except: return m[0] name = m[2] if name in newNames: # print('sub %r => %r' % (m[0], m[1] + newNames[name] + m[4])) <mask>: print(""sub %r => %r"" % (m[0], m[1] + newNames[name] + m[4])) state[""didChange""] = True return m[1] + newNames[name] + m[4] return m[0]",False,"if name == ""uni0402"" :","if state [ ""didChange"" ] :",0.8820016898747209,7.809849842300637
"def log_graph(self, model: LightningModule, input_array=None): if self._log_graph: if input_array is None: input_array = model.example_input_array <mask>: input_array = model._apply_batch_transfer_handler(input_array) self.experiment.add_graph(model, input_array) else: rank_zero_warn( ""Could not log computational graph since the"" "" `model.example_input_array` attribute is not set"" "" or `input_array` was not given"", UserWarning, )",False,if input_array is not None :,if self . _apply_batch_transfer_handler is not None :,0.8820016898747209,18.20705281109213
"def apply(self, db, person): for family_handle in person.get_family_handle_list(): family = db.get_family_from_handle(family_handle) if family: for event_ref in family.get_event_ref_list(): if event_ref: event = db.get_event_from_handle(event_ref.ref) <mask>: return True if not event.get_date_object(): return True return False",False,if not event . get_place_handle ( ) :,if not event . get_date_object ( ) :,0.8820016898747209,54.52469119630866
"def format(m): if m > 1000: <mask>: return (str(int(m / 1000)), ""km"") else: return (str(round(m / 1000, 1)), ""km"") return (str(m), ""m"")",True,if m % 1000 == 0 :,if m % 1000 == 0 :,0.8820016898747209,100.00000000000004
"def previous(self): try: idx = _jump_list_index next_index = idx + 1 <mask>: next_index = 100 next_index = min(len(_jump_list) - 1, next_index) _jump_list_index = next_index return _jump_list[next_index] except (IndexError, KeyError) as e: return None",True,if next_index > 100 :,if next_index > 100 :,0.8820016898747209,100.00000000000004
"def _validate_and_set_default_hyperparameters(self): """"""Placeholder docstring"""""" # Check if all the required hyperparameters are set. If there is a default value # for one, set it. for name, definition in self.hyperparameter_definitions.items(): if name not in self.hyperparam_dict: spec = definition[""spec""] <mask>: self.hyperparam_dict[name] = spec[""DefaultValue""] elif ""IsRequired"" in spec and spec[""IsRequired""]: raise ValueError(""Required hyperparameter: %s is not set"" % name)",False,"if ""DefaultValue"" in spec :","if ""DefaultValue"" in spec and spec [ ""DefaultValue"" ] :",0.8820016898747209,36.362270465000705
"def _actions_read(self, c): self.action_input.handle_read(c) if c in [curses.KEY_ENTER, util.KEY_ENTER2]: # take action if self.action_input.selected_index == 0: # Cancel self.back_to_parent() <mask>: # Apply self._apply_prefs() client.core.get_config().addCallback(self._update_preferences) elif self.action_input.selected_index == 2: # OK self._apply_prefs() self.back_to_parent()",True,elif self . action_input . selected_index == 1 :,elif self . action_input . selected_index == 1 :,0.8820016898747209,100.00000000000004
"def _split_anonymous_function(s): # Regex is not sufficient to handle differences between anonymous # functions and YAML encoded lists. We perform a sniff test to see # if it might be an anonymous function and then confirm by # decoding it as YAML and testing the result. if s[:1] == ""["" and s[-1:] == ""]"" and "":"" in s: try: l = yaml_util.decode_yaml(s) except Exception: return None, s[1:-1] else: <mask>: return None, s[1:-1] return None",False,"if len ( l ) == 1 and isinstance ( l [ 0 ] , ( six . string_types , int ) ) :","if s [ - 1 ] != ""["" and s [ - 1 ] != ""]"" :",0.8820016898747209,2.3272769937761413
"def test_source_address(self): for addr, is_ipv6 in VALID_SOURCE_ADDRESSES: <mask>: warnings.warn(""No IPv6 support: skipping."", NoIPv6Warning) continue pool = HTTPConnectionPool( self.host, self.port, source_address=addr, retries=False ) self.addCleanup(pool.close) r = pool.request(""GET"", ""/source_address"") self.assertEqual(r.data, b(addr[0]))",False,if is_ipv6 and not HAS_IPV6_AND_DNS :,if not is_ipv6 :,0.8820016898747209,9.471153562668167
"def vim_G(self): """"""Put the cursor on the last character of the file."""""" if self.is_text_wrapper(self.w): <mask>: self.do(""end-of-buffer-extend-selection"") else: self.do(""end-of-buffer"") self.done() else: self.quit()",False,"if self . state == ""visual"" :",if self . w . get_cursor ( ) == self . cursor_start :,0.8820016898747209,10.521495173810226
"def backend_supported(module, manager, **kwargs): if CollectionNodeModule.backend_supported(module, manager, **kwargs): if ""tid"" not in kwargs: return True conn = manager.connection(did=kwargs[""did""]) template_path = ""partitions/sql/{0}/#{0}#{1}#"".format( manager.server_type, manager.version ) SQL = render_template( ""/"".join([template_path, ""backend_support.sql""]), tid=kwargs[""tid""] ) status, res = conn.execute_scalar(SQL) # check if any errors <mask>: return internal_server_error(errormsg=res) return res",False,if not status :,"if status != ""SUCCESS"" :",0.8820016898747209,7.267884212102741
"def _get_regex_config(self, data_asset_name: Optional[str] = None) -> dict: regex_config: dict = copy.deepcopy(self._default_regex) asset: Optional[Asset] = None if data_asset_name: asset = self._get_asset(data_asset_name=data_asset_name) if asset is not None: # Override the defaults <mask>: regex_config[""pattern""] = asset.pattern if asset.group_names: regex_config[""group_names""] = asset.group_names return regex_config",True,if asset . pattern :,if asset . pattern :,0.8820016898747209,100.00000000000004
"def resolve(self, other): if other == ANY_TYPE: return self elif isinstance(other, ComplexType): f = self.first.resolve(other.first) s = self.second.resolve(other.second) <mask>: return ComplexType(f, s) else: return None elif self == ANY_TYPE: return other else: return None",False,if f and s :,if f is not None and s is not None :,0.8820016898747209,11.208466750961147
"def collect_pages(app): new_images = {} for full_path, basename in app.builder.images.iteritems(): base, ext = os.path.splitext(full_path) retina_path = base + ""@2x"" + ext <mask>: new_images[retina_path] = app.env.images[retina_path][1] app.builder.images.update(new_images) return []",True,if retina_path in app . env . images :,if retina_path in app . env . images :,0.8820016898747209,100.00000000000004
"def has_bad_headers(self): headers = [self.sender, self.reply_to] + self.recipients for header in headers: if _has_newline(header): return True if self.subject: if _has_newline(self.subject): for linenum, line in enumerate(self.subject.split(""\r\n"")): <mask>: return True if linenum > 0 and line[0] not in ""\t "": return True if _has_newline(line): return True if len(line.strip()) == 0: return True return False",False,if not line :,if _has_newline ( line ) :,0.8820016898747209,6.27465531099474
"def reader(): try: imgs = mp4_loader(video_path, seg_num, seglen, mode) <mask>: logger.error( ""{} frame length {} less than 1."".format(video_path, len(imgs)) ) yield None, None except: logger.error(""Error when loading {}"".format(mp4_path)) yield None, None imgs_ret = imgs_transform( imgs, mode, seg_num, seglen, short_size, target_size, img_mean, img_std ) label_ret = video_path yield imgs_ret, label_ret",True,if len ( imgs ) < 1 :,if len ( imgs ) < 1 :,0.8820016898747209,100.00000000000004
"def translate_from_sortname(name, sortname): """"""'Translate' the artist name by reversing the sortname."""""" for c in name: ctg = unicodedata.category(c) <mask>: for separator in ("" & "", ""; "", "" and "", "" vs. "", "" with "", "" y ""): if separator in sortname: parts = sortname.split(separator) break else: parts = [sortname] separator = """" return separator.join(map(_reverse_sortname, parts)) return name",False,"if ctg [ 0 ] == ""L"" and unicodedata . name ( c ) . find ( ""LATIN"" ) == - 1 :","if ctg == ""artist"" :",0.8820016898747209,2.036793448355394
"def _to_local_path(path): """"""Convert local path to SFTP path"""""" if sys.platform == ""win32"": # pragma: no cover path = os.fsdecode(path) <mask>: path = path[1:] path = path.replace(""/"", ""\\"") return path",False,"if path [ : 1 ] == ""/"" and path [ 2 : 3 ] == "":"" :","if path . startswith ( ""/"" ) :",0.8820016898747209,5.069215085099986
"def __call__(self, text: str) -> str: for t in self.cleaner_types: if t == ""tacotron"": text = tacotron_cleaner.cleaners.custom_english_cleaners(text) <mask>: text = jaconv.normalize(text) elif t == ""vietnamese"": if vietnamese_cleaners is None: raise RuntimeError(""Please install underthesea"") text = vietnamese_cleaners.vietnamese_cleaner(text) else: raise RuntimeError(f""Not supported: type={t}"") return text",True,"elif t == ""jaconv"" :","elif t == ""jaconv"" :",0.8820016898747209,100.00000000000004
"def cb_syncthing_system_data(self, daemon, mem, cpu, d_failed, d_total): if self.daemon.get_my_id() in self.devices: # Update my device display device = self.devices[self.daemon.get_my_id()] device[""ram""] = sizeof_fmt(mem) device[""cpu""] = ""%3.2f%%"" % (cpu) <mask>: device[""announce""] = _(""disabled"") else: device[""announce""] = ""%s/%s"" % (d_total - d_failed, d_total)",False,if d_total == 0 :,if d_failed == 0 :,0.8820016898747209,50.000000000000014
"def update_kls(self, sampled_kls): for i, kl in enumerate(sampled_kls): <mask>: self.kl_coeff_val[i] *= 0.5 elif kl > 1.5 * self.kl_target: self.kl_coeff_val[i] *= 2.0 return self.kl_coeff_val",False,if kl < self . kl_target / 1.5 :,if kl < 0.5 * self . kl_target :,0.8820016898747209,48.326978309062206
"def DeleteEmptyCols(self): cols2delete = [] for c in range(0, self.GetCols()): f = True for r in range(0, self.GetRows()): if self.FindItemAtPosition((r, c)) is not None: f = False <mask>: cols2delete.append(c) for i in range(0, len(cols2delete)): self.ShiftColsLeft(cols2delete[i] + 1) cols2delete = [x - 1 for x in cols2delete]",True,if f :,if f :,0.8820016898747209,0.0
"def get_session(self): if self._session is None: session = super(ChildResourceManager, self).get_session() <mask>: session = session.get_session_for_resource(self.resource_type.resource) self._session = session return self._session",False,if self . resource_type . resource != constants . RESOURCE_ACTIVE_DIRECTORY :,if self . resource_type is not None :,0.8820016898747209,24.141769716889275
"def _get_master_authorized_networks_config(self, raw_cluster): if raw_cluster.get(""masterAuthorizedNetworksConfig""): config = raw_cluster.get(""masterAuthorizedNetworksConfig"") config[""includes_public_cidr""] = False for block in config[""cidrBlocks""]: <mask>: config[""includes_public_cidr""] = True return config else: return {""enabled"": False, ""cidrBlocks"": [], ""includes_public_cidr"": False}",False,"if block [ ""cidrBlock"" ] == ""0.0.0.0/0"" :","if block [ ""cidr_id"" ] == self . _get_master_authorized_cidr_id ( block ) :",0.8820016898747209,17.89570640154152
"def scan_folder(folder): scanned_files = [] for root, dirs, files in os.walk(folder): dirs[:] = [d for d in dirs if d != ""__pycache__""] relative_path = os.path.relpath(root, folder) for f in files: <mask>: continue relative_name = os.path.normpath(os.path.join(relative_path, f)).replace( ""\\"", ""/"" ) scanned_files.append(relative_name) return sorted(scanned_files)",False,"if f . endswith ( "".pyc"" ) :","if f . startswith ( ""__pycache__"" ) :",0.8820016898747209,20.200106912694157
"def read_progress(self): while True: processed_file = self.queue.get() self.threading_completed.append(processed_file) total_number = len(self.file_list) completed_number = len(self.threading_completed) # Just for the record, this slows down book searching by about 20% if _progress_emitter: # Skip update in reading mode _progress_emitter.update_progress(completed_number * 100 // total_number) <mask>: break",False,if total_number == completed_number :,if completed_number == total_number :,0.8820016898747209,51.33450480401705
"def next_instruction_is_function_or_class(lines): """"""Is the first non-empty, non-commented line of the cell either a function or a class?"""""" parser = StringParser(""python"") for i, line in enumerate(lines): if parser.is_quoted(): parser.read_line(line) continue parser.read_line(line) if not line.strip(): # empty line <mask>: return False continue if line.startswith(""def "") or line.startswith(""class ""): return True if line.startswith((""#"", ""@"", "" "", "")"")): continue return False return False",False,if i > 0 and not lines [ i - 1 ] . strip ( ) :,if i == 0 :,0.8820016898747209,3.086457674499703
def __next__(self): try: data = next(self.iter_loader) except StopIteration: self._epoch += 1 <mask>: self._dataloader.sampler.set_epoch(self._epoch) self.iter_loader = iter(self._dataloader) data = next(self.iter_loader) return data,False,"if hasattr ( self . _dataloader . sampler , ""set_epoch"" ) :",if self . _epoch % self . _max_epoch == 0 :,0.8820016898747209,12.09431425432932
"def dgl_mp_batchify_fn(data): if isinstance(data[0], tuple): data = zip(*data) return [dgl_mp_batchify_fn(i) for i in data] for dt in data: if dt is not None: <mask>: return [d for d in data if isinstance(d, dgl.DGLGraph)] elif isinstance(dt, nd.NDArray): pad = Pad(axis=(1, 2), num_shards=1, ret_length=False) data_list = [dt for dt in data if dt is not None] return pad(data_list)",False,"if isinstance ( dt , dgl . DGLGraph ) :","if isinstance ( dt , dgl . DGLArray ) :",0.8820016898747209,70.71067811865478
"def f(self, info): for k in keys: <mask>: for k2 in list(info.keys()): if k(k2): info.pop(k2) else: info.pop(k, None)",False,if callable ( k ) :,"if isinstance ( info , dict ) :",0.8820016898747209,13.134549472120788
"def create(path, binary=False): for i in range(10): try: os.makedirs(os.path.dirname(path), exist_ok=True) <mask>: return open(path, ""wb"") else: return open(path, ""w"", encoding=""utf-8"") if i > 0: log(True, f""Created {path} at attempt {i + 1}"") except: time.sleep(0.5) else: raise Error(f""Failed to create {path}"")",True,if binary :,if binary :,0.8820016898747209,0.0
"def validate_update(self, update_query): structure = DotCollapsedDict(self.doc_class.structure) for op, fields in update_query.iteritems(): for field in fields: if op != ""$unset"" and op != ""$rename"": <mask>: raise UpdateQueryError( ""'%s' not found in %s's structure"" % (field, self.doc_class.__name__) )",True,if field not in structure :,if field not in structure :,0.8820016898747209,100.00000000000004
"def check_enums_ATLAS_ISAEXT(lines): for i, isaext in enumerate(ATLAS_ISAEXT): got = lines.pop(0).strip() <mask>: expect = ""none: 1"" else: expect = ""{0}: {1}"".format(isaext, 1 << i) if got != expect: raise RuntimeError( ""ATLAS_ISAEXT mismatch at position "" + str(i) + "": got >>"" + got + ""<<, expected >>"" + expect + ""<<"" )",False,if i == 0 :,"if isaext == ""none"" :",0.8820016898747209,13.134549472120788
"def _test_export_session_csv(self, test_session=None): with self.app.test_request_context(): <mask>: test_session = SessionFactory() field_data = export_sessions_csv([test_session]) session_row = field_data[1] self.assertEqual(session_row[0], ""example (accepted)"") self.assertEqual(session_row[9], ""accepted"")",False,if not test_session :,if test_session is None :,0.8820016898747209,27.77619034011791
"def get_report_to_platform(self, args, scan_reports): if self.bc_api_key: <mask>: repo_id = self.get_repository(args) self.setup_bridgecrew_credentials( bc_api_key=self.bc_api_key, repo_id=repo_id ) if self.is_integration_configured(): self._upload_run(args, scan_reports)",False,if args . directory :,if self . is_bridgecrew_configured ( args ) :,0.8820016898747209,4.789232204309912
"def test_fvalue(self): if not getattr(self, ""skip_f"", False): rtol = getattr(self, ""rtol"", 1e-10) assert_allclose(self.res1.fvalue, self.res2.F, rtol=rtol) <mask>: # only available with ivreg2 assert_allclose(self.res1.f_pvalue, self.res2.Fp, rtol=rtol) else: raise pytest.skip(""TODO: document why this test is skipped"")",False,"if hasattr ( self . res2 , ""Fp"" ) :",if self . res1 . f_pvalue is not None :,0.8820016898747209,8.054496384843702
"def fix_repeating_arguments(self): """"""Fix elements that should accumulate/increment values."""""" either = [list(child.children) for child in transform(self).children] for case in either: for e in [child for child in case if case.count(child) > 1]: if type(e) is Argument or type(e) is Option and e.argcount: if e.value is None: e.value = [] <mask>: e.value = e.value.split() if type(e) is Command or type(e) is Option and e.argcount == 0: e.value = 0 return self",False,elif type ( e . value ) is not list :,elif type ( e ) is str :,0.8820016898747209,29.36697784628212
"def touch(self): if not self.exists(): try: self.parent().touch() except ValueError: pass node = self._fs.touch(self.pathnames, {}) if not node.isdir: raise AssertionError(""Not a folder: %s"" % self.path) <mask>: self.watcher.emit(""created"", self)",True,if self . watcher :,if self . watcher :,0.8820016898747209,100.00000000000004
"def __init__(self, _inf=None, _tzinfos=None): if _inf: self._tzinfos = _tzinfos self._utcoffset, self._dst, self._tzname = _inf else: _tzinfos = {} self._tzinfos = _tzinfos self._utcoffset, self._dst, self._tzname = self._transition_info[0] _tzinfos[self._transition_info[0]] = self for inf in self._transition_info[1:]: <mask>: _tzinfos[inf] = self.__class__(inf, _tzinfos)",False,if not _tzinfos . has_key ( inf ) :,if inf not in _tzinfos :,0.8820016898747209,8.462236332847391
"def test_sample_output(): comment = ""SAMPLE OUTPUT"" skip_files = [""__init__.py""] errors = [] for _file in sorted(MODULE_PATH.iterdir()): <mask>: with _file.open() as f: if comment not in f.read(): errors.append((comment, _file)) if errors: line = ""Missing sample error(s) detected!\n\n"" for error in errors: line += ""`{}` is not in module `{}`\n"".format(*error) print(line[:-1]) assert False",False,"if _file . suffix == "".py"" and _file . name not in skip_files :",if _file . name not in skip_files :,0.8820016898747209,36.78794411714425
"def http_get(url, target): req = requests.get(url, stream=True) content_length = req.headers.get(""Content-Length"") total = int(content_length) if content_length is not None else None progress = tqdm(unit=""B"", total=total) with open(target, ""wb"") as target_file: for chunk in req.iter_content(chunk_size=1024): <mask>: # filter out keep-alive new chunks progress.update(len(chunk)) target_file.write(chunk) progress.close()",True,if chunk :,if chunk :,0.8820016898747209,0.0
"def _elements_to_datasets(self, elements, level=0): for element in elements: extra_kwds = {""identifier_%d"" % level: element[""name""]} <mask>: for inner_element in self._elements_to_datasets( element[""elements""], level=level + 1 ): dataset = extra_kwds.copy() dataset.update(inner_element) yield dataset else: dataset = extra_kwds extra_kwds.update(element) yield extra_kwds",True,"if ""elements"" in element :","if ""elements"" in element :",0.8820016898747209,100.00000000000004
"def update_dict(a, b): for key, value in b.items(): if value is None: continue <mask>: a[key] = value elif isinstance(a[key], dict) and isinstance(value, dict): update_dict(a[key], value) elif isinstance(a[key], list): a[key].append(value) else: a[key] = [a[key], value]",False,if key not in a :,"if isinstance ( a [ key ] , dict ) and isinstance ( value , dict ) :",0.8820016898747209,3.0372940354383413
"def scan(self, targets): for target in targets: target.print_infos() if self.is_interesting(target): self.target[""other""].append(target) <mask>: return target return None",False,if self . match ( target ) :,if self . is_interesting ( target ) :,0.8820016898747209,37.99178428257963
"def printConnections(switches): ""Compactly print connected nodes to each switch"" for sw in switches: output(""%s: "" % sw) for intf in sw.intfList(): link = intf.link <mask>: intf1, intf2 = link.intf1, link.intf2 remote = intf1 if intf1.node != sw else intf2 output(""%s(%s) "" % (remote.node, sw.ports[intf])) output(""\n"")",True,if link :,if link :,0.8820016898747209,0.0
"def __cut(sentence): global emit_P prob, pos_list = viterbi(sentence, ""BMES"", start_P, trans_P, emit_P) begin, nexti = 0, 0 # print pos_list, sentence for i, char in enumerate(sentence): pos = pos_list[i] if pos == ""B"": begin = i <mask>: yield sentence[begin : i + 1] nexti = i + 1 elif pos == ""S"": yield char nexti = i + 1 if nexti < len(sentence): yield sentence[nexti:]",False,"elif pos == ""E"" :",if begin < len ( sentence ) :,0.8820016898747209,5.522397783539471
"def check_files(self, paths=None): """"""Run all checks on the paths."""""" if paths is None: paths = self.paths report = self.options.report runner = self.runner report.start() try: for path in paths: <mask>: self.input_dir(path) elif not self.excluded(path): runner(path) except KeyboardInterrupt: print(""... stopped"") report.stop() return report",False,if os . path . isdir ( path ) :,if self . is_dir ( path ) :,0.8820016898747209,29.071536848410968
"def verts_of_loop(edge_loop): verts = [] for e0, e1 in iter_pairs(edge_loop, False): <mask>: v0 = e0.shared_vert(e1) verts += [e0.other_vert(v0), v0] verts += [e1.other_vert(verts[-1])] if len(verts) > 1 and verts[0] == verts[-1]: return verts[:-1] return verts",False,if not verts :,if e0 . shared_vert ( e1 ) :,0.8820016898747209,4.990049701936832
"def generator(self, data): for task in data: # Do we scan everything or just /bin/bash instances? <mask>: continue for bucket in task.bash_hash_entries(): yield ( 0, [ int(task.p_pid), str(task.p_comm), int(bucket.times_found), str(bucket.key), str(bucket.data.path), ], )",False,"if not ( self . _config . SCAN_ALL or str ( task . p_comm ) == ""bash"" ) :","if task . p_pid == ""bin"" :",0.8820016898747209,9.603965741863096
"def __get_ratio(self): """"""Return splitter ratio of the main splitter."""""" c = self.c free_layout = c.free_layout if free_layout: w = free_layout.get_main_splitter() if w: aList = w.sizes() <mask>: n1, n2 = aList # 2017/06/07: guard against division by zero. ratio = 0.5 if n1 + n2 == 0 else float(n1) / float(n1 + n2) return ratio return 0.5",False,if len ( aList ) == 2 :,if len ( aList ) > 0 :,0.8820016898747209,47.750342648354646
"def geterrors(self): """"""Get all error messages."""""" notes = self.getnotes(origin=""translator"").split(""\n"") errordict = {} for note in notes: <mask>: error = note.replace(""(pofilter) "", """") errorname, errortext = error.split("": "", 1) errordict[errorname] = errortext return errordict",False,"if ""(pofilter) "" in note :","if note . startswith ( ""pofilter"" ) :",0.8820016898747209,7.056995965394887
"def rename_path(self, path, new_path): logger.debug(""rename_path '%s' -> '%s'"" % (path, new_path)) dirs = self.readdir(path) for d in dirs: if d in [""."", ""..""]: continue d_path = """".join([path, ""/"", d]) d_new_path = """".join([new_path, ""/"", d]) attr = self.getattr(d_path) <mask>: self.rename_path(d_path, d_new_path) else: self.rename_item(d_path, d_new_path) self.rename_item(path, new_path, dir=True)",False,"if stat . S_ISDIR ( attr [ ""st_mode"" ] ) :","if attr == ""dir"" :",0.8820016898747209,2.6809511148331087
"def index(self, url_id: int) -> FlaskResponse: # pylint: disable=no-self-use url = db.session.query(models.Url).get(url_id) if url and url.url: explore_url = ""//superset/explore/?"" <mask>: explore_url += f""r={url_id}"" return redirect(explore_url[1:]) return redirect(url.url[1:]) flash(""URL to nowhere..."", ""danger"") return redirect(""/"")",False,if url . url . startswith ( explore_url ) :,if url_id :,0.8820016898747209,6.265199648711648
"def testShortCircuit(self): """"""Test that creation short-circuits to reuse existing references"""""" sd = {} for s in self.ss: sd[s] = 1 for t in self.ts: <mask>: self.assertTrue(sd.has_key(safeRef(t.x))) self.assertTrue(safeRef(t.x) in sd) else: self.assertTrue(sd.has_key(safeRef(t))) self.assertTrue(safeRef(t) in sd)",False,"if hasattr ( t , ""x"" ) :","if isinstance ( t , ( int , long ) ) :",0.8820016898747209,16.59038701421971
"def wrapped(request, *args, **kwargs): if not request.user.is_authenticated(): request.session[""_next""] = request.get_full_path() <mask>: redirect_uri = reverse( ""sentry-auth-organization"", args=[kwargs[""organization_slug""]] ) else: redirect_uri = get_login_url() return HttpResponseRedirect(redirect_uri) return func(request, *args, **kwargs)",False,"if ""organization_slug"" in kwargs :","if kwargs . get ( ""organization_slug"" ) :",0.8820016898747209,35.65506208559251
"def read_info(reader, dump=None): line_number_table_length = reader.read_u2() <mask>: reader.debug( "" "" * dump, ""Line numbers (%s total):"" % line_number_table_length ) line_numbers = [] for i in range(0, line_number_table_length): start_pc = reader.read_u2() line_number = reader.read_u2() if dump is not None: reader.debug("" "" * (dump + 1), ""%s: %s"" % (start_pc, line_number)) line_numbers.append((start_pc, line_number)) return LineNumberTable(line_numbers)",True,if dump is not None :,if dump is not None :,0.8820016898747209,100.00000000000004
"def compute_timer_precision(timer): precision = None points = 0 timeout = timeout_timer() + 1.0 previous = timer() while timeout_timer() < timeout or points < 5: for _ in XRANGE(10): t1 = timer() t2 = timer() dt = t2 - t1 if 0 < dt: break else: dt = t2 - previous if dt <= 0.0: continue <mask>: precision = min(precision, dt) else: precision = dt points += 1 previous = timer() return precision",False,if precision is not None :,if precision :,0.8820016898747209,0.0
def get_hi_lineno(self): lineno = Node.get_hi_lineno(self) if self.expr1 is None: pass else: lineno = self.expr1.get_hi_lineno() <mask>: pass else: lineno = self.expr2.get_hi_lineno() if self.expr3 is None: pass else: lineno = self.expr3.get_hi_lineno() return lineno,True,if self . expr2 is None :,if self . expr2 is None :,0.8820016898747209,100.00000000000004
"def validate_cluster_resource_group(cmd, namespace): if namespace.cluster_resource_group is not None: client = get_mgmt_service_client( cmd.cli_ctx, ResourceType.MGMT_RESOURCE_RESOURCES ) <mask>: raise InvalidArgumentValueError( ""Invalid --cluster-resource-group '%s': resource group must not exist."" % namespace.cluster_resource_group )",False,if client . resource_groups . check_existence ( namespace . cluster_resource_group ) :,if not client . cluster_resource_group_exists ( namespace . cluster_resource_group ) :,0.8820016898747209,51.18071215493855
"def find_word_bounds(self, text, index, allowed_chars): right = left = index done = False while not done: <mask>: done = True elif not self.word_boundary_char(text[left - 1]): left -= 1 else: done = True done = False while not done: if right == len(text): done = True elif not self.word_boundary_char(text[right]): right += 1 else: done = True return left, right",False,if left == 0 :,if left == len ( text ) :,0.8820016898747209,31.55984539112946
"def _check_good_input(self, X, y=None): if isinstance(X, dict): lengths = [len(X1) for X1 in X.values()] if len(set(lengths)) > 1: raise ValueError(""Not all values of X are of equal length."") x_len = lengths[0] else: x_len = len(X) if y is not None: <mask>: raise ValueError(""X and y are not of equal length."") if self.regression and y is not None and y.ndim == 1: y = y.reshape(-1, 1) return X, y",False,if len ( y ) != x_len :,if x_len != self . max_length :,0.8820016898747209,17.242221289766636
"def _get_text_nodes(nodes, html_body): text = [] open_tags = 0 for node in nodes: if isinstance(node, HtmlTag): if node.tag_type == OPEN_TAG: open_tags += 1 <mask>: open_tags -= 1 elif ( isinstance(node, HtmlDataFragment) and node.is_text_content and open_tags == 0 ): text.append(html_body[node.start : node.end]) return text",True,elif node . tag_type == CLOSE_TAG :,elif node . tag_type == CLOSE_TAG :,0.8820016898747209,100.00000000000004
"def _get_spyne_type(cls_name, k, v): try: v = NATIVE_MAP.get(v, v) except TypeError: return try: subc = issubclass(v, ModelBase) or issubclass(v, SelfReference) except: subc = False if subc: if issubclass(v, Array) and len(v._type_info) != 1: raise Exception(""Invalid Array definition in %s.%s."" % (cls_name, k)) <mask>: raise Exception(""Please specify the number of dimensions"") return v",False,"elif issubclass ( v , Point ) and v . Attributes . dim is None :",if len ( v . _type_info ) != 2 :,0.8820016898747209,7.362479602707965
"def customize(cls, **kwargs): """"""return a class with some existing attributes customized"""""" for name, value in kwargs.iteritems(): <mask>: raise TransportError( ""you cannot customize the protected attribute %s"" % name ) if not hasattr(cls, name): raise TransportError(""Transport has no attribute %s"" % name) NewSubClass = type(""Customized_{}"".format(cls.__name__), (cls,), kwargs) return NewSubClass",False,"if name in [ ""cookie"" , ""circuit"" , ""upstream"" , ""downstream"" , ""stream"" ] :",if value is not None :,0.8820016898747209,0.4067907049567216
"def test_UNrelativize(self): import URIlib relative = self.relative + self.full_relativize for base, rel, fullpath, common in relative: URI = uriparse.UnRelativizeURL(base, rel) fullURI = URIlib.URIParser(URI) # We need to canonicalize the result from unrelativize # compared to the original full path we expect to see. <mask>: fullpath = fullpath[:-1] self.failUnlessSamePath( os.path.normcase(fullURI.path), os.path.normcase(fullpath) )",False,"if fullpath [ - 1 ] in ( ""/"" , ""\\"" ) :","if fullpath . endswith ( ""/"" ) :",0.8820016898747209,18.473424219567708
"def get_release_info(file_path=RELEASE_FILE): RELEASE_TYPE_REGEX = re.compile(r""^[Rr]elease [Tt]ype: (major|minor|patch)$"") with open(file_path, ""r"") as f: line = f.readline() match = RELEASE_TYPE_REGEX.match(line) <mask>: print( ""The file RELEASE.md should start with `Release type` "" ""and specify one of the following values: major, minor or patch."" ) sys.exit(1) type_ = match.group(1) changelog = """".join([line for line in f.readlines()]).strip() return type_, changelog",True,if not match :,if not match :,0.8820016898747209,100.00000000000004
"def _get_next_history_entry(self): if self._history: hist_len = len(self._history) - 1 self.history_index = min(hist_len, self.history_index + 1) index = self.history_index <mask>: self.history_index += 1 return self._history[index] return """"",False,if self . history_index == hist_len :,if self . _history [ index ] is not None :,0.8820016898747209,15.580105704117443
"def star_op(self): """"""Put a '*' op, with special cases for *args."""""" val = ""*"" if self.paren_level: i = len(self.code_list) - 1 if self.code_list[i].kind == ""blank"": i -= 1 token = self.code_list[i] <mask>: self.op_no_blanks(val) elif token.value == "","": self.blank() self.add_token(""op-no-blanks"", val) else: self.op(val) else: self.op(val)",False,"if token . kind == ""lt"" :","if token . kind == ""no-blanks"" :",0.8820016898747209,70.71067811865478
"def get_safe_settings(): ""Returns a dictionary of the settings module, with sensitive settings blurred out."" settings_dict = {} for k in dir(settings): <mask>: if HIDDEN_SETTINGS.search(k): settings_dict[k] = ""********************"" else: settings_dict[k] = getattr(settings, k) return settings_dict",False,if k . isupper ( ) :,"if k . startswith ( ""_"" ) :",0.8820016898747209,20.556680845025987
"def nextEditable(self): """"""Moves focus of the cursor to the next editable window"""""" if self.currentEditable is None: if len(self._editableChildren): self._currentEditableRef = self._editableChildren[0] else: for ref in weakref.getweakrefs(self.currentEditable): <mask>: cei = self._editableChildren.index(ref) nei = cei + 1 if nei >= len(self._editableChildren): nei = 0 self._currentEditableRef = self._editableChildren[nei] return self.currentEditable",True,if ref in self . _editableChildren :,if ref in self . _editableChildren :,0.8820016898747209,100.00000000000004
"def _handle_dependents_type(types, type_str, type_name, rel_name, row): if types[type_str[0]] is None: <mask>: type_name = ""index"" rel_name = row[""indname""] + "" ON "" + rel_name elif type_str[0] == ""o"": type_name = ""operator"" rel_name = row[""relname""] else: type_name = types[type_str[0]] return type_name, rel_name",True,"if type_str [ 0 ] == ""i"" :","if type_str [ 0 ] == ""i"" :",0.8820016898747209,100.00000000000004
"def streamErrorHandler(self, conn, error): name, text = ""error"", error.getData() for tag in error.getChildren(): <mask>: if tag.getName() == ""text"": text = tag.getData() else: name = tag.getName() if name in stream_exceptions.keys(): exc = stream_exceptions[name] else: exc = StreamError raise exc((name, text))",False,if tag . getNamespace ( ) == NS_XMPP_STREAMS :,"if tag . getName ( ) == ""name"" :",0.8820016898747209,26.83544415402699
"def _validate_names(self, settings: _SettingsType) -> None: """"""Make sure all settings exist."""""" unknown = [] for name in settings: <mask>: unknown.append(name) if unknown: errors = [ configexc.ConfigErrorDesc( ""While loading options"", ""Unknown option {}"".format(e) ) for e in sorted(unknown) ] raise configexc.ConfigFileErrors(""autoconfig.yml"", errors)",False,if name not in configdata . DATA :,if name not in self . _DEFAULT_OPTIONS :,0.8820016898747209,25.965358893403383
"def can_haz(self, target, credentials): """"""Check whether key-values in target are present in credentials."""""" # TODO(termie): handle ANDs, probably by providing a tuple instead of a # string for requirement in target: key, match = requirement.split("":"", 1) check = credentials.get(key) <mask>: check = [check] if match in check: return True",False,"if check is None or isinstance ( check , basestring ) :","if not isinstance ( check , tuple ) :",0.8820016898747209,26.432408210372945
"def _recursive_fx_apply(input: dict, fx): for k, v in input.items(): <mask>: v = torch.tensor(v) if isinstance(v, torch.Tensor): v = fx(v.float()) input[k] = v else: _recursive_fx_apply(v, fx)",False,"if isinstance ( v , list ) :","if isinstance ( v , torch . Tensor ) :",0.8820016898747209,45.180100180492246
"def get(self, url, **kwargs): app, url = self._prepare_call(url, kwargs) if app: <mask>: self._first_ping = False return EmptyCapabilitiesResponse() elif ""Hello0"" in url and ""1.2.1"" in url and ""v1"" in url: return ErrorApiResponse() else: response = app.get(url, **kwargs) return TestingResponse(response) else: return requests.get(url, **kwargs)",False,"if url . endswith ( ""ping"" ) and self . _first_ping :","if ""Hello0"" in url and ""1.2.1"" in url and ""v1"" in url :",0.8820016898747209,3.168429634207702
"def server_thread_fn(): server_ctx = ssl.create_default_context(ssl.Purpose.CLIENT_AUTH) server_ctx.load_cert_chain(""trio-test-1.pem"") server = server_ctx.wrap_socket( server_sock, server_side=True, suppress_ragged_eofs=False, ) while True: data = server.recv(4096) print(""server got:"", data) <mask>: print(""server waiting for client to finish everything"") client_done.wait() print(""server attempting to send back close-notify"") server.unwrap() print(""server ok"") break server.sendall(data)",False,if not data :,if client_done :,0.8820016898747209,12.703318703865365
"def find_hostnames(data): # sends back an array of hostnames hostnames = [] for i in re.finditer(hostname_regex, data): h = string.lower(i.group(1)) tld = h.split(""."")[-1:][0] <mask>: hostnames.append(h) return hostnames",False,if tld in tlds :,"if tld == ""host"" :",0.8820016898747209,12.22307556087252
"def Validate(self, win): textCtrl = self.GetWindow() text = textCtrl.GetValue().strip() sChar = Character.getInstance() try: <mask>: raise ValueError(_t(""You must supply a name for the Character!"")) elif text in [x.name for x in sChar.getCharacterList()]: raise ValueError( _t(""Character name already in use, please choose another."") ) return True except ValueError as e: pyfalog.error(e) wx.MessageBox(""{}"".format(e), _t(""Error"")) textCtrl.SetFocus() return False",False,if len ( text ) == 0 :,"if text == """" :",0.8820016898747209,12.411264901419441
def get_random_user_agent(agent_list=UA_CACHE): if not len(agent_list): ua_file = file(UA_FILE) for line in ua_file: line = line.strip() <mask>: agent_list.append(line) ua = random.choice(UA_CACHE) return ua,True,if line :,if line :,0.8820016898747209,0.0
"def _validate_action_like_for_prefixes(self, key): for statement in self._statements: <mask>: if isinstance(statement[key], string_types): self._validate_action_prefix(statement[key]) else: for action in statement[key]: self._validate_action_prefix(action)",True,if key in statement :,if key in statement :,0.8820016898747209,100.00000000000004
"def predict(self, X): if self.regression: return self.predict_proba(X) else: y_pred = np.argmax(self.predict_proba(X), axis=1) <mask>: y_pred = self.enc_.inverse_transform(y_pred) return y_pred",False,if self . use_label_encoder :,if self . enc_inverse :,0.8820016898747209,20.873176328735713
"def _threaded_request_tracker(self, builder): while True: event_type = self._read_q.get() <mask>: return payload = {""body"": b""""} request_id = builder.build_record(event_type, payload, """") self._write_q.put_nowait(request_id)",False,if event_type is False :,if event_type is None :,0.8820016898747209,64.34588841607616
"def __call__(self, value): try: super(EmailValidator, self).__call__(value) except ValidationError as e: # Trivial case failed. Try for possible IDN domain-part <mask>: parts = value.split(""@"") try: parts[-1] = parts[-1].encode(""idna"").decode(""ascii"") except UnicodeError: raise e super(EmailValidator, self).__call__(""@"".join(parts)) else: raise",False,"if value and ""@"" in value :","if ""@idna"" in e . message :",0.8820016898747209,13.134549472120794
"def PreprocessConditionalStatement(self, IfList, ReplacedLine): while self: if self.__Token: x = 1 <mask>: if self <= 2: continue RegionSizeGuid = 3 if not RegionSizeGuid: RegionLayoutLine = 5 continue RegionLayoutLine = self.CurrentLineNumber return 1",False,elif not IfList :,if self . CurrentLineNumber == 1 :,0.8820016898747209,5.522397783539471
"def _arg_with_type(self): for t in self.d[""Args""]: m = re.search(""([A-Za-z0-9_-]+)\s{0,4}(\(.+\))\s{0,4}:"", t) <mask>: self.args[m.group(1)] = m.group(2) return self.args",True,if m :,if m :,0.8820016898747209,0.0
"def get_palette_for_custom_classes(self, class_names, palette=None): if self.label_map is not None: # return subset of palette palette = [] for old_id, new_id in sorted(self.label_map.items(), key=lambda x: x[1]): if new_id != -1: palette.append(self.PALETTE[old_id]) palette = type(self.PALETTE)(palette) elif palette is None: <mask>: palette = np.random.randint(0, 255, size=(len(class_names), 3)) else: palette = self.PALETTE return palette",False,if self . PALETTE is None :,if len ( class_names ) == 3 :,0.8820016898747209,4.456882760699063
"def Visit_star_expr(self, node): # pylint: disable=invalid-name # star_expr ::= '*' expr for child in node.children: self.Visit(child) <mask>: _AppendTokenSubtype(child, format_token.Subtype.UNARY_OPERATOR) _AppendTokenSubtype(child, format_token.Subtype.VARARGS_STAR)",False,"if isinstance ( child , pytree . Leaf ) and child . value == ""*"" :","if child . type == ""star_expr"" :",0.8820016898747209,10.690239565034746
"def create_if_compatible(cls, typ: Type, *, root: ""RootNode"") -> Optional[""Node""]: if cls.compatible_types: target_type: Type = typ <mask>: target_type = getattr(typ, ""__origin__"", None) or typ if cls._issubclass(target_type, cls.compatible_types): return cls(typ, root=root) return None",False,if cls . use_origin :,"if not isinstance ( target_type , type ) :",0.8820016898747209,4.9323515694897075
"def grep_full_py_identifiers(tokens): global pykeywords tokens = list(tokens) i = 0 while i < len(tokens): tokentype, token = tokens[i] i += 1 if tokentype != ""id"": continue while ( i + 1 < len(tokens) and tokens[i] == (""op"", ""."") and tokens[i + 1][0] == ""id"" ): token += ""."" + tokens[i + 1][1] i += 2 <mask>: continue if token in pykeywords: continue if token[0] in "".0123456789"": continue yield token",False,"if token == """" :","if token == ""op"" :",0.8820016898747209,59.4603557501361
"def create_config_filepath(cls, visibility=None): if cls.is_local(visibility): # Local to this directory base_path = os.path.join(""."") <mask>: # Add it to the current ""./.polyaxon"" base_path = os.path.join(base_path, "".polyaxon"") cls._create_dir(base_path) elif cls.CONFIG_PATH: # Custom path pass else: # Handle both global and all cases base_path = polyaxon_user_path() cls._create_dir(base_path)",False,if cls . IS_POLYAXON_DIR :,"if cls . CONFIG_PATH == ""global"" :",0.8820016898747209,14.323145079400492
"def test_len(self): eq = self.assertEqual eq(base64MIME.base64_len(""hello""), len(base64MIME.encode(""hello"", eol=""""))) for size in range(15): if size == 0: bsize = 0 <mask>: bsize = 4 elif size <= 6: bsize = 8 elif size <= 9: bsize = 12 elif size <= 12: bsize = 16 else: bsize = 20 eq(base64MIME.base64_len(""x"" * size), bsize)",False,elif size <= 3 :,elif size <= 5 :,0.8820016898747209,53.7284965911771
"def as_dict(path="""", version=""latest"", section=""meta-data""): result = {} dirs = dir(path, version, section) if not dirs: return None for item in dirs: <mask>: records = as_dict(path + item, version, section) if records: result[item[:-1]] = records elif is_dict.match(item): idx, name = is_dict.match(item).groups() records = as_dict(path + idx + ""/"", version, section) if records: result[name] = records else: result[item] = valueconv(get(path + item, version, section)) return result",False,"if item . endswith ( ""/"" ) :",if is_dict . match ( item ) :,0.8820016898747209,11.044795567078939
"def api_read(self): result = {} files = [""my.cnf"", ""debian.cnf""] directory_list = self.exec_payload(""mysql_config_directory"")[""directory""] for _file in files: for directory in directory_list: mysql_conf = directory + _file content = self.shell.read(mysql_conf) <mask>: result[mysql_conf] = content return result",True,if content :,if content :,0.8820016898747209,0.0
"def generate(self, count=100): self.pre_generate() counter = iter(range(count)) created = 0 while True: batch = list(islice(counter, self.batch_size)) <mask>: break self.do_generate(batch, self.batch_size) from_size = created created += len(batch) print(""Generate %s: %s-%s"" % (self.resource, from_size, created)) self.after_generate()",False,if not batch :,if len ( batch ) == 0 :,0.8820016898747209,6.27465531099474
"def _normalize_fields(self, document, loader): # type: (Dict[Text, Text], Loader) -> None # Normalize fields which are prefixed or full URIn to vocabulary terms for d in list(document.keys()): d2 = loader.expand_url(d, u"""", scoped_id=False, vocab_term=True) <mask>: document[d2] = document[d] del document[d]",False,if d != d2 :,if d2 in document :,0.8820016898747209,11.51015341649912
"def load_cache(filename, get_key=mangle_key): cache = {} if not os.path.exists(filename): return cache f = open(filename, ""rb"") l = 0 for line in f.readlines(): l += 1 fields = line.split(b"" "") <mask>: sys.stderr.write(""Invalid file format in [%s], line %d\n"" % (filename, l)) continue # put key:value in cache, key without ^: cache[get_key(fields[0][1:])] = fields[1].split(b""\n"")[0] f.close() return cache",False,"if fields == None or not len ( fields ) == 2 or fields [ 0 ] [ 0 : 1 ] != b"":"" :",if len ( fields ) != 2 :,0.8820016898747209,3.604179790223565
"def __lshift__(self, other): if not self.symbolic and type(other) is int: return RegisterOffset( self._bits, self.reg, self._to_signed(self.offset << other) ) else: <mask>: return RegisterOffset(self._bits, self.reg, self.offset << other) else: return RegisterOffset( self._bits, self.reg, ArithmeticExpression( ArithmeticExpression.LShift, ( self.offset, other, ), ), )",False,if self . symbolic :,if type ( other ) is int :,0.8820016898747209,6.567274736060395
"def SaveSettings(self, force=False): if self.config is not None: frame.ShellFrameMixin.SaveSettings(self) <mask>: frame.Frame.SaveSettings(self, self.config) self.shell.SaveSettings(self.config)",False,if self . autoSaveSettings or force :,if force :,0.8820016898747209,0.0
"def _parse_gene(element): for genename_element in element: if ""type"" in genename_element.attrib: ann_key = ""gene_%s_%s"" % ( genename_element.tag.replace(NS, """"), genename_element.attrib[""type""], ) <mask>: self.ParsedSeqRecord.annotations[ann_key] = genename_element.text else: append_to_annotations(ann_key, genename_element.text)",False,"if genename_element . attrib [ ""type"" ] == ""primary"" :","if genename_element . tag == ""seq"" :",0.8820016898747209,30.717570412730073
"def _write_pkg_file(self, file): with TemporaryFile(mode=""w+"") as tmpfd: _write_pkg_file_orig(self, tmpfd) tmpfd.seek(0) for line in tmpfd: <mask>: file.write(""Metadata-Version: 2.1\n"") elif line.startswith(""Description: ""): file.write( ""Description-Content-Type: %s; charset=UTF-8\n"" % long_description_content_type ) file.write(line) else: file.write(line)",False,"if line . startswith ( ""Metadata-Version: "" ) :","if line . startswith ( ""Metadata: "" ) :",0.8820016898747209,70.16879391277372
"def get(self): """"""If a value/an exception is stored, return/raise it. Otherwise until switch() or throw() is called."""""" if self._exception is not _NONE: <mask>: return self.value getcurrent().throw(*self._exception) # pylint:disable=undefined-variable else: if self.greenlet is not None: raise ConcurrentObjectUseError( ""This Waiter is already used by %r"" % (self.greenlet,) ) self.greenlet = getcurrent() # pylint:disable=undefined-variable try: return self.hub.switch() finally: self.greenlet = None",False,if self . _exception is None :,if self . value is not _NONE :,0.8820016898747209,21.10534063187263
"def connect(self, *args): """"""connects to the dropbox. args[0] is the username."""""" if len(args) != 1: return ""expected one argument!"" try: dbci = get_dropbox_client(args[0], False, None, None) except Exception as e: return e.message else: <mask>: return ""No Dropbox configured for '{u}'."".format(u=args[0]) else: self.client = dbci return True",False,if dbci is None :,if not dbci :,0.8820016898747209,16.37226966703825
"def escape(text, newline=False): """"""Escape special html characters."""""" if isinstance(text, str): if ""&"" in text: text = text.replace(""&"", ""&amp;"") if "">"" in text: text = text.replace("">"", ""&gt;"") if ""<"" in text: text = text.replace(""<"", ""&lt;"") if '""' in text: text = text.replace('""', ""&quot;"") if ""'"" in text: text = text.replace(""'"", ""&quot;"") if newline: <mask>: text = text.replace(""\n"", ""<br>"") return text",True,"if ""\n"" in text :","if ""\n"" in text :",0.8820016898747209,100.00000000000004
def t(ret): with IPDB() as ipdb: with ipdb.eventqueue() as evq: for msg in evq: <mask>: ret.append(msg) return,False,"if msg . get_attr ( ""IFLA_IFNAME"" ) == ""test1984"" :","if msg . type == ""message"" :",0.8820016898747209,12.135071233528631
"def check_stmt(self, stmt): if is_future(stmt): for name, asname in stmt.names: <mask>: self.found[name] = 1 else: raise SyntaxError(""future feature %s is not defined"" % name) stmt.valid_future = 1 return 1 return 0",False,if name in self . features :,"if asname == ""future"" :",0.8820016898747209,6.567274736060395
"def process_pypi_option(option, option_str, option_value, parser): if option_str.startswith(""--no""): setattr(parser.values, option.dest, []) else: indexes = getattr(parser.values, option.dest, []) <mask>: indexes.append(_PYPI) setattr(parser.values, option.dest, indexes)",False,if _PYPI not in indexes :,"if option_value == ""PYPI"" :",0.8820016898747209,5.934202609760488
"def modify_address(self, name, address, domain): if not self.get_entries_by_name(name, domain): raise exception.NotFound infile = open(self.filename, ""r"") outfile = tempfile.NamedTemporaryFile(""w"", delete=False) for line in infile: entry = self.parse_line(line) <mask>: outfile.write( ""%s %s %s\n"" % (address, self.qualify(name, domain), entry[""type""]) ) else: outfile.write(line) infile.close() outfile.close() shutil.move(outfile.name, self.filename)",False,"if entry and entry [ ""name"" ] . lower ( ) == self . qualify ( name , domain ) . lower ( ) :",if entry :,0.8820016898747209,0.0
"def tms_to_quadkey(self, tms, google=False): quadKey = """" x, y, z = tms # this algorithm works with google tiles, rather than tms, so convert # to those first. if not google: y = (2 ** z - 1) - y for i in range(z, 0, -1): digit = 0 mask = 1 << (i - 1) if (x & mask) != 0: digit += 1 <mask>: digit += 2 quadKey += str(digit) return quadKey",True,if ( y & mask ) != 0 :,if ( y & mask ) != 0 :,0.8820016898747209,100.00000000000004
"def add_if_unique(self, issuer, use, keys): if use in self.issuer_keys[issuer] and self.issuer_keys[issuer][use]: for typ, key in keys: flag = 1 for _typ, _key in self.issuer_keys[issuer][use]: if _typ == typ and key is _key: flag = 0 break <mask>: self.issuer_keys[issuer][use].append((typ, key)) else: self.issuer_keys[issuer][use] = keys",True,if flag :,if flag :,0.8820016898747209,0.0
"def scan_error(self): ""A string describing why the last scan failed, or None if it didn't."" self.acquire_lock() try: <mask>: try: self._load_buf_data_once() except NotFoundInDatabase: pass return self._scan_error_cache finally: self.release_lock()",True,if self . _scan_error_cache is None :,if self . _scan_error_cache is None :,0.8820016898747209,100.00000000000004
"def _query(self): if self._mongo_query is None: self._mongo_query = self._query_obj.to_query(self._document) <mask>: if ""_cls"" in self._mongo_query: self._mongo_query = {""$and"": [self._cls_query, self._mongo_query]} else: self._mongo_query.update(self._cls_query) return self._mongo_query",False,if self . _cls_query :,if self . _mongo_query :,0.8820016898747209,50.000000000000014
"def CountButtons(self): """"""Returns the number of visible buttons in the docked pane."""""" n = 0 if self.HasCaption() or self.HasCaptionLeft(): if isinstance(wx.GetTopLevelParent(self.window), AuiFloatingFrame): return 1 if self.HasCloseButton(): n += 1 if self.HasMaximizeButton(): n += 1 if self.HasMinimizeButton(): n += 1 <mask>: n += 1 return n",False,if self . HasPinButton ( ) :,if self . HasMaximize ( ) :,0.8820016898747209,41.11336169005196
"def testBind(self): try: with socket.socket(socket.PF_CAN, socket.SOCK_DGRAM, socket.CAN_J1939) as s: addr = ( self.interface, socket.J1939_NO_NAME, socket.J1939_NO_PGN, socket.J1939_NO_ADDR, ) s.bind(addr) self.assertEqual(s.getsockname(), addr) except OSError as e: <mask>: self.skipTest(""network interface `%s` does not exist"" % self.interface) else: raise",False,if e . errno == errno . ENODEV :,if e . errno == errno . EEXIST :,0.8820016898747209,78.25422900366438
"def createFields(self): while self.current_size < self.size: pos = self.stream.searchBytes( ""\0\0\1"", self.current_size, self.current_size + 1024 * 1024 * 8 ) # seek forward by at most 1MB if pos is not None: padsize = pos - self.current_size <mask>: yield PaddingBytes(self, ""pad[]"", padsize // 8) chunk = Chunk(self, ""chunk[]"") try: # force chunk to be processed, so that CustomFragments are complete chunk[""content/data""] except: pass yield chunk",False,if padsize :,if padsize > 0 :,0.8820016898747209,23.643540225079384
"def index_modulemd_files(repo_path): merger = Modulemd.ModuleIndexMerger() for fn in sorted(os.listdir(repo_path)): <mask>: continue yaml_path = os.path.join(repo_path, fn) mmd = Modulemd.ModuleIndex() mmd.update_from_file(yaml_path, strict=True) merger.associate_index(mmd, 0) return merger.resolve()",True,"if not fn . endswith ( "".yaml"" ) :","if not fn . endswith ( "".yaml"" ) :",0.8820016898747209,100.00000000000004
"def set_visible(self, visible=True): self._visible = visible if self._nswindow is not None: <mask>: # Not really sure why on_resize needs to be here, # but it's what pyglet wants. self.dispatch_event(""on_resize"", self._width, self._height) self.dispatch_event(""on_show"") self.dispatch_event(""on_expose"") self._nswindow.makeKeyAndOrderFront_(None) else: self._nswindow.orderOut_(None)",True,if visible :,if visible :,0.8820016898747209,0.0
"def __repr__(self): if self._in_repr: return ""<recursion>"" try: self._in_repr = True if self.is_computed(): status = ""computed, "" if self.error() is None: <mask>: status += ""= self"" else: status += ""= "" + repr(self.value()) else: status += ""error = "" + repr(self.error()) else: status = ""isn't computed"" return ""%s (%s)"" % (type(self), status) finally: self._in_repr = False",False,if self . value ( ) is self :,if self . value is None :,0.8820016898747209,34.191776499651844
"def _individual_get(self, segment, index_type, index, strictdoc): if index_type == ""val"": for key, value in segment.items(): if key == index[0]: return value <mask>: if key.text == index[0]: return value raise Exception(""Invalid state"") elif index_type == ""index"": return segment[index] elif index_type == ""textslice"": return segment[index[0] : index[1]] elif index_type == ""key"": return index[1] if strictdoc else index[0] else: raise Exception(""Invalid state"")",False,"if hasattr ( key , ""text"" ) :","elif index_type == ""text"" :",0.8820016898747209,16.784459625186194
"def _makeSafeAbsoluteURI(base, rel=None): # bail if ACCEPTABLE_URI_SCHEMES is empty if not ACCEPTABLE_URI_SCHEMES: return _urljoin(base, rel or u"""") if not base: return rel or u"""" if not rel: try: scheme = urlparse.urlparse(base)[0] except ValueError: return u"""" <mask>: return base return u"""" uri = _urljoin(base, rel) if uri.strip().split("":"", 1)[0] not in ACCEPTABLE_URI_SCHEMES: return u"""" return uri",False,if not scheme or scheme in ACCEPTABLE_URI_SCHEMES :,"if scheme . scheme == ""file"" :",0.8820016898747209,4.85851417160653
"def _write_packet(self, packet): # Immediately writes the given packet to the network. The caller must # have the write lock acquired before calling this method. try: for listener in self.early_outgoing_packet_listeners: listener.call_packet(packet) <mask>: packet.write(self.socket, self.options.compression_threshold) else: packet.write(self.socket) for listener in self.outgoing_packet_listeners: listener.call_packet(packet) except IgnorePacket: pass",False,if self . options . compression_enabled :,if self . options . compression_threshold :,0.8820016898747209,75.06238537503395
"def rangelist_to_set(rangelist): result = set() if not rangelist: return result for x in rangelist.split("",""): <mask>: result.add(int(x)) continue m = re.match(r""^(\d+)-(\d+)$"", x) if m: start = int(m.group(1)) end = int(m.group(2)) result.update(set(range(start, end + 1))) continue msg = ""Cannot understand data input: %s %s"" % (x, rangelist) raise ValueError(msg) return result",False,"if re . match ( r""^(\d+)$"" , x ) :","if x . startswith ( ""#"" ) :",0.8820016898747209,4.825333471764194
"def test_device_property_logfile_isinstance(self): mock = MagicMock() with patch(builtin_string + "".open"", mock): <mask>: builtin_file = ""io.TextIOWrapper"" else: builtin_file = builtin_string + "".file"" with patch(builtin_file, MagicMock): handle = open(""filename"", ""r"") self.dev.logfile = handle self.assertEqual(self.dev.logfile, handle)",False,"if sys . version > ""3"" :","if sys . platform == ""win32"" :",0.8820016898747209,20.556680845025987
"def _line_ranges(statements, lines): """"""Produce a list of ranges for `format_lines`."""""" statements = sorted(statements) lines = sorted(lines) pairs = [] start = None lidx = 0 for stmt in statements: if lidx >= len(lines): break if stmt == lines[lidx]: lidx += 1 if not start: start = stmt end = stmt <mask>: pairs.append((start, end)) start = None if start: pairs.append((start, end)) return pairs",False,elif start :,if start :,0.8820016898747209,0.0
"def reset_parameters(self): initialize = layers.get_initializer(self._hparams.initializer) if initialize is not None: # Do not re-initialize LayerNorm modules. for name, param in self.named_parameters(): <mask>: initialize(param)",False,"if name . split ( ""."" ) [ - 1 ] == ""weight"" and ""layer_norm"" not in name :","if name == ""layernorm"" :",0.8820016898747209,2.307989345220562
"def billing_invoice_show_validator(namespace): from azure.cli.core.azclierror import ( RequiredArgumentMissingError, MutuallyExclusiveArgumentError, ) valid_combs = ( ""only --account-name, --name / --name / --name, --by-subscription is valid"" ) if namespace.account_name is not None: if namespace.by_subscription is not None: raise MutuallyExclusiveArgumentError(valid_combs) <mask>: raise RequiredArgumentMissingError(""--name is also required"") if namespace.by_subscription is not None: if namespace.name is None: raise RequiredArgumentMissingError(""--name is also required"")",True,if namespace . name is None :,if namespace . name is None :,0.8820016898747209,100.00000000000004
"def DeleteDocuments(self, document_ids, response): """"""Deletes documents for the given document_ids."""""" for document_id in document_ids: <mask>: document = self._documents[document_id] self._inverted_index.RemoveDocument(document) del self._documents[document_id] delete_status = response.add_status() delete_status.set_code(search_service_pb.SearchServiceError.OK)",True,if document_id in self . _documents :,if document_id in self . _documents :,0.8820016898747209,100.00000000000004
"def generate_new_element(items, prefix, numeric=False): """"""Creates a random string with prefix, that is not in 'items' list."""""" while True: <mask>: candidate = prefix + generate_random_numeric(8) else: candidate = prefix + generate_random_alphanumeric(8) if not candidate in items: return candidate LOG.debug(""Random collision on %s"" % candidate)",True,if numeric :,if numeric :,0.8820016898747209,0.0
"def generate_text_for_vocab(self, data_dir, tmp_dir): for i, sample in enumerate( self.generate_samples(data_dir, tmp_dir, problem.DatasetSplit.TRAIN) ): if self.has_inputs: yield sample[""inputs""] yield sample[""targets""] <mask>: break",False,if self . max_samples_for_vocab and ( i + 1 ) >= self . max_samples_for_vocab :,elif self . has_targets :,0.8820016898747209,0.777659056082747
"def _get_ccp(config=None, config_path=None, saltenv=""base""): """""" """""" if config_path: config = __salt__[""cp.get_file_str""](config_path, saltenv=saltenv) <mask>: raise SaltException(""{} is not available"".format(config_path)) if isinstance(config, six.string_types): config = config.splitlines() ccp = ciscoconfparse.CiscoConfParse(config) return ccp",False,if config is False :,if not config :,0.8820016898747209,16.37226966703825
"def rpush(key, *vals, **kwargs): ttl = kwargs.get(""ttl"") cap = kwargs.get(""cap"") if not ttl and not cap: _client.rpush(key, *vals) else: pipe = _client.pipeline() pipe.rpush(key, *vals) <mask>: pipe.ltrim(key, 0, cap) if ttl: pipe.expire(key, ttl) pipe.execute()",True,if cap :,if cap :,0.8820016898747209,0.0
"def check_apns_certificate(ss): mode = ""start"" for s in ss.split(""\n""): <mask>: if ""BEGIN RSA PRIVATE KEY"" in s or ""BEGIN PRIVATE KEY"" in s: mode = ""key"" elif mode == ""key"": if ""END RSA PRIVATE KEY"" in s or ""END PRIVATE KEY"" in s: mode = ""end"" break elif s.startswith(""Proc-Type"") and ""ENCRYPTED"" in s: raise ImproperlyConfigured( ""Encrypted APNS private keys are not supported"" ) if mode != ""end"": raise ImproperlyConfigured(""The APNS certificate doesn't contain a private key"")",False,"if mode == ""start"" :","if s . startswith ( ""RSA-Key"" ) :",0.8820016898747209,5.934202609760488
"def _add_communication_type(apps, schema_editor, communication_type): Worker = apps.get_model(""orchestra"", ""Worker"") CommunicationPreference = apps.get_model(""orchestra"", ""CommunicationPreference"") for worker in Worker.objects.all(): ( communication_preference, created, ) = CommunicationPreference.objects.get_or_create( worker=worker, communication_type=communication_type ) # By default set both Slack and Email notifications to True <mask>: communication_preference.methods.slack = True communication_preference.methods.email = True communication_preference.save()",True,if created :,if created :,0.8820016898747209,0.0
"def get_postgresql_driver_name(): # pylint: disable=unused-variable try: driver = os.getenv(""CODECHECKER_DB_DRIVER"") <mask>: return driver try: # pylint: disable=W0611 import psycopg2 return ""psycopg2"" except Exception: # pylint: disable=W0611 import pg8000 return ""pg8000"" except Exception as ex: LOG.error(str(ex)) LOG.error(""Failed to import psycopg2 or pg8000 module."") raise",True,if driver :,if driver :,0.8820016898747209,0.0
"def env_purge_doc(app: Sphinx, env: BuildEnvironment, docname: str) -> None: modules = getattr(env, ""_viewcode_modules"", {}) for modname, entry in list(modules.items()): if entry is False: continue code, tags, used, refname = entry for fullname in list(used): if used[fullname] == docname: used.pop(fullname) <mask>: modules.pop(modname)",False,if len ( used ) == 0 :,if modname in modules :,0.8820016898747209,5.70796903405875
"def do_query(data, q): ret = [] if not q: return ret qkey = q[0] for key, value in iterate(data): if len(q) == 1: if key == qkey: ret.append(value) elif is_iterable(value): ret.extend(do_query(value, q)) else: <mask>: continue if key == qkey: ret.extend(do_query(value, q[1:])) else: ret.extend(do_query(value, q)) return ret",False,if not is_iterable ( value ) :,if key == qkey :,0.8820016898747209,5.854497694024015
"def _get_bucket_for_key(self, key: bytes) -> Optional[_DBValueTuple]: dbs: Iterable[PartitionDB] try: partition = self._key_index[key] dbs = [PartitionDB(partition, self._dbs[partition])] except KeyError: dbs = cast(Iterable[PartitionDB], self._dbs.items()) for partition, db in dbs: if db.key_may_exist(key)[0]: value = db.get(key) <mask>: self._key_index[key] = partition return _DBValueTuple(db, value) return None",True,if value is not None :,if value is not None :,0.8820016898747209,100.00000000000004
"def _clean(self): logger.info(""Cleaning up..."") if self._process is not None: if self._process.poll() is None: for _ in range(3): self._process.terminate() time.sleep(0.5) <mask>: break else: self._process.kill() self._process.wait() logger.error(""KILLED"") if os.path.exists(self._tmp_dir): shutil.rmtree(self._tmp_dir) self._process = None self._ws = None logger.info(""Cleanup complete"")",False,if self . _process . poll ( ) is not None :,if self . _process . returncode == 0 :,0.8820016898747209,40.009985513101846
"def _calculate_runtimes(states): results = {""runtime"": 0.00, ""num_failed_states"": 0, ""num_passed_states"": 0} for state, resultset in states.items(): if isinstance(resultset, dict) and ""duration"" in resultset: # Count the pass vs failures <mask>: results[""num_passed_states""] += 1 else: results[""num_failed_states""] += 1 # Count durations results[""runtime""] += resultset[""duration""] log.debug(""Parsed state metrics: {}"".format(results)) return results",False,"if resultset [ ""result"" ] :","if state == ""pass"" :",0.8820016898747209,7.809849842300637
"def spaces_after(token, prev, next, min=-1, max=-1, min_desc=None, max_desc=None): if next is not None and token.end_mark.line == next.start_mark.line: spaces = next.start_mark.pointer - token.end_mark.pointer if max != -1 and spaces > max: return LintProblem( token.start_mark.line + 1, next.start_mark.column, max_desc ) <mask>: return LintProblem( token.start_mark.line + 1, next.start_mark.column + 1, min_desc )",False,elif min != - 1 and spaces < min :,if min != - 1 and spaces < min :,0.8820016898747209,89.31539818068698
"def getfileinfo(name): finfo = FInfo() with io.open(name, ""rb"") as fp: # Quick check for textfile data = fp.read(512) <mask>: finfo.Type = ""TEXT"" fp.seek(0, 2) dsize = fp.tell() dir, file = os.path.split(name) file = file.replace("":"", ""-"", 1) return file, finfo, dsize, 0",False,if 0 not in data :,"if data == ""text"" :",0.8820016898747209,7.267884212102741
"def dict_to_XML(tag, dictionary, **kwargs): """"""Return XML element converting dicts recursively."""""" elem = Element(tag, **kwargs) for key, val in dictionary.items(): if tag == ""layers"": child = dict_to_XML(""layer"", val, name=key) <mask>: child = dict_to_XML(key, val) else: if tag == ""config"": child = Element(""variable"", name=key) else: child = Element(key) child.text = str(val) elem.append(child) return elem",False,"elif isinstance ( val , MutableMapping ) :","elif tag == ""data"" :",0.8820016898747209,6.567274736060395
"def _read_bytes(self, length): buffer = b"""" while length: chunk = self.request.recv(length) <mask>: log.debug(""Connection closed"") return False length -= len(chunk) buffer += chunk return buffer",False,"if chunk == b"""" :",if not chunk :,0.8820016898747209,7.733712583165139
"def rec_deps(services, container_by_name, cnt, init_service): deps = cnt[""_deps""] for dep in deps.copy(): dep_cnts = services.get(dep) <mask>: continue dep_cnt = container_by_name.get(dep_cnts[0]) if dep_cnt: # TODO: avoid creating loops, A->B->A if init_service and init_service in dep_cnt[""_deps""]: continue new_deps = rec_deps(services, container_by_name, dep_cnt, init_service) deps.update(new_deps) return deps",True,if not dep_cnts :,if not dep_cnts :,0.8820016898747209,100.00000000000004
"def fix_repeating_arguments(self): """"""Fix elements that should accumulate/increment values."""""" either = [list(child.children) for child in transform(self).children] for case in either: for e in [child for child in case if case.count(child) > 1]: <mask>: if e.value is None: e.value = [] elif type(e.value) is not list: e.value = e.value.split() if type(e) is Command or type(e) is Option and e.argcount == 0: e.value = 0 return self",False,if type ( e ) is Argument or type ( e ) is Option and e . argcount :,if type ( e ) is Command or type ( e ) is Option :,0.8820016898747209,56.50247638590732
"def do_cli(manager, options): header = [""Name"", ""Description""] table_data = [header] for filter_name, filter in get_filters(): <mask>: continue filter_doc = inspect.getdoc(filter) or """" table_data.append([filter_name, filter_doc]) try: table = TerminalTable(options.table_type, table_data) except TerminalTableError as e: console(""ERROR: %s"" % str(e)) else: console(table.output)",False,if options . name and not options . name in filter_name :,"if filter_name == ""name"" :",0.8820016898747209,12.451233733093902
"def _do_cmp(f1, f2): bufsize = BUFSIZE with open(f1, ""rb"") as fp1, open(f2, ""rb"") as fp2: while True: b1 = fp1.read(bufsize) b2 = fp2.read(bufsize) if b1 != b2: return False <mask>: return True",False,if not b1 :,if b1 == b2 :,0.8820016898747209,10.682175159905853
"def apply(self, db, person): families = person.get_parent_family_handle_list() if families == []: return True for family_handle in person.get_parent_family_handle_list(): family = db.get_family_from_handle(family_handle) if family: father_handle = family.get_father_handle() mother_handle = family.get_mother_handle() <mask>: return True if not mother_handle: return True return False",False,if not father_handle :,if not father_handle or not mother_handle :,0.8820016898747209,41.72261448611506
"def caesar_cipher(s, k): result = """" for char in s: n = ord(char) if 64 < n < 91: n = ((n - 65 + k) % 26) + 65 <mask>: n = ((n - 97 + k) % 26) + 97 result = result + chr(n) return result",False,if 96 < n < 123 :,elif 97 < n < 91 :,0.8820016898747209,26.269098944241588
"def title_by_index(self, trans, index, context): d_type = self.get_datatype(trans, context) for i, (composite_name, composite_file) in enumerate(d_type.writable_files.items()): if i == index: rval = composite_name if composite_file.description: rval = ""{} ({})"".format(rval, composite_file.description) <mask>: rval = ""%s [optional]"" % rval return rval if index < self.get_file_count(trans, context): return ""Extra primary file"" return None",True,if composite_file . optional :,if composite_file . optional :,0.8820016898747209,100.00000000000004
"def __str__(self): t = "" "" if self._name != ""root"": r = f""{t * (self._level-1)}{self._name}:\n"" else: r = """" level = self._level for i, (k, v) in enumerate(self._pointer.items()): <mask>: r += f""{t * (self._level)}{v}\n"" self._level += 1 else: r += f""{t * (self._level)}{k}: {v} ({type(v).__name__})\n"" self._level = level return r[:-1]",False,"if isinstance ( v , Config ) :",if i == level :,0.8820016898747209,6.916271812933183
"def __get_securitygroups(vm_): vm_securitygroups = config.get_cloud_config_value( ""securitygroups"", vm_, __opts__, search_global=False ) if not vm_securitygroups: return [] securitygroups = list_securitygroups() for i in range(len(vm_securitygroups)): vm_securitygroups[i] = six.text_type(vm_securitygroups[i]) <mask>: raise SaltCloudNotFound( ""The specified securitygroups '{0}' could not be found."".format( vm_securitygroups[i] ) ) return vm_securitygroups",True,if vm_securitygroups [ i ] not in securitygroups :,if vm_securitygroups [ i ] not in securitygroups :,0.8820016898747209,100.00000000000004
"def assert_walk_snapshot( self, field, filespecs_or_globs, paths, ignore_patterns=None, prepare=None ): with self.mk_project_tree(ignore_patterns=ignore_patterns) as project_tree: scheduler = self.mk_scheduler( rules=create_fs_rules(), project_tree=project_tree ) <mask>: prepare(project_tree) result = self.execute(scheduler, Snapshot, self.specs(filespecs_or_globs))[0] self.assertEqual(sorted(getattr(result, field)), sorted(paths))",True,if prepare :,if prepare :,0.8820016898747209,0.0
"def _parse_rowids(self, rowids): xploded = [] rowids = [x.strip() for x in rowids.split("","")] for rowid in rowids: try: <mask>: start = int(rowid.split(""-"")[0].strip()) end = int(rowid.split(""-"")[-1].strip()) xploded += range(start, end + 1) else: xploded.append(int(rowid)) except ValueError: continue return sorted(list(set(xploded)))",True,"if ""-"" in rowid :","if ""-"" in rowid :",0.8820016898747209,100.00000000000004
"def ensemble(self, pairs, other_preds): """"""Ensemble the dict with statistical model predictions."""""" lemmas = [] assert len(pairs) == len(other_preds) for p, pred in zip(pairs, other_preds): w, pos = p <mask>: lemma = self.composite_dict[(w, pos)] elif w in self.word_dict: lemma = self.word_dict[w] else: lemma = pred if lemma is None: lemma = w lemmas.append(lemma) return lemmas",False,"if ( w , pos ) in self . composite_dict :",if w in self . composite_dict :,0.8820016898747209,49.56678178292308
"def selectionToChunks(self, remove=False, add=False): box = self.selectionBox() if box: <mask>: self.selectedChunks = set(self.level.allChunks) return selectedChunks = self.selectedChunks boxedChunks = set(box.chunkPositions) if boxedChunks.issubset(selectedChunks): remove = True if remove and not add: selectedChunks.difference_update(boxedChunks) else: selectedChunks.update(boxedChunks) self.selectionTool.selectNone()",False,if box == self . level . bounds :,if box . chunkPositions is None :,0.8820016898747209,10.175282441454787
"def _ensure_max_size(cls, image, max_size, interpolation): if max_size is not None: size = max(image.shape[0], image.shape[1]) <mask>: resize_factor = max_size / size new_height = int(image.shape[0] * resize_factor) new_width = int(image.shape[1] * resize_factor) image = ia.imresize_single_image( image, (new_height, new_width), interpolation=interpolation ) return image",False,if size > max_size :,if size > 0 :,0.8820016898747209,28.641904579795423
"def _1_0_cloud_ips(self, method, url, body, headers): if method == ""GET"": return self.test_response(httplib.OK, self.fixtures.load(""list_cloud_ips.json"")) elif method == ""POST"": <mask>: body = json.loads(body) node = json.loads(self.fixtures.load(""create_cloud_ip.json"")) if ""reverse_dns"" in body: node[""reverse_dns""] = body[""reverse_dns""] return self.test_response(httplib.ACCEPTED, json.dumps(node))",True,if body :,if body :,0.8820016898747209,0.0
"def get_formatted_stats(self): """"""Get percentage or number of rar's done"""""" if self.cur_setname and self.cur_setname in self.total_volumes: # This won't work on obfuscated posts <mask>: return ""%02d/%02d"" % (self.cur_volume, self.total_volumes[self.cur_setname]) return self.cur_volume",False,if self . total_volumes [ self . cur_setname ] >= self . cur_volume and self . cur_volume :,if self . cur_volume < self . total_volumes [ self . cur_setname ] :,0.8820016898747209,56.13312901593494
"def wdayset(self, year, month, day): # We need to handle cross-year weeks here. dset = [None] * (self.yearlen + 7) i = datetime.date(year, month, day).toordinal() - self.yearordinal start = i for j in range(7): dset[i] = i i += 1 # if (not (0 <= i < self.yearlen) or # self.wdaymask[i] == self.rrule._wkst): # This will cross the year boundary, if necessary. <mask>: break return dset, start, i",True,if self . wdaymask [ i ] == self . rrule . _wkst :,if self . wdaymask [ i ] == self . rrule . _wkst :,0.8820016898747209,100.00000000000004
"def do_acquire_read_lock(self, wait=True): self.condition.acquire() try: # see if a synchronous operation is waiting to start # or is already running, in which case we wait (or just # give up and return) <mask>: while self.current_sync_operation is not None: self.condition.wait() else: if self.current_sync_operation is not None: return False self.asynch += 1 finally: self.condition.release() if not wait: return True",False,if wait :,if self . asynch == 0 :,0.8820016898747209,6.567274736060395
"def _blend(x, y): # pylint: disable=invalid-name """"""Implements the ""blend"" strategy for `deep_merge`."""""" if isinstance(x, (dict, OrderedDict)): <mask>: return y return _merge(x, y, recursion_func=_blend) if isinstance(x, (list, tuple)): if not isinstance(y, (list, tuple)): return y result = [_blend(*i) for i in zip(x, y)] if len(x) > len(y): result += x[len(y) :] elif len(x) < len(y): result += y[len(x) :] return result return y",False,"if not isinstance ( y , ( dict , OrderedDict ) ) :","if not isinstance ( y , dict ) :",0.8820016898747209,43.624306402227546
"def update_forum_nums_topic_post(modeladmin, request, queryset): for forum in queryset: forum.num_topics = forum.count_nums_topic() forum.num_posts = forum.count_nums_post() <mask>: forum.last_post = forum.topic_set.order_by(""-last_reply_on"")[0].last_post else: forum.last_post = """" forum.save()",False,if forum . num_topics :,if forum . last_post is None :,0.8820016898747209,20.164945583740657
"def get_docname_for_node(self, node: Node) -> str: while node: if isinstance(node, nodes.document): return self.env.path2doc(node[""source""]) <mask>: return node[""docname""] else: node = node.parent return None # never reached here. only for type hinting",False,"elif isinstance ( node , addnodes . start_of_file ) :","elif isinstance ( node , nodes . name ) :",0.8820016898747209,31.313195073576246
"def _selected_machines(self, virtual_machines): selected_machines = [] for machine in virtual_machines: if self._args.host and self._args.host == machine.name: selected_machines.append(machine) if self.tags and self._tags_match(machine.tags, self.tags): selected_machines.append(machine) <mask>: selected_machines.append(machine) return selected_machines",False,if self . locations and machine . location in self . locations :,"elif self . _tags_match ( machine . tags , self . tags ) :",0.8820016898747209,8.233514927922947
"def transform_kwarg(self, name, value, split_single_char_options): if len(name) == 1: if value is True: return [""-%s"" % name] <mask>: if split_single_char_options: return [""-%s"" % name, ""%s"" % value] else: return [""-%s%s"" % (name, value)] else: if value is True: return [""--%s"" % dashify(name)] elif value is not False and value is not None: return [""--%s=%s"" % (dashify(name), value)] return []",False,"elif value not in ( False , None ) :",elif value is not False and value is not None :,0.8820016898747209,9.864703138979419
"def indent(elem, level=0): i = ""\n"" + level * "" "" if len(elem): if not elem.text or not elem.text.strip(): elem.text = i + "" "" if not elem.tail or not elem.tail.strip(): elem.tail = i for elem in elem: indent(elem, level + 1) if not elem.tail or not elem.tail.strip(): elem.tail = i else: <mask>: elem.tail = i",False,if level and ( not elem . tail or not elem . tail . strip ( ) ) :,if not elem . tail or not elem . tail . strip ( ) :,0.8820016898747209,68.89837659037367
"def _run_instances_op(self, op, instance_ids, **kwargs): while instance_ids: try: return self.manager.retry(op, InstanceIds=instance_ids, **kwargs) except ClientError as e: <mask>: instance_ids.remove(extract_instance_id(e)) raise",False,"if e . response [ ""Error"" ] [ ""Code"" ] == ""IncorrectInstanceState"" :","if e . response [ ""Error"" ] [ ""Code"" ] == ""NoSuchEntity"" :",0.8820016898747209,87.39351325046809
"def runTest(self): self.poco(text=""wait UI"").click() bomb_count = 0 while True: blue_fish = self.poco(""fish_emitter"").child(""blue"") yellow_fish = self.poco(""fish_emitter"").child(""yellow"") bomb = self.poco(""fish_emitter"").child(""bomb"") fish = self.poco.wait_for_any([blue_fish, yellow_fish, bomb]) <mask>: bomb_count += 1 if bomb_count > 3: return else: fish.click() time.sleep(2.5)",False,if fish is bomb :,if fish :,0.8820016898747209,0.0
"def lineWidth(self, lw=None): """"""Set/get width of mesh edges. Same as `lw()`."""""" if lw is not None: <mask>: self.GetProperty().EdgeVisibilityOff() self.GetProperty().SetRepresentationToSurface() return self self.GetProperty().EdgeVisibilityOn() self.GetProperty().SetLineWidth(lw) else: return self.GetProperty().GetLineWidth() return self",False,if lw == 0 :,if self . GetProperty ( ) . GetLineWidth ( ) == lw :,0.8820016898747209,7.141816289329644
"def _current_date_updater(doc, field_name, value): if isinstance(doc, dict): <mask>: # TODO(juannyg): get_current_timestamp should also be using helpers utcnow, # as it currently using time.time internally doc[field_name] = helpers.get_current_timestamp() else: doc[field_name] = mongomock.utcnow()",False,"if value == { ""$type"" : ""timestamp"" } :",if value :,0.8820016898747209,0.0
"def fill_members(self): if self._get_retrieve(): after = self.after.id if self.after else None data = await self.get_members(self.guild.id, self.retrieve, after) <mask>: # no data, terminate return if len(data) < 1000: self.limit = 0 # terminate loop self.after = Object(id=int(data[-1][""user""][""id""])) for element in reversed(data): await self.members.put(self.create_member(element))",True,if not data :,if not data :,0.8820016898747209,100.00000000000004
"def extract(self, page, start_index=0, end_index=None): items = [] for extractor in self.extractors: extracted = extractor.extract( page, start_index, end_index, self.template.ignored_regions ) for item in arg_to_iter(extracted): if item: <mask>: item[u""_template""] = self.template.id items.append(item) return items",False,"if isinstance ( item , ( ItemProcessor , dict ) ) :","if item [ u""_template"" ] is None :",0.8820016898747209,4.456882760699063
"def _get_node_type_specific_fields(self, node_id: str, fields_key: str) -> Any: fields = self.config[fields_key] node_tags = self.provider.node_tags(node_id) if TAG_RAY_USER_NODE_TYPE in node_tags: node_type = node_tags[TAG_RAY_USER_NODE_TYPE] if node_type not in self.available_node_types: raise ValueError(f""Unknown node type tag: {node_type}."") node_specific_config = self.available_node_types[node_type] <mask>: fields = node_specific_config[fields_key] return fields",True,if fields_key in node_specific_config :,if fields_key in node_specific_config :,0.8820016898747209,100.00000000000004
"def _write_all(self, writer): """"""Writes messages and insert comments here and there."""""" # Note: we make no assumptions about the length of original_messages and original_comments for msg, comment in zip_longest( self.original_messages, self.original_comments, fillvalue=None ): # msg and comment might be None <mask>: print(""writing comment: "", comment) writer.log_event(comment) # we already know that this method exists if msg is not None: print(""writing message: "", msg) writer(msg)",True,if comment is not None :,if comment is not None :,0.8820016898747209,100.00000000000004
"def run_tests(): # type: () -> None x = 5 with switch(x) as case: <mask>: print(""zero"") print(""zero"") elif case(1, 2): print(""one or two"") elif case(3, 4): print(""three or four"") else: print(""default"") print(""another"")",False,if case ( 0 ) :,"if case ( 0 , 1 ) :",0.8820016898747209,41.11336169005198
"def date_to_format(value, target_format): """"""Convert date to specified format"""""" if target_format == str: if isinstance(value, datetime.date): ret = value.strftime(""%d/%m/%y"") <mask>: ret = value.strftime(""%d/%m/%y"") elif isinstance(value, datetime.time): ret = value.strftime(""%H:%M:%S"") else: ret = value return ret",False,"elif isinstance ( value , datetime . datetime ) :","elif isinstance ( value , datetime . date_local ) :",0.8820016898747209,57.067457770559976
"def database_app(request): if request.param == ""postgres_app"": if not which(""initdb""): pytest.skip(""initdb must be on PATH for postgresql fixture"") <mask>: pytest.skip(""psycopg2 must be installed for postgresql fixture"") if request.param == ""sqlite_rabbitmq_app"": if not os.environ.get(""GALAXY_TEST_AMQP_INTERNAL_CONNECTION""): pytest.skip( ""rabbitmq tests will be skipped if GALAXY_TEST_AMQP_INTERNAL_CONNECTION env var is unset"" ) return request.getfixturevalue(request.param)",False,if not psycopg2 :,"if not which ( ""psycopg2"" ) :",0.8820016898747209,11.339582221952005
"def poll_ms(self, timeout=-1): s = bytearray(self.evbuf) <mask>: deadline = utime.ticks_add(utime.ticks_ms(), timeout) while True: n = epoll_wait(self.epfd, s, 1, timeout) if not os.check_error(n): break if timeout >= 0: timeout = utime.ticks_diff(deadline, utime.ticks_ms()) if timeout < 0: n = 0 break res = [] if n > 0: vals = struct.unpack(epoll_event, s) res.append((vals[1], vals[0])) return res",True,if timeout >= 0 :,if timeout >= 0 :,0.8820016898747209,100.00000000000004
"def get_all_active_plugins(self) -> List[BotPlugin]: """"""This returns the list of plugins in the callback ordered defined from the config."""""" all_plugins = [] for name in self.plugins_callback_order: # None is a placeholder for any plugin not having a defined order if name is None: all_plugins += [ plugin for name, plugin in self.plugins.items() if name not in self.plugins_callback_order and plugin.is_activated ] else: plugin = self.plugins[name] <mask>: all_plugins.append(plugin) return all_plugins",True,if plugin . is_activated :,if plugin . is_activated :,0.8820016898747209,100.00000000000004
"def get_expected_sql(self): sql_base_path = path.join(path.dirname(path.realpath(__file__)), ""sql"") # Iterate the version mapping directories. for version_mapping in get_version_mapping_directories(self.server[""type""]): <mask>: continue complete_path = path.join(sql_base_path, version_mapping[""name""]) if not path.exists(complete_path): continue break data_sql = """" with open(path.join(complete_path, ""test_sql_output.sql"")) as fp: data_sql = fp.read() return data_sql",False,"if version_mapping [ ""number"" ] > self . server_information [ ""server_version"" ] :","if not path . exists ( version_mapping [ ""name"" ] ) :",0.8820016898747209,18.586851126555597
"def _validate_headers(self, headers): if headers is None: return headers res = {} for key, value in headers.items(): if isinstance(value, (int, float)): value = str(value) <mask>: raise ScriptError( { ""message"": ""headers must be a table"" "" with strings as keys and values."" ""Header: `{!r}:{!r}` is not valid"".format(key, value) } ) res[key] = value return res",False,"if not isinstance ( key , ( bytes , str ) ) or not isinstance ( value , ( bytes , str ) ) :","if not isinstance ( value , ( list , tuple ) ) :",0.8820016898747209,23.531897256100365
"def _get_literal_value(self, pyval): if pyval == self.vm.lookup_builtin(""builtins.True""): return True elif pyval == self.vm.lookup_builtin(""builtins.False""): return False elif isinstance(pyval, str): prefix, value = parser_constants.STRING_RE.match(pyval).groups()[:2] value = value[1:-1] # remove quotation marks <mask>: value = compat.bytestring(value) elif ""u"" in prefix and self.vm.PY2: value = compat.UnicodeType(value) return value else: return pyval",False,"if ""b"" in prefix and not self . vm . PY2 :","if ""b"" in prefix and self . vm . PY1 :",0.8820016898747209,63.436083375358535
"def decode_query_ids(self, trans, conditional): if conditional.operator == ""and"": self.decode_query_ids(trans, conditional.left) self.decode_query_ids(trans, conditional.right) else: left_base = conditional.left.split(""."")[0] <mask>: field = self.FIELDS[left_base] if field.id_decode: conditional.right = trans.security.decode_id(conditional.right)",True,if left_base in self . FIELDS :,if left_base in self . FIELDS :,0.8820016898747209,100.00000000000004
"def testLastPhrases(self): for day in (11, 12, 13, 14, 15, 16, 17): start = datetime.datetime(2012, 11, day, 9, 0, 0) (yr, mth, dy, _, _, _, wd, yd, isdst) = start.timetuple() n = 4 - wd <mask>: n -= 7 target = start + datetime.timedelta(days=n) self.assertExpectedResult( self.cal.parse(""last friday"", start.timetuple()), (target.timetuple(), 1), dateOnly=True, )",False,if n >= 0 :,if isdst :,0.8820016898747209,0.0
"def _convertNbCharsInNbBits(self, nbChars): nbMinBit = None nbMaxBit = None if nbChars is not None: if isinstance(nbChars, int): nbMinBit = nbChars * 8 nbMaxBit = nbMinBit else: <mask>: nbMinBit = nbChars[0] * 8 if nbChars[1] is not None: nbMaxBit = nbChars[1] * 8 return (nbMinBit, nbMaxBit)",True,if nbChars [ 0 ] is not None :,if nbChars [ 0 ] is not None :,0.8820016898747209,100.00000000000004
"def getpystone(): # Start calculation maxpystone = 0 # Start with a short run, find the the pystone, and increase runtime until duration took > 0.1 second for pyseed in [1000, 2000, 5000, 10000, 20000, 50000, 100000, 200000]: duration, pystonefloat = pystones(pyseed) maxpystone = max(maxpystone, int(pystonefloat)) # Stop when pystone() has been running for at least 0.1 second <mask>: break return maxpystone",True,if duration > 0.1 :,if duration > 0.1 :,0.8820016898747209,100.00000000000004
"def _append_to_io_queue(self, data, stream_name): # Make sure ANSI CSI codes and object links are stored as separate events # TODO: try to complete previously submitted incomplete code parts = re.split(OUTPUT_SPLIT_REGEX, data) for part in parts: <mask>: # split may produce empty string in the beginning or start # split the data so that very long lines separated for block in re.split( ""(.{%d,})"" % (self._get_squeeze_threshold() + 1), part ): if block: self._queued_io_events.append((block, stream_name))",True,if part :,if part :,0.8820016898747209,0.0
"def qtTypeIdent(conn, *args): # We're not using the conn object at the moment, but - we will # modify the # logic to use the server version specific keywords later. res = None value = None for val in args: # DataType doesn't have len function then convert it to string <mask>: val = str(val) if len(val) == 0: continue value = val if Driver.needsQuoting(val, True): value = value.replace('""', '""""') value = '""' + value + '""' res = ((res and res + ""."") or """") + value return res",False,"if not hasattr ( val , ""__len__"" ) :","if isinstance ( val , str ) :",0.8820016898747209,11.277832374502772
"def SetVerbose(self, level): """"""Sets the verbose level."""""" try: <mask>: level = int(level) if (level >= 0) and (level <= 3): self._verbose = level return except ValueError: pass self.Error(""Verbose level (%s) must be between 0 and 3 inclusive."" % level)",False,if type ( level ) != types . IntType :,"if isinstance ( level , int ) :",0.8820016898747209,9.545138913210204
"def step(self) -> None: """"""Performs a single optimization step."""""" for group in self.param_groups: for p in group[""params""]: <mask>: continue p.add_(p.grad, alpha=(-group[""lr""] * self.num_data)) return None",True,if p . grad is None :,if p . grad is None :,0.8820016898747209,100.00000000000004
"def fill(self, values): if lupa.lua_type(values) != ""table"": raise ScriptError( { ""argument"": ""values"", ""message"": ""element:fill values is not a table"", ""splash_method"": ""fill"", } ) # marking all tables as arrays by default for key, value in values.items(): <mask>: _mark_table_as_array(self.lua, value) values = self.lua.lua2python(values) return self.element.fill(values)",False,"if lupa . lua_type ( value ) == ""table"" :","if key == ""table"" :",0.8820016898747209,29.476596093379342
"def _gen_repr(self, buf): print >> buf, "" def __repr__(self):"" if self.argnames: fmt = COMMA.join([""%s""] * self.nargs) <mask>: fmt = ""(%s)"" % fmt vals = [""repr(self.%s)"" % name for name in self.argnames] vals = COMMA.join(vals) if self.nargs == 1: vals = vals + "","" print >> buf, ' return ""%s(%s)"" %% (%s)' % (self.name, fmt, vals) else: print >> buf, ' return ""%s()""' % self.name",False,"if ""("" in self . args :",if self . nargs == 0 :,0.8820016898747209,11.59119922599073
"def render_observation(self): x = self.read_head_position label = ""Observation Grid : "" x_str = """" for j in range(-1, self.rows + 1): <mask>: x_str += "" "" * len(label) for i in range(-2, self.input_width + 2): if i == x[0] and j == x[1]: x_str += colorize(self._get_str_obs((i, j)), ""green"", highlight=True) else: x_str += self._get_str_obs((i, j)) x_str += ""\n"" x_str = label + x_str return x_str",False,if j != - 1 :,if x_str :,0.8820016898747209,8.51528917838043
"def get_module_comment(self, attrname: str) -> Optional[List[str]]: try: analyzer = ModuleAnalyzer.for_module(self.modname) analyzer.analyze() key = ("""", attrname) <mask>: return list(analyzer.attr_docs[key]) except PycodeError: pass return None",True,if key in analyzer . attr_docs :,if key in analyzer . attr_docs :,0.8820016898747209,100.00000000000004
"def tms_to_quadkey(self, tms, google=False): quadKey = """" x, y, z = tms # this algorithm works with google tiles, rather than tms, so convert # to those first. if not google: y = (2 ** z - 1) - y for i in range(z, 0, -1): digit = 0 mask = 1 << (i - 1) <mask>: digit += 1 if (y & mask) != 0: digit += 2 quadKey += str(digit) return quadKey",True,if ( x & mask ) != 0 :,if ( x & mask ) != 0 :,0.8820016898747209,100.00000000000004
"def test_enumerate(app): async with new_stream(app) as stream: for i in range(100): await stream.channel.deliver(message(key=i, value=i * 4)) async for i, value in stream.enumerate(): current_event = stream.current_event assert i == current_event.key assert value == i * 4 <mask>: break assert await channel_empty(stream.channel)",False,if i >= 99 :,if current_event . value == value :,0.8820016898747209,5.522397783539471
"def print_messages(self): output_reports = self.config.get_output_report() for report in output_reports: output_format, output_files = report self.summary[""formatter""] = output_format formatter = FORMATTERS[output_format]( self.summary, self.messages, self.config.profile ) <mask>: self.write_to(formatter, sys.stdout) for output_file in output_files: with open(output_file, ""w+"") as target: self.write_to(formatter, target)",False,if not output_files :,if output_files :,0.8820016898747209,57.89300674674101
"def eval_metrics(self): for task in self.task_list: <mask>: return [ metrics.Metrics.ACC, metrics.Metrics.NEG_LOG_PERPLEXITY, metrics.Metrics.ROUGE_2_F, metrics.Metrics.ROUGE_L_F, ] return [ metrics.Metrics.ACC, metrics.Metrics.NEG_LOG_PERPLEXITY, ]",False,"if ""summarize"" in task . name :","if task . get_state ( ) == ""running"" :",0.8820016898747209,7.474875887495341
"def _getBuildRequestForBrdict(self, brdict): # Turn a brdict into a BuildRequest into a brdict. This is useful # for API like 'nextBuild', which operate on BuildRequest objects. breq = self.breqCache.get(brdict[""buildrequestid""]) if not breq: breq = yield BuildRequest.fromBrdict(self.master, brdict) <mask>: self.breqCache[brdict[""buildrequestid""]] = breq defer.returnValue(breq)",True,if breq :,if breq :,0.8820016898747209,0.0
"def _stash_splitter(states): keep, split = [], [] if state_func is not None: for s in states: ns = state_func(s) <mask>: split.append(ns) elif isinstance(ns, (list, tuple, set)): split.extend(ns) else: split.append(s) if stash_func is not None: split = stash_func(states) if to_stash is not stash: keep = states return keep, split",False,"if isinstance ( ns , SimState ) :","if isinstance ( ns , ( tuple , set ) ) :",0.8820016898747209,36.462858619364674
"def sequence_to_text(sequence): """"""Converts a sequence of IDs back to a string"""""" result = """" for symbol_id in sequence: <mask>: s = _id_to_symbol[symbol_id] # Enclose ARPAbet back in curly braces: if len(s) > 1 and s[0] == ""@"": s = ""{%s}"" % s[1:] result += s return result.replace(""}{"", "" "")",True,if symbol_id in _id_to_symbol :,if symbol_id in _id_to_symbol :,0.8820016898747209,100.00000000000004
"def get_code(self, fullname=None): fullname = self._fix_name(fullname) if self.code is None: mod_type = self.etc[2] <mask>: source = self.get_source(fullname) self.code = compile(source, self.filename, ""exec"") elif mod_type == imp.PY_COMPILED: self._reopen() try: self.code = read_code(self.file) finally: self.file.close() elif mod_type == imp.PKG_DIRECTORY: self.code = self._get_delegate().get_code() return self.code",False,if mod_type == imp . PY_SOURCE :,if mod_type == imp . PY_COMPILED :,0.8820016898747209,82.651681837938
"def identwaf(self, findall=False): detected = list() try: self.attackres = self.performCheck(self.centralAttack) except RequestBlocked: return detected for wafvendor in self.checklist: self.log.info(""Checking for %s"" % wafvendor) if self.wafdetections[wafvendor](self): detected.append(wafvendor) <mask>: break self.knowledge[""wafname""] = detected return detected",False,if not findall :,if findall :,0.8820016898747209,0.0
"def SessionId(self): """"""Returns the Session ID of the process"""""" if self.Session.is_valid(): process_space = self.get_process_address_space() <mask>: return obj.Object( ""_MM_SESSION_SPACE"", offset=self.Session, vm=process_space ).SessionId return obj.NoneObject(""Cannot find process session"")",True,if process_space :,if process_space :,0.8820016898747209,100.00000000000004
"def _convert_java_pattern_to_python(pattern): """"""Convert a replacement pattern from the Java-style `$5` to the Python-style `\\5`."""""" s = list(pattern) i = 0 while i < len(s) - 1: c = s[i] if c == ""$"" and s[i + 1] in ""0123456789"": s[i] = ""\\"" <mask>: s[i] = """" i += 1 i += 1 return pattern[:0].join(s)",False,"elif c == ""\\"" and s [ i + 1 ] == ""$"" :","elif c == ""\\"" and s [ i ] in ""0123456789"" :",0.8820016898747209,58.95496065101167
"def __init__(self, coverage): self.coverage = coverage self.config = self.coverage.config self.source_paths = set() if self.config.source: for src in self.config.source: <mask>: if not self.config.relative_files: src = files.canonical_filename(src) self.source_paths.add(src) self.packages = {} self.xml_out = None",False,if os . path . exists ( src ) :,"if src . startswith ( ""src"" ) :",0.8820016898747209,11.044795567078939
"def populate_vol_format(self): rhel6_file_whitelist = [""raw"", ""qcow2"", ""qed""] model = self.widget(""vol-format"").get_model() model.clear() formats = self.vol_class.formats if hasattr(self.vol_class, ""create_formats""): formats = getattr(self.vol_class, ""create_formats"") if self.vol_class == Storage.FileVolume and not self.conn.rhel6_defaults_caps(): newfmts = [] for f in rhel6_file_whitelist: <mask>: newfmts.append(f) formats = newfmts for f in formats: model.append([f, f])",False,if f in formats :,if f not in formats :,0.8820016898747209,37.99178428257963
"def get_file_sources(): global _file_sources if _file_sources is None: from galaxy.files import ConfiguredFileSources file_sources = None <mask>: file_sources_as_dict = None with open(""file_sources.json"", ""r"") as f: file_sources_as_dict = json.load(f) if file_sources_as_dict is not None: file_sources = ConfiguredFileSources.from_dict(file_sources_as_dict) if file_sources is None: ConfiguredFileSources.from_dict([]) _file_sources = file_sources return _file_sources",False,"if os . path . exists ( ""file_sources.json"" ) :","if os . path . isfile ( ""file_sources.json"" ) :",0.8820016898747209,81.53551038173119
"def _blend(x, y): # pylint: disable=invalid-name """"""Implements the ""blend"" strategy for `deep_merge`."""""" if isinstance(x, (dict, OrderedDict)): if not isinstance(y, (dict, OrderedDict)): return y return _merge(x, y, recursion_func=_blend) if isinstance(x, (list, tuple)): if not isinstance(y, (list, tuple)): return y result = [_blend(*i) for i in zip(x, y)] <mask>: result += x[len(y) :] elif len(x) < len(y): result += y[len(x) :] return result return y",False,if len ( x ) > len ( y ) :,if len ( x ) < len ( y ) :,0.8820016898747209,70.16879391277372
"def copy_dicts(dct): if ""_remote_data"" in dct: dsindex = dct[""_remote_data""][""_content""].dsindex newdct = dct.copy() newdct[""_remote_data""] = {""_content"": dsindex} return list(newdct.items()) elif ""_data"" in dct: newdct = dct.copy() newdata = copy_dicts(dct[""_data""]) <mask>: newdct[""_data""] = newdata return list(newdct.items()) return None",True,if newdata :,if newdata :,0.8820016898747209,0.0
"def _import_epic_activity(self, project_data, taiga_epic, epic, options): offset = 0 while True: activities = self._client.get( ""/projects/{}/epics/{}/activity"".format( project_data[""id""], epic[""id""], ), {""envelope"": ""true"", ""limit"": 300, ""offset"": offset}, ) offset += 300 for activity in activities[""data""]: self._import_activity(taiga_epic, activity, options) <mask>: break",False,"if len ( activities [ ""data"" ] ) < 300 :","if not activities [ ""success"" ] :",0.8820016898747209,15.564390138142427
"def __get__(self, instance, instance_type=None): if instance: <mask>: rel_obj = self.get_obj(instance) if rel_obj: instance._obj_cache[self.att_name] = rel_obj return instance._obj_cache.get(self.att_name) return self",True,if self . att_name not in instance . _obj_cache :,if self . att_name not in instance . _obj_cache :,0.8820016898747209,100.00000000000004
"def download_main( download, download_playlist, urls, playlist, output_dir, merge, info_only ): for url in urls: if url.startswith(""https://""): url = url[8:] if not url.startswith(""http://""): url = ""http://"" + url <mask>: download_playlist( url, output_dir=output_dir, merge=merge, info_only=info_only ) else: download(url, output_dir=output_dir, merge=merge, info_only=info_only)",True,if playlist :,if playlist :,0.8820016898747209,0.0
"def _mksubs(self): self._subs = {} commit_dir = CommitDir(self, "".commit"") self._subs["".commit""] = commit_dir tag_dir = TagDir(self, "".tag"") self._subs["".tag""] = tag_dir for (name, sha) in git.list_refs(): <mask>: name = name[11:] date = git.rev_get_date(sha.encode(""hex"")) n1 = BranchList(self, name, sha) n1.ctime = n1.mtime = date self._subs[name] = n1",False,"if name . startswith ( ""refs/heads/"" ) :","if name . startswith ( ""git"" ) :",0.8820016898747209,48.74858042804567
"def readAtOffset(self, offset, size, shortok=False): ret = b"""" self.fd.seek(offset) while len(ret) != size: rlen = size - len(ret) x = self.fd.read(rlen) <mask>: if not shortok: return None return ret ret += x return ret",True,"if x == b"""" :","if x == b"""" :",0.8820016898747209,100.00000000000004
"def remove_indent(self): """"""Remove one tab-width of blanks from the previous token."""""" w = abs(self.tab_width) if self.result: s = self.result[-1] <mask>: self.result.pop() s = s.replace(""\t"", "" "" * w) if s.startswith(""\n""): s2 = s[1:] self.result.append(""\n"" + s2[:-w]) else: self.result.append(s[:-w])",False,if s . isspace ( ) :,"if s . startswith ( ""\t"" ) :",0.8820016898747209,18.36028134946796
"def flush(self, *args, **kwargs): with self._lock: self._last_updated = time.time() try: if kwargs.get(""in_place"", False): self._locked_flush_without_tempfile() else: mailbox.mbox.flush(self, *args, **kwargs) except OSError: <mask>: self._locked_flush_without_tempfile() else: raise self._last_updated = time.time()",False,"if ""_create_temporary"" in traceback . format_exc ( ) :","if kwargs . get ( ""in_place"" , False ) :",0.8820016898747209,8.527902588160915
"def _collect_manual_intervention_nodes(pipeline_tree): for act in pipeline_tree[""activities""].values(): <mask>: _collect_manual_intervention_nodes(act[""pipeline""]) elif act[""component""][""code""] in MANUAL_INTERVENTION_COMP_CODES: manual_intervention_nodes.add(act[""id""])",False,"if act [ ""type"" ] == ""SubProcess"" :","if act [ ""component"" ] [ ""code"" ] in MANUAL_INTERVENTION_PIPELINE_CODES :",0.8820016898747209,14.962848372546667
"def banned(): if request.endpoint == ""views.themes"": return if authed(): user = get_current_user_attrs() team = get_current_team_attrs() <mask>: return ( render_template( ""errors/403.html"", error=""You have been banned from this CTF"" ), 403, ) if team and team.banned: return ( render_template( ""errors/403.html"", error=""Your team has been banned from this CTF"", ), 403, )",True,if user and user . banned :,if user and user . banned :,0.8820016898747209,100.00000000000004
"def remove(self, values): if not isinstance(values, (list, tuple, set)): values = [values] for v in values: v = str(v) if isinstance(self._definition, dict): self._definition.pop(v, None) elif self._definition == ""ANY"": if v == ""ANY"": self._definition = [] <mask>: self._definition.remove(v) if ( self._value is not None and self._value not in self._definition and self._not_any() ): raise ConanException(bad_value_msg(self._name, self._value, self.values_range))",True,elif v in self . _definition :,elif v in self . _definition :,0.8820016898747209,100.00000000000004
"def save(self, learner, file_name): """"""Save the model to location specified in file_name."""""" with open(file_name, ""wb"") as f: <mask>: # don't store the large inference cache! learner.inference_cache_, tmp = (None, learner.inference_cache_) pickle.dump(learner, f, -1) learner.inference_cache_ = tmp else: pickle.dump(learner, f, -1)",False,"if hasattr ( learner , ""inference_cache_"" ) :",if learner . inference_cache_ :,0.8820016898747209,21.283887316959504
"def __init__(self, exprs, savelist=False): super(ParseExpression, self).__init__(savelist) if isinstance(exprs, _generatorType): exprs = list(exprs) if isinstance(exprs, basestring): self.exprs = [ParserElement._literalStringClass(exprs)] elif isinstance(exprs, collections.Iterable): exprs = list(exprs) # if sequence of strings provided, wrap with Literal <mask>: exprs = map(ParserElement._literalStringClass, exprs) self.exprs = list(exprs) else: try: self.exprs = list(exprs) except TypeError: self.exprs = [exprs] self.callPreparse = False",False,"if all ( isinstance ( expr , basestring ) for expr in exprs ) :","if isinstance ( exprs , ( list , tuple ) ) :",0.8820016898747209,9.136248401161414
"def find(self, back=False): flags = 0 <mask>: flags = QTextDocument.FindBackward if self.csBox.isChecked(): flags = flags | QTextDocument.FindCaseSensitively text = self.searchEdit.text() if not self.findMain(text, flags): if text in self.editBoxes[self.ind].toPlainText(): cursor = self.editBoxes[self.ind].textCursor() if back: cursor.movePosition(QTextCursor.End) else: cursor.movePosition(QTextCursor.Start) self.editBoxes[self.ind].setTextCursor(cursor) self.findMain(text, flags)",False,if back :,if self . csBox . isChecked ( ) :,0.8820016898747209,5.669791110976001
"def _load_storage(self): self._storage = {} for row in self(""SELECT object, resource, amount FROM storage""): ownerid = int(row[0]) <mask>: self._storage[ownerid].append(row[1:]) else: self._storage[ownerid] = [row[1:]]",True,if ownerid in self . _storage :,if ownerid in self . _storage :,0.8820016898747209,100.00000000000004
"def parse_chunked(self, unreader): (size, rest) = self.parse_chunk_size(unreader) while size > 0: while size > len(rest): size -= len(rest) yield rest rest = unreader.read() <mask>: raise NoMoreData() yield rest[:size] # Remove \r\n after chunk rest = rest[size:] while len(rest) < 2: rest += unreader.read() if rest[:2] != b""\r\n"": raise ChunkMissingTerminator(rest[:2]) (size, rest) = self.parse_chunk_size(unreader, data=rest[2:])",False,if not rest :,"if rest [ : size ] == b""\r\n"" :",0.8820016898747209,3.21858262703621
"def _augment_batch_(self, batch, random_state, parents, hooks): for column in batch.columns: <mask>: for i, cbaoi in enumerate(column.value): column.value[i] = cbaoi.clip_out_of_image_() return batch",False,"if column . name in [ ""keypoints"" , ""bounding_boxes"" , ""polygons"" , ""line_strings"" ] :","if isinstance ( column , ImageColumn ) :",0.8820016898747209,0.7264291938935746
"def to_nim(self): if self.is_pointer == 2: s = ""cstringArray"" if self.type == ""GLchar"" else ""ptr pointer"" else: s = self.type <mask>: default = ""ptr "" + s s = self.NIM_POINTER_MAP.get(s, default) return s",True,if self . is_pointer == 1 :,if self . is_pointer == 1 :,0.8820016898747209,100.00000000000004
"def find(self, path): if os.path.isfile(path) or os.path.islink(path): self.num_files = self.num_files + 1 if self.match_function(path): self.files.append(path) elif os.path.isdir(path): for content in os.listdir(path): file = os.path.join(path, content) <mask>: self.num_files = self.num_files + 1 if self.match_function(file): self.files.append(file) else: self.find(file)",True,if os . path . isfile ( file ) or os . path . islink ( file ) :,if os . path . isfile ( file ) or os . path . islink ( file ) :,0.8820016898747209,100.00000000000004
"def remove(self, event): try: self._events_current_sweep.remove(event) <mask>: assert event.in_sweep == True assert event.other.in_sweep == True event.in_sweep = False event.other.in_sweep = False return True except KeyError: if USE_DEBUG: assert event.in_sweep == False assert event.other.in_sweep == False return False",True,if USE_DEBUG :,if USE_DEBUG :,0.8820016898747209,100.00000000000004
"def update_metadata(self): for attrname in dir(self): if attrname.startswith(""__""): continue attrvalue = getattr(self, attrname, None) if attrvalue == 0: continue <mask>: attrname = ""version"" if hasattr(self.metadata, ""set_{0}"".format(attrname)): getattr(self.metadata, ""set_{0}"".format(attrname))(attrvalue) elif hasattr(self.metadata, attrname): try: setattr(self.metadata, attrname, attrvalue) except AttributeError: pass",False,"if attrname == ""salt_version"" :","if attrname == ""version"" :",0.8820016898747209,53.849523560640876
"def _init_auxiliary_head(self, auxiliary_head): """"""Initialize ``auxiliary_head``"""""" if auxiliary_head is not None: <mask>: self.auxiliary_head = nn.ModuleList() for head_cfg in auxiliary_head: self.auxiliary_head.append(builder.build_head(head_cfg)) else: self.auxiliary_head = builder.build_head(auxiliary_head)",True,"if isinstance ( auxiliary_head , list ) :","if isinstance ( auxiliary_head , list ) :",0.8820016898747209,100.00000000000004
"def _str_param_list(self, name): out = [] if self[name]: out += self._str_header(name) for param in self[name]: parts = [] if param.name: parts.append(param.name) <mask>: parts.append(param.type) out += ["" : "".join(parts)] if param.desc and """".join(param.desc).strip(): out += self._str_indent(param.desc) out += [""""] return out",False,if param . type :,elif param . type :,0.8820016898747209,66.87403049764218
"def _set_handler( self, name, handle=None, obj=None, constructor_args=(), constructor_kwds={} ): if handle is None: handle = obj is not None if handle: handler_class = self.handler_classes[name] <mask>: newhandler = handler_class(obj) else: newhandler = handler_class(*constructor_args, **constructor_kwds) else: newhandler = None self._replace_handler(name, newhandler)",False,if obj is not None :,"if isinstance ( obj , type ) :",0.8820016898747209,7.267884212102741
"def _extract_subtitles(src): subtitles = {} for caption in try_get(src, lambda x: x[""captions""], list) or []: subtitle_url = url_or_none(caption.get(""uri"")) <mask>: lang = caption.get(""language"", ""deu"") subtitles.setdefault(lang, []).append( { ""url"": subtitle_url, } ) return subtitles",True,if subtitle_url :,if subtitle_url :,0.8820016898747209,100.00000000000004
"def get_keys(struct, ignore_first_level=False): res = [] if isinstance(struct, dict): <mask>: keys = [x.split(""("")[0] for x in struct.keys()] res.extend(keys) for key in struct: if key in IGNORED_KEYS: logging.debug(""Ignored: %s: %s"", key, struct[key]) continue res.extend(get_keys(struct[key], key in IGNORED_FIRST_LEVEL)) elif isinstance(struct, list): for item in struct: res.extend(get_keys(item)) return res",False,if not ignore_first_level :,if ignore_first_level :,0.8820016898747209,72.89545183625967
"def create_dir(path): curr_path = None for p in path: if curr_path is None: curr_path = os.path.abspath(p) else: curr_path = os.path.join(curr_path, p) <mask>: os.mkdir(curr_path)",True,if not os . path . exists ( curr_path ) :,if not os . path . exists ( curr_path ) :,0.8820016898747209,100.00000000000004
"def dataToDumpFile(dumpFile, data): try: dumpFile.write(data) dumpFile.flush() except IOError as ex: if ""No space left"" in getUnicode(ex): errMsg = ""no space left on output device"" logger.error(errMsg) <mask>: errMsg = ""permission denied when flushing dump data"" logger.error(errMsg) else: errMsg = ( ""error occurred when writing dump data to file ('%s')"" % getUnicode(ex) ) logger.error(errMsg)",True,"elif ""Permission denied"" in getUnicode ( ex ) :","elif ""Permission denied"" in getUnicode ( ex ) :",0.8820016898747209,100.00000000000004
"def elements(self, top): res = [] # try: # string = ""== %s (%s)"" % (self.name,self.__class__) # except AttributeError: # string = ""== (%s)"" % (self.__class__,) # print(string) for part in self.parts: <mask>: res.append(name_or_ref(part, top)) else: if isinstance(part, Extension): res.append(part.base) res.extend(part.elements(top)) return res",False,"if isinstance ( part , Element ) :","if isinstance ( part , Name ) :",0.8820016898747209,59.4603557501361
"def _parse_param_value(name, datatype, default): if datatype == ""bool"": if default.lower() == ""true"": return True <mask>: return False else: _s = ""{}: Invalid default value '{}' for bool parameter {}"" raise SyntaxError(_s.format(self.name, default, p)) elif datatype == ""int"": if type(default) == int: return default else: return int(default, 0) elif datatype == ""real"": if type(default) == float: return default else: return float(default) else: return str(default)",True,"elif default . lower ( ) == ""false"" :","elif default . lower ( ) == ""false"" :",0.8820016898747209,100.00000000000004
"def dvmethod(c, dx, doAST=False): for m in c.get_methods(): mx = dx.get_method(m) ms = DvMethod(mx) ms.process(doAST=doAST) <mask>: assert ms.get_ast() is not None assert isinstance(ms.get_ast(), dict) assert ""body"" in ms.get_ast() else: assert ms.get_source() is not None",True,if doAST :,if doAST :,0.8820016898747209,0.0
"def _repr_pretty_(self, p, cycle): if cycle: return ""{{...}"" with p.group(2, ""{"", ""}""): p.breakable("""") for idx, key in enumerate(self._items): <mask>: p.text("","") p.breakable() value = self._items[key] p.pretty(key) p.text("": "") if isinstance(value, bytes): value = trimmed_repr(value) p.pretty(value) p.breakable("""")",False,if idx :,if idx == 0 :,0.8820016898747209,17.965205598154213
"def remove_rating(self, songs, librarian): count = len(songs) if count > 1 and config.getboolean(""browsers"", ""rating_confirm_multiple""): parent = qltk.get_menu_item_top_parent(self) dialog = ConfirmRateMultipleDialog(parent, _(""_Remove Rating""), count, None) <mask>: return reset = [] for song in songs: if ""~#rating"" in song: del song[""~#rating""] reset.append(song) librarian.changed(reset)",False,if dialog . run ( ) != Gtk . ResponseType . YES :,if dialog . is_active ( ) :,0.8820016898747209,13.40110063389608
"def get_or_create_place(self, place_name): ""Return the requested place object tuple-packed with a new indicator."" LOG.debug(""get_or_create_place: looking for: %s"", place_name) for place_handle in self.db.iter_place_handles(): place = self.db.get_place_from_handle(place_handle) place_title = place_displayer.display(self.db, place) <mask>: return (0, place) place = Place() place.set_title(place_name) place.name = PlaceName(value=place_name) self.db.add_place(place, self.trans) return (1, place)",True,if place_title == place_name :,if place_title == place_name :,0.8820016898747209,100.00000000000004
def _skip_trivial(constraint_data): if skip_trivial_constraints: <mask>: if constraint_data.variables is None: return True else: if constraint_data.body.polynomial_degree() == 0: return True return False,False,"if isinstance ( constraint_data , LinearCanonicalRepn ) :",if constraint_data . body is None :,0.8820016898747209,18.04438612975343
"def get_other(self, data, items): is_tuple = False if type(data) == tuple: data = list(data) is_tuple = True if type(data) == list: m_items = items.copy() for idx, item in enumerate(items): if item < 0: m_items[idx] = len(data) - abs(item) for i in sorted(set(m_items), reverse=True): <mask>: del data[i] if is_tuple: return tuple(data) else: return data else: return None",False,if i < len ( data ) and i > - 1 :,if data [ i ] == item :,0.8820016898747209,4.32319463004898
"def test_case_insensitivity(self): with support.EnvironmentVarGuard() as env: env.set(""PYTHONCASEOK"", ""1"") <mask>: self.skipTest(""os.environ changes not reflected in "" ""_os.environ"") loader = self.find_module() self.assertTrue(hasattr(loader, ""load_module""))",False,"if b""PYTHONCASEOK"" not in _bootstrap . _os . environ :","if not hasattr ( os , ""_os.environ"" ) :",0.8820016898747209,20.2729092237656
def field_spec(self): <mask>: self.lazy_init_lock_.acquire() try: if self.field_spec_ is None: self.field_spec_ = FieldSpec() finally: self.lazy_init_lock_.release() return self.field_spec_,True,if self . field_spec_ is None :,if self . field_spec_ is None :,0.8820016898747209,100.00000000000004
"def reduce(self, f, init): for x in range(self._idx, rt.count(self._w_array)): <mask>: return rt.deref(init) init = f.invoke([init, rt.nth(self._w_array, rt.wrap(x))]) return init",False,if rt . reduced_QMARK_ ( init ) :,if x == self . _idx :,0.8820016898747209,5.3990167242108145
"def _find(event: E) -> None: # We first check values after the selected value, then all values. values = list(self.values) for value in values[self._selected_index + 1 :] + values: text = fragment_list_to_text(to_formatted_text(value[1])).lower() <mask>: self._selected_index = self.values.index(value) return",False,if text . startswith ( event . data . lower ( ) ) :,if text in self . values :,0.8820016898747209,5.746166391236874
"def check_permissions(): if platform_os() != ""Windows"": <mask>: print(localization.lang_check_permissions[""permissions_granted""]) else: print(localization.lang_check_permissions[""permissions_denied""]) exit() else: print(localization.lang_check_permissions[""windows_warning""]) exit()",False,if getuid ( ) == 0 :,"if localization . lang_check_permissions [ ""permissions_granted"" ] :",0.8820016898747209,2.908317710573757
"def _ProcessName(self, name, dependencies): """"""Retrieve a module name from a node name."""""" module_name, dot, base_name = name.rpartition(""."") if dot: if module_name: <mask>: dependencies[module_name].add(base_name) else: dependencies[module_name] = {base_name} else: # If we have a relative import that did not get qualified (usually due # to an empty package_name), don't insert module_name='' into the # dependencies; we get a better error message if we filter it out here # and fail later on. logging.warning(""Empty package name: %s"", name)",True,if module_name in dependencies :,if module_name in dependencies :,0.8820016898747209,100.00000000000004
"def _load_db(self): try: with open(self.db) as db: content = db.read(8) db.seek(0) if content == (""Salted__""): data = StringIO() <mask>: self.encryptor.decrypt(db, data) else: raise EncryptionError( ""Encrpyted credential storage: {}"".format(self.db) ) return json.loads(data.getvalue()) else: return json.load(db) except: return {""creds"": []}",True,if self . encryptor :,if self . encryptor :,0.8820016898747209,100.00000000000004
"def _parse(self, stream, context): obj = [] try: context_for_subcon = context if self.subcon.conflags & self.FLAG_COPY_CONTEXT: context_for_subcon = context.__copy__() while True: subobj = self.subcon._parse(stream, context_for_subcon) <mask>: break obj.append(subobj) except ConstructError as ex: raise ArrayError(""missing terminator"", ex) return obj",False,"if self . predicate ( subobj , context ) :",if subobj is None :,0.8820016898747209,5.171845311465849
"def is_active_for_user(self, user): is_active = super(AbstractUserFlag, self).is_active_for_user(user) if is_active: return is_active user_ids = self._get_user_ids() if hasattr(user, ""pk"") and user.pk in user_ids: return True if hasattr(user, ""groups""): group_ids = self._get_group_ids() if group_ids: user_groups = set(user.groups.all().values_list(""pk"", flat=True)) <mask>: return True return None",False,if group_ids . intersection ( user_groups ) :,if user_groups and user_groups . pk in group_ids :,0.8820016898747209,18.207052811092137
"def lookup_member(self, member_name): document_choices = self.choices or [] for document_choice in document_choices: doc_and_subclasses = [document_choice] + document_choice.__subclasses__() for doc_type in doc_and_subclasses: field = doc_type._fields.get(member_name) <mask>: return field",True,if field :,if field :,0.8820016898747209,0.0
"def apply(self, db, person): families = person.get_parent_family_handle_list() if families == []: return True for family_handle in person.get_parent_family_handle_list(): family = db.get_family_from_handle(family_handle) if family: father_handle = family.get_father_handle() mother_handle = family.get_mother_handle() if not father_handle: return True <mask>: return True return False",True,if not mother_handle :,if not mother_handle :,0.8820016898747209,100.00000000000004
"def init_weights(self): for m in self.modules(): if isinstance(m, nn.Linear): normal_init(m, std=0.01) if isinstance(m, nn.Conv3d): xavier_init(m, distribution=""uniform"") <mask>: constant_init(m, 1)",False,"if isinstance ( m , nn . BatchNorm3d ) :","if isinstance ( m , nn . Linear ) :",0.8820016898747209,70.71067811865478
"def _update_learning_params(self): model = self.model hparams = self.hparams fd = self.runner.feed_dict step_num = self.step_num if hparams.model_type == ""resnet_tf"": <mask>: lrn_rate = hparams.mom_lrn elif step_num < 30000: lrn_rate = hparams.mom_lrn / 10 elif step_num < 35000: lrn_rate = hparams.mom_lrn / 100 else: lrn_rate = hparams.mom_lrn / 1000 fd[model.lrn_rate] = lrn_rate",False,if step_num < hparams . lrn_step :,if step_num < 100000 :,0.8820016898747209,36.337289265247364
"def token_producer(source): token = source.read_uint8() while token is not None: if is_push_data_token(token): yield DataToken(read_data(token, source)) <mask>: yield SmallIntegerToken(read_small_integer(token)) else: yield Token(token) token = source.read_uint8()",False,elif is_small_integer ( token ) :,elif is_push_small_integer_token ( token ) :,0.8820016898747209,42.718025135819786
"def user_info(oicsrv, userdb, sub, client_id="""", user_info_claims=None): identity = userdb[sub] if user_info_claims: result = {} for key, restr in user_info_claims[""claims""].items(): try: result[key] = identity[key] except KeyError: <mask>: raise Exception(""Missing property '%s'"" % key) else: result = identity return OpenIDSchema(**result)",False,"if restr == { ""essential"" : True } :",if client_id not in identity :,0.8820016898747209,3.983253478176822
"def _helpSlot(self, *args): help_text = ""Filters are applied to packets in both direction.\n\n"" filter_nb = 0 for filter in self._filters: help_text += ""{}: {}"".format(filter[""name""], filter[""description""]) filter_nb += 1 <mask>: help_text += ""\n\n"" QtWidgets.QMessageBox.information(self, ""Help for filters"", help_text)",False,if len ( self . _filters ) != filter_nb :,if filter_nb > 0 :,0.8820016898747209,10.218289380194191
"def find_user_theme(self, name: str) -> Theme: """"""Find a theme named as *name* from latex_theme_path."""""" for theme_path in self.theme_paths: config_path = path.join(theme_path, name, ""theme.conf"") <mask>: try: return UserTheme(name, config_path) except ThemeError as exc: logger.warning(exc) return None",False,if path . isfile ( config_path ) :,if path . exists ( config_path ) :,0.8820016898747209,65.80370064762461
"def decompress(self, value): if value: <mask>: if value.country_code and value.national_number: return [ ""+%d"" % value.country_code, national_significant_number(value), ] else: return value.split(""."") return [None, """"]",False,if type ( value ) == PhoneNumber :,"if value . startswith ( ""country_"" ) :",0.8820016898747209,5.604233375480572
"def update_prevdoc_status(self, flag): for quotation in list(set([d.prevdoc_docname for d in self.get(""items"")])): <mask>: doc = frappe.get_doc(""Quotation"", quotation) if doc.docstatus == 2: frappe.throw(_(""Quotation {0} is cancelled"").format(quotation)) doc.set_status(update=True) doc.update_opportunity()",False,if quotation :,if flag :,0.8820016898747209,0.0
"def map(item): if item.deleted: return exploration = exp_fetchers.get_exploration_from_model(item) for state_name, state in exploration.states.items(): hints_length = len(state.interaction.hints) <mask>: exp_and_state_key = ""%s %s"" % (item.id, state_name.encode(""utf-8"")) yield (python_utils.UNICODE(hints_length), exp_and_state_key)",True,if hints_length > 0 :,if hints_length > 0 :,0.8820016898747209,100.00000000000004
"def _selected_machines(self, virtual_machines): selected_machines = [] for machine in virtual_machines: if self._args.host and self._args.host == machine.name: selected_machines.append(machine) <mask>: selected_machines.append(machine) if self.locations and machine.location in self.locations: selected_machines.append(machine) return selected_machines",False,"if self . tags and self . _tags_match ( machine . tags , self . tags ) :",elif self . _args . port and self . _args . port == machine . port :,0.8820016898747209,16.68234857317378
"def _ripple_trim_compositors_move(self, delta): comp_ids = self.multi_data.moved_compositors_destroy_ids tracks_compositors = _get_tracks_compositors_list() track_moved = self.multi_data.track_affected for i in range(1, len(current_sequence().tracks) - 1): if not track_moved[i - 1]: continue track_comps = tracks_compositors[i - 1] for comp in track_comps: <mask>: comp.move(delta)",False,if comp . destroy_id in comp_ids :,if comp . id in comp_ids :,0.8820016898747209,59.86908497649472
"def stream_docker_log(log_stream): async for line in log_stream: if ""stream"" in line and line[""stream""].strip(): logger.debug(line[""stream""].strip()) <mask>: logger.debug(line[""status""].strip()) elif ""error"" in line: logger.error(line[""error""].strip()) raise DockerBuildError",False,"elif ""status"" in line :","elif ""status"" in line and line [ ""status"" ] . strip ( ) :",0.8820016898747209,27.499775953224148
"def create_keyfile(self, keyfile, size=64, force=False): if force or not os.path.exists(keyfile): keypath = os.path.dirname(keyfile) <mask>: os.makedirs(keypath) subprocess.run( [""dd"", ""if=/dev/random"", f""of={keyfile}"", f""bs={size}"", ""count=1""], check=True, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL, )",True,if not os . path . exists ( keypath ) :,if not os . path . exists ( keypath ) :,0.8820016898747209,100.00000000000004
"def calc(self, arg): op = arg[""op""] if op == ""C"": self.clear() return str(self.current) num = decimal.Decimal(arg[""num""]) if self.op: if self.op == ""+"": self.current += num elif self.op == ""-"": self.current -= num <mask>: self.current *= num elif self.op == ""/"": self.current /= num self.op = op else: self.op = op self.current = num res = str(self.current) if op == ""="": self.clear() return res",False,"elif self . op == ""*"" :","elif self . op == ""/"" :",0.8820016898747209,70.71067811865478
"def chop(expr, delta=10.0 ** (-10.0)): if isinstance(expr, Real): if -delta < expr.get_float_value() < delta: return Integer(0) elif isinstance(expr, Complex) and expr.is_inexact(): real, imag = expr.real, expr.imag <mask>: real = Integer(0) if -delta < imag.get_float_value() < delta: imag = Integer(0) return Complex(real, imag) elif isinstance(expr, Expression): return Expression(chop(expr.head), *[chop(leaf) for leaf in expr.leaves]) return expr",True,if - delta < real . get_float_value ( ) < delta :,if - delta < real . get_float_value ( ) < delta :,0.8820016898747209,100.00000000000004
"def get_file_sources(): global _file_sources if _file_sources is None: from galaxy.files import ConfiguredFileSources file_sources = None if os.path.exists(""file_sources.json""): file_sources_as_dict = None with open(""file_sources.json"", ""r"") as f: file_sources_as_dict = json.load(f) <mask>: file_sources = ConfiguredFileSources.from_dict(file_sources_as_dict) if file_sources is None: ConfiguredFileSources.from_dict([]) _file_sources = file_sources return _file_sources",True,if file_sources_as_dict is not None :,if file_sources_as_dict is not None :,0.8820016898747209,100.00000000000004
"def _get_sort_map(tags): """"""See TAG_TO_SORT"""""" tts = {} for name, tag in tags.items(): <mask>: if tag.user: tts[name] = ""%ssort"" % name if tag.internal: tts[""~%s"" % name] = ""~%ssort"" % name return tts",False,if tag . has_sort :,if tag . sort :,0.8820016898747209,33.51600230178196
"def __init__(self, **kwargs): if self.name is None: raise RuntimeError(""RenderPrimitive cannot be used directly"") self.option_values = {} for key, val in kwargs.items(): <mask>: raise ValueError( ""primitive `{0}' has no option `{1}'"".format(self.name, key) ) self.option_values[key] = val # set up defaults for name, (description, default) in self.options.items(): if not name in self.option_values: self.option_values[name] = default",False,if not key in self . options :,if key not in self . options :,0.8820016898747209,58.14307369682194
"def modify_bottle_params(self, output_stride=None): if output_stride is not None and output_stride % 2 != 0: raise Exception(""output stride must to be even number"") if output_stride is None: return else: stride = 2 for i, _cfg in enumerate(self.cfg): stride = stride * _cfg[-1] <mask>: s = 1 self.cfg[i][-1] = s",True,if stride > output_stride :,if stride > output_stride :,0.8820016898747209,100.00000000000004
"def do_query(data, q): ret = [] if not q: return ret qkey = q[0] for key, value in iterate(data): if len(q) == 1: if key == qkey: ret.append(value) <mask>: ret.extend(do_query(value, q)) else: if not is_iterable(value): continue if key == qkey: ret.extend(do_query(value, q[1:])) else: ret.extend(do_query(value, q)) return ret",False,elif is_iterable ( value ) :,elif key == qkey :,0.8820016898747209,6.916271812933183
"def make_shares(self, plaintext): share_arrays = [] for i, p in enumerate(plaintext): share_array = self.make_byte_shares(p) for sa in share_array: <mask>: share_arrays.append(array.array(""H"")) current_share_array = sa current_share_array.append(sa) return share_arrays",False,if i == 0 :,if i == len ( current_share_array ) :,0.8820016898747209,20.448007360218387
"def populate(self, item): # log.message('populate: %s', item) path = self.getItemPath(item) # log.message('populate: path=%s', path) value = self.getValue(path) for name in sorted(value.__dict__.keys()): <mask>: continue child = getattr(value, name, None) if hasattr(child, ""__dict__""): item.addChild(name, True) else: item.addChild(name, False)",False,"if name [ : 2 ] == ""__"" and name [ - 2 : ] == ""__"" :","if name == ""children"" :",0.8820016898747209,3.184506239916983
"def __repr__(self): try: if self._semlock._is_mine(): name = current_process().name <mask>: name += ""|"" + threading.current_thread().name elif self._semlock._get_value() == 1: name = ""None"" elif self._semlock._count() > 0: name = ""SomeOtherThread"" else: name = ""SomeOtherProcess"" except Exception: name = ""unknown"" return ""<Lock(owner=%s)>"" % name",False,"if threading . current_thread ( ) . name != ""MainThread"" :",if threading . current_thread ( ) :,0.8820016898747209,40.84937416616266
"def buffer(self, lines, scroll_end=True, scroll_if_editing=False): ""Add data to be displayed in the buffer."" self.values.extend(lines) if scroll_end: <mask>: self.start_display_at = len(self.values) - len(self._my_widgets) elif scroll_if_editing: self.start_display_at = len(self.values) - len(self._my_widgets)",False,if not self . editing :,if self . _my_widgets :,0.8820016898747209,13.134549472120788
"def warehouses(self) -> tuple: from ..repositories import WarehouseBaseRepo repos = dict() for dep in chain(self.dependencies, [self]): if dep.repo is None: continue <mask>: continue for repo in dep.repo.repos: if repo.from_config: continue repos[repo.name] = repo return tuple(repos.values())",False,"if not isinstance ( dep . repo , WarehouseBaseRepo ) :",if dep . repo . is_dir :,0.8820016898747209,16.14682615668325
"def _apply_flag_attrs(src_flag, dest_flag): # Use a baseline flag def to get default values for empty data. baseline_flag = FlagDef("""", {}, None) for name in dir(src_flag): <mask>: continue dest_val = getattr(dest_flag, name, None) baseline_val = getattr(baseline_flag, name, None) if dest_val == baseline_val: setattr(dest_flag, name, getattr(src_flag, name))",False,"if name [ : 1 ] == ""_"" :","if name . startswith ( ""_"" ) :",0.8820016898747209,16.830386789031852
"def out(parent, attr, indent=0): val = getattr(parent, attr) prefix = ""%s%s:"" % ("" "" * indent, attr.replace(""_"", ""-"")) if val is None: cli.out(prefix) else: <mask>: val = [flag_util.encode_flag_val(c.value) for c in val] cli.out(""%s %s"" % (prefix, flag_util.encode_flag_val(val)))",False,"if attr == ""choices"" :","if isinstance ( val , list ) :",0.8820016898747209,6.567274736060395
"def add_cand_to_check(cands): for cand in cands: x = cand.creator if x is None: continue <mask>: # `len(fan_out)` is in order to avoid comparing `x` heapq.heappush(cand_funcs, (-x.rank, len(fan_out), x)) fan_out[x] += 1",False,if x not in fan_out :,if x . rank > 0 :,0.8820016898747209,12.600736402830258
"def task_tree_lines(task=None): if task is None: task = current_root_task() rendered_children = [] nurseries = list(task.child_nurseries) while nurseries: nursery = nurseries.pop() nursery_children = _rendered_nursery_children(nursery) <mask>: nested = _render_subtree(""(nested nursery)"", rendered_children) nursery_children.append(nested) rendered_children = nursery_children return _render_subtree(task.name, rendered_children)",False,if rendered_children :,if nursery_children :,0.8820016898747209,42.72870063962342
"def lock_workspace(build_dir): _BUILDING_LOCK_FILE = "".blade.building.lock"" lock_file_fd, ret_code = lock_file(os.path.join(build_dir, _BUILDING_LOCK_FILE)) if lock_file_fd == -1: <mask>: console.fatal(""There is already an active building in current workspace."") else: console.fatal(""Lock exception, please try it later."") return lock_file_fd",False,if ret_code == errno . EAGAIN :,if ret_code == 0 :,0.8820016898747209,55.0695314903184
"def test_list(self): self._create_locations() response = self.client.get(self.geojson_boxedlocation_list_url) self.assertEqual(response.status_code, 200) self.assertEqual(len(response.data[""features""]), 2) for feature in response.data[""features""]: self.assertIn(""bbox"", feature) fid = feature[""id""] if fid == 1: self.assertEqual(feature[""bbox""], self.bl1.bbox_geometry.extent) <mask>: self.assertEqual(feature[""bbox""], self.bl2.bbox_geometry.extent) else: self.fail(""Unexpected id: {0}"".format(fid)) BoxedLocation.objects.all().delete()",True,elif fid == 2 :,elif fid == 2 :,0.8820016898747209,100.00000000000004
"def result(): # ""global"" does not work here... R, V = rays, virtual_rays if V is not None: if normalize: V = normalize_rays(V, lattice) if check: R = PointCollection(V, lattice) V = PointCollection(V, lattice) d = lattice.dimension() <mask>: raise ValueError( ""virtual rays must be linearly "" ""independent and with other rays span the ambient space."" ) return RationalPolyhedralFan(cones, R, lattice, is_complete, V)",False,if len ( V ) != d - R . dim ( ) or ( R + V ) . dim ( ) != d :,if d > 1 :,0.8820016898747209,0.14131406583082426
"def search_host(self, search_string): results = [] for host_entry in self.config_data: if host_entry.get(""type"") != ""entry"": continue if host_entry.get(""host"") == ""*"": continue searchable_information = host_entry.get(""host"") for key, value in six.iteritems(host_entry.get(""options"")): <mask>: value = "" "".join(value) if isinstance(value, int): value = str(value) searchable_information += "" "" + value if search_string in searchable_information: results.append(host_entry) return results",True,"if isinstance ( value , list ) :","if isinstance ( value , list ) :",0.8820016898747209,100.00000000000004
"def test_async_iterator(app): async with new_stream(app) as stream: for i in range(100): await stream.channel.deliver(message(key=i, value=i)) received = 0 async for value in stream: assert value == received received += 1 <mask>: break assert await channel_empty(stream.channel)",True,if received >= 100 :,if received >= 100 :,0.8820016898747209,100.00000000000004
"def has_google_credentials(): global _HAS_GOOGLE_CREDENTIALS if _HAS_GOOGLE_CREDENTIALS is None: provider = Provider(""google"") <mask>: _HAS_GOOGLE_CREDENTIALS = False else: _HAS_GOOGLE_CREDENTIALS = True return _HAS_GOOGLE_CREDENTIALS",False,if provider . get_access_key ( ) is None or provider . get_secret_key ( ) is None :,if provider . get_credentials ( ) is None :,0.8820016898747209,19.65194682191236
"def __cmp__(self, other): if isinstance(other, date) or isinstance(other, datetime): a = self._d.getTime() b = other._d.getTime() if a < b: return -1 <mask>: return 0 else: raise TypeError(""expected date or datetime object"") return 1",False,elif a == b :,elif a > b :,0.8820016898747209,24.736929544091932
"def validate_weight(self, weight): try: add_acl_to_obj(self.context[""user_acl""], self.category) except AttributeError: return weight # don't validate weight further if category failed if weight > self.category.acl.get(""can_pin_threads"", 0): <mask>: raise ValidationError( _( ""You don't have permission to pin threads globally "" ""in this category."" ) ) else: raise ValidationError( _(""You don't have permission to pin threads in this category."") ) return weight",False,if weight == 2 :,if weight < 0 :,0.8820016898747209,19.3576934939088
"def effective(line): for b in line: if not b.cond: return else: try: val = 5 <mask>: if b.ignore: b.ignore -= 1 else: return (b, True) except: return (b, False) return",False,if val :,if val < b . cond :,0.8820016898747209,14.535768424205482
"def wheelEvent(self, event): """"""Handle a wheel event."""""" if QtCore.Qt.ControlModifier & event.modifiers(): d = {""c"": self.leo_c} if isQt5: point = event.angleDelta() delta = point.y() or point.x() else: delta = event.delta() <mask>: zoom_out(d) else: zoom_in(d) event.accept() return QtWidgets.QTextBrowser.wheelEvent(self, event)",False,if delta < 0 :,if delta > 0.5 :,0.8820016898747209,23.643540225079384
"def test_evname_in_mp_events_testcases(): ok = True for evname in ins.mp_events: if evname == ""version"": continue for i, args in enumerate(ins.mp_events[evname][""test_cases""]): <mask>: msg = ""Error, for evname %s the testase #%d does not match evname"" print(msg % (evname, i)) ok = False if ok: print(""test_evname_in_mp_events_testcases: passed"")",False,if evname != args [ 0 ] :,"if args [ ""test_cases"" ] != evname :",0.8820016898747209,10.390302174233558
"def check_database(): if len(EmailAddress.objects.all()) > 0: print( ""Are you sure you want to wipe the existing development database and reseed it? (Y/N)"" ) <mask>: destroy_database() else: return False else: return True",False,"if raw_input ( ) . lower ( ) == ""y"" :",if not is_database_destroyed ( ) :,0.8820016898747209,5.791428261249877
"def _get_requested_databases(self): """"""Returns a list of databases requested, not including ignored dbs"""""" requested_databases = [] if (self._requested_namespaces is not None) and (self._requested_namespaces != []): for requested_namespace in self._requested_namespaces: if requested_namespace[0] is ""*"": return [] <mask>: requested_databases.append(requested_namespace[0]) return requested_databases",False,elif requested_namespace [ 0 ] not in IGNORE_DBS :,if requested_namespace [ 0 ] not in self . _ignored_dbs :,0.8820016898747209,46.825687910244035
"def decorated(self, *args, **kwargs): start_time = time.perf_counter() stderr = """" saved_exception = None try: yield from fn(self, *args, **kwargs) except GitSavvyError as e: stderr = e.stderr saved_exception = e finally: end_time = time.perf_counter() util.debug.log_git(args, None, ""<SNIP>"", stderr, end_time - start_time) <mask>: raise saved_exception from None",True,if saved_exception :,if saved_exception :,0.8820016898747209,100.00000000000004
"def is_suppressed_warning( type: str, subtype: str, suppress_warnings: List[str] ) -> bool: """"""Check the warning is suppressed or not."""""" if type is None: return False for warning_type in suppress_warnings: if ""."" in warning_type: target, subtarget = warning_type.split(""."", 1) else: target, subtarget = warning_type, None <mask>: if ( subtype is None or subtarget is None or subtarget == subtype or subtarget == ""*"" ): return True return False",True,if target == type :,if target == type :,0.8820016898747209,100.00000000000004
"def talk(self, words): if self.writeSentence(words) == 0: return r = [] while 1: i = self.readSentence() if len(i) == 0: continue reply = i[0] attrs = {} for w in i[1:]: j = w.find(""="", 1) if j == -1: attrs[w] = """" else: attrs[w[:j]] = w[j + 1 :] r.append((reply, attrs)) <mask>: return r",False,"if reply == ""!done"" :",if len ( r ) == 0 :,0.8820016898747209,11.339582221952005
"def encrypt(self, plaintext): encrypted = [] for p in _string_to_bytes(plaintext): <mask>: self._remaining_block = self._aes.encrypt(self._last_precipherblock) self._last_precipherblock = [] precipherbyte = self._remaining_block.pop(0) self._last_precipherblock.append(precipherbyte) cipherbyte = p ^ precipherbyte encrypted.append(cipherbyte) return _bytes_to_string(encrypted)",False,if len ( self . _remaining_block ) == 0 :,if len ( self . _remaining_block ) > self . _max_block :,0.8820016898747209,52.41705759002534
"def find_symbol(self, r, globally=False): query = self.view.substr(self.view.word(r)) fname = self.view.file_name().replace(""\\"", ""/"") locations = self.view.window().lookup_symbol_in_index(query) if not locations: return try: <mask>: location = [hit[2] for hit in locations if fname.endswith(hit[1])][0] return location[0] - 1, location[1] - 1 else: # TODO: There might be many symbols with the same name. return locations[0] except IndexError: return",False,if not globally :,if globally :,0.8820016898747209,0.0
"def __getslice__(self, i, j): try: <mask>: # handle the case where the right bound is unspecified j = len(self) if i < 0 or j < 0: raise dns.exception.FormError # If it's not an empty slice, access left and right bounds # to make sure they're valid if i != j: super(WireData, self).__getitem__(i) super(WireData, self).__getitem__(j - 1) return WireData(super(WireData, self).__getslice__(i, j)) except IndexError: raise dns.exception.FormError",False,if j == sys . maxint :,if j == 0 :,0.8820016898747209,38.49815007763549
"def main(): r = redis.StrictRedis() curr_memory = prev_memory = r.info()[""used_memory""] while True: <mask>: print( ""Delta Memory : %d, Total Memory : %d"" % ((curr_memory - prev_memory), curr_memory) ) time.sleep(1) prev_memory = curr_memory curr_memory = r.info()[""used_memory""]",False,if prev_memory != curr_memory :,if curr_memory > prev_memory :,0.8820016898747209,33.584386823726156
"def _visit(self, func): fname = func[0] if fname in self._flags: if self._flags[fname] == 1: logger.critical(""Fatal error! network ins not Dag."") import sys sys.exit(-1) else: return else: <mask>: self._flags[fname] = 1 for output in func[3]: for f in self._orig: for input in f[2]: if output == input: self._visit(f) self._flags[fname] = 2 self._sorted.insert(0, func)",False,if fname not in self . _flags :,if fname in self . _orig :,0.8820016898747209,37.70794596593207
"def urls(self, version=None): """"""Returns all URLS that are mapped to this interface"""""" urls = [] for _base_url, routes in self.api.http.routes.items(): for url, methods in routes.items(): for _method, versions in methods.items(): for interface_version, interface in versions.items(): <mask>: if not url in urls: urls.append( (""/v{0}"".format(version) if version else """") + url ) return urls",False,if interface_version == version and interface == self :,if interface_version == interface_version :,0.8820016898747209,42.38405077952113
"def _handle_data(self, text): if self._translate: <mask>: self._data.append(text) else: self._translate = False self._data = [] self._comments = []",False,"if not text . startswith ( ""gtk-"" ) :",if text :,0.8820016898747209,0.0
"def set_dir_modes(self, dirname, mode): if not self.is_chmod_supported(): return for dirpath, dirnames, fnames in os.walk(dirname): <mask>: continue log.info(""changing mode of %s to %o"", dirpath, mode) if not self.dry_run: os.chmod(dirpath, mode)",False,if os . path . islink ( dirpath ) :,if not self . is_dir_supported ( dirpath ) :,0.8820016898747209,21.401603033752977
"def language(self): if self.lang_data: lang_data = [s if s != ""None"" else None for s in self.lang_data] <mask>: return Language(lang_data[0], country=lang_data[1], script=lang_data[2])",False,if lang_data [ 0 ] :,if lang_data :,0.8820016898747209,38.80684294761701
"def _addItemToLayout(self, sample, label): col = self.layout.columnCount() row = self.layout.rowCount() if row: row -= 1 nCol = self.columnCount * 2 # FIRST ROW FULL if col == nCol: for col in range(0, nCol, 2): # FIND RIGHT COLUMN <mask>: break if col + 2 == nCol: # MAKE NEW ROW col = 0 row += 1 self.layout.addItem(sample, row, col) self.layout.addItem(label, row, col + 1)",False,"if not self . layout . itemAt ( row , col ) :",if sample [ col ] == label :,0.8820016898747209,4.023185929567685
"def align_comments(tlist): tidx, token = tlist.token_next_by(i=sql.Comment) while token: pidx, prev_ = tlist.token_prev(tidx) <mask>: tlist.group_tokens(sql.TokenList, pidx, tidx, extend=True) tidx = pidx tidx, token = tlist.token_next_by(i=sql.Comment, idx=tidx)",False,"if isinstance ( prev_ , sql . TokenList ) :",if prev_ is not sql . Comment :,0.8820016898747209,11.949988385687533
"def hook_GetVariable(ql, address, params): if params[""VariableName""] in ql.env: var = ql.env[params[""VariableName""]] read_len = read_int64(ql, params[""DataSize""]) if params[""Attributes""] != 0: write_int64(ql, params[""Attributes""], 0) write_int64(ql, params[""DataSize""], len(var)) <mask>: return EFI_BUFFER_TOO_SMALL if params[""Data""] != 0: ql.mem.write(params[""Data""], var) return EFI_SUCCESS return EFI_NOT_FOUND",False,if read_len < len ( var ) :,if read_len > 0 :,0.8820016898747209,28.319415510892387
"def _PromptMySQL(self, config): """"""Prompts the MySQL configuration, retrying if the configuration is invalid."""""" while True: self._PromptMySQLOnce(config) if self._CheckMySQLConnection(): print(""Successfully connected to MySQL with the given configuration."") return else: print(""Error: Could not connect to MySQL with the given configuration."") retry = RetryBoolQuestion(""Do you want to retry MySQL configuration?"", True) <mask>: raise ConfigInitError()",False,if not retry :,if retry :,0.8820016898747209,0.0
"def split_long_line_with_indent(line, max_per_line, indent): """"""Split the `line` so that it doesn't go over `max_per_line` and adds `indent` to new lines."""""" words = line.split("" "") lines = [] current_line = words[0] for word in words[1:]: <mask>: lines.append(current_line) current_line = "" "" * indent + word else: current_line = f""{current_line} {word}"" lines.append(current_line) return ""\n"".join(lines)",False,"if len ( f""{current_line} {word}"" ) > max_per_line :",if max_per_line > indent :,0.8820016898747209,11.840379131985358
"def gen_cli(docs_dir): with open(os.path.join(docs_dir, ""CLI_template.md""), ""r"") as cli_temp_file: temp_lines = cli_temp_file.readlines() lines = [] for line in temp_lines: matched = re.match(r""{onnx-tf.*}"", line) <mask>: command = matched.string.strip()[1:-1] output = subprocess.check_output(command.split("" "")).decode(""UTF-8"") lines.append(output) else: lines.append(line) with open(os.path.join(docs_dir, ""CLI.md""), ""w"") as cli_file: cli_file.writelines(lines)",True,if matched :,if matched :,0.8820016898747209,0.0
"def read(self, size=None): if size == 0: return """" data = list() while size is None or size > 0: line = self.readline(size or -1) if not line: break <mask>: size -= len(line) data.append(line) return """".join(data)",False,if size is not None :,if len ( line ) > 0 :,0.8820016898747209,6.567274736060395
"def _get_format_and_pattern(file_path): file_path = Path(file_path) with file_path.open() as f: first_line = f.readline().strip() match = re.match(r""format *: *(.+)"", first_line) <mask>: return ""gztar"", first_line, 1 return match.group(1), f.readline().strip(), 2",True,if match is None :,if match is None :,0.8820016898747209,100.00000000000004
"def remove_old_snapshot(install_dir): logging.info(""Removing any old files in {}"".format(install_dir)) for file in glob.glob(""{}/*"".format(install_dir)): try: if os.path.isfile(file): os.unlink(file) <mask>: shutil.rmtree(file) except Exception as error: logging.error(""Error: {}"".format(error)) sys.exit(1)",True,elif os . path . isdir ( file ) :,elif os . path . isdir ( file ) :,0.8820016898747209,100.00000000000004
"def _test_forever(self, tests): while True: for test_name in tests: yield test_name if self.bad: return <mask>: return",False,if self . ns . fail_env_changed and self . environment_changed :,if self . bad_test :,0.8820016898747209,6.656592803413299
"def _swig_extract_dependency_files(self, src): dep = [] for line in open(src): if line.startswith(""#include"") or line.startswith(""%include""): line = line.split("" "")[1].strip(""""""'""\r\n"""""") <mask>: dep.append(line) return [i for i in dep if os.path.exists(i)]",False,"if not ( ""<"" in line or line in dep ) :",if line :,0.8820016898747209,0.0
"def update_service_key(kid, name=None, metadata=None): try: with db_transaction(): key = db_for_update(ServiceKey.select().where(ServiceKey.kid == kid)).get() <mask>: key.name = name if metadata is not None: key.metadata.update(metadata) key.save() except ServiceKey.DoesNotExist: raise ServiceKeyDoesNotExist",True,if name is not None :,if name is not None :,0.8820016898747209,100.00000000000004
"def range(self, dimension, data_range=True, dimension_range=True): if self.nodes and dimension in self.nodes.dimensions(): node_range = self.nodes.range(dimension, data_range, dimension_range) <mask>: path_range = self._edgepaths.range(dimension, data_range, dimension_range) return max_range([node_range, path_range]) return node_range return super(Graph, self).range(dimension, data_range, dimension_range)",False,if self . _edgepaths :,if self . _edgepaths and dimension in self . _edgepaths . dimensions ( ) :,0.8820016898747209,22.407508680204355
"def handler(chan, host, port): sock = socket() try: sock.connect((host, port)) except Exception as e: if verbose == True: print(e) return while True: r, w, x = select.select([sock, chan], [], []) if sock in r: data = sock.recv(1024) <mask>: break chan.send(data) if chan in r: data = chan.recv(1024) if len(data) == 0: break sock.send(data) chan.close() sock.close()",True,if len ( data ) == 0 :,if len ( data ) == 0 :,0.8820016898747209,100.00000000000004
"def output_layer(self, features, **kwargs): """"""Project features to the vocabulary size."""""" if self.adaptive_softmax is None: # project back to size of vocabulary <mask>: return F.linear(features, self.embed_tokens.weight) else: return F.linear(features, self.embed_out) else: return features",False,if self . share_input_output_embed :,if self . embed_tokens :,0.8820016898747209,16.417223692914014
"def generate(self, dest, vars): util.ensure_dir(dest) for relpath, src, template in self._file_templates: file_dest = os.path.join(dest, relpath) util.ensure_dir(os.path.dirname(file_dest)) <mask>: shutil.copyfile(src, file_dest) else: _render_template(template, vars, file_dest)",False,if template is None :,if os . path . isfile ( src ) :,0.8820016898747209,4.990049701936832
"def _py_matching_callback(self, context, result, sender, device): d = HIDDevice.get_device(c_void_p(device)) if d not in self.devices: self.devices.add(d) for x in self.matching_observers: <mask>: x.device_discovered(d)",False,"if hasattr ( x , ""device_discovered"" ) :",if x . device_discovered ( d ) :,0.8820016898747209,18.0854275844538
"def urlquote(*args, **kwargs): new_kwargs = dict(kwargs) if not PY3: new_kwargs = dict(kwargs) <mask>: del new_kwargs[""encoding""] if ""errors"" in kwargs: del new_kwargs[""errors""] return quote(*args, **new_kwargs)",False,"if ""encoding"" in new_kwargs :","if ""encoding"" in kwargs :",0.8820016898747209,53.137468984124546
"def Set(self, attr, value): hook = getattr(self, ""_set_%s"" % attr, None) if hook: # If there is a set hook we must use the context manager. <mask>: raise ValueError( ""Can only update attribute %s using the context manager."" % attr ) if attr not in self._pending_hooks: self._pending_hooks.append(attr) self._pending_parameters[attr] = value else: super(Configuration, self).Set(attr, value)",False,if self . _lock > 0 :,"if not hasattr ( self , ""_contextmanager"" ) :",0.8820016898747209,4.789232204309912
"def on_profiles_loaded(self, profiles): cb = self.builder.get_object(""cbProfile"") model = cb.get_model() model.clear() for f in profiles: name = f.get_basename() if name.endswith("".mod""): continue <mask>: name = name[0:-11] model.append((name, f, None)) cb.set_active(0)",False,"if name . endswith ( "".sccprofile"" ) :","if name [ - 11 : ] == "".py"" :",0.8820016898747209,8.889175589171739
"def get_eval_task(self, worker_id): """"""Return next evaluation (task_id, Task) tuple"""""" with self._lock: <mask>: return -1, None self._task_id += 1 task = self._eval_todo.pop() self._doing[self._task_id] = (worker_id, task, time.time()) return self._task_id, task",False,if not self . _eval_todo :,if self . _task_id >= self . _max_eval_tasks :,0.8820016898747209,13.973536728747975
"def queries(self): if DEV: cmd = ShellCommand(""docker"", ""ps"", ""-qf"", ""name=%s"" % self.path.k8s) <mask>: if not cmd.stdout.strip(): log_cmd = ShellCommand( ""docker"", ""logs"", self.path.k8s, stderr=subprocess.STDOUT ) if log_cmd.check(f""docker logs for {self.path.k8s}""): print(cmd.stdout) pytest.exit(f""container failed to start for {self.path.k8s}"") return ()",False,"if not cmd . check ( f""docker check for {self.path.k8s}"" ) :",if cmd . check ( ) :,0.8820016898747209,6.87682893933032
"def disjoined(data): # create marginalized distributions and multiple them together data_disjoined = None dim = len(data.shape) for d in range(dim): axes = list(range(dim)) axes.remove(d) data1d = multisum(data, axes) shape = [1 for k in range(dim)] shape[d] = len(data1d) data1d = data1d.reshape(tuple(shape)) <mask>: data_disjoined = data1d else: data_disjoined = data_disjoined * data1d return data_disjoined",False,if d == 0 :,if data_disjoined is None :,0.8820016898747209,7.809849842300637
"def safe_repr(val): try: <mask>: # We special case dicts to have a sorted repr. This makes testing # significantly easier val = _obj_with_safe_repr(val) ret = repr(val) if six.PY2: ret = ret.decode(""utf-8"") except UnicodeEncodeError: ret = red(""a %r that cannot be represented"" % type(val)) else: ret = green(ret) return ret",True,"if isinstance ( val , dict ) :","if isinstance ( val , dict ) :",0.8820016898747209,100.00000000000004
"def wrapper(*args, **kwargs): resp = view_func(*args, **kwargs) if isinstance(resp, dict): ctx_params = request.environ.get(""webrec.template_params"") <mask>: resp.update(ctx_params) template = self.jinja_env.jinja_env.get_or_select_template(template_name) return template.render(**resp) else: return resp",True,if ctx_params :,if ctx_params :,0.8820016898747209,100.00000000000004
"def post(self, request, *args, **kwargs): contact_id = kwargs.get(""pk"") self.object = get_object_or_404(Contact, id=contact_id) if ( self.request.user.role != ""ADMIN"" and not self.request.user.is_superuser and self.request.user != self.object.created_by ) or self.object.company != self.request.company: raise PermissionDenied else: if self.object.address_id: self.object.address.delete() self.object.delete() <mask>: return JsonResponse({""error"": False}) return redirect(""contacts:list"")",False,if self . request . is_ajax ( ) :,if not self . object . address :,0.8820016898747209,9.545138913210204
"def escape(text, newline=False): """"""Escape special html characters."""""" if isinstance(text, str): if ""&"" in text: text = text.replace(""&"", ""&amp;"") if "">"" in text: text = text.replace("">"", ""&gt;"") <mask>: text = text.replace(""<"", ""&lt;"") if '""' in text: text = text.replace('""', ""&quot;"") if ""'"" in text: text = text.replace(""'"", ""&quot;"") if newline: if ""\n"" in text: text = text.replace(""\n"", ""<br>"") return text",True,"if ""<"" in text :","if ""<"" in text :",0.8820016898747209,100.00000000000004
"def everythingIsUnicode(d): """"""Takes a dictionary, recursively verifies that every value is unicode"""""" for k, v in d.iteritems(): if isinstance(v, dict) and k != ""headers"": <mask>: return False elif isinstance(v, list): for i in v: if isinstance(i, dict) and not everythingIsUnicode(i): return False elif isinstance(i, _bytes): return False elif isinstance(v, _bytes): return False return True",True,if not everythingIsUnicode ( v ) :,if not everythingIsUnicode ( v ) :,0.8820016898747209,100.00000000000004
"def fill(self): try: while ( not self.stopping.wait(self.sample_wait) and len(self.queue) < self.queue.maxlen ): self.queue.append(self.parent._read()) <mask>: self.parent._fire_events() self.full.set() while not self.stopping.wait(self.sample_wait): self.queue.append(self.parent._read()) if isinstance(self.parent, EventsMixin): self.parent._fire_events() except ReferenceError: # Parent is dead; time to die! pass",False,"if self . partial and isinstance ( self . parent , EventsMixin ) :","if isinstance ( self . parent , EventsMixin ) :",0.8820016898747209,60.57025366576469
"def _SetListviewTextItems(self, items): self.listview.DeleteAllItems() index = -1 for item in items: index = self.listview.InsertItem(index + 1, item[0]) data = item[1] <mask>: data = """" self.listview.SetItemText(index, 1, data)",False,if data is None :,"if data == """" :",0.8820016898747209,14.535768424205482
"def process_request(self, request): for old, new in self.names_name: request.uri = request.uri.replace(old, new) if is_text_payload(request) and request.body: body = six.ensure_str(request.body) <mask>: request.body = body.replace(old, new) return request",False,if old in body :,if body :,0.8820016898747209,0.0
"def serialize(cls, value, *args, **kwargs): if value is None: return """" value_as_string = six.text_type(value) if SHOULD_NOT_USE_LOCALE: return value_as_string else: grouping = kwargs.get(""grouping"", None) has_decimal_places = value_as_string.find(""."") != -1 <mask>: string_format = ""%d"" else: decimal_places = len(value_as_string.split(""."")[1]) string_format = ""%.{}f"".format(decimal_places) return locale.format(string_format, value, grouping=grouping)",False,if not has_decimal_places :,if has_decimal_places :,0.8820016898747209,72.89545183625967
"def review_link(request, path_obj): try: <mask>: if check_permission(""translate"", request): text = _(""Review Suggestions"") else: text = _(""View Suggestions"") return { ""href"": dispatch.translate( request, path_obj.pootle_path, matchnames=[""hassuggestion""] ), ""text"": text, } except IOError: pass",False,if path_obj . has_suggestions ( ) :,if path_obj . pootle_path :,0.8820016898747209,38.875142041440206
"def _migrate_key(self, key): """"""migrate key from old .dat file"""""" key_path = os.path.join(self.home_path, ""keys.dat"") if os.path.exists(key_path): try: key_data = json.loads(open(key_path, ""rb"").read()) <mask>: self.add_key(key, key_data.get(key)) except: self.error(f""Corrupt key file. Manual migration of '{key}' required."")",False,if key_data . get ( key ) :,if key_data :,0.8820016898747209,26.013004751144457
"def gather_callback_args(self, obj, callbacks): session = sa.orm.object_session(obj) for callback in callbacks: backref = callback.backref root_objs = getdotattr(obj, backref) if backref else obj if root_objs: <mask>: root_objs = [root_objs] with session.no_autoflush: for root_obj in root_objs: if root_obj: args = self.get_callback_args(root_obj, callback) if args: yield args",False,"if not isinstance ( root_objs , Iterable ) :","if not isinstance ( root_objs , list ) :",0.8820016898747209,74.19446627365011
"def GetDefFile(self, gyp_to_build_path): """"""Returns the .def file from sources, if any. Otherwise returns None."""""" spec = self.spec if spec[""type""] in (""shared_library"", ""loadable_module"", ""executable""): def_files = [s for s in spec.get(""sources"", []) if s.endswith("".def"")] <mask>: return gyp_to_build_path(def_files[0]) elif len(def_files) > 1: raise Exception(""Multiple .def files"") return None",True,if len ( def_files ) == 1 :,if len ( def_files ) == 1 :,0.8820016898747209,100.00000000000004
"def _validate_gallery(images): for image in images: image_path = image.get(""image_path"", """") if image_path: if not isfile(image_path): raise TypeError(f""{image_path!r} is not a valid image path."") else: raise TypeError(""'image_path' is required."") <mask>: raise TypeError(""Caption must be 180 characters or less."")",False,"if not len ( image . get ( ""caption"" , """" ) ) <= 180 :","if len ( image . get ( ""caption"" ) ) < 180 :",0.8820016898747209,54.96072627753009
"def VType(self): if ""DW_AT_type"" in self.attributes: target = self.types[self.type_id] target_type = target.VType() <mask>: target_type = [target_type, None] return [""Pointer"", dict(target=target_type[0], target_args=target_type[1])] return [""Pointer"", dict(target=""Void"")]",False,"if not isinstance ( target_type , list ) :","if isinstance ( target_type , tuple ) :",0.8820016898747209,54.182204258059556
"def addInPlace(self, value1, value2): for group in value2: for key in value2[group]: <mask>: value1[group][key] = value2[group][key] else: value1[group][key] += value2[group][key] return value1",False,if key not in value1 [ group ] :,if key not in value1 :,0.8820016898747209,48.23560797692261
"def _mongo_query_and(self, queries): if len(queries) == 1: return queries[0] query = {} for q in queries: for k, v in q.items(): <mask>: query[k] = {} if isinstance(v, list): # TODO check exists of k in query, may be it should be update query[k] = v else: query[k].update(v) return query",True,if k not in query :,if k not in query :,0.8820016898747209,100.00000000000004
"def _handled_eventtype(self, eventtype, handler): if eventtype not in known_events: log.error('The event ""%s"" is not known', eventtype) return False if known_events[eventtype].__module__.startswith(""deluge.event""): <mask>: return True log.error( ""You cannot register custom notification providers "" ""for built-in event types."" ) return False return True",False,if handler . __self__ is self :,"if handler . __module__ == ""deluge.event"" :",0.8820016898747209,27.22589423069701
"def get_ax_arg(uri): if not ax_ns: return u"""" prefix = ""openid."" + ax_ns + "".type."" ax_name = None for name, values in self.request.arguments.iteritems(): <mask>: part = name[len(prefix) :] ax_name = ""openid."" + ax_ns + "".value."" + part break if not ax_name: return u"""" return self.get_argument(ax_name, u"""")",False,if values [ - 1 ] == uri and name . startswith ( prefix ) :,if name . startswith ( prefix ) :,0.8820016898747209,28.22664073782293
"def handle_starttag(self, tag, attrs): if tag == ""base"": self.base_url = dict(attrs).get(""href"") if self.scan_tag(tag): for attr, value in attrs: <mask>: if self.strip: value = strip_html5_whitespace(value) url = self.process_attr(value) link = Link(url=url) self.links.append(link) self.current_link = link",False,if self . scan_attr ( attr ) :,"if attr == ""href"" :",0.8820016898747209,5.660233915657916
"def test_long_steadystate_queue_popright(self): for size in (0, 1, 2, 100, 1000): d = deque(reversed(range(size))) append, pop = d.appendleft, d.pop for i in range(size, BIG): append(i) x = pop() <mask>: self.assertEqual(x, i - size) self.assertEqual(list(reversed(list(d))), list(range(BIG - size, BIG)))",False,if x != i - size :,if x is not None :,0.8820016898747209,12.872632311973014
"def _update_read(self): """"""Update state when there is read event"""""" try: msg = bytes(self._sock.recv(4096)) <mask>: self.on_message(msg) return True # normal close, remote is closed self.close() except socket.error as err: if err.args[0] in (errno.EAGAIN, errno.EWOULDBLOCK): pass else: self.on_error(err) return False",True,if msg :,if msg :,0.8820016898747209,0.0
"def prepend(self, value): """"""prepend value to nodes"""""" root, root_text = self._get_root(value) for i, tag in enumerate(self): if not tag.text: tag.text = """" <mask>: root[-1].tail = tag.text tag.text = root_text else: tag.text = root_text + tag.text if i > 0: root = deepcopy(list(root)) tag[:0] = root root = tag[: len(root)] return self",False,if len ( root ) > 0 :,if i == 0 :,0.8820016898747209,12.872632311973014
"def cmp(self, other): v_is_ptr = not isinstance(self, CTypesGenericPrimitive) w_is_ptr = isinstance(other, CTypesData) and not isinstance( other, CTypesGenericPrimitive ) if v_is_ptr and w_is_ptr: return cmpfunc(self._convert_to_address(None), other._convert_to_address(None)) elif v_is_ptr or w_is_ptr: return NotImplemented else: if isinstance(self, CTypesGenericPrimitive): self = self._value <mask>: other = other._value return cmpfunc(self, other)",False,"if isinstance ( other , CTypesGenericPrimitive ) :","elif isinstance ( other , CTypesGenericPrimitive ) :",0.8820016898747209,84.08964152537145
"def get_external_addresses(self, label=None) -> List[str]: result = [] for c in self._conf[""pools""].values(): <mask>: if label == c[""label""]: result.append(c[""external_address""][0]) else: result.append(c[""external_address""][0]) return result",False,if label is not None :,"if c [ ""external_address"" ] :",0.8820016898747209,4.990049701936832
"def coerce_text(v): if not isinstance(v, basestring_): <mask>: attr = ""__unicode__"" else: attr = ""__str__"" if hasattr(v, attr): return unicode(v) else: return bytes(v) return v",False,if sys . version_info [ 0 ] < 3 :,"if isinstance ( v , basestring_ ) :",0.8820016898747209,4.4959869933858485
"def check_localhost(self): """"""Warn if any socket_host is 'localhost'. See #711."""""" for k, v in cherrypy.config.items(): <mask>: warnings.warn( ""The use of 'localhost' as a socket host can "" ""cause problems on newer systems, since "" ""'localhost' can map to either an IPv4 or an "" ""IPv6 address. You should use '127.0.0.1' "" ""or '[::1]' instead."" )",False,"if k == ""server.socket_host"" and v == ""localhost"" :","if k == ""socket_host"" :",0.8820016898747209,28.400281431154657
"def add_songs(self, filenames, library): changed = [] for i in range(len(self)): <mask>: song = library[self._list[i]] self._list[i] = song changed.append(song) if changed: self._emit_changed(changed, msg=""add"") return bool(changed)",False,"if isinstance ( self [ i ] , str ) and self . _list [ i ] in filenames :",if self . _list [ i ] in filenames :,0.8820016898747209,36.85202394149233
"def _expand_deps_java_generation(self): """"""Ensure that all multilingual dependencies such as proto_library generate java code."""""" queue = collections.deque(self.deps) keys = set() while queue: k = queue.popleft() if k not in keys: keys.add(k) dep = self.target_database[k] <mask>: # Has this attribute dep.attr[""generate_java""] = True queue.extend(dep.deps)",False,"if ""generate_java"" in dep . attr :",if dep . attr :,0.8820016898747209,21.297646969725616
"def get(self): name = request.args.get(""filename"") if name is not None: opts = dict() opts[""type""] = ""episode"" result = guessit(name, options=opts) res = dict() <mask>: res[""episode""] = result[""episode""] else: res[""episode""] = 0 if ""season"" in result: res[""season""] = result[""season""] else: res[""season""] = 0 if ""subtitle_language"" in result: res[""subtitle_language""] = str(result[""subtitle_language""]) return jsonify(data=res) else: return """", 400",True,"if ""episode"" in result :","if ""episode"" in result :",0.8820016898747209,100.00000000000004
def _get_error_file(self) -> Optional[str]: error_file = None min_timestamp = sys.maxsize for replicas in self.role_replicas.values(): for replica in replicas: <mask>: continue mtime = os.path.getmtime(replica.error_file) if mtime < min_timestamp: min_timestamp = mtime error_file = replica.error_file return error_file,False,if not os . path . exists ( replica . error_file ) :,if replica . error_file is None :,0.8820016898747209,24.925978674400294
"def findChapterNameForPosition(self, p): """"""Return the name of a chapter containing p or None if p does not exist."""""" cc, c = self, self.c if not p or not c.positionExists(p): return None for name in cc.chaptersDict: <mask>: theChapter = cc.chaptersDict.get(name) if theChapter.positionIsInChapter(p): return name return ""main""",False,"if name != ""main"" :",if name . startswith ( p ) :,0.8820016898747209,12.22307556087252
"def remove_files(folder, file_extensions): for f in os.listdir(folder): f_path = os.path.join(folder, f) if os.path.isfile(f_path): extension = os.path.splitext(f_path)[1] <mask>: os.remove(f_path)",True,if extension in file_extensions :,if extension in file_extensions :,0.8820016898747209,100.00000000000004
"def execute_uncomment(self, event): cursor = self._editor.GetCurrentPos() line, pos = self._editor.GetCurLine() spaces = "" "" * self._tab_size comment = ""Comment"" + spaces cpos = cursor - len(comment) lenline = len(line) if lenline > 0: idx = 0 while idx < lenline and line[idx] == "" "": idx += 1 <mask>: self._editor.DeleteRange(cursor - pos + idx, len(comment)) self._editor.SetCurrentPos(cpos) self._editor.SetSelection(cpos, cpos) self.store_position()",False,if ( line [ idx : len ( comment ) + idx ] ) . lower ( ) == comment . lower ( ) :,if idx < lenline :,0.8820016898747209,0.21081581353052783
"def test_batch_kwarg_path_relative_dot_slash_is_modified_and_found_in_a_code_cell( critical_suite_with_citations, empty_data_context ): obs = SuiteEditNotebookRenderer.from_data_context(empty_data_context).render( critical_suite_with_citations, {""path"": ""./foo/data""} ) assert isinstance(obs, dict) found_expected = False for cell in obs[""cells""]: <mask>: source_code = cell[""source""] if 'batch_kwargs = {""path"": ""../.././foo/data""}' in source_code: found_expected = True break assert found_expected",False,"if cell [ ""cell_type"" ] == ""code"" :","if cell [ ""type"" ] == ""code"" :",0.8820016898747209,72.9836014355472
"def _get_file(self): if self._file is None: self._file = SpooledTemporaryFile( max_size=self._storage.max_memory_size, suffix="".S3Boto3StorageFile"", dir=setting(""FILE_UPLOAD_TEMP_DIR""), ) if ""r"" in self._mode: self._is_dirty = False self.obj.download_fileobj(self._file) self._file.seek(0) <mask>: self._file = GzipFile(mode=self._mode, fileobj=self._file, mtime=0.0) return self._file",False,"if self . _storage . gzip and self . obj . content_encoding == ""gzip"" :","if ""z"" in self . _mode :",0.8820016898747209,6.42603291593205
"def _parse_filters(f_strs): filters = [] if not f_strs: return filters for f_str in f_strs: <mask>: fname, fopts = f_str.split("":"", 1) filters.append((fname, _parse_options([fopts]))) else: filters.append((f_str, {})) return filters",True,"if "":"" in f_str :","if "":"" in f_str :",0.8820016898747209,100.00000000000004
"def update_completion(self): """"""Update completion model with exist tags"""""" orig_text = self.widget.text() text = "", "".join(orig_text.replace("", "", "","").split("","")[:-1]) tags = [] for tag in self.tags_list: <mask>: if orig_text[-1] not in ("","", "" ""): tags.append(""%s,%s"" % (text, tag)) tags.append(""%s, %s"" % (text, tag)) else: tags.append(tag) if tags != self.completer_model.stringList(): self.completer_model.setStringList(tags)",False,"if "","" in orig_text :",if tag . startswith ( orig_text ) :,0.8820016898747209,17.747405280050266
"def _get_startup_packages(lib_path: Path, packages) -> Set[str]: names = set() for path in lib_path.iterdir(): name = path.name if name == ""__pycache__"": continue <mask>: names.add(name.split(""."")[0]) elif path.is_dir() and ""."" not in name: names.add(name) if packages: packages = {package.lower().replace(""-"", ""_"") for package in packages} if len(names & packages) == len(packages): return packages return names",False,"if name . endswith ( "".py"" ) :","if path . is_file ( ) and ""."" in name :",0.8820016898747209,7.655122720591221
"def get_cloud_credential(self): """"""Return the credential which is directly tied to the inventory source type."""""" credential = None for cred in self.credentials.all(): <mask>: if cred.kind == self.source.replace(""ec2"", ""aws""): credential = cred break else: # these need to be returned in the API credential field if cred.credential_type.kind != ""vault"": credential = cred break return credential",False,if self . source in CLOUD_PROVIDERS :,"if cred . credential_type . kind == ""inventory"" :",0.8820016898747209,4.016138436407654
"def newickize(clade): """"""Convert a node tree to a Newick tree string, recursively."""""" label = clade.name or """" if label: unquoted_label = re.match(token_dict[""unquoted node label""], label) <mask>: label = ""'%s'"" % label.replace(""\\"", ""\\\\"").replace(""'"", ""\\'"") if clade.is_terminal(): # terminal return label + make_info_string(clade, terminal=True) else: subtrees = (newickize(sub) for sub in clade) return ""(%s)%s"" % ("","".join(subtrees), label + make_info_string(clade))",False,if ( not unquoted_label ) or ( unquoted_label . end ( ) < len ( label ) ) :,if unquoted_label :,0.8820016898747209,1.234488517472643
"def __iter__(self): for name, value in self._vars.store.data.items(): source = self._sources[name] prefix = self._get_prefix(value) name = u""{0}{{{1}}}"".format(prefix, name) <mask>: yield ArgumentInfo(name, value) else: yield VariableInfo(name, value, source)",False,if source == self . ARGUMENT_SOURCE :,if source is None :,0.8820016898747209,8.697972365316721
"def filepath_enumerate(paths): """"""Enumerate the file paths of all subfiles of the list of paths"""""" out = [] for path in paths: <mask>: out.append(path) else: for root, dirs, files in os.walk(path): for name in files: out.append(os.path.normpath(os.path.join(root, name))) return out",False,if os . path . isfile ( path ) :,if os . path . isdir ( path ) :,0.8820016898747209,65.80370064762461
"def del_(self, key): hash_ = self.hash(key) node_ = self._table[hash_] pre_node = None while node_ is not None: <mask>: if pre_node is None: self._table[hash_] = node_.next else: pre_node.next = node_.next self._len -= 1 pre_node = node_ node_ = node_.next",True,if node_ . key == key :,if node_ . key == key :,0.8820016898747209,100.00000000000004
"def _recurse(self, base_path, rel_source, rel_zip): submodules_path = Path(base_path) / ""submodules"" if not submodules_path.is_dir(): return for submodule in submodules_path.iterdir(): source_path = submodule / rel_source <mask>: continue output_path = submodule / rel_zip self._build_lambdas(source_path, output_path) self._recurse(submodule, rel_source, rel_zip)",False,if not source_path . is_dir ( ) :,if not source_path . is_file ( ) :,0.8820016898747209,73.48889200874659
"def find_test_functions(collections): if not isinstance(collections, list): collections = [collections] functions = [] for collection in collections: if not isinstance(collection, dict): collection = vars(collection) keys = collection.keys() keys.sort() for key in keys: value = collection[key] <mask>: functions.append(value) return functions",False,"if isinstance ( value , types . FunctionType ) and hasattr ( value , ""unittest"" ) :","if isinstance ( value , Function ) :",0.8820016898747209,15.033932314270844
"def __init__( self, classifier, layer_name=None, transpose=None, distance=None, copy_weights=True, ): super().__init__() self.copy_weights = copy_weights ### set layer weights ### if layer_name is not None: self.set_weights(getattr(classifier, layer_name)) else: for x in self.possible_layer_names: layer = getattr(classifier, x, None) <mask>: self.set_weights(layer) break ### set distance measure ### self.distance = classifier.distance if distance is None else distance self.transpose = transpose",True,if layer is not None :,if layer is not None :,0.8820016898747209,100.00000000000004
def multi_dev_generator(self): for data in self._data_loader(): if len(self._tail_data) < self._base_number: self._tail_data += data <mask>: yield self._tail_data self._tail_data = [],False,if len ( self . _tail_data ) == self . _base_number :,if len ( self . _tail_data ) > self . _base_number :,0.8820016898747209,79.4069318995806
"def Resolve(self, updater=None): if len(self.Conflicts): for setting, edge in self.Conflicts: answer = self.AskUser(self.Setting, setting) if answer == Gtk.ResponseType.YES: value = setting.Value.split(""|"") value.remove(edge) setting.Value = ""|"".join(value) if updater: updater.UpdateSetting(setting) <mask>: return False return True",False,if answer == Gtk . ResponseType . NO :,if answer != Gtk . ResponseType . NO :,0.8820016898747209,70.71067811865478
"def _post_process_ttl(zone): for name in zone: for record_type in zone[name]: records = zone[name][record_type] if isinstance(records, list): ttl = min([x[""ttl""] for x in records]) for record in records: <mask>: logger.warning( ""Using lowest TTL {} for the record set. Ignoring value {}"".format( ttl, record[""ttl""] ) ) record[""ttl""] = ttl",False,"if record [ ""ttl"" ] != ttl :","if ttl < record [ ""ttl"" ] :",0.8820016898747209,51.7679965241078
"def __init__(self, cmds, env, cleanup=[]): self.handle = None self.cmds = cmds self.env = env if cleanup: <mask>: cleanup = [cleanup] else: try: cleanup = [c for c in cleanup if callable(c)] except: cleanup = [] self.cleanup = cleanup",False,if callable ( cleanup ) :,"if isinstance ( cleanup , list ) :",0.8820016898747209,16.515821590069027
"def _parse_data_of_birth(cls, data_of_birth_string): if data_of_birth_string: format = ""%m/%d/%Y"" try: parsed_date = datetime.datetime.strptime(data_of_birth_string, format) return parsed_date except ValueError: # Facebook sometimes provides a partial date format # ie 04/07 (ignore those) <mask>: raise",False,"if data_of_birth_string . count ( ""/"" ) != 1 :","if not datetime . datetime . strptime ( data_of_birth_string , ""%Y-%m-%d"" ) :",0.8820016898747209,27.18635034387555
"def process_lib(vars_, coreval): for d in vars_: var = d.upper() if var == ""QTCORE"": continue value = env[""LIBPATH_"" + var] if value: core = env[coreval] accu = [] for lib in value: <mask>: continue accu.append(lib) env[""LIBPATH_"" + var] = accu",False,if lib in core :,if lib not in core :,0.8820016898747209,37.99178428257963
"def throttle_status(server=None): result = AmonStruct() result.allow = False last_check = server.get(""last_check"") server_check_period = server.get(""check_every"", 60) if last_check: period_since_last_check = unix_utc_now() - last_check # Add 15 seconds buffer, for statsd period_since_last_check = period_since_last_check + 15 <mask>: result.allow = True else: result.allow = True # Never checked return result",False,if period_since_last_check >= server_check_period :,if period_since_last_check > server_check_period :,0.8820016898747209,81.96501312471537
"def fetch_scatter_outputs(self, task): scatteroutputs = [] for var in task[""body""]: # TODO variable support if var.startswith(""call""): <mask>: for output in self.tasks_dictionary[task[""body""][var][""task""]][ ""outputs"" ]: scatteroutputs.append( {""task"": task[""body""][var][""alias""], ""output"": output[0]} ) return scatteroutputs",False,"if ""outputs"" in self . tasks_dictionary [ task [ ""body"" ] [ var ] [ ""task"" ] ] :","if task [ ""body"" ] [ var ] [ ""task"" ] in self . tasks_dictionary :",0.8820016898747209,67.1701188415296
"def _add_constant_node(self, source_node): parent_ids = range(len(source_node.in_edges)) for idx in parent_ids: parent_node = self.tf_graph.get_node(source_node.in_edges[idx]) <mask>: self._rename_Const(parent_node)",False,"if parent_node . type == ""Const"" :",if parent_node is not None :,0.8820016898747209,22.17204504793461
"def enableCtrls(self): # Check if each ctrl has a requirement or an incompatibility, # look it up, and enable/disable if so for data in self.storySettingsData: name = data[""name""] if name in self.ctrls: <mask>: set = self.getSetting(data[""requires""]) for i in self.ctrls[name]: i.Enable(set not in [""off"", ""false"", ""0""])",False,"if ""requires"" in data :","if data [ ""requires"" ] :",0.8820016898747209,24.446151121745064
"def update_realtime(self, stdout="""", stderr="""", delete=False): wooey_cache = wooey_settings.WOOEY_REALTIME_CACHE if delete == False and wooey_cache is None: self.stdout = stdout self.stderr = stderr self.save() elif wooey_cache is not None: cache = django_cache[wooey_cache] <mask>: cache.delete(self.get_realtime_key()) else: cache.set( self.get_realtime_key(), json.dumps({""stdout"": stdout, ""stderr"": stderr}), )",True,if delete :,if delete :,0.8820016898747209,0.0
"def _check_for_batch_clashes(xs): """"""Check that batch names do not overlap with sample names."""""" names = set([x[""description""] for x in xs]) dups = set([]) for x in xs: batches = tz.get_in((""metadata"", ""batch""), x) if batches: if not isinstance(batches, (list, tuple)): batches = [batches] for batch in batches: <mask>: dups.add(batch) if len(dups) > 0: raise ValueError( ""Batch names must be unique from sample descriptions.\n"" ""Clashing batch names: %s"" % sorted(list(dups)) )",False,if batch in names :,if batch not in names :,0.8820016898747209,37.99178428257963
"def toggle(self, event=None): if self.absolute: if self.save == self.split: self.save = 100 if self.split > 20: self.save = self.split self.split = 1 else: self.split = self.save else: if self.save == self.split: self.save = 0.3 <mask>: self.split = self.save elif self.split < 0.5: self.split = self.min else: self.split = self.max self.placeChilds()",False,if self . split <= self . min or self . split >= self . max :,elif self . save > 0.5 :,0.8820016898747209,2.8129878312564873
"def can_read(self): if hasattr(self.file, ""__iter__""): iterator = iter(self.file) head = next(iterator, None) <mask>: self.repaired = [] return True if isinstance(head, str): self.repaired = itertools.chain([head], iterator) return True else: # We may have mangled a generator at this point, so just abort raise IOSourceError( ""Could not open source: %r (mode: %r)"" % (self.file, self.options[""mode""]) ) return False",True,if head is None :,if head is None :,0.8820016898747209,100.00000000000004
"def _print_message_content(self, peer, data): inheaders = 1 lines = data.splitlines() for line in lines: # headers first if inheaders and not line: peerheader = ""X-Peer: "" + peer[0] <mask>: # decoded_data=false; make header match other binary output peerheader = repr(peerheader.encode(""utf-8"")) print(peerheader) inheaders = 0 if not isinstance(data, str): # Avoid spurious 'str on bytes instance' warning. line = repr(line) print(line)",False,"if not isinstance ( data , str ) :",if not self . decoded_data :,0.8820016898747209,11.59119922599073
"def connect(self): # Makes connection with MySQL server try: <mask>: connection = pymysql.connect(read_default_file=""/etc/mysql/conf.d/my.cnf"") else: connection = pymysql.connect(read_default_file=""~/.my.cnf"") return connection except ValueError as e: Log.debug(self, str(e)) raise MySQLConnectionError except pymysql.err.InternalError as e: Log.debug(self, str(e)) raise MySQLConnectionError",True,"if os . path . exists ( ""/etc/mysql/conf.d/my.cnf"" ) :","if os . path . exists ( ""/etc/mysql/conf.d/my.cnf"" ) :",0.8820016898747209,100.00000000000004
"def _copy_package_apps( local_bin_dir: Path, app_paths: List[Path], suffix: str = """" ) -> None: for src_unresolved in app_paths: src = src_unresolved.resolve() app = src.name dest = Path(local_bin_dir / add_suffix(app, suffix)) if not dest.parent.is_dir(): mkdir(dest.parent) if dest.exists(): logger.warning(f""{hazard} Overwriting file {str(dest)} with {str(src)}"") dest.unlink() <mask>: shutil.copy(src, dest)",False,if src . exists ( ) :,if os . path . exists ( src ) :,0.8820016898747209,21.36435031981171
"def update(self, x, who=None, metadata=None): self._retain_refs(metadata) y = self._get_key(x) if self.keep == ""last"": # remove key if already present so that emitted value # will reflect elements' actual relative ordering self._buffer.pop(y, None) self._metadata_buffer.pop(y, None) self._buffer[y] = x self._metadata_buffer[y] = metadata else: # self.keep == ""first"" <mask>: self._buffer[y] = x self._metadata_buffer[y] = metadata return self.last",False,if y not in self . _buffer :,"if self . keep == ""last"" :",0.8820016898747209,9.980099403873663
"def resolve_credential_keys(m_keys, keys): res = [] for k in m_keys: if k[""c7n:match-type""] == ""credential"": c_date = parse_date(k[""last_rotated""]) for ak in keys: if c_date == ak[""CreateDate""]: ak = dict(ak) ak[""c7n:match-type""] = ""access"" <mask>: res.append(ak) elif k not in res: res.append(k) return res",False,if ak not in res :,"if ak [ ""c7n:match-type"" ] == ""credential"" :",0.8820016898747209,5.816635421147513
"def _apply_flag_attrs(src_flag, dest_flag): # Use a baseline flag def to get default values for empty data. baseline_flag = FlagDef("""", {}, None) for name in dir(src_flag): if name[:1] == ""_"": continue dest_val = getattr(dest_flag, name, None) baseline_val = getattr(baseline_flag, name, None) <mask>: setattr(dest_flag, name, getattr(src_flag, name))",False,if dest_val == baseline_val :,if dest_val != baseline_val :,0.8820016898747209,65.80370064762461
"def _ws_keep_reading(self): import websockets.exceptions while not self._reader_stopped: try: data = await self._ws.recv() <mask>: data = data.encode(""UTF-8"") if len(data) == 0: self._error = ""EOF"" break except websockets.exceptions.ConnectionClosedError: # TODO: try to reconnect in case of Ctrl+D self._error = ""EOF"" break self.num_bytes_received += len(data) self._make_output_available(data, block=False)",False,"if isinstance ( data , str ) :","if isinstance ( data , bytes ) :",0.8820016898747209,59.4603557501361
"def to_dict(self) -> Dict[str, Any]: result = {} for field_name in self.API_FIELDS: <mask>: result[""stream_id""] = self.id continue elif field_name == ""date_created"": result[""date_created""] = datetime_to_timestamp(self.date_created) continue result[field_name] = getattr(self, field_name) result[""is_announcement_only""] = ( self.stream_post_policy == Stream.STREAM_POST_POLICY_ADMINS ) return result",False,"if field_name == ""id"" :","if field_name == ""stream_id"" :",0.8820016898747209,63.40466277046863
"def all_masks( cls, images, run, run_key, step, ): all_mask_groups = [] for image in images: <mask>: mask_group = {} for k in image._masks: mask = image._masks[k] mask_group[k] = mask.to_json(run) all_mask_groups.append(mask_group) else: all_mask_groups.append(None) if all_mask_groups and not all(x is None for x in all_mask_groups): return all_mask_groups else: return False",False,if image . _masks :,if image . _key == run_key :,0.8820016898747209,24.808415001701817
"def disconnect_all(listener): """"""Disconnect from all signals"""""" for emitter in listener._signal_data.emitters: for signal in emitter._signal_data.listeners: emitter._signal_data.listeners[signal] = [ i for i in emitter._signal_data.listeners[signal] <mask>: ]",False,"if getattr ( i , ""__self__"" , None ) != listener",if i . _signal_data . listeners [ signal ] [ 0 ] == listener,0.8820016898747209,5.653041175801492
"def wait(self, timeout=None): if self.returncode is None: if timeout is None: msecs = _subprocess.INFINITE else: msecs = max(0, int(timeout * 1000 + 0.5)) res = _subprocess.WaitForSingleObject(int(self._handle), msecs) if res == _subprocess.WAIT_OBJECT_0: code = _subprocess.GetExitCodeProcess(self._handle) <mask>: code = -signal.SIGTERM self.returncode = code return self.returncode",False,if code == TERMINATE :,if code == _subprocess . WAIT_OBJECT_1 :,0.8820016898747209,20.448007360218387
"def set_pbar_fraction(self, frac, progress, stage=None): gtk.gdk.threads_enter() try: self.is_pulsing = False self.set_stage_text(stage or _(""Processing..."")) self.pbar.set_text(progress) <mask>: frac = 1.0 if frac < 0: frac = 0 self.pbar.set_fraction(frac) finally: gtk.gdk.threads_leave()",False,if frac > 1 :,if frac > 1.0 :,0.8820016898747209,42.72870063962342
"def get_aa_from_codonre(re_aa): aas = [] m = 0 for i in re_aa: if i == ""["": m = -1 aas.append("""") <mask>: m = 0 continue elif m == -1: aas[-1] = aas[-1] + i elif m == 0: aas.append(i) return aas",True,"elif i == ""]"" :","elif i == ""]"" :",0.8820016898747209,100.00000000000004
"def link(token, base_url): """"""Validation for ``link``."""""" if get_keyword(token) == ""none"": return ""none"" parsed_url = get_url(token, base_url) if parsed_url: return parsed_url function = parse_function(token) if function: name, args = function prototype = (name, [a.type for a in args]) args = [getattr(a, ""value"", a) for a in args] <mask>: return (""attr()"", args[0])",False,"if prototype == ( ""attr"" , [ ""ident"" ] ) :","if prototype in ( ""attr"" , ""attr_value"" ) :",0.8820016898747209,31.01950461129554
"def on_bt_search_clicked(self, widget): if self.current_provider is None: return query = self.en_query.get_text() @self.obtain_podcasts_with def load_data(): if self.current_provider.kind == directory.Provider.PROVIDER_SEARCH: return self.current_provider.on_search(query) <mask>: return self.current_provider.on_url(query) elif self.current_provider.kind == directory.Provider.PROVIDER_FILE: return self.current_provider.on_file(query)",True,elif self . current_provider . kind == directory . Provider . PROVIDER_URL :,elif self . current_provider . kind == directory . Provider . PROVIDER_URL :,0.8820016898747209,100.00000000000004
"def test_handle_single(self): self.skipTest( ""Pops up windows and needs user input.. so disabled."" ""Still worth keeping whilst we don't have unit tests "" ""for all plugins."" ) # Ignored... for id_, plugin in self.plugins.items(): <mask>: self.h.plugin_enable(plugin, None) self.h.handle(id_, self.lib, self.parent, SONGS) self.h.plugin_disable(plugin)",False,if self . h . plugin_handle ( plugin ) :,if self . h . plugin_is_enabled ( plugin ) :,0.8820016898747209,61.62607099729587
"def __repr__(self): attrs = [] for k in self._keydata: <mask>: attrs.append(""p(%d)"" % (self.size() + 1,)) elif hasattr(self, k): attrs.append(k) if self.has_private(): attrs.append(""private"") # PY3K: This is meant to be text, do not change to bytes (data) return ""<%s @0x%x %s>"" % (self.__class__.__name__, id(self), "","".join(attrs))",False,"if k == ""p"" :","if k == ""size"" :",0.8820016898747209,59.4603557501361
"def apply(self, node, code, required): yield ""try:"" yield from self.iterIndented(code) yield "" pass"" yield ""except {}:"".format(self.exceptionString) outputVariables = node.getOutputSocketVariables() for i, s in enumerate(node.outputs): <mask>: if hasattr(s, ""getDefaultValueCode""): yield f"" {outputVariables[s.identifier]} = {s.getDefaultValueCode()}"" else: yield f"" {outputVariables[s.identifier]} = self.outputs[{i}].getDefaultValue()"" yield "" pass""",False,if s . identifier in required :,if s . identifier in outputVariables :,0.8820016898747209,64.34588841607616
"def __import__(name, globals=None, locals=None, fromlist=(), level=0): module = orig___import__(name, globals, locals, fromlist, level) if fromlist and module.__name__ in modules: if ""*"" in fromlist: fromlist = list(fromlist) fromlist.remove(""*"") fromlist.extend(getattr(module, ""__all__"", [])) for x in fromlist: <mask>: from_name = ""{}.{}"".format(module.__name__, x) if from_name in modules: importlib.import_module(from_name) return module",False,"if isinstance ( getattr ( module , x , None ) , types . ModuleType ) :","if x . startswith ( ""_"" ) :",0.8820016898747209,5.484683161991908
"def _consume_msg(self): ws = self._ws try: while True: r = await ws.recv() <mask>: r = r.decode(""utf-8"") msg = json.loads(r) stream = msg.get(""stream"") if stream is not None: await self._dispatch(stream, msg) except websockets.WebSocketException as wse: logging.warn(wse) await self.close() asyncio.ensure_future(self._ensure_ws())",True,"if isinstance ( r , bytes ) :","if isinstance ( r , bytes ) :",0.8820016898747209,100.00000000000004
"def add_source(self, source, name=None): """"""Adds a new data source to an existing provider."""""" if self.randomize: <mask>: raise ValueError( ""Cannot add a non-shuffleable source to an "" ""already shuffled provider."" ) super().add_source(source, name=name) if self.randomize is True: self._shuffle_len = self.entries",False,if not source . can_shuffle ( ) :,if self . shuffleable :,0.8820016898747209,5.171845311465849
"def __str__(self): buf = [""""] if self.fileName: buf.append(self.fileName + "":"") if self.line != -1: if not self.fileName: buf.append(""line "") buf.append(str(self.line)) <mask>: buf.append("":"" + str(self.column)) buf.append("":"") buf.append("" "") return str("""").join(buf)",False,if self . column != - 1 :,if not self . column :,0.8820016898747209,20.82186541080652
"def has_bad_headers(self): headers = [self.sender, self.reply_to] + self.recipients for header in headers: if _has_newline(header): return True if self.subject: if _has_newline(self.subject): for linenum, line in enumerate(self.subject.split(""\r\n"")): if not line: return True if linenum > 0 and line[0] not in ""\t "": return True if _has_newline(line): return True <mask>: return True return False",False,if len ( line . strip ( ) ) == 0 :,if _has_newline ( line ) :,0.8820016898747209,7.687847996206941
"def scanHexEscape(self, prefix): code = 0 leng = 4 if (prefix == ""u"") else 2 for i in xrange(leng): <mask>: ch = self.source[self.index] self.index += 1 code = code * 16 + HEX_CONV[ch] else: return """" return unichr(code)",False,if self . index < self . length and isHexDigit ( self . source [ self . index ] ) :,if self . source [ self . index ] == prefix :,0.8820016898747209,34.348046664316776
"def _get_table_info(self, table_name): table_addr = self.addr_space.profile.get_symbol(table_name) table_size = self._get_table_info_distorm() <mask>: table_size = self._get_table_info_other(table_addr, table_name) if table_size == 0: debug.error(""Unable to get system call table size"") return [table_addr, table_size]",True,if table_size == 0 :,if table_size == 0 :,0.8820016898747209,100.00000000000004
"def format_file_path(filepath): """"""Formats a path as absolute and with the correct platform separator."""""" try: is_windows_network_mount = WINDOWS_NETWORK_MOUNT_PATTERN.match(filepath) filepath = os.path.realpath(os.path.abspath(filepath)) filepath = re.sub(BACKSLASH_REPLACE_PATTERN, ""/"", filepath) is_windows_drive = WINDOWS_DRIVE_PATTERN.match(filepath) <mask>: filepath = filepath.capitalize() if is_windows_network_mount: # Add back a / to the front, since the previous modifications # will have replaced any double slashes with single filepath = ""/"" + filepath except: pass return filepath",True,if is_windows_drive :,if is_windows_drive :,0.8820016898747209,100.00000000000004
"def _match(self, cre, s): # Run compiled regular expression match method on 's'. # Save result, return success. self.mo = cre.match(s) if __debug__: <mask>: self._mesg(""\tmatched r'%r' => %r"" % (cre.pattern, self.mo.groups())) return self.mo is not None",False,if self . mo is not None and self . debug >= 5 :,if self . mo is not None :,0.8820016898747209,36.24372413507827
"def reload_sanitize_allowlist(self, explicit=True): self.sanitize_allowlist = [] try: with open(self.sanitize_allowlist_file) as f: for line in f.readlines(): if not line.startswith(""#""): self.sanitize_allowlist.append(line.strip()) except OSError: <mask>: log.warning( ""Sanitize log file explicitly specified as '%s' but does not exist, continuing with no tools allowlisted."", self.sanitize_allowlist_file, )",True,if explicit :,if explicit :,0.8820016898747209,0.0
"def conj(self): dtype = self.dtype if issubclass(self.dtype.type, np.complexfloating): if not self.flags.forc: raise RuntimeError( ""only contiguous arrays may "" ""be used as arguments to this operation"" ) <mask>: order = ""F"" else: order = ""C"" result = self._new_like_me(order=order) func = elementwise.get_conj_kernel(dtype) func.prepared_async_call( self._grid, self._block, None, self.gpudata, result.gpudata, self.mem_size ) return result else: return self",False,if self . flags . f_contiguous :,if self . flags . forc :,0.8820016898747209,48.35447404743731
"def scan_spec_conf(self, conf): if ""metadata"" in conf: if ""annotations"" in conf[""metadata""] and conf[""metadata""].get(""annotations""): for annotation in conf[""metadata""][""annotations""]: for key in annotation: <mask>: if ( ""docker/default"" in annotation[key] or ""runtime/default"" in annotation[key] ): return CheckResult.PASSED return CheckResult.FAILED",False,"if ""seccomp.security.alpha.kubernetes.io/defaultProfileName"" in key :","if key . startswith ( ""docker"" ) :",0.8820016898747209,3.2612121198882003
"def test_error_through_destructor(self): # Test that the exception state is not modified by a destructor, # even if close() fails. rawio = self.CloseFailureIO() with support.catch_unraisable_exception() as cm: with self.assertRaises(AttributeError): self.tp(rawio).xyzzy if not IOBASE_EMITS_UNRAISABLE: self.assertIsNone(cm.unraisable) <mask>: self.assertEqual(cm.unraisable.exc_type, OSError)",False,elif cm . unraisable is not None :,if not IOBASE_EMITS_UNRAISABLE :,0.8820016898747209,6.567274736060395
"def _dumpf(frame): if frame is None: return ""<None>"" else: addn = ""(with trace!)"" <mask>: addn = "" **No Trace Set **"" return ""Frame at %d, file %s, line: %d%s"" % ( id(frame), frame.f_code.co_filename, frame.f_lineno, addn, )",False,if frame . f_trace is None :,if frame . f_code . co_filename . co_filename . co_filename . co_lineno == 0 :,0.8820016898747209,14.15394535061703
"def containsBadbytes(self, value, bytecount=4): for b in self.badbytes: tmp = value <mask>: b = ord(b) for i in range(bytecount): if (tmp & 0xFF) == b: return True tmp >>= 8 return False",False,if type ( b ) == str :,if tmp & 0xFF :,0.8820016898747209,5.70796903405875
"def _set_peer_statuses(self): """"""Set peer statuses."""""" cutoff = time.time() - STALE_SECS for peer in self.peers: <mask>: peer.status = PEER_BAD elif peer.last_good > cutoff: peer.status = PEER_GOOD elif peer.last_good: peer.status = PEER_STALE else: peer.status = PEER_NEVER",False,if peer . bad :,if peer . last_good < cutoff :,0.8820016898747209,19.070828081828378
"def afterTest(self, test): try: # If the browser window is still open, close it now. self.driver.quit() except AttributeError: pass except Exception: pass if self.options.headless: <mask>: try: self.display.stop() except AttributeError: pass except Exception: pass",False,if self . headless_active :,if self . display :,0.8820016898747209,28.641904579795423
"def _written_variables_in_proxy(self, contract): variables = [] if contract.is_upgradeable: variables_name_written_in_proxy = self._variable_written_in_proxy() <mask>: variables_in_contract = [ contract.get_state_variable_from_name(v) for v in variables_name_written_in_proxy ] variables_in_contract = [v for v in variables_in_contract if v] variables += variables_in_contract return list(set(variables))",True,if variables_name_written_in_proxy :,if variables_name_written_in_proxy :,0.8820016898747209,100.00000000000004
"def _available_symbols(self, scoperef, expr): cplns = [] found_names = set() while scoperef: elem = self._elem_from_scoperef(scoperef) for child in elem: name = child.get(""name"", """") if name.startswith(expr): <mask>: found_names.add(name) ilk = child.get(""ilk"") or child.tag cplns.append((ilk, name)) scoperef = self.parent_scoperef_from_scoperef(scoperef) if not scoperef: break return sorted(cplns, key=operator.itemgetter(1))",True,if name not in found_names :,if name not in found_names :,0.8820016898747209,100.00000000000004
"def get_resource_public_actions(resource_class): resource_class_members = inspect.getmembers(resource_class) resource_methods = {} for name, member in resource_class_members: if not name.startswith(""_""): <mask>: if not name.startswith(""wait_until""): if is_resource_action(member): resource_methods[name] = member return resource_methods",False,if not name [ 0 ] . isupper ( ) :,"if not name . startswith ( ""resource_action"" ) :",0.8820016898747209,15.727800941615351
def UpdateControlState(self): active = self.demoModules.GetActiveID() # Update the radio/restore buttons for moduleID in self.radioButtons: btn = self.radioButtons[moduleID] <mask>: btn.SetValue(True) else: btn.SetValue(False) if self.demoModules.Exists(moduleID): btn.Enable(True) if moduleID == modModified: self.btnRestore.Enable(True) else: btn.Enable(False) if moduleID == modModified: self.btnRestore.Enable(False),False,if moduleID == active :,if active == moduleID :,0.8820016898747209,21.3643503198117
"def test_controlcharacters(self): for i in range(128): c = chr(i) testString = ""string containing %s"" % c if i >= 32 or c in ""\r\n\t"": # \r, \n and \t are the only legal control chars in XML data = plistlib.dumps(testString, fmt=plistlib.FMT_XML) <mask>: self.assertEqual(plistlib.loads(data), testString) else: with self.assertRaises(ValueError): plistlib.dumps(testString, fmt=plistlib.FMT_XML) plistlib.dumps(testString, fmt=plistlib.FMT_BINARY)",False,"if c != ""\r"" :",if data :,0.8820016898747209,0.0
"def remove_usernames(self, username: SLT[str]) -> None: with self.__lock: <mask>: raise RuntimeError( f""Can't set {self.username_name} in conjunction with (already set) "" f""{self.chat_id_name}s."" ) parsed_username = self._parse_username(username) self._usernames -= parsed_username",False,if self . _chat_ids :,if self . _usernames :,0.8820016898747209,38.49815007763549
"def get_size(self, shape_info): # The size is the data, that have constant size. state = np.random.RandomState().get_state() size = 0 for elem in state: if isinstance(elem, str): size += len(elem) elif isinstance(elem, np.ndarray): size += elem.size * elem.itemsize <mask>: size += np.dtype(""int"").itemsize elif isinstance(elem, float): size += np.dtype(""float"").itemsize else: raise NotImplementedError() return size",True,"elif isinstance ( elem , int ) :","elif isinstance ( elem , int ) :",0.8820016898747209,100.00000000000004
"def before_step(self, step, feed_dict): if step == 0: for _type, mem in self.memories.items(): <mask>: self.gan.session.run(tf.assign(mem[""var""], mem[""source""]))",True,"if ""var"" in mem and ""source"" in mem :","if ""var"" in mem and ""source"" in mem :",0.8820016898747209,100.00000000000004
"def write(self, *bits): for bit in bits: if not self.bytestream: self.bytestream.append(0) byte = self.bytestream[self.bytenum] <mask>: if self.bytenum == len(self.bytestream) - 1: byte = 0 self.bytestream += bytes([byte]) self.bytenum += 1 self.bitnum = 0 mask = 2 ** self.bitnum if bit: byte |= mask else: byte &= ~mask self.bytestream[self.bytenum] = byte self.bitnum += 1",False,if self . bitnum == 8 :,if byte :,0.8820016898747209,0.0
"def _validate_parameter_range(self, value_hp, parameter_range): """"""Placeholder docstring"""""" for ( parameter_range_key, parameter_range_value, ) in parameter_range.__dict__.items(): if parameter_range_key == ""scaling_type"": continue # Categorical ranges <mask>: for categorical_value in parameter_range_value: value_hp.validate(categorical_value) # Continuous, Integer ranges else: value_hp.validate(parameter_range_value)",False,"if isinstance ( parameter_range_value , list ) :","if parameter_range_key == ""categorical_ranges"" :",0.8820016898747209,19.67497981115564
"def _trackA(self, tracks): try: track, start, end = self.featureA assert track in tracks return track except TypeError: for track in tracks: for feature_set in track.get_sets(): if hasattr(feature_set, ""features""): <mask>: return track return None",False,if self . featureA in feature_set . features . values ( ) :,if feature_set . features [ 0 ] == start :,0.8820016898747209,26.970156334232563
"def walk(directory, path_so_far): for name in sorted(os.listdir(directory)): if any(fnmatch(name, pattern) for pattern in basename_ignore): continue path = path_so_far + ""/"" + name if path_so_far else name if any(fnmatch(path, pattern) for pattern in path_ignore): continue full_name = os.path.join(directory, name) <mask>: for file_path in walk(full_name, path): yield file_path elif os.path.isfile(full_name): yield path",True,if os . path . isdir ( full_name ) :,if os . path . isdir ( full_name ) :,0.8820016898747209,100.00000000000004
"def _poll_ipc_requests(self) -> None: try: <mask>: return while not self._ipc_requests.empty(): args = self._ipc_requests.get() try: for filename in args: if os.path.isfile(filename): self.get_editor_notebook().show_file(filename) except Exception as e: logger.exception(""Problem processing ipc request"", exc_info=e) self.become_active_window() finally: self.after(50, self._poll_ipc_requests)",True,if self . _ipc_requests . empty ( ) :,if self . _ipc_requests . empty ( ) :,0.8820016898747209,100.00000000000004
"def test_read1(self): self.test_write() blocks = [] nread = 0 with gzip.GzipFile(self.filename, ""r"") as f: while True: d = f.read1() <mask>: break blocks.append(d) nread += len(d) # Check that position was updated correctly (see issue10791). self.assertEqual(f.tell(), nread) self.assertEqual(b"""".join(blocks), data1 * 50)",True,if not d :,if not d :,0.8820016898747209,100.00000000000004
"def _target_generator(self): if self._internal_target_generator is None: <mask>: return None from ....model_zoo.rcnn.rpn.rpn_target import RPNTargetGenerator self._internal_target_generator = RPNTargetGenerator( num_sample=self._num_sample, pos_iou_thresh=self._pos_iou_thresh, neg_iou_thresh=self._neg_iou_thresh, pos_ratio=self._pos_ratio, stds=self._box_norm, **self._kwargs ) return self._internal_target_generator else: return self._internal_target_generator",False,if self . _net_none :,if self . _num_sample == 0 :,0.8820016898747209,25.965358893403383
"def time_left(self): """"""Return how many seconds are left until the timeout expires"""""" if self.is_non_blocking: return 0 elif self.is_infinite: return None else: delta = self.target_time - self.TIME() <mask>: # clock jumped, recalculate self.target_time = self.TIME() + self.duration return self.duration else: return max(0, delta)",False,if delta > self . duration :,if delta < 0 :,0.8820016898747209,15.848738972120703
"def _decorator(cls): for name, meth in inspect.getmembers(cls, inspect.isroutine): if name not in cls.__dict__: continue if name != ""__init__"": if not private and name.startswith(""_""): continue <mask>: continue setattr(cls, name, decorator(meth)) return cls",False,if name in butnot :,if not meth :,0.8820016898747209,14.794015674776452
"def load_vocab(vocab_file: str) -> List: """"""Loads a vocabulary file into a dictionary."""""" vocab = collections.OrderedDict() with io.open(vocab_file, ""r"", encoding=""UTF-8"") as file: for num, line in enumerate(file): items = convert_to_unicode(line.strip()).split(""\t"") <mask>: break token = items[0] index = items[1] if len(items) == 2 else num token = token.strip() vocab[token] = int(index) return vocab",False,if len ( items ) > 2 :,if len ( items ) == 0 :,0.8820016898747209,46.713797772819994
"def slice_fill(self, slice_): ""Fills the slice with zeroes for the dimensions that have single elements and squeeze_dims true"" if isinstance(self.indexes, int): new_slice_ = [0] offset = 0 else: new_slice_ = [slice_[0]] offset = 1 for i in range(1, len(self.nums)): <mask>: new_slice_.append(0) elif offset < len(slice_): new_slice_.append(slice_[offset]) offset += 1 new_slice_ += slice_[offset:] return new_slice_",False,if self . squeeze_dims [ i ] :,if i == self . indexes :,0.8820016898747209,10.816059393812111
"def check_update_function(url, folder, update_setter, version_setter, auto): remote_version = urllib.urlopen(url).read() if remote_version.isdigit(): local_version = get_local_timestamp(folder) if remote_version > local_version: <mask>: update_setter.set_value(True) version_setter.set_value(remote_version) return True else: return False else: return False",True,if auto :,if auto :,0.8820016898747209,0.0
"def iter_content(self, chunk_size_bytes): while True: try: data = self._fp.read(chunk_size_bytes) except IOError as e: raise Fetcher.PermanentError( ""Problem reading chunk from {}: {}"".format(self._fp.name, e) ) <mask>: break yield data",True,if not data :,if not data :,0.8820016898747209,100.00000000000004
"def gvariant_args(args: List[Any]) -> str: """"""Convert args into gvariant."""""" gvariant = """" for arg in args: if isinstance(arg, bool): gvariant += "" {}"".format(str(arg).lower()) elif isinstance(arg, (int, float)): gvariant += f"" {arg}"" <mask>: gvariant += f' ""{arg}""' else: gvariant += f"" {arg!s}"" return gvariant.lstrip()",True,"elif isinstance ( arg , str ) :","elif isinstance ( arg , str ) :",0.8820016898747209,100.00000000000004
"def _element_keywords(cls, backend, elements=None): ""Returns a dictionary of element names to allowed keywords"" if backend not in Store.loaded_backends(): return {} mapping = {} backend_options = Store.options(backend) elements = elements if elements is not None else backend_options.keys() for element in elements: <mask>: continue element = element if isinstance(element, tuple) else (element,) element_keywords = [] options = backend_options[""."".join(element)] for group in Options._option_groups: element_keywords.extend(options[group].allowed_keywords) mapping[element[0]] = element_keywords return mapping",False,"if ""."" in element :","if not isinstance ( element , dict ) :",0.8820016898747209,6.27465531099474
"def setup_parameter_node(self, param_node): if param_node.bl_idname == ""SvNumberNode"": if self.use_prop or self.get_prop_name(): value = self.sv_get()[0][0] print(""V"", value) <mask>: param_node.selected_mode = ""int"" param_node.int_ = value elif isinstance(value, float): param_node.selected_mode = ""float"" param_node.float_ = value",True,"if isinstance ( value , int ) :","if isinstance ( value , int ) :",0.8820016898747209,100.00000000000004
"def _get_oshape(indices_shape, depth, axis): oshape = [] true_axis = len(indices_shape) if axis == -1 else axis ndim = len(indices_shape) + 1 indices_index = 0 for i in range(0, ndim): <mask>: oshape.append(depth) else: oshape.append(indices_shape[indices_index]) indices_index += 1 return oshape",False,if i == true_axis :,if indices_index >= true_axis :,0.8820016898747209,41.11336169005198
"def check(self, value): value = String.check(self, value) if isinstance(value, str): value = value.upper() for prefix in (self.prefix, self.prefix.split(""_"", 1)[1]): # e.g. PANGO_WEIGHT_BOLD --> BOLD but also WEIGHT_BOLD --> BOLD <mask>: value = value[len(prefix) :] value = value.lstrip(""_"") if hasattr(self.group, value): return getattr(self.group, value) else: raise ValueError(""No such constant: %s_%s"" % (self.prefix, value)) else: return value",False,if value . startswith ( prefix ) :,if prefix . startswith ( value ) :,0.8820016898747209,29.071536848410968
"def shuffle_unison_inplace(list_of_lists, random_state=None): if list_of_lists: assert all(len(l) == len(list_of_lists[0]) for l in list_of_lists) <mask>: random_state.permutation(len(list_of_lists[0])) else: p = np.random.permutation(len(list_of_lists[0])) return [l[p] for l in list_of_lists] return None",False,if random_state is not None :,if random_state :,0.8820016898747209,38.80684294761701
"def _load_module(self): spec = self.default_module_spec module_identifier = self.module_identifier if module_identifier: impls = self.get_module_implementation_map() <mask>: raise ModuleNotFound( ""Invalid module identifier %r in %s"" % (module_identifier, force_ascii(repr(self))) ) spec = impls[module_identifier] cls = load( spec, context_explanation=""Loading module for %s"" % force_ascii(repr(self)) ) options = getattr(self, self.module_options_field, None) or {} return cls(self, options)",True,if module_identifier not in impls :,if module_identifier not in impls :,0.8820016898747209,100.00000000000004
"def get_data(self, state=None, request=None): if self.load_in_memory: data, shapes = self._in_memory_get_data(state, request) else: data, shapes = self._out_of_memory_get_data(state, request) for i in range(len(data)): <mask>: if isinstance(request, numbers.Integral): data[i] = data[i].reshape(shapes[i]) else: for j in range(len(data[i])): data[i][j] = data[i][j].reshape(shapes[i][j]) return tuple(data)",False,if shapes [ i ] is not None :,if i in shapes :,0.8820016898747209,6.787957387517878
"def resolve_credential_keys(m_keys, keys): res = [] for k in m_keys: if k[""c7n:match-type""] == ""credential"": c_date = parse_date(k[""last_rotated""]) for ak in keys: <mask>: ak = dict(ak) ak[""c7n:match-type""] = ""access"" if ak not in res: res.append(ak) elif k not in res: res.append(k) return res",False,"if c_date == ak [ ""CreateDate"" ] :","if c_date < ak [ ""last_rotated"" ] :",0.8820016898747209,32.85702044797774
"def _is_legacy_mode(self, node): """"""Checks if the ``ast.Call`` node's keywords signal using legacy mode."""""" script_mode = False py_version = ""py2"" for kw in node.keywords: <mask>: script_mode = ( bool(kw.value.value) if isinstance(kw.value, ast.NameConstant) else True ) if kw.arg == ""py_version"": py_version = kw.value.s if isinstance(kw.value, ast.Str) else ""py3"" return not (py_version.startswith(""py3"") or script_mode)",False,"if kw . arg == ""script_mode"" :","if isinstance ( kw . value , ast . NameConstant ) :",0.8820016898747209,8.054496384843702
"def get_upstream_statuses_events(self, upstream: Set) -> Dict[str, V1Statuses]: statuses_by_refs = {u: [] for u in upstream} events = self.events or [] # type: List[V1EventTrigger] for e in events: entity_ref = contexts_refs.get_entity_ref(e.ref) if not entity_ref: continue <mask>: continue for kind in e.kinds: status = V1EventKind.events_statuses_mapping.get(kind) if status: statuses_by_refs[entity_ref].append(status) return statuses_by_refs",False,if entity_ref not in statuses_by_refs :,if entity_ref in statuses_by_refs :,0.8820016898747209,74.26141117870938
"def items(self): dict = {} for userdir in self.XDG_DIRS.keys(): prefix = self.get(userdir).strip('""').split(""/"")[0] <mask>: path = ( os.getenv(""HOME"") + ""/"" + ""/"".join(self.get(userdir).strip('""').split(""/"")[1:]) ) else: path = self.get(userdir).strip('""') dict[userdir] = path return dict.items()",False,if prefix :,"if prefix == ""HOME"" :",0.8820016898747209,12.22307556087252
"def clean_objects(string, common_attributes): """"""Return object and attribute lists"""""" string = clean_string(string) words = string.split() if len(words) > 1: prefix_words_are_adj = True for att in words[:-1]: <mask>: prefix_words_are_adj = False if prefix_words_are_adj: return words[-1:], words[:-1] else: return [string], [] else: return [string], []",False,if att not in common_attributes :,if att in common_attributes :,0.8820016898747209,61.29752413741059
"def extract_custom(extractor, *args, **kw): for match in extractor(*args, **kw): msg = match[2] <mask>: unused = ( ""<unused singular (hash=%s)>"" % md5(msg[1].encode(""utf8"")).hexdigest() ) msg = (unused, msg[1], msg[2]) match = (match[0], match[1], msg, match[3]) yield match",False,"if isinstance ( msg , tuple ) and msg [ 0 ] == """" :",if len ( msg ) == 3 :,0.8820016898747209,6.376384919709712
"def test_convex_decomposition(self): mesh = g.get_mesh(""quadknot.obj"") engines = [(""vhacd"", g.trimesh.interfaces.vhacd.exists)] for engine, exists in engines: <mask>: g.log.warning(""skipping convex decomposition engine %s"", engine) continue g.log.info(""Testing convex decomposition with engine %s"", engine) meshes = mesh.convex_decomposition(engine=engine) self.assertTrue(len(meshes) > 1) for m in meshes: self.assertTrue(m.is_watertight) g.log.info(""convex decomposition succeeded with %s"", engine)",True,if not exists :,if not exists :,0.8820016898747209,100.00000000000004
"def _to_string_infix(self, ostream, idx, verbose): if verbose: ostream.write("" , "") else: hasConst = not ( self._const.__class__ in native_numeric_types and self._const == 0 ) if hasConst: idx -= 1 _l = self._coef[id(self._args[idx])] _lt = _l.__class__ <mask>: ostream.write("" - "") else: ostream.write("" + "")",False,if _lt is _NegationExpression or ( _lt in native_numeric_types and _l < 0 ) :,if _lt . __class__ == int :,0.8820016898747209,6.808798745384609
"def get_other(self, data, items): is_tuple = False if type(data) == tuple: data = list(data) is_tuple = True if type(data) == list: m_items = items.copy() for idx, item in enumerate(items): if item < 0: m_items[idx] = len(data) - abs(item) for i in sorted(set(m_items), reverse=True): if i < len(data) and i > -1: del data[i] <mask>: return tuple(data) else: return data else: return None",False,if is_tuple :,elif is_tuple :,0.8820016898747209,66.87403049764218
"def process_error(self, data): if data.get(""error""): <mask>: raise AuthCanceled(self, data.get(""error_description"", """")) raise AuthFailed(self, data.get(""error_description"") or data[""error""]) elif ""denied"" in data: raise AuthCanceled(self, data[""denied""])",False,"if ""denied"" in data [ ""error"" ] or ""cancelled"" in data [ ""error"" ] :","if ""error_description"" in data :",0.8820016898747209,5.505761164761841
"def tamper(payload, **kwargs): junk_chars = ""!#$%&()*~+-_.,:;?@[/|\]^`"" retval = """" for i, char in enumerate(payload, start=1): amount = random.randint(10, 15) if char == "">"": retval += "">"" for _ in range(amount): retval += random.choice(junk_chars) elif char == ""<"": retval += ""<"" for _ in range(amount): retval += random.choice(junk_chars) <mask>: for _ in range(amount): retval += random.choice(junk_chars) else: retval += char return retval",False,"elif char == "" "" :","elif char == "">"" :",0.8820016898747209,59.4603557501361
"def retry_http_digest_auth(self, req, auth): token, challenge = auth.split("" "", 1) chal = parse_keqv_list(parse_http_list(challenge)) auth = self.get_authorization(req, chal) if auth: auth_val = ""Digest %s"" % auth <mask>: return None req.add_unredirected_header(self.auth_header, auth_val) resp = self.parent.open(req) return resp",False,"if req . headers . get ( self . auth_header , None ) == auth_val :","if auth_val == """" :",0.8820016898747209,6.3987120800392105
"def close(self): self.selector.close() if self.sock: sockname = None try: sockname = self.sock.getsockname() except (socket.error, OSError): pass self.sock.close() if type(sockname) is str: # it was a Unix domain socket, remove it from the filesystem <mask>: os.remove(sockname) self.sock = None",True,if os . path . exists ( sockname ) :,if os . path . exists ( sockname ) :,0.8820016898747209,100.00000000000004
"def to_nurbs(self, curves): result = [] for i, c in enumerate(curves): nurbs = SvNurbsCurve.to_nurbs(c) <mask>: raise Exception(f""Curve #{i} - {c} - can not be converted to NURBS!"") result.append(nurbs) return result",True,if nurbs is None :,if nurbs is None :,0.8820016898747209,100.00000000000004
"def handle_1_roomid_raffle(self, i): if i[1] in [""handle_1_room_TV"", ""handle_1_room_captain""]: <mask>: await self.notify(""post_watching_history"", i[0]) await self.notify(i[1], i[0], i[2]) else: print(""hhjjkskddrsfvsfdfvdfvvfdvdvdfdfffdfsvh"", i)",False,"if await self . notify ( ""check_if_normal_room"" , i [ 0 ] , - 1 ) :","if i [ 1 ] == ""post_watching_history"" :",0.8820016898747209,4.035170365074488
"def init_ps_var_partition(self): ps_vars = {} for v in self._non_embed_vars.values(): if v.name not in self._var_to_ps: self._var_to_ps[v.name] = string_to_id(v.name, self._ps_num) ps_id = self._var_to_ps[v.name] <mask>: ps_vars[ps_id] = [v] else: ps_vars[ps_id].append(v) self._ps_vars = ps_vars",True,if ps_id not in ps_vars :,if ps_id not in ps_vars :,0.8820016898747209,100.00000000000004
"def get_files(d): f = [] for root, dirs, files in os.walk(d): for name in files: if ""meta-environment"" in root or ""cross-canadian"" in root: continue if ""qemux86copy-"" in root or ""qemux86-"" in root: continue <mask>: f.append(os.path.join(root, name)) return f",False,"if ""do_build"" not in name and ""do_populate_sdk"" not in name :","if name . startswith ( ""qemux86"" ) :",0.8820016898747209,2.0886513132027296
"def setSelectedLabelState(self, p): # selected, disabled c = self.c # g.trace(p,c.edit_widget(p)) if p and c.edit_widget(p): <mask>: g.trace(self.trace_n, c.edit_widget(p), p) # g.trace(g.callers(6)) self.trace_n += 1 self.setDisabledHeadlineColors(p)",False,if 0 :,if p . is_visible ( ) :,0.8820016898747209,5.669791110976001
"def filter_tasks(self, task_types=None, task_states=None, task_text=None): tasks = self.api.tasks(self.id).get(""tasks"", {}) if tasks and tasks.get(""task""): return [ Task(self, task) for task in tasks.get(""task"", []) <mask>: and (not task_states or task[""state""].lower() in task_states) and (not task_text or task_text.lower() in str(task).lower()) ] else: return []",False,"if ( not task_types or task [ ""type"" ] . lower ( ) in task_types )","if task_types and task [ ""type"" ] . lower ( ) in task_types",0.8820016898747209,68.54146534428018
"def GenerateVector(self, hits, vector, level): """"""Generate possible hit vectors which match the rules."""""" for item in hits.get(level, []): <mask>: if item < vector[-1]: continue if item > self.max_separation + vector[-1]: break new_vector = vector + [item] if level + 1 == len(hits): yield new_vector elif level + 1 < len(hits): for result in self.GenerateVector(hits, new_vector, level + 1): yield result",False,if vector :,if item not in vector :,0.8820016898747209,17.965205598154213
def _transmit_from_storage(self) -> None: for blob in self.storage.gets(): # give a few more seconds for blob lease operation # to reduce the chance of race (for perf consideration) if blob.lease(self._timeout + 5): envelopes = [TelemetryItem(**x) for x in blob.get()] result = self._transmit(list(envelopes)) <mask>: blob.lease(1) else: blob.delete(),False,if result == ExportResult . FAILED_RETRYABLE :,if result :,0.8820016898747209,0.0
"def load_dictionary(file): oui = {} with open(file, ""r"") as f: for line in f: <mask>: data = line.split(""(hex)"") key = data[0].replace(""-"", "":"").lower().strip() company = data[1].strip() oui[key] = company return oui",False,"if ""(hex)"" in line :","if line . startswith ( ""company:"" ) :",0.8820016898747209,6.09604579835652
"def _yield_minibatches_idx(self, rgen, n_batches, data_ary, shuffle=True): indices = np.arange(data_ary.shape[0]) if shuffle: indices = rgen.permutation(indices) if n_batches > 1: remainder = data_ary.shape[0] % n_batches <mask>: minis = np.array_split(indices[:-remainder], n_batches) minis[-1] = np.concatenate((minis[-1], indices[-remainder:]), axis=0) else: minis = np.array_split(indices, n_batches) else: minis = (indices,) for idx_batch in minis: yield idx_batch",False,if remainder :,if remainder > 0 :,0.8820016898747209,23.643540225079384
"def canonical_custom_headers(self, headers): hoi = [] custom_headers = {} for key in headers: lk = key.lower() if headers[key] is not None: <mask>: custom_headers[lk] = "","".join(v.strip() for v in headers.get_all(key)) sorted_header_keys = sorted(custom_headers.keys()) for key in sorted_header_keys: hoi.append(""%s:%s"" % (key, custom_headers[key])) return ""\n"".join(hoi)",False,"if lk . startswith ( ""x-amz-"" ) :",if lk in custom_headers :,0.8820016898747209,9.469167282754096
"def validate(self, data): if not data.get(""reason""): # If reason is not provided, message is required and can not be # null or blank. message = data.get(""message"") if not message: if ""message"" not in data: msg = serializers.Field.default_error_messages[""required""] <mask>: msg = serializers.Field.default_error_messages[""null""] else: msg = serializers.CharField.default_error_messages[""blank""] raise serializers.ValidationError({""message"": [msg]}) return data",False,elif message is None :,"elif ""message"" not in data :",0.8820016898747209,7.267884212102741
def tearDown(self): try: os.chdir(self.cwd) <mask>: os.remove(self.pythonexe) test_support.rmtree(self.parent_dir) finally: BaseTestCase.tearDown(self),False,if self . pythonexe != sys . executable :,if self . pythonexe :,0.8820016898747209,26.013004751144457
"def update(self, value, label): if self._disabled: return try: self._progress.value = value self._label.value = label <mask>: self._displayed = True display_widget(self._widget) except Exception as e: self._disabled = True logger.exception(e) wandb.termwarn(""Unable to render progress bar, see the user log for details"")",False,if not self . _displayed :,if self . _displayed :,0.8820016898747209,67.31821382417488
"def GetBinaryOperationBinder(self, op): with self._lock: <mask>: return self._binaryOperationBinders[op] b = runtime.SymplBinaryOperationBinder(op) self._binaryOperationBinders[op] = b return b",False,if self . _binaryOperationBinders . ContainsKey ( op ) :,if op in self . _binaryOperationBinders :,0.8820016898747209,27.329052280893862
"def apply(self, l, b, evaluation): ""FromDigits[l_, b_]"" if l.get_head_name() == ""System`List"": value = Integer(0) for leaf in l.leaves: value = Expression(""Plus"", Expression(""Times"", value, b), leaf) return value elif isinstance(l, String): value = FromDigits._parse_string(l.get_string_value(), b) <mask>: evaluation.message(""FromDigits"", ""nlst"") else: return value else: evaluation.message(""FromDigits"", ""nlst"")",True,if value is None :,if value is None :,0.8820016898747209,100.00000000000004
"def hsconn_sender(self): while not self.stop_event.is_set(): try: # Block, but timeout, so that we can exit the loop gracefully request = self.send_queue.get(True, 6.0) if self.socket is not None: # Socket got closed and set to None in another thread... self.socket.sendall(request) <mask>: self.send_queue.task_done() except queue.Empty: pass except OSError: self.stop_event.set()",False,if self . send_queue is not None :,if request is not None :,0.8820016898747209,27.585129929794586
"def check_expected(result, expected, contains=False): if sys.version_info[0] >= 3: if isinstance(result, str): result = result.encode(""ascii"") if isinstance(expected, str): expected = expected.encode(""ascii"") resultlines = result.splitlines() expectedlines = expected.splitlines() if len(resultlines) != len(expectedlines): return False for rline, eline in zip(resultlines, expectedlines): if contains: <mask>: return False else: if not rline.endswith(eline): return False return True",False,if eline not in rline :,if rline . startswith ( eline ) :,0.8820016898747209,7.809849842300637
"def init_weights(self): """"""Initialize model weights."""""" for _, m in self.multi_deconv_layers.named_modules(): <mask>: normal_init(m, std=0.001) elif isinstance(m, nn.BatchNorm2d): constant_init(m, 1) for m in self.multi_final_layers.modules(): if isinstance(m, nn.Conv2d): normal_init(m, std=0.001, bias=0)",False,"if isinstance ( m , nn . ConvTranspose2d ) :","if isinstance ( m , nn . Conv2d ) :",0.8820016898747209,70.71067811865478
"def filter_rel_attrs(field_name, **rel_attrs): clean_dict = {} for k, v in rel_attrs.items(): <mask>: splitted_key = k.split(""__"") key = ""__"".join(splitted_key[1:]) clean_dict[key] = v else: clean_dict[k] = v return clean_dict",False,"if k . startswith ( field_name + ""__"" ) :",if field_name == k :,0.8820016898747209,10.19067192997668
"def cancel(self): with self._condition: <mask>: self._squash( state_root=self._previous_state_hash, context_ids=[self._previous_context_id], persist=False, clean_up=True, ) self._cancelled = True self._condition.notify_all()",False,if not self . _cancelled and not self . _final and self . _previous_context_id :,if self . _previous_state_hash != self . _previous_context_hash :,0.8820016898747209,34.63422239124766
"def _get_level(levels, level_ref): if level_ref in levels: return levels.index(level_ref) if isinstance(level_ref, six.integer_types): <mask>: level_ref += len(levels) if not (0 <= level_ref < len(levels)): raise PatsyError(""specified level %r is out of range"" % (level_ref,)) return level_ref raise PatsyError(""specified level %r not found"" % (level_ref,))",True,if level_ref < 0 :,if level_ref < 0 :,0.8820016898747209,100.00000000000004
"def parse_node(self, node, alias_map=None, conv=None): sql, params, unknown = self._parse(node, alias_map, conv) if unknown and conv and params: params = [conv.db_value(i) for i in params] if isinstance(node, Node): if node._negated: sql = ""NOT %s"" % sql <mask>: sql = "" "".join((sql, ""AS"", node._alias)) if node._ordering: sql = "" "".join((sql, node._ordering)) return sql, params",False,if node . _alias :,elif node . _alias :,0.8820016898747209,75.98356856515926
"def parse_object_id(_, values): if values: for key in values: <mask>: val = values[key] if len(val) > 10: try: values[key] = utils.ObjectIdSilent(val) except: values[key] = None",False,"if key . endswith ( ""_id"" ) :",if key in _ :,0.8820016898747209,7.652332131360532
"def TryMerge(self, d): while d.avail() > 0: tt = d.getVarInt32() if tt == 10: self.set_app_id(d.getPrefixedString()) continue if tt == 16: self.set_max_rows(d.getVarInt32()) continue <mask>: raise ProtocolBuffer.ProtocolBufferDecodeError d.skipData(tt)",True,if tt == 0 :,if tt == 0 :,0.8820016898747209,100.00000000000004
"def has_invalid_cce(yaml_file, product_yaml=None): rule = yaml.open_and_macro_expand(yaml_file, product_yaml) if ""identifiers"" in rule and rule[""identifiers""] is not None: for i_type, i_value in rule[""identifiers""].items(): <mask>: if not checks.is_cce_value_valid(""CCE-"" + str(i_value)): return True return False",False,"if i_type [ 0 : 3 ] == ""cce"" :","if i_type == ""CCE"" :",0.8820016898747209,25.681706510882123
"def _generate_table(self, fromdesc, todesc, diffs): if fromdesc or todesc: yield ( simple_colorize(fromdesc, ""description""), simple_colorize(todesc, ""description""), ) for i, line in enumerate(diffs): if line is None: # mdiff yields None on separator lines; skip the bogus ones # generated for the first line <mask>: yield ( simple_colorize(""---"", ""separator""), simple_colorize(""---"", ""separator""), ) else: yield line",False,if i > 0 :,elif i == 0 :,0.8820016898747209,17.965205598154213
"def _getPatternTemplate(pattern, key=None): if key is None: key = pattern if ""%"" not in pattern: key = pattern.upper() template = DD_patternCache.get(key) if not template: if key in (""EPOCH"", ""{^LN-BEG}EPOCH"", ""^EPOCH""): template = DateEpoch(lineBeginOnly=(key != ""EPOCH"")) <mask>: template = DateTai64n(wordBegin=(""start"" if key != ""TAI64N"" else False)) else: template = DatePatternRegex(pattern) DD_patternCache.set(key, template) return template",False,"elif key in ( ""TAI64N"" , ""{^LN-BEG}TAI64N"" , ""^TAI64N"" ) :","elif key in ( ""TAI64N"" , ""{^LN-BEG}TAI64N"" ) :",0.8820016898747209,73.15098156435263
"def ref_max_pooling_2d(x, kernel, stride, ignore_border, pad): y = [] for xx in x.reshape((-1,) + x.shape[-3:]): <mask>: xx = xx[np.newaxis] y += [ refs.pooling_2d(xx, ""max"", kernel, stride, pad, ignore_border)[np.newaxis] ] y = np.vstack(y) if x.ndim == 2: y = np.squeeze(y, 1) return y.reshape(x.shape[:-3] + y.shape[1:])",True,if xx . ndim == 2 :,if xx . ndim == 2 :,0.8820016898747209,100.00000000000004
"def show_topics(): """"""prints all available miscellaneous help topics."""""" print(_stash.text_color(""Miscellaneous Topics:"", ""yellow"")) for pp in PAGEPATHS: if not os.path.isdir(pp): continue content = os.listdir(pp) for pn in content: <mask>: name = pn[: pn.index(""."")] else: name = pn print(name)",False,"if ""."" in pn :","if pn . startswith ( ""."" ) :",0.8820016898747209,18.575057999133595
"def justify_toggle_auto(self, event=None): c = self if c.editCommands.autojustify == 0: c.editCommands.autojustify = abs(c.config.getInt(""autojustify"") or 0) <mask>: g.es(""Autojustify on, @int autojustify == %s"" % c.editCommands.autojustify) else: g.es(""Set @int autojustify in @settings"") else: c.editCommands.autojustify = 0 g.es(""Autojustify off"")",True,if c . editCommands . autojustify :,if c . editCommands . autojustify :,0.8820016898747209,100.00000000000004
"def render_token_list(self, tokens): result = [] vars = [] for token in tokens: <mask>: result.append(token.contents.replace(""%"", ""%%"")) elif token.token_type == TOKEN_VAR: result.append(""%%(%s)s"" % token.contents) vars.append(token.contents) return """".join(result), vars",False,if token . token_type == TOKEN_TEXT :,if token . token_type == TOKEN_STRING :,0.8820016898747209,82.651681837938
"def get_target_dimensions(self): width, height = self.engine.size for operation in self.operations: if operation[""type""] == ""crop"": width = operation[""right""] - operation[""left""] height = operation[""bottom""] - operation[""top""] <mask>: width = operation[""width""] height = operation[""height""] return (width, height)",False,"if operation [ ""type"" ] == ""resize"" :","elif operation [ ""type"" ] == ""resize"" :",0.8820016898747209,91.21679090703874
"def get_eval_matcher(self): if isinstance(self.data[""match""], str): <mask>: values = [""explicitDeny"", ""implicitDeny""] else: values = [""allowed""] vf = ValueFilter( {""type"": ""value"", ""key"": ""EvalDecision"", ""value"": values, ""op"": ""in""} ) else: vf = ValueFilter(self.data[""match""]) vf.annotate = False return vf",False,"if self . data [ ""match"" ] == ""denied"" :","if self . data [ ""match"" ] == ""implicitDeny"" :",0.8820016898747209,82.42367502646057
"def test_training(self): if not self.model_tester.is_training: return config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common() config.return_dict = True for model_class in self.all_model_classes: <mask>: continue model = model_class(config) model.to(torch_device) model.train() inputs = self._prepare_for_class(inputs_dict, model_class, return_labels=True) loss = model(**inputs).loss loss.backward()",False,if model_class in MODEL_MAPPING . values ( ) :,if model_class . is_training :,0.8820016898747209,22.01137162342707
"def prehook(self, emu, op, eip): if op in self.badops: emu.stopEmu() raise v_exc.BadOpBytes(op.va) if op.mnem in STOS: <mask>: reg = emu.getRegister(envi.archs.i386.REG_EDI) elif self.arch == ""amd64"": reg = emu.getRegister(envi.archs.amd64.REG_RDI) if self.vw.isValidPointer(reg) and self.vw.getLocation(reg) is None: self.vw.makePointer(reg, follow=True)",True,"if self . arch == ""i386"" :","if self . arch == ""i386"" :",0.8820016898747209,100.00000000000004
"def test_len(self): eq = self.assertEqual eq(base64mime.base64_len(""hello""), len(base64mime.encode(""hello"", eol=""""))) for size in range(15): <mask>: bsize = 0 elif size <= 3: bsize = 4 elif size <= 6: bsize = 8 elif size <= 9: bsize = 12 elif size <= 12: bsize = 16 else: bsize = 20 eq(base64mime.base64_len(""x"" * size), bsize)",False,if size == 0 :,if size <= 1 :,0.8820016898747209,19.304869754804482
"def __new__(cls, dependencies): deps = check.list_param(dependencies, ""dependencies"", of_type=DependencyDefinition) seen = {} for dep in deps: key = dep.solid + "":"" + dep.output <mask>: raise DagsterInvalidDefinitionError( 'Duplicate dependencies on solid ""{dep.solid}"" output ""{dep.output}"" ' ""used in the same MultiDependencyDefinition."".format(dep=dep) ) seen[key] = True return super(MultiDependencyDefinition, cls).__new__(cls, deps)",True,if key in seen :,if key in seen :,0.8820016898747209,100.00000000000004
"def get_explanation(self, spec): """"""Expand an explanation."""""" if spec: try: a = self.dns_txt(spec) if len(a) == 1: return str(self.expand(to_ascii(a[0]), stripdot=False)) except PermError: # RFC4408 6.2/4 syntax errors cause exp= to be ignored <mask>: raise # but report in harsh mode for record checking tools pass elif self.strict > 1: raise PermError(""Empty domain-spec on exp="") # RFC4408 6.2/4 empty domain spec is ignored # (unless you give precedence to the grammar). return None",False,if self . strict > 1 :,if self . strict == 1 :,0.8820016898747209,41.11336169005198
"def build(self): if self.args.get(""sle_id""): self.process_sle_against_current_voucher() else: entries_to_fix = self.get_future_entries_to_fix() i = 0 while i < len(entries_to_fix): sle = entries_to_fix[i] i += 1 self.process_sle(sle) <mask>: self.get_dependent_entries_to_fix(entries_to_fix, sle) if self.exceptions: self.raise_exceptions() self.update_bin()",False,if sle . dependant_sle_voucher_detail_no :,"if self . args . get ( ""dependent"" ) :",0.8820016898747209,4.100530090638892
"def ValidateStopLatitude(self, problems): if self.stop_lat is not None: value = self.stop_lat try: if not isinstance(value, (float, int)): self.stop_lat = util.FloatStringToFloat(value, problems) except (ValueError, TypeError): problems.InvalidValue(""stop_lat"", value) del self.stop_lat else: <mask>: problems.InvalidValue(""stop_lat"", value)",False,if self . stop_lat > 90 or self . stop_lat < - 90 :,if not util . IsNumeric ( value ) :,0.8820016898747209,2.308316689352168
"def set(self, obj, **kwargs): """"""Check for missing event functions and substitute these with"""""" """"""the ignore method"""""" ignore = getattr(self, ""ignore"") for k, v in kwargs.iteritems(): setattr(self, k, getattr(obj, v)) <mask>: for k1 in self.combinations[k]: if not hasattr(self, k1): setattr(self, k1, ignore)",True,if k in self . combinations :,if k in self . combinations :,0.8820016898747209,100.00000000000004
"def split(self, duration, include_remainder=True): # Convert seconds to timedelta, if appropriate. duration = _seconds_or_timedelta(duration) if duration <= timedelta(seconds=0): raise ValueError(""cannot call split with a non-positive timedelta"") start = self.start while start < self.end: if start + duration <= self.end: yield MayaInterval(start, start + duration) <mask>: yield MayaInterval(start, self.end) start += duration",False,elif include_remainder :,if include_remainder :,0.8820016898747209,66.87403049764218
"def get_first_field(layout, clz): for layout_object in layout.fields: if issubclass(layout_object.__class__, clz): return layout_object <mask>: gf = get_first_field(layout_object, clz) if gf: return gf",False,"elif hasattr ( layout_object , ""get_field_names"" ) :","if issubclass ( layout_object , Layout ) :",0.8820016898747209,24.795364698947967
"def _getPatternTemplate(pattern, key=None): if key is None: key = pattern if ""%"" not in pattern: key = pattern.upper() template = DD_patternCache.get(key) if not template: <mask>: template = DateEpoch(lineBeginOnly=(key != ""EPOCH"")) elif key in (""TAI64N"", ""{^LN-BEG}TAI64N"", ""^TAI64N""): template = DateTai64n(wordBegin=(""start"" if key != ""TAI64N"" else False)) else: template = DatePatternRegex(pattern) DD_patternCache.set(key, template) return template",True,"if key in ( ""EPOCH"" , ""{^LN-BEG}EPOCH"" , ""^EPOCH"" ) :","if key in ( ""EPOCH"" , ""{^LN-BEG}EPOCH"" , ""^EPOCH"" ) :",0.8820016898747209,100.00000000000004
"def findOwningViewController(self, object): while object: <mask>: description = fb.evaluateExpressionValue(object).GetObjectDescription() print(""Found the owning view controller.\n{}"".format(description)) cmd = 'echo {} | tr -d ""\n"" | pbcopy'.format(object) os.system(cmd) return else: object = self.nextResponder(object) print(""Could not find an owning view controller"")",False,if self . isViewController ( object ) :,if fb . evaluateExpressionValue ( object ) :,0.8820016898747209,38.260294162784454
"def __get_file_by_num(self, num, file_list, idx=0): for element in file_list: if idx == num: return element if element[3] and element[4]: i = self.__get_file_by_num(num, element[3], idx + 1) <mask>: return i idx = i else: idx += 1 return idx",False,"if not isinstance ( i , int ) :",if i :,0.8820016898747209,0.0
"def promtool(**kwargs): key = ""prometheus:promtool"" try: path = pathlib.Path(util.setting(key)) except TypeError: yield checks.Warning( ""Missing setting for %s in %s "" % (key, settings.PROMGEN_CONFIG_FILE), id=""promgen.W001"", ) else: <mask>: yield checks.Warning(""Unable to execute file %s"" % path, id=""promgen.W003"")",False,"if not os . access ( path , os . X_OK ) :",if not os . path . exists ( path ) :,0.8820016898747209,22.69624786949032
"def parse_config(schema, config): schemaparser = ConfigParser() schemaparser.readfp(StringIO(schema)) cfgparser = ConfigParser() cfgparser.readfp(StringIO(config)) result = {} for section in cfgparser.sections(): result_section = {} schema = {} <mask>: schema = dict(schemaparser.items(section)) for key, value in cfgparser.items(section): converter = converters[schema.get(key, ""string"")] result_section[key] = converter(value) result[section] = result_section return result",False,if section in schemaparser . sections ( ) :,if section in schemaparser :,0.8820016898747209,31.772355751081438
"def validate_arguments(args): if args.num_pss < 1: print(""Value error: must have ore than one parameter servers."") exit(1) if not GPU_IDS: num_cpus = multiprocessing.cpu_count() <mask>: print( ""Value error: there are %s available CPUs but you are requiring %s."" % (num_cpus, args.cpu_trainers) ) exit(1) if not os.path.isfile(args.file): print(""Value error: model trainning file does not exist"") exit(1)",False,if args . cpu_trainers > num_cpus :,if num_cpus != args . cpu_trainers :,0.8820016898747209,44.833867003844574
"def infer_dataset_impl(path): if IndexedRawTextDataset.exists(path): return ""raw"" elif IndexedDataset.exists(path): with open(index_file_path(path), ""rb"") as f: magic = f.read(8) if magic == IndexedDataset._HDR_MAGIC: return ""cached"" <mask>: return ""mmap"" else: return None elif FastaDataset.exists(path): return ""fasta"" else: return None",False,elif magic == MMapIndexedDataset . Index . _HDR_MAGIC [ : 8 ] :,elif magic == IndexedDataset . _MAGIC :,0.8820016898747209,16.58011393376885
"def _add_resource_group(obj): if isinstance(obj, list): for array_item in obj: _add_resource_group(array_item) elif isinstance(obj, dict): try: if ""resourcegroup"" not in [x.lower() for x in obj.keys()]: <mask>: obj[""resourceGroup""] = _parse_id(obj[""id""])[""resource-group""] except (KeyError, IndexError, TypeError): pass for item_key in obj: if item_key != ""sourceVault"": _add_resource_group(obj[item_key])",False,"if obj [ ""id"" ] :","if ""id"" in obj :",0.8820016898747209,25.201472805660515
"def reformatBody(self, event=None): """"""Reformat all paragraphs in the body."""""" c, p = self, self.p undoType = ""reformat-body"" w = c.frame.body.wrapper c.undoer.beforeChangeGroup(p, undoType) w.setInsertPoint(0) while 1: progress = w.getInsertPoint() c.reformatParagraph(event, undoType=undoType) ins = w.getInsertPoint() s = w.getAllText() w.setInsertPoint(ins) <mask>: break c.undoer.afterChangeGroup(p, undoType)",False,if ins <= progress or ins >= len ( s ) :,"if s == """" :",0.8820016898747209,3.612710856430898
"def make_sources(project: RootDependency) -> str: content = [] if project.readme: content.append(project.readme.path.name) <mask>: content.append(project.readme.to_rst().path.name) path = project.package.path for fname in (""setup.cfg"", ""setup.py""): if (path / fname).exists(): content.append(fname) for package in chain(project.package.packages, project.package.data): for fpath in package: fpath = fpath.relative_to(project.package.path) content.append(""/"".join(fpath.parts)) return ""\n"".join(content)",False,"if project . readme . markup != ""rst"" :",if project . readme . to_rst ( ) :,0.8820016898747209,34.84694488743309
"def __init__(self, response): error = ""{} {}"".format(response.status_code, response.reason) extra = [] try: response_json = response.json() <mask>: error = "" "".join(error[""message""] for error in response_json[""error_list""]) extra = [ error[""extra""] for error in response_json[""error_list""] if ""extra"" in error ] except JSONDecodeError: pass super().__init__(response=response, error=error, extra=extra)",True,"if ""error_list"" in response_json :","if ""error_list"" in response_json :",0.8820016898747209,100.00000000000004
"def handle_event(self, fileno=None, events=None): if self._state == RUN: <mask>: self._it = self._process_result(0) # non-blocking try: next(self._it) except (StopIteration, CoroStop): self._it = None",True,if self . _it is None :,if self . _it is None :,0.8820016898747209,100.00000000000004
"def find_query(self, needle, haystack): try: import pinyin haystack_py = pinyin.get_initial(haystack, """") needle_len = len(needle) start = 0 result = [] while True: found = haystack_py.find(needle, start) <mask>: break result.append((found, needle_len)) start = found + needle_len return result except: return None",False,if found < 0 :,if found == - 1 :,0.8820016898747209,14.535768424205482
"def decorated_function(*args, **kwargs): rv = f(*args, **kwargs) if ""Last-Modified"" not in rv.headers: try: result = date if callable(result): result = result(rv) if not isinstance(result, basestring): from werkzeug.http import http_date result = http_date(result) <mask>: rv.headers[""Last-Modified""] = result except Exception: logging.getLogger(__name__).exception( ""Error while calculating the lastmodified value for response {!r}"".format( rv ) ) return rv",True,if result :,if result :,0.8820016898747209,0.0
"def check_require(require_modules, require_lines): for require_module in require_modules: st = try_import(require_module) if st == 0: continue <mask>: print( ""installed {}: {}\n"".format( require_module, require_lines[require_module] ) ) elif st == 2: print( ""failed installed {}: {}\n"".format( require_module, require_lines[require_module] ) )",False,elif st == 1 :,if st == 1 :,0.8820016898747209,75.98356856515926
"def bundle_directory(self, dirpath): """"""Bundle all modules/packages in the given directory."""""" dirpath = os.path.abspath(dirpath) for nm in os.listdir(dirpath): nm = _u(nm) if nm.startswith("".""): continue itempath = os.path.join(dirpath, nm) if os.path.isdir(itempath): if os.path.exists(os.path.join(itempath, ""__init__.py"")): self.bundle_package(itempath) <mask>: self.bundle_module(itempath)",False,"elif nm . endswith ( "".py"" ) :","elif os . path . exists ( os . path . join ( itempath , ""__init__.py"" ) ) :",0.8820016898747209,11.434338200880834
"def _find_root(): test_dirs = [""Src"", ""Build"", ""Package"", ""Tests"", ""Util""] root = os.getcwd() test = all([os.path.exists(os.path.join(root, x)) for x in test_dirs]) while not test: last_root = root root = os.path.dirname(root) <mask>: raise Exception(""Root not found"") test = all([os.path.exists(os.path.join(root, x)) for x in test_dirs]) return root",True,if root == last_root :,if root == last_root :,0.8820016898747209,100.00000000000004
"def findMarkForUnitTestNodes(self): """"""return the position of *all* non-ignored @mark-for-unit-test nodes."""""" c = self.c p, result, seen = c.rootPosition(), [], [] while p: if p.v in seen: p.moveToNodeAfterTree() else: seen.append(p.v) if g.match_word(p.h, 0, ""@ignore""): p.moveToNodeAfterTree() <mask>: result.append(p.copy()) p.moveToNodeAfterTree() else: p.moveToThreadNext() return result",False,"elif p . h . startswith ( ""@mark-for-unit-tests"" ) :","elif g . match_word ( p . h , 0 , ""@mark-for-unit-test"" ) :",0.8820016898747209,15.593439508212386
"def startTagFrameset(self, token): self.parser.parseError(""unexpected-start-tag"", {""name"": ""frameset""}) if len(self.tree.openElements) == 1 or self.tree.openElements[1].name != ""body"": assert self.parser.innerHTML elif not self.parser.framesetOK: pass else: <mask>: self.tree.openElements[1].parent.removeChild(self.tree.openElements[1]) while self.tree.openElements[-1].name != ""html"": self.tree.openElements.pop() self.tree.insertElement(token) self.parser.phase = self.parser.phases[""inFrameset""]",False,if self . tree . openElements [ 1 ] . parent :,"if self . tree . openElements [ 1 ] . name == ""body"" :",0.8820016898747209,55.81600587827485
"def try_split(self, split_text: List[str]): ret = [] for i in split_text: <mask>: continue val = int(i, 2) if val > 255 or val < 0: return None ret.append(val) if len(ret) != 0: ret = bytes(ret) logger.debug(f""binary successful, returning {ret.__repr__()}"") return ret",False,if len ( i ) == 0 :,"if i == """" :",0.8820016898747209,12.411264901419441
"def generator(self, data): for sock in data: <mask>: offset = sock.obj_offset else: offset = sock.obj_vm.vtop(sock.obj_offset) yield ( 0, [ Address(offset), int(sock.Pid), int(sock.LocalPort), int(sock.Protocol), str(protos.protos.get(sock.Protocol.v(), ""-"")), str(sock.LocalIpAddress), str(sock.CreateTime), ], )",False,if not self . _config . PHYSICAL_OFFSET :,"if isinstance ( sock , VtopSocket ) :",0.8820016898747209,4.513617516969122
"def __init__(self, num_bits=4, always_apply=False, p=0.5): super(Posterize, self).__init__(always_apply, p) if isinstance(num_bits, (list, tuple)): <mask>: self.num_bits = [to_tuple(i, 0) for i in num_bits] else: self.num_bits = to_tuple(num_bits, 0) else: self.num_bits = to_tuple(num_bits, num_bits)",False,if len ( num_bits ) == 3 :,if len ( num_bits ) == 2 :,0.8820016898747209,80.70557274927978
"def tearDown(self): """"""Just in case yn00 creates some junk files, do a clean-up."""""" del_files = [self.out_file, ""2YN.dN"", ""2YN.dS"", ""2YN.t"", ""rst"", ""rst1"", ""rub""] for filename in del_files: <mask>: os.remove(filename) if os.path.exists(self.working_dir): for filename in os.listdir(self.working_dir): filepath = os.path.join(self.working_dir, filename) os.remove(filepath) os.rmdir(self.working_dir)",True,if os . path . exists ( filename ) :,if os . path . exists ( filename ) :,0.8820016898747209,100.00000000000004
"def reverse_search_history(self, searchfor, startpos=None): if startpos is None: startpos = self.history_cursor if _ignore_leading_spaces: res = [ (idx, line.lstrip()) for idx, line in enumerate(self.history[startpos:0:-1]) <mask>: ] else: res = [ (idx, line) for idx, line in enumerate(self.history[startpos:0:-1]) if line.startswith(searchfor) ] if res: self.history_cursor -= res[0][0] return res[0][1].get_line_text() return """"",False,if line . lstrip ( ) . startswith ( searchfor . lstrip ( ) ),if line . startswith ( searchfor ),0.8820016898747209,18.962297349534435
"def ComboBoxDroppedHeightTest(windows): ""Check if each combobox height is the same as the reference"" bugs = [] for win in windows: if not win.ref: continue <mask>: continue if win.DroppedRect().height() != win.ref.DroppedRect().height(): bugs.append( ( [ win, ], {}, testname, 0, ) ) return bugs",False,"if win . Class ( ) != ""ComboBox"" or win . ref . Class ( ) != ""ComboBox"" :",if win . ref . OverlappedRect ( ) . height ( ) == 0 :,0.8820016898747209,15.414969183668543
"def get_changed(self): if self._is_expression(): result = self._get_node_text(self.ast) if result == self.source: return None return result else: collector = codeanalyze.ChangeCollector(self.source) last_end = -1 for match in self.matches: start, end = match.get_region() <mask>: if not self._is_expression(): continue last_end = end replacement = self._get_matched_text(match) collector.add_change(start, end, replacement) return collector.get_changed()",False,if start < last_end :,if end > last_end :,0.8820016898747209,43.47208719449914
"def unpickle_from_file(file_path, gzip=False): """"""Unpickle obj from file_path with gzipping."""""" with tf.io.gfile.GFile(file_path, ""rb"") as f: <mask>: obj = pickle.load(f) else: with gzip_lib.GzipFile(fileobj=f, compresslevel=2) as gzipf: obj = pickle.load(gzipf) return obj",False,if not gzip :,if gzip :,0.8820016898747209,0.0
"def get_user_context(request, escape=False): if isinstance(request, HttpRequest): user = getattr(request, ""user"", None) result = {""ip_address"": request.META[""REMOTE_ADDR""]} if user and user.is_authenticated(): result.update( { ""email"": user.email, ""id"": user.id, } ) <mask>: result[""name""] = user.name else: result = {} return mark_safe(json.dumps(result))",False,if user . name :,elif escape :,0.8820016898747209,0.0
"def get_item_address(self, item): """"""Get an item's address as a collection of names"""""" result = [] while True: name = self.tree_ctrl.GetItemPyData(item) <mask>: break else: result.insert(0, name) item = self.tree_ctrl.GetItemParent(item) return result",True,if name is None :,if name is None :,0.8820016898747209,100.00000000000004
"def closest_unseen(self, row1, col1, filter=None): # find the closest unseen from this row/col min_dist = maxint closest_unseen = None for row in range(self.height): for col in range(self.width): if filter is None or (row, col) not in filter: <mask>: dist = self.distance(row1, col1, row, col) if dist < min_dist: min_dist = dist closest_unseen = (row, col) return closest_unseen",False,if self . map [ row ] [ col ] == UNSEEN :,"if self . is_closest_unseen ( row1 , col1 , row , col ) :",0.8820016898747209,9.507244120026236
"def log_graph(self, model: LightningModule, input_array=None): if self._log_graph: <mask>: input_array = model.example_input_array if input_array is not None: input_array = model._apply_batch_transfer_handler(input_array) self.experiment.add_graph(model, input_array) else: rank_zero_warn( ""Could not log computational graph since the"" "" `model.example_input_array` attribute is not set"" "" or `input_array` was not given"", UserWarning, )",True,if input_array is None :,if input_array is None :,0.8820016898747209,100.00000000000004
"def get_scene_exceptions_by_season(self, season=-1): scene_exceptions = [] for scene_exception in self.scene_exceptions: if not len(scene_exception) == 2: continue scene_name, scene_season = scene_exception.split(""|"") <mask>: scene_exceptions.append(scene_name) return scene_exceptions",False,if season == scene_season :,if scene_season == season :,0.8820016898747209,39.28146509005134
def _clean_temp_files(): for pattern in _temp_files: for path in glob.glob(pattern): <mask>: os.remove(path) else: shutil.rmtree(path),False,if os . path . islink ( path ) or os . path . isfile ( path ) :,if os . path . exists ( path ) :,0.8820016898747209,26.753788181976983
"def wait_for_completion(self, job_id, offset, max_results, start_time, timeout): """"""Wait for job completion and return the first page."""""" while True: result = self.get_query_results( job_id=job_id, page_token=None, start_index=offset, max_results=max_results ) <mask>: return result if (time.time() - start_time) > timeout: raise Exception( ""Timeout: the query doesn't finish within %d seconds."" % timeout ) time.sleep(1)",False,"if result [ ""jobComplete"" ] :",if result :,0.8820016898747209,0.0
"def get_data(self, element, ranges, style): <mask>: groups = element.groupby(element.kdims).items() else: groups = [(element.label, element)] plots = [] axis = ""x"" if self.invert_axes else ""y"" for key, group in groups: if element.kdims: label = "","".join([d.pprint_value(v) for d, v in zip(element.kdims, key)]) else: label = key data = {axis: group.dimension_values(group.vdims[0]), ""name"": label} plots.append(data) return plots",True,if element . kdims :,if element . kdims :,0.8820016898747209,100.00000000000004
"def get_files(self, dirname): if not self._data.has_key(dirname): self._create(dirname) else: new_time = self._changed(dirname) <mask>: self._update(dirname, new_time) dcLog.debug(""==> "" + ""\t\n"".join(self._data[dirname][""flist""])) return self._data[dirname][""flist""]",True,if new_time :,if new_time :,0.8820016898747209,100.00000000000004
"def __init__(self, dir): self.module_names = set() for name in os.listdir(dir): <mask>: self.module_names.add(name[:-3]) elif ""."" not in name: self.module_names.add(name)",True,"if name . endswith ( "".py"" ) :","if name . endswith ( "".py"" ) :",0.8820016898747209,100.00000000000004
"def logic(): for i in range(100): yield clock.posedge, reset.negedge <mask>: count.next = 0 else: if enable: count.next = (count + 1) % n raise StopSimulation",False,if reset == ACTIVE_LOW :,if enable :,0.8820016898747209,0.0
"def sortkeypicker(keynames): negate = set() for i, k in enumerate(keynames): if k[:1] == ""-"": keynames[i] = k[1:] negate.add(k[1:]) def getit(adict): composite = [adict[k] for k in keynames] for i, (k, v) in enumerate(zip(keynames, composite)): <mask>: composite[i] = -v return composite return getit",False,if k in negate :,if k not in negate :,0.8820016898747209,37.99178428257963
"def show_image(self, wnd_name, img): if wnd_name in self.named_windows: <mask>: self.named_windows[wnd_name] = 1 self.on_create_window(wnd_name) if wnd_name in self.capture_mouse_windows: self.capture_mouse(wnd_name) self.on_show_image(wnd_name, img) else: print(""show_image: named_window "", wnd_name, "" not found."")",True,if self . named_windows [ wnd_name ] == 0 :,if self . named_windows [ wnd_name ] == 0 :,0.8820016898747209,100.00000000000004
"def check_action_permitted(self): if ( self._action == ""sts:GetCallerIdentity"" ): # always allowed, even if there's an explicit Deny for it return True policies = self._access_key.collect_policies() permitted = False for policy in policies: iam_policy = IAMPolicy(policy) permission_result = iam_policy.is_action_permitted(self._action) if permission_result == PermissionResult.DENIED: self._raise_access_denied() <mask>: permitted = True if not permitted: self._raise_access_denied()",False,elif permission_result == PermissionResult . PERMITTED :,if permission_result == PermissionResult . PERMITTED :,0.8820016898747209,88.01117367933934
"def _limit_value(key, value, config): if config[key].get(""upper_limit""): limit = config[key][""upper_limit""] # auto handle datetime if isinstance(value, datetime) and isinstance(limit, timedelta): if config[key][""inverse""] is True: <mask>: value = datetime.now() - limit else: if (datetime.now() + limit) < value: value = datetime.now() + limit elif value > limit: value = limit return value",True,if ( datetime . now ( ) - limit ) > value :,if ( datetime . now ( ) - limit ) > value :,0.8820016898747209,100.00000000000004
"def replace_dataset_ids(path, key, value): """"""Exchanges dataset_ids (HDA, LDA, HDCA, not Dataset) in input_values with dataset ids used in job."""""" current_case = input_values if key == ""id"": for i, p in enumerate(path): if isinstance(current_case, (list, dict)): current_case = current_case[p] <mask>: return key, translate_values.get(current_case[""id""], value) return key, value",False,"if src == current_case . get ( ""src"" ) :",if i == len ( path ) :,0.8820016898747209,7.662098194868579
"def load_ext(name, funcs): ExtModule = namedtuple(""ExtModule"", funcs) ext_list = [] lib_root = os.path.dirname(os.path.dirname(os.path.realpath(__file__))) for fun in funcs: <mask>: ext_list.append(extension.load(fun, name, lib_dir=lib_root).op) else: ext_list.append(extension.load(fun, name, lib_dir=lib_root).op_) return ExtModule(*ext_list)",False,"if fun in [ ""nms"" , ""softnms"" ] :","if name == ""ext"" :",0.8820016898747209,4.18031138310865
"def execute_action(self): selected_actions = self.model_action.get_selected_results_with_index() if selected_actions and self.args_for_action: for name, _, act_idx in selected_actions: try: action = self.actions[act_idx] <mask>: action.act([arg for arg, _, _ in self.args_for_action], self) except Exception as e: debug.log(""execute_action"", e)",False,if action :,if action . act :,0.8820016898747209,23.643540225079384
"def __getattr__(self, attr): proxy = self.__proxy if proxy and hasattr(proxy, attr): return getattr(proxy, attr) attrmap = self.__attrmap if attr in attrmap: source = attrmap[attr] <mask>: value = source() else: value = _import_object(source) setattr(self, attr, value) self.__log.debug(""loaded lazy attr %r: %r"", attr, value) return value raise AttributeError(""'module' object has no attribute '%s'"" % (attr,))",True,if callable ( source ) :,if callable ( source ) :,0.8820016898747209,100.00000000000004
"def forward(self, x): # BxT -> BxCxT x = x.unsqueeze(1) for conv in self.conv_layers: residual = x x = conv(x) <mask>: tsz = x.size(2) r_tsz = residual.size(2) residual = residual[..., :: r_tsz // tsz][..., :tsz] x = (x + residual) * self.residual_scale if self.log_compression: x = x.abs() x = x + 1 x = x.log() return x",False,if self . skip_connections and x . size ( 1 ) == residual . size ( 1 ) :,if self . use_r_tsz :,0.8820016898747209,4.756448542858119
"def __Prefix_Step2a(self, token): for prefix in self.__prefix_step2a: <mask>: token = token[len(prefix) :] self.prefix_step2a_success = True break return token",False,if token . startswith ( prefix ) and len ( token ) > 5 :,if token . startswith ( prefix ) :,0.8820016898747209,36.24372413507827
"def is_valid(sample): if sample is None: return False if isinstance(sample, tuple): for s in sample: if s is None: return False elif isinstance(s, np.ndarray) and s.size == 0: return False <mask>: return False return True",False,"elif isinstance ( s , collections . abc . Sequence ) and len ( s ) == 0 :","elif isinstance ( s , ( list , tuple ) ) and s . shape [ 0 ] != 1 :",0.8820016898747209,22.115966812339867
"def get_all_comments(self, gallery_id, post_no, comment_cnt): comment_page_cnt = (comment_cnt - 1) // self.options.comments_per_page + 1 comments = [] headers = {""X-Requested-With"": ""XMLHttpRequest""} data = {""ci_t"": self._session.cookies[""ci_c""], ""id"": gallery_id, ""no"": post_no} for i in range(comment_page_cnt): data[""comment_page""] = i + 1 response = self.request_comment(headers, data) batch = self.parse_comments(response.text) <mask>: break comments = batch + comments return comments",True,if not batch :,if not batch :,0.8820016898747209,100.00000000000004
def run_on_module(self): try: self.module_base.disable(self.opts.module_spec) except dnf.exceptions.MarkingErrors as e: <mask>: if e.no_match_group_specs or e.error_group_specs: raise e if ( e.module_depsolv_errors and e.module_depsolv_errors[1] != libdnf.module.ModulePackageContainer.ModuleErrorType_ERROR_IN_DEFAULTS ): raise e logger.error(str(e)),False,if self . base . conf . strict :,if e . module_spec is not None :,0.8820016898747209,5.522397783539471
"def find_field_notnull_differ(self, meta, table_description, table_name): if not self.can_detect_notnull_differ: return for field in all_local_fields(meta): attname = field.db_column or field.attname <mask>: continue null = self.get_field_db_nullable(field, table_name) if field.null != null: action = field.null and ""DROP"" or ""SET"" self.add_difference(""notnull-differ"", table_name, attname, action)",False,"if ( table_name , attname ) in self . new_db_fields :",if attname == table_description :,0.8820016898747209,4.5088043638672195
"def _change_moving_module(self, changes, dest): if not self.source.is_folder(): pymodule = self.pycore.resource_to_pyobject(self.source) source = self.import_tools.relatives_to_absolutes(pymodule) pymodule = self.tools.new_pymodule(pymodule, source) source = self._change_occurrences_in_module(dest, pymodule) source = self.tools.new_source(pymodule, source) <mask>: changes.add_change(ChangeContents(self.source, source))",False,if source != self . source . read ( ) :,if self . source . is_folder ( ) :,0.8820016898747209,32.03558799120807
"def get(quality_name): """"""Returns a quality object based on canonical quality name."""""" found_components = {} for part in quality_name.lower().split(): component = _registry.get(part) <mask>: raise ValueError(""`%s` is not a valid quality string"" % part) if component.type in found_components: raise ValueError( ""`%s` cannot be defined twice in a quality"" % component.type ) found_components[component.type] = component if not found_components: raise ValueError(""No quality specified"") result = Quality() for type, component in found_components.items(): setattr(result, type, component) return result",True,if not component :,if not component :,0.8820016898747209,100.00000000000004
def _unselected(self): selected = self._selected k = 0 z = selected[k] k += 1 for i in range(self._n): if i == z: <mask>: z = selected[k] k += 1 else: z = -1 else: yield i,False,if k < len ( selected ) :,if selected [ k ] > 0 :,0.8820016898747209,7.809849842300637
"def render_headers(self) -> bytes: if not hasattr(self, ""_headers""): parts = [ b""Content-Disposition: form-data; "", format_form_param(""name"", self.name), ] <mask>: filename = format_form_param(""filename"", self.filename) parts.extend([b""; "", filename]) if self.content_type is not None: content_type = self.content_type.encode() parts.extend([b""\r\nContent-Type: "", content_type]) parts.append(b""\r\n\r\n"") self._headers = b"""".join(parts) return self._headers",False,if self . filename :,if self . filename is not None :,0.8820016898747209,36.55552228545123
"def app_middleware(next, root, info, **kwargs): app_auth_header = ""HTTP_AUTHORIZATION"" prefix = ""bearer"" request = info.context if request.path == API_PATH: if not hasattr(request, ""app""): request.app = None auth = request.META.get(app_auth_header, """").split() <mask>: auth_prefix, auth_token = auth if auth_prefix.lower() == prefix: request.app = SimpleLazyObject(lambda: get_app(auth_token)) return next(root, info, **kwargs)",False,if len ( auth ) == 2 :,if auth :,0.8820016898747209,0.0
"def _shortest_hypernym_paths(self, simulate_root): if self.offset == ""00000000"": return {self: 0} queue = deque([(self, 0)]) path = {} while queue: s, depth = queue.popleft() <mask>: continue path[s] = depth depth += 1 queue.extend((hyp, depth) for hyp in s._hypernyms()) if simulate_root: root = Synset(self._wordnet_corpus_reader, None, self.pos(), ""00000000"", """") path[root] = max(path.values()) + 1 return path",False,if s in path :,if s == self :,0.8820016898747209,17.965205598154213
"def _populate_class_variables(): lookup = {} reverse_lookup = {} characters_for_re = [] for codepoint, name in list(codepoint2name.items()): character = chr(codepoint) <mask>: # There's no point in turning the quotation mark into # &quot;, unless it happens within an attribute value, which # is handled elsewhere. characters_for_re.append(character) lookup[character] = name # But we do want to turn &quot; into the quotation mark. reverse_lookup[name] = character re_definition = ""[%s]"" % """".join(characters_for_re) return lookup, reverse_lookup, re.compile(re_definition)",False,if codepoint != 34 :,if character not in lookup :,0.8820016898747209,9.652434877402245
"def prepare_data_status(self, view: sublime.View, data: Dict[str, Any]) -> Any: """"""Prepare the returned data for status"""""" if ( data[""success""] and ""No docstring"" not in data[""doc""] and data[""doc""] != ""list\n"" ): self.signature = data[""doc""] <mask>: return try: self.signature = self.signature.splitlines()[2] except KeyError: return return self._show_status(view)",False,if self . _signature_excluded ( self . signature ) :,if not self . signature :,0.8820016898747209,10.690302152100664
"def _setup_once_tables(cls): if cls.run_define_tables == ""once"": cls.define_tables(cls.metadata) <mask>: cls.metadata.create_all(cls.bind) cls.tables.update(cls.metadata.tables)",False,"if cls . run_create_tables == ""once"" :",if cls . bind :,0.8820016898747209,7.063006710882745
"def _send_recursive(self, files): for base in files: <mask>: # filename mixed into the bunch self._send_files([base]) continue last_dir = asbytes(base) for root, dirs, fls in os.walk(base): self._chdir(last_dir, asbytes(root)) self._send_files([os.path.join(root, f) for f in fls]) last_dir = asbytes(root) # back out of the directory for i in range(len(os.path.split(last_dir))): self._send_popd()",False,if not os . path . isdir ( base ) :,if not os . path . isfile ( base ) :,0.8820016898747209,70.16879391277372
"def __init__(self, *args, **kwargs): super().__init__(*args, **kwargs) # Automatically register models if required. if not is_registered(self.model): inline_fields = () for inline in self.inlines: inline_model, follow_field = self._reversion_introspect_inline_admin(inline) if inline_model: self._reversion_autoregister(inline_model, ()) <mask>: inline_fields += (follow_field,) self._reversion_autoregister(self.model, inline_fields)",True,if follow_field :,if follow_field :,0.8820016898747209,100.00000000000004
"def dispatch_hook(key, hooks, hook_data, **kwargs): """"""Dispatches a hook dictionary on a given piece of data."""""" hooks = hooks or dict() hooks = hooks.get(key) if hooks: if hasattr(hooks, ""__call__""): hooks = [hooks] for hook in hooks: _hook_data = hook(hook_data, **kwargs) <mask>: hook_data = _hook_data return hook_data",False,if _hook_data is not None :,if _hook_data :,0.8820016898747209,48.23560797692261
"def __call__(self, image, crop=True): if isinstance(image, PTensor): return self.crop_to_output( numpy_to_paddle(self(paddle_to_numpy(image), crop=False)) ) else: warp = cv.warpAffine( image, self.transform_matrix, image.shape[1::-1], borderMode=cv.BORDER_REPLICATE, ) <mask>: return self.crop_to_output(warp) else: return warp",True,if crop :,if crop :,0.8820016898747209,0.0
"def _analyze(self): lines = open(self.log_path, ""r"").readlines() prev_line = None for line in lines: <mask>: self.errors.append(line[len(""ERROR:"") :].strip()) elif line.startswith(""FAIL:"") and prev_line and prev_line.startswith(""=""): self.failures.append(line[len(""FAIL:"") :].strip()) prev_line = line",True,"if line . startswith ( ""ERROR:"" ) and prev_line and prev_line . startswith ( ""="" ) :","if line . startswith ( ""ERROR:"" ) and prev_line and prev_line . startswith ( ""="" ) :",0.8820016898747209,100.00000000000004
"def end(self, name): self.soup.endData() completed_tag = self.soup.tagStack[-1] namespace, name = self._getNsTag(name) nsprefix = None if namespace is not None: for inverted_nsmap in reversed(self.nsmaps): <mask>: nsprefix = inverted_nsmap[namespace] break self.soup.handle_endtag(name, nsprefix) if len(self.nsmaps) > 1: # This tag, or one of its parents, introduced a namespace # mapping, so pop it off the stack. self.nsmaps.pop()",False,if inverted_nsmap is not None and namespace in inverted_nsmap :,if namespace in inverted_nsmap :,0.8820016898747209,30.93485033266056
"def _bind_parameters(operation, parameters): # inspired by MySQL Python Connector (conversion.py) string_parameters = {} for (name, value) in parameters.iteritems(): if value is None: string_parameters[name] = ""NULL"" <mask>: string_parameters[name] = ""'"" + _escape(value) + ""'"" else: string_parameters[name] = str(value) return operation % string_parameters",False,"elif isinstance ( value , basestring ) :","elif isinstance ( value , str ) :",0.8820016898747209,59.4603557501361
"def plugin_on_song_ended(self, song, skipped): if song is not None: rating = song(""~#rating"") invrating = 1.0 - rating delta = min(rating, invrating) / 2.0 <mask>: rating -= delta else: rating += delta song[""~#rating""] = rating",True,if skipped :,if skipped :,0.8820016898747209,0.0
"def on_activated_async(self, view): if settings[""modified_lines_only""]: self.freeze_last_version(view) if settings[""enabled""]: match_trailing_spaces(view) # continuously watch view for changes to the visible region <mask>: # track active_views[view.id()] = view.visible_region() self.update_on_region_change(view)",False,if not view . id ( ) in active_views :,if view . visible_region ( ) :,0.8820016898747209,11.113458655312735
"def _notin_text(term, text, verbose=False): index = text.find(term) head = text[:index] tail = text[index + len(term) :] correct_text = head + tail diff = _diff_text(correct_text, text, verbose) newdiff = [u(""%s is contained here:"") % py.io.saferepr(term, maxsize=42)] for line in diff: <mask>: continue if line.startswith(u(""- "")): continue if line.startswith(u(""+ "")): newdiff.append(u("" "") + line[2:]) else: newdiff.append(line) return newdiff",False,"if line . startswith ( u ( ""Skipping"" ) ) :","if line . startswith ( u ( "" "" ) ) :",0.8820016898747209,76.7733168433653
"def delete_all(path): ppath = os.getcwd() os.chdir(path) for fn in glob.glob(""*""): fn_full = os.path.join(path, fn) if os.path.isdir(fn): delete_all(fn_full) elif fn.endswith("".png""): os.remove(fn_full) <mask>: os.remove(fn_full) elif DELETE_ALL_OLD: os.remove(fn_full) os.chdir(ppath) os.rmdir(path)",False,"elif fn . endswith ( "".md"" ) :","elif fn . endswith ( "".png"" ) :",0.8820016898747209,70.16879391277372
"def reward(self): """"""Returns a tuple of sum of raw and processed rewards."""""" raw_rewards, processed_rewards = 0, 0 for ts in self.time_steps: # NOTE: raw_reward and processed_reward are None for the first time-step. if ts.raw_reward is not None: raw_rewards += ts.raw_reward <mask>: processed_rewards += ts.processed_reward return raw_rewards, processed_rewards",True,if ts . processed_reward is not None :,if ts . processed_reward is not None :,0.8820016898747209,100.00000000000004
"def formatmonthname(self, theyear, themonth, withyear=True): with TimeEncoding(self.locale) as encoding: s = month_name[themonth] <mask>: s = s.decode(encoding) if withyear: s = ""%s %s"" % (s, theyear) return '<tr><th colspan=""7"" class=""month"">%s</th></tr>' % s",False,if encoding is not None :,if encoding :,0.8820016898747209,0.0
"def check_digest_auth(user, passwd): """"""Check user authentication using HTTP Digest auth"""""" if request.headers.get(""Authorization""): credentails = parse_authorization_header(request.headers.get(""Authorization"")) <mask>: return response_hash = response( credentails, passwd, dict( uri=request.script_root + request.path, body=request.data, method=request.method, ), ) if credentails.get(""response"") == response_hash: return True return False",True,if not credentails :,if not credentails :,0.8820016898747209,100.00000000000004
"def wrapped(self, request): try: return self._finished except AttributeError: <mask>: if not request.session.shouldfail and not request.session.shouldstop: log.debug( ""%s is still going to be used, not terminating it. "" ""Still in use on:\n%s"", self, pprint.pformat(list(self.node_ids)), ) return log.debug(""Finish called on %s"", self) try: return func(request) finally: self._finished = True",False,if self . node_ids :,if self . _finished :,0.8820016898747209,29.05925408079185
"def run_tests(): # type: () -> None x = 5 with switch(x) as case: if case(0): print(""zero"") print(""zero"") elif case(1, 2): print(""one or two"") <mask>: print(""three or four"") else: print(""default"") print(""another"")",True,"elif case ( 3 , 4 ) :","elif case ( 3 , 4 ) :",0.8820016898747209,100.00000000000004
"def task_done(self): with self._cond: <mask>: raise ValueError(""task_done() called too many times"") if self._unfinished_tasks._semlock._is_zero(): self._cond.notify_all()",False,if not self . _unfinished_tasks . acquire ( False ) :,if self . _unfinished_tasks . _semlock . _is_zero ( ) :,0.8820016898747209,38.05371078682543
"def _set_uid(self, val): if val is not None: <mask>: self.bus.log(""pwd module not available; ignoring uid."", level=30) val = None elif isinstance(val, text_or_bytes): val = pwd.getpwnam(val)[2] self._uid = val",False,if pwd is None :,if not pwd :,0.8820016898747209,16.37226966703825
"def process_tag(hive_name, company, company_key, tag, default_arch): with winreg.OpenKeyEx(company_key, tag) as tag_key: version = load_version_data(hive_name, company, tag, tag_key) if version is not None: # if failed to get version bail major, minor, _ = version arch = load_arch_data(hive_name, company, tag, tag_key, default_arch) if arch is not None: exe_data = load_exe(hive_name, company, company_key, tag) <mask>: exe, args = exe_data return company, major, minor, arch, exe, args",True,if exe_data is not None :,if exe_data is not None :,0.8820016898747209,100.00000000000004
"def run(algs): for alg in algs: vcs = alg.get(""variantcaller"") if vcs: if isinstance(vcs, dict): vcs = reduce(operator.add, vcs.values()) <mask>: vcs = [vcs] return any(vc.startswith(prefix) for vc in vcs if vc)",False,"if not isinstance ( vcs , ( list , tuple ) ) :","elif isinstance ( vcs , list ) :",0.8820016898747209,22.871025343125112
"def wrapper(self, *args, **kwargs): if not self.request.path.endswith(""/""): <mask>: uri = self.request.path + ""/"" if self.request.query: uri += ""?"" + self.request.query self.redirect(uri, permanent=True) return raise HTTPError(404) return method(self, *args, **kwargs)",False,"if self . request . method in ( ""GET"" , ""HEAD"" ) :","if not self . request . path . startswith ( ""/"" ) :",0.8820016898747209,23.827376217791336
"def check_response(self, response): """"""Specialized version of check_response()."""""" for line in response: # Skip blank lines: <mask>: continue if line.startswith(b""OK""): return elif line.startswith(b""Benutzer/Passwort Fehler""): raise BadLogin(line) else: raise FailedPost(""Server returned '%s'"" % six.ensure_text(line))",False,if not line . strip ( ) :,"if line . startswith ( b""No response"" ) :",0.8820016898747209,10.600313379512592
"def Walk(self, hMenu=None): if not hMenu: hMenu = self.handle n = user32.GetMenuItemCount(hMenu) mi = MENUITEMINFO() for i in range(n): mi.fMask = 2 # MIIM_ID user32.GetMenuItemInfoA(hMenu, i, 1, byref(mi)) handle = user32.GetSubMenu(hMenu, i) <mask>: yield handle, self.ListItems(handle) for i in self.Walk(handle): yield i",True,if handle :,if handle :,0.8820016898747209,0.0
"def setSelection(self, labels): input = self.__validateInput(labels) if len(input) == 0 and not self.__allowEmptySelection: return if self.__allowMultipleSelection: self.__selectedLabels[:] = input self.__selectionChanged() else: <mask>: raise RuntimeError( ""Parameter must be single item or a list with one element."" ) else: self.__selectedLabels[:] = input self.__selectionChanged() # Remove all selected labels that are not in the menu, emit signals if necessary and update the button. self.__validateState()",False,if len ( input ) > 1 :,"if not isinstance ( input , list ) :",0.8820016898747209,11.99014838091355
"def _parse(self, engine): """"""Parse the layer."""""" if isinstance(self.args, dict): if ""axis"" in self.args: self.axis = engine.evaluate(self.args[""axis""], recursive=True) if not isinstance(self.axis, int): raise ParsingError('""axis"" must be an integer.') if ""momentum"" in self.args: self.momentum = engine.evaluate(self.args[""momentum""], recursive=True) <mask>: raise ParsingError('""momentum"" must be numeric.')",False,"if not isinstance ( self . momentum , ( int , float ) ) :","if not isinstance ( self . momentum , int ) :",0.8820016898747209,52.81951634615037
"def get_order(self, aBuf): if not aBuf: return -1, 1 # find out current char's byte length first_char = wrap_ord(aBuf[0]) if (0x81 <= first_char <= 0x9F) or (0xE0 <= first_char <= 0xFC): charLen = 2 else: charLen = 1 # return its order if it is hiragana if len(aBuf) > 1: second_char = wrap_ord(aBuf[1]) <mask>: return second_char - 0x9F, charLen return -1, charLen",False,if ( first_char == 202 ) and ( 0x9F <= second_char <= 0xF1 ) :,if ( 0x81 <= second_char <= 0xFC ) or ( 0xE0 <= second_char <= 0xFC ) :,0.8820016898747209,31.685087793658365
"def saveSpecial(self, **kwargs): for kw in SPECIAL_BOOL_LIST + SPECIAL_VALUE_LIST + SPECIAL_LIST_LIST: item = config.get_config(""misc"", kw) value = kwargs.get(kw) msg = item.set(value) <mask>: return badParameterResponse(msg) config.save_config() raise Raiser(self.__root)",True,if msg :,if msg :,0.8820016898747209,0.0
"def sanitize_event_keys(kwargs, valid_keys): # Sanity check: Don't honor keys that we don't recognize. for key in list(kwargs.keys()): <mask>: kwargs.pop(key) # Truncate certain values over 1k for key in [""play"", ""role"", ""task"", ""playbook""]: if isinstance(kwargs.get(""event_data"", {}).get(key), str): if len(kwargs[""event_data""][key]) > 1024: kwargs[""event_data""][key] = Truncator(kwargs[""event_data""][key]).chars( 1024 )",False,if key not in valid_keys :,if key in valid_keys :,0.8820016898747209,61.29752413741059
"def toggleFactorReload(self, value=None): self.serviceFittingOptions[""useGlobalForceReload""] = ( value if value is not None else not self.serviceFittingOptions[""useGlobalForceReload""] ) fitIDs = set() for fit in set(self._loadedFits): <mask>: continue if fit.calculated: fit.factorReload = self.serviceFittingOptions[""useGlobalForceReload""] fit.clearFactorReloadDependentData() fitIDs.add(fit.ID) return fitIDs",False,if fit is None :,if fit . ID in fitIDs :,0.8820016898747209,14.535768424205482
"def closest_unseen(self, row1, col1, filter=None): # find the closest unseen from this row/col min_dist = maxint closest_unseen = None for row in range(self.height): for col in range(self.width): <mask>: if self.map[row][col] == UNSEEN: dist = self.distance(row1, col1, row, col) if dist < min_dist: min_dist = dist closest_unseen = (row, col) return closest_unseen",False,"if filter is None or ( row , col ) not in filter :",if filter and filter [ row ] == UNSEEN :,0.8820016898747209,7.175377580688497
"def getAlphaClone(lookfor, eager=None): if isinstance(lookfor, int): <mask>: item = get_gamedata_session().query(AlphaClone).get(lookfor) else: item = ( get_gamedata_session() .query(AlphaClone) .options(*processEager(eager)) .filter(AlphaClone.ID == lookfor) .first() ) else: raise TypeError(""Need integer as argument"") return item",True,if eager is None :,if eager is None :,0.8820016898747209,100.00000000000004
"def _rle_encode(string): new = b"""" count = 0 for cur in string: if not cur: count += 1 else: <mask>: new += b""\0"" + bytes([count]) count = 0 new += bytes([cur]) return new",True,if count :,if count :,0.8820016898747209,0.0
def result_iterator(): try: for future in fs: <mask>: yield future.result() else: yield future.result(end_time - time.time()) finally: for future in fs: future.cancel(),False,if timeout is None :,if end_time is None :,0.8820016898747209,26.269098944241588
"def _individual_get(self, segment, index_type, index, strictdoc): if index_type == ""val"": for key, value in segment.items(): <mask>: return value if hasattr(key, ""text""): if key.text == index[0]: return value raise Exception(""Invalid state"") elif index_type == ""index"": return segment[index] elif index_type == ""textslice"": return segment[index[0] : index[1]] elif index_type == ""key"": return index[1] if strictdoc else index[0] else: raise Exception(""Invalid state"")",False,if key == index [ 0 ] :,if key . text == index [ 0 ] :,0.8820016898747209,63.15552371794039
"def _reset_sequences(self, db_name): conn = connections[db_name] if conn.features.supports_sequence_reset: sql_list = conn.ops.sequence_reset_by_name_sql( no_style(), conn.introspection.sequence_list() ) <mask>: try: cursor = conn.cursor() for sql in sql_list: cursor.execute(sql) except Exception: transaction.rollback_unless_managed(using=db_name) raise transaction.commit_unless_managed(using=db_name)",True,if sql_list :,if sql_list :,0.8820016898747209,100.00000000000004
"def translate_to_statements(self, statements, conditional_write_vars): lines = [] for stmt in statements: <mask>: self.temporary_vars.add((stmt.var, stmt.dtype)) line = self.translate_statement(stmt) if stmt.var in conditional_write_vars: subs = {} condvar = conditional_write_vars[stmt.var] lines.append(""if %s:"" % condvar) lines.append(indent(line)) else: lines.append(line) return lines",False,"if stmt . op == "":="" and not stmt . var in self . variables :",if stmt . var not in self . temporary_vars :,0.8820016898747209,14.095879498999276
"def _bytecode_filenames(self, py_filenames): bytecode_files = [] for py_file in py_filenames: # Since build_py handles package data installation, the # list of outputs can contain more than just .py files. # Make sure we only report bytecode for the .py files. ext = os.path.splitext(os.path.normcase(py_file))[1] if ext != PYTHON_SOURCE_EXTENSION: continue <mask>: bytecode_files.append(py_file + ""c"") if self.optimize > 0: bytecode_files.append(py_file + ""o"") return bytecode_files",False,if self . compile :,if self . optimize > 0 :,0.8820016898747209,26.269098944241588
"def logic(): for i in range(100): yield clock.posedge, reset.negedge if reset == ACTIVE_LOW: count.next = 0 else: <mask>: count.next = (count + 1) % n raise StopSimulation",False,if enable :,if reset == ACTIVE_HIGH :,0.8820016898747209,6.567274736060395
"def _is_subnet_of(a, b): try: # Always false if one is v4 and the other is v6. <mask>: raise TypeError(""%s and %s are not of the same version"" % (a, b)) return ( b.network_address <= a.network_address and b.broadcast_address >= a.broadcast_address ) except AttributeError: raise TypeError( ""Unable to test subnet containment "" ""between %s and %s"" % (a, b) )",False,if a . _version != b . _version :,if a . version != b . version :,0.8820016898747209,47.269442068339785
"def _filter_paths(basename, path, is_dir, exclude): """""".gitignore style file filtering."""""" for item in exclude: # Items ending in '/' apply only to directories. if item.endswith(""/"") and not is_dir: continue # Items starting with '/' apply to the whole path. # In any other cases just the basename is used. match = path if item.startswith(""/"") else basename <mask>: return True return False",False,"if fnmatch . fnmatch ( match , item . strip ( ""/"" ) ) :",if match . startswith ( item ) :,0.8820016898747209,4.904484887838445
"def __recv_null(self): """"""Receive a null byte."""""" while 1: c = self.sock.recv(1) if c == """": self.close() raise EOFError(""Socket Closed"") <mask>: return",False,"if c == ""\0"" :","if c == """" :",0.8820016898747209,53.137468984124546
"def onMessage(self, payload, isBinary): if isBinary: self.result = ""Expected text message with payload, but got binary."" else: <mask>: self.result = ( ""Expected text message with payload of length %d, but got %d."" % (self.DATALEN, len(payload)) ) else: ## FIXME : check actual content ## self.behavior = Case.OK self.result = ""Received text message of length %d."" % len(payload) self.p.createWirelog = True self.p.sendClose(self.p.CLOSE_STATUS_CODE_NORMAL)",False,if len ( payload ) != self . DATALEN :,if self . DATALEN != len ( payload ) :,0.8820016898747209,39.76353643835254
"def rename_path(self, path, new_path): logger.debug(""rename_path '%s' -> '%s'"" % (path, new_path)) dirs = self.readdir(path) for d in dirs: <mask>: continue d_path = """".join([path, ""/"", d]) d_new_path = """".join([new_path, ""/"", d]) attr = self.getattr(d_path) if stat.S_ISDIR(attr[""st_mode""]): self.rename_path(d_path, d_new_path) else: self.rename_item(d_path, d_new_path) self.rename_item(path, new_path, dir=True)",False,"if d in [ ""."" , "".."" ] :",if d == new_path :,0.8820016898747209,5.773772066582297
"def dir_box_click(self, double): if double: name = self.list_box.get_selected_name() path = os.path.join(self.directory, name) suffix = os.path.splitext(name)[1] <mask>: self.directory = path else: self.double_click_file(name) self.update()",False,if suffix not in self . suffixes and os . path . isdir ( path ) :,"if suffix == "".py"" :",0.8820016898747209,4.661841620661271
"def __getattr__(self, key): try: value = self.__parent.contents[key] except KeyError: pass else: if value is not None: <mask>: return value.mod_ns else: assert isinstance(value, _MultipleClassMarker) return value.attempt_get(self.__parent.path, key) raise AttributeError( ""Module %r has no mapped classes "" ""registered under the name %r"" % (self.__parent.name, key) )",False,"if isinstance ( value , _ModuleMarker ) :","if isinstance ( value , _NamespaceMarker ) :",0.8820016898747209,66.06328636027612
"def poll_thread(): time.sleep(0.5) if process.wait() and process_state: time.sleep(0.25) <mask>: stdout, stderr = process._communicate(None) logger.error( ""Web server process exited unexpectedly"", ""app"", stdout=stdout, stderr=stderr, ) time.sleep(1) restart_server(1)",False,if not check_global_interrupt ( ) :,if not process . is_alive ( ) :,0.8820016898747209,20.556680845025987
"def apply_dateparser_timezone(utc_datetime, offset_or_timezone_abb): for name, info in _tz_offsets: <mask>: tz = StaticTzInfo(name, info[""offset""]) return utc_datetime.astimezone(tz)",False,"if info [ ""regex"" ] . search ( "" %s"" % offset_or_timezone_abb ) :","if info [ ""name"" ] == offset_or_timezone_abb :",0.8820016898747209,35.852944654634165
"def _load_wordlist(filename): if filename is None: return {} path = None for dir in (CONFIG_DIR, ASSETS_DIR): path = os.path.realpath(os.path.join(dir, filename)) <mask>: break words = {} with open(path, encoding=""utf-8"") as f: pairs = [word.strip().rsplit("" "", 1) for word in f] pairs.sort(reverse=True, key=lambda x: int(x[1])) words = {p[0]: int(p[1]) for p in pairs} return words",False,if os . path . exists ( path ) :,if path is None :,0.8820016898747209,5.171845311465849
"def terminate_processes_matching_names(match_strings, kill=False): """"""Terminates processes matching particular names (case sensitive)."""""" if isinstance(match_strings, str): match_strings = [match_strings] for process in psutil.process_iter(): try: process_info = process.as_dict(attrs=[""name"", ""pid""]) process_name = process_info[""name""] except (psutil.AccessDenied, psutil.NoSuchProcess, OSError): continue <mask>: terminate_process(process_info[""pid""], kill)",False,if any ( x == process_name for x in match_strings ) :,if process_name in match_strings :,0.8820016898747209,18.897243358570165
"def has_scheme(self, inp): if ""://"" in inp: return True else: authority = inp.replace(""/"", ""#"").replace(""?"", ""#"").split(""#"")[0] <mask>: _, host_or_port = authority.split("":"", 1) # Assert it's not a port number if re.match(r""^\d+$"", host_or_port): return False else: return False return True",True,"if "":"" in authority :","if "":"" in authority :",0.8820016898747209,100.00000000000004
"def close(self): with BrowserContext._BROWSER_LOCK: BrowserContext._BROWSER_REFCNT -= 1 <mask>: logger.info(""Destroying browser main loop"") BrowserContext._BROWSER_LOOP.destroy() BrowserContext._BROWSER_LOOP = None",True,if BrowserContext . _BROWSER_REFCNT == 0 :,if BrowserContext . _BROWSER_REFCNT == 0 :,0.8820016898747209,100.00000000000004
"def _mock_get_merge_ticks(self, order_book_id_list, trading_date, last_dt=None): for tick in self._ticks: <mask>: continue if ( self.env.data_proxy.get_future_trading_date(tick.datetime).date() != trading_date.date() ): continue if last_dt and tick.datetime <= last_dt: continue yield tick",False,if tick . order_book_id not in order_book_id_list :,if tick . order_book_id_list != order_book_id_list :,0.8820016898747209,69.3395566222006
"def messageSourceStamps(self, source_stamps): text = """" for ss in source_stamps: source = """" <mask>: source += ""[branch %s] "" % ss[""branch""] if ss[""revision""]: source += str(ss[""revision""]) else: source += ""HEAD"" if ss[""patch""] is not None: source += "" (plus patch)"" discriminator = """" if ss[""codebase""]: discriminator = "" '%s'"" % ss[""codebase""] text += ""Build Source Stamp%s: %s\n"" % (discriminator, source) return text",True,"if ss [ ""branch"" ] :","if ss [ ""branch"" ] :",0.8820016898747209,100.00000000000004
"def test_open_read_bytes(self, sftp): """"""Test reading bytes from a file"""""" f = None try: self._create_file(""file"", ""xxx"") f = yield from sftp.open(""file"", ""rb"") self.assertEqual((yield from f.read()), b""xxx"") finally: <mask>: # pragma: no branch yield from f.close() remove(""file"")",True,if f :,if f :,0.8820016898747209,0.0
"def handler(chan, host, port): sock = socket() try: sock.connect((host, port)) except Exception as e: if verbose == True: print(e) return while True: r, w, x = select.select([sock, chan], [], []) if sock in r: data = sock.recv(1024) if len(data) == 0: break chan.send(data) <mask>: data = chan.recv(1024) if len(data) == 0: break sock.send(data) chan.close() sock.close()",False,if chan in r :,elif chan in x :,0.8820016898747209,23.643540225079384
"def detect(get_page): retval = False for vector in WAF_ATTACK_VECTORS: page, headers, code = get_page(get=vector) retval = re.search(r""url\('/ks-waf-error\.png'\)"", page, re.I) is not None <mask>: break return retval",True,if retval :,if retval :,0.8820016898747209,0.0
"def __init__(self, raw): ticker_ticks = {} for tick in raw[""results""]: <mask>: ticker_ticks[tick[""T""]].append(tick) else: ticker_ticks[tick[""T""]] = [tick] super().__init__( {ticker: Aggsv2({""results"": ticks}) for ticker, ticks in ticker_ticks.items()} )",False,"if ticker_ticks . get ( tick [ ""T"" ] ) :","if tick [ ""T"" ] in ticker_ticks :",0.8820016898747209,43.48783281197403
"def _makefiles(self, f): if isinstance(f, dict): for k, v in list(f.items()): <mask>: self.makedir(dirname=k, content=v) elif isinstance(v, str): self.make_file(filename=k, content=v) else: # pragma: nocover raise ValueError(""Unexpected:"", k, v) elif isinstance(f, str): self._make_empty_file(f) elif isinstance(f, list): self.make_list(f) else: # pragma: nocover raise ValueError(""Unknown type:"", f)",False,"if isinstance ( v , list ) :","if isinstance ( v , str ) :",0.8820016898747209,59.4603557501361
"def migrate_command_storage(apps, schema_editor): model = apps.get_model(""terminal"", ""CommandStorage"") init_storage_data(model) setting = get_setting(apps, schema_editor, ""TERMINAL_COMMAND_STORAGE"") if not setting: return values = get_storage_data(setting) for name, meta in values.items(): tp = meta.pop(""TYPE"") <mask>: continue model.objects.create(name=name, type=tp, meta=meta)",False,"if not tp or name in [ ""default"" , ""null"" ] :",if tp is None :,0.8820016898747209,1.5577298727187734
"def build_vertices(self, ulines): vertex_idx = 0 vertices = collections.OrderedDict() for line in ulines: for vt in line: <mask>: continue new_vertex = (vt.u, vt.v, 0.0) if new_vertex in vertices: continue vt.index = vertex_idx vertex_idx += 1 vertices[new_vertex] = 1 return vertex_idx, list(vertices.keys())",False,if vt . replacement is not None :,if vt . index == vertex_idx :,0.8820016898747209,16.784459625186194
"def get_quarantine_count(self): """"""get obj/container/account quarantine counts"""""" qcounts = {""objects"": 0, ""containers"": 0, ""accounts"": 0} qdir = ""quarantined"" for device in os.listdir(self.devices): for qtype in qcounts: qtgt = os.path.join(self.devices, device, qdir, qtype) if os.path.exists(qtgt): linkcount = os.lstat(qtgt).st_nlink <mask>: qcounts[qtype] += linkcount - 2 return qcounts",True,if linkcount > 2 :,if linkcount > 2 :,0.8820016898747209,100.00000000000004
"def _format_arg(self, name, trait_spec, value): if name == ""mask_file"": return """" if name == ""op_string"": <mask>: if isdefined(self.inputs.mask_file): return self.inputs.op_string % self.inputs.mask_file else: raise ValueError(""-k %s option in op_string requires mask_file"") return super(ImageStats, self)._format_arg(name, trait_spec, value)",False,"if ""-k %s"" in self . inputs . op_string :",if self . inputs . op_string :,0.8820016898747209,45.64995457685804
"def _update_theme_style(self, *args): self.line_color_normal = self.theme_cls.divider_color if not any([self.error, self._text_len_error]): if not self.focus: self._current_hint_text_color = self.theme_cls.disabled_hint_text_color self._current_right_lbl_color = self.theme_cls.disabled_hint_text_color <mask>: self._current_error_color = self.theme_cls.disabled_hint_text_color",False,"if self . helper_text_mode == ""persistent"" :",if not self . focus :,0.8820016898747209,5.0887084190633125
"def createFields(self): for item in self.format: <mask>: yield item[0](self, *item[1:-1], **item[-1]) else: yield item[0](self, *item[1:])",False,"if isinstance ( item [ - 1 ] , dict ) :",if len ( item ) > 1 :,0.8820016898747209,8.816389211763417
"def execute(self, statement, arguments=None): while True: try: <mask>: self.cursor.execute(statement, arguments) else: self.cursor.execute(statement) except sqlite3.OperationalError as ex: if ""locked"" not in getSafeExString(ex): raise else: break if statement.lstrip().upper().startswith(""SELECT""): return self.cursor.fetchall()",True,if arguments :,if arguments :,0.8820016898747209,0.0
"def set_income_account_for_fixed_assets(self): disposal_account = depreciation_cost_center = None for d in self.get(""items""): <mask>: if not disposal_account: ( disposal_account, depreciation_cost_center, ) = get_disposal_account_and_cost_center(self.company) d.income_account = disposal_account if not d.cost_center: d.cost_center = depreciation_cost_center",False,if d . is_fixed_asset :,if d . income_account is None :,0.8820016898747209,21.10534063187263
"def _convertNbCharsInNbBits(self, nbChars): nbMinBit = None nbMaxBit = None if nbChars is not None: if isinstance(nbChars, int): nbMinBit = nbChars * 8 nbMaxBit = nbMinBit else: if nbChars[0] is not None: nbMinBit = nbChars[0] * 8 <mask>: nbMaxBit = nbChars[1] * 8 return (nbMinBit, nbMaxBit)",False,if nbChars [ 1 ] is not None :,elif nbChars [ 1 ] is not None :,0.8820016898747209,86.33400213704509
"def _get_service_full_name(self, name, help_command_table): if help_command_table and name not in self._NON_SERVICE_COMMANDS: <mask>: return self._HIGH_LEVEL_SERVICE_FULL_NAMES[name] service = help_command_table.get(name) if service: return service.service_model.metadata[""serviceFullName""]",True,if name in self . _HIGH_LEVEL_SERVICE_FULL_NAMES :,if name in self . _HIGH_LEVEL_SERVICE_FULL_NAMES :,0.8820016898747209,100.00000000000004
"def print_addresses(self): p = 3 tmp_str = ""["" if self.get_len() >= 7: # at least one complete IP address while 1: <mask>: tmp_str += ""#"" tmp_str += self.get_ip_address(p) p += 4 if p >= self.get_len(): break else: tmp_str += "", "" tmp_str += ""] "" if self.get_ptr() % 4: # ptr field should be a multiple of 4 tmp_str += ""nonsense ptr field: %d "" % self.get_ptr() return tmp_str",False,if p + 1 == self . get_ptr ( ) :,if self . get_ptr ( ) % 4 == 0 :,0.8820016898747209,50.317476265301394
"def run(self): for _ in range(self.n): error = True try: self.collection.insert_one({""test"": ""insert""}) error = False except: if not self.expect_exception: raise <mask>: assert error",False,if self . expect_exception :,if not self . expect_exception :,0.8820016898747209,70.71067811865478
"def create_composite_mounter_by_args(args): """"""Creates a CompositeMounter by the images in given args."""""" logging.info(""Mount images..."") mounter = composite_mounter.CompositeMounter() for partition in composite_mounter.SUPPORTED_PARTITIONS: image_source = vars(args)[partition] <mask>: logging.info("" %s=%s"", partition, image_source) mounter.add_by_mount_target(partition, image_source) if mounter.is_empty(): raise RuntimeError(""Must give at least one image source."") return mounter",True,if image_source :,if image_source :,0.8820016898747209,100.00000000000004
"def _get_containing_class(self, pyname): if isinstance(pyname, pynames.DefinedName): scope = pyname.get_object().get_scope() parent = scope.parent <mask>: return parent.pyobject",False,"if parent is not None and parent . get_kind ( ) == ""Class"" :",if parent is not None :,0.8820016898747209,9.110529534877564
"def test_chunkcoding(self): tstring_lines = [] for b in self.tstring: lines = b.split(b""\n"") last = lines.pop() assert last == b"""" lines = [line + b""\n"" for line in lines] tstring_lines.append(lines) for native, utf8 in zip(*tstring_lines): u = self.decode(native)[0] self.assertEqual(u, utf8.decode(""utf-8"")) <mask>: self.assertEqual(native, self.encode(u)[0])",False,if self . roundtriptest :,if self . is_unicode :,0.8820016898747209,26.269098944241588
"def set_default_variants(apps, schema_editor): Product = apps.get_model(""product"", ""Product"") for product in Product.objects.iterator(): first_variant = product.variants.first() <mask>: product.default_variant = first_variant product.save(update_fields=[""default_variant"", ""updated_at""])",False,if first_variant :,if first_variant is not None :,0.8820016898747209,36.55552228545123
"def json(self): try: if self.is_json(): raw_data = self.raw_data() <mask>: raw_data = raw_data.decode(""utf-8"") return json.loads(raw_data) except ValueError: pass",False,"if not isinstance ( raw_data , text_type ) :","if isinstance ( raw_data , bytes ) :",0.8820016898747209,44.360636895626136
"def clear_react(self, message: discord.Message, emoji: MutableMapping = None) -> None: try: await message.clear_reactions() except discord.Forbidden: <mask>: return with contextlib.suppress(discord.HTTPException): async for key in AsyncIter(emoji.values(), delay=0.2): await message.remove_reaction(key, self.bot.user) except discord.HTTPException: return",True,if not emoji :,if not emoji :,0.8820016898747209,100.00000000000004
"def check(self, value): value = String.check(self, value) if isinstance(value, str): value = value.upper() for prefix in (self.prefix, self.prefix.split(""_"", 1)[1]): # e.g. PANGO_WEIGHT_BOLD --> BOLD but also WEIGHT_BOLD --> BOLD if value.startswith(prefix): value = value[len(prefix) :] value = value.lstrip(""_"") <mask>: return getattr(self.group, value) else: raise ValueError(""No such constant: %s_%s"" % (self.prefix, value)) else: return value",False,"if hasattr ( self . group , value ) :",if self . group :,0.8820016898747209,16.62083000646927
"def value(self): quote = False if self.defects: quote = True else: for x in self: if x.token_type == ""quoted-string"": quote = True if quote: pre = post = """" if self[0].token_type == ""cfws"" or self[0][0].token_type == ""cfws"": pre = "" "" <mask>: post = "" "" return pre + quote_string(self.display_name) + post else: return super(DisplayName, self).value",False,"if self [ - 1 ] . token_type == ""cfws"" or self [ - 1 ] [ - 1 ] . token_type == ""cfws"" :","elif self [ 0 ] [ 0 ] . token_type == ""cfws"" or self [ 0 ] [ 0 ] . token_type == ""cfws"" :",0.8820016898747209,66.23922210263933
"def get_drive(self, root_path="""", volume_guid_path=""""): for drive in self.drives: if root_path: config_root_path = drive.get(""root_path"") if config_root_path and root_path == config_root_path: return drive <mask>: config_volume_guid_path = drive.get(""volume_guid_path"") if config_volume_guid_path and config_volume_guid_path == volume_guid_path: return drive",False,elif volume_guid_path :,if volume_guid_path :,0.8820016898747209,80.91067115702207
"def parse_edges(self, pcb): edges = [] drawings = list(pcb.GetDrawings()) bbox = None for m in pcb.GetModules(): for g in m.GraphicalItems(): drawings.append(g) for d in drawings: if d.GetLayer() == pcbnew.Edge_Cuts: parsed_drawing = self.parse_drawing(d) <mask>: edges.append(parsed_drawing) if bbox is None: bbox = d.GetBoundingBox() else: bbox.Merge(d.GetBoundingBox()) if bbox: bbox.Normalize() return edges, bbox",True,if parsed_drawing :,if parsed_drawing :,0.8820016898747209,100.00000000000004
"def to_key(literal_or_identifier): """"""returns string representation of this object"""""" if literal_or_identifier[""type""] == ""Identifier"": return literal_or_identifier[""name""] elif literal_or_identifier[""type""] == ""Literal"": k = literal_or_identifier[""value""] if isinstance(k, float): return unicode(float_repr(k)) elif ""regex"" in literal_or_identifier: return compose_regex(k) elif isinstance(k, bool): return ""true"" if k else ""false"" <mask>: return ""null"" else: return unicode(k)",True,elif k is None :,elif k is None :,0.8820016898747209,100.00000000000004
"def find_multiple_stats(stats, name, _found=None, _on_found=None): if _found is None: _found = [] for child_stats in stats: if child_stats.name == name: _found.append(child_stats) <mask>: _on_found(_found) find_multiple_stats(child_stats, name, _found) return _found",False,if callable ( _on_found ) :,if _on_found is not None :,0.8820016898747209,33.03164318013809
"def _run_generated_code( self, code, globs, locs, fails_under_py3k=True, ): import warnings from zope.interface._compat import PYTHON3 with warnings.catch_warnings(record=True) as log: warnings.resetwarnings() <mask>: exec(code, globs, locs) self.assertEqual(len(log), 0) # no longer warn return True else: try: exec(code, globs, locs) except TypeError: return False else: if fails_under_py3k: self.fail(""Didn't raise TypeError"")",False,if not PYTHON3 :,if PYTHON3 :,0.8820016898747209,0.0
"def _get_node(self, node_id): self.non_terminated_nodes({}) # Side effect: updates cache with self.lock: <mask>: return self.cached_nodes[node_id] instance = ( self.compute.instances() .get( project=self.provider_config[""project_id""], zone=self.provider_config[""availability_zone""], instance=node_id, ) .execute() ) return instance",True,if node_id in self . cached_nodes :,if node_id in self . cached_nodes :,0.8820016898747209,100.00000000000004
"def skip_to_close_match(self): nestedCount = 1 while 1: tok = self.tokenizer.get_next_token() ttype = tok[""style""] if ttype == SCE_PL_UNUSED: return elif self.classifier.is_index_op(tok): tval = tok[""text""] <mask>: if self.opHash[tval][1] == 1: nestedCount += 1 else: nestedCount -= 1 if nestedCount <= 0: break",False,if self . opHash . has_key ( tval ) :,if tval in self . opHash :,0.8820016898747209,14.231728394642222
"def _create_or_get_helper(self, infer_mode: Optional[bool] = None, **kwargs) -> Helper: # Prefer creating a new helper when at least one kwarg is specified. prefer_new = len(kwargs) > 0 kwargs.update(infer_mode=infer_mode) is_training = not infer_mode if infer_mode is not None else self.training helper = self._train_helper if is_training else self._infer_helper if prefer_new or helper is None: helper = self.create_helper(**kwargs) if is_training and self._train_helper is None: self._train_helper = helper <mask>: self._infer_helper = helper return helper",False,elif not is_training and self . _infer_helper is None :,if is_infer and self . _infer_helper is None :,0.8820016898747209,67.71111323098607
"def get_ldset(self, ldsets): ldset = None if self._properties[""ldset_name""] == """": nldset = len(ldsets) if nldset == 0: msg = _(""Logical Disk Set could not be found."") raise exception.NotFound(msg) else: ldset = None else: <mask>: msg = ( _(""Logical Disk Set `%s` could not be found."") % self._properties[""ldset_name""] ) raise exception.NotFound(msg) ldset = ldsets[self._properties[""ldset_name""]] return ldset",False,"if self . _properties [ ""ldset_name"" ] not in ldsets :",if nldset != len ( ldsets ) :,0.8820016898747209,2.882738686795162
"def calc_fractal_serial(q, maxiter): # calculate z using pure python on a numpy array # note that, unlike the other two implementations, # the number of iterations per point is NOT constant z = np.zeros(q.shape, complex) output = np.resize( np.array( 0, ), q.shape, ) for i in range(len(q)): for iter in range(maxiter): z[i] = z[i] * z[i] + q[i] <mask>: output[i] = iter break return output",False,if abs ( z [ i ] ) > 2.0 :,if z [ i ] > 0.0 :,0.8820016898747209,27.329052280893862
"def _verifySubs(self): for inst in self.subs: if not isinstance(inst, (_Block, _Instantiator, Cosimulation)): raise BlockError(_error.ArgType % (self.name,)) <mask>: if not inst.modctxt: raise BlockError(_error.InstanceError % (self.name, inst.callername))",False,"if isinstance ( inst , ( _Block , _Instantiator ) ) :","if isinstance ( inst , _Block ) :",0.8820016898747209,34.11558250990524
"def walks_generator(): if filelist is not None: bucket = [] for filename in filelist: with io.open(filename) as inf: for line in inf: walk = [int(x) for x in line.strip(""\n"").split("" "")] bucket.append(walk) <mask>: yield bucket bucket = [] if len(bucket): yield bucket else: for _ in range(epoch): for nodes in graph.node_batch_iter(batch_size): walks = graph.random_walk(nodes, walk_len) yield walks",False,if len ( bucket ) == batch_size :,if len ( bucket ) :,0.8820016898747209,34.56232340002788
def _traverse(op): if op in visited: return visited.add(op) if tag.is_injective(op.tag): if op not in s.outputs: s[op].compute_inline() for tensor in op.input_tensors: <mask>: _traverse(tensor.op) callback(op),False,"if isinstance ( tensor . op , tvm . te . ComputeOp ) :",if tensor . op . is_inline ( ) :,0.8820016898747209,15.020004628709785
"def unwatch_run(self, run_id, handler): with self._dict_lock: <mask>: self._handlers_dict[run_id] = [ (start_cursor, callback) for (start_cursor, callback) in self._handlers_dict[run_id] if callback != handler ] if not self._handlers_dict[run_id]: del self._handlers_dict[run_id] run_id_dict = self._run_id_dict del run_id_dict[run_id] self._run_id_dict = run_id_dict",False,if run_id in self . _run_id_dict :,if run_id not in self . _handlers_dict :,0.8820016898747209,44.06401630925027
"def _PromptMySQL(self, config): """"""Prompts the MySQL configuration, retrying if the configuration is invalid."""""" while True: self._PromptMySQLOnce(config) <mask>: print(""Successfully connected to MySQL with the given configuration."") return else: print(""Error: Could not connect to MySQL with the given configuration."") retry = RetryBoolQuestion(""Do you want to retry MySQL configuration?"", True) if not retry: raise ConfigInitError()",False,if self . _CheckMySQLConnection ( ) :,if self . _ConnectMySQL ( config ) :,0.8820016898747209,36.88939732334405
"def get_courses_without_topic(topic): data = [] for entry in frappe.db.get_all(""Course""): course = frappe.get_doc(""Course"", entry.name) topics = [t.topic for t in course.topics] <mask>: data.append(course.name) return data",False,if not topics or topic not in topics :,if topic in topics :,0.8820016898747209,20.300727612812874
"def _error_handler(action, **keywords): if keywords: file_type = keywords.get(""file_type"", None) if file_type: raise exceptions.FileTypeNotSupported( constants.FILE_TYPE_NOT_SUPPORTED_FMT % (file_type, action) ) else: <mask>: keywords.pop(""on_demand"") msg = ""Please check if there were typos in "" msg += ""function parameters: %s. Otherwise "" msg += ""unrecognized parameters were given."" raise exceptions.UnknownParameters(msg % keywords) else: raise exceptions.UnknownParameters(""No parameters found!"")",True,"if ""on_demand"" in keywords :","if ""on_demand"" in keywords :",0.8820016898747209,100.00000000000004
"def select(self, regions, register): self.view.sel().clear() to_store = [] for r in regions: self.view.sel().add(r) <mask>: to_store.append(self.view.substr(self.view.full_line(r))) if register: text = """".join(to_store) if not text.endswith(""\n""): text = text + ""\n"" state = State(self.view) state.registers[register] = [text]",False,if register :,if self . view . full_line ( r ) :,0.8820016898747209,4.02724819242185
"def has_actor(self, message: HasActorMessage) -> ResultMessage: actor_ref = message.actor_ref # lookup allocated for address, item in self._allocated_actors.items(): ref = create_actor_ref(address, actor_ref.uid) <mask>: return ResultMessage(message.message_id, True, protocol=message.protocol) return ResultMessage(message.message_id, False, protocol=message.protocol)",False,if ref in item :,if ref == item . ref :,0.8820016898747209,13.134549472120788
"def toggleMetaButton(self, event): """"""Process clicks on toggle buttons"""""" clickedBtn = event.EventObject if wx.GetMouseState().GetModifiers() == wx.MOD_CONTROL: activeBtns = [btn for btn in self.metaButtons if btn.GetValue()] <mask>: clickedBtn.setUserSelection(clickedBtn.GetValue()) self.itemView.filterItemStore() else: # Do 'nothing' if we're trying to turn last active button off # Keep button in the same state clickedBtn.setUserSelection(True) else: for btn in self.metaButtons: btn.setUserSelection(btn == clickedBtn) self.itemView.filterItemStore()",True,if activeBtns :,if activeBtns :,0.8820016898747209,0.0
"def __init__(self, hub=None): # pylint: disable=unused-argument if resolver._resolver is None: _resolver = resolver._resolver = _DualResolver() if config.resolver_nameservers: _resolver.network_resolver.nameservers[:] = config.resolver_nameservers <mask>: _resolver.network_resolver.lifetime = config.resolver_timeout # Different hubs in different threads could be sharing the same # resolver. assert isinstance(resolver._resolver, _DualResolver) self._resolver = resolver._resolver",True,if config . resolver_timeout :,if config . resolver_timeout :,0.8820016898747209,100.00000000000004
"def sub_paragraph(self, li): """"""Search for checkbox in sub-paragraph."""""" found = False if len(li): first = list(li)[0] <mask>: m = RE_CHECKBOX.match(first.text) if m is not None: first.text = self.markdown.htmlStash.store( get_checkbox(m.group(""state"")), safe=True ) + m.group(""line"") found = True return found",False,"if first . tag == ""p"" and first . text is not None :",if first . text :,0.8820016898747209,5.3941217755126925
"def _check_mswin_locale(locale): msloc = None try: msloc = _LOCALE_NAMES[locale[:5]][:2] locale = locale[:5] except KeyError: try: msloc = _LOCALE_NAMES[locale[:2]][:2] locale = locale[:2] except KeyError: # US English is the outlier, all other English locales want # real English: <mask>: return (""en_GB"", ""1252"") return (None, None) return (locale, msloc)",False,"if locale [ : 2 ] == ( ""en"" ) and locale [ : 5 ] != ""en_US"" :","if locale == ""US"" :",0.8820016898747209,3.0724214181894407
"def setLabel(self, s, protect=False): """"""Set the label of the minibuffer."""""" c, k, w = self.c, self, self.w if w: # Support for the curses gui. <mask>: g.app.gui.set_minibuffer_label(c, s) w.setAllText(s) n = len(s) w.setSelectionRange(n, n, insert=n) if protect: k.mb_prefix = s",False,"if hasattr ( g . app . gui , ""set_minibuffer_label"" ) :","if g . app . gui . has_minibuffer_label ( c , s ) :",0.8820016898747209,38.8905561152711
"def getProc(su, innerTarget): if len(su) == 1: # have a one element wedge proc = (""first"", ""last"") else: if su.isFirst(innerTarget) and su.isLast(innerTarget): proc = (""first"", ""last"") # same element can be first and last <mask>: proc = (""first"",) elif su.isLast(innerTarget): proc = (""last"",) else: proc = () return proc",True,elif su . isFirst ( innerTarget ) :,elif su . isFirst ( innerTarget ) :,0.8820016898747209,100.00000000000004
"def await_test_end(self): iterations = 0 while True: if iterations > 100: self.log.debug(""Await: iteration limit reached"") return status = self.master.get_status() <mask>: return iterations += 1 time.sleep(1.0)",False,"if status . get ( ""status"" ) == ""ENDED"" :","if status == ""RUNNING"" :",0.8820016898747209,12.59496650349099
"def _handle_autocomplete_request_for_text(text): if not hasattr(text, ""autocompleter""): if isinstance(text, (CodeViewText, ShellText)) and text.is_python_text(): <mask>: text.autocompleter = Completer(text) elif isinstance(text, ShellText): text.autocompleter = ShellCompleter(text) text.bind(""<1>"", text.autocompleter.on_text_click) else: return text.autocompleter.handle_autocomplete_request()",True,"if isinstance ( text , CodeViewText ) :","if isinstance ( text , CodeViewText ) :",0.8820016898747209,100.00000000000004
"def validate_party_details(self): if self.party: <mask>: frappe.throw(_(""Invalid {0}: {1}"").format(self.party_type, self.party)) if self.party_account and self.party_type in (""Customer"", ""Supplier""): self.validate_account_type( self.party_account, [erpnext.get_party_account_type(self.party_type)] )",False,"if not frappe . db . exists ( self . party_type , self . party ) :","if not self . party_type in ( ""Customer"" , ""Supplier"" ) :",0.8820016898747209,25.63380983782248
"def format(self, formatstr): pieces = [] for i, piece in enumerate(re_formatchars.split(force_text(formatstr))): <mask>: pieces.append(force_text(getattr(self, piece)())) elif piece: pieces.append(re_escaped.sub(r""\1"", piece)) return """".join(pieces)",False,if i % 2 :,if i == 0 :,0.8820016898747209,17.965205598154213
"def _convert_java_pattern_to_python(pattern): """"""Convert a replacement pattern from the Java-style `$5` to the Python-style `\\5`."""""" s = list(pattern) i = 0 while i < len(s) - 1: c = s[i] <mask>: s[i] = ""\\"" elif c == ""\\"" and s[i + 1] == ""$"": s[i] = """" i += 1 i += 1 return pattern[:0].join(s)",False,"if c == ""$"" and s [ i + 1 ] in ""0123456789"" :","if c == ""\\"" and s [ i + 1 ] == ""\\"" :",0.8820016898747209,51.23350305765596
"def download(self, url, filename, **kwargs): try: r = self.get(url, timeout=10, stream=True, **kwargs) <mask>: return False with open(filename, ""wb"") as f: for chunk in r.iter_content(chunk_size=1024): if chunk: f.write(chunk) helpers.chmod_as_parent(filename) except Exception as e: sickrage.app.log.debug( ""Failed to download file from {} - ERROR: {}"".format(url, e) ) if os.path.exists(filename): os.remove(filename) return False return True",False,if r . status_code >= 400 :,if not r :,0.8820016898747209,4.690733795095046
"def run(self, paths=[]): items = [] for item in SideBarSelection(paths).getSelectedFilesWithExtension(""js""): items.append( '<script type=""text/javascript"" src=""' + item.pathAbsoluteFromProjectEncoded() + '""></script>' ) if len(items) > 0: sublime.set_clipboard(""\n"".join(items)) <mask>: sublime.status_message(""Items copied"") else: sublime.status_message(""Item copied"")",False,if len ( items ) > 1 :,if len ( items ) == 1 :,0.8820016898747209,51.33450480401705
"def work(self): while True: timeout = self.timeout if idle.is_set(): timeout = self.idle_timeout log.debug(""Wait for {}"".format(timeout)) fetch.wait(timeout) <mask>: log.info(""Stop fetch worker"") break self.fetch()",False,if shutting_down . is_set ( ) :,if fetch . is_set ( ) :,0.8820016898747209,60.10525952194528
"def check_apns_certificate(ss): mode = ""start"" for s in ss.split(""\n""): if mode == ""start"": <mask>: mode = ""key"" elif mode == ""key"": if ""END RSA PRIVATE KEY"" in s or ""END PRIVATE KEY"" in s: mode = ""end"" break elif s.startswith(""Proc-Type"") and ""ENCRYPTED"" in s: raise ImproperlyConfigured( ""Encrypted APNS private keys are not supported"" ) if mode != ""end"": raise ImproperlyConfigured(""The APNS certificate doesn't contain a private key"")",False,"if ""BEGIN RSA PRIVATE KEY"" in s or ""BEGIN PRIVATE KEY"" in s :","if ""RSA PRIVATE KEY"" in s or ""RSA PRIVATE KEY"" in s :",0.8820016898747209,69.26989774939868
"def compare_lists(self, l1, l2, key): l2_lookup = {o.get(key): o for o in l2} for obj1 in l1: obj2 = l2_lookup.get(obj1.get(key)) for k in obj1: <mask>: self.assertEqual(obj1.get(k), obj2.get(k))",False,"if k not in ""id"" and obj1 . get ( k ) :",if k in obj2 :,0.8820016898747209,3.4384144696651595
"def before_get_object(self, view_kwargs): if view_kwargs.get(""id"") is not None: try: user_favourite_event = find_user_favourite_event_by_id( event_id=view_kwargs[""id""] ) except NoResultFound: raise ObjectNotFound( {""source"": ""/data/relationships/event""}, ""Object: not found"" ) else: <mask>: view_kwargs[""id""] = user_favourite_event.id else: view_kwargs[""id""] = None",False,if user_favourite_event is not None :,if user_favourite_event :,0.8820016898747209,54.77927682341229
"def close(self): super().close() if not sys.is_finalizing(): for sig in list(self._signal_handlers): self.remove_signal_handler(sig) else: <mask>: warnings.warn( f""Closing the loop {self!r} "" f""on interpreter shutdown "" f""stage, skipping signal handlers removal"", ResourceWarning, source=self, ) self._signal_handlers.clear()",True,if self . _signal_handlers :,if self . _signal_handlers :,0.8820016898747209,100.00000000000004
"def install_script(self, script, install_options=None): try: fname = utils.do_script( script, python_exe=osp.join(self.target, ""python.exe""), architecture=self.architecture, verbose=self.verbose, install_options=install_options, ) except RuntimeError: <mask>: print(""Failed!"") raise",False,if not self . verbose :,if self . verbose :,0.8820016898747209,57.89300674674101
"def GetRouterForUser(self, username): """"""Returns a router corresponding to a given username."""""" for index, router in enumerate(self.routers): router_id = str(index) <mask>: logging.debug( ""Matched router %s to user %s"", router.__class__.__name__, username ) return router logging.debug( ""No router ACL rule match for user %s. Using default "" ""router %s"", username, self.default_router.__class__.__name__, ) return self.default_router",False,"if self . auth_manager . CheckPermissions ( username , router_id ) :",if router . GetUser ( username ) == username :,0.8820016898747209,6.507499376567245
"def charset(self): """"""The charset from the content type."""""" header = self.environ.get(""CONTENT_TYPE"") if header: ct, options = parse_options_header(header) charset = options.get(""charset"") <mask>: if is_known_charset(charset): return charset return self.unknown_charset(charset) return self.default_charset",False,if charset :,"if ct == ""text/plain"" :",0.8820016898747209,4.990049701936832
def isFinished(self): # returns true if episode timesteps has reached episode length and resets the task if self.count > self.epiLen: self.res() return True else: if self.count == 1: self.pertGlasPos(0) <mask>: self.env.reset() self.pertGlasPos(1) self.count += 1 return False,False,if self . count == self . epiLen / 2 + 1 :,elif self . count == 2 :,0.8820016898747209,26.563123324397914
"def mtimes_of_files(dirnames: List[str], suffix: str) -> Iterator[float]: for dirname in dirnames: for root, dirs, files in os.walk(dirname): for sfile in files: <mask>: try: yield path.getmtime(path.join(root, sfile)) except OSError: pass",True,if sfile . endswith ( suffix ) :,if sfile . endswith ( suffix ) :,0.8820016898747209,100.00000000000004
"def get_all_hashes(self): event_hashes = [] sample_hashes = [] for a in self.event.attributes: h = None if a.type in (""md5"", ""sha1"", ""sha256""): h = a.value event_hashes.append(h) elif a.type in (""filename|md5"", ""filename|sha1"", ""filename|sha256""): h = a.value.split(""|"")[1] event_hashes.append(h) <mask>: h = a.value.split(""|"")[1] sample_hashes.append(h) return event_hashes, sample_hashes",False,"elif a . type == ""malware-sample"" :","elif a . type in ( ""sample"" , ""sample"" ) :",0.8820016898747209,18.92240568795936
"def _validate(self, event): if self.type is None: return new = self.value if not isinstance(new, self.type) and new is not None: <mask>: self.value = event.old types = repr(self.type) if isinstance(self.type, tuple) else self.type.__name__ raise ValueError( ""LiteralInput expected %s type but value %s "" ""is of type %s."" % (types, new, type(new).__name__) )",False,if event :,if new is event . old :,0.8820016898747209,8.643019616048525
"def update_dict(a, b): for key, value in b.items(): <mask>: continue if key not in a: a[key] = value elif isinstance(a[key], dict) and isinstance(value, dict): update_dict(a[key], value) elif isinstance(a[key], list): a[key].append(value) else: a[key] = [a[key], value]",False,if value is None :,if key in a :,0.8820016898747209,12.703318703865365
"def on_pre_save(self, view): extOrClause = ""|"".join(s.get(""format_on_save_extensions"")) extRegex = ""\\.("" + extOrClause + "")$"" if s.get(""format_on_save"") and re.search(extRegex, view.file_name()): # only auto-format on save if there are no ""lint errors"" # here are some named regions from sublimelint see https://github.com/lunixbochs/sublimelint/tree/st3 lints_regions = [""lint-keyword-underline"", ""lint-keyword-outline""] for linter in lints_regions: <mask>: return view.run_command(""js_format"")",False,if len ( view . get_regions ( linter ) ) :,"if linter . get ( ""format_on_save"" ) :",0.8820016898747209,9.55204080682377
"def readMemory(self, va, size): for mva, mmaxva, mmap, mbytes in self._map_defs: if mva <= va < mmaxva: mva, msize, mperms, mfname = mmap <mask>: raise envi.SegmentationViolation(va) offset = va - mva return mbytes[offset : offset + size] raise envi.SegmentationViolation(va)",False,if not mperms & MM_READ :,if msize != 0 :,0.8820016898747209,6.916271812933183
"def assertFilepathsEqual(self, p1, p2): if sys.platform == ""win32"": <mask>: p1 = [normcase(normpath(x)) for x in p1] p2 = [normcase(normpath(x)) for x in p2] else: assert isinstance(p1, (str, unicode)) p1 = normcase(normpath(p1)) p2 = normcase(normpath(p2)) self.assertEqual(p1, p2)",False,"if isinstance ( p1 , ( list , tuple ) ) :","if isinstance ( p1 , list ) :",0.8820016898747209,37.28878639930421
"def add_directory_csv_files(dir_path, paths=None): if not paths: paths = [] for p in listdir(dir_path): path = join(dir_path, p) if isdir(path): # call recursively for each dir paths = add_directory_csv_files(path, paths) <mask>: # add every file to the list paths.append(path) return paths",False,"elif isfile ( path ) and path . endswith ( "".csv"" ) :",elif isfile ( path ) :,0.8820016898747209,15.882481735499006
"def _verifySubs(self): for inst in self.subs: <mask>: raise BlockError(_error.ArgType % (self.name,)) if isinstance(inst, (_Block, _Instantiator)): if not inst.modctxt: raise BlockError(_error.InstanceError % (self.name, inst.callername))",False,"if not isinstance ( inst , ( _Block , _Instantiator , Cosimulation ) ) :","if isinstance ( inst , _ArgType ) :",0.8820016898747209,16.58011393376885
"def __annotations_bytes(self): if self.annotations: a = [] for k, v in self.annotations.items(): if len(k) != 4: raise errors.ProtocolError(""annotation key must be of length 4"") <mask>: k = k.encode(""ASCII"") a.append(struct.pack(""!4sH"", k, len(v))) a.append(v) return b"""".join(a) return b""""",False,"if sys . version_info >= ( 3 , 0 ) :","if isinstance ( k , bytes ) :",0.8820016898747209,6.560271639619885
"def session(self, profile: str = ""default"", region: str = None) -> boto3.Session: region = self._get_region(region, profile) try: session = self._cache_lookup( self._session_cache, [profile, region], self._boto3.Session, [], {""region_name"": region, ""profile_name"": profile}, ) except ProfileNotFound: <mask>: raise session = self._boto3.Session(region_name=region) self._cache_set(self._session_cache, [profile, region], session) return session",False,"if profile != ""default"" :",if region is None :,0.8820016898747209,6.9717291216921975
"def spans_score(gold_spans, system_spans): correct, gi, si = 0, 0, 0 while gi < len(gold_spans) and si < len(system_spans): if system_spans[si].start < gold_spans[gi].start: si += 1 <mask>: gi += 1 else: correct += gold_spans[gi].end == system_spans[si].end si += 1 gi += 1 return Score(len(gold_spans), len(system_spans), correct)",False,elif gold_spans [ gi ] . start < system_spans [ si ] . start :,elif gold_spans [ si ] . end > system_spans [ gi ] . end :,0.8820016898747209,68.59238121837059
"def to_api(tag, raw_value): try: api_tag, converter = _QL_TO_SC[tag] if tag else (""q"", None) except KeyError: <mask>: raise self.error( ""Unsupported '%s' tag. Try: %s"" % (tag, "", "".join(SUPPORTED)) ) return None, None else: value = str(converter(raw_value) if converter else raw_value) return api_tag, value",False,if tag not in SUPPORTED :,if not supported :,0.8820016898747209,12.750736437345598
"def unpack(self, buf): dpkt.Packet.unpack(self, buf) buf = buf[self.__hdr_len__ :] # single-byte IE if self.type & 0x80: self.len = 0 self.data = b"""" # multi-byte IE else: # special PER-encoded UUIE <mask>: self.len = struct.unpack("">H"", buf[:2])[0] buf = buf[2:] # normal TLV-like IE else: self.len = struct.unpack(""B"", buf[:1])[0] buf = buf[1:] self.data = buf[: self.len]",False,if self . type == USER_TO_USER :,if self . type & 0x80 :,0.8820016898747209,21.28139770959968
"def on_bt_search_clicked(self, widget): if self.current_provider is None: return query = self.en_query.get_text() @self.obtain_podcasts_with def load_data(): <mask>: return self.current_provider.on_search(query) elif self.current_provider.kind == directory.Provider.PROVIDER_URL: return self.current_provider.on_url(query) elif self.current_provider.kind == directory.Provider.PROVIDER_FILE: return self.current_provider.on_file(query)",True,if self . current_provider . kind == directory . Provider . PROVIDER_SEARCH :,if self . current_provider . kind == directory . Provider . PROVIDER_SEARCH :,0.8820016898747209,100.00000000000004
"def _text(bitlist): out = """" for typ, text in bitlist: if not typ: out += text <mask>: out += ""\\fI%s\\fR"" % text elif typ in [""strong"", ""code""]: out += ""\\fB%s\\fR"" % text else: raise ValueError(""unexpected tag %r inside text"" % (typ,)) out = out.strip() out = re.sub(re.compile(r""^\s+"", re.M), """", out) return out",False,"elif typ == ""em"" :","elif typ in [ ""strong"" , ""code"" ] :",0.8820016898747209,7.768562846380176
"def process(self, buckets): with self.executor_factory(max_workers=3) as w: futures = {} results = [] for b in buckets: futures[w.submit(self.process_bucket, b)] = b for f in as_completed(futures): <mask>: b = futures[f] self.log.error( ""error modifying bucket:%s\n%s"", b[""Name""], f.exception() ) results += filter(None, [f.result()]) return results",False,if f . exception ( ) :,if f in futures :,0.8820016898747209,15.848738972120703
"def check_settings(self): if self.settings_dict[""TIME_ZONE""] is not None: <mask>: raise ImproperlyConfigured( ""Connection '%s' cannot set TIME_ZONE because USE_TZ is "" ""False."" % self.alias ) elif self.features.supports_timezones: raise ImproperlyConfigured( ""Connection '%s' cannot set TIME_ZONE because its engine "" ""handles time zones conversions natively."" % self.alias )",False,if not settings . USE_TZ :,if self . features . supports_tz :,0.8820016898747209,6.742555929751843
"def process_webhook_prop(namespace): if not isinstance(namespace.webhook_properties, list): return result = {} for each in namespace.webhook_properties: if each: <mask>: key, value = each.split(""="", 1) else: key, value = each, """" result[key] = value namespace.webhook_properties = result",True,"if ""="" in each :","if ""="" in each :",0.8820016898747209,100.00000000000004
"def _expand_query_values(original_query_list): query_list = [] for key, value in original_query_list: <mask>: query_list.append((key, value)) else: key_fmt = key + ""[%s]"" value_list = _to_kv_list(value) query_list.extend((key_fmt % k, v) for k, v in value_list) return query_list",False,"if isinstance ( value , basestring ) :","if isinstance ( value , dict ) :",0.8820016898747209,59.4603557501361
"def tags(): """"""Return a dictionary of all tags in the form {hash: [tag_names, ...]}."""""" tags = {} for (n, c) in list_refs(): if n.startswith(""refs/tags/""): name = n[10:] <mask>: tags[c] = [] tags[c].append(name) # more than one tag can point at 'c' return tags",False,if not c in tags :,if c not in tags :,0.8820016898747209,35.930411196308434
"def test_colorspiral(self): """"""Set of 625 colours, with jitter, using get_colors()."""""" boxedge = 20 boxes_per_row = 25 rows = 0 for i, c in enumerate(get_colors(625)): self.c.setFillColor(c) x1 = boxedge * (i % boxes_per_row) y1 = rows * boxedge self.c.rect(x1, y1, boxedge, boxedge, fill=1, stroke=0) <mask>: rows += 1 self.finish()",False,if not ( i + 1 ) % boxes_per_row :,if i % boxes_per_row == 0 :,0.8820016898747209,39.085161980674464
"def oldest_pending_update_in_days(): """"""Return the datestamp of the oldest pending update"""""" pendingupdatespath = os.path.join( prefs.pref(""ManagedInstallDir""), ""UpdateNotificationTracking.plist"" ) try: pending_updates = FoundationPlist.readPlist(pendingupdatespath) except FoundationPlist.NSPropertyListSerializationException: return 0 oldest_date = now = NSDate.date() for category in pending_updates: for name in pending_updates[category]: this_date = pending_updates[category][name] <mask>: oldest_date = this_date return now.timeIntervalSinceDate_(oldest_date) / (24 * 60 * 60)",False,if this_date < oldest_date :,if this_date > oldest_date :,0.8820016898747209,59.694917920196445
"def _try_read_gpg(path): path = os.path.expanduser(path) cmd = _gpg_cmd() + [path] log.debug(""gpg cmd: %s"", cmd) try: p = subprocess.Popen( cmd, env=os.environ, stdout=subprocess.PIPE, stderr=subprocess.PIPE ) except OSError as e: log.error(""cannot decode %s with command '%s' (%s)"", path, "" "".join(cmd), e) else: out, err = p.communicate() <mask>: log.error(err.decode(errors=""replace"").strip()) return None return out.decode(errors=""replace"")",False,if p . returncode != 0 :,if err :,0.8820016898747209,0.0
"def sort_nested_dictionary_lists(d): for k, v in d.items(): <mask>: for i in range(0, len(v)): if isinstance(v[i], dict): v[i] = await sort_nested_dictionary_lists(v[i]) d[k] = sorted(v) if isinstance(v, dict): d[k] = await sort_nested_dictionary_lists(v) return d",True,"if isinstance ( v , list ) :","if isinstance ( v , list ) :",0.8820016898747209,100.00000000000004
"def _the_callback(widget, event_id): point = widget.GetCenter() index = widget.WIDGET_INDEX if hasattr(callback, ""__call__""): <mask>: args = [point, index] else: args = [point] if pass_widget: args.append(widget) try_callback(callback, *args) return",False,if num > 1 :,if event_id == widget . WIDGET_EVENT :,0.8820016898747209,4.02724819242185
"def _add_cs(master_cs, sub_cs, prefix, delimiter=""."", parent_hp=None): new_parameters = [] for hp in sub_cs.get_hyperparameters(): new_parameter = copy.deepcopy(hp) # Allow for an empty top-level parameter <mask>: new_parameter.name = prefix elif not prefix == """": new_parameter.name = ""{}{}{}"".format(prefix, SPLITTER, new_parameter.name) new_parameters.append(new_parameter) for hp in new_parameters: _add_hp(master_cs, hp)",False,"if new_parameter . name == """" :","if delimiter == """" :",0.8820016898747209,36.337289265247364
"def tearDown(self): """"""Shutdown the server."""""" try: <mask>: self.server.stop() if self.sl_hdlr: self.root_logger.removeHandler(self.sl_hdlr) self.sl_hdlr.close() finally: BaseTest.tearDown(self)",True,if self . server :,if self . server :,0.8820016898747209,100.00000000000004
"def app_uninstall_all(self, excludes=[], verbose=False): """"""Uninstall all apps"""""" our_apps = [""com.github.uiautomator"", ""com.github.uiautomator.test""] output, _ = self.shell([""pm"", ""list"", ""packages"", ""-3""]) pkgs = re.findall(r""package:([^\s]+)"", output) pkgs = set(pkgs).difference(our_apps + excludes) pkgs = list(pkgs) for pkg_name in pkgs: <mask>: print(""uninstalling"", pkg_name, "" "", end="""", flush=True) ok = self.app_uninstall(pkg_name) if verbose: print(""OK"" if ok else ""FAIL"") return pkgs",True,if verbose :,if verbose :,0.8820016898747209,0.0
"def httpapi(self, arg, opts): sc = HttpAPIStatsCollector() headers = [""#Item"", ""Value""] table = [] for k, v in sc.get().getStats().items(): if isinstance(v, dict): v = json.dumps(v) row = [] row.append(""#%s"" % k) <mask>: row.append(formatDateTime(v)) else: row.append(v) table.append(row) self.protocol.sendData( tabulate(table, headers, tablefmt=""plain"", numalign=""left"").encode(""ascii"") )",False,"if k [ - 3 : ] == ""_at"" :","elif isinstance ( v , datetime . datetime ) :",0.8820016898747209,2.812739937159535
"def Get_Gene(self, id): """"""Retreive the gene name (GN)."""""" entry = self.Get(id) if not entry: return None GN = """" for line in string.split(entry, ""\n""): <mask>: GN = string.strip(line[5:]) if GN[-1] == ""."": GN = GN[0:-1] return GN if line[0:2] == ""//"": break return GN",False,"if line [ 0 : 5 ] == ""GN   "" :","if line . startswith ( ""GN"" ) :",0.8820016898747209,15.228763726734105
"def replace_dir_vars(path, d): """"""Replace common directory paths with appropriate variable references (e.g. /etc becomes ${sysconfdir})"""""" dirvars = {} # Sort by length so we get the variables we're interested in first for var in sorted(list(d.keys()), key=len): if var.endswith(""dir"") and var.lower() == var: value = d.getVar(var) <mask>: dirvars[value] = var for dirpath in sorted(list(dirvars.keys()), reverse=True): path = path.replace(dirpath, ""${%s}"" % dirvars[dirpath]) return path",False,"if value . startswith ( ""/"" ) and not ""\n"" in value and value not in dirvars :",if value :,0.8820016898747209,0.0
"def _scrub_generated_timestamps(self, target_workdir): """"""Remove the first line of comment from each file if it contains a timestamp."""""" for root, _, filenames in safe_walk(target_workdir): for filename in filenames: source = os.path.join(root, filename) with open(source, ""r"") as f: lines = f.readlines() if len(lines) < 1: return with open(source, ""w"") as f: <mask>: f.write(lines[0]) for line in lines[1:]: f.write(line)",False,if not self . _COMMENT_WITH_TIMESTAMP_RE . match ( lines [ 0 ] ) :,if lines [ 0 ] :,0.8820016898747209,4.615978690441124
"def get_all_active_plugins(self) -> List[BotPlugin]: """"""This returns the list of plugins in the callback ordered defined from the config."""""" all_plugins = [] for name in self.plugins_callback_order: # None is a placeholder for any plugin not having a defined order <mask>: all_plugins += [ plugin for name, plugin in self.plugins.items() if name not in self.plugins_callback_order and plugin.is_activated ] else: plugin = self.plugins[name] if plugin.is_activated: all_plugins.append(plugin) return all_plugins",False,if name is None :,"if name == ""active"" :",0.8820016898747209,12.22307556087252
"def test_query_level(self): ""Tests querying at a level other than max"" # level 2 l2 = set() for p in self.tile_paths: l2.add(p[0:2]) for path in iterate_base4(2): <mask>: self.assertTrue(self.tree.query_path(path)) else: self.assertFalse(self.tree.query_path(path)) # level 1: self.assertTrue(self.tree.query_path((0,))) self.assertTrue(self.tree.query_path((1,))) self.assertTrue(self.tree.query_path((2,))) self.assertFalse(self.tree.query_path((3,)))",True,if path in l2 :,if path in l2 :,0.8820016898747209,100.00000000000004
"def program_exists(name): paths = (os.getenv(""PATH"") or os.defpath).split(os.pathsep) for p in paths: fn = ""%s/%s"" % (p, name) <mask>: return not os.path.isdir(fn) and os.access(fn, os.X_OK)",True,if os . path . exists ( fn ) :,if os . path . exists ( fn ) :,0.8820016898747209,100.00000000000004
"def decoration_helper(self, patched, args, keywargs): extra_args = [] with contextlib.ExitStack() as exit_stack: for patching in patched.patchings: arg = exit_stack.enter_context(patching) <mask>: keywargs.update(arg) elif patching.new is DEFAULT: extra_args.append(arg) args += tuple(extra_args) yield (args, keywargs)",False,if patching . attribute_name is not None :,if patching . new is KEY :,0.8820016898747209,18.094495256969623
"def update_neighbor(neigh_ip_address, changes): rets = [] for k, v in changes.items(): if k == neighbors.MULTI_EXIT_DISC: rets.append(_update_med(neigh_ip_address, v)) <mask>: rets.append(update_neighbor_enabled(neigh_ip_address, v)) if k == neighbors.CONNECT_MODE: rets.append(_update_connect_mode(neigh_ip_address, v)) return all(rets)",False,if k == neighbors . ENABLED :,elif k == neighbors . NEIGH_ENABLED :,0.8820016898747209,45.180100180492246
"def calcUniqueStates(self): # Here we show which colors can be relied on to map to an # internal state. The current position will be at the first # character in the buffer styled that color, so this might not # work in all cases. self.uniqueStates = {} for k in self.holdUniqueStates.keys(): v = self.holdUniqueStates[k] <mask>: self.uniqueStates[k] = v.keys()[0] log.debug(""Map style [%s] to state [%s]"", k, v.keys()[0]) log.debug(""Style [%s] maps to states [%s]"", k, "", "".join(v.keys())) self.holdUniqueStates = None",False,if len ( v . keys ( ) ) == 1 :,if v :,0.8820016898747209,0.0
"def init_logger(): configured_loggers = [log_config.get(""root"", {})] + [ logger for logger in log_config.get(""loggers"", {}).values() ] used_handlers = { handler for log in configured_loggers for handler in log.get(""handlers"", []) } for handler_id, handler in list(log_config[""handlers""].items()): <mask>: del log_config[""handlers""][handler_id] elif ""filename"" in handler.keys(): filename = handler[""filename""] logfile_path = Path(filename).expanduser().resolve() handler[""filename""] = str(logfile_path) logging.config.dictConfig(log_config)",False,if handler_id not in used_handlers :,if handler_id in used_handlers :,0.8820016898747209,66.90484408935988
"def _selected_machines(self, virtual_machines): selected_machines = [] for machine in virtual_machines: <mask>: selected_machines.append(machine) if self.tags and self._tags_match(machine.tags, self.tags): selected_machines.append(machine) if self.locations and machine.location in self.locations: selected_machines.append(machine) return selected_machines",False,if self . _args . host and self . _args . host == machine . name :,"if self . _match ( machine . name , self . name ) :",0.8820016898747209,19.508205674664694
"def init(self): r = self.get_redis() if r: key = ""pocsuite_target"" info_msg = ""[PLUGIN] try fetch targets from redis..."" logger.info(info_msg) targets = r.get(key) count = 0 if targets: for target in targets: <mask>: count += 1 info_msg = ""[PLUGIN] get {0} target(s) from redis"".format(count) logger.info(info_msg)",False,if self . add_target ( target ) :,if target . is_active ( ) :,0.8820016898747209,11.670826803245712
"def tearDown(self): suffix = str(os.getgid()) cli = monitoring_v3.MetricServiceClient() for md in cli.list_metric_descriptors(""projects/{}"".format(PROJECT)): <mask>: try: cli.delete_metric_descriptor(md.name) except Exception: pass",False,"if ""OpenCensus"" in md . name and suffix in md . name :",if md . name . endswith ( suffix ) :,0.8820016898747209,11.708995388048033
"def InitializeColours(self): """"""Initializes the 16 custom colours in :class:`CustomPanel`."""""" curr = self._colourData.GetColour() self._colourSelection = -1 for i in range(16): c = self._colourData.GetCustomColour(i) if c.IsOk(): self._customColours[i] = self._colourData.GetCustomColour(i) else: self._customColours[i] = wx.WHITE <mask>: self._colourSelection = i",False,if c == curr :,if curr == c :,0.8820016898747209,21.3643503198117
"def __getitem__(self, index): if self._check(): if isinstance(index, int): if index < 0 or index >= len(self.features): raise IndexError(index) if self.features[index] is None: feature = self.device.feature_request(FEATURE.FEATURE_SET, 0x10, index) <mask>: (feature,) = _unpack(""!H"", feature[:2]) self.features[index] = FEATURE[feature] return self.features[index] elif isinstance(index, slice): indices = index.indices(len(self.features)) return [self.__getitem__(i) for i in range(*indices)]",True,if feature :,if feature :,0.8820016898747209,0.0
"def _get_data_from_buffer(obj): try: view = memoryview(obj) except TypeError: # try to use legacy buffer protocol if 2.7, otherwise re-raise <mask>: view = memoryview(buffer(obj)) warnings.warn( ""using old buffer interface to unpack %s; "" ""this leads to unpacking errors if slicing is used and "" ""will be removed in a future version"" % type(obj), RuntimeWarning, stacklevel=3, ) else: raise if view.itemsize != 1: raise ValueError(""cannot unpack from multi-byte object"") return view",False,if PY2 :,"if sys . version_info >= ( 2 , 7 ) :",0.8820016898747209,3.377156414337854
"def import_modules(modules, safe=True): """"""Safely import a list of *modules*"""""" all = [] for mname in modules: if mname.endswith("".*""): to_load = expand_star(mname) else: to_load = [mname] for module in to_load: try: all.append(import_module(module)) except ImportError: <mask>: raise return all",False,if not safe :,if safe :,0.8820016898747209,0.0
"def pack(types, *args): if len(types) != len(args): raise Exception(""number of arguments does not match format string"") port = StringIO() for (type, value) in zip(types, args): if type == ""V"": write_vuint(port, value) <mask>: write_vint(port, value) elif type == ""s"": write_bvec(port, value) else: raise Exception('unknown xpack format string item ""' + type + '""') return port.getvalue()",False,"elif type == ""v"" :","elif type == ""V"" :",0.8820016898747209,59.4603557501361
"def create_local_app_folder(local_app_path): if exists(local_app_path): raise ValueError(""There is already a '%s' folder! Aborting!"" % local_app_path) for folder in subfolders(local_app_path): <mask>: os.mkdir(folder) init_path = join(folder, ""__init__.py"") if not exists(init_path): create_file(init_path)",True,if not exists ( folder ) :,if not exists ( folder ) :,0.8820016898747209,100.00000000000004
"def _get_node_type_specific_fields(self, node_id: str, fields_key: str) -> Any: fields = self.config[fields_key] node_tags = self.provider.node_tags(node_id) if TAG_RAY_USER_NODE_TYPE in node_tags: node_type = node_tags[TAG_RAY_USER_NODE_TYPE] <mask>: raise ValueError(f""Unknown node type tag: {node_type}."") node_specific_config = self.available_node_types[node_type] if fields_key in node_specific_config: fields = node_specific_config[fields_key] return fields",True,if node_type not in self . available_node_types :,if node_type not in self . available_node_types :,0.8820016898747209,100.00000000000004
"def _maybe_fix_sequence_in_union( aliases: List[Alias], typecst: cst.SubscriptElement ) -> cst.SubscriptElement: slc = typecst.slice if isinstance(slc, cst.Index): val = slc.value <mask>: return cst.ensure_type( typecst.deep_replace(val, _get_clean_type_from_subscript(aliases, val)), cst.SubscriptElement, ) return typecst",False,"if isinstance ( val , cst . Subscript ) :","if isinstance ( val , cst . Union ) :",0.8820016898747209,70.71067811865478
"def cancel_download(self, downloads): # Make sure we're always dealing with a list if isinstance(downloads, Download): downloads = [downloads] for download in downloads: <mask>: self.cancel_current_download() else: self.__paused = True new_queue = queue.Queue() while not self.__queue.empty(): queued_download = self.__queue.get() if download == queued_download: download.cancel() else: new_queue.put(queued_download) self.__queue = new_queue self.__paused = False",False,if download == self . __current_download :,if download . is_current ( ) :,0.8820016898747209,10.693319442988287
"def migrate_account_metadata(account_id): from inbox.models.session import session_scope from inbox.models import Account with session_scope(versioned=False) as db_session: account = db_session.query(Account).get(account_id) if account.discriminator == ""easaccount"": create_categories_for_easfoldersyncstatuses(account, db_session) else: create_categories_for_folders(account, db_session) <mask>: set_labels_for_imapuids(account, db_session) db_session.commit()",False,"if account . discriminator == ""gmailaccount"" :","if account . discriminator == ""imapuids"" :",0.8820016898747209,70.71067811865478
"def __init__(self, fmt=None, *args): if not isinstance(fmt, BaseException): Error.__init__(self, fmt, *args) else: e = fmt cls = e.__class__ fmt = ""%s.%s: %s"" % (cls.__module__, cls.__name__, e) tb = sys.exc_info()[2] <mask>: fmt += ""\n"" fmt += """".join(traceback.format_tb(tb)) Error.__init__(self, fmt)",True,if tb :,if tb :,0.8820016898747209,0.0
"def setLabel(self, label): if label is None: <mask>: self.label.scene().removeItem(self.label) self.label = None else: if self.label is None: self.label = TextItem() self.label.setParentItem(self) self.label.setText(label) self._updateLabel()",True,if self . label is not None :,if self . label is not None :,0.8820016898747209,100.00000000000004
"def serve_until_stopped(self) -> None: while True: rd, wr, ex = select.select([self.socket.fileno()], [], [], self.timeout) <mask>: self.handle_request() if self.event is not None and self.event.is_set(): break",False,if rd :,if rd == rd :,0.8820016898747209,21.3643503198117
"def generateCompressedFile(inputfile, outputfile, formatstring): try: <mask>: in_file = open(inputfile, ""rb"") in_data = in_file.read() out_file = open(inputfile + "".xz"", ""wb"") out_file.write(xz.compress(in_data)) in_file.close() out_file.close() else: tarout = tarfile.open(outputfile, formatstring) tarout.add(inputfile, arcname=os.path.basename(inputfile)) tarout.close() except Exception as e: print(e) return False return True",False,"if formatstring == ""w:xz"" :",if os . path . isfile ( inputfile ) :,0.8820016898747209,4.990049701936832
"def _datastore_get_handler(signal, sender, keys, **kwargs): txn = current_transaction() if txn: for key in keys: <mask>: raise PreventedReadError( ""Attempted to read key (%s:%s) inside a transaction "" ""where it was marked protected"" % (key.kind(), key.id_or_name()) ) txn._fetched_keys.update(set(keys))",False,if key in txn . _protected_keys :,if key . protected :,0.8820016898747209,9.882804650471988
"def TryMerge(self, d): while d.avail() > 0: tt = d.getVarInt32() <mask>: self.set_access_token(d.getPrefixedString()) continue if tt == 16: self.set_expiration_time(d.getVarInt64()) continue if tt == 0: raise ProtocolBuffer.ProtocolBufferDecodeError d.skipData(tt)",True,if tt == 10 :,if tt == 10 :,0.8820016898747209,100.00000000000004
"def write_vuint(port, x): if x < 0: raise Exception(""vuints must not be negative"") elif x == 0: port.write(""\0"") else: while x: seven_bits = x & 0x7F x >>= 7 <mask>: port.write(chr(0x80 | seven_bits)) else: port.write(chr(seven_bits))",False,if x :,if x & 0x80 :,0.8820016898747209,23.643540225079384
"def _expand_srcs(self): """"""Expand src to [(src, full_path)]"""""" result = [] for src in self.srcs: full_path = self._source_file_path(src) <mask>: # Assume generated full_path = self._target_file_path(src) result.append((src, full_path)) return result",False,if not os . path . exists ( full_path ) :,if not self . _is_file_file ( src ) :,0.8820016898747209,9.238430210261097
"def pytest_collection_modifyitems(items): for item in items: if item.nodeid.startswith(""tests/ops""): if ""stage"" not in item.keywords: item.add_marker(pytest.mark.stage(""unit"")) <mask>: item.add_marker(pytest.mark.init(rng_seed=123))",False,"if ""init"" not in item . keywords :","elif ""init"" not in item . keywords :",0.8820016898747209,88.01117367933934
"def set_shape(self, shape): """"""Sets a shape."""""" if self._shape is not None: logger.warning('Modifying the shape of Placeholder ""%s"".', self.name) if not isinstance(shape, (list, tuple)): shape = (shape,) shape = tuple(x if x != ""None"" else None for x in shape) for x in shape: <mask>: raise ParsingError( 'All entries in ""shape"" must be integers, or in special ' ""cases None. Shape is: {}"".format(shape) ) self._shape = shape",False,"if not isinstance ( x , ( int , type ( None ) ) ) :","if not isinstance ( x , ( int , float ) ) :",0.8820016898747209,60.43026468442617
"def _get_field_actual(cant_be_number, raw_string, field_names): for line in raw_string.splitlines(): for field_name in field_names: field_name = field_name.lower() if "":"" in line: left, right = line.split("":"", 1) left = left.strip().lower() right = right.strip() <mask>: if cant_be_number: if not right.isdigit(): return right else: return right return None",False,if left == field_name and len ( right ) > 0 :,if left . lower ( ) == field_name :,0.8820016898747209,31.04578053097406
"def validate_attributes(self): for attribute in self.get_all_attributes(): value = getattr(self, attribute.code, None) if value is None: <mask>: raise ValidationError( _(""%(attr)s attribute cannot be blank"") % {""attr"": attribute.code} ) else: try: attribute.validate_value(value) except ValidationError as e: raise ValidationError( _(""%(attr)s attribute %(err)s"") % {""attr"": attribute.code, ""err"": e} )",False,if attribute . required :,if not attribute . blank :,0.8820016898747209,19.304869754804482
"def append(self, s): buf = self.buf if buf is None: strbuf = self.strbuf <mask>: self.strbuf = strbuf + s return buf = self._create_buffer() buf.append(s) # use buf.__len__ rather than len(buf) FBO of not getting # OverflowError on Python 2 sz = buf.__len__() if not self.overflowed: if sz >= self.overflow: self._set_large_buffer()",False,if len ( strbuf ) + len ( s ) < STRBUF_LIMIT :,if strbuf :,0.8820016898747209,0.0
"def billing_invoice_show_validator(namespace): from azure.cli.core.azclierror import ( RequiredArgumentMissingError, MutuallyExclusiveArgumentError, ) valid_combs = ( ""only --account-name, --name / --name / --name, --by-subscription is valid"" ) if namespace.account_name is not None: <mask>: raise MutuallyExclusiveArgumentError(valid_combs) if namespace.name is None: raise RequiredArgumentMissingError(""--name is also required"") if namespace.by_subscription is not None: if namespace.name is None: raise RequiredArgumentMissingError(""--name is also required"")",True,if namespace . by_subscription is not None :,if namespace . by_subscription is not None :,0.8820016898747209,100.00000000000004
"def Handle(self, args, context=None): for client_id in args.client_ids: cid = str(client_id) data_store.REL_DB.RemoveClientLabels(cid, context.username, args.labels) labels_to_remove = set(args.labels) existing_labels = data_store.REL_DB.ReadClientLabels(cid) for label in existing_labels: labels_to_remove.discard(label.name) <mask>: idx = client_index.ClientIndex() idx.RemoveClientLabels(cid, labels_to_remove)",False,if labels_to_remove :,if len ( labels_to_remove ) > 0 :,0.8820016898747209,34.48444257953326
"def delete_snapshot(self, snapshot): snap_name = self._get_snap_name(snapshot[""id""]) LOG.debug(""Deleting snapshot (%s)"", snapshot[""id""]) self.client_login() try: self.client.delete_snapshot(snap_name, self.backend_type) except exception.DotHillRequestError as ex: # if the volume wasn't found, ignore the error <mask>: return LOG.exception(""Deleting snapshot %s failed"", snapshot[""id""]) raise exception.Invalid(ex) finally: self.client_logout()",False,"if ""The volume was not found on this system."" in ex . args :",if ex . status_code == 404 :,0.8820016898747209,4.955970694341068
"def jobs(self): # How many jobs have we done? total_processed = 0 for jobEntity in self.jobItems.query_entities(): # Process the items in the page yield AzureJob.fromEntity(jobEntity) total_processed += 1 <mask>: # Produce some feedback for the user, because this can take # a long time on, for example, Azure logger.debug(""Processed %d total jobs"" % total_processed) logger.debug(""Processed %d total jobs"" % total_processed)",False,if total_processed % 1000 == 0 :,if total_processed % 100 == 0 :,0.8820016898747209,65.80370064762461
def run(self): while not self.completed: if self.block: time.sleep(self.period) else: self._completed.wait(self.period) self.counter += 1 try: self.callback(self.counter) except Exception: self.stop() if self.timeout is not None: dt = time.time() - self._start_time if dt > self.timeout: self.stop() <mask>: self.stop(),False,if self . counter == self . count :,elif dt < self . timeout :,0.8820016898747209,9.469167282754096
"def get_instance(cls, pool_size=None): if cls._instance is not None: return cls._instance # Lazy init with cls._SINGLETON_LOCK: <mask>: cls._instance = cls( ARCTIC_ASYNC_NWORKERS if pool_size is None else pool_size ) return cls._instance",True,if cls . _instance is None :,if cls . _instance is None :,0.8820016898747209,100.00000000000004
"def set_state(self, state): if self._inhibit_play: # PLAYING, PAUSED change the state for after buffering is finished, # everything else aborts buffering <mask>: # abort self.__set_inhibit_play(False) self.bin.set_state(state) return self._wanted_state = state else: self.bin.set_state(state)",False,"if state not in ( Gst . State . PLAYING , Gst . State . PAUSED ) :",if self . _wanted_state == state :,0.8820016898747209,2.8049136322597743
"def seen_add(options): seen_name = options.add_value if is_imdb_url(seen_name): console(""IMDB url detected, try to parse ID"") imdb_id = extract_id(seen_name) <mask>: seen_name = imdb_id else: console(""Could not parse IMDB ID"") db.add(seen_name, ""cli_add"", {""cli_add"": seen_name}) console(""Added %s as seen. This will affect all tasks."" % seen_name)",True,if imdb_id :,if imdb_id :,0.8820016898747209,100.00000000000004
"def test_204_invalid_content_length(self): # 204 status with non-zero content length is malformed with ExpectLog(gen_log, "".*Response with code 204 should not have body""): response = self.fetch(""/?error=1"") if not self.http1: self.skipTest(""requires HTTP/1.x"") <mask>: self.skipTest(""curl client accepts invalid headers"") self.assertEqual(response.code, 599)",False,if self . http_client . configured_class != SimpleAsyncHTTPClient :,if response . status_code != 204 :,0.8820016898747209,7.40354787297858
"def set_related_perm(_mapper: Mapper, _connection: Connection, target: Slice) -> None: src_class = target.cls_model id_ = target.datasource_id if id_: ds = db.session.query(src_class).filter_by(id=int(id_)).first() <mask>: target.perm = ds.perm target.schema_perm = ds.schema_perm",True,if ds :,if ds :,0.8820016898747209,0.0
"def on_modified_async(self, view): if self.is_command_line(view): <mask>: view.run_command(""text_pastry_selection_preview"")",False,"if view . size ( ) > 6 and view . substr ( sublime . Region ( 0 , 6 ) ) . lower ( ) == ""search"" :","if view . command ( ""text_pastry_selection_preview"" ) :",0.8820016898747209,4.279641176459531
"def _improve_answer_span( doc_tokens, input_start, input_end, tokenizer, orig_answer_text ): """"""Returns tokenized answer spans that better match the annotated answer."""""" tok_answer_text = "" "".join(tokenizer.tokenize(orig_answer_text)) for new_start in range(input_start, input_end + 1): for new_end in range(input_end, new_start - 1, -1): text_span = "" "".join(doc_tokens[new_start : (new_end + 1)]) <mask>: return new_start, new_end return input_start, input_end",False,if text_span == tok_answer_text :,if tok_answer_text . startswith ( text_span ) :,0.8820016898747209,37.59663529467017
"def TryMerge(self, d): while d.avail() > 0: tt = d.getVarInt32() <mask>: self.set_url(d.getPrefixedString()) continue if tt == 18: self.set_app_version_id(d.getPrefixedString()) continue if tt == 26: self.set_method(d.getPrefixedString()) continue if tt == 34: self.set_queue(d.getPrefixedString()) continue if tt == 0: raise ProtocolBuffer.ProtocolBufferDecodeError d.skipData(tt)",True,if tt == 10 :,if tt == 10 :,0.8820016898747209,100.00000000000004
"def _add_resource_group(obj): if isinstance(obj, list): for array_item in obj: _add_resource_group(array_item) elif isinstance(obj, dict): try: <mask>: if obj[""id""]: obj[""resourceGroup""] = _parse_id(obj[""id""])[""resource-group""] except (KeyError, IndexError, TypeError): pass for item_key in obj: if item_key != ""sourceVault"": _add_resource_group(obj[item_key])",False,"if ""resourcegroup"" not in [ x . lower ( ) for x in obj . keys ( ) ] :","if ""resource-group"" in obj :",0.8820016898747209,2.4116926544041073
"def build(opt): dpath = os.path.join(opt[""datapath""], DECODE) version = DECODE_VERSION if not build_data.built(dpath, version_string=version): print(""[building data: "" + dpath + ""]"") <mask>: # An older version exists, so remove these outdated files. build_data.remove_dir(dpath) build_data.make_dir(dpath) # Download the data. for downloadable_file in RESOURCES: downloadable_file.download_file(dpath) # Mark the data as built. build_data.mark_done(dpath, version_string=version)",True,if build_data . built ( dpath ) :,if build_data . built ( dpath ) :,0.8820016898747209,100.00000000000004
"def toterminal(self, tw): # the entries might have different styles last_style = None for i, entry in enumerate(self.reprentries): <mask>: tw.line("""") entry.toterminal(tw) if i < len(self.reprentries) - 1: next_entry = self.reprentries[i + 1] if ( entry.style == ""long"" or entry.style == ""short"" and next_entry.style == ""long"" ): tw.sep(self.entrysep) if self.extraline: tw.line(self.extraline)",False,"if entry . style == ""long"" :","if entry . style == ""short"" and last_style is not None :",0.8820016898747209,37.08163623065085
"def reposition_division(f1): lines = f1.splitlines() if lines[2] == division: lines.pop(2) found = 0 for i, line in enumerate(lines): <mask>: found += 1 if found == 2: if division in ""\n"".join(lines): break # already in the right place lines.insert(i + 1, """") lines.insert(i + 2, division) break return ""\n"".join(lines)",False,"if line . startswith ( '""""""' ) :",if line . startswith ( division ) :,0.8820016898747209,36.06452879987793
def run_on_module(self): try: self.module_base.disable(self.opts.module_spec) except dnf.exceptions.MarkingErrors as e: if self.base.conf.strict: <mask>: raise e if ( e.module_depsolv_errors and e.module_depsolv_errors[1] != libdnf.module.ModulePackageContainer.ModuleErrorType_ERROR_IN_DEFAULTS ): raise e logger.error(str(e)),False,if e . no_match_group_specs or e . error_group_specs :,if e . module_depsolv_errors and e . module_depsolv_errors [ 0 ] != libdnf . module . ModulePackageContainer . ModuleErrorType_ERROR_IN_DEFAULTS :,0.8820016898747209,6.019608768705656
"def test_len(self): eq = self.assertEqual eq(base64mime.base64_len(""hello""), len(base64mime.encode(""hello"", eol=""""))) for size in range(15): if size == 0: bsize = 0 elif size <= 3: bsize = 4 elif size <= 6: bsize = 8 <mask>: bsize = 12 elif size <= 12: bsize = 16 else: bsize = 20 eq(base64mime.base64_len(""x"" * size), bsize)",False,elif size <= 9 :,elif size <= 10 :,0.8820016898747209,53.7284965911771
"def is_valid(self): """"""Determines whether file is valid for this reader"""""" blocklist = self.open() valid = True for line in blocklist: line = decode_bytes(line) <mask>: try: (start, end) = self.parse(line) if not re.match(r""^(\d{1,3}\.){4}$"", start + ""."") or not re.match( r""^(\d{1,3}\.){4}$"", end + ""."" ): valid = False except Exception: valid = False break blocklist.close() return valid",False,if not self . is_ignored ( line ) :,if line :,0.8820016898747209,0.0
"def next(self): while self.index < len(self.data): uid = self._read_next_word() dont_care = self._read_next_word() entry = self._read_next_string() total_size = int(4 + 4 + len(entry)) count = int(total_size / self.SIZE) if count == 0: mod = self.SIZE - total_size else: mod = self.SIZE - int(total_size - (count * self.SIZE)) <mask>: remainder = self._read_next_block(mod) yield (uid, entry)",False,if mod > 0 :,if dont_care :,0.8820016898747209,12.703318703865365
"def _str_param_list(self, name): out = [] if self[name]: out += self._str_header(name) for param in self[name]: parts = [] if param.name: parts.append(param.name) if param.type: parts.append(param.type) out += ["" : "".join(parts)] <mask>: out += self._str_indent(param.desc) out += [""""] return out",False,"if param . desc and """" . join ( param . desc ) . strip ( ) :",if param . desc :,0.8820016898747209,4.299920764667028
"def assert_backend(self, expected_translated, language=""cs""): """"""Check that backend has correct data."""""" translation = self.get_translation(language) translation.commit_pending(""test"", None) store = translation.component.file_format_cls(translation.get_filename(), None) messages = set() translated = 0 for unit in store.content_units: id_hash = unit.id_hash self.assertFalse(id_hash in messages, ""Duplicate string in in backend file!"") <mask>: translated += 1 self.assertEqual( translated, expected_translated, ""Did not found expected number of translations ({} != {})."".format( translated, expected_translated ), )",False,if unit . is_translated ( ) :,if unit . is_translated :,0.8820016898747209,63.191456189157286
"def status(self, name, error=""No matching script logs found""): with self.script_lock: <mask>: return self.script_running[1:] elif self.script_last and self.script_last[1] == name: return self.script_last[1:] else: raise ValueError(error)",True,if self . script_running and self . script_running [ 1 ] == name :,if self . script_running and self . script_running [ 1 ] == name :,0.8820016898747209,100.00000000000004
"def dict_no_value_from_proto_list(obj_list): d = dict() for item in obj_list: possible_dict = json.loads(item.value_json) <mask>: # (tss) TODO: This is protecting against legacy 'wandb_version' field. # Should investigate why the config payload even has 'wandb_version'. logger.warning(""key '{}' has no 'value' attribute"".format(item.key)) continue d[item.key] = possible_dict[""value""] return d",False,"if not isinstance ( possible_dict , dict ) or ""value"" not in possible_dict :","if possible_dict [ ""wandb_version"" ] is None :",0.8820016898747209,8.800046366764844
"def visit(self, node): """"""dispatcher on node's class/bases name."""""" cls = node.__class__ try: visitmethod = self.cache[cls] except KeyError: for subclass in cls.__mro__: visitmethod = getattr(self, subclass.__name__, None) <mask>: break else: visitmethod = self.__object self.cache[cls] = visitmethod visitmethod(node)",False,if visitmethod is not None :,if visitmethod is None :,0.8820016898747209,40.93653765389909
"def _get_adapter( mcls, reversed_mro: Tuple[type, ...], collection: Dict[Any, Dict[type, Adapter]], kwargs: Dict[str, Any], ) -> Optional[Adapter]: registry_key = mcls.get_registry_key(kwargs) adapters = collection.get(registry_key) if adapters is None: return None result = None seen: Set[Adapter] = set() for base in reversed_mro: for adaptee, adapter in adapters.items(): found = mcls._match_adapter(base, adaptee, adapter) <mask>: result = found seen.add(found) return result",False,if found and found not in seen :,if found not in seen :,0.8820016898747209,60.25286104785454
"def test_pt_BR_rg(self): for _ in range(100): to_test = self.fake.rg() <mask>: assert re.search(r""^\d{8}X"", to_test) else: assert re.search(r""^\d{9}$"", to_test)",False,"if ""X"" in to_test :",if self . test_pt_BR_X :,0.8820016898747209,5.604233375480572
"def get_user_extra_data_by_client_id(self, client_id, username): extra_data = {} current_client = self.clients.get(client_id, None) if current_client: for readable_field in current_client.get_readable_fields(): attribute = list( filter( lambda f: f[""Name""] == readable_field, self.users.get(username).attributes, ) ) <mask>: extra_data.update({attribute[0][""Name""]: attribute[0][""Value""]}) return extra_data",False,if len ( attribute ) > 0 :,if attribute :,0.8820016898747209,0.0
"def augment(self, resources): super().augment(resources) for r in resources: md = r.get(""SAMLMetadataDocument"") <mask>: continue root = sso_metadata(md) r[""IDPSSODescriptor""] = root[""IDPSSODescriptor""] return resources",True,if not md :,if not md :,0.8820016898747209,100.00000000000004
"def __init__(self, mode=0, decode=None): self.regex = self.REGEX[mode] self.decode = decode if decode: self.header = _( ""### This log has been decoded with automatic search pattern\n"" ""### If some paths are not decoded you can manually decode them with:\n"" ) self.header += ""### 'backintime --quiet "" <mask>: self.header += '--profile ""%s"" ' % decode.config.profileName() self.header += ""--decode <path>'\n\n"" else: self.header = """"",False,if int ( decode . config . currentProfile ( ) ) > 1 :,if decode . config . profileName ( ) :,0.8820016898747209,21.88390569648905
"def _get_dynamic_attr(self, attname, obj, default=None): try: attr = getattr(self, attname) except AttributeError: return default if callable(attr): # Check co_argcount rather than try/excepting the function and # catching the TypeError, because something inside the function # may raise the TypeError. This technique is more accurate. try: code = six.get_function_code(attr) except AttributeError: code = six.get_function_code(attr.__call__) <mask>: # one argument is 'self' return attr(obj) else: return attr() return attr",False,if code . co_argcount == 2 :,if code == co_argcount :,0.8820016898747209,24.329268691415322
"def grep_full_py_identifiers(tokens): global pykeywords tokens = list(tokens) i = 0 while i < len(tokens): tokentype, token = tokens[i] i += 1 <mask>: continue while ( i + 1 < len(tokens) and tokens[i] == (""op"", ""."") and tokens[i + 1][0] == ""id"" ): token += ""."" + tokens[i + 1][1] i += 2 if token == """": continue if token in pykeywords: continue if token[0] in "".0123456789"": continue yield token",False,"if tokentype != ""id"" :","if tokentype != ""op"" :",0.8820016898747209,59.4603557501361
"def _add_disk_config(self, context, images): for image in images: metadata = image[""metadata""] <mask>: raw_value = metadata[INTERNAL_DISK_CONFIG] value = utils.bool_from_str(raw_value) image[API_DISK_CONFIG] = disk_config_to_api(value)",False,if INTERNAL_DISK_CONFIG in metadata :,if internal_DISK_CONFIG in metadata :,0.8820016898747209,75.06238537503395
"def test_edgeql_expr_valid_setop_07(self): expected_error_msg = ""cannot be applied to operands"" # IF ELSE with every scalar as the condition for val in get_test_values(): query = f""""""SELECT 1 IF {val} ELSE 2;"""""" <mask>: await self.assert_query_result(query, [1]) else: # every other combination must produce an error with self.assertRaisesRegex( edgedb.QueryError, expected_error_msg, msg=query ): async with self.con.transaction(): await self.con.execute(query)",False,"if val == ""<bool>True"" :","if val == ""1"" :",0.8820016898747209,40.866465020165684
"def get_all_url_infos() -> Dict[str, UrlInfo]: """"""Returns dict associating URL to UrlInfo."""""" url_infos = {} for path in _checksum_paths().values(): dataset_url_infos = load_url_infos(path) for url, url_info in dataset_url_infos.items(): <mask>: raise AssertionError( ""URL {} is registered with 2+ distinct size/checksum tuples. "" ""{} vs {}"".format(url, url_info, url_infos[url]) ) url_infos.update(dataset_url_infos) return url_infos",False,"if url_infos . get ( url , url_info ) != url_info :",if url_info . size != 2 :,0.8820016898747209,11.476641793765525
"def global_fixes(): """"""Yield multiple (code, function) tuples."""""" for function in list(globals().values()): <mask>: arguments = _get_parameters(function) if arguments[:1] != [""source""]: continue code = extract_code_from_function(function) if code: yield (code, function)",False,if inspect . isfunction ( function ) :,"if isinstance ( function , ( list , tuple ) ) :",0.8820016898747209,10.127993013562818
"def createSocket(self): skt = Port.createSocket(self) if self.listenMultiple: skt.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1) <mask>: skt.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEPORT, 1) return skt",False,"if hasattr ( socket , ""SO_REUSEPORT"" ) :",if self . listenMultiple :,0.8820016898747209,3.1325998243558226
"def _asStringList(self, sep=""""): out = [] for item in self._toklist: if out and sep: out.append(sep) <mask>: out += item._asStringList() else: out.append(str(item)) return out",False,"if isinstance ( item , ParseResults ) :","elif isinstance ( item , str ) :",0.8820016898747209,41.11336169005198
"def parse_c_comments(lexer, tok, ntok): if tok != ""/"" or ntok != ""*"": return False quotes = lexer.quotes lexer.quotes = """" while True: tok = lexer.get_token() ntok = lexer.get_token() <mask>: lexer.quotes = quotes break else: lexer.push_token(ntok) return True",False,"if tok == ""*"" and ntok == ""/"" :","if tok == ""\\"" :",0.8820016898747209,26.356013563443188
"def doWorkForFindAll(self, v, target, partialMatch): sibling = self while sibling: c1 = partialMatch and sibling.equalsTreePartial(target) if c1: v.append(sibling) else: c2 = not partialMatch and sibling.equalsTree(target) <mask>: v.append(sibling) ### regardless of match or not, check any children for matches if sibling.getFirstChild(): sibling.getFirstChild().doWorkForFindAll(v, target, partialMatch) sibling = sibling.getNextSibling()",True,if c2 :,if c2 :,0.8820016898747209,0.0
"def __view_beside(self, onsideof, **kwargs): bounds = self.info[""bounds""] min_dist, found = -1, None for ui in UiObject(self.session, Selector(**kwargs)): dist = onsideof(bounds, ui.info[""bounds""]) <mask>: min_dist, found = dist, ui return found",False,if dist >= 0 and ( min_dist < 0 or dist < min_dist ) :,if dist < min_dist :,0.8820016898747209,11.039212850851191
"def __eq__(self, other): if isinstance(other, numeric_range): empty_self = not bool(self) empty_other = not bool(other) <mask>: return empty_self and empty_other # True if both empty else: return ( self._start == other._start and self._step == other._step and self._get_by_index(-1) == other._get_by_index(-1) ) else: return False",False,if empty_self or empty_other :,if empty_self and empty_other :,0.8820016898747209,59.694917920196445
"def _buffered_generator(self, size): buf = [] c_size = 0 push = buf.append while 1: try: while c_size < size: c = next(self._gen) push(c) if c: c_size += 1 except StopIteration: <mask>: return yield concat(buf) del buf[:] c_size = 0",False,if not c_size :,if c_size >= size :,0.8820016898747209,25.848657697858535
"def connect(self): with self._conn_lock: <mask>: raise Exception( ""Error, database not properly initialized "" ""before opening connection"" ) with self.exception_wrapper(): self.__local.conn = self._connect(self.database, **self.connect_kwargs) self.__local.closed = False self.initialize_connection(self.__local.conn)",False,if self . deferred :,if self . __local . conn is None :,0.8820016898747209,14.991106946711685
"def _merge_substs(self, subst, new_substs): subst = subst.copy() for new_subst in new_substs: for name, var in new_subst.items(): <mask>: subst[name] = var elif subst[name] is not var: subst[name].PasteVariable(var) return subst",False,if name not in subst :,"if isinstance ( var , Variable ) :",0.8820016898747209,6.567274736060395
"def remove(self, tag): """"""Removes a tag recursively from all containers."""""" new_contents = [] self.content_size = 0 for element in self.contents: if element.name != tag: new_contents.append(element) <mask>: element.remove(tag) self.content_size += element.size() self.contents = new_contents",False,"if isinstance ( element , Container ) :",if element . size ( ) > self . content_size :,0.8820016898747209,4.6192151051305474
"def _create_object(self, obj_body): props = obj_body[SYMBOL_PROPERTIES] for prop_name, prop_value in props.items(): if isinstance(prop_value, dict) and prop_value: # get the first key as the convert function func_name = list(prop_value.keys())[0] <mask>: func = getattr(self, func_name) props[prop_name] = func(prop_value[func_name]) if SYMBOL_TYPE in obj_body and obj_body[SYMBOL_TYPE] in self.fake_func_mapping: return self.fake_func_mapping[obj_body[SYMBOL_TYPE]](**props) else: return props",False,"if func_name . startswith ( ""_"" ) :",if func_name in self . fake_func_mapping :,0.8820016898747209,22.242469397936766
"def visit_try_stmt(self, o: ""mypy.nodes.TryStmt"") -> str: a = [o.body] # type: List[Any] for i in range(len(o.vars)): a.append(o.types[i]) <mask>: a.append(o.vars[i]) a.append(o.handlers[i]) if o.else_body: a.append((""Else"", o.else_body.body)) if o.finally_body: a.append((""Finally"", o.finally_body.body)) return self.dump(a, o)",False,if o . vars [ i ] :,if o . handlers :,0.8820016898747209,23.4500081062036
"def everythingIsUnicode(d): """"""Takes a dictionary, recursively verifies that every value is unicode"""""" for k, v in d.iteritems(): if isinstance(v, dict) and k != ""headers"": if not everythingIsUnicode(v): return False elif isinstance(v, list): for i in v: if isinstance(i, dict) and not everythingIsUnicode(i): return False <mask>: return False elif isinstance(v, _bytes): return False return True",False,"elif isinstance ( i , _bytes ) :","elif isinstance ( i , unicode ) and not everythingIsUnicode ( i ) :",0.8820016898747209,30.576902884505124
"def msg_ser(inst, sformat, lev=0): if sformat in [""urlencoded"", ""json""]: if isinstance(inst, Message): res = inst.serialize(sformat, lev) else: res = inst elif sformat == ""dict"": if isinstance(inst, Message): res = inst.serialize(sformat, lev) <mask>: res = inst elif isinstance(inst, str): # Iff ID Token res = inst else: raise MessageException(""Wrong type: %s"" % type(inst)) else: raise PyoidcError(""Unknown sformat"", inst) return res",True,"elif isinstance ( inst , dict ) :","elif isinstance ( inst , dict ) :",0.8820016898747209,100.00000000000004
"def start_container_if_stopped(self, container, attach_logs=False, quiet=False): if not container.is_running: <mask>: log.info(""Starting %s"" % container.name) if attach_logs: container.attach_log_stream() return self.start_container(container)",False,if not quiet :,if quiet :,0.8820016898747209,0.0
"def layer_op(self, input_image, mask=None): if not isinstance(input_image, dict): self._set_full_border(input_image) input_image = np.pad(input_image, self.full_border, mode=self.mode) return input_image, mask for name, image in input_image.items(): self._set_full_border(image) <mask>: tf.logging.warning( ""could not pad, dict name %s not in %s"", name, self.image_name ) continue input_image[name] = np.pad(image, self.full_border, mode=self.mode) return input_image, mask",True,if name not in self . image_name :,if name not in self . image_name :,0.8820016898747209,100.00000000000004
"def __Suffix_Noun_Step2b(self, token): for suffix in self.__suffix_noun_step2b: <mask>: token = token[:-2] self.suffix_noun_step2b_success = True break return token",False,if token . endswith ( suffix ) and len ( token ) >= 5 :,if token [ - 2 : ] == suffix :,0.8820016898747209,5.982491996190264
"def replace_header_items(ps, replacments): match = read_while(ps, header_item_or_end_re.match, lambda match: match is None) while not ps.current_line.startswith(""*/""): match = header_item_re.match(ps.current_line) <mask>: key = match.groupdict()[""key""] if key in replacments: ps.current_line = match.expand( ""\g<key>\g<space>%s\n"" % replacments[key] ) ps.read_line()",False,if match is not None :,if match :,0.8820016898747209,0.0
"def __projectBookmark(widget, location): script = None while widget is not None: <mask>: script = widget.scriptNode() if isinstance(script, Gaffer.ScriptNode): break widget = widget.parent() if script is not None: p = script.context().substitute(location) if not os.path.exists(p): try: os.makedirs(p) except OSError: pass return p else: return os.getcwd()",False,"if hasattr ( widget , ""scriptNode"" ) :","if isinstance ( widget , Gaffer . Widget ) :",0.8820016898747209,20.556680845025987
"def events_to_str(event_field, all_events): result = [] for (flag, string) in all_events: c_flag = flag if event_field & c_flag: result.append(string) event_field = event_field & (~c_flag) <mask>: break if event_field: result.append(hex(event_field)) return ""|"".join(result)",False,if not event_field :,if event_field == 0 :,0.8820016898747209,23.356898886410015
"def get_s3_bucket_locations(buckets, self_log=False): """"""return (bucket_name, prefix) for all s3 logging targets"""""" for b in buckets: if b.get(""Logging""): if self_log: if b[""Name""] != b[""Logging""][""TargetBucket""]: continue yield (b[""Logging""][""TargetBucket""], b[""Logging""][""TargetPrefix""]) <mask>: yield (b[""Name""], """")",False,"if not self_log and b [ ""Name"" ] . startswith ( ""cf-templates-"" ) :","if b [ ""Logging"" ] [ ""TargetPrefix"" ] :",0.8820016898747209,9.774695706598658
"def extract_file(tgz, tarinfo, dst_path, buffer_size=10 << 20, log_function=None): """"""Extracts 'tarinfo' from 'tgz' and writes to 'dst_path'."""""" src = tgz.extractfile(tarinfo) if src is None: return dst = tf.compat.v1.gfile.GFile(dst_path, ""wb"") while 1: buf = src.read(buffer_size) if not buf: break dst.write(buf) <mask>: log_function(len(buf)) dst.close() src.close()",True,if log_function is not None :,if log_function is not None :,0.8820016898747209,100.00000000000004
"def make_index_fields(rec): fields = {} for k, v in rec.iteritems(): if k in (""lccn"", ""oclc"", ""isbn""): fields[k] = v continue <mask>: fields[""title""] = [read_short_title(v)] return fields",False,"if k == ""full_title"" :",if v :,0.8820016898747209,0.0
"def disconnect_application(self): if not self.is_app_running(self.APP_BACKDROP): self.socket.send(commands.CloseCommand(destination_id=False)) start_time = time.time() while not self.is_app_running(None): try: self.socket.send_and_wait(commands.StatusCommand()) except cast_socket.ConnectionTerminatedException: break current_time = time.time() <mask>: raise TimeoutException() time.sleep(self.WAIT_INTERVAL) else: logger.debug(""Closing not necessary. Backdrop is running ..."")",False,if current_time - start_time > self . timeout :,if current_time - start_time > self . WAIT_INTERVAL :,0.8820016898747209,71.66258375282708
"def matches(self, cursor_offset, line, **kwargs): cs = lineparts.current_string(cursor_offset, line) if cs is None: return None matches = set() username = cs.word.split(os.path.sep, 1)[0] user_dir = os.path.expanduser(username) for filename in self.safe_glob(os.path.expanduser(cs.word)): if os.path.isdir(filename): filename += os.path.sep <mask>: filename = username + filename[len(user_dir) :] matches.add(filename) return matches",False,"if cs . word . startswith ( ""~"" ) :",if filename . startswith ( user_dir ) :,0.8820016898747209,16.830386789031852
"def eventFilter(self, obj, event): if event.type() == QEvent.MouseButtonPress: button = event.button() <mask>: self._app.browser.back() return True elif button == Qt.ForwardButton: self._app.browser.forward() return True return False",True,if button == Qt . BackButton :,if button == Qt . BackButton :,0.8820016898747209,100.00000000000004
"def reset_parameters(self): for m in self.modules(): if isinstance(m, nn.Embedding): continue <mask>: nn.init.constant_(m.weight, 0.1) nn.init.constant_(m.bias, 0) else: for p in m.parameters(): nn.init.normal_(p, 0, 0.1)",False,"elif isinstance ( m , nn . LayerNorm ) :","if isinstance ( m , nn . Conv2d ) :",0.8820016898747209,58.14307369682194
"def get_scalding_core(self): lib_dir = os.path.join(self.scalding_home, ""lib"") for j in os.listdir(lib_dir): <mask>: p = os.path.join(lib_dir, j) logger.debug(""Found scalding-core: %s"", p) return p raise luigi.contrib.hadoop.HadoopJobError(""Could not find scalding-core."")",False,"if j . startswith ( ""scalding-core-"" ) :","if os . path . exists ( os . path . join ( lib_dir , j ) ) :",0.8820016898747209,4.780204393760627
"def save(self): """"""Saves a new set of golden output frames to disk."""""" for pixels, (relative_to_assets, filename) in zip( self.iter_render(), self._iter_paths() ): full_directory_path = os.path.join(self._ASSETS_DIR, relative_to_assets) <mask>: os.makedirs(full_directory_path) path = os.path.join(full_directory_path, filename) _save_pixels(pixels, path)",True,if not os . path . exists ( full_directory_path ) :,if not os . path . exists ( full_directory_path ) :,0.8820016898747209,100.00000000000004
"def _fix_var_naming(operators, names, mod=""input""): new_names = [] map = {} for op in operators: <mask>: iter = op.inputs else: iter = op.outputs for i in iter: for name in names: if i.raw_name == name and name not in map: map[i.raw_name] = i.full_name if len(map) == len(names): break for name in names: new_names.append(map[name]) return new_names",True,"if mod == ""input"" :","if mod == ""input"" :",0.8820016898747209,100.00000000000004
"def Tokenize(s): # type: (str) -> Iterator[Token] for item in TOKEN_RE.findall(s): # The type checker can't know the true type of item! item = cast(TupleStr4, item) if item[0]: typ = ""number"" val = item[0] <mask>: typ = ""name"" val = item[1] elif item[2]: typ = item[2] val = item[2] elif item[3]: typ = item[3] val = item[3] yield Token(typ, val)",True,elif item [ 1 ] :,elif item [ 1 ] :,0.8820016898747209,100.00000000000004
"def init_errorhandler(): # http error handling for ex in default_exceptions: if ex < 500: app.register_error_handler(ex, error_http) <mask>: app.register_error_handler(ex, internal_error) if services.ldap: # Only way of catching the LDAPException upon logging in with LDAP server down @app.errorhandler(services.ldap.LDAPException) def handle_exception(e): log.debug(""LDAP server not accessible while trying to login to opds feed"") return error_http(FailedDependency())",False,elif ex == 500 :,elif ex < 500 :,0.8820016898747209,24.736929544091932
"def decode(self, ids): ids = pad_decr(ids) tokens = [] for int_id in ids: <mask>: tokens.append(self._vocab_list[int_id]) else: tokens.append(self._oov_token) return self._decode_token_separator.join(tokens)",False,if int_id < len ( self . _vocab_list ) :,if int_id in self . _vocab_list :,0.8820016898747209,50.57032536203352
"def remove_contest(contest_id): with SessionGen() as session: contest = session.query(Contest).filter(Contest.id == contest_id).first() if not contest: print(""No contest with id %s found."" % contest_id) return False contest_name = contest.name <mask>: print(""Not removing contest `%s'."" % contest_name) return False session.delete(contest) session.commit() print(""Contest `%s' removed."" % contest_name) return True",False,if not ask ( contest ) :,if not contest_name :,0.8820016898747209,16.341219448835542
def get_hi_lineno(self): lineno = Node.get_hi_lineno(self) if self.expr1 is None: pass else: lineno = self.expr1.get_hi_lineno() if self.expr2 is None: pass else: lineno = self.expr2.get_hi_lineno() <mask>: pass else: lineno = self.expr3.get_hi_lineno() return lineno,True,if self . expr3 is None :,if self . expr3 is None :,0.8820016898747209,100.00000000000004
"def _send_internal(self, bytes_): # buffering if self.pendings: self.pendings += bytes_ bytes_ = self.pendings try: # reconnect if possible self._reconnect() # send message self.socket.sendall(bytes_) # send finished self.pendings = None except Exception: # pylint: disable=broad-except # close socket self._close() # clear buffer if it exceeds max bufer size <mask>: # TODO: add callback handler here self.pendings = None else: self.pendings = bytes_",False,if self . pendings and ( len ( self . pendings ) > self . bufmax ) :,if bytes_ > self . bufer_size :,0.8820016898747209,7.974423230253494
"def _unpack(self, fmt, byt): d = unpack(self._header[""byteorder""] + fmt, byt)[0] if fmt[-1] in self.MISSING_VALUES: nmin, nmax = self.MISSING_VALUES[fmt[-1]] if d < nmin or d > nmax: <mask>: return StataMissingValue(nmax, d) else: return None return d",False,if self . _missing_values :,if d < nmin :,0.8820016898747209,6.9717291216921975
"def tuple_iter(self): for x in range( self.center.x - self.max_radius, self.center.x + self.max_radius + 1 ): for y in range( self.center.y - self.max_radius, self.center.y + self.max_radius + 1 ): <mask>: yield (x, y)",False,"if self . min_radius <= self . center . distance ( ( x , y ) ) <= self . max_radius :",if x <= y <= self . center . x :,0.8820016898747209,14.94135887537907
"def _parse_gene(element): for genename_element in element: <mask>: ann_key = ""gene_%s_%s"" % ( genename_element.tag.replace(NS, """"), genename_element.attrib[""type""], ) if genename_element.attrib[""type""] == ""primary"": self.ParsedSeqRecord.annotations[ann_key] = genename_element.text else: append_to_annotations(ann_key, genename_element.text)",False,"if ""type"" in genename_element . attrib :",if genename_element . tag . startswith ( NS ) :,0.8820016898747209,23.462350320527996
"def invalidateDependentSlices(self, iFirstCurve): # only user defined curve can have slice dependency relationships if self.isSystemCurveIndex(iFirstCurve): return nCurves = self.getNCurves() for i in range(iFirstCurve, nCurves): c = self.getSystemCurve(i) if isinstance(c.getSymbol().getSymbolType(), SymbolType.PieSliceSymbolType): c.invalidate() <mask>: # if first curve isn't a slice, break # there are no dependent slices",False,elif i == iFirstCurve :,if c . isDependent ( ) :,0.8820016898747209,6.567274736060395
"def gen_app_versions(self): for app_config in apps.get_app_configs(): name = app_config.verbose_name app = app_config.module version = self.get_app_version(app) <mask>: yield app.__name__, name, version",True,if version :,if version :,0.8820016898747209,0.0
"def verify_relative_valid_path(root, path): if len(path) < 1: raise PackagerError(""Empty chown path"") checkpath = root parts = path.split(os.sep) for part in parts: if part in (""."", ""..""): raise PackagerError("". and .. is not allowed in chown path"") checkpath = os.path.join(checkpath, part) relpath = checkpath[len(root) + 1 :] <mask>: raise PackagerError(f""chown path {relpath} does not exist"") if os.path.islink(checkpath): raise PackagerError(f""chown path {relpath} is a soft link"")",True,if not os . path . exists ( checkpath ) :,if not os . path . exists ( checkpath ) :,0.8820016898747209,100.00000000000004
"def create_or_update_tag_at_scope(cmd, resource_id=None, tags=None, tag_name=None): rcf = _resource_client_factory(cmd.cli_ctx) if resource_id is not None: <mask>: raise IncorrectUsageError(""Tags could not be empty."") Tags = cmd.get_models(""Tags"") tag_obj = Tags(tags=tags) return rcf.tags.create_or_update_at_scope(scope=resource_id, properties=tag_obj) return rcf.tags.create_or_update(tag_name=tag_name)",False,if not tags :,if tags is None :,0.8820016898747209,14.058533129758727
"def generate_auto_complete(self, base, iterable_var): sugg = [] for entry in iterable_var: compare_entry = entry compare_base = base if self.settings.get(IGNORE_CASE_SETTING): compare_entry = compare_entry.lower() compare_base = compare_base.lower() <mask>: if entry not in sugg: sugg.append(entry) return sugg",False,"if self . compare_entries ( compare_entry , compare_base ) :",if compare_entry == compare_base :,0.8820016898747209,15.491846006709249
"def createFields(self): yield String(self, ""dict_start"", 2) while not self.eof: addr = self.absolute_address + self.current_size <mask>: for field in parsePDFType(self): yield field else: break yield String(self, ""dict_end"", 2)",False,"if self . stream . readBytes ( addr , 2 ) != "">>"" :",if addr < self . absolute_address :,0.8820016898747209,4.410929085933151
"def Visit_and_test(self, node): # pylint: disable=invalid-name # and_test ::= not_test ('and' not_test)* for child in node.children: self.Visit(child) <mask>: _AppendTokenSubtype(child, format_token.Subtype.BINARY_OPERATOR)",False,"if isinstance ( child , pytree . Leaf ) and child . value == ""and"" :","if isinstance ( child , ast . Expr ) :",0.8820016898747209,17.96191510244705
"def getfiledata(directories): columns = None data = [] counter = 1 for directory in directories: for f in os.listdir(directory): if not os.path.isfile(os.path.join(directory, f)): continue counter += 1 st = os.stat(os.path.join(directory, f)) <mask>: columns = [""rowid"", ""name"", ""directory""] + [ x for x in dir(st) if x.startswith(""st_"") ] data.append([counter, f, directory] + [getattr(st, x) for x in columns[3:]]) return columns, data",True,if columns is None :,if columns is None :,0.8820016898747209,100.00000000000004
"def copy_attributes(info_add, obj, name_fmt, attributes, formatter=None): for attr in attributes: value = getattr(obj, attr, None) if value is None: continue name = name_fmt % attr <mask>: value = formatter(attr, value) info_add(name, value)",True,if formatter is not None :,if formatter is not None :,0.8820016898747209,100.00000000000004
"def main(args): ap = argparse.ArgumentParser() ap.add_argument(""job_ids"", nargs=""+"", type=int, help=""ID of a running job"") ns = ap.parse_args(args) _stash = globals()[""_stash""] """""":type : StaSh"""""" for job_id in ns.job_ids: <mask>: print(""killing job {} ..."".format(job_id)) worker = _stash.runtime.worker_registry.get_worker(job_id) worker.kill() time.sleep(1) else: print(""error: no such job with id: {}"".format(job_id)) break",False,if job_id in _stash . runtime . worker_registry :,if _stash . runtime . worker_registry . has_job ( job_id ) :,0.8820016898747209,45.80519369844352
"def _check_choice(self): if self.type == ""choice"": if self.choices is None: raise OptionError(""must supply a list of choices for type 'choice'"", self) <mask>: raise OptionError( ""choices must be a list of strings ('%s' supplied)"" % str(type(self.choices)).split(""'"")[1], self, ) elif self.choices is not None: raise OptionError(""must not supply choices for type %r"" % self.type, self)",False,"elif type ( self . choices ) not in ( types . TupleType , types . ListType ) :","elif not isinstance ( self . choices , list ) :",0.8820016898747209,14.921115647284694
"def add_file(pipe, srcpath, tgtpath): with open(srcpath, ""rb"") as handle: <mask>: write(pipe, enc(""M 100755 inline %s\n"" % tgtpath)) else: write(pipe, enc(""M 100644 inline %s\n"" % tgtpath)) data = handle.read() write(pipe, enc(""data %d\n"" % len(data))) write(pipe, enc(data)) write(pipe, enc(""\n""))",False,"if os . access ( srcpath , os . X_OK ) :",if os . path . exists ( tgtpath ) :,0.8820016898747209,14.320952289897711
"def cdf(self, x): if x == numpy.inf: return 1.0 else: # Inefficient sum. <mask>: raise RuntimeError(""Invalid value."") c = 0.0 for i in xrange(x + 1): c += self.probability(i) return c",False,if x != int ( x ) :,if x < 0 :,0.8820016898747209,10.62372743739878
"def convert_to_strings(self, out, seq_len): results = [] for b, batch in enumerate(out): utterances = [] for p, utt in enumerate(batch): size = seq_len[b][p] <mask>: transcript = """".join( map(lambda x: self.int_to_char[x.item()], utt[0:size]) ) else: transcript = """" utterances.append(transcript) results.append(utterances) return results",True,if size > 0 :,if size > 0 :,0.8820016898747209,100.00000000000004
"def get_date_range(self): if not hasattr(self, ""start"") or not hasattr(self, ""end""): args = (self.today.year, self.today.month) form = self.get_form() <mask>: args = (int(form.cleaned_data[""year""]), int(form.cleaned_data[""month""])) self.start = self.get_start(*args) self.end = self.get_end(*args) return self.start, self.end",False,if form . is_valid ( ) :,if form :,0.8820016898747209,0.0
"def save_stats(self): LOGGER.info(""Saving task-level statistics."") has_headers = os.path.isfile(paths.TABLE_COUNT_PATH) with open(paths.TABLE_COUNT_PATH, ""a"") as csvfile: headers = [""start_time"", ""database_name"", ""number_tables""] writer = csv.DictWriter( csvfile, delimiter="","", lineterminator=""\n"", fieldnames=headers ) <mask>: writer.writeheader() writer.writerow( { ""start_time"": self.start_time, ""database_name"": self.database_name, ""number_tables"": self.count, } )",False,if not has_headers :,if has_headers :,0.8820016898747209,57.89300674674101
"def _CheckCanaryCommand(self): <mask>: # fast path return with self._lock: if OpenStackVirtualMachine.command_works: return logging.info(""Testing OpenStack CLI command is installed and working"") cmd = os_utils.OpenStackCLICommand(self, ""image"", ""list"") stdout, stderr, _ = cmd.Issue() if stderr: raise errors.Config.InvalidValue( ""OpenStack CLI test command failed. Please make sure the OpenStack "" ""CLI client is installed and properly configured"" ) OpenStackVirtualMachine.command_works = True",False,if OpenStackVirtualMachine . command_works :,if os_utils . OpenStackCLICommand . command_works :,0.8820016898747209,36.72056269893591
"def test_windows_hidden(self): if not sys.platform == ""win32"": self.skipTest(""sys.platform is not windows"") return # FILE_ATTRIBUTE_HIDDEN = 2 (0x2) from GetFileAttributes documentation. hidden_mask = 2 with tempfile.NamedTemporaryFile() as f: # Hide the file using success = ctypes.windll.kernel32.SetFileAttributesW(f.name, hidden_mask) <mask>: self.skipTest(""unable to set file attributes"") self.assertTrue(hidden.is_hidden(f.name))",True,if not success :,if not success :,0.8820016898747209,100.00000000000004
"def recv_some(p, t=0.1, e=1, tr=5, stderr=0): if tr < 1: tr = 1 x = time.time() + t y = [] r = """" if stderr: pr = p.recv_err else: pr = p.recv while time.time() < x or r: r = pr() if r is None: break <mask>: y.append(r) else: time.sleep(max((x - time.time()) / tr, 0)) return b"""".join(y)",False,elif r :,if e :,0.8820016898747209,0.0
"def _is_xml(accepts): if accepts.startswith(b""application/""): has_xml = accepts.find(b""xml"") <mask>: semicolon = accepts.find(b"";"") if semicolon < 0 or has_xml < semicolon: return True return False",True,if has_xml > 0 :,if has_xml > 0 :,0.8820016898747209,100.00000000000004
"def times(self, value: int): if value is None: self._times = None else: try: candidate = int(value) except ValueError: # pylint: disable:raise-missing-from raise BarException(f""cannot set repeat times to: {value!r}"") if candidate < 0: raise BarException( f""cannot set repeat times to a value less than zero: {value}"" ) <mask>: raise BarException(""cannot set repeat times on a start Repeat"") self._times = candidate",False,"if self . direction == ""start"" :",if candidate > self . _start_repeat :,0.8820016898747209,10.552670315936318
"def __call__(self, *args, **kwargs): if not NET_INITTED: return self.raw(*args, **kwargs) for stack in traceback.walk_stack(None): if ""self"" in stack[0].f_locals: layer = stack[0].f_locals[""self""] <mask>: log.pytorch_layer_name = layer_names[layer] print(layer_names[layer]) break out = self.obj(self.raw, *args, **kwargs) # if isinstance(out,Variable): # out=[out] return out",True,if layer in layer_names :,if layer in layer_names :,0.8820016898747209,100.00000000000004
"def do_begin(self, byte): if byte.isspace(): return if byte != ""<"": <mask>: self._leadingBodyData = byte return ""bodydata"" self._parseError(""First char of document [{!r}] wasn't <"".format(byte)) return ""tagstart""",False,if self . beExtremelyLenient :,if byte in self . _leadingBodyData :,0.8820016898747209,13.134549472120788
"def pretty(self, n, comment=True): if isinstance(n, (str, bytes, list, tuple, dict)): r = repr(n) <mask>: # then it can be inside a comment! r = r.replace(""*/"", r""\x2a/"") return r if not isinstance(n, six.integer_types): return n if isinstance(n, constants.Constant): if comment: return ""%s /* %s */"" % (n, self.pretty(int(n))) else: return ""%s (%s)"" % (n, self.pretty(int(n))) elif abs(n) < 10: return str(n) else: return hex(n)",False,if not comment :,"if r . startswith ( ""/*"" ) :",0.8820016898747209,4.456882760699063
"def test_training_script_with_max_history_set(tmpdir): train_dialogue_model( DEFAULT_DOMAIN_PATH, DEFAULT_STORIES_FILE, tmpdir.strpath, interpreter=RegexInterpreter(), policy_config=""data/test_config/max_hist_config.yml"", kwargs={}, ) agent = Agent.load(tmpdir.strpath) for policy in agent.policy_ensemble.policies: if hasattr(policy.featurizer, ""max_history""): <mask>: assert policy.featurizer.max_history == 2 else: assert policy.featurizer.max_history == 5",False,if type ( policy ) == FormPolicy :,if policy . featurizer . max_history == 1 :,0.8820016898747209,8.516593018819643
"def cli_uninstall_distro(): distro_list = install_distro_list() if distro_list is not None: for index, _distro_dir in enumerate(distro_list): log(str(index) + "" --->> "" + _distro_dir) user_input = read_input_uninstall() <mask>: for index, _distro_dir in enumerate(distro_list): if index == user_input: config.uninstall_distro_dir_name = _distro_dir unin_distro() else: log(""No distro installed on "" + config.usb_disk)",False,if user_input is not False :,if user_input is not None :,0.8820016898747209,70.71067811865478
"def set_random_avatar(user): galleries = get_available_galleries(include_default=True) if not galleries: raise RuntimeError(""no avatar galleries are set"") avatars_list = [] for gallery in galleries: <mask>: avatars_list = gallery[""images""] break else: avatars_list += gallery[""images""] random_avatar = random.choice(avatars_list) store.store_new_avatar(user, Image.open(random_avatar.image))",False,"if gallery [ ""name"" ] == DEFAULT_GALLERY :","if gallery [ ""type"" ] == ""avatar"" :",0.8820016898747209,39.832871551569504
"def make_query(self, key, filters): meta = self.get_meta(key) q = {meta.facet_key: self.normalize_key(meta.path)} if filters: if filters.get(""has_fulltext"") == ""true"": q[""has_fulltext""] = ""true"" <mask>: q[""publish_year""] = filters[""publish_year""] return q",False,"if filters . get ( ""publish_year"" ) :","elif filters . get ( ""publish_year"" ) == ""true"" :",0.8820016898747209,55.81600587827485
"def test_named_parameters_and_constraints(self): likelihood = gpytorch.likelihoods.GaussianLikelihood() model = ExactGPModel(None, None, likelihood) for name, _param, constraint in model.named_parameters_and_constraints(): if name == ""likelihood.noise_covar.raw_noise"": self.assertIsInstance(constraint, gpytorch.constraints.GreaterThan) <mask>: self.assertIsNone(constraint) elif name == ""covar_module.raw_outputscale"": self.assertIsInstance(constraint, gpytorch.constraints.Positive) elif name == ""covar_module.base_kernel.raw_lengthscale"": self.assertIsInstance(constraint, gpytorch.constraints.Positive)",False,"elif name == ""mean_module.constant"" :","elif name == ""likelihood.covar_module.raw_output"" :",0.8820016898747209,33.64932442330151
"def _test_pooling(input_shape, **kwargs): _test_pooling_iteration(input_shape, **kwargs) if is_gpu_available(): <mask>: input_shape = [input_shape[ii] for ii in (0, 3, 1, 2)] kwargs[""data_format""] = ""NCHW"" _test_pooling_iteration(input_shape, **kwargs)",False,if len ( input_shape ) == 4 :,"if isinstance ( input_shape , tuple ) :",0.8820016898747209,27.3385351346167
"def init(self): r = self.get_redis() if r: key = ""pocsuite_target"" info_msg = ""[PLUGIN] try fetch targets from redis..."" logger.info(info_msg) targets = r.get(key) count = 0 <mask>: for target in targets: if self.add_target(target): count += 1 info_msg = ""[PLUGIN] get {0} target(s) from redis"".format(count) logger.info(info_msg)",True,if targets :,if targets :,0.8820016898747209,0.0
"def reload_json_api_settings(*args, **kwargs): django_setting = kwargs[""setting""] setting = django_setting.replace(JSON_API_SETTINGS_PREFIX, """") value = kwargs[""value""] if setting in DEFAULTS.keys(): if value is not None: setattr(json_api_settings, setting, value) <mask>: delattr(json_api_settings, setting)",False,"elif hasattr ( json_api_settings , setting ) :",elif setting in DEFAULTS . keys ( ) :,0.8820016898747209,8.591316733350183
"def update_metadata(self): for attrname in dir(self): if attrname.startswith(""__""): continue attrvalue = getattr(self, attrname, None) if attrvalue == 0: continue if attrname == ""salt_version"": attrname = ""version"" if hasattr(self.metadata, ""set_{0}"".format(attrname)): getattr(self.metadata, ""set_{0}"".format(attrname))(attrvalue) <mask>: try: setattr(self.metadata, attrname, attrvalue) except AttributeError: pass",False,"elif hasattr ( self . metadata , attrname ) :","if attrname != ""version"" :",0.8820016898747209,5.11459870708889
"def test_02_looking_at_listdir_path_(name): for dline in listdir.json(): <mask>: assert dline[""type""] in (""DIRECTORY"", ""FILE""), listdir.text assert dline[""uid""] == 0, listdir.text assert dline[""gid""] == 0, listdir.text assert dline[""name""] == name, listdir.text break else: raise AssertionError(f""/{path}/{name} not found"")",False,"if dline [ ""path"" ] == f""{path}/{name}"" :","if dline [ ""type"" ] == ""FILE"" :",0.8820016898747209,25.404753321283366
"def DeletePlugin(): oid = request.form.get(""oid"", """") if oid: result = Mongo.coll[""Plugin""].find_one_and_delete( {""_id"": ObjectId(oid)}, remove=True ) <mask>: result[""filename""] = result[""filename""] + "".py"" if os.path.exists(file_path + result[""filename""]): os.remove(file_path + result[""filename""]) return ""success"" return ""fail""",False,"if not result [ ""filename"" ] . find ( ""."" ) > - 1 :",if result :,0.8820016898747209,0.0
"def iterparent(self, node): """"""Iterator wrapper to get allowed parent and child all at once."""""" # We do not allow the marker inside a header as that # would causes an enless loop of placing a new TOC # inside previously generated TOC. for child in node: <mask>: yield node, child yield from self.iterparent(child)",False,"if not self . header_rgx . match ( child . tag ) and child . tag not in [ ""pre"" , ""code"" ] :","if isinstance ( child , TOC ) :",0.8820016898747209,0.9292405863554505
"def _get_matched_layout(command): # don't use command.split_script here because a layout mismatch will likely # result in a non-splitable script as per shlex cmd = command.script.split("" "") for source_layout in source_layouts: is_all_match = True for cmd_part in cmd: if not all([ch in source_layout or ch in ""-_"" for ch in cmd_part]): is_all_match = False break <mask>: return source_layout",True,if is_all_match :,if is_all_match :,0.8820016898747209,100.00000000000004
"def _update_tileable_and_chunk_shape(self, tileable_graph, chunk_result, failed_ops): for n in tileable_graph: if n.op in failed_ops: continue tiled_n = get_tiled(n) if has_unknown_shape(tiled_n): <mask>: # some of the chunks has been fused continue new_nsplits = self.get_tileable_nsplits(n, chunk_result=chunk_result) for node in (n, tiled_n): node._update_shape(tuple(sum(nsplit) for nsplit in new_nsplits)) tiled_n._nsplits = new_nsplits",False,if any ( c . key not in chunk_result for c in tiled_n . chunks ) :,if chunk_result is None :,0.8820016898747209,3.759098586913923
"def _get_items(self, name, target=1): all_items = self.get_items(name) items = [o for o in all_items if not o.disabled] if len(items) < target: if len(all_items) < target: raise ItemNotFoundError(""insufficient items with name %r"" % name) else: raise AttributeError(""insufficient non-disabled items with name %s"" % name) on = [] off = [] for o in items: <mask>: on.append(o) else: off.append(o) return on, off",False,if o . selected :,if o . disabled :,0.8820016898747209,42.72870063962342
def parse_flow_sequence_entry_mapping_value(self): if self.check_token(ValueToken): token = self.get_token() <mask>: self.states.append(self.parse_flow_sequence_entry_mapping_end) return self.parse_flow_node() else: self.state = self.parse_flow_sequence_entry_mapping_end return self.process_empty_scalar(token.end_mark) else: self.state = self.parse_flow_sequence_entry_mapping_end token = self.peek_token() return self.process_empty_scalar(token.start_mark),False,"if not self . check_token ( FlowEntryToken , FlowSequenceEndToken ) :","if token . end_mark == ""flow"" :",0.8820016898747209,4.659101701766641
"def serialize_config(self, session, key, tid, language): cache_key = gen_cache_key(key, tid, language) cache_obj = None if cache_key not in self.cache: <mask>: cache_obj = db_admin_serialize_node(session, tid, language) elif key == ""notification"": cache_obj = db_get_notification(session, tid, language) self.cache[cache_key] = cache_obj return self.cache[cache_key]",False,"if key == ""node"" :","if key == ""admin"" :",0.8820016898747209,59.4603557501361
"def get_lldp_neighbors(self): commands = [""show lldp neighbors""] output = self.device.run_commands(commands)[0][""lldpNeighbors""] lldp = {} for n in output: <mask>: lldp[n[""port""]] = [] lldp[n[""port""]].append( {""hostname"": n[""neighborDevice""], ""port"": n[""neighborPort""]} ) return lldp",False,"if n [ ""port"" ] not in lldp . keys ( ) :","if n [ ""port"" ] not in lldp :",0.8820016898747209,63.58420474065946
"def handle(self): from poetry.utils.env import EnvManager manager = EnvManager(self.poetry) current_env = manager.get() for venv in manager.list(): name = venv.path.name if self.option(""full-path""): name = str(venv.path) <mask>: self.line(""<info>{} (Activated)</info>"".format(name)) continue self.line(name)",False,if venv == current_env :,if name in current_env . activated :,0.8820016898747209,20.164945583740657
"def resolve_env_secrets(config, environ): """"""Create copy that recursively replaces {""$env"": ""NAME""} with values from environ"""""" if isinstance(config, dict): if list(config.keys()) == [""$env""]: return environ.get(list(config.values())[0]) <mask>: return open(list(config.values())[0]).read() else: return { key: resolve_env_secrets(value, environ) for key, value in config.items() } elif isinstance(config, list): return [resolve_env_secrets(value, environ) for value in config] else: return config",False,"elif list ( config . keys ( ) ) == [ ""$file"" ] :","elif list ( config . keys ( ) ) == [ ""NAME"" ] :",0.8820016898747209,78.09325628873462
"def _is_valid_16bit_as_path(cls, buf): two_byte_as_size = struct.calcsize(""!H"") while buf: (type_, num_as) = struct.unpack_from( cls._SEG_HDR_PACK_STR, six.binary_type(buf) ) if type_ is not cls._AS_SET and type_ is not cls._AS_SEQUENCE: return False buf = buf[struct.calcsize(cls._SEG_HDR_PACK_STR) :] <mask>: return False buf = buf[num_as * two_byte_as_size :] return True",False,if len ( buf ) < num_as * two_byte_as_size :,if num_as * two_byte_as_size != 2 :,0.8820016898747209,60.04287712485597
"def reparentChildren(self, newParent): if newParent.childNodes: newParent.childNodes[-1]._element.tail += self._element.text else: <mask>: newParent._element.text = """" if self._element.text is not None: newParent._element.text += self._element.text self._element.text = """" base.Node.reparentChildren(self, newParent)",False,if not newParent . _element . text :,if self . _element . text is None :,0.8820016898747209,42.7287006396234
"def get_operation_ast(document_ast, operation_name=None): operation = None for definition in document_ast.definitions: if isinstance(definition, ast.OperationDefinition): if not operation_name: # If no operation name is provided, only return an Operation if it is the only one present in the # document. This means that if we've encountered a second operation as we were iterating over the # definitions in the document, there are more than one Operation defined, and we should return None. if operation: return None operation = definition <mask>: return definition return operation",False,elif definition . name and definition . name . value == operation_name :,if operation_name == definition . name :,0.8820016898747209,17.65688614877521
"def reprSmart(vw, item): ptype = type(item) if ptype is int: <mask>: return str(item) elif vw.isValidPointer(item): return vw.reprPointer(item) else: return hex(item) elif ptype in (list, tuple): return reprComplex(vw, item) # recurse elif ptype is dict: return ""{%s}"" % "","".join( [""%s:%s"" % (reprSmart(vw, k), reprSmart(vw, v)) for k, v in item.items()] ) else: return repr(item)",False,if - 1024 < item < 1024 :,if vw . isValidString ( item ) :,0.8820016898747209,7.267884212102741
"def cleanDataCmd(cmd): newcmd = ""AbracadabrA ** <?php "" if cmd[:6] != ""php://"": <mask>: cmds = cmd.split(""&"") for c in cmds: if len(c) > 0: newcmd += ""system('%s');"" % c else: b64cmd = base64.b64encode(cmd) newcmd += ""system(base64_decode('%s'));"" % b64cmd else: newcmd += cmd[6:] newcmd += ""?> **"" return newcmd",False,if reverseConn not in cmd :,"if ""&"" in cmd :",0.8820016898747209,26.269098944241588
"def render_tasks(self) -> List: results = [] for task in self.tasks.values(): job_entry = self.jobs.get(task.job_id) <mask>: if not self.should_render_job(job_entry): continue files = self.get_file_counts([task]) entry = ( task.job_id, task.task_id, task.state, task.type.name, task.target, files, task.pool, task.end_time, ) results.append(entry) return results",True,if job_entry :,if job_entry :,0.8820016898747209,100.00000000000004
"def __call__(self, environ, start_response): for key in ""REQUEST_URL"", ""REQUEST_URI"", ""UNENCODED_URL"": if key not in environ: continue request_uri = unquote(environ[key]) script_name = unquote(environ.get(""SCRIPT_NAME"", """")) <mask>: environ[""PATH_INFO""] = request_uri[len(script_name) :].split(""?"", 1)[0] break return self.app(environ, start_response)",True,if request_uri . startswith ( script_name ) :,if request_uri . startswith ( script_name ) :,0.8820016898747209,100.00000000000004
"def _add_role_information(self, function_dict, role_id): # Make it easier to build rules based on policies attached to execution roles function_dict[""role_arn""] = role_id role_name = role_id.split(""/"")[-1] function_dict[ ""execution_role"" ] = await self.facade.awslambda.get_role_with_managed_policies(role_name) if function_dict.get(""execution_role""): statements = [] for policy in function_dict[""execution_role""].get(""policies""): <mask>: statements += policy[""Document""][""Statement""] function_dict[""execution_role""][""policy_statements""] = statements",False,"if ""Document"" in policy and ""Statement"" in policy [ ""Document"" ] :","if policy [ ""Document"" ] [ ""Policy"" ] :",0.8820016898747209,32.575790798096165
"def TryMerge(self, d): while d.avail() > 0: tt = d.getVarInt32() if tt == 8: self.set_ts(d.getVarInt64()) continue <mask>: raise ProtocolBuffer.ProtocolBufferDecodeError d.skipData(tt)",True,if tt == 0 :,if tt == 0 :,0.8820016898747209,100.00000000000004
"def format_counts(results, json_output=False, human_readable=False): if json_output: for result in results: yield json.dumps(result) else: for result in results: space_consumed = result.get(""spaceConsumed"") <mask>: space_consumed = _sizeof_fmt(int(result.get(""spaceConsumed""))) yield ""%12s %12s %18s %s"" % ( result.get(""directoryCount""), result.get(""fileCount""), space_consumed, result.get(""path""), )",True,if human_readable :,if human_readable :,0.8820016898747209,100.00000000000004
"def parse_edges(self, pcb): edges = [] drawings = list(pcb.GetDrawings()) bbox = None for m in pcb.GetModules(): for g in m.GraphicalItems(): drawings.append(g) for d in drawings: if d.GetLayer() == pcbnew.Edge_Cuts: parsed_drawing = self.parse_drawing(d) if parsed_drawing: edges.append(parsed_drawing) <mask>: bbox = d.GetBoundingBox() else: bbox.Merge(d.GetBoundingBox()) if bbox: bbox.Normalize() return edges, bbox",True,if bbox is None :,if bbox is None :,0.8820016898747209,100.00000000000004
"def __getitem__(self, k) -> ""SimMemView"": if isinstance(k, slice): if k.step is not None: raise ValueError(""Slices with strides are not supported"") <mask>: raise ValueError(""Must specify start index"") elif k.stop is not None: raise ValueError(""Slices with stop index are not supported"") else: addr = k.start elif self._type is not None and self._type._can_refine_int: return self._type._refine(self, k) else: addr = k return self._deeper(addr=addr)",True,elif k . start is None :,elif k . start is None :,0.8820016898747209,100.00000000000004
"def _parse(self, stream, context): obj = [] try: if self.subcon.conflags & self.FLAG_COPY_CONTEXT: while True: subobj = self.subcon._parse(stream, context.__copy__()) obj.append(subobj) <mask>: break else: while True: subobj = self.subcon._parse(stream, context) obj.append(subobj) if self.predicate(subobj, context): break except ConstructError as ex: raise ArrayError(""missing terminator"", ex) return obj",True,"if self . predicate ( subobj , context ) :","if self . predicate ( subobj , context ) :",0.8820016898747209,100.00000000000004
"def before_run(self, run_context): if ""featurizer"" in self.model_portion and ( self.need_to_refresh or self.refresh_base_model ): <mask>: self.refresh_base_model = True self.init_fn( None, run_context.session, self.model_portion, self.refresh_base_model ) self.need_to_refresh = False self.refresh_base_model = False",False,"if self . model_portion == ""whole_featurizer"" :",if self . refresh_base_model :,0.8820016898747209,12.58503278125222
"def run(self): while True: task = self.requestQueue.get() if task is None: # The ""None"" value is used as a sentinel by # ThreadPool.cleanup(). This indicates that there # are no more tasks, so we should quit. break try: <mask>: raise SCons.Errors.BuildError(task.targets[0], errstr=interrupt_msg) task.execute() except: task.exception_set() ok = False else: ok = True self.resultsQueue.put((task, ok))",False,if self . interrupted ( ) :,if task . targets :,0.8820016898747209,9.423716574733431
"def get_overdue_evergreen_documents(*, db_session) -> List[Optional[Document]]: """"""Returns all documents that have need had a recent evergreen notification."""""" documents = ( db_session.query(Document).filter(Document.evergreen == True) ).all() # noqa overdue_documents = [] now = datetime.utcnow() for d in documents: next_reminder = d.evergreen_last_reminder_at + timedelta( days=d.evergreen_reminder_interval ) <mask>: overdue_documents.append(d) return overdue_documents",False,if now > next_reminder :,if next_reminder < now :,0.8820016898747209,29.071536848410968
"def create_local_app_folder(local_app_path): if exists(local_app_path): raise ValueError(""There is already a '%s' folder! Aborting!"" % local_app_path) for folder in subfolders(local_app_path): if not exists(folder): os.mkdir(folder) init_path = join(folder, ""__init__.py"") <mask>: create_file(init_path)",True,if not exists ( init_path ) :,if not exists ( init_path ) :,0.8820016898747209,100.00000000000004
"def generate(): for leaf in u.leaves: <mask>: val = leaf.get_int_value() if val in (0, 1): yield val else: raise _NoBoolVector elif isinstance(leaf, Symbol): if leaf == SymbolTrue: yield 1 elif leaf == SymbolFalse: yield 0 else: raise _NoBoolVector else: raise _NoBoolVector",False,"if isinstance ( leaf , Integer ) :","if isinstance ( leaf , Symbol ) :",0.8820016898747209,59.4603557501361
"def replace(self, old, new): v_m = self.var_map size = v_m[self.size] if not (size.is_const() or size.is_ident()): size.replace(old, new) else: <mask>: v_m[new.value()] = new self.size = new.value() else: v_m[old] = new",True,if new . is_ident ( ) :,if new . is_ident ( ) :,0.8820016898747209,100.00000000000004
"def method_for_doctype(doctype): method = ""xhtml"" if doctype: if doctype.startswith(""html""): method = ""html"" <mask>: method = ""xhtml"" elif doctype.startswith(""svg""): method = ""xml"" else: method = ""xhtml"" return method",True,"elif doctype . startswith ( ""xhtml"" ) :","elif doctype . startswith ( ""xhtml"" ) :",0.8820016898747209,100.00000000000004
"def delete(self, trans, **kwd): idnum = kwd[self.tagged_item_id] item = self._get_item_from_id(trans, idnum, check_writable=True) if item is not None: ex_obj = self.get_item_extended_metadata_obj(trans, item) <mask>: self.unset_item_extended_metadata_obj(trans, item) self.delete_extended_metadata(trans, ex_obj)",False,if ex_obj is not None :,if ex_obj is None :,0.8820016898747209,61.29752413741059
"def check_testv(self, testv): test_good = True f = open(self.home, ""rb+"") for (offset, length, operator, specimen) in testv: data = self._read_share_data(f, offset, length) <mask>: test_good = False break f.close() return test_good",False,"if not testv_compare ( data , operator , specimen ) :",if not data :,0.8820016898747209,4.004304603105518
"def get_history_user(self, instance): """"""Get the modifying user from instance or middleware."""""" try: return instance._history_user except AttributeError: request = None try: <mask>: request = self.thread.request except AttributeError: pass return self.get_user(instance=instance, request=request)",False,if self . thread . request . user . is_authenticated :,if self . thread :,0.8820016898747209,14.276239697197271
"def _check(self, name, size=None, *extra): func = getattr(imageop, name) for height in VALUES: for width in VALUES: strlen = abs(width * height) <mask>: strlen *= size if strlen < MAX_LEN: data = ""A"" * strlen else: data = AAAAA if size: arguments = (data, size, width, height) + extra else: arguments = (data, width, height) + extra try: func(*arguments) except (ValueError, imageop.error): pass",True,if size :,if size :,0.8820016898747209,0.0
"def __setattr__(self, name, value): if name == ""path"": if value and value != """": if value[0] != ""/"": raise ValueError( 'The page path should always start with a slash (""/"").' ) elif name == ""load_time"": <mask>: raise ValueError( ""Page load time must be specified in integer milliseconds."" ) object.__setattr__(self, name, value)",False,"if value and not isinstance ( value , int ) :",if value and value < 0 :,0.8820016898747209,15.685718045401451
"def __repr__(self): if self._in_repr: return ""<recursion>"" try: self._in_repr = True if self.is_computed(): status = ""computed, "" <mask>: if self.value() is self: status += ""= self"" else: status += ""= "" + repr(self.value()) else: status += ""error = "" + repr(self.error()) else: status = ""isn't computed"" return ""%s (%s)"" % (type(self), status) finally: self._in_repr = False",False,if self . error ( ) is None :,if self . error ( ) :,0.8820016898747209,63.191456189157286
"def _exclude_node(self, name): if ""exclude_nodes"" in self.node_filters: <mask>: self.loggit.info('Excluding node ""{0}"" due to node_filters'.format(name)) return True return False",False,"if name in self . node_filters [ ""exclude_nodes"" ] :","if self . node_filters [ ""exclude_nodes"" ] [ name ] :",0.8820016898747209,72.97627709554281
"def enumerate_projects(): """"""List projects in _DEFAULT_APP_DIR."""""" src_path = os.path.join(_DEFAULT_APP_DIR, ""src"") projects = {} for project in os.listdir(src_path): projects[project] = [] project_path = os.path.join(src_path, project) for file in os.listdir(project_path): <mask>: projects[project].append(file[:-8]) return projects",False,"if file . endswith ( "".gwt.xml"" ) :","if file . endswith ( "".py"" ) :",0.8820016898747209,58.50343668259105
"def zip_readline_read_test(self, f, compression): self.make_test_archive(f, compression) # Read the ZIP archive with zipfile.ZipFile(f, ""r"") as zipfp, zipfp.open(TESTFN) as zipopen: data = b"""" while True: read = zipopen.readline() <mask>: break data += read read = zipopen.read(100) if not read: break data += read self.assertEqual(data, self.data)",True,if not read :,if not read :,0.8820016898747209,100.00000000000004
"def f(view, s): if mode == modes.NORMAL: return sublime.Region(0) elif mode == modes.VISUAL: <mask>: return sublime.Region(s.a + 1, 0) else: return sublime.Region(s.a, 0) elif mode == modes.INTERNAL_NORMAL: return sublime.Region(view.full_line(s.b).b, 0) elif mode == modes.VISUAL_LINE: if s.a < s.b: return sublime.Region(0, s.b) else: return sublime.Region(0, s.a) return s",True,if s . a < s . b :,if s . a < s . b :,0.8820016898747209,100.00000000000004
def response(self): try: response = requests.get(str(self)) rjson = response.json() <mask>: raise Exception(response.text) return rjson except Exception as e: raise ResponseFanartError(str(e)),False,"if not isinstance ( rjson , dict ) :",if rjson is None :,0.8820016898747209,6.316906128202129
"def __get_type(self, cexpr): """"""Returns one of the following types: 'R' - read value, 'W' - write value, 'A' - function argument"""""" child = cexpr for p in reversed(self.parents): assert p, ""Failed to get type at "" + helper.to_hex(self.__function_address) if p.cexpr.op == idaapi.cot_call: return ""Arg"" if not p.is_expr(): return ""R"" if p.cexpr.op == idaapi.cot_asg: <mask>: return ""W"" return ""R"" child = p.cexpr",False,if p . cexpr . x == child :,if child . is_expr ( ) :,0.8820016898747209,6.033504141761816
"def _extract_lemma(self, parse: Parse) -> str: special_feats = [x for x in self.SPECIAL_FEATURES if x in parse.tag] if len(special_feats) == 0: return parse.normal_form # here we process surnames and patronyms since PyMorphy lemmatizes them incorrectly for other in parse.lexeme: tag = other.tag <mask>: continue if ( tag.case == ""nomn"" and tag.gender == parse.tag.gender and tag.number == ""sing"" ): return other.word return parse.normal_form",False,if any ( x not in tag for x in special_feats ) :,"if tag . case == ""surnames"" and tag . gender == parse . tag . gender and tag . number == ""patronyms"" :",0.8820016898747209,1.633898440122165
"def evaluateWord(self, argument): wildcard_count = argument[0].count(""*"") if wildcard_count > 0: if wildcard_count == 1 and argument[0].startswith(""*""): return self.GetWordWildcard(argument[0][1:], method=""endswith"") <mask>: return self.GetWordWildcard(argument[0][:-1], method=""startswith"") else: _regex = argument[0].replace(""*"", "".+"") matched = False for w in self.words: matched = bool(re.search(_regex, w)) if matched: break return matched return self.GetWord(argument[0])",False,"if wildcard_count == 1 and argument [ 0 ] . endswith ( ""*"" ) :","elif wildcard_count == 2 and argument [ 0 ] . endswith ( ""*"" ) :",0.8820016898747209,80.3154665668484
def getAllEntries(self): entries = [] for bucket in self.buckets: last = None for entry in bucket.entries: if last is not None: last.size = entry.virtualOffset - last.virtualOffset last = entry entries.append(entry) <mask>: entries[-1].size = bucket.endOffset - entries[-1].virtualOffset return entries,False,if len ( entries ) != 0 :,if entries :,0.8820016898747209,0.0
def clean(self): if self._ctx: <mask>: libcrypto.EVP_CIPHER_CTX_cleanup(self._ctx) else: libcrypto.EVP_CIPHER_CTX_reset(self._ctx) libcrypto.EVP_CIPHER_CTX_free(self._ctx),False,"if hasattr ( libcrypto , ""EVP_CIPHER_CTX_cleanup"" ) :",if self . _ctx . get_mode ( ) == libcrypto . EVP_CIPHER_CTX_MODE_NONE :,0.8820016898747209,21.592878551044475
"def _addTab(self, name, label, idx=None): label = getLanguageString(label) tab = Tab(self, name, label) tab.idx = self._makeTab(tab, idx) if idx != None: # Update index list when inserting tabs at arbitrary positions newIdxList = {} for tIdx, t in list(self._tabs_by_idx.items()): <mask>: t.idx += 1 newIdxList[t.idx] = t self._tabs_by_idx = newIdxList self._tabs_by_idx[tab.idx] = tab self._tabs_by_name[tab.name] = tab return tab",False,if int ( tIdx ) >= idx :,if t . idx == idx :,0.8820016898747209,19.493995755254467
"def set(self, _key, _new_login=True): with self.lock: user = self.users.get(current_user.id, None) <mask>: self.users[current_user.id] = dict(session_count=1, key=_key) else: if _new_login: user[""session_count""] += 1 user[""key""] = _key",True,if user is None :,if user is None :,0.8820016898747209,100.00000000000004
"def stop(self): # Try to shut the connection down, but if we get any sort of # errors, go ahead and ignore them.. as we're shutting down anyway try: self.rpcserver.stop() <mask>: self.backend_rpcserver.stop() if self.cluster_rpcserver: self.cluster_rpcserver.stop() except Exception: pass if self.coordination: try: coordination.COORDINATOR.stop() except Exception: pass super(Service, self).stop(graceful=True)",True,if self . backend_rpcserver :,if self . backend_rpcserver :,0.8820016898747209,100.00000000000004
"def __genmenuOnlyAllocated(menu): for submenu in menu.Submenus: __genmenuOnlyAllocated(submenu) if menu.OnlyUnallocated == True: tmp[""cache""].addMenuEntries(menu.AppDirs) menuentries = [] for rule in menu.Rules: menuentries = rule.do( tmp[""cache""].getMenuEntries(menu.AppDirs), rule.Type, 2 ) for menuentry in menuentries: <mask>: menuentry.Parents.append(menu) # menuentry.Add = False # menuentry.Allocated = True menu.MenuEntries.append(menuentry)",True,if menuentry . Add == True :,if menuentry . Add == True :,0.8820016898747209,100.00000000000004
"def __init__(self, **options): self.func_name_highlighting = get_bool_opt(options, ""func_name_highlighting"", True) self.disabled_modules = get_list_opt(options, ""disabled_modules"", []) self._functions = set() if self.func_name_highlighting: from pygments.lexers._lua_builtins import MODULES for mod, func in iteritems(MODULES): <mask>: self._functions.update(func) RegexLexer.__init__(self, **options)",False,if mod not in self . disabled_modules :,if mod in self . disabled_modules :,0.8820016898747209,71.89393375176813
"def recv_some(p, t=0.1, e=1, tr=5, stderr=0): if tr < 1: tr = 1 x = time.time() + t y = [] r = """" if stderr: pr = p.recv_err else: pr = p.recv while time.time() < x or r: r = pr() <mask>: break elif r: y.append(r) else: time.sleep(max((x - time.time()) / tr, 0)) return """".join(y)",False,if r is None :,"if r == """" :",0.8820016898747209,14.535768424205482
"def get_menu_items(node): aList = [] for child in node.children: for tag in (""@menu"", ""@item""): if child.h.startswith(tag): name = child.h[len(tag) + 1 :].strip() <mask>: aList.append((""%s %s"" % (tag, name), get_menu_items(child), None)) else: b = g.splitLines("""".join(child.b)) aList.append((tag, name, b[0] if b else """")) break return aList",False,"if tag == ""@menu"" :",if child . b is None :,0.8820016898747209,5.868924818816531
"def import_suffix_generator(a_block, datatype=False): if datatype is False: for name, suffix in iteritems(a_block.component_map(Suffix)): <mask>: yield name, suffix else: for name, suffix in iteritems(a_block.component_map(Suffix)): if (suffix.import_enabled() is True) and ( suffix.get_datatype() is datatype ): yield name, suffix",True,if suffix . import_enabled ( ) is True :,if suffix . import_enabled ( ) is True :,0.8820016898747209,100.00000000000004
"def verify_relative_valid_path(root, path): if len(path) < 1: raise PackagerError(""Empty chown path"") checkpath = root parts = path.split(os.sep) for part in parts: if part in (""."", ""..""): raise PackagerError("". and .. is not allowed in chown path"") checkpath = os.path.join(checkpath, part) relpath = checkpath[len(root) + 1 :] if not os.path.exists(checkpath): raise PackagerError(f""chown path {relpath} does not exist"") <mask>: raise PackagerError(f""chown path {relpath} is a soft link"")",False,if os . path . islink ( checkpath ) :,if not os . path . islink ( checkpath ) :,0.8820016898747209,80.70557274927978
"def load_syntax(syntax): context = _create_scheme() or {} partition_scanner = PartitionScanner(syntax.get(""partitions"", [])) scanners = {} for part_name, part_scanner in list(syntax.get(""scanner"", {}).items()): scanners[part_name] = Scanner(part_scanner) formats = [] for fname, fstyle in list(syntax.get(""formats"", {}).items()): <mask>: if fstyle.startswith(""%("") and fstyle.endswith("")s""): key = fstyle[2:-2] fstyle = context[key] else: fstyle = fstyle % context formats.append((fname, fstyle)) return partition_scanner, scanners, formats",False,"if isinstance ( fstyle , basestring ) :","if isinstance ( fstyle , str ) :",0.8820016898747209,59.4603557501361
"def should_keep_alive(commit_msg): result = False ci = get_current_ci() or """" for line in commit_msg.splitlines(): parts = line.strip(""# "").split("":"", 1) (key, val) = parts if len(parts) > 1 else (parts[0], """") <mask>: ci_names = val.replace("","", "" "").lower().split() if val else [] if len(ci_names) == 0 or ci.lower() in ci_names: result = True return result",False,"if key == ""CI_KEEP_ALIVE"" :","if key == ""ci"" :",0.8820016898747209,36.06452879987793
"def get_note_title_file(note): mo = note_title_re.match(note.get(""content"", """")) if mo: fn = mo.groups()[0] fn = fn.replace("" "", ""_"") fn = fn.replace(""/"", ""_"") if not fn: return """" <mask>: fn = unicode(fn, ""utf-8"") else: fn = unicode(fn) if note_markdown(note): fn += "".mkdn"" else: fn += "".txt"" return fn else: return """"",True,"if isinstance ( fn , str ) :","if isinstance ( fn , str ) :",0.8820016898747209,100.00000000000004
"def post(self, orgname, teamname): if _syncing_setup_allowed(orgname): try: team = model.team.get_organization_team(orgname, teamname) except model.InvalidTeamException: raise NotFound() config = request.get_json() # Ensure that the specified config points to a valid group. status, err = authentication.check_group_lookup_args(config) <mask>: raise InvalidRequest(""Could not sync to group: %s"" % err) # Set the team's syncing config. model.team.set_team_syncing(team, authentication.federated_service, config) return team_view(orgname, team) raise Unauthorized()",False,if not status :,if status != 200 :,0.8820016898747209,10.682175159905853
"def _marshalData(self): if self._cache == None: d = self._data s = """" s = time.strftime(""%H:%M:%S"", (0, 0, 0) + d + (0, 0, -1)) f = d[2] - int(d[2]) <mask>: s += (""%g"" % f)[1:] s += ""Z"" self._cache = s return self._cache",False,if f != 0 :,if f > 0 :,0.8820016898747209,24.736929544091932
"def _get_level(levels, level_ref): if level_ref in levels: return levels.index(level_ref) if isinstance(level_ref, six.integer_types): if level_ref < 0: level_ref += len(levels) <mask>: raise PatsyError(""specified level %r is out of range"" % (level_ref,)) return level_ref raise PatsyError(""specified level %r not found"" % (level_ref,))",False,if not ( 0 <= level_ref < len ( levels ) ) :,if level_ref >= len ( levels ) :,0.8820016898747209,24.645080904880025
"def iterfieldselect(source, field, where, complement, missing): it = iter(source) hdr = next(it) yield tuple(hdr) indices = asindices(hdr, field) getv = operator.itemgetter(*indices) for row in it: try: v = getv(row) except IndexError: v = missing <mask>: # XOR yield tuple(row)",False,if bool ( where ( v ) ) != complement :,if v == where :,0.8820016898747209,4.465061041725592
"def _test_wait_read_invalid_switch(self, sleep): sock1, sock2 = socket.socketpair() try: p = gevent.spawn( util.wrap_errors( AssertionError, socket.wait_read ), # pylint:disable=no-member sock1.fileno(), ) gevent.get_hub().loop.run_callback(switch_None, p) <mask>: gevent.sleep(sleep) result = p.get() assert isinstance(result, AssertionError), result assert ""Invalid switch"" in str(result), repr(str(result)) finally: sock1.close() sock2.close()",False,if sleep is not None :,if sleep :,0.8820016898747209,0.0
"def train(config, args): gan = setup_gan(config, inputs, args) test_batches = [] for i in range(args.steps): gan.step() <mask>: correct_prediction = 0 total = 0 for (x, y) in gan.inputs.testdata(): prediction = gan.generator(x) correct_prediction += ( torch.argmax(prediction, 1) == torch.argmax(y, 1) ).sum() total += y.shape[0] accuracy = (float(correct_prediction) / total) * 100 print(""accuracy: "", accuracy) return sum_metrics",False,if i % args . sample_every == 0 and i > 0 :,if i % args . steps == 0 :,0.8820016898747209,30.861946272099846
"def process_response(self, request, response, spider): if not response.body: return response for fmt, func in six.iteritems(self._formats): new_response = func(response) <mask>: logger.debug( ""Decompressed response with format: %(responsefmt)s"", {""responsefmt"": fmt}, extra={""spider"": spider}, ) return new_response return response",True,if new_response :,if new_response :,0.8820016898747209,100.00000000000004
"def detect_ssl_option(self): for option in self.ssl_options(): <mask>: for other_option in self.ssl_options(): if option != other_option: if scan_argv(self.argv, other_option) is not None: raise ConfigurationError( ""Cannot give both %s and %s"" % (option, other_option) ) return option",False,"if scan_argv ( self . argv , option ) is not None :","if option != ""default"" :",0.8820016898747209,3.0297048914466935
"def load(cls, storefile, template_store): # Did we get file or filename? if not hasattr(storefile, ""read""): storefile = open(storefile, ""rb"") # Adjust store to have translations store = cls.convertfile(storefile, template_store) for unit in store.units: <mask>: continue # HTML does this properly on loading, others need it if cls.needs_target_sync: unit.target = unit.source unit.rich_target = unit.rich_source return store",False,if unit . isheader ( ) :,if unit . source is None :,0.8820016898747209,26.269098944241588
"def _pre_get_table(self, _ctx, table_name): vsctl_table = self._get_table(table_name) schema_helper = self.schema_helper schema_helper.register_table(vsctl_table.table_name) for row_id in vsctl_table.row_ids: <mask>: schema_helper.register_table(row_id.table) if row_id.name_column: schema_helper.register_columns(row_id.table, [row_id.name_column]) if row_id.uuid_column: schema_helper.register_columns(row_id.table, [row_id.uuid_column]) return vsctl_table",True,if row_id . table :,if row_id . table :,0.8820016898747209,100.00000000000004
"def __init__(self, pin=None, pull_up=False): super(InputDevice, self).__init__(pin) try: self.pin.function = ""input"" pull = ""up"" if pull_up else ""down"" <mask>: self.pin.pull = pull except: self.close() raise self._active_state = False if pull_up else True self._inactive_state = True if pull_up else False",False,if self . pin . pull != pull :,if pull :,0.8820016898747209,0.0
"def _increment_operations_count(self, operation, executed): with self._lock: <mask>: self._executed_operations += 1 self._executed[operation.job_type] += 1 else: self._skipped[operation.job_type] += 1",True,if executed :,if executed :,0.8820016898747209,0.0
"def emit(self, type, info=None): # Overload emit() to send events to the proxy object at the other end ev = super().emit(type, info) if self._has_proxy is True and self._session.status > 0: # implicit: and self._disposed is False: <mask>: self._session.send_command(""INVOKE"", self._id, ""_emit_at_proxy"", [ev]) elif type in self.__event_types_at_proxy: self._session.send_command(""INVOKE"", self._id, ""_emit_at_proxy"", [ev])",False,if type in self . __proxy_properties__ :,if type in self . __event_types_at_proxy :,0.8820016898747209,47.587330964125215
"def validate_pull_secret(namespace): if namespace.pull_secret is None: # TODO: add aka.ms link here warning = ( ""No --pull-secret provided: cluster will not include samples or operators from "" + ""Red Hat or from certified partners."" ) logger.warning(warning) else: try: <mask>: raise Exception() except: raise InvalidArgumentValueError(""Invalid --pull-secret."")",False,"if not isinstance ( json . loads ( namespace . pull_secret ) , dict ) :",if namespace . pull_secret != namespace . pull_secret :,0.8820016898747209,22.4687979920349
"def pack(types, *args): if len(types) != len(args): raise Exception(""number of arguments does not match format string"") port = StringIO() for (type, value) in zip(types, args): if type == ""V"": write_vuint(port, value) elif type == ""v"": write_vint(port, value) <mask>: write_bvec(port, value) else: raise Exception('unknown xpack format string item ""' + type + '""') return port.getvalue()",False,"elif type == ""s"" :","elif type == ""b"" :",0.8820016898747209,59.4603557501361
"def data(self): if self._data is not None: return self._data else: <mask>: with open(self.path, ""rb"") as jsonfile: data = jsonfile.read().decode(""utf8"") data = json.loads(data) self._data = data return self._data else: return dict()",False,if os . path . exists ( self . path ) :,if self . path :,0.8820016898747209,11.141275535087015
"def interact(self): self.output.write(""\n"") while True: try: request = self.getline(""help> "") <mask>: break except (KeyboardInterrupt, EOFError): break request = strip(request) # Make sure significant trailing quotation marks of literals don't # get deleted while cleaning input if ( len(request) > 2 and request[0] == request[-1] in (""'"", '""') and request[0] not in request[1:-1] ): request = request[1:-1] if lower(request) in (""q"", ""quit""): break self.help(request)",False,if not request :,"if request == """" :",0.8820016898747209,8.643019616048525
"def api_attachment_metadata(self): resp = [] for part in self.parts: <mask>: continue k = { ""content_type"": part.block.content_type, ""size"": part.block.size, ""filename"": part.block.filename, ""id"": part.block.public_id, } content_id = part.content_id if content_id: if content_id[0] == ""<"" and content_id[-1] == "">"": content_id = content_id[1:-1] k[""content_id""] = content_id resp.append(k) return resp",False,if not part . is_attachment :,if not part . block :,0.8820016898747209,38.49815007763549
"def _notin_text(term, text, verbose=False): index = text.find(term) head = text[:index] tail = text[index + len(term) :] correct_text = head + tail diff = _diff_text(correct_text, text, verbose) newdiff = [u(""%s is contained here:"") % py.io.saferepr(term, maxsize=42)] for line in diff: if line.startswith(u(""Skipping"")): continue <mask>: continue if line.startswith(u(""+ "")): newdiff.append(u("" "") + line[2:]) else: newdiff.append(line) return newdiff",False,"if line . startswith ( u ( ""- "" ) ) :","if not line . startswith ( u ( "" "" ) ) :",0.8820016898747209,68.50836912969523
"def get_api(user, url): global API_CACHE if API_CACHE is None or API_CACHE.get(url) is None: API_CACHE_LOCK.acquire() try: <mask>: API_CACHE = {} if API_CACHE.get(url) is None: API_CACHE[url] = ImpalaDaemonApi(url) finally: API_CACHE_LOCK.release() api = API_CACHE[url] api.set_user(user) return api",True,if API_CACHE is None :,if API_CACHE is None :,0.8820016898747209,100.00000000000004
"def __str__(self, prefix="""", printElemNumber=0): res = """" if self.has_index_name_: res += prefix + (""index_name: %s\n"" % self.DebugFormatString(self.index_name_)) cnt = 0 for e in self.prefix_value_: elm = """" <mask>: elm = ""(%d)"" % cnt res += prefix + (""prefix_value%s: %s\n"" % (elm, self.DebugFormatString(e))) cnt += 1 if self.has_value_prefix_: res += prefix + ( ""value_prefix: %s\n"" % self.DebugFormatBool(self.value_prefix_) ) return res",True,if printElemNumber :,if printElemNumber :,0.8820016898747209,0.0
"def add_group(x, nl, in_group, mw): if len(x) == 0: return x if len(x) > 1 and not in_group: <mask>: return [""[[""] + x + [""]]""] mw.warn( ""Equation will multiplex and may produce inaccurate results (see manual)"" ) return [""[""] + x + [""]""]",False,"if supports_group ( x , nl ) :",if nl :,0.8820016898747209,0.0
"def unfulfilled_items(self): unfulfilled_items = 0 for order_item in self.items.all(): <mask>: aggr = order_item.deliver_item.aggregate(delivered=Sum(""quantity"")) unfulfilled_items += order_item.quantity - (aggr[""delivered""] or 0) return unfulfilled_items",False,if not order_item . canceled :,if order_item . deliver_item :,0.8820016898747209,33.03164318013809
"def _get_pattern(self, pattern_id): """"""Get pattern item by id."""""" for key in (Tag.PATTERNS1, Tag.PATTERNS2, Tag.PATTERNS3): <mask>: data = self.tagged_blocks.get_data(key) for pattern in data: if pattern.pattern_id == pattern_id: return pattern return None",True,if key in self . tagged_blocks :,if key in self . tagged_blocks :,0.8820016898747209,100.00000000000004
"def query_lister(domain, query="""", max_items=None, attr_names=None): more_results = True num_results = 0 next_token = None while more_results: rs = domain.connection.query_with_attributes( domain, query, attr_names, next_token=next_token ) for item in rs: if max_items: <mask>: raise StopIteration yield item num_results += 1 next_token = rs.next_token more_results = next_token != None",False,if num_results == max_items :,if num_results >= max_items :,0.8820016898747209,65.80370064762461
"def find_deprecated_settings(source): # pragma: no cover from celery.utils import deprecated for name, opt in flatten(NAMESPACES): <mask>: deprecated.warn( description=""The {0!r} setting"".format(name), deprecation=opt.deprecate_by, removal=opt.remove_by, alternative=""Use the {0.alt} instead"".format(opt), ) return source",False,"if ( opt . deprecate_by or opt . remove_by ) and getattr ( source , name , None ) :",if opt . deprecated :,0.8820016898747209,0.568366089080667
"def tearDown(self): """"""Shutdown the server."""""" try: <mask>: self.server.stop(2.0) if self.sl_hdlr: self.root_logger.removeHandler(self.sl_hdlr) self.sl_hdlr.close() finally: BaseTest.tearDown(self)",True,if self . server :,if self . server :,0.8820016898747209,100.00000000000004
"def broadcast_events(self, events): LOGGER.debug(""Broadcasting events: %s"", events) with self._subscribers_cv: # Copy the subscribers subscribers = {conn: sub.copy() for conn, sub in self._subscribers.items()} if subscribers: for connection_id, subscriber in subscribers.items(): <mask>: subscriber_events = [ event for event in events if subscriber.is_subscribed(event) ] event_list = EventList(events=subscriber_events) self._send(connection_id, event_list.SerializeToString())",False,if subscriber . is_listening ( ) :,if subscriber . is_connected ( ) :,0.8820016898747209,59.694917920196445
"def _get_info(self, path): info = OrderedDict() if not self._is_mac() or self._has_xcode_tools(): stdout = None try: stdout, stderr = Popen( [self._find_binary(), ""info"", os.path.realpath(path)], stdout=PIPE, stderr=PIPE, ).communicate() except OSError: pass else: if stdout: for line in stdout.splitlines(): line = u(line).split("": "", 1) <mask>: info[line[0]] = line[1] return info",False,if len ( line ) == 2 :,if len ( line ) > 1 :,0.8820016898747209,47.750342648354646
"def test_call_extern_c_fn(self): global memcmp memcmp = cffi_support.ExternCFunction( ""memcmp"", (""int memcmp ( const uint8_t * ptr1, "" ""const uint8_t * ptr2, size_t num )""), ) @udf(BooleanVal(FunctionContext, StringVal, StringVal)) def fn(context, a, b): if a.is_null != b.is_null: return False <mask>: return True if len(a) != b.len: return False if a.ptr == b.ptr: return True return memcmp(a.ptr, b.ptr, a.len) == 0",False,if a is None :,if a . len != b . len :,0.8820016898747209,9.287528999566801
"def _flatten(*args): ahs = set() if len(args) > 0: for item in args: if type(item) is ActionHandle: ahs.add(item) elif type(item) in (list, tuple, dict, set): for ah in item: <mask>: # pragma:nocover raise ActionManagerError(""Bad argument type %s"" % str(ah)) ahs.add(ah) else: # pragma:nocover raise ActionManagerError(""Bad argument type %s"" % str(item)) return ahs",False,if type ( ah ) is not ActionHandle :,if type ( ah ) is ActionHandle :,0.8820016898747209,67.5291821812656
"def startElement(self, name, attrs, connection): if name == ""Parameter"": <mask>: self[self._current_param.name] = self._current_param self._current_param = Parameter(self) return self._current_param",False,if self . _current_param :,if self . _current_param . name not in self :,0.8820016898747209,48.63383168079942
"def _find_class_in_descendants(self, search_key): for cls in self.primitive_classes: cls_key = (cls.__name__, cls.__module__) self.class_cache[cls_key] = cls <mask>: return cls",False,if cls_key == search_key :,if cls_key in search_key :,0.8820016898747209,53.417359568998464
"def doWorkForFindAll(self, v, target, partialMatch): sibling = self while sibling: c1 = partialMatch and sibling.equalsTreePartial(target) if c1: v.append(sibling) else: c2 = not partialMatch and sibling.equalsTree(target) if c2: v.append(sibling) ### regardless of match or not, check any children for matches <mask>: sibling.getFirstChild().doWorkForFindAll(v, target, partialMatch) sibling = sibling.getNextSibling()",False,if sibling . getFirstChild ( ) :,if sibling . hasChildren ( ) :,0.8820016898747209,41.11336169005196
"def forward(self, inputs: paddle.Tensor): outputs = [] blocks = self.block(inputs) route = None for i, block in enumerate(blocks): if i > 0: block = paddle.concat([route, block], axis=1) route, tip = self.yolo_blocks[i](block) block_out = self.block_outputs[i](tip) outputs.append(block_out) <mask>: route = self.route_blocks_2[i](route) route = self.upsample(route) return outputs",False,if i < 2 :,if i < len ( self . route_blocks_2 ) :,0.8820016898747209,12.011055432195764
"def _filter_paths(basename, path, is_dir, exclude): """""".gitignore style file filtering."""""" for item in exclude: # Items ending in '/' apply only to directories. <mask>: continue # Items starting with '/' apply to the whole path. # In any other cases just the basename is used. match = path if item.startswith(""/"") else basename if fnmatch.fnmatch(match, item.strip(""/"")): return True return False",False,"if item . endswith ( ""/"" ) and not is_dir :","if is_dir and item . startswith ( ""/"" ) :",0.8820016898747209,38.84637341996162
"def reposition_division(f1): lines = f1.splitlines() if lines[2] == division: lines.pop(2) found = 0 for i, line in enumerate(lines): if line.startswith('""""""'): found += 1 if found == 2: <mask>: break # already in the right place lines.insert(i + 1, """") lines.insert(i + 2, division) break return ""\n"".join(lines)",False,"if division in ""\n"" . join ( lines ) :",if lines [ i + 1 ] == division :,0.8820016898747209,4.41902110634
"def buildImage(opt): dpath = os.path.join(opt[""datapath""], ""COCO-IMG-2015"") version = ""1"" if not build_data.built(dpath, version_string=version): print(""[building image data: "" + dpath + ""]"") <mask>: # An older version exists, so remove these outdated files. build_data.remove_dir(dpath) build_data.make_dir(dpath) # Download the data. for downloadable_file in RESOURCES[:1]: downloadable_file.download_file(dpath) # Mark the data as built. build_data.mark_done(dpath, version_string=version)",True,if build_data . built ( dpath ) :,if build_data . built ( dpath ) :,0.8820016898747209,100.00000000000004
"def colorformat(text): if text[0:1] == ""#"": col = text[1:] <mask>: return col elif len(col) == 3: return col[0] * 2 + col[1] * 2 + col[2] * 2 elif text == """": return """" assert False, ""wrong color format %r"" % text",False,if len ( col ) == 6 :,if len ( col ) == 2 :,0.8820016898747209,75.06238537503395
"def tree_print(tree): for key in tree: print(key, end="" "") # end=' ' prevents a newline character tree_element = tree[key] # multiple lookups is expensive, even amortized O(1)! for subElem in tree_element: print("" -> "", subElem, end="" "") <mask>: # OP wants indenting after digits print(""\n "") # newline and a space to match indenting print() # forces a newline",False,if type ( subElem ) != str :,if subElem . is_digits ( ) :,0.8820016898747209,7.129384882260374
"def is_dse_cluster(path): try: with open(os.path.join(path, ""CURRENT""), ""r"") as f: name = f.readline().strip() cluster_path = os.path.join(path, name) filename = os.path.join(cluster_path, ""cluster.conf"") with open(filename, ""r"") as f: data = yaml.load(f) <mask>: return True except IOError: return False",False,"if ""dse_dir"" in data :","if data [ ""name"" ] == ""dse"" :",0.8820016898747209,8.130850857597444
"def delete_old_target_output_files(classpath_prefix): """"""Delete existing output files or symlinks for target."""""" directory, basename = os.path.split(classpath_prefix) pattern = re.compile( r""^{basename}(([0-9]+)(\.jar)?|classpath\.txt)$"".format( basename=re.escape(basename) ) ) files = [filename for filename in os.listdir(directory) if pattern.match(filename)] for rel_path in files: path = os.path.join(directory, rel_path) <mask>: safe_delete(path)",False,if os . path . islink ( path ) or os . path . isfile ( path ) :,if os . path . exists ( path ) :,0.8820016898747209,26.753788181976983
"def test_files(self): # get names of files to test dist_dir = os.path.join(os.path.dirname(__file__), os.pardir, os.pardir) names = [] for d in self.test_directories: test_dir = os.path.join(dist_dir, d) for n in os.listdir(test_dir): if n.endswith("".py"") and not n.startswith(""bad""): names.append(os.path.join(test_dir, n)) for filename in names: <mask>: print(""Testing %s"" % filename) source = read_pyfile(filename) self.check_roundtrip(source)",False,if test_support . verbose :,"if filename . endswith ( "".py"" ) :",0.8820016898747209,4.9323515694897075
"def __str__(self): if self.HasError(): return self.ErrorAsStr() else: # Format is: {action} ""{target}"" ({filename}:{lineno}) string = self._action if self._target is not None: string += ' ""{target}""'.format(target=self._target) <mask>: path = self._filename if self._lineno is not None: path += "":{lineno}"".format(lineno=self._lineno) string += "" ({path})"".format(path=path) return string",True,if self . _filename is not None :,if self . _filename is not None :,0.8820016898747209,100.00000000000004
"def extra_action_out(self, input_dict, state_batches, model, action_dist): with self._no_grad_context(): <mask>: stats_dict = extra_action_out_fn( self, input_dict, state_batches, model, action_dist ) else: stats_dict = parent_cls.extra_action_out( self, input_dict, state_batches, model, action_dist ) return self._convert_to_non_torch_type(stats_dict)",True,if extra_action_out_fn :,if extra_action_out_fn :,0.8820016898747209,100.00000000000004
"def _retract_bindings(fstruct, inv_bindings, fs_class, visited): # Visit each node only once: if id(fstruct) in visited: return visited.add(id(fstruct)) if _is_mapping(fstruct): items = fstruct.items() elif _is_sequence(fstruct): items = enumerate(fstruct) else: raise ValueError(""Expected mapping or sequence"") for (fname, fval) in items: if isinstance(fval, fs_class): <mask>: fstruct[fname] = inv_bindings[id(fval)] _retract_bindings(fval, inv_bindings, fs_class, visited)",True,if id ( fval ) in inv_bindings :,if id ( fval ) in inv_bindings :,0.8820016898747209,100.00000000000004
"def warehouses(self) -> tuple: from ..repositories import WarehouseBaseRepo repos = dict() for dep in chain(self.dependencies, [self]): <mask>: continue if not isinstance(dep.repo, WarehouseBaseRepo): continue for repo in dep.repo.repos: if repo.from_config: continue repos[repo.name] = repo return tuple(repos.values())",False,if dep . repo is None :,if dep . name in repos :,0.8820016898747209,26.269098944241588
"def detype(self): if self._detyped is not None: return self._detyped ctx = {} for key, val in self._d.items(): if not isinstance(key, str): key = str(key) detyper = self.get_detyper(key) <mask>: # cannot be detyped continue deval = detyper(val) if deval is None: # cannot be detyped continue ctx[key] = deval self._detyped = ctx return ctx",True,if detyper is None :,if detyper is None :,0.8820016898747209,100.00000000000004
"def populate_obj(self, obj, name): field = getattr(obj, name, None) if field is not None: # If field should be deleted, clean it up <mask>: field.delete() return if isinstance(self.data, FileStorage) and not is_empty(self.data.stream): if not field.grid_id: func = field.put else: func = field.replace func( self.data.stream, filename=self.data.filename, content_type=self.data.content_type, )",False,if self . _should_delete :,if field . is_deleted ( ) :,0.8820016898747209,6.742555929751843
"def _load(container): if isinstance(container, str): # If container is a filename. <mask>: with open(container, ""rb"") as f: return pickle.load(f) # If container is a pickle string. else: return pickle.loads(container) # If container is an open file elif isinstance(container, IOBase): return pickle.load(container) # What else could it be? else: l.error(""Cannot unpickle container of type %s"", type(container)) return None",False,if all ( c in string . printable for c in container ) and os . path . exists ( container ) :,if os . path . isfile ( container ) :,0.8820016898747209,14.746212096418224
"def append_row(self, row): self.allocate_future_payments(row) self.set_invoice_details(row) self.set_party_details(row) self.set_ageing(row) if self.filters.get(""group_by_party""): self.update_sub_total_row(row, row.party) <mask>: self.append_subtotal_row(self.previous_party) self.previous_party = row.party self.data.append(row)",False,if self . previous_party and ( self . previous_party != row . party ) :,if self . previous_party :,0.8820016898747209,13.127910466261701
"def gg1(): while 1: tt = 3 while tt > 0: trace.append(tt) val = yield <mask>: tt = 10 # <= uncomment this line trace.append(""breaking early..."") break tt -= 1 trace.append(""try!"")",False,if val is not None :,"if val == ""0"" :",0.8820016898747209,12.22307556087252
"def migrate_common_facts(facts): """"""Migrate facts from various roles into common"""""" params = {""node"": (""portal_net""), ""master"": (""portal_net"")} if ""common"" not in facts: facts[""common""] = {} # pylint: disable=consider-iterating-dictionary for role in params.keys(): if role in facts: for param in params[role]: <mask>: facts[""common""][param] = facts[role].pop(param) return facts",True,if param in facts [ role ] :,if param in facts [ role ] :,0.8820016898747209,100.00000000000004
"def get_measurements(self, pipeline, object_name, category): if self.get_categories(pipeline, object_name) == [category]: results = [] <mask>: if object_name == ""Image"": results += [""Correlation"", ""Slope""] else: results += [""Correlation""] if self.do_overlap: results += [""Overlap"", ""K""] if self.do_manders: results += [""Manders""] if self.do_rwc: results += [""RWC""] if self.do_costes: results += [""Costes""] return results return []",False,if self . do_corr_and_slope :,if self . do_correlation :,0.8820016898747209,36.337289265247364
"def access_modes(self): """"""access_modes property"""""" if self._access_modes is None: self._access_modes = self.get_access_modes() <mask>: self._access_modes = list(self._access_modes) return self._access_modes",False,"if not isinstance ( self . _access_modes , list ) :",if self . _access_modes :,0.8820016898747209,34.53521209125189
"def unwrap_envelope(self, data, many): if many: <mask>: if isinstance(data, InstrumentedList) or isinstance(data, list): self.context[""total""] = len(data) return data else: self.context[""total""] = data[""total""] else: self.context[""total""] = 0 data = {""items"": []} return data[""items""] return data",False,"if data [ ""items"" ] :","if data [ ""total"" ] == 0 :",0.8820016898747209,28.997844147152072
"def to_string(self, fmt=""{:.4f}""): result_str = """" for key in self.measures: result = self.m_dict[key][0]() result_str += ( "","".join(fmt.format(x) for x in result) <mask>: else fmt.format(result) ) result_str += "","" return result_str[:-1] # trim the last comma",False,"if isinstance ( result , tuple )",if len ( result ) > 1,0.8820016898747209,15.619699684601283
"def on_torrent_created(self, result): if not result: return self.dialog_widget.btn_create.setEnabled(True) self.dialog_widget.edit_channel_create_torrent_progress_label.setText( ""Created torrent"" ) if ""torrent"" in result: self.create_torrent_notification.emit({""msg"": ""Torrent successfully created""}) <mask>: self.add_torrent_to_channel(result[""torrent""]) self.close_dialog()",False,if self . dialog_widget . add_to_channel_checkbox . isChecked ( ) :,"if ""torrent"" in result :",0.8820016898747209,1.4064939156282428
"def save(self): for var_name in self.default_config: <mask>: if var_name in self.file_config: del self.file_config[var_name] else: self.file_config[var_name] = getattr(self, var_name) with open(self.config_path, ""w"") as f: f.write(json.dumps(self.file_config, indent=2))",False,"if getattr ( self , var_name , None ) == self . default_config [ var_name ] :",if var_name not in self . file_config :,0.8820016898747209,7.258042956179844
"def get_class_parameters(kwarg): ret = {""attrs"": []} for key in (""rsc"", ""fsc"", ""usc""): <mask>: ret[""attrs""].append( [ ""TCA_HFSC_%s"" % key.upper(), { ""m1"": get_rate(kwarg[key].get(""m1"", 0)), ""d"": get_time(kwarg[key].get(""d"", 0)), ""m2"": get_rate(kwarg[key].get(""m2"", 0)), }, ] ) return ret",True,if key in kwarg :,if key in kwarg :,0.8820016898747209,100.00000000000004
"def forward(self, x): f_x = x if self.exp: f_x = self.exp_swish(self.exp_bn(self.exp(f_x))) f_x = self.dwise_swish(self.dwise_bn(self.dwise(f_x))) f_x = self.se(f_x) f_x = self.lin_proj_bn(self.lin_proj(f_x)) if self.has_skip: <mask>: f_x = drop_connect(f_x, effnet_cfg.EN.DC_RATIO) f_x = x + f_x return f_x",False,if self . training and effnet_cfg . EN . DC_RATIO > 0.0 :,if self . drop_connect :,0.8820016898747209,6.656592803413299
"def cli_uninstall_distro(): distro_list = install_distro_list() if distro_list is not None: for index, _distro_dir in enumerate(distro_list): log(str(index) + "" --->> "" + _distro_dir) user_input = read_input_uninstall() if user_input is not False: for index, _distro_dir in enumerate(distro_list): <mask>: config.uninstall_distro_dir_name = _distro_dir unin_distro() else: log(""No distro installed on "" + config.usb_disk)",False,if index == user_input :,if _distro_dir != user_input :,0.8820016898747209,36.72056269893591
"def IMPORTFROM(self, node): if node.module == ""__future__"": <mask>: self.report(messages.LateFutureImport, node, [n.name for n in node.names]) else: self.futuresAllowed = False for alias in node.names: if alias.name == ""*"": self.scope.importStarred = True self.report(messages.ImportStarUsed, node, node.module) continue name = alias.asname or alias.name importation = Importation(name, node) if node.module == ""__future__"": importation.used = (self.scope, node) self.addBinding(node, importation)",False,if not self . futuresAllowed :,if self . futuresAllowed :,0.8820016898747209,57.89300674674101
"def _split_and_load(batch, ctx_list): """"""Split data to 1 batch each device."""""" new_batch = [] for _, data in enumerate(batch): <mask>: new_data = [x.as_in_context(ctx) for x, ctx in zip(data, ctx_list)] else: new_data = [data.as_in_context(ctx_list[0])] new_batch.append(new_data) return new_batch",True,"if isinstance ( data , ( list , tuple ) ) :","if isinstance ( data , ( list , tuple ) ) :",0.8820016898747209,100.00000000000004
"def wait_success(self, timeout=60 * 10): for i in range(timeout // 10): time.sleep(10) status = self.query_job() print(""job {} status is {}"".format(self.job_id, status)) <mask>: return True if status and status in [ StatusSet.CANCELED, StatusSet.TIMEOUT, StatusSet.FAILED, ]: return False return False",False,if status and status == StatusSet . SUCCESS :,if status :,0.8820016898747209,0.0
"def copy_tree(self, src_dir, dst_dir, skip_variables=False): for src_root, _, files in os.walk(src_dir): <mask>: rel_root = os.path.relpath(src_root, src_dir) else: rel_root = """" if skip_variables and rel_root.startswith(""variables""): continue dst_root = os.path.join(dst_dir, rel_root) if not os.path.exists(dst_root): os.makedirs(dst_root) for f in files: shutil.copy(os.path.join(src_root, f), os.path.join(dst_root, f))",False,if src_root != src_dir :,if os . path . isdir ( src_root ) :,0.8820016898747209,14.323145079400492
"def _make_padded_shapes(self, dataset, decoders): padded_shapes = dataset.output_shapes for i, hparams_i in enumerate(self._hparams.datasets): <mask>: continue if not hparams_i[""pad_to_max_seq_length""]: continue text_and_id_shapes = MonoTextData._make_padded_text_and_id_shapes( dataset, hparams_i, decoders[i], self.text_name(i), self.text_id_name(i) ) padded_shapes.update(text_and_id_shapes) return padded_shapes",False,"if not _is_text_data ( hparams_i [ ""data_type"" ] ) :","if not hparams_i [ ""pad_to_min_seq_length"" ] :",0.8820016898747209,24.419888293138573
"def format_errors(messages): errors = {} for k, v in messages.items(): key = camelize(k, uppercase_first_letter=False) <mask>: errors[key] = format_errors(v) elif isinstance(v, list): errors[key] = v[0] return errors",True,"if isinstance ( v , dict ) :","if isinstance ( v , dict ) :",0.8820016898747209,100.00000000000004
"def generic_visit(self, node, parents=None): parents = (parents or []) + [node] for field, value in iter_fields(node): <mask>: for item in value: if isinstance(item, AST): self.visit(item, parents) elif isinstance(value, AST): self.visit(value, parents)",False,"if isinstance ( value , list ) :","if isinstance ( field , list ) :",0.8820016898747209,50.000000000000014
"def get_override_css(self): """"""handls allow_css_overrides setting."""""" if self.settings.get(""allow_css_overrides""): filename = self.view.file_name() filetypes = self.settings.get(""markdown_filetypes"") if filename and filetypes: for filetype in filetypes: <mask>: css_filename = filename.rpartition(filetype)[0] + "".css"" if os.path.isfile(css_filename): return u""<style>%s</style>"" % load_utf8(css_filename) return """"",False,if filename . endswith ( filetype ) :,"if filetype . startswith ( ""css"" ) :",0.8820016898747209,11.044795567078939
"def clean(self): super().clean() # If the Cluster is assigned to a Site, all Devices must be assigned to that Site. if self.cluster.site is not None: for device in self.cleaned_data.get(""devices"", []): <mask>: raise ValidationError( { ""devices"": ""{} belongs to a different site ({}) than the cluster ({})"".format( device, device.site, self.cluster.site ) } )",True,if device . site != self . cluster . site :,if device . site != self . cluster . site :,0.8820016898747209,100.00000000000004
"def _setProcessPriority(process, nice_val, disable_gc): org_nice_val = Computer._process_original_nice_value try: process.nice(nice_val) Computer.in_high_priority_mode = nice_val != org_nice_val <mask>: gc.disable() else: gc.enable() return True except psutil.AccessDenied: print2err( ""WARNING: Could not set process {} priority "" ""to {}"".format(process.pid, nice_val) ) return False",True,if disable_gc :,if disable_gc :,0.8820016898747209,100.00000000000004
"def _setResultsName(self, name, listAllMatches=False): if __diag__.warn_multiple_tokens_in_named_alternation: <mask>: warnings.warn( ""{}: setting results name {!r} on {} expression "" ""may only return a single token for an And alternative, "" ""in future will return the full list of tokens"".format( ""warn_multiple_tokens_in_named_alternation"", name, type(self).__name__, ), stacklevel=3, ) return super()._setResultsName(name, listAllMatches)",False,"if any ( isinstance ( e , And ) for e in self . exprs ) :","if name in [ ""And"" , ""And"" ] :",0.8820016898747209,3.395776631557144
"def make_sources(project: RootDependency) -> str: content = [] if project.readme: content.append(project.readme.path.name) if project.readme.markup != ""rst"": content.append(project.readme.to_rst().path.name) path = project.package.path for fname in (""setup.cfg"", ""setup.py""): <mask>: content.append(fname) for package in chain(project.package.packages, project.package.data): for fpath in package: fpath = fpath.relative_to(project.package.path) content.append(""/"".join(fpath.parts)) return ""\n"".join(content)",False,if ( path / fname ) . exists ( ) :,if fname . startswith ( path ) :,0.8820016898747209,12.347293198886947
"def findControlPointsInMesh(glyph, va, subsegments): controlPointIndices = np.zeros((len(va), 1)) index = 0 for i, c in enumerate(subsegments): segmentCount = len(glyph.contours[i].segments) - 1 for j, s in enumerate(c): if j < segmentCount: <mask>: controlPointIndices[index] = 1 index += s[1] return controlPointIndices",False,"if glyph . contours [ i ] . segments [ j ] . type == ""line"" :",if s [ 0 ] == va [ j ] :,0.8820016898747209,9.426482755071667
"def MergeFrom(self, other): if self.message_class is not None: if other.Parse(self.message_class): self.message.MergeFrom(other.message) elif other.message_class is not None: <mask>: self.message = other.message_class() self.message_class = other.message_class self.message.MergeFrom(other.message) else: self.message += other.message",False,if not self . Parse ( other . message_class ) :,"if not isinstance ( self . message , Message ) :",0.8820016898747209,12.498879161997976
"def remove_old_snapshot(install_dir): logging.info(""Removing any old files in {}"".format(install_dir)) for file in glob.glob(""{}/*"".format(install_dir)): try: <mask>: os.unlink(file) elif os.path.isdir(file): shutil.rmtree(file) except Exception as error: logging.error(""Error: {}"".format(error)) sys.exit(1)",True,if os . path . isfile ( file ) :,if os . path . isfile ( file ) :,0.8820016898747209,100.00000000000004
"def writexml( self, stream, indent="""", addindent="""", newl="""", strip=0, nsprefixes={}, namespace="""", ): w = _streamWriteWrapper(stream) if self.raw: val = self.nodeValue if not isinstance(val, str): val = str(self.nodeValue) else: v = self.nodeValue if not isinstance(v, str): v = str(v) <mask>: v = "" "".join(v.split()) val = escape(v) w(val)",False,if strip :,if len ( v ) > 1 :,0.8820016898747209,6.567274736060395
"def validate_attributes(self): for attribute in self.get_all_attributes(): value = getattr(self, attribute.code, None) <mask>: if attribute.required: raise ValidationError( _(""%(attr)s attribute cannot be blank"") % {""attr"": attribute.code} ) else: try: attribute.validate_value(value) except ValidationError as e: raise ValidationError( _(""%(attr)s attribute %(err)s"") % {""attr"": attribute.code, ""err"": e} )",True,if value is None :,if value is None :,0.8820016898747209,100.00000000000004
"def PyJsHoisted_BinaryExpression_(node, parent, this, arguments, var=var): var = Scope( {u""node"": node, u""this"": this, u""arguments"": arguments, u""parent"": parent}, var ) var.registers([u""node"", u""parent""]) if PyJsStrictEq(var.get(u""node"").get(u""operator""), Js(u""in"")): <mask>: return var.get(u""true"") if var.get(u""t"").callprop(u""isFor"", var.get(u""parent"")): return var.get(u""true"") return Js(False)",False,"if var . get ( u""t"" ) . callprop ( u""isVariableDeclarator"" , var . get ( u""parent"" ) ) :","if var . get ( u""t"" ) . callprop ( u""isFor"" , var . get ( u""parent"" ) ) :",0.8820016898747209,90.61874434879648
"def distinct(expr, *on): fields = frozenset(expr.fields) _on = [] append = _on.append for n in on: if isinstance(n, Field): if n._child.isidentical(expr): n = n._name else: raise ValueError(""{0} is not a field of {1}"".format(n, expr)) <mask>: raise TypeError(""on must be a name or field, not: {0}"".format(n)) elif n not in fields: raise ValueError(""{0} is not a field of {1}"".format(n, expr)) append(n) return Distinct(expr, tuple(_on))",False,"if not isinstance ( n , _strtypes ) :","elif not isinstance ( n , Field ) :",0.8820016898747209,45.93613320783059
"def encode(self, msg): """"""Encodes the message to the stream encoding."""""" stream = self.stream rv = msg + ""\n"" if (PY2 and is_unicode(rv)) or not ( PY2 or is_unicode(rv) or _is_text_stream(stream) ): enc = self.encoding <mask>: enc = getattr(stream, ""encoding"", None) or ""utf-8"" rv = rv.encode(enc, ""replace"") return rv",False,if enc is None :,if not enc :,0.8820016898747209,16.37226966703825
"def color_convert(self, to_color_space, preserve_alpha=True): if to_color_space == self.color_space and preserve_alpha: return self else: pixels = pixels_as_float(self.pixels) converted = convert_color( pixels, self.color_space, to_color_space, preserve_alpha ) <mask>: return None return Image(converted, to_color_space)",True,if converted is None :,if converted is None :,0.8820016898747209,100.00000000000004
"def seek(self, pos): if self.closed: raise IOError(""Cannot seek on a closed file"") for n, idx in enumerate(self._indexes[::-1]): if idx.offset <= pos: <mask>: self._idxiter = iter(self._indexes[-(n + 1) :]) self._nextidx() break else: raise Exception(""Cannot seek to pos"") self._curfile.seek(pos - self._curidx.offset)",False,if idx != self . _curidx :,if n < len ( self . _indexes ) :,0.8820016898747209,15.851165692617148
"def load_from_json(self, node_data: dict, import_version: float): if import_version <= 0.08: self.image_pointer = unpack_pointer_property_name( bpy.data.images, node_data, ""image_name"" ) <mask>: proposed_name = node_data.get(""image_name"") self.info(f""image data not found in current {proposed_name}"")",False,if not self . image_pointer :,if self . image_pointer is None :,0.8820016898747209,48.54917717073236
"def __init__(self, execution_context, aggregate_operators): super(_QueryExecutionAggregateEndpointComponent, self).__init__(execution_context) self._local_aggregators = [] self._results = None self._result_index = 0 for operator in aggregate_operators: if operator == ""Average"": self._local_aggregators.append(_AverageAggregator()) <mask>: self._local_aggregators.append(_CountAggregator()) elif operator == ""Max"": self._local_aggregators.append(_MaxAggregator()) elif operator == ""Min"": self._local_aggregators.append(_MinAggregator()) elif operator == ""Sum"": self._local_aggregators.append(_SumAggregator())",True,"elif operator == ""Count"" :","elif operator == ""Count"" :",0.8820016898747209,100.00000000000004
"def attrgetter(item): items = [None] * len(attribute) for i, attribute_part in enumerate(attribute): item_i = item for part in attribute_part: item_i = environment.getitem(item_i, part) <mask>: item_i = postprocess(item_i) items[i] = item_i return items",False,if postprocess is not None :,if postprocess :,0.8820016898747209,0.0
"def work(self): while True: timeout = self.timeout <mask>: timeout = self.idle_timeout log.debug(""Wait for {}"".format(timeout)) fetch.wait(timeout) if shutting_down.is_set(): log.info(""Stop fetch worker"") break self.fetch()",False,if idle . is_set ( ) :,if timeout is None :,0.8820016898747209,6.316906128202129
"def testCoreInterfaceIntInputData(): result_testing = False for _ in range(10): hsyncnet_instance = hsyncnet( [[1], [2], [3], [20], [21], [22]], 2, initial_type.EQUIPARTITION, ccore=True ) analyser = hsyncnet_instance.process() <mask>: result_testing = True break assert result_testing",False,if len ( analyser . allocate_clusters ( 0.1 ) ) == 2 :,if analyser . get_input_data ( ) :,0.8820016898747209,6.507499376567245
"def _gen(): buf = [] iterable = dataset() try: while len(buf) < buffer_size: buf.append(next(iterable)) while 1: i = random.randint(0, buffer_size - 1) n = next(iterable) yield buf[i] buf[i] = n except StopIteration: <mask>: random.shuffle(buf) for i in buf: yield i",False,if len ( buf ) :,if random . random ( ) < buffer_size :,0.8820016898747209,5.300156689756295
"def debug_tree(tree): l = [] for elt in tree: if isinstance(elt, (int, long)): l.append(_names.get(elt, elt)) <mask>: l.append(elt) else: l.append(debug_tree(elt)) return l",True,"elif isinstance ( elt , str ) :","elif isinstance ( elt , str ) :",0.8820016898747209,100.00000000000004
"def reverse_code(apps: StateApps, schema_editor: DatabaseSchemaEditor) -> None: PreregistrationUser = apps.get_model(""zerver"", ""PreregistrationUser"") for user in PreregistrationUser.objects.all(): <mask>: # PreregistrationUser.INVITE_AS['REALM_ADMIN'] user.invited_as_admin = True else: # PreregistrationUser.INVITE_AS['MEMBER'] user.invited_as_admin = False user.save(update_fields=[""invited_as_admin""])",False,if user . invited_as == 2 :,if user . invited_as_admin :,0.8820016898747209,54.627576446464936
"def _fastqc_data_section(self, section_name): out = [] in_section = False data_file = os.path.join(self._dir, ""fastqc_data.txt"") if os.path.exists(data_file): with open(data_file) as in_handle: for line in in_handle: if line.startswith("">>%s"" % section_name): in_section = True elif in_section: <mask>: break out.append(line.rstrip(""\r\n"")) return out",False,"if line . startswith ( "">>END"" ) :","if not line . startswith ( ""\r\n"" ) :",0.8820016898747209,36.6192636299943
"def determine_block_hints(self, text): hints = """" if text: if text[0] in "" \n\x85\u2028\u2029"": hints += str(self.best_indent) <mask>: hints += ""-"" elif len(text) == 1 or text[-2] in ""\n\x85\u2028\u2029"": hints += ""+"" return hints",False,"if text [ - 1 ] not in ""\n\x85\u2028\u2029"" :","elif text [ 0 ] in ""\n\x85\u2028\u2029"" :",0.8820016898747209,64.87662298067647
"def database_app(request): if request.param == ""postgres_app"": if not which(""initdb""): pytest.skip(""initdb must be on PATH for postgresql fixture"") if not psycopg2: pytest.skip(""psycopg2 must be installed for postgresql fixture"") if request.param == ""sqlite_rabbitmq_app"": <mask>: pytest.skip( ""rabbitmq tests will be skipped if GALAXY_TEST_AMQP_INTERNAL_CONNECTION env var is unset"" ) return request.getfixturevalue(request.param)",False,"if not os . environ . get ( ""GALAXY_TEST_AMQP_INTERNAL_CONNECTION"" ) :","if not getenv ( ""GALAXY_TEST_AMQP_INTERNAL_CONNECTION"" ) :",0.8820016898747209,67.03806971317177
"def do_rollout(agent, env, num_steps, render=False): total_rew = 0 ob = env.reset() for t in range(num_steps): a = agent.act(ob) (ob, reward, done, _info) = env.step(a) total_rew += reward <mask>: env.render() if done: break return total_rew, t + 1",False,if render and t % 3 == 0 :,if render :,0.8820016898747209,0.0
"def _handle_subrepos(self, ctx, dirty_trees): substate = util.parse_hgsubstate(ctx["".hgsubstate""].data().splitlines()) sub = util.OrderedDict() if "".hgsub"" in ctx: sub = util.parse_hgsub(ctx["".hgsub""].data().splitlines()) for path, sha in substate.iteritems(): # Ignore non-Git repositories keeping state in .hgsubstate. <mask>: continue d = os.path.dirname(path) dirty_trees.add(d) tree = self._dirs.setdefault(d, dulobjs.Tree()) tree.add(os.path.basename(path), dulobjs.S_IFGITLINK, sha)",False,"if path in sub and not sub [ path ] . startswith ( ""[git]"" ) :",if not os . path . isdir ( path ) :,0.8820016898747209,4.677209851690883
"def get_property_file_image_choices(self, pipeline): columns = pipeline.get_measurement_columns() image_names = [] for column in columns: object_name, feature, coltype = column[:3] choice = feature[(len(C_FILE_NAME) + 1) :] <mask>: image_names.append(choice) return image_names",False,"if object_name == ""Image"" and ( feature . startswith ( C_FILE_NAME ) ) :",if choice . startswith ( C_FILE_NAME ) :,0.8820016898747209,30.754858697597314
"def check_all_decorator_order(): """"""Check that in all test files, the slow decorator is always last."""""" errors = [] for fname in os.listdir(PATH_TO_TESTS): <mask>: filename = os.path.join(PATH_TO_TESTS, fname) new_errors = check_decorator_order(filename) errors += [f""- {filename}, line {i}"" for i in new_errors] if len(errors) > 0: msg = ""\n"".join(errors) raise ValueError( f""The parameterized decorator (and its variants) should always be first, but this is not the case in the following files:\n{msg}"" )",True,"if fname . endswith ( "".py"" ) :","if fname . endswith ( "".py"" ) :",0.8820016898747209,100.00000000000004
"def on_edit_button_clicked(self, event=None, a=None, col=None): tree, tree_id = self.treeView.get_selection().get_selected() watchdir_id = str(self.store.get_value(tree_id, 0)) if watchdir_id: if col and col.get_title() == _(""Active""): <mask>: client.autoadd.disable_watchdir(watchdir_id) else: client.autoadd.enable_watchdir(watchdir_id) else: self.opts_dialog.show(self.watchdirs[watchdir_id], watchdir_id)",False,"if self . watchdirs [ watchdir_id ] [ ""enabled"" ] :","if self . store . get_value ( tree_id , 0 ) == ""Disabled"" :",0.8820016898747209,9.560408787521254
"def get_conv_output_size(input_size, kernel_size, stride, padding, dilation): ndim = len(input_size) output_size = [] for i in range(ndim): size = ( input_size[i] + 2 * padding[i] - dilation[i] * (kernel_size[i] - 1) - 1 ) // stride[i] + 1 <mask>: output_size.append(1) else: output_size.append(size) return output_size",False,if kernel_size [ i ] == - 1 :,if size == 0 :,0.8820016898747209,7.509307647752128
"def from_location(cls, location, basename, metadata=None, **kw): project_name, version, py_version, platform = [None] * 4 basename, ext = os.path.splitext(basename) if ext.lower() in ("".egg"", "".egg-info""): match = EGG_NAME(basename) <mask>: project_name, version, py_version, platform = match.group( ""name"", ""ver"", ""pyver"", ""plat"" ) return cls( location, metadata, project_name=project_name, version=version, py_version=py_version, platform=platform, **kw )",True,if match :,if match :,0.8820016898747209,0.0
"def __new__(metacls, typename, bases, namespace): annotations = namespace.get(""__annotations__"", {}) for t in annotations.values(): <mask>: for ut in t.__args__: _assert_tensorizer_type(ut) else: _assert_tensorizer_type(t) return super().__new__(metacls, typename, bases, namespace)",False,"if getattr ( t , ""__origin__"" , """" ) is Union :","if isinstance ( t , ( tuple , list ) ) :",0.8820016898747209,8.694246557709919
"def decode_content(self): """"""Return the best possible representation of the response body."""""" ct = self.headers.get(""content-type"") if ct: ct, options = parse_options_header(ct) charset = options.get(""charset"") <mask>: return self.json(charset) elif ct.startswith(""text/""): return self.text(charset) elif ct == FORM_URL_ENCODED: return parse_qsl(self.content.decode(charset), keep_blank_values=True) return self.content",False,if ct in JSON_CONTENT_TYPES :,"if ct . startswith ( ""application/json"" ) :",0.8820016898747209,7.495553473355845
"def get_full_path(path): if ""://"" not in path: path = os.path.join(self.AUTO_COLL_TEMPL, path, """") <mask>: path = os.path.join(abs_path, path) return path",False,if abs_path :,if os . path . exists ( abs_path ) :,0.8820016898747209,14.323145079400492
"def __getitem__(self, name_or_path): if isinstance(name_or_path, integer_types): return list.__getitem__(self, name_or_path) elif isinstance(name_or_path, tuple): try: val = self for fid in name_or_path: <mask>: raise KeyError # path contains base value val = val[fid] return val except (KeyError, IndexError): raise KeyError(name_or_path) else: raise TypeError(self._INDEX_ERROR % name_or_path)",False,"if not isinstance ( val , FeatStruct ) :",if fid not in val :,0.8820016898747209,6.962210312500384
"def scan(scope): for s in scope.children: if s.start_pos <= position <= s.end_pos: if isinstance(s, (tree.Scope, tree.Flow)): return scan(s) or s <mask>: return scan(s) return None",False,"elif s . type in ( ""suite"" , ""decorated"" ) :","elif isinstance ( s , tree . Tree ) :",0.8820016898747209,6.962210312500384
"def _get_key(self): if not self.key: self._channel.send(u""pake"", self.msg1) pake_msg = self._channel.get(u""pake"") self.key = self.sp.finish(pake_msg) self.verifier = self.derive_key(u""wormhole:verifier"") <mask>: return confkey = self.derive_key(u""wormhole:confirmation"") nonce = os.urandom(CONFMSG_NONCE_LENGTH) confmsg = make_confmsg(confkey, nonce) self._channel.send(u""_confirm"", confmsg)",False,if not self . _send_confirm :,if not self . verifier :,0.8820016898747209,32.58798048281462
"def executeScript(self, script): if len(script) > 0: commands = [] for l in script: extracted = self.extract_command(l) <mask>: commands.append(extracted) for command in commands: cmd, argv = command self.dispatch_command(cmd, argv)",False,if extracted :,if extracted is not None :,0.8820016898747209,17.965205598154213
"def create_path(n, fullname, meta): if meta: meta.create_path(fullname) else: # These fallbacks are important -- meta could be null if, for # example, save created a ""fake"" item, i.e. a new strip/graft # path element, etc. You can find cases like that by # searching for ""Metadata()"". unlink(fullname) if stat.S_ISDIR(n.mode): mkdirp(fullname) <mask>: os.symlink(n.readlink(), fullname)",False,elif stat . S_ISLNK ( n . mode ) :,if stat . S_ISLINK ( n . mode ) :,0.8820016898747209,63.40466277046863
def get_cycle(self): if self.has_cycle(): cross_node = self.path[-1] <mask>: return self.path[self.path.index(cross_node) :] else: return self.path return [],False,if self . path . count ( cross_node ) > 1 :,if cross_node in self . path :,0.8820016898747209,18.402097851927994
"def _select_block(str_in, start_tag, end_tag): """"""Select first block delimited by start_tag and end_tag"""""" start_pos = str_in.find(start_tag) if start_pos < 0: raise ValueError(""start_tag not found"") depth = 0 for pos in range(start_pos, len(str_in)): <mask>: depth += 1 elif str_in[pos] == end_tag: depth -= 1 if depth == 0: break sel = str_in[start_pos + 1 : pos] return sel",True,if str_in [ pos ] == start_tag :,if str_in [ pos ] == start_tag :,0.8820016898747209,100.00000000000004
"def device(self): """"""Device on which the data array of this variable reside."""""" # lazy initialization for performance if self._device is None: <mask>: self._device = backend.CpuDevice() else: self._device = backend.get_device_from_array(self._data[0]) return self._device",False,if self . _data [ 0 ] is None :,if self . _data is None :,0.8820016898747209,47.52203774792177
"def function_out(*args, **kwargs): try: return function_in(*args, **kwargs) except dbus.exceptions.DBusException as e: if e.get_dbus_name() == DBUS_UNKNOWN_METHOD: raise ItemNotFoundException(""Item does not exist!"") <mask>: raise ItemNotFoundException(e.get_dbus_message()) if e.get_dbus_name() in (DBUS_NO_REPLY, DBUS_NOT_SUPPORTED): raise SecretServiceNotAvailableException(e.get_dbus_message()) raise",False,if e . get_dbus_name ( ) == DBUS_NO_SUCH_OBJECT :,if e . get_dbus_name ( ) == DBUS_NOT_FOUND :,0.8820016898747209,69.97150369717171
"def run(self): """"""Continual loop evaluating when_statements"""""" while len(self.library) > 0: for name, expression in self.library.items(): <mask>: del self.library[name] else: expression.evaluate() sleep(0.01) return",False,if expression . remove_me == True :,if expression . is_statement ( ) :,0.8820016898747209,18.04438612975343
"def tamper(payload, **kwargs): junk_chars = ""!#$%&()*~+-_.,:;?@[/|\]^`"" retval = """" for i, char in enumerate(payload, start=1): amount = random.randint(10, 15) <mask>: retval += "">"" for _ in range(amount): retval += random.choice(junk_chars) elif char == ""<"": retval += ""<"" for _ in range(amount): retval += random.choice(junk_chars) elif char == "" "": for _ in range(amount): retval += random.choice(junk_chars) else: retval += char return retval",True,"if char == "">"" :","if char == "">"" :",0.8820016898747209,100.00000000000004
"def _source_target_path(source, source_path, source_location): target_path_attr = source.target_path or source.resdef.target_path if source.preserve_path: <mask>: log.warning( ""target-path '%s' specified with preserve-path - ignoring"", target_path_attr, ) return os.path.relpath(os.path.dirname(source_path), source_location) else: return target_path_attr or source.resdef.target_path or """"",True,if target_path_attr :,if target_path_attr :,0.8820016898747209,100.00000000000004
"def _load_user_from_header(self, header): if self._header_callback: user = self._header_callback(header) <mask>: app = current_app._get_current_object() user_loaded_from_header.send(app, user=user) return user return None",False,if user is not None :,if user :,0.8820016898747209,0.0
"def setup(cls): ""Check dependencies and warn about firewalling"" pathCheck(""brctl"", moduleName=""bridge-utils"") # Disable Linux bridge firewalling so that traffic can flow! for table in ""arp"", ""ip"", ""ip6"": cmd = ""sysctl net.bridge.bridge-nf-call-%stables"" % table out = quietRun(cmd).strip() <mask>: warn(""Warning: Linux bridge may not work with"", out, ""\n"")",False,"if out . endswith ( ""1"" ) :",if out :,0.8820016898747209,0.0
"def _browse_your_music(web_client, variant): if not web_client.logged_in: return [] if variant in (""tracks"", ""albums""): items = flatten( [ page.get(""items"", []) for page in web_client.get_all( f""me/{variant}"", params={""market"": ""from_token"", ""limit"": 50}, ) if page ] ) <mask>: return list(translator.web_to_track_refs(items)) else: return list(translator.web_to_album_refs(items)) else: return []",True,"if variant == ""tracks"" :","if variant == ""tracks"" :",0.8820016898747209,100.00000000000004
"def reset_styling(self): for edge in self.fsm_graph.edges_iter(): style_attr = self.fsm_graph.style_attributes.get(""edge"", {}).get(""default"") edge.attr.update(style_attr) for node in self.fsm_graph.nodes_iter(): <mask>: style_attr = self.fsm_graph.style_attributes.get(""node"", {}).get(""inactive"") node.attr.update(style_attr) for sub_graph in self.fsm_graph.subgraphs_iter(): style_attr = self.fsm_graph.style_attributes.get(""graph"", {}).get(""default"") sub_graph.graph_attr.update(style_attr)",False,"if ""point"" not in node . attr [ ""shape"" ] :","if node . state == ""inactive"" :",0.8820016898747209,6.699007141691558
"def set_message_type_visibility(self, message_type: MessageType): try: rows = { i for i, msg in enumerate(self.proto_analyzer.messages) <mask>: } if message_type.show: self.ui.tblViewProtocol.show_rows(rows) else: self.ui.tblViewProtocol.hide_rows(rows) except Exception as e: logger.exception(e)",False,if msg . message_type == message_type,if msg . visibility == message_type . visibility,0.8820016898747209,46.92470064105599
"def POP(cpu, *regs): for reg in regs: val = cpu.stack_pop(cpu.address_bit_size // 8) <mask>: cpu._set_mode_by_val(val) val = val & ~0x1 reg.write(val)",False,"if reg . reg in ( ""PC"" , ""R15"" ) :",if cpu . _mode_by_val :,0.8820016898747209,3.3495035708457803
"def processMovie(self, atom): for field in atom: if ""track"" in field: self.processTrack(field[""track""]) <mask>: self.processMovieHeader(field[""movie_hdr""])",False,"if ""movie_hdr"" in field :","elif ""movie_hdr"" in field :",0.8820016898747209,86.33400213704509
"def check_update_function(url, folder, update_setter, version_setter, auto): remote_version = urllib.urlopen(url).read() if remote_version.isdigit(): local_version = get_local_timestamp(folder) <mask>: if auto: update_setter.set_value(True) version_setter.set_value(remote_version) return True else: return False else: return False",False,if remote_version > local_version :,if local_version > remote_version :,0.8820016898747209,43.47208719449914
"def init(self, view, items=None): selections = [] if view.sel(): for region in view.sel(): selections.append(view.substr(region)) values = [] for idx, index in enumerate(map(int, items)): if idx >= len(selections): break i = index - 1 <mask>: values.append(selections[i]) else: values.append(None) # fill up for idx, value in enumerate(selections): if len(values) + 1 < idx: values.append(value) self.stack = values",False,if i >= 0 and i < len ( selections ) :,if i >= 0 :,0.8820016898747209,24.764986882297123
"def find_int_identifiers(directory): results = find_rules(directory, has_int_identifier) print(""Number of rules with integer identifiers: %d"" % len(results)) for result in results: rule_path = result[0] product_yaml_path = result[1] product_yaml = None <mask>: product_yaml = yaml.open_raw(product_yaml_path) fix_file(rule_path, product_yaml, fix_int_identifier)",False,if product_yaml_path is not None :,if product_yaml_path :,0.8820016898747209,54.77927682341229
"def condition(self): if self.__condition is None: if len(self.flat_conditions) == 1: # Avoid an extra indirection in the common case of only one condition. self.__condition = self.flat_conditions[0] <mask>: # Possible, if unlikely, due to filter predicate rewriting self.__condition = lambda _: True else: self.__condition = lambda x: all(cond(x) for cond in self.flat_conditions) return self.__condition",False,elif len ( self . flat_conditions ) == 0 :,elif self . flat_conditions [ 0 ] . condition is None :,0.8820016898747209,29.89950354998137
"def get_scene_exceptions_by_season(self, season=-1): scene_exceptions = [] for scene_exception in self.scene_exceptions: <mask>: continue scene_name, scene_season = scene_exception.split(""|"") if season == scene_season: scene_exceptions.append(scene_name) return scene_exceptions",False,if not len ( scene_exception ) == 2 :,"if scene_exception . startswith ( ""Scene"" ) :",0.8820016898747209,15.580105704117443
"def init(self, view, items=None): selections = [] if view.sel(): for region in view.sel(): selections.append(view.substr(region)) values = [] for idx, index in enumerate(map(int, items)): <mask>: break i = index - 1 if i >= 0 and i < len(selections): values.append(selections[i]) else: values.append(None) # fill up for idx, value in enumerate(selections): if len(values) + 1 < idx: values.append(value) self.stack = values",False,if idx >= len ( selections ) :,if idx == index :,0.8820016898747209,11.708995388048026
"def to_tool_path(self, path_or_uri_like, **kwds): if ""://"" not in path_or_uri_like: path = path_or_uri_like else: uri_like = path_or_uri_like <mask>: raise Exception(""Invalid URI passed to get_tool_source"") scheme, rest = uri_like.split("":"", 2) if scheme not in self.resolver_classes: raise Exception( ""Unknown tool scheme [{}] for URI [{}]"".format(scheme, uri_like) ) path = self.resolver_classes[scheme]().get_tool_source_path(uri_like) return path",False,"if "":"" not in path_or_uri_like :","if "":"" not in uri_like :",0.8820016898747209,52.6623069750211
def mainWindow(): global MW if not MW: for i in qApp.topLevelWidgets(): <mask>: MW = i return MW return None else: return MW,False,"if i . objectName ( ) == ""MainWindow"" :",if i . isWidget ( ) :,0.8820016898747209,15.749996500436227
"def async_get_service(hass, config, discovery_info=None): # pylint: disable=unused-argument """"""Get the demo notification service."""""" for account, account_dict in hass.data[DATA_ALEXAMEDIA][""accounts""].items(): for key, _ in account_dict[""devices""][""media_player""].items(): <mask>: _LOGGER.debug( ""%s: Media player %s not loaded yet; delaying load"", hide_email(account), hide_serial(key), ) return False return AlexaNotificationService(hass)",False,"if key not in account_dict [ ""entities"" ] [ ""media_player"" ] :",if account == key :,0.8820016898747209,1.1131164169159875
"def _migrate_bool(self, name: str, true_value: str, false_value: str) -> None: if name not in self._settings: return values = self._settings[name] if not isinstance(values, dict): return for scope, val in values.items(): <mask>: new_value = true_value if val else false_value self._settings[name][scope] = new_value self.changed.emit()",True,"if isinstance ( val , bool ) :","if isinstance ( val , bool ) :",0.8820016898747209,100.00000000000004
"def send(self, data, flags=0): self._checkClosed() if self._sslobj: <mask>: raise ValueError( ""non-zero flags not allowed in calls to send() on %s"" % self.__class__ ) return self._sslobj.write(data) else: return socket.send(self, data, flags)",False,if flags != 0 :,if flags == 0 :,0.8820016898747209,37.99178428257963
"def rec_deps(services, container_by_name, cnt, init_service): deps = cnt[""_deps""] for dep in deps.copy(): dep_cnts = services.get(dep) if not dep_cnts: continue dep_cnt = container_by_name.get(dep_cnts[0]) <mask>: # TODO: avoid creating loops, A->B->A if init_service and init_service in dep_cnt[""_deps""]: continue new_deps = rec_deps(services, container_by_name, dep_cnt, init_service) deps.update(new_deps) return deps",True,if dep_cnt :,if dep_cnt :,0.8820016898747209,100.00000000000004
"def as_dict(path="""", version=""latest"", section=""meta-data""): result = {} dirs = dir(path, version, section) if not dirs: return None for item in dirs: if item.endswith(""/""): records = as_dict(path + item, version, section) if records: result[item[:-1]] = records <mask>: idx, name = is_dict.match(item).groups() records = as_dict(path + idx + ""/"", version, section) if records: result[name] = records else: result[item] = valueconv(get(path + item, version, section)) return result",False,elif is_dict . match ( item ) :,"elif item . startswith ( ""/"" ) :",0.8820016898747209,11.044795567078939
"def PrintColGroup(col_names, schema): """"""Print HTML colgroup element, used for JavaScript sorting."""""" print("" <colgroup>"") for i, col in enumerate(col_names): if col.endswith(""_HREF""): continue # CSS class is used for sorting <mask>: css_class = ""number"" else: css_class = ""case-insensitive"" # NOTE: id is a comment only; not used print(' <col id=""{}"" type=""{}"" />'.format(col, css_class)) print("" </colgroup>"")",False,if schema . IsNumeric ( col ) :,if i == 0 :,0.8820016898747209,6.916271812933183
"def check_region(self, region): for other in self.regions: <mask>: continue if (other.start < region.start < other.end) or ( other.start < region.end < other.end ): raise Exception(""%r overlaps with %r"" % (region, other))",False,if other is region :,if region . start == other . start :,0.8820016898747209,5.934202609760488
"def _write_value(self, rng, value, scalar): if rng.api and value: # it is assumed by this stage that value is a list of lists <mask>: value = value[0][0] else: rng = rng.resize(len(value), len(value[0])) rng.raw_value = value",False,if scalar :,"if isinstance ( value [ 0 ] , list ) :",0.8820016898747209,4.456882760699063
"def TryMerge(self, d): while d.avail() > 0: tt = d.getVarInt32() if tt == 10: length = d.getVarInt32() tmp = ProtocolBuffer.Decoder(d.buffer(), d.pos(), d.pos() + length) d.skip(length) self.mutable_cost().TryMerge(tmp) continue if tt == 24: self.add_version(d.getVarInt64()) continue <mask>: raise ProtocolBuffer.ProtocolBufferDecodeError d.skipData(tt)",True,if tt == 0 :,if tt == 0 :,0.8820016898747209,100.00000000000004
"def generate_sv_faces(dcel_mesh, point_index, only_select=False, del_flag=None): # This part of function creates faces in SV format # It ignores boundless super face sv_faces = [] for i, face in enumerate(dcel_mesh.faces): <mask>: ""Face ({}) has inner components! Sverchok cant show polygons with holes."".format( i ) if not face.outer or del_flag in face.flags: continue if only_select and not face.select: continue sv_faces.append([point_index[hedge.origin] for hedge in face.outer.loop_hedges]) return sv_faces",False,if face . inners and face . outer :,if face . outer and face . outer . loop_hedges :,0.8820016898747209,28.917849332325716
"def _get_x_for_y(self, xValue, x, y): # print(""searching ""+x+"" with the value ""+str(xValue)+"" and want to give back ""+y) x_value = str(xValue) for anime in self.xmlMap.findall(""anime""): try: <mask>: return int(anime.get(y, 0)) except ValueError as e: continue return 0",False,"if anime . get ( x , False ) == x_value :","if anime . get ( x , 0 ) == x_value :",0.8820016898747209,80.03203203845001
"def dir_copy(src_dir, dest_dir, merge_if_exists=True): try: if not os.path.exists(dest_dir): shutil.copytree(src_dir, dest_dir) <mask>: merge_dir(src_dir, dest_dir) except OSError as e: # If source is not a directory, copy with shutil.copy if e.errno == errno.ENOTDIR: shutil.copy(src_dir, dest_dir) else: logging.error(""Could not copy %s to %s"", src_dir, dest_dir)",True,elif merge_if_exists :,elif merge_if_exists :,0.8820016898747209,100.00000000000004
"def mapping(self): m = {} if getGdriveCredentialsFile() is not None: m[""gdrive""] = """" unknown = 0 for f in self.scan: bits = f.split(""#"", 2) if len(bits) == 1: label = os.path.basename(f) else: label = bits[1] <mask>: label = ""L"" + str(unknown) unknown += 1 m[label] = bits[0] return m",False,"if not label or len ( label ) == 0 or label == """" :",if unknown :,0.8820016898747209,0.0
"def get_tag_values(self, event): http = event.interfaces.get(""sentry.interfaces.Http"") if not http: return [] if not http.headers: return [] headers = http.headers # XXX: transitional support for workers if isinstance(headers, dict): headers = headers.items() output = [] for key, value in headers: if key != ""User-Agent"": continue ua = Parse(value) <mask>: continue result = self.get_tag_from_ua(ua) if result: output.append(result) return output",True,if not ua :,if not ua :,0.8820016898747209,100.00000000000004
"def __iter__(self): it = DiskHashMerger.__iter__(self) direct_upstreams = self.direct_upstreams for k, groups in it: t = list([[] for _ in range(self.size)]) for i, g in enumerate(groups): if g: <mask>: t[i] = g else: g.sort(key=itemgetter(0)) g1 = [] for _, vs in g: g1.extend(vs) t[i] = g1 yield k, tuple(t)",False,if i in direct_upstreams :,if direct_upstreams :,0.8820016898747209,47.39878501170795
"def process_question(qtxt): question = """" skip = False for letter in qtxt: <mask>: skip = True if letter == "">"": skip = False if skip: continue if letter.isalnum() or letter == "" "": if letter == "" "": letter = ""_"" question += letter.lower() return question",True,"if letter == ""<"" :","if letter == ""<"" :",0.8820016898747209,100.00000000000004
"def _module_repr_from_spec(spec): """"""Return the repr to use for the module."""""" # We mostly replicate _module_repr() using the spec attributes. name = ""?"" if spec.name is None else spec.name if spec.origin is None: if spec.loader is None: return ""<module {!r}>"".format(name) else: return ""<module {!r} ({!r})>"".format(name, spec.loader) else: <mask>: return ""<module {!r} from {!r}>"".format(name, spec.origin) else: return ""<module {!r} ({})>"".format(spec.name, spec.origin)",False,if spec . has_location :,if spec . loader is None :,0.8820016898747209,26.269098944241588
"def test_row(self, row): for idx, test in self.patterns.items(): try: value = row[idx] except IndexError: value = """" result = test(value) if self.any_match: if result: return not self.inverse # True else: <mask>: return self.inverse # False if self.any_match: return self.inverse # False else: return not self.inverse # True",False,if not result :,if result :,0.8820016898747209,0.0
"def frequent_thread_switches(): """"""Make concurrency bugs more likely to manifest."""""" interval = None if not sys.platform.startswith(""java""): <mask>: interval = sys.getswitchinterval() sys.setswitchinterval(1e-6) else: interval = sys.getcheckinterval() sys.setcheckinterval(1) try: yield finally: if not sys.platform.startswith(""java""): if hasattr(sys, ""setswitchinterval""): sys.setswitchinterval(interval) else: sys.setcheckinterval(interval)",False,"if hasattr ( sys , ""getswitchinterval"" ) :","if hasattr ( sys , ""setswitchinterval"" ) :",0.8820016898747209,65.80370064762461
"def record_expected_exportable_production(self, ticks): """"""Record the amount of production that should be transferred to other islands."""""" for (quota_holder, resource_id), amount in self._low_priority_requests.items(): <mask>: self._settlement_manager_id[quota_holder] = WorldObject.get_object_by_id( int(quota_holder[1:].split("","")[0]) ).settlement_manager.worldid self.trade_storage[self._settlement_manager_id[quota_holder]][resource_id] += ( ticks * amount )",True,if quota_holder not in self . _settlement_manager_id :,if quota_holder not in self . _settlement_manager_id :,0.8820016898747209,100.00000000000004
"def _method_events_callback(self, values): try: previous_echoed = ( values[""child_result_list""][-1].decode().split(""\n"")[-2].strip() ) <mask>: return ""echo foo2\n"" elif previous_echoed.endswith(""foo2""): return ""echo foo3\n"" elif previous_echoed.endswith(""foo3""): return ""exit\n"" else: raise Exception(""Unexpected output {0!r}"".format(previous_echoed)) except IndexError: return ""echo foo1\n""",True,"if previous_echoed . endswith ( ""foo1"" ) :","if previous_echoed . endswith ( ""foo1"" ) :",0.8820016898747209,100.00000000000004
"def describe_cluster_snapshots(self, cluster_identifier=None, snapshot_identifier=None): if cluster_identifier: cluster_snapshots = [] for snapshot in self.snapshots.values(): <mask>: cluster_snapshots.append(snapshot) if cluster_snapshots: return cluster_snapshots if snapshot_identifier: if snapshot_identifier in self.snapshots: return [self.snapshots[snapshot_identifier]] raise ClusterSnapshotNotFoundError(snapshot_identifier) return self.snapshots.values()",False,if snapshot . cluster . cluster_identifier == cluster_identifier :,if snapshot . cluster_identifier == cluster_identifier :,0.8820016898747209,82.19198246700309
def get_snippet_edit_handler(model): if model not in SNIPPET_EDIT_HANDLERS: <mask>: # use the edit handler specified on the page class edit_handler = model.edit_handler else: panels = extract_panel_definitions_from_model_class(model) edit_handler = ObjectList(panels) SNIPPET_EDIT_HANDLERS[model] = edit_handler.bind_to(model=model) return SNIPPET_EDIT_HANDLERS[model],False,"if hasattr ( model , ""edit_handler"" ) :",if model . edit_handler :,0.8820016898747209,14.231728394642222
"def start(): if os.environ.get(""RUN_MAIN"") != ""true"": try: exit_code = restart_with_reloader() <mask>: os.kill(os.getpid(), -exit_code) else: sys.exit(exit_code) except KeyboardInterrupt: pass",False,if exit_code < 0 :,if exit_code == 0 :,0.8820016898747209,41.11336169005198
"def discover(self, *objlist): ret = [] for l in self.splitlines(): if len(l) < 5: continue <mask>: continue try: int(l[2]) int(l[3]) except: continue # ret.append(improve(l[0])) ret.append(l[0]) ret.sort() for item in objlist: ret.append(item) return ret",False,"if l [ 0 ] == ""Filename"" :","if l [ 0 ] == ""improve"" :",0.8820016898747209,74.19446627365011
"def ipfs_publish(self, lib): with tempfile.NamedTemporaryFile() as tmp: self.ipfs_added_albums(lib, tmp.name) try: <mask>: cmd = ""ipfs add --nocopy -q "".split() else: cmd = ""ipfs add -q "".split() cmd.append(tmp.name) output = util.command_output(cmd) except (OSError, subprocess.CalledProcessError) as err: msg = ""Failed to publish library. Error: {0}"".format(err) self._log.error(msg) return False self._log.info(""hash of library: {0}"", output)",False,"if self . config [ ""nocopy"" ] :",if lib . is_nocopy :,0.8820016898747209,6.050259138270144
"def spends(self): # Return spends indexed by hashX spends = defaultdict(list) utxos = self.mempool_utxos() for tx_hash, tx in self.txs.items(): for n, input in enumerate(tx.inputs): if input.is_generation(): continue prevout = (input.prev_hash, input.prev_idx) <mask>: hashX, value = utxos.pop(prevout) else: hashX, value = self.db_utxos[prevout] spends[hashX].append(prevout) return spends",True,if prevout in utxos :,if prevout in utxos :,0.8820016898747209,100.00000000000004
"def terminate(self): if self.returncode is None: try: os.kill(self.pid, TERM_SIGNAL) except OSError as exc: if getattr(exc, ""errno"", None) != errno.ESRCH: <mask>: raise",False,if self . wait ( timeout = 0.1 ) is None :,if exc . errno != errno . EINTR :,0.8820016898747209,4.85851417160653
"def _getVolumeScalar(self): if self._volumeScalar is not None: return self._volumeScalar # use default elif self._value in dynamicStrToScalar: return dynamicStrToScalar[self._value] else: thisDynamic = self._value # ignore leading s like in sf <mask>: thisDynamic = thisDynamic[1:] # ignore closing z like in fz if thisDynamic[-1] == ""z"": thisDynamic = thisDynamic[:-1] if thisDynamic in dynamicStrToScalar: return dynamicStrToScalar[thisDynamic] else: return dynamicStrToScalar[None]",False,"if ""s"" in thisDynamic :","if thisDynamic [ - 1 ] == ""s"" :",0.8820016898747209,14.991106946711685
"def init_values(self): config = self._raw_config for valname, value in self.overrides.iteritems(): if ""."" in valname: realvalname, key = valname.split(""."", 1) config.setdefault(realvalname, {})[key] = value else: config[valname] = value for name in config: <mask>: self.__dict__[name] = config[name] del self._raw_config",False,if name in self . values :,if name in self . __dict__ :,0.8820016898747209,36.72056269893591
"def modified(self): paths = set() dictionary_list = [] for op_list in self._operations: if not isinstance(op_list, list): op_list = (op_list,) for item in chain(*op_list): if item is None: continue dictionary = item.dictionary <mask>: continue paths.add(dictionary.path) dictionary_list.append(dictionary) return dictionary_list",True,if dictionary . path in paths :,if dictionary . path in paths :,0.8820016898747209,100.00000000000004
"def __getitem__(self, key, _get_mode=False): if not _get_mode: <mask>: return self._list[key] elif isinstance(key, slice): return self.__class__(self._list[key]) ikey = key.lower() for k, v in self._list: if k.lower() == ikey: return v # micro optimization: if we are in get mode we will catch that # exception one stack level down so we can raise a standard # key error instead of our special one. if _get_mode: raise KeyError() raise BadRequestKeyError(key)",False,"if isinstance ( key , ( int , long ) ) :","if isinstance ( key , dict ) :",0.8820016898747209,36.06452879987793
"def _get_items(self, name, target=1): all_items = self.get_items(name) items = [o for o in all_items if not o.disabled] if len(items) < target: <mask>: raise ItemNotFoundError(""insufficient items with name %r"" % name) else: raise AttributeError(""insufficient non-disabled items with name %s"" % name) on = [] off = [] for o in items: if o.selected: on.append(o) else: off.append(o) return on, off",False,if len ( all_items ) < target :,if not items :,0.8820016898747209,4.690733795095046
"def get_genome_dir(gid, galaxy_dir, data): """"""Return standard location of genome directories."""""" if galaxy_dir: refs = genome.get_refs(gid, None, galaxy_dir, data) seq_file = tz.get_in([""fasta"", ""base""], refs) <mask>: return os.path.dirname(os.path.dirname(seq_file)) else: gdirs = glob.glob(os.path.join(_get_data_dir(), ""genomes"", ""*"", gid)) if len(gdirs) == 1 and os.path.exists(gdirs[0]): return gdirs[0]",False,if seq_file and os . path . exists ( seq_file ) :,if seq_file :,0.8820016898747209,7.834966465489322
"def _PrintFuncs(self, names): # type: (List[str]) -> int status = 0 for name in names: <mask>: print(name) # TODO: Could print LST for -f, or render LST. Bash does this. 'trap' # could use that too. else: status = 1 return status",False,if name in self . funcs :,"if name . startswith ( ""-f"" ) :",0.8820016898747209,9.980099403873663
"def package_files(self): seen_package_directories = () directories = self.distribution.package_dir or {} empty_directory_exists = """" in directories packages = self.distribution.packages or [] for package in packages: if package in directories: package_directory = directories[package] elif empty_directory_exists: package_directory = os.path.join(directories[""""], package) else: package_directory = package <mask>: seen_package_directories += (package_directory + ""."",) yield package_directory",False,if not package_directory . startswith ( seen_package_directories ) :,if package_directory not in seen_package_directories :,0.8820016898747209,35.758619990303956
"def apply_conf_file(fn, conf_filename): for env in LSF_CONF_ENV: conf_file = get_conf_file(conf_filename, env) <mask>: with open(conf_file) as conf_handle: value = fn(conf_handle) if value: return value return None",True,if conf_file :,if conf_file :,0.8820016898747209,100.00000000000004
"def on_text(self, text): if text != self.chosen_text: self.fail_test('Expected ""{}"", received ""{}""'.format(self.chosen_text, text)) else: self.checks_passed += 1 <mask>: self.pass_test() else: self._select_next_text()",False,if self . checks_passed >= self . number_of_checks :,if self . checks_passed == self . max_checks :,0.8820016898747209,46.23497919151069
"def test_field_attr_existence(self): for name, item in ast.__dict__.items(): if self._is_ast_node(name, item): <mask>: # Index(value) just returns value now. # The argument is required. continue x = item() if isinstance(x, ast.AST): self.assertEqual(type(x._fields), tuple)",False,"if name == ""Index"" :","if isinstance ( item , ast . Index ) :",0.8820016898747209,5.522397783539471
"def apply(self, response): updated_headers = self.update_headers(response) if updated_headers: response.headers.update(updated_headers) warning_header_value = self.warning(response) <mask>: response.headers.update({""Warning"": warning_header_value}) return response",False,if warning_header_value is not None :,if warning_header_value :,0.8820016898747209,54.77927682341229
"def validate(self): self.assertEqual(len(self.inputs), len(self.outputs)) for batch_in, batch_out in zip(self.inputs, self.outputs): self.assertEqual(len(batch_in), len(batch_out)) if self.use_parallel_executor and not self.use_double_buffer: self.validate_unordered_batch(batch_in, batch_out) else: for in_data, out_data in zip(batch_in, batch_out): self.assertEqual(in_data.shape, out_data.shape) <mask>: self.assertTrue((in_data == out_data).all())",False,if not self . use_parallel_executor :,if self . use_parallel_executor and not self . use_double_buffer :,0.8820016898747209,41.12175645551035
def finalize(self): if self._started: <mask>: self._queue.put(None) self._queue.join() self._consumer.join() self._started = False self._finalized = True,False,if not self . _finalized :,if self . _queue is not None :,0.8820016898747209,21.10534063187263
"def _get_ilo_version(self): try: self._get_ilo2('<?xml version=""1.0""?><RIBCL VERSION=""2.0""></RIBCL>') except ResponseError as e: <mask>: if e.code == 405: return 3 if e.code == 501: return 1 raise return 2",False,"if hasattr ( e , ""code"" ) :",if e . code == 404 :,0.8820016898747209,6.082317172853824
"def _check_data(self, source, expected_bytes, expected_duration): received_bytes = 0 received_seconds = 0.0 bytes_to_read = 1024 while True: data = source.get_audio_data(bytes_to_read) <mask>: break received_bytes += data.length received_seconds += data.duration self.assertEqual(data.length, len(data.data)) self.assertAlmostEqual(expected_duration, received_seconds, places=1) self.assertAlmostEqual(expected_bytes, received_bytes, delta=5)",False,if data is None :,if not data :,0.8820016898747209,16.37226966703825
"def __randomize_interval_task(self): for job in self.aps_scheduler.get_jobs(): <mask>: self.aps_scheduler.modify_job( job.id, next_run_time=datetime.now() + timedelta( seconds=randrange( job.trigger.interval.total_seconds() * 0.75, job.trigger.interval.total_seconds(), ) ), )",False,"if isinstance ( job . trigger , IntervalTrigger ) :",if job . trigger . interval . total_seconds ( ) > 0.5 :,0.8820016898747209,12.090340630072072
"def find_approximant(x): c = 1e-4 it = sympy.ntheory.continued_fraction_convergents( sympy.ntheory.continued_fraction_iterator(x) ) for i in it: p, q = i.as_numer_denom() tol = c / q ** 2 <mask>: return i if tol < machine_epsilon: break return x",False,if abs ( i - x ) <= tol :,if p < machine_epsilon :,0.8820016898747209,4.880869806051147
"def fix_newlines(lines): """"""Convert newlines to unix."""""" for i, line in enumerate(lines): if line.endswith(""\r\n""): lines[i] = line[:-2] + ""\n"" <mask>: lines[i] = line[:-1] + ""\n""",False,"elif line . endswith ( ""\r"" ) :","elif line . endswith ( ""\n"" ) :",0.8820016898747209,70.16879391277372
"def payment_control_render(self, request: HttpRequest, payment: OrderPayment): template = get_template(""pretixplugins/paypal/control.html"") sale_id = None for trans in payment.info_data.get(""transactions"", []): for res in trans.get(""related_resources"", []): <mask>: sale_id = res[""sale""][""id""] ctx = { ""request"": request, ""event"": self.event, ""settings"": self.settings, ""payment_info"": payment.info_data, ""order"": payment.order, ""sale_id"": sale_id, } return template.render(ctx)",False,"if ""sale"" in res and ""id"" in res [ ""sale"" ] :","if res [ ""sale"" ] :",0.8820016898747209,24.909923021496894
"def for_name(self, name): try: name_resources = self._resources[name] except KeyError: raise LookupError(name) else: for res in name_resources: try: inst = res.inst() except Exception as e: <mask>: log.exception(""error initializing %s"", res) else: log.error(""error initializing %s: %s"", res, e) else: yield inst",False,if log . getEffectiveLevel ( ) <= logging . DEBUG :,"if e . args [ 0 ] == ""No instance found"" :",0.8820016898747209,3.716499092256817
"def describe(self, done=False): description = ShellCommand.describe(self, done) if done: <mask>: description = [""compile""] description.append(""%d projects"" % self.getStatistic(""projects"", 0)) description.append(""%d files"" % self.getStatistic(""files"", 0)) warnings = self.getStatistic(""warnings"", 0) if warnings > 0: description.append(""%d warnings"" % warnings) errors = self.getStatistic(""errors"", 0) if errors > 0: description.append(""%d errors"" % errors) return description",False,if not description :,if description is None :,0.8820016898747209,14.058533129758727
"def parse_list(tl): ls = [] nm = [] while True: term, nmt, tl = parse_term(tl) ls.append(term) <mask>: nm.append(nmt) if tl[0] != "","": break tl = tl[1:] return ls, nm, tl",False,if nmt is not None :,if nmt :,0.8820016898747209,0.0
"def infer_dataset_impl(path): if IndexedRawTextDataset.exists(path): return ""raw"" elif IndexedDataset.exists(path): with open(index_file_path(path), ""rb"") as f: magic = f.read(8) <mask>: return ""cached"" elif magic == MMapIndexedDataset.Index._HDR_MAGIC[:8]: return ""mmap"" else: return None elif FastaDataset.exists(path): return ""fasta"" else: return None",False,if magic == IndexedDataset . _HDR_MAGIC :,if magic == MMapIndexedDataset . Index . _HDR_MAGIC [ : 8 ] :,0.8820016898747209,36.821398145189974
"def _get(self): fut = item = None with self._mutex: # Critical section never blocks. <mask>: fut = Future() fut.add_done_callback( lambda f: self._get_complete() if not f.cancelled() else None ) self._getters.append(fut) else: item = self._get_item() self._get_complete() return item, fut",False,if not self . _queue or self . _getters :,if self . _is_future :,0.8820016898747209,14.827340167306767
"def validate(self): dates = [] for d in self.get(""leave_block_list_dates""): # date is not repeated <mask>: frappe.msgprint( _(""Date is repeated"") + "":"" + d.block_date, raise_exception=1 ) dates.append(d.block_date)",False,if d . block_date in dates :,if d . block_date not in dates :,0.8820016898747209,65.80370064762461
"def on_choose_watch_dir_clicked(self): if self.window().watchfolder_enabled_checkbox.isChecked(): previous_watch_dir = self.window().watchfolder_location_input.text() or """" watch_dir = QFileDialog.getExistingDirectory( self.window(), ""Please select the watch folder"", previous_watch_dir, QFileDialog.ShowDirsOnly, ) <mask>: return self.window().watchfolder_location_input.setText(watch_dir)",False,if not watch_dir :,if watch_dir is None :,0.8820016898747209,27.77619034011791
"def log_generator(self, limit=6000, **kwargs): # Generator for show_log_panel skip = 0 while True: logs = self.log(limit=limit, skip=skip, **kwargs) if not logs: break for entry in logs: yield entry <mask>: break skip = skip + limit",False,if len ( logs ) < limit :,if skip >= limit :,0.8820016898747209,12.872632311973014
"def _setUpClass(cls): global solver import pyomo.environ from pyomo.solvers.tests.io.writer_test_cases import testCases for test_case in testCases: <mask>: solver[(test_case.name, test_case.io)] = True",False,"if ( ( test_case . name , test_case . io ) in solver ) and ( test_case . available ) :","if test_case . name in pyomo . environ . get ( ""TEST_CASES"" ) :",0.8820016898747209,16.95847624191497
"def _get_file_data(self, normpath, normrev): data = self.client.cat(normpath, normrev) if has_expanded_svn_keywords(data): # Find out if this file has any keyword expansion set. # If it does, collapse these keywords. This is because SVN # will return the file expanded to us, which would break patching. keywords = self.client.propget(""svn:keywords"", normpath, normrev, recurse=True) <mask>: data = collapse_svn_keywords(data, force_bytes(keywords[normpath])) return data",False,if normpath in keywords :,if keywords :,0.8820016898747209,0.0
"def add_controller_list(path): if not os.path.exists(os.path.join(path, ""__init__.py"")): bb.fatal(""Controllers directory %s exists but is missing __init__.py"" % path) files = sorted( [f for f in os.listdir(path) if f.endswith("".py"") and not f.startswith(""_"")] ) for f in files: module = ""oeqa.controllers."" + f[:-3] <mask>: controllerslist.append(module) else: bb.warn( ""Duplicate controller module found for %s, only one added. Layers should create unique controller module names"" % module )",False,if module not in controllerslist :,"if os . path . exists ( os . path . join ( path , module ) ) :",0.8820016898747209,2.6643211213888947
"def on_session2(event): new_xmpp.get_roster() new_xmpp.send_presence() logging.info(roster[0]) data = roster[0][""roster""][""items""] logging.info(data) for jid, item in data.items(): <mask>: new_xmpp.send_presence(ptype=""subscribe"", pto=jid) new_xmpp.update_roster(jid, name=item[""name""], groups=item[""groups""]) new_xmpp.disconnect()",False,"if item [ ""subscription"" ] != ""none"" :","if item [ ""type"" ] == ""subscribe"" :",0.8820016898747209,28.917849332325716
"def _parse_class_simplified(symbol): results = {} name = symbol.name + ""("" name += "", "".join([analyzer.expand_attribute(base) for base in symbol.bases]) name += "")"" for sym in symbol.body: <mask>: result = _parse_function_simplified(sym, symbol.name) results.update(result) elif isinstance(sym, ast.ClassDef): result = _parse_class_simplified(sym) results.update(result) lineno = symbol.lineno for decorator in symbol.decorator_list: lineno += 1 results[lineno] = (name, ""c"") return results",True,"if isinstance ( sym , ast . FunctionDef ) :","if isinstance ( sym , ast . FunctionDef ) :",0.8820016898747209,100.00000000000004
"def check_args(args): """"""Checks that the args are coherent."""""" check_args_has_attributes(args) if args.v: non_version_attrs = [v for k, v in args.__dict__.items() if k != ""v""] print(""non_version_attrs"", non_version_attrs) <mask>: fail(""Cannot show the version number with another command."") return if args.i is None: fail(""Cannot draw ER diagram of no database."") if args.o is None: fail(""Cannot draw ER diagram with no output file."")",False,if len ( [ v for v in non_version_attrs if v is not None ] ) != 0 :,if non_version_attrs :,0.8820016898747209,5.895794377172451
"def handle(self, *args, **options): if not settings.ST_BASE_DIR.endswith(""spirit""): raise CommandError( ""settings.ST_BASE_DIR is not the spirit root folder, are you overriding it?"" ) for root, dirs, files in os.walk(settings.ST_BASE_DIR): <mask>: continue with utils.pushd(root): call_command( ""makemessages"", stdout=self.stdout, stderr=self.stderr, **options ) self.stdout.write(""ok"")",False,"if ""locale"" not in dirs :",if not dirs or not files :,0.8820016898747209,8.051153633013374
"def scan(scope): for s in scope.children: if s.start_pos <= position <= s.end_pos: <mask>: return scan(s) or s elif s.type in (""suite"", ""decorated""): return scan(s) return None",False,"if isinstance ( s , ( tree . Scope , tree . Flow ) ) :","if s . type in ( ""suite"" , ""decorated"" ) :",0.8820016898747209,6.725321874176006
def run_sync(self): count = 0 while count < self.args.num_messages: batch = self.receiver.fetch_next(max_batch_size=self.args.num_messages - count) <mask>: for msg in batch: msg.complete() count += len(batch),False,if self . args . peeklock :,if len ( batch ) > 0 :,0.8820016898747209,6.567274736060395
"def __getitem__(self, item): if self._datas is not None: ret = [] for data in self._datas: <mask>: ret.append(data[self._offset]) else: ret.append(data.iloc[self._offset]) self._offset += 1 return ret else: return self._get_data(item)",False,"if isinstance ( data , np . ndarray ) :","if isinstance ( data , dict ) :",0.8820016898747209,46.307771619910305
"def removedir(self, path): # type: (Text) -> None _path = self.validatepath(path) if _path == ""/"": raise errors.RemoveRootError() with ftp_errors(self, path): try: self.ftp.rmd(_encode(_path, self.ftp.encoding)) except error_perm as error: code, _ = _parse_ftp_error(error) if code == ""550"": <mask>: raise errors.DirectoryExpected(path) if not self.isempty(path): raise errors.DirectoryNotEmpty(path) raise # pragma: no cover",False,if self . isfile ( path ) :,if not self . isexpected ( path ) :,0.8820016898747209,36.88939732334405
"def replaces_in_file(file, replacement_list): rs = [(re.compile(regexp), repl) for (regexp, repl) in replacement_list] file_tmp = file + ""."" + str(os.getpid()) + "".tmp"" with open(file, ""r"") as f: with open(file_tmp, ""w"") as f_tmp: for line in f: for r, replace in rs: match = r.search(line) <mask>: line = replace + ""\n"" f_tmp.write(line) shutil.move(file_tmp, file)",True,if match :,if match :,0.8820016898747209,0.0
"def _get_path_check_mem(self, i, size): if size > 0: <mask>: p = self._get_path(i, -1) else: p = self._get_path(i, size) if p.startswith(""/dev/shm""): env.meminfo.add(size) else: p = self._get_path(i, size) return p",False,if env . meminfo . rss + size > env . meminfo . mem_limit_soft :,if size == 0 :,0.8820016898747209,1.2237376376462188
"def find_widget_by_id(self, id, parent=None): """"""Recursively searches for widget with specified ID"""""" if parent == None: if id in self: return self[id] # Do things fast if possible parent = self[""editor""] for c in parent.get_children(): <mask>: if c.get_id() == id: return c if isinstance(c, Gtk.Container): r = self.find_widget_by_id(id, c) if not r is None: return r return None",False,"if hasattr ( c , ""get_id"" ) :","if isinstance ( c , Gtk . Widget ) :",0.8820016898747209,16.830386789031852
"def _deserialize(cls, io): flags = VideoFlags() flags.byte = U8.read(io) if flags.bit.type == VIDEO_FRAME_TYPE_COMMAND_FRAME: data = VideoCommandFrame.deserialize(io) else: <mask>: data = AVCVideoData.deserialize(io) else: data = io.read() return cls(flags.bit.type, flags.bit.codec, data)",False,if flags . bit . codec == VIDEO_CODEC_ID_AVC :,if flags . bit . type == AVC_FRAME_TYPE_AUDIO_DATA :,0.8820016898747209,26.46015952359329
"def asciiLogData(data, maxlen=64, replace=False): ellipses = "" ..."" try: <mask>: dd = data[:maxlen] + ellipses else: dd = data return dd.decode(""utf8"", errors=""replace"" if replace else ""strict"") except: return ""0x"" + binLogData(data, maxlen)",False,if len ( data ) > maxlen - len ( ellipses ) :,if len ( data ) > maxlen :,0.8820016898747209,46.53786298485943
"def _check_units(self, new_unit_system): # If no unit system has been specified for me yet, adopt the incoming # system if self.unit_system is None: self.unit_system = new_unit_system else: # Otherwise, make sure they match <mask>: raise ValueError( ""Unit system mismatch %d v. %d"" % (self.unit_system, new_unit_system) )",True,if self . unit_system != new_unit_system :,if self . unit_system != new_unit_system :,0.8820016898747209,100.00000000000004
"def command(filenames, dirnames, fix): for filename in gather_files(dirnames, filenames): visitor = process_file(filename) <mask>: print(""%s: %s"" % (filename, visitor.get_stats())) if fix: print(""Fixing: %s"" % filename) fix_file(filename)",False,if visitor . needs_fix ( ) :,if visitor :,0.8820016898747209,0.0
"def assign_attributes_to_variants(variant_attributes): for value in variant_attributes: pk = value[""pk""] defaults = value[""fields""] defaults[""variant_id""] = defaults.pop(""variant"") defaults[""assignment_id""] = defaults.pop(""assignment"") assigned_values = defaults.pop(""values"") assoc, created = AssignedVariantAttribute.objects.update_or_create( pk=pk, defaults=defaults ) <mask>: assoc.values.set(AttributeValue.objects.filter(pk__in=assigned_values))",True,if created :,if created :,0.8820016898747209,0.0
"def _info(self, userlist): for strng in userlist: group_matched = False for env in self.base.comps.environments_by_pattern(strng): self.output.display_groups_in_environment(env) group_matched = True for group in self.base.comps.groups_by_pattern(strng): self.output.display_pkgs_in_groups(group) group_matched = True <mask>: logger.error(_(""Warning: Group %s does not exist.""), strng) return 0, []",True,if not group_matched :,if not group_matched :,0.8820016898747209,100.00000000000004
"def parse_implements_interfaces(parser): types = [] if parser.token.value == ""implements"": advance(parser) while True: types.append(parse_named_type(parser)) <mask>: break return types",False,"if not peek ( parser , TokenKind . NAME ) :","if parser . token . value == ""implements"" :",0.8820016898747209,4.789232204309912
"def generate(): for leaf in u.leaves: if isinstance(leaf, Integer): val = leaf.get_int_value() if val in (0, 1): yield val else: raise _NoBoolVector elif isinstance(leaf, Symbol): if leaf == SymbolTrue: yield 1 <mask>: yield 0 else: raise _NoBoolVector else: raise _NoBoolVector",True,elif leaf == SymbolFalse :,elif leaf == SymbolFalse :,0.8820016898747209,100.00000000000004
"def update_gstin(context): dirty = False for key, value in iteritems(frappe.form_dict): if key != ""party"": address_name = frappe.get_value(""Address"", key) <mask>: address = frappe.get_doc(""Address"", address_name) address.gstin = value.upper() address.save(ignore_permissions=True) dirty = True if dirty: frappe.db.commit() context.updated = True",True,if address_name :,if address_name :,0.8820016898747209,100.00000000000004
"def everythingIsUnicode(d): """"""Takes a dictionary, recursively verifies that every value is unicode"""""" for k, v in d.iteritems(): <mask>: if not everythingIsUnicode(v): return False elif isinstance(v, list): for i in v: if isinstance(i, dict) and not everythingIsUnicode(i): return False elif isinstance(i, _bytes): return False elif isinstance(v, _bytes): return False return True",False,"if isinstance ( v , dict ) and k != ""headers"" :","if isinstance ( v , dict ) :",0.8820016898747209,36.24372413507827
"def check_graph(graph): # pragma: no cover for c in graph: <mask>: raise RuntimeError(""cannot have fuse"") for inp in c.inputs: if isinstance(inp.op, Fuse): raise RuntimeError(""cannot have fuse"")",True,"if isinstance ( c . op , Fuse ) :","if isinstance ( c . op , Fuse ) :",0.8820016898747209,100.00000000000004
"def __getattr__(self, key): try: value = self.__parent.contents[key] except KeyError: pass else: <mask>: if isinstance(value, _ModuleMarker): return value.mod_ns else: assert isinstance(value, _MultipleClassMarker) return value.attempt_get(self.__parent.path, key) raise AttributeError( ""Module %r has no mapped classes "" ""registered under the name %r"" % (self.__parent.name, key) )",False,if value is not None :,if self . __parent . name == key :,0.8820016898747209,4.02724819242185
"def filter_ports(self, dpid, in_port, nw_id, allow_nw_id_external=None): assert nw_id != self.nw_id_unknown ret = [] for port in self.get_ports(dpid): nw_id_ = port.network_id if port.port_no == in_port: continue <mask>: ret.append(port.port_no) elif allow_nw_id_external is not None and nw_id_ == allow_nw_id_external: ret.append(port.port_no) return ret",True,if nw_id_ == nw_id :,if nw_id_ == nw_id :,0.8820016898747209,100.00000000000004
"def _parse(self, contents): entries = [] for line in contents.splitlines(): if not len(line.strip()): entries.append((""blank"", [line])) continue (head, tail) = chop_comment(line.strip(), ""#"") <mask>: entries.append((""all_comment"", [line])) continue entries.append((""option"", [head.split(None), tail])) return entries",False,if not len ( head ) :,"if head == """" :",0.8820016898747209,8.643019616048525
"def _get_documented_completions(self, table, startswith=None): names = [] for key, command in table.items(): if getattr(command, ""_UNDOCUMENTED"", False): # Don't tab complete undocumented commands/params continue <mask>: continue if getattr(command, ""positional_arg"", False): continue names.append(key) return names",False,if startswith is not None and not key . startswith ( startswith ) :,if startswith and key . startswith ( startswith ) :,0.8820016898747209,48.663863137761844
"def _convert_example(example, use_bfloat16): """"""Cast int64 into int32 and float32 to bfloat16 if use_bfloat16."""""" for key in list(example.keys()): val = example[key] <mask>: val = tf.sparse.to_dense(val) if val.dtype == tf.int64: val = tf.cast(val, tf.int32) if use_bfloat16 and val.dtype == tf.float32: val = tf.cast(val, tf.bfloat16) example[key] = val",False,if tf . keras . backend . is_sparse ( val ) :,if use_bfloat16 and val . dtype == tf . float32 :,0.8820016898747209,7.768562846380172
"def _get_lang_zone(self, lang): if lang not in self._lang_zone_from_lang: <mask>: self._lang_zone_from_lang[lang] = MultiLangZone(self.mgr, lang) else: self._lang_zone_from_lang[lang] = LangZone(self.mgr, lang) return self._lang_zone_from_lang[lang]",False,if self . mgr . is_multilang ( lang ) :,if lang in self . _multi_lang_zones :,0.8820016898747209,8.91376552139813
"def dispatch(self, request, *args, **kwargs): try: return super(Handler, self).dispatch(request, *args, **kwargs) except Http404 as e: <mask>: try: request.original_path_info = request.path_info request.path_info = settings.FEINCMS_CMS_404_PAGE response = super(Handler, self).dispatch(request, *args, **kwargs) response.status_code = 404 return response except Http404: raise e else: raise",False,if settings . FEINCMS_CMS_404_PAGE :,if e . status_code == 404 :,0.8820016898747209,5.6775429106661015
"def _maybe_update_dropout(self, step): for i in range(len(self.dropout_steps)): <mask>: self.model.update_dropout(self.dropout[i]) logger.info(""Updated dropout to %f from step %d"" % (self.dropout[i], step))",False,if step > 1 and step == self . dropout_steps [ i ] + 1 :,if self . dropout [ i ] [ 0 ] == step :,0.8820016898747209,15.304677073113364
"def bulk_move(*args, **kwargs): for arg in args: <mask>: raise PopupException(_(""Source path and destination path cannot be same"")) request.fs.rename( urllib.unquote(arg[""src_path""]), urllib.unquote(arg[""dest_path""]) )",False,"if arg [ ""src_path"" ] == arg [ ""dest_path"" ] :","if arg [ ""src_path"" ] != arg [ ""dest_path"" ] :",0.8820016898747209,85.78928092681438
"def asisWrite(self, root): at, c = self, self.c try: c.endEditing() c.init_error_dialogs() fileName = at.initWriteIvars(root, root.atAsisFileNodeName()) <mask>: at.addToOrphanList(root) return at.openOutputStream() for p in root.self_and_subtree(copy=False): at.writeAsisNode(p) contents = at.closeOutputStream() at.replaceFile(contents, at.encoding, fileName, root) except Exception: at.writeException(fileName, root)",False,"if not at . precheck ( fileName , root ) :",if at . isOrphanList ( root ) :,0.8820016898747209,19.324558191221733
"def next_event(it): """"""read an event from an eventstream"""""" while True: try: line = next(it) except StopIteration: return <mask>: return json.loads(line.split("":"", 1)[1])",False,"if line . startswith ( ""data:"" ) :","if line . startswith ( ""event:"" ) :",0.8820016898747209,70.16879391277372
"def process_formdata(self, valuelist): if valuelist: if valuelist[0] == ""__None"": self.data = None else: <mask>: self.data = None return try: obj = self.queryset.get(pk=valuelist[0]) self.data = obj except DoesNotExist: self.data = None",False,if self . queryset is None :,if len ( valuelist ) == 0 :,0.8820016898747209,5.669791110976001
"def _setResultsName(self, name, listAllMatches=False): if __diag__.warn_multiple_tokens_in_named_alternation: <mask>: warnings.warn( ""{}: setting results name {!r} on {} expression "" ""will return a list of all parsed tokens in an And alternative, "" ""in prior versions only the first token was returned"".format( ""warn_multiple_tokens_in_named_alternation"", name, type(self).__name__, ), stacklevel=3, ) return super()._setResultsName(name, listAllMatches)",False,"if any ( isinstance ( e , And ) for e in self . exprs ) :",if name in self . _tokens :,0.8820016898747209,7.582874853312503
"def add(request): form_type = ""servers"" if request.method == ""POST"": form = BookMarkForm(request.POST) if form.is_valid(): form_type = form.save() messages.add_message(request, messages.INFO, ""Bookmark created"") else: messages.add_message(request, messages.INFO, form.errors) <mask>: url = reverse(""servers"") else: url = reverse(""metrics"") return redirect(url) else: return redirect(reverse(""servers""))",False,"if form_type == ""server"" :","if form_type == ""servers"" :",0.8820016898747209,70.71067811865478
"def __init__(self, post_id, artist, page, tzInfo=None, dateFormat=None): self.imageUrls = list() self.imageResizedUrls = list() self.imageId = int(post_id) self._tzInfo = tzInfo self.dateFormat = dateFormat if page is not None: post_json = demjson.decode(page) <mask>: artist_id = post_json[""data""][""item""][""user""][""id""] self.artist = SketchArtist(artist_id, page, tzInfo, dateFormat) else: self.artist = artist self.parse_post(post_json[""data""][""item""])",False,if artist is None :,"if ""user"" in post_json :",0.8820016898747209,5.669791110976001
"def _create_batch_iterator( self, mark_as_delete: Callable[[Any], None], to_key: Callable[[Any], Any], to_value: Callable[[Any], Any], batch: Iterable[EventT], ) -> Iterable[Tuple[Any, Any]]: for event in batch: key = to_key(event.key) # to delete keys in the table we set the raw value to None <mask>: mark_as_delete(key) continue yield key, to_value(event.value)",False,if event . message . value is None :,if mark_as_delete :,0.8820016898747209,5.868924818816531
"def test_lc_numeric_nl_langinfo(self): # Test nl_langinfo against known values tested = False for loc in candidate_locales: try: setlocale(LC_NUMERIC, loc) setlocale(LC_CTYPE, loc) except Error: continue for li, lc in ((RADIXCHAR, ""decimal_point""), (THOUSEP, ""thousands_sep"")): <mask>: tested = True if not tested: self.skipTest(""no suitable locales"")",False,"if self . numeric_tester ( ""nl_langinfo"" , nl_langinfo ( li ) , lc , loc ) :","if lc in ( LC_NUMERIC , lc ) :",0.8820016898747209,3.4145996011322652
"def _level_up_logging(self): for handler in self.log.handlers: <mask>: if handler.level != logging.DEBUG: handler.setLevel(logging.DEBUG) self.log.debug(""Leveled up log file verbosity"")",False,"if issubclass ( handler . __class__ , logging . FileHandler ) :",if handler . isEnabledFor ( logging . DEBUG ) :,0.8820016898747209,8.572272939203375
def _show_axes_changed(self): marker = self.marker if (self._vtk_control is not None) and (marker is not None): <mask>: marker.interactor = None marker.enabled = False else: marker.interactor = self.interactor marker.enabled = True self.render(),False,if not self . show_axes :,if self . interactor is None :,0.8820016898747209,13.540372457315735
"def handle_keypress(self, rawKey, modifiers, key, *args): if self.recordKeyboard and self.__delayPassed(): <mask>: self.insideKeys = True self.targetParent.start_key_sequence() modifierCount = len(modifiers) if ( modifierCount > 1 or (modifierCount == 1 and Key.SHIFT not in modifiers) or (Key.SHIFT in modifiers and len(rawKey) > 1) ): self.targetParent.append_hotkey(rawKey, modifiers) elif key not in MODIFIERS: self.targetParent.append_key(key)",False,if not self . insideKeys :,if key in MODIFIERS :,0.8820016898747209,10.400597689005304
"def transform(self, data): with timer(""transform %s"" % self.name, logging.DEBUG): if self.operator in {""lat"", ""latitude""}: return self.series(data).apply(GeoIP.get_latitude) <mask>: return self.series(data).apply(GeoIP.get_longitude) elif self.operator in {""acc"", ""accuracy""}: return self.series(data).apply(GeoIP.get_accuracy) raise NameError(""Unknown GeoIP operator [lat, lon, acc]: %s"" % self.operator)",False,"elif self . operator in { ""lon"" , ""longitude"" } :","elif self . operator in { lon , ""longitude"" :",0.8820016898747209,51.68242559252794
"def _get_sidebar_selected(self): sidebar_selected = None if self.businessline_id: sidebar_selected = ""bl_%s"" % self.businessline_id <mask>: sidebar_selected += ""_s_%s"" % self.service_id if self.environment_id: sidebar_selected += ""_env_%s"" % self.environment_id return sidebar_selected",True,if self . service_id :,if self . service_id :,0.8820016898747209,100.00000000000004
"def _run_response_middleware(self, request, response, request_name=None): named_middleware = self.named_response_middleware.get(request_name, deque()) applicable_middleware = self.response_middleware + named_middleware if applicable_middleware: for middleware in applicable_middleware: _response = middleware(request, response) <mask>: _response = await _response if _response: response = _response break return response",False,if isawaitable ( _response ) :,if _response :,0.8820016898747209,17.946048174042403
"def populate_obj(self, obj, name): field = getattr(obj, name, None) if field is not None: # If field should be deleted, clean it up if self._should_delete: field.delete() return <mask>: if not field.grid_id: func = field.put else: func = field.replace func( self.data.stream, filename=self.data.filename, content_type=self.data.content_type, )",False,"if isinstance ( self . data , FileStorage ) and not is_empty ( self . data . stream ) :",if self . _should_replace :,0.8820016898747209,2.4133890647162253
"def _import_hash(self, operator): # Import required modules into local namespace so that pipelines # may be evaluated directly for key in sorted(operator.import_hash.keys()): module_list = "", "".join(sorted(operator.import_hash[key])) <mask>: exec(""from {} import {}"".format(key[4:], module_list)) else: exec(""from {} import {}"".format(key, module_list)) for var in operator.import_hash[key]: self.operators_context[var] = eval(var)",False,"if key . startswith ( ""tpot."" ) :","if key . startswith ( ""import_"" ) :",0.8820016898747209,58.77283725105324
"def remove_files(folder, file_extensions): for f in os.listdir(folder): f_path = os.path.join(folder, f) <mask>: extension = os.path.splitext(f_path)[1] if extension in file_extensions: os.remove(f_path)",True,if os . path . isfile ( f_path ) :,if os . path . isfile ( f_path ) :,0.8820016898747209,100.00000000000004
"def clearBuffer(self): if self.shouldLose == -1: return if self.producer: self.producer.resumeProducing() if self.buffer: <mask>: self.logFile.write(""loopback receiving %s\n"" % repr(self.buffer)) buffer = self.buffer self.buffer = b"""" self.target.dataReceived(buffer) if self.shouldLose == 1: self.shouldLose = -1 self.target.connectionLost(failure.Failure(main.CONNECTION_DONE))",True,if self . logFile :,if self . logFile :,0.8820016898747209,100.00000000000004
"def write(self, data): if mock_target._mirror_on_stderr: if self._write_line: sys.stderr.write(fn + "": "") if bytes: sys.stderr.write(data.decode(""utf8"")) else: sys.stderr.write(data) <mask>: self._write_line = True else: self._write_line = False super(Buffer, self).write(data)",False,"if ( data [ - 1 ] ) == ""\n"" :",if self . _write_line :,0.8820016898747209,2.7376474102577792
def stop(self): self.queue_com.state_lock.acquire() try: <mask>: self.queue_com.state = STOPPED self.remove() return True return False finally: self.queue_com.state_lock.release(),False,if self . queue_com . state == RUNNING and self . stop_task ( ) :,if self . queue_com . state == STOPPED :,0.8820016898747209,42.434788372432536
"def _handle_special_args(self, pyobjects): if len(pyobjects) == len(self.arguments.args): if self.arguments.vararg: pyobjects.append(rope.base.builtins.get_list()) <mask>: pyobjects.append(rope.base.builtins.get_dict())",False,if self . arguments . kwarg :,elif self . arguments . args . dict :,0.8820016898747209,31.55984539112946
"def go_to_last_edit_location(self): if self.last_edit_cursor_pos is not None: filename, position = self.last_edit_cursor_pos if not osp.isfile(filename): self.last_edit_cursor_pos = None return else: self.load(filename) editor = self.get_current_editor() <mask>: editor.set_cursor_position(position)",False,if position < editor . document ( ) . characterCount ( ) :,if editor is not None :,0.8820016898747209,3.3264637832151163
"def _create_sentence_objects(self): """"""Returns a list of Sentence objects from the raw text."""""" sentence_objects = [] sent_tokenizer = SentenceTokenizer(locale=self.language.code) seq = Sequence(self.raw) seq = sent_tokenizer.transform(seq) for start_index, end_index in zip(seq.idx[:-1], seq.idx[1:]): # Sentences share the same models as their parent blob sent = seq.text[start_index:end_index].strip() <mask>: continue s = Sentence(sent, start_index=start_index, end_index=end_index) s.detected_languages = self.detected_languages sentence_objects.append(s) return sentence_objects",True,if not sent :,if not sent :,0.8820016898747209,100.00000000000004
"def to_json_schema(self, parent=None): schema = {} if not parent: schema[""title""] = self.title <mask>: schema[""description""] = self.description if self.has_default: schema[""default""] = self.default schema[""_required_""] = self.required if self.null: schema[""type""] = [""string"", ""null""] else: schema[""type""] = ""string"" if self.enum is not None: schema[""enum""] = self.enum return schema",False,if self . description :,if self . description is not None :,0.8820016898747209,36.55552228545123
def rmdir(dirname): if dirname[-1] == os.sep: dirname = dirname[:-1] if os.path.islink(dirname): return # do not clear link - we can get out of dir for f in os.listdir(dirname): <mask>: continue path = dirname + os.sep + f if os.path.isdir(path): rmdir(path) else: os.unlink(path) os.rmdir(dirname),False,"if f in ( ""."" , "".."" ) :",if f . startswith ( os . sep ) :,0.8820016898747209,9.150273711870005
"def convert_whole_dir(path=Path(""marian_ckpt/"")): for subdir in tqdm(list(path.ls())): dest_dir = f""marian_converted/{subdir.name}"" <mask>: continue convert(source_dir, dest_dir)",False,"if ( dest_dir / ""pytorch_model.bin"" ) . exists ( ) :","if subdir . name == ""marian_ckpt"" :",0.8820016898747209,2.9576737109722444
"def colorformat(text): if text[0:1] == ""#"": col = text[1:] if len(col) == 6: return col <mask>: return col[0] * 2 + col[1] * 2 + col[2] * 2 elif text == """": return """" assert False, ""wrong color format %r"" % text",True,elif len ( col ) == 3 :,elif len ( col ) == 3 :,0.8820016898747209,100.00000000000004
"def _init_rel_seek(self): ""Sets the file object's position to the relative location set above."" rs, fo = self._rel_seek, self._file_obj if rs == 0.0: fo.seek(0, os.SEEK_SET) else: fo.seek(0, os.SEEK_END) size = fo.tell() <mask>: self._cur_pos = size else: target = int(size * rs) fo.seek(target, os.SEEK_SET) self._align_to_newline() self._cur_pos = fo.tell()",False,if rs == 1.0 :,if rs == 0.0 :,0.8820016898747209,53.7284965911771
"def parse_command_line(self, argv=None): """"""Parse the command line"""""" if self.config: parser = argparse.ArgumentParser(add_help=False) self.settings[""config""].add_argument(parser) opts, _ = parser.parse_known_args(argv) if opts.config is not None: self.set(""config"", opts.config) self.params.update(self.import_from_module()) parser = self.parser() opts = parser.parse_args(argv) for k, v in opts.__dict__.items(): <mask>: continue self.set(k.lower(), v)",False,if v is None :,"if k . startswith ( ""_"" ) :",0.8820016898747209,4.990049701936832
"def process(self, resources, event=None): client = local_session(self.manager.session_factory).client( ""shield"", region_name=""us-east-1"" ) protections = get_type_protections(client, self.manager.get_model()) protected_resources = {p[""ResourceArn""] for p in protections} state = self.data.get(""state"", False) results = [] for arn, r in zip(self.manager.get_arns(resources), resources): r[""c7n:ShieldProtected""] = shielded = arn in protected_resources <mask>: results.append(r) elif not shielded and not state: results.append(r) return results",True,if shielded and state :,if shielded and state :,0.8820016898747209,100.00000000000004
"def removeTrailingWs(self, aList): i = 0 while i < len(aList): <mask>: j = i i = self.skip_ws(aList, i) assert j < i if i >= len(aList) or aList[i] == ""\n"": # print ""removing trailing ws:"", `i-j` del aList[j:i] i = j else: i += 1",False,if self . is_ws ( aList [ i ] ) :,"if aList [ i ] == ""\r"" :",0.8820016898747209,21.586404366478295
"def predict(request: Request): form = await request.form() files, entry = convert_input(form) try: <mask>: return JSONResponse(ALL_FEATURES_PRESENT_ERROR, status_code=400) try: resp = model.predict(data_dict=[entry]).to_dict(""records"")[0] return JSONResponse(resp) except Exception as e: logger.error(""Error: {}"".format(str(e))) return JSONResponse(COULD_NOT_RUN_INFERENCE_ERROR, status_code=500) finally: for f in files: os.remove(f.name)",False,if ( entry . keys ( ) & input_features ) != input_features :,if entry is None :,0.8820016898747209,1.044177559991939
"def reset(self): logger.debug(""Arctic.reset()"") with self._lock: <mask>: self.__conn.close() self.__conn = None for _, l in self._library_cache.items(): if hasattr(l, ""_reset"") and callable(l._reset): logger.debug(""Library reset() %s"" % l) l._reset() # the existence of _reset() is not guaranteed/enforced, it also triggers re-auth",False,if self . __conn is not None :,if self . __conn :,0.8820016898747209,54.77927682341229
"def read(self): if op.isfile(self.fileName): with textfile_open(self.fileName, ""rt"") as fid: items = json.load(fid) # TODO: catch JSON exception... <mask>: items = dict() else: items = dict() self._items.clear() self._items.update(items) self._haveReadData = True",False,if items is None :,if not items :,0.8820016898747209,16.37226966703825
"def get_django_comment(text: str, i: int) -> str: end = i + 4 unclosed_end = 0 while end <= len(text): if text[end - 2 : end] == ""#}"": return text[i:end] <mask>: unclosed_end = end end += 1 raise TokenizationException(""Unclosed comment"", text[i:unclosed_end])",False,"if not unclosed_end and text [ end ] == ""<"" :","elif text [ end - 2 : end ] == ""#}"" :",0.8820016898747209,33.64932442330151
"def _wrap_forwarded(self, key, value): if isinstance(value, SourceCode) and value.late_binding: # get cached return value if present value_ = self._late_binding_returnvalues.get(key, KeyError) <mask>: # evaluate the late-bound function value_ = self._eval_late_binding(value) schema = self.late_bind_schemas.get(key) if schema is not None: value_ = schema.validate(value_) # cache result of late bound func self._late_binding_returnvalues[key] = value_ return value_ else: return value",False,if value_ is KeyError :,if value_ is None :,0.8820016898747209,53.7284965911771
"def connect(*args, **ckwargs): if ""give_content_type"" in kwargs: <mask>: kwargs[""give_content_type""](args[6][""content-type""]) else: kwargs[""give_content_type""]("""") if ""give_connect"" in kwargs: kwargs[""give_connect""](*args, **ckwargs) status = code_iter.next() etag = etag_iter.next() timestamp = timestamps_iter.next() if status == -1: raise HTTPException() return FakeConn(status, etag, body=kwargs.get(""body"", """"), timestamp=timestamp)",False,"if len ( args ) >= 7 and ""content_type"" in args [ 6 ] :","if args [ 6 ] [ ""content-type"" ] :",0.8820016898747209,13.229147212652599
"def _reset(self): self._handle_connect() if self.rewarder_session: <mask>: env_id = random.choice(self._sample_env_ids) logger.info(""Randomly sampled env_id={}"".format(env_id)) else: env_id = None self.rewarder_session.reset(env_id=env_id) else: logger.info( ""No rewarder session exists, so cannot send a reset via the rewarder channel"" ) self._reset_mask() return [None] * self.n",True,if self . _sample_env_ids :,if self . _sample_env_ids :,0.8820016898747209,100.00000000000004
"def _create_architecture_list(architectures, current_arch): if not architectures: return [_Architecture(build_on=[current_arch])] build_architectures: List[str] = [] architecture_list: List[_Architecture] = [] for item in architectures: if isinstance(item, str): build_architectures.append(item) <mask>: architecture_list.append( _Architecture(build_on=item.get(""build-on""), run_on=item.get(""run-on"")) ) if build_architectures: architecture_list.append(_Architecture(build_on=build_architectures)) return architecture_list",False,"if isinstance ( item , dict ) :","elif isinstance ( item , dict ) :",0.8820016898747209,84.08964152537145
"def inspect(self, pokemon): # Make sure it was not caught! for caught_pokemon in self.cache: same_latitude = ""{0:.4f}"".format(pokemon[""latitude""]) == ""{0:.4f}"".format( caught_pokemon[""latitude""] ) same_longitude = ""{0:.4f}"".format(pokemon[""longitude""]) == ""{0:.4f}"".format( caught_pokemon[""longitude""] ) <mask>: return if len(self.cache) >= 200: self.cache.pop(0) self.cache.append(pokemon)",False,if same_latitude and same_longitude :,"if same_latitude != caught_pokemon [ ""latitude"" ] :",0.8820016898747209,18.20705281109213
"def parley(self): for x in [0, 1]: a = self.agents[x].act() <mask>: if ""[DONE]"" in a[""text""]: self.agents[x - 1].observe( {""id"": ""World"", ""text"": ""The other agent has ended the chat.""} ) self.episodeDone = True else: self.agents[x - 1].observe(a)",False,if a is not None :,"if a [ ""id"" ] == ""World"" :",0.8820016898747209,6.837203339116283
"def _prepare_subset( full_data: torch.Tensor, full_targets: torch.Tensor, num_samples: int, digits: Sequence, ): classes = {d: 0 for d in digits} indexes = [] for idx, target in enumerate(full_targets): label = target.item() if classes.get(label, float(""inf"")) >= num_samples: continue indexes.append(idx) classes[label] += 1 <mask>: break data = full_data[indexes] targets = full_targets[indexes] return data, targets",False,if all ( classes [ k ] >= num_samples for k in classes ) :,if classes [ label ] == 0 :,0.8820016898747209,4.6166333787043365
"def get_work_root(self, flags): _flags = flags.copy() _flags[""is_toplevel""] = True target = self._get_target(_flags) if target: _flags[""target""] = target.name tool = self.get_tool(_flags) <mask>: return target.name + ""-"" + tool else: raise SyntaxError( ""Failed to determine work root. Could not resolve tool for target "" + target.name ) else: raise SyntaxError(""Failed to determine work root. Could not resolve target"")",True,if tool :,if tool :,0.8820016898747209,0.0
"def run_command(self, data): """"""Run editor commands."""""" parts = data.split("" "") cmd = parts[0].lower() if cmd in self.operations.keys(): return self.run_operation(cmd) args = "" "".join(parts[1:]) self.logger.debug(""Looking for command '{0}'"".format(cmd)) if cmd in self.modules.modules.keys(): self.logger.debug(""Trying to run command '{0}'"".format(cmd)) self.get_editor().store_action_state(cmd) <mask>: return False else: self.set_status(""Command '{0}' not found."".format(cmd)) return False return True",False,"if not self . run_module ( cmd , args ) :","if self . run_command ( cmd , args ) :",0.8820016898747209,59.74178044844197
"def get_main_chain_layers(self): """"""Return a list of layer IDs in the main chain."""""" main_chain = self.get_main_chain() ret = [] for u in main_chain: for v, layer_id in self.adj_list[u]: <mask>: ret.append(layer_id) return ret",False,if v in main_chain and u in main_chain :,if v == layer_id :,0.8820016898747209,7.030417713400723
"def hash(self, context): with context: <mask>: return IECore.MurmurHash() h = GafferDispatch.TaskNode.hash(self, context) h.append(self[""fileName""].hash()) h.append(self[""in""].hash()) h.append(self.__parameterHandler.hash()) return h",False,"if not self [ ""fileName"" ] . getValue ( ) or self [ ""in"" ] . source ( ) == self [ ""in"" ] :",if self . __parameterHandler is None :,0.8820016898747209,0.5235532762795567
"def consume_buf(): ty = state[""ty""] - 1 for i in xrange(state[""buf""].shape[1] // N): tx = x // N + i src = state[""buf""][:, i * N : (i + 1) * N, :] <mask>: with self.tile_request(tx, ty, readonly=False) as dst: mypaintlib.tile_convert_rgba8_to_rgba16(src, dst, self.EOTF) if state[""progress""]: try: state[""progress""].completed(ty - ty0) except Exception: logger.exception(""Progress.completed() failed"") state[""progress""] = None",False,"if src [ : , : , 3 ] . any ( ) :",if tx != 0 :,0.8820016898747209,2.544354209531657
"def check_permissions(self, obj): request = self.context.get(""request"") for Perm in permissions: perm = Perm() if not perm.has_permission(request, self): return False <mask>: return False return True",False,"if not perm . has_object_permission ( request , self , obj ) :",if not perm . is_valid ( obj ) :,0.8820016898747209,20.94181156631843
"def _post_order(op): if isinstance(op, tvm.tir.Allocate): lift_stmt[-1].append(op) return op.body if isinstance(op, tvm.tir.AttrStmt): if op.attr_key == ""storage_scope"": lift_stmt[-1].append(op) return op.body <mask>: return _merge_block(lift_stmt.pop() + [op], op.body) return op if isinstance(op, tvm.tir.For): return _merge_block(lift_stmt.pop() + [op], op.body) raise RuntimeError(""not reached"")",False,"if op . attr_key == ""virtual_thread"" :","if isinstance ( op , tvm . tir . For ) :",0.8820016898747209,4.053997537205932
"def task_done(self): with self._cond: if not self._unfinished_tasks.acquire(False): raise ValueError(""task_done() called too many times"") <mask>: self._cond.notify_all()",False,if self . _unfinished_tasks . _semlock . _is_zero ( ) :,if self . _cond . is_set ( ) :,0.8820016898747209,21.279762431577232
"def get_json(self): if not hasattr(self, ""_json""): self._json = None <mask>: self._json = json.loads(self.request.body) return self._json",False,"if self . request . headers . get ( ""Content-Type"" , """" ) . startswith ( ""application/json"" ) :","if self . request . method == ""GET"" :",0.8820016898747209,11.10310128232865
"def userfullname(): """"""Get the user's full name."""""" global _userfullname <mask>: uid = os.getuid() entry = pwd_from_uid(uid) if entry: _userfullname = entry[4].split("","")[0] or entry[0] if not _userfullname: _userfullname = ""user%d"" % uid return _userfullname",True,if not _userfullname :,if not _userfullname :,0.8820016898747209,100.00000000000004
"def test_scatter(self): for rank in range(self.world_size): tensor = [] <mask>: tensor = [torch.tensor(i) for i in range(self.world_size)] result = comm.get().scatter(tensor, rank, size=()) self.assertTrue(torch.is_tensor(result)) self.assertEqual(result.item(), self.rank)",False,if self . rank == rank :,if rank == self . rank :,0.8820016898747209,39.28146509005134
"def decompile(decompiler): for pos, next_pos, opname, arg in decompiler.instructions: if pos in decompiler.targets: decompiler.process_target(pos) method = getattr(decompiler, opname, None) if method is None: throw(DecompileError(""Unsupported operation: %s"" % opname)) decompiler.pos = pos decompiler.next_pos = next_pos x = method(*arg) <mask>: decompiler.stack.append(x)",True,if x is not None :,if x is not None :,0.8820016898747209,100.00000000000004
"def print_scenario_ran(self, scenario): if scenario.passed: self.wrt(""OK"") elif scenario.failed: reason = self.scenarios_and_its_fails[scenario] <mask>: self.wrt(""FAILED"") else: self.wrt(""ERROR"") self.wrt(""\n"")",False,"if isinstance ( reason . exception , AssertionError ) :",if reason :,0.8820016898747209,0.0
"def detect_ssl_option(self): for option in self.ssl_options(): if scan_argv(self.argv, option) is not None: for other_option in self.ssl_options(): if option != other_option: <mask>: raise ConfigurationError( ""Cannot give both %s and %s"" % (option, other_option) ) return option",False,"if scan_argv ( self . argv , other_option ) is not None :",if other_option != option :,0.8820016898747209,7.582874853312503
"def print_po_snippet(en_loc_old_lists, context): for m, localized, old in zip(*en_loc_old_lists): if m == """": continue <mask>: localized = old print( ""#: {file}:{line}\n"" 'msgid ""{context}{en_month}""\n' 'msgstr ""{localized_month}""\n'.format( context=context, file=filename, line=print_po_snippet.line, en_month=m, localized_month=localized, ) ) print_po_snippet.line += 1",False,if m == localized :,"if localized == """" :",0.8820016898747209,16.515821590069034
"def set_status(self, dict_new): for i, value in dict_new.items(): self.dict_bili[i] = value <mask>: self.dict_bili[""pcheaders""][""cookie""] = value self.dict_bili[""appheaders""][""cookie""] = value",False,"if i == ""cookie"" :","if ""pcheaders"" in self . dict_bili :",0.8820016898747209,5.300156689756295
"def makeSomeFiles(pathobj, dirdict): pathdict = {} for (key, value) in dirdict.items(): child = pathobj.child(key) <mask>: pathdict[key] = child child.setContent(value) elif isinstance(value, dict): child.createDirectory() pathdict[key] = makeSomeFiles(child, value) else: raise ValueError(""only strings and dicts allowed as values"") return pathdict",False,"if isinstance ( value , bytes ) :","if isinstance ( value , str ) :",0.8820016898747209,59.4603557501361
"def _truncate_to_length(generator, len_map=None): for example in generator: example = list(example) if len_map is not None: for key, max_len in len_map.items(): example_len = example[key].shape <mask>: example[key] = np.resize(example[key], max_len) yield tuple(example)",True,if example_len > max_len :,if example_len > max_len :,0.8820016898747209,100.00000000000004
"def check(self, **kw): if not kw: return exists(self.strpath) if len(kw) == 1: if ""dir"" in kw: return not kw[""dir""] ^ isdir(self.strpath) <mask>: return not kw[""file""] ^ isfile(self.strpath) return super(LocalPath, self).check(**kw)",False,"if ""file"" in kw :","elif ""file"" in kw :",0.8820016898747209,80.91067115702207
"def next_instruction_is_function_or_class(lines): """"""Is the first non-empty, non-commented line of the cell either a function or a class?"""""" parser = StringParser(""python"") for i, line in enumerate(lines): if parser.is_quoted(): parser.read_line(line) continue parser.read_line(line) if not line.strip(): # empty line if i > 0 and not lines[i - 1].strip(): return False continue if line.startswith(""def "") or line.startswith(""class ""): return True <mask>: continue return False return False",False,"if line . startswith ( ( ""#"" , ""@"" , "" "" , "")"" ) ) :","if line . startswith ( ""function "" ) or line . startswith ( ""class "" ) :",0.8820016898747209,23.18552731654568
"def askCheckReadFile(self, localFile, remoteFile): if not kb.bruteMode: message = ""do you want confirmation that the remote file '%s' "" % remoteFile message += ""has been successfully downloaded from the back-end "" message += ""DBMS file system? [Y/n] "" <mask>: return self._checkFileLength(localFile, remoteFile, True) return None",False,"if readInput ( message , default = ""Y"" , boolean = True ) :",if self . confirm ( message ) :,0.8820016898747209,6.075831217041836
"def process_tag(hive_name, company, company_key, tag, default_arch): with winreg.OpenKeyEx(company_key, tag) as tag_key: version = load_version_data(hive_name, company, tag, tag_key) if version is not None: # if failed to get version bail major, minor, _ = version arch = load_arch_data(hive_name, company, tag, tag_key, default_arch) <mask>: exe_data = load_exe(hive_name, company, company_key, tag) if exe_data is not None: exe, args = exe_data return company, major, minor, arch, exe, args",True,if arch is not None :,if arch is not None :,0.8820016898747209,100.00000000000004
"def _get_matching_bracket(self, s, pos): if s[pos] != ""{"": return None end = len(s) depth = 1 pos += 1 while pos != end: c = s[pos] if c == ""{"": depth += 1 <mask>: depth -= 1 if depth == 0: break pos += 1 if pos < end and s[pos] == ""}"": return pos return None",True,"elif c == ""}"" :","elif c == ""}"" :",0.8820016898747209,100.00000000000004
"def pred(field, value, item): for suffix, p in _BUILTIN_PREDS.iteritems(): if field.endswith(suffix): f = field[: field.index(suffix)] <mask>: return False return p(getattr(item, f), value) if not hasattr(item, field) or getattr(item, field) is None: return False if isinstance(value, type(lambda x: x)): return value(getattr(item, field)) return getattr(item, field) == value",True,"if not hasattr ( item , f ) or getattr ( item , f ) is None :","if not hasattr ( item , f ) or getattr ( item , f ) is None :",0.8820016898747209,100.00000000000004
"def init_weights(self): """"""Initialize model weights."""""" for _, m in self.multi_deconv_layers.named_modules(): if isinstance(m, nn.ConvTranspose2d): normal_init(m, std=0.001) elif isinstance(m, nn.BatchNorm2d): constant_init(m, 1) for m in self.multi_final_layers.modules(): <mask>: normal_init(m, std=0.001, bias=0)",True,"if isinstance ( m , nn . Conv2d ) :","if isinstance ( m , nn . Conv2d ) :",0.8820016898747209,100.00000000000004
"def test_byteswap(self): if self.typecode == ""u"": example = ""\U00100100"" else: example = self.example a = array.array(self.typecode, example) self.assertRaises(TypeError, a.byteswap, 42) if a.itemsize in (1, 2, 4, 8): b = array.array(self.typecode, example) b.byteswap() <mask>: self.assertEqual(a, b) else: self.assertNotEqual(a, b) b.byteswap() self.assertEqual(a, b)",False,if a . itemsize == 1 :,elif a . itemsize == 1 :,0.8820016898747209,84.08964152537145
"def _remove_blocks_from_variables(variables): new_variables = [] for name, variable in variables: <mask>: new_variables.extend(variable.locals) new_variables.append((name, variable.result)) else: new_variables.append((name, variable)) return new_variables",False,if variable . is_block ( ) :,"if isinstance ( variable , Block ) :",0.8820016898747209,12.256200970377108
def scope(self): <mask>: self.lazy_init_lock_.acquire() try: if self.scope_ is None: self.scope_ = Scope() finally: self.lazy_init_lock_.release() return self.scope_,True,if self . scope_ is None :,if self . scope_ is None :,0.8820016898747209,100.00000000000004
"def translate(): assert Lex.next() is AttributeList reader.read() # Discard attribute list from reader. attrs = {} d = AttributeList.match.groupdict() for k, v in d.items(): if v is not None: if k == ""attrlist"": v = subs_attrs(v) <mask>: parse_attributes(v, attrs) else: AttributeList.attrs[k] = v AttributeList.subs(attrs) AttributeList.attrs.update(attrs)",False,if v :,"elif k == ""attrs"" :",0.8820016898747209,5.522397783539471
"def parse(self, response): try: content = response.content.decode(""utf-8"", ""ignore"") content = json.loads(content, strict=False) except: self.logger.error(""Fail to parse the response in json format"") return for item in content[""data""]: <mask>: img_url = self._decode_url(item[""objURL""]) elif ""hoverURL"" in item: img_url = item[""hoverURL""] else: continue yield dict(file_url=img_url)",True,"if ""objURL"" in item :","if ""objURL"" in item :",0.8820016898747209,100.00000000000004
"def canonicalize_instruction_name(instr): name = instr.insn_name().upper() # XXX bypass a capstone bug that incorrectly labels some insns as mov if name == ""MOV"": if instr.mnemonic.startswith(""lsr""): return ""LSR"" elif instr.mnemonic.startswith(""lsl""): return ""LSL"" <mask>: return ""ASR"" return OP_NAME_MAP.get(name, name)",True,"elif instr . mnemonic . startswith ( ""asr"" ) :","elif instr . mnemonic . startswith ( ""asr"" ) :",0.8820016898747209,100.00000000000004
"def _clean_regions(items, region): """"""Intersect region with target file if it exists"""""" variant_regions = bedutils.population_variant_regions(items, merged=True) with utils.tmpfile() as tx_out_file: target = subset_variant_regions(variant_regions, region, tx_out_file, items) <mask>: if isinstance(target, six.string_types) and os.path.isfile(target): target = _load_regions(target) else: target = [target] return target",True,if target :,if target :,0.8820016898747209,0.0
def reader_leaves(self): self.mutex.acquire() try: self.active_readers -= 1 <mask>: self.active_writers += 1 self.waiting_writers -= 1 self.can_write.release() finally: self.mutex.release(),False,if self . active_readers == 0 and self . waiting_writers != 0 :,if self . waiting_readers > 0 :,0.8820016898747209,15.377854660688898
"def _bpe_to_words(sentence, delimiter=""@@""): """"""Convert a sequence of bpe words into sentence."""""" words = [] word = """" delimiter_len = len(delimiter) for subwords in sentence: <mask>: word += subwords[:-delimiter_len] else: word += subwords words.append(word) word = """" return words",False,if len ( subwords ) >= delimiter_len and subwords [ - delimiter_len : ] == delimiter :,if subwords [ - delimiter_len : ] == delimiter :,0.8820016898747209,43.121378656560694
"def _make_var_names(exog): if hasattr(exog, ""name""): var_names = exog.name elif hasattr(exog, ""columns""): var_names = exog.columns else: raise ValueError(""exog is not a Series or DataFrame or is unnamed."") try: var_names = "" "".join(var_names) except TypeError: # cannot have names that are numbers, pandas default from statsmodels.base.data import _make_exog_names <mask>: var_names = ""x1"" else: var_names = "" "".join(_make_exog_names(exog)) return var_names",False,if exog . ndim == 1 :,"if isinstance ( exog , ( int , float ) ) :",0.8820016898747209,4.456882760699063
"def __start_element_handler(self, name, attrs): if name == ""mime-type"": <mask>: for extension in self.extensions: self[extension] = self.type self.type = attrs[""type""].lower() self.extensions = [] elif name == ""glob"": pattern = attrs[""pattern""] if pattern.startswith(""*.""): self.extensions.append(pattern[1:].lower())",True,if self . type :,if self . type :,0.8820016898747209,100.00000000000004
"def nodes(self, id=None, name=None): for node_dict in self.node_ls(id=id, name=name): node_id = node_dict[""ID""] node = DockerNode(self, node_id, inspect=node_dict) <mask>: continue yield node",False,if self . _node_prefix and not node . name . startswith ( self . _node_prefix ) :,if node . is_active ( ) :,0.8820016898747209,3.2737451753202267
"def fix_repeating_arguments(self): """"""Fix elements that should accumulate/increment values."""""" either = [list(child.children) for child in transform(self).children] for case in either: for e in [child for child in case if case.count(child) > 1]: if type(e) is Argument or type(e) is Option and e.argcount: if e.value is None: e.value = [] elif type(e.value) is not list: e.value = e.value.split() <mask>: e.value = 0 return self",False,if type ( e ) is Command or type ( e ) is Option and e . argcount == 0 :,elif type ( e . value ) is None :,0.8820016898747209,6.914662646185369
"def vi_search(self, rng): for i in rng: line_history = self._history.history[i] pos = line_history.get_line_text().find(self._vi_search_text) <mask>: self._history.history_cursor = i self.l_buffer.line_buffer = list(line_history.line_buffer) self.l_buffer.point = pos self.vi_undo_restart() return True self._bell() return False",False,if pos >= 0 :,if pos != - 1 :,0.8820016898747209,15.619699684601283
"def visitIf(self, node, scope): for test, body in node.tests: <mask>: if type(test.value) in self._const_types: if not test.value: continue self.visit(test, scope) self.visit(body, scope) if node.else_: self.visit(node.else_, scope)",False,"if isinstance ( test , ast . Const ) :","if isinstance ( test , ast . If ) :",0.8820016898747209,70.71067811865478
"def collect(self): for nickname in self.squid_hosts.keys(): squid_host = self.squid_hosts[nickname] fulldata = self._getData(squid_host[""host""], squid_host[""port""]) <mask>: fulldata = fulldata.splitlines() for data in fulldata: matches = self.stat_pattern.match(data) if matches: self.publish_counter( ""%s.%s"" % (nickname, matches.group(1)), float(matches.group(2)) )",False,if fulldata is not None :,"if isinstance ( fulldata , str ) :",0.8820016898747209,7.267884212102741
"def convert(x, base, exponents): out = [] for e in exponents: d = int(x / (base ** e)) x -= d * (base ** e) out.append(digits[d]) <mask>: break return out",False,if x == 0 and e < 0 :,if x < 0 :,0.8820016898747209,18.393972058572114
"def print_doc(manager, options): plugin_name = options.doc plugin = plugins.get(plugin_name, None) if plugin: <mask>: console(""Plugin %s does not have documentation"" % plugin_name) else: console("""") console(trim(plugin.instance.__doc__)) console("""") else: console(""Could not find plugin %s"" % plugin_name)",True,if not plugin . instance . __doc__ :,if not plugin . instance . __doc__ :,0.8820016898747209,100.00000000000004
"def _set_attrs(self, attrs): for attr in self.ATTRS: if attr in attrs: setattr(self, attr, attrs[attr]) del attrs[attr] else: <mask>: setattr(self, attr, NO_DEFAULT) else: setattr(self, attr, None) if attrs: attrs = sorted(attrs.keys()) raise OptionError(""invalid keyword arguments: %s"" % "", "".join(attrs), self)",False,"if attr == ""default"" :",if attr == NO_DEFAULT :,0.8820016898747209,36.55552228545123
"def _get_set_scope( ir_set: irast.Set, scope_tree: irast.ScopeTreeNode ) -> irast.ScopeTreeNode: if ir_set.path_scope_id: new_scope = scope_tree.root.find_by_unique_id(ir_set.path_scope_id) <mask>: raise errors.InternalServerError( f""dangling scope pointer to node with uid"" f"":{ir_set.path_scope_id} in {ir_set!r}"" ) else: new_scope = scope_tree return new_scope",True,if new_scope is None :,if new_scope is None :,0.8820016898747209,100.00000000000004
"def test_leave_one_out(self): correct = 0 k = 3 model = kNN.train(xs, ys, k) predictions = [1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1] for i in range(len(predictions)): model = kNN.train(xs[:i] + xs[i + 1 :], ys[:i] + ys[i + 1 :], k) prediction = kNN.classify(model, xs[i]) self.assertEqual(prediction, predictions[i]) <mask>: correct += 1 self.assertEqual(correct, 13)",False,if prediction == ys [ i ] :,if model . is_train ( ) :,0.8820016898747209,5.669791110976001
"def import_files(self, files): """"""Import a list of MORE (.csv) files."""""" c = self.c if files: changed = False self.tab_width = c.getTabWidth(c.p) for fileName in files: g.setGlobalOpenDir(fileName) p = self.import_file(fileName) <mask>: p.contract() p.setDirty() c.setChanged(True) changed = True if changed: c.redraw(p)",True,if p :,if p :,0.8820016898747209,0.0
"def getPageTemplate(payload, place): retVal = (kb.originalPage, kb.errorIsNone) if payload and place: <mask>: page, _, _ = Request.queryPage(payload, place, content=True, raise404=False) kb.pageTemplates[(payload, place)] = (page, kb.lastParserStatus is None) retVal = kb.pageTemplates[(payload, place)] return retVal",True,"if ( payload , place ) not in kb . pageTemplates :","if ( payload , place ) not in kb . pageTemplates :",0.8820016898747209,100.00000000000004
"def _skip_trivial(constraint_data): if skip_trivial_constraints: if isinstance(constraint_data, LinearCanonicalRepn): if constraint_data.variables is None: return True else: <mask>: return True return False",False,if constraint_data . body . polynomial_degree ( ) == 0 :,if constraint_data . variables . is_trivial ( ) :,0.8820016898747209,28.86883041186855
"def get_unique_attribute(self, name: str): feat = None for f in self.features: if self._return_feature(f) and hasattr(f, name): <mask>: raise RuntimeError(""The attribute was not unique."") feat = f if feat is None: raise RuntimeError(""The attribute did not exist"") return getattr(feat, name)",False,if feat is not None :,if name in f . unique_attributes :,0.8820016898747209,5.669791110976001
"def hideEvent(self, event): """"""Reimplement Qt method"""""" if not self.light: for plugin in self.widgetlist: <mask>: plugin.visibility_changed(True) QMainWindow.hideEvent(self, event)",False,if plugin . isAncestorOf ( self . last_focused_widget ) :,if plugin . isVisible ( ) :,0.8820016898747209,11.835764736093042
"def move_stdout_to_stderr(self): to_remove = [] to_add = [] for consumer_level, consumer in self.consumers: <mask>: to_remove.append((consumer_level, consumer)) to_add.append((consumer_level, sys.stderr)) for item in to_remove: self.consumers.remove(item) self.consumers.extend(to_add)",False,if consumer == sys . stdout :,if consumer . is_stdout ( ) :,0.8820016898747209,11.99014838091355
"def create(exported_python_target): if exported_python_target not in created: self.context.log.info( ""Creating setup.py project for {}"".format(exported_python_target) ) subject = self.derived_by_original.get( exported_python_target, exported_python_target ) setup_dir, dependencies = self.create_setup_py(subject, dist_dir) created[exported_python_target] = setup_dir <mask>: for dep in dependencies: if is_exported_python_target(dep): create(dep)",False,if self . _recursive :,if dependencies :,0.8820016898747209,0.0
"def __add__(self, other): other = ArithmeticExpression.try_unpack_const(other) if not self.symbolic and type(other) is int: return SpOffset(self._bits, self._to_signed(self.offset + other)) else: <mask>: return SpOffset(self._bits, self.offset + other) else: return SpOffset( self._bits, ArithmeticExpression( ArithmeticExpression.Add, ( self.offset, other, ), ), )",False,if self . symbolic :,if type ( other ) is int :,0.8820016898747209,6.567274736060395
"def check_connection(conn): tables = [ r[0] for r in conn.execute( ""select name from sqlite_master where type='table'"" ).fetchall() ] for table in tables: try: conn.execute( f""PRAGMA table_info({escape_sqlite(table)});"", ) except sqlite3.OperationalError as e: <mask>: raise SpatialiteConnectionProblem(e) else: raise ConnectionProblem(e)",False,"if e . args [ 0 ] == ""no such module: VirtualSpatialIndex"" :",if e . errno == errno . EEXIST :,0.8820016898747209,10.208145602370278
"def _get_github_client(self) -> ""Github"": from github import Github if self.access_token_secret is not None: # If access token secret specified, load it access_token = Secret(self.access_token_secret).get() else: # Otherwise, fallback to loading from local secret or environment variable access_token = prefect.context.get(""secrets"", {}).get(""GITHUB_ACCESS_TOKEN"") <mask>: access_token = os.getenv(""GITHUB_ACCESS_TOKEN"") return Github(access_token)",True,if access_token is None :,if access_token is None :,0.8820016898747209,100.00000000000004
"def make_tab(lists): if hasattr(lists, ""tolist""): lists = lists.tolist() ut = [] for rad in lists: <mask>: ut.append(""\t"".join([""%s"" % x for x in rad])) else: ut.append(""%s"" % rad) return ""\n"".join(ut)",False,"if type ( rad ) in [ list , tuple ] :","if isinstance ( rad , list ) :",0.8820016898747209,9.1627840649916
"def _ensure_ffi_initialized(cls): with cls._init_lock: <mask>: cls.lib = build_conditional_library(lib, CONDITIONAL_NAMES) cls._lib_loaded = True # initialize the SSL library cls.lib.SSL_library_init() # adds all ciphers/digests for EVP cls.lib.OpenSSL_add_all_algorithms() # loads error strings for libcrypto and libssl functions cls.lib.SSL_load_error_strings() cls._register_osrandom_engine()",False,if not cls . _lib_loaded :,if not cls . lib :,0.8820016898747209,34.1077254951379
def writer_leaves(self): self.mutex.acquire() try: self.active_writers -= 1 if self.waiting_writers != 0: self.active_writers += 1 self.waiting_writers -= 1 self.can_write.release() <mask>: t = self.waiting_readers self.waiting_readers = 0 self.active_readers += t while t > 0: self.can_read.release() t -= 1 finally: self.mutex.release(),False,elif self . waiting_readers != 0 :,if self . waiting_readers != 0 :,0.8820016898747209,88.01117367933934
"def _spans(self, operands): spans = {} k = 0 j = 0 for mode in (self.FLOAT, self.MPMATH): for i, operand in enumerate(operands[k:]): if operand[0] > mode: break j = i + k + 1 <mask>: # only init state? then ignore. j = 0 spans[mode] = slice(k, j) k = j spans[self.SYMBOLIC] = slice(k, len(operands)) return spans",False,if k == 0 and j == 1 :,if j == 0 :,0.8820016898747209,20.55088044916909
"def _report_error(self, completion_routine, response=None, message=None): if response: # Only include the text in case of error. <mask>: status = location.Status(response.status_code, response.text) else: status = location.Status(response.status_code) else: status = location.Status(500, message) if response is None or not response.ok: if completion_routine: return completion_routine(status) raise IOError(response.text) else: if completion_routine: completion_routine(status) return location.Status(200, response.content)",False,if not response . ok :,if response . text :,0.8820016898747209,20.80119537801062
"def readinto(self, buf): if self.current_frame: n = self.current_frame.readinto(buf) if n == 0 and len(buf) != 0: self.current_frame = None n = len(buf) buf[:] = self.file_read(n) return n <mask>: raise UnpicklingError(""pickle exhausted before end of frame"") return n else: n = len(buf) buf[:] = self.file_read(n) return n",False,if n < len ( buf ) :,if n == 0 :,0.8820016898747209,12.872632311973014
"def __getitem__(self, name, set=set, getattr=getattr, id=id): visited = set() mydict = self.basedict while 1: value = mydict[name] <mask>: return value myid = id(mydict) assert myid not in visited visited.add(myid) mydict = mydict.Parent if mydict is None: return",True,if value is not None :,if value is not None :,0.8820016898747209,100.00000000000004
"def _handle_Mul(self, expr): arg0, arg1 = expr.args expr_0 = self._expr(arg0) if expr_0 is None: return None expr_1 = self._expr(arg1) if expr_1 is None: return None try: <mask>: # self.tyenv is not used mask = (1 << expr.result_size(self.tyenv)) - 1 return (expr_0 * expr_1) & mask else: return expr_0 * expr_1 except TypeError as e: self.l.warning(e) return None",False,"if isinstance ( expr_0 , int ) and isinstance ( expr_1 , int ) :",if self . tyenv is not None :,0.8820016898747209,1.6604670898042333
"def end_request(self, request_id): """"""Removes the information associated with given request_id."""""" with self._lock: del self._request_wsgi_environ[request_id] del self._request_id_to_server_configuration[request_id] <mask>: del self._request_id_to_instance[request_id]",True,if request_id in self . _request_id_to_instance :,if request_id in self . _request_id_to_instance :,0.8820016898747209,100.00000000000004
def generate(): <mask>: decoder = zlib.decompressobj(16 + zlib.MAX_WBITS) while True: chunk = self.raw.read(chunk_size) if not chunk: break if self._gzipped: chunk = decoder.decompress(chunk) yield chunk,True,if self . _gzipped :,if self . _gzipped :,0.8820016898747209,100.00000000000004
"def handle(self): from poetry.utils.env import EnvManager manager = EnvManager(self.poetry) current_env = manager.get() for venv in manager.list(): name = venv.path.name <mask>: name = str(venv.path) if venv == current_env: self.line(""<info>{} (Activated)</info>"".format(name)) continue self.line(name)",False,"if self . option ( ""full-path"" ) :","if name == ""activate"" :",0.8820016898747209,6.082317172853824
"def addAggregators(sheet, cols, aggrnames): ""Add each aggregator in list of *aggrnames* to each of *cols*."" for aggrname in aggrnames: aggrs = vd.aggregators.get(aggrname) aggrs = aggrs if isinstance(aggrs, list) else [aggrs] for aggr in aggrs: for c in cols: <mask>: c.aggregators = [] if aggr and aggr not in c.aggregators: c.aggregators += [aggr]",False,"if not hasattr ( c , ""aggregators"" ) :",if not c . aggregations :,0.8820016898747209,8.389861810900507
"def on_pre_output_coercion( directive_args: Dict[str, Any], next_directive: Callable, value: Any, ctx: Optional[Any], info: ""ResolveInfo"", ): value = await next_directive(value, ctx, info) if value is None: return value try: py_enum = _ENUM_MAP[directive_args[""name""]] <mask>: return [None if item is None else py_enum(item).name for item in value] return py_enum(value).name except Exception: pass return value",True,"if isinstance ( value , list ) :","if isinstance ( value , list ) :",0.8820016898747209,100.00000000000004
def cut(sentence): sentence = strdecode(sentence) blocks = re_han.split(sentence) for blk in blocks: <mask>: for word in __cut(blk): if word not in Force_Split_Words: yield word else: for c in word: yield c else: tmp = re_skip.split(blk) for x in tmp: if x: yield x,False,if re_han . match ( blk ) :,if blk in Force_Split_Words :,0.8820016898747209,6.033504141761816
"def refresh_archive_action(self): archive_name = self.selected_archive_name() if archive_name is not None: params = BorgInfoArchiveThread.prepare(self.profile(), archive_name) <mask>: thread = BorgInfoArchiveThread(params[""cmd""], params, parent=self.app) thread.updated.connect(self._set_status) thread.result.connect(self.refresh_archive_result) self._toggle_all_buttons(False) thread.start()",False,"if params [ ""ok"" ] :","if params [ ""cmd"" ] is not None :",0.8820016898747209,28.997844147152072
"def get_resource_public_actions(resource_class): resource_class_members = inspect.getmembers(resource_class) resource_methods = {} for name, member in resource_class_members: if not name.startswith(""_""): if not name[0].isupper(): if not name.startswith(""wait_until""): <mask>: resource_methods[name] = member return resource_methods",False,if is_resource_action ( member ) :,if member . is_public :,0.8820016898747209,10.759051250985632
"def _get_compressor(compress_type, compresslevel=None): if compress_type == ZIP_DEFLATED: <mask>: return zlib.compressobj(compresslevel, zlib.DEFLATED, -15) return zlib.compressobj(zlib.Z_DEFAULT_COMPRESSION, zlib.DEFLATED, -15) elif compress_type == ZIP_BZIP2: if compresslevel is not None: return bz2.BZ2Compressor(compresslevel) return bz2.BZ2Compressor() # compresslevel is ignored for ZIP_LZMA elif compress_type == ZIP_LZMA: return LZMACompressor() else: return None",True,if compresslevel is not None :,if compresslevel is not None :,0.8820016898747209,100.00000000000004
"def parse_header(plyfile, ext): # Variables line = [] properties = [] num_points = None while b""end_header"" not in line and line != b"""": line = plyfile.readline() if b""element"" in line: line = line.split() num_points = int(line[2]) <mask>: line = line.split() properties.append((line[2].decode(), ext + ply_dtypes[line[1]])) return num_points, properties",False,"elif b""property"" in line :","elif b""properties"" in line :",0.8820016898747209,50.000000000000014
"def download_release_artifacts(self, version): try: os.mkdir(self.artifacts_dir) except FileExistsError: pass for job_name in self.build_ids: build_number = self.build_ids.get(job_name) build_status = self._get_build_status(job_name, build_number) <mask>: self._download_job_artifact(job_name, build_number, version) else: print(""Build for {} is not fininished"".format(job_name)) print(""\tRun 'build' action to check status of {}"".format(job_name))",False,"if build_status == ""built"" :","if build_status == ""fininished"" :",0.8820016898747209,70.71067811865478
"def update_metadata(self): for attrname in dir(self): <mask>: continue attrvalue = getattr(self, attrname, None) if attrvalue == 0: continue if attrname == ""salt_version"": attrname = ""version"" if hasattr(self.metadata, ""set_{0}"".format(attrname)): getattr(self.metadata, ""set_{0}"".format(attrname))(attrvalue) elif hasattr(self.metadata, attrname): try: setattr(self.metadata, attrname, attrvalue) except AttributeError: pass",False,"if attrname . startswith ( ""__"" ) :","if attrname . startswith ( ""_"" ) :",0.8820016898747209,80.45268749630647
"def check_heuristic_in_sql(): heurs = set() excluded = [""Equal assembly or pseudo-code"", ""All or most attributes""] for heur in HEURISTICS: name = heur[""name""] if name in excluded: continue sql = heur[""sql""] <mask>: print((""SQL command not correctly associated to %s"" % repr(name))) print(sql) assert sql.find(name) != -1 heurs.add(name) print(""Heuristics:"") import pprint pprint.pprint(heurs)",False,if sql . lower ( ) . find ( name . lower ( ) ) == - 1 :,if not sql :,0.8820016898747209,0.38503887711545237
def gettext(rv): for child in rv.childNodes: <mask>: yield child.nodeValue if child.nodeType == child.ELEMENT_NODE: for item in gettext(child): yield item,True,if child . nodeType == child . TEXT_NODE :,if child . nodeType == child . TEXT_NODE :,0.8820016898747209,100.00000000000004
"def update(self): """"""Update properties over dbus."""""" self._check_dbus() _LOGGER.info(""Updating service information"") self._services.clear() try: systemd_units = await self.sys_dbus.systemd.list_units() for service_data in systemd_units[0]: <mask>: continue self._services.add(ServiceInfo.read_from(service_data)) except (HassioError, IndexError): _LOGGER.warning(""Can't update host service information!"")",False,"if not service_data [ 0 ] . endswith ( "".service"" ) or service_data [ 2 ] != ""loaded"" :","if service_data == ""host"" :",0.8820016898747209,3.550281962116935
"def filtercomments(source): """"""NOT USED: strips trailing comments and put them at the top."""""" trailing_comments = [] comment = True while comment: <mask>: comment = source[0, source.index(""*/"") + 2] elif re.search(r""^\s*\/\/"", source): comment = re.search(r""^\s*\/\/"", source).group(0) else: comment = None if comment: source = re.sub(r""^\s+"", """", source[len(comment) :]) trailing_comments.append(comment) return ""\n"".join(trailing_comments) + source",False,"if re . search ( r""^\s*\/\*"" , source ) :","if re . search ( r""^\s*/"" , source ) :",0.8820016898747209,70.48590795061648
"def _getSourceStamp_sync(self, ssid): if ssid in self.sourcestamps: ssdict = self.sourcestamps[ssid].copy() ssdict[""ssid""] = ssid patchid = ssdict[""patchid""] <mask>: ssdict.update(self.patches[patchid]) ssdict[""patchid""] = patchid else: ssdict[""patch_body""] = None ssdict[""patch_level""] = None ssdict[""patch_subdir""] = None ssdict[""patch_author""] = None ssdict[""patch_comment""] = None return ssdict else: return None",False,if patchid :,if patchid in self . patches :,0.8820016898747209,14.535768424205482
"def parseImpl(self, instring, loc, doActions=True): try: loc, tokens = self.expr._parse(instring, loc, doActions, callPreParse=False) except (ParseException, IndexError): if self.defaultValue is not self.__optionalNotMatched: <mask>: tokens = ParseResults([self.defaultValue]) tokens[self.expr.resultsName] = self.defaultValue else: tokens = [self.defaultValue] else: tokens = [] return loc, tokens",True,if self . expr . resultsName :,if self . expr . resultsName :,0.8820016898747209,100.00000000000004
"def _find_exceptions(): for _name, obj in iteritems(globals()): try: is_http_exception = issubclass(obj, HTTPException) except TypeError: is_http_exception = False if not is_http_exception or obj.code is None: continue __all__.append(obj.__name__) old_obj = default_exceptions.get(obj.code, None) <mask>: continue default_exceptions[obj.code] = obj",False,"if old_obj is not None and issubclass ( obj , old_obj ) :",if old_obj is not None :,0.8820016898747209,28.22664073782293
"def generator(self, data): for (proc_as, key_buf_ptr) in data: key_buf = proc_as.read(key_buf_ptr, 24) <mask>: continue key = """".join(""%02X"" % ord(k) for k in key_buf) yield ( 0, [ str(key), ], )",True,if not key_buf :,if not key_buf :,0.8820016898747209,100.00000000000004
"def calculateEnableMargins(self): self.cnc.resetEnableMargins() for block in self.blocks: <mask>: CNC.vars[""xmin""] = min(CNC.vars[""xmin""], block.xmin) CNC.vars[""ymin""] = min(CNC.vars[""ymin""], block.ymin) CNC.vars[""zmin""] = min(CNC.vars[""zmin""], block.zmin) CNC.vars[""xmax""] = max(CNC.vars[""xmax""], block.xmax) CNC.vars[""ymax""] = max(CNC.vars[""ymax""], block.ymax) CNC.vars[""zmax""] = max(CNC.vars[""zmax""], block.zmax)",True,if block . enable :,if block . enable :,0.8820016898747209,100.00000000000004
"def __init__(self, client, job_id, callback=None): self.client = client self.job_id = job_id # If a job event has been received already then we must set an Event # to wait for this job to finish. # Otherwise we create a new stub for the job with the Event for when # the job event arrives to use existing event. with client._jobs_lock: job = client._jobs.get(job_id) self.event = None <mask>: self.event = job.get(""__ready"") if self.event is None: self.event = job[""__ready""] = Event() job[""__callback""] = callback",False,if job :,if job is not None :,0.8820016898747209,17.965205598154213
"def asset(*paths): for path in paths: fspath = www_root + ""/assets/"" + path etag = """" try: if env.cache_static: etag = asset_etag(fspath) else: os.stat(fspath) except FileNotFoundError as e: if path == paths[-1]: <mask>: tell_sentry(e, {}) else: continue except Exception as e: tell_sentry(e, {}) return asset_url + path + (etag and ""?etag="" + etag)",False,"if not os . path . exists ( fspath + "".spt"" ) :",if env . cache_static :,0.8820016898747209,2.389389104935703
"def set_conf(): """"""Collapse all object_trail config into cherrypy.request.config."""""" base = cherrypy.config.copy() # Note that we merge the config from each node # even if that node was None. for name, obj, conf, segleft in object_trail: base.update(conf) <mask>: base[""tools.staticdir.section""] = ""/"" + ""/"".join( fullpath[0 : fullpath_len - segleft] ) return base",False,"if ""tools.staticdir.dir"" in conf :",if segleft > 0 :,0.8820016898747209,3.8261660656802645
"def __init__(self): self.setLayers(None, None) self.interface = None self.event_callbacks = {} self.__stack = None self.lock = threading.Lock() members = inspect.getmembers(self, predicate=inspect.ismethod) for m in members: <mask>: fname = m[0] fn = m[1] self.event_callbacks[fn.event_callback] = getattr(self, fname)",False,"if hasattr ( m [ 1 ] , ""event_callback"" ) :","if isinstance ( m , tuple ) :",0.8820016898747209,7.205893226533905
def multi_dev_generator(self): for data in self._data_loader(): <mask>: self._tail_data += data if len(self._tail_data) == self._base_number: yield self._tail_data self._tail_data = [],False,if len ( self . _tail_data ) < self . _base_number :,if data is not None :,0.8820016898747209,1.4456752008489673
"def replace_field_to_value(layout, cb): for i, lo in enumerate(layout.fields): if isinstance(lo, Field) or issubclass(lo.__class__, Field): layout.fields[i] = ShowField( cb, *lo.fields, attrs=lo.attrs, wrapper_class=lo.wrapper_class ) elif isinstance(lo, basestring): layout.fields[i] = ShowField(cb, lo) <mask>: replace_field_to_value(lo, cb)",False,"elif hasattr ( lo , ""get_field_names"" ) :","elif isinstance ( lo , ( int , long ) ) :",0.8820016898747209,14.043459416399545
"def function_out(*args, **kwargs): try: return function_in(*args, **kwargs) except dbus.exceptions.DBusException as e: if e.get_dbus_name() == DBUS_UNKNOWN_METHOD: raise ItemNotFoundException(""Item does not exist!"") if e.get_dbus_name() == DBUS_NO_SUCH_OBJECT: raise ItemNotFoundException(e.get_dbus_message()) <mask>: raise SecretServiceNotAvailableException(e.get_dbus_message()) raise",False,"if e . get_dbus_name ( ) in ( DBUS_NO_REPLY , DBUS_NOT_SUPPORTED ) :",if e . get_dbus_name ( ) == DBUS_NO_SUCH_SECRET_SERVICE :,0.8820016898747209,48.17832209407104
"def results_iter(self): if self.connection.ops.oracle: from django.db.models.fields import DateTimeField fields = [DateTimeField()] else: needs_string_cast = self.connection.features.needs_datetime_string_cast offset = len(self.query.extra_select) for rows in self.execute_sql(MULTI): for row in rows: date = row[offset] if self.connection.ops.oracle: date = self.resolve_columns(row, fields)[offset] <mask>: date = typecast_timestamp(str(date)) yield date",False,elif needs_string_cast :,if needs_string_cast :,0.8820016898747209,80.91067115702207
"def handle_label(self, path, **options): verbosity = int(options.get(""verbosity"", 1)) result = finders.find(path, all=options[""all""]) path = smart_unicode(path) if result: if not isinstance(result, (list, tuple)): result = [result] output = u""\n "".join( (smart_unicode(os.path.realpath(path)) for path in result) ) self.stdout.write(smart_str(u""Found '%s' here:\n %s\n"" % (path, output))) else: <mask>: self.stderr.write(smart_str(""No matching file found for '%s'.\n"" % path))",False,if verbosity >= 1 :,if verbosity == 0 :,0.8820016898747209,19.304869754804482
"def name(self): """"""Get the enumeration name of this storage class."""""" if self._name_map is None: self._name_map = {} for key, value in list(StorageClass.__dict__.items()): <mask>: self._name_map[value] = key return self._name_map[self]",True,"if isinstance ( value , StorageClass ) :","if isinstance ( value , StorageClass ) :",0.8820016898747209,100.00000000000004
"def index(self, value): if self._growing: if self._start <= value < self._stop: q, r = divmod(value - self._start, self._step) if r == self._zero: return int(q) else: <mask>: q, r = divmod(self._start - value, -self._step) if r == self._zero: return int(q) raise ValueError(""{} is not in numeric range"".format(value))",False,if self . _start >= value > self . _stop :,if self . _start >= value < self . _stop :,0.8820016898747209,78.25422900366432
"def extract_cookie(cookie_header, cookie_name): inx = cookie_header.find(cookie_name) if inx >= 0: end_inx = cookie_header.find("";"", inx) <mask>: value = cookie_header[inx:end_inx] else: value = cookie_header[inx:] return value return """"",False,if end_inx > 0 :,if end_inx >= 0 :,0.8820016898747209,59.4603557501361
"def get_size(self, shape_info): # The size is the data, that have constant size. state = np.random.RandomState().get_state() size = 0 for elem in state: if isinstance(elem, str): size += len(elem) <mask>: size += elem.size * elem.itemsize elif isinstance(elem, int): size += np.dtype(""int"").itemsize elif isinstance(elem, float): size += np.dtype(""float"").itemsize else: raise NotImplementedError() return size",True,"elif isinstance ( elem , np . ndarray ) :","elif isinstance ( elem , np . ndarray ) :",0.8820016898747209,100.00000000000004
"def createFields(self): size = self.size / 8 if size > 2: <mask>: yield UInt8(self, ""cs"", ""10ms units, values from 0 to 199"") yield Bits(self, ""2sec"", 5, ""seconds/2"") yield Bits(self, ""min"", 6, ""minutes"") yield Bits(self, ""hour"", 5, ""hours"") yield Bits(self, ""day"", 5, ""(1-31)"") yield Bits(self, ""month"", 4, ""(1-12)"") yield Bits(self, ""year"", 7, ""(0 = 1980, 127 = 2107)"")",False,if size > 4 :,if size % 2 == 0 :,0.8820016898747209,12.22307556087252
"def detect(get_page): retval = False for vector in WAF_ATTACK_VECTORS: page, headers, code = get_page(get=vector) retval = ( re.search( r""incap_ses|visid_incap"", headers.get(HTTP_HEADER.SET_COOKIE, """"), re.I ) is not None ) retval |= re.search(r""Incapsula"", headers.get(""X-CDN"", """"), re.I) is not None <mask>: break return retval",True,if retval :,if retval :,0.8820016898747209,0.0
"def _get_order_information(self, node_id, timeout=1200, check_interval=5): mask = { ""billingItem"": """", ""powerState"": """", ""operatingSystem"": {""passwords"": """"}, ""provisionDate"": """", } for i in range(0, timeout, check_interval): res = self.connection.request( ""SoftLayer_Virtual_Guest"", ""getObject"", id=node_id, object_mask=mask ).object <mask>: return res time.sleep(check_interval) raise SoftLayerException(""Timeout on getting node details"")",False,"if res . get ( ""provisionDate"" , None ) :",if res :,0.8820016898747209,0.0
"def _process_param_change(self, msg): msg = super(Select, self)._process_param_change(msg) labels, values = self.labels, self.values if ""value"" in msg: msg[""value""] = [ labels[indexOf(v, values)] for v in msg[""value""] if isIn(v, values) ] if ""options"" in msg: msg[""options""] = labels <mask>: self.value = [v for v in self.value if isIn(v, values)] return msg",False,"if any ( not isIn ( v , values ) for v in self . value ) :",if len ( self . value ) > 0 :,0.8820016898747209,13.575914775035756
"def get_object_from_name(self, name, check_symlinks=True): if not name: return None name = name.rstrip(""\\"") for a, o in self.objects.items(): if not o.name: continue if o.name.lower() == name.lower(): return o if check_symlinks: m = [sl[1] for sl in self.symlinks if name.lower() == sl[0].lower()] <mask>: name = m[0] return self.get_object_from_name(name, False)",False,if m :,if len ( m ) == 1 :,0.8820016898747209,6.27465531099474
"def run(self): for k, v in iteritems(self.objs): <mask>: continue if v[""_class""] == ""User"": if v[""email""] == """": v[""email""] = None if v[""ip""] == ""0.0.0.0"": v[""ip""] = None return self.objs",False,"if k . startswith ( ""_"" ) :","if k == ""id"" :",0.8820016898747209,10.816059393812111
"def _providers(self, descriptor): res = [] for _md in self.metadata.values(): for ent_id, ent_desc in _md.items(): if descriptor in ent_desc: <mask>: # print(""duplicated entity_id: %s"" % res) pass else: res.append(ent_id) return res",True,if ent_id in res :,if ent_id in res :,0.8820016898747209,100.00000000000004
"def test_add_participant(self): async with self.chat_client: await self._create_thread() async with self.chat_thread_client: share_history_time = datetime.utcnow() share_history_time = share_history_time.replace(tzinfo=TZ_UTC) new_participant = ChatThreadParticipant( user=self.new_user, display_name=""name"", share_history_time=share_history_time, ) await self.chat_thread_client.add_participant(new_participant) <mask>: await self.chat_client.delete_chat_thread(self.thread_id)",False,if not self . is_playback ( ) :,if self . thread_id :,0.8820016898747209,10.759051250985632
"def url(regex, view, kwargs=None, name=None, prefix=""""): if isinstance(view, (list, tuple)): # For include(...) processing. urlconf_module, app_name, namespace = view return RegexURLResolver( regex, urlconf_module, kwargs, app_name=app_name, namespace=namespace ) else: if isinstance(view, basestring): if not view: raise ImproperlyConfigured( ""Empty URL pattern view name not permitted (for pattern %r)"" % regex ) <mask>: view = prefix + ""."" + view return RegexURLPattern(regex, view, kwargs, name)",True,if prefix :,if prefix :,0.8820016898747209,0.0
"def tx(): # Sync receiver ready to avoid loss of first packets while not sub_ready.ready(): pub.send(b""test BEGIN"") eventlet.sleep(0.005) for i in range(1, 101): msg = ""test {0}"".format(i).encode() <mask>: pub.send(msg) else: pub.send(b""test LAST"") sub_last.wait() # XXX: putting a real delay of 1ms here fixes sporadic failures on Travis # just yield eventlet.sleep(0) doesn't cut it eventlet.sleep(0.001) pub.send(b""done DONE"")",False,if i != 50 :,if pub . send ( msg ) :,0.8820016898747209,6.567274736060395
"def remove_tmp_snapshot_file(self, files): for filepath in files: path = Path(filepath) if path.is_dir() and path.exists(): shutil.rmtree(path) <mask>: path.unlink()",False,elif path . is_file ( ) and path . exists ( ) :,if path . is_file ( ) :,0.8820016898747209,41.920332227987466
"def f(view, s): if mode == modes.INTERNAL_NORMAL: if count == 1: <mask>: eol = view.line(s.b).b return R(s.b, eol) return s return s",False,if view . line ( s . b ) . size ( ) > 0 :,if s . b . isspace ( ) :,0.8820016898747209,11.530954747180651
"def get_ids(self, **kwargs): id = [] if ""id"" in kwargs: id = kwargs[""id""] # Coerce ids to list <mask>: id = id.split("","") # Ensure ids are integers try: id = list(map(int, id)) except Exception: decorators.error(""Invalid id"") return id",False,"if not isinstance ( id , list ) :","if "","" in id :",0.8820016898747209,6.979367151952678
"def param_value(self): # This is part of the ""handle quoted extended parameters"" hack. for token in self: <mask>: return token.stripped_value if token.token_type == ""quoted-string"": for token in token: if token.token_type == ""bare-quoted-string"": for token in token: if token.token_type == ""value"": return token.stripped_value return """"",False,"if token . token_type == ""value"" :","if token . token_type == ""quoted-string"" :",0.8820016898747209,76.91605673134588
"def get_all_start_methods(self): if sys.platform == ""win32"": return [""spawn""] else: methods = [""spawn"", ""fork""] if sys.platform == ""darwin"" else [""fork"", ""spawn""] <mask>: methods.append(""forkserver"") return methods",False,if reduction . HAVE_SEND_HANDLE :,"if sys . platform == ""win32"" :",0.8820016898747209,5.522397783539471
"def _process_watch(self, watched_event): logger.debug(""process_watch: %r"", watched_event) with handle_exception(self._tree._error_listeners): <mask>: assert self._parent is None, ""unexpected CREATED on non-root"" self.on_created() elif watched_event.type == EventType.DELETED: self.on_deleted() elif watched_event.type == EventType.CHANGED: self._refresh_data() elif watched_event.type == EventType.CHILD: self._refresh_children()",True,if watched_event . type == EventType . CREATED :,if watched_event . type == EventType . CREATED :,0.8820016898747209,100.00000000000004
"def assert_open(self, sock, *rest): if isinstance(sock, fd_types): self.__assert_fd_open(sock) else: fileno = sock.fileno() assert isinstance(fileno, fd_types), fileno sockname = sock.getsockname() assert isinstance(sockname, tuple), sockname <mask>: self.__assert_fd_open(fileno) else: self._assert_sock_open(sock) if rest: self.assert_open(rest[0], *rest[1:])",False,if not WIN :,if fileno :,0.8820016898747209,0.0
"def detype(self): """"""De-types the instance, allowing it to be exported to the environment."""""" style = self.style if self._detyped is None: self._detyped = "":"".join( [ key + ""="" + "";"".join( [ LsColors.target_value <mask>: else ansi_color_name_to_escape_code(v, cmap=style) for v in val ] ) for key, val in sorted(self._d.items()) ] ) return self._detyped",False,if key in self . _targets,if val is None,0.8820016898747209,7.545383788761362
"def gather_metrics(dry_run=False): today = datetime.date.today() first = today.replace(day=1) last_month = first - datetime.timedelta(days=1) filename = ""form_types_{}.csv"".format(last_month.strftime(""%Y-%m"")) with connection.cursor() as cursor: cursor.execute(REGISTRATION_METRICS_SQL) <mask>: for row in cursor.fetchall(): logger.info(encode_row(row)) else: write_raw_data(cursor=cursor, filename=filename)",True,if dry_run :,if dry_run :,0.8820016898747209,100.00000000000004
"def cat(tensors, dim=0): assert isinstance(tensors, list), ""input to cat must be a list"" if len(tensors) == 1: return tensors[0] from .autograd_cryptensor import AutogradCrypTensor if any(isinstance(t, AutogradCrypTensor) for t in tensors): <mask>: tensors[0] = AutogradCrypTensor(tensors[0], requires_grad=False) return tensors[0].cat(*tensors[1:], dim=dim) else: return get_default_backend().cat(tensors, dim=dim)",False,"if not isinstance ( tensors [ 0 ] , AutogradCrypTensor ) :",if len ( tensors ) == 1 :,0.8820016898747209,8.591316733350183
"def is_installed(self, dlc_title="""") -> bool: installed = False if dlc_title: dlc_version = self.get_dlc_info(""version"", dlc_title) installed = True if dlc_version else False # Start: Code for compatibility with minigalaxy 1.0 if not installed: status = self.legacy_get_dlc_status(dlc_title) installed = True if status in [""installed"", ""updatable""] else False # End: Code for compatibility with minigalaxy 1.0 else: <mask>: installed = True return installed",False,if self . install_dir and os . path . exists ( self . install_dir ) :,"if self . legacy_get_dlc_status ( dlc_title ) == ""installed"" :",0.8820016898747209,8.638804535733374
"def on_copy(self): source_objects = self.__getSelection() for source in source_objects: <mask>: new_obj = model.Phrase("""", """") else: new_obj = model.Script("""", """") new_obj.copy(source) self.cutCopiedItems.append(new_obj)",False,"if isinstance ( source , model . Phrase ) :",if source . is_text :,0.8820016898747209,6.050259138270144
"def FetchFn(type_name): """"""Fetches all hunt results of a given type."""""" offset = 0 while True: results = data_store.REL_DB.ReadHuntResults( hunt_id, offset=offset, count=self._RESULTS_PAGE_SIZE, with_type=type_name ) <mask>: break for r in results: msg = r.AsLegacyGrrMessage() msg.source_urn = source_urn yield msg offset += self._RESULTS_PAGE_SIZE",True,if not results :,if not results :,0.8820016898747209,100.00000000000004
"def get_blob_type_declaration_sql(self, column): length = column.get(""length"") if length: <mask>: return ""TINYBLOB"" if length <= self.LENGTH_LIMIT_BLOB: return ""BLOB"" if length <= self.LENGTH_LIMIT_MEDIUMBLOB: return ""MEDIUMBLOB"" return ""LONGBLOB""",True,if length <= self . LENGTH_LIMIT_TINYBLOB :,if length <= self . LENGTH_LIMIT_TINYBLOB :,0.8820016898747209,100.00000000000004
"def decode(cls, data): while data: ( length, atype, ) = unpack(cls.Header.PACK, data[: cls.Header.LEN]) <mask>: raise AttributesError(""Buffer underrun %d < %d"" % (len(data), length)) payload = data[cls.Header.LEN : length] yield atype, payload data = data[int((length + 3) / 4) * 4 :]",False,if len ( data ) < length :,if length < cls . Header . LENGTH :,0.8820016898747209,6.742555929751843
"def test_join_diffs(db, series_of_diffs, expected): diffs = [] for changes in series_of_diffs: tracker = DBDiffTracker() for key, val in changes.items(): <mask>: del tracker[key] else: tracker[key] = val diffs.append(tracker.diff()) DBDiff.join(diffs).apply_to(db) assert db == expected",True,if val is None :,if val is None :,0.8820016898747209,100.00000000000004
"def ant_map(m): tmp = ""rows %s\ncols %s\n"" % (len(m), len(m[0])) players = {} for row in m: tmp += ""m "" for col in row: if col == LAND: tmp += ""."" elif col == BARRIER: tmp += ""%"" elif col == FOOD: tmp += ""*"" <mask>: tmp += ""?"" else: players[col] = True tmp += chr(col + 97) tmp += ""\n"" tmp = (""players %s\n"" % len(players)) + tmp return tmp",False,elif col == UNSEEN :,elif col == SAND :,0.8820016898747209,53.7284965911771
"def _report_error(self, completion_routine, response=None, message=None): if response: # Only include the text in case of error. if not response.ok: status = location.Status(response.status_code, response.text) else: status = location.Status(response.status_code) else: status = location.Status(500, message) if response is None or not response.ok: <mask>: return completion_routine(status) raise IOError(response.text) else: if completion_routine: completion_routine(status) return location.Status(200, response.content)",True,if completion_routine :,if completion_routine :,0.8820016898747209,100.00000000000004
"def _generate_examples(self, src_path=None, tgt_path=None, replace_unk=None): """"""Yields examples."""""" with tf.io.gfile.GFile(src_path) as f_d, tf.io.gfile.GFile(tgt_path) as f_s: for i, (doc_text, sum_text) in enumerate(zip(f_d, f_s)): <mask>: yield i, { _DOCUMENT: doc_text.strip().replace(""<unk>"", ""UNK""), _SUMMARY: sum_text.strip().replace(""<unk>"", ""UNK""), } else: yield i, {_DOCUMENT: doc_text.strip(), _SUMMARY: sum_text.strip()}",True,if replace_unk :,if replace_unk :,0.8820016898747209,100.00000000000004
"def escape(text, newline=False): """"""Escape special html characters."""""" if isinstance(text, str): if ""&"" in text: text = text.replace(""&"", ""&amp;"") if "">"" in text: text = text.replace("">"", ""&gt;"") if ""<"" in text: text = text.replace(""<"", ""&lt;"") if '""' in text: text = text.replace('""', ""&quot;"") if ""'"" in text: text = text.replace(""'"", ""&quot;"") <mask>: if ""\n"" in text: text = text.replace(""\n"", ""<br>"") return text",True,if newline :,if newline :,0.8820016898747209,0.0
"def _handle_url_click(self, event): url = _extract_click_text(self.info_text, event, ""url"") if url is not None: <mask>: import webbrowser webbrowser.open(url) elif os.path.sep in url: os.makedirs(url, exist_ok=True) open_path_in_system_file_manager(url) else: self._start_show_package_info(url)",False,"if url . startswith ( ""http:"" ) or url . startswith ( ""https:"" ) :",if os . path . isfile ( url ) :,0.8820016898747209,3.820942032434038
"def SConsignFile(self, name="".sconsign"", dbm_module=None): if name is not None: name = self.subst(name) <mask>: name = os.path.join(str(self.fs.SConstruct_dir), name) if name: name = os.path.normpath(name) sconsign_dir = os.path.dirname(name) if sconsign_dir and not os.path.exists(sconsign_dir): self.Execute(SCons.Defaults.Mkdir(sconsign_dir)) SCons.SConsign.File(name, dbm_module)",False,if not os . path . isabs ( name ) :,if self . fs . SConstruct_dir is not None :,0.8820016898747209,5.063996506781411
"def on_train_start(self, trainer: Trainer, pl_module: LightningModule) -> None: super().on_train_start(trainer, pl_module) submodule_dict = dict(pl_module.named_modules()) self._hook_handles = [] for name in self._get_submodule_names(pl_module): <mask>: rank_zero_warn( f""{name} is not a valid identifier for a submodule in {pl_module.__class__.__name__},"" "" skipping this key."" ) continue handle = self._register_hook(name, submodule_dict[name]) self._hook_handles.append(handle)",True,if name not in submodule_dict :,if name not in submodule_dict :,0.8820016898747209,100.00000000000004
"def validate_configuration(self, configuration: Optional[ExpectationConfiguration]): super().validate_configuration(configuration) if configuration is None: configuration = self.configuration try: assert ""value_set"" in configuration.kwargs, ""value_set is required"" assert isinstance( configuration.kwargs[""value_set""], (list, set, dict) ), ""value_set must be a list or a set"" <mask>: assert ( ""$PARAMETER"" in configuration.kwargs[""value_set""] ), 'Evaluation Parameter dict for value_set kwarg must have ""$PARAMETER"" key.' except AssertionError as e: raise InvalidExpectationConfigurationError(str(e)) return True",False,"if isinstance ( configuration . kwargs [ ""value_set"" ] , dict ) :","if ""value_set"" in configuration . kwargs :",0.8820016898747209,28.75683693213116
"def check_refcounts(expected, timeout=10): start = time.time() while True: try: _check_refcounts(expected) break except AssertionError as e: <mask>: raise e else: time.sleep(0.1)",True,if time . time ( ) - start > timeout :,if time . time ( ) - start > timeout :,0.8820016898747209,100.00000000000004
"def pickline(file, key, casefold=1): try: f = open(file, ""r"") except IOError: return None pat = re.escape(key) + "":"" prog = re.compile(pat, casefold and re.IGNORECASE) while 1: line = f.readline() <mask>: break if prog.match(line): text = line[len(key) + 1 :] while 1: line = f.readline() if not line or not line[0].isspace(): break text = text + line return text.strip() return None",False,if not line :,if not line or not line [ 0 ] . isspace ( ) :,0.8820016898747209,11.359354890271161
def _is_perf_file(file_path): f = get_file(file_path) for line in f: <mask>: continue r = event_regexp.search(line) if r: f.close() return True f.close() return False,False,"if line [ 0 ] == ""#"" :",if not line :,0.8820016898747209,3.6531471527995247
"def link_pantsrefs(soups, precomputed): """"""Transorm soups: <a pantsref=""foo""> becomes <a href=""../foo_page.html#foo"">"""""" for (page, soup) in soups.items(): for a in soup.find_all(""a""): <mask>: continue pantsref = a[""pantsref""] if pantsref not in precomputed.pantsref: raise TaskError( f'Page {page} has pantsref ""{pantsref}"" and I cannot find pantsmark for it' ) a[""href""] = rel_href(page, precomputed.pantsref[pantsref])",False,"if not a . has_attr ( ""pantsref"" ) :","if a [ ""href"" ] == ""../foo_page.html#foo"" :",0.8820016898747209,2.8094617486126285
"def __init__(self, querylist=None): self.query_id = -1 if querylist is None: self.querylist = [] else: self.querylist = querylist for query in self.querylist: <mask>: self.query_id = query.query_id else: if self.query_id != query.query_id: raise ValueError(""query in list must be same query_id"")",False,if self . query_id == - 1 :,"if isinstance ( query , Query ) :",0.8820016898747209,4.995138898472386
"def _draw_number( screen, x_offset, y_offset, number, token=Token.Clock, transparent=False ): ""Write number at position."" fg = Char("" "", token) bg = Char("" "", Token) for y, row in enumerate(_numbers[number]): screen_row = screen.data_buffer[y + y_offset] for x, n in enumerate(row): <mask>: screen_row[x + x_offset] = fg elif not transparent: screen_row[x + x_offset] = bg",False,"if n == ""#"" :",if n == number :,0.8820016898747209,38.49815007763549
"def init(self): self.sock.setblocking(True) if self.parser is None: # wrap the socket if needed <mask>: self.sock = ssl.wrap_socket( self.sock, server_side=True, **self.cfg.ssl_options ) # initialize the parser self.parser = http.RequestParser(self.cfg, self.sock)",False,if self . cfg . is_ssl :,if self . cfg . ssl_options :,0.8820016898747209,50.197242487957936
"def intersect_face(pt): # todo: rewrite! inefficient! nonlocal vis_faces2D for f, vs in vis_faces2D: v0 = vs[0] for v1, v2 in iter_pairs(vs[1:], False): <mask>: return f return None",False,"if intersect_point_tri_2d ( pt , v0 , v1 , v2 ) :",if v0 == v1 and v2 == pt :,0.8820016898747209,3.104147203142849
"def IMPORTFROM(self, node): if node.module == ""__future__"": if not self.futuresAllowed: self.report(messages.LateFutureImport, node, [n.name for n in node.names]) else: self.futuresAllowed = False for alias in node.names: <mask>: self.scope.importStarred = True self.report(messages.ImportStarUsed, node, node.module) continue name = alias.asname or alias.name importation = Importation(name, node) if node.module == ""__future__"": importation.used = (self.scope, node) self.addBinding(node, importation)",False,"if alias . name == ""*"" :",if alias . asname :,0.8820016898747209,15.719010513286515
"def PyObject_Bytes(obj): if type(obj) == bytes: return obj if hasattr(obj, ""__bytes__""): res = obj.__bytes__() <mask>: raise TypeError( ""__bytes__ returned non-bytes (type %s)"" % type(res).__name__ ) return PyBytes_FromObject(obj)",True,"if not isinstance ( res , bytes ) :","if not isinstance ( res , bytes ) :",0.8820016898747209,100.00000000000004
"def on_bt_search_clicked(self, widget): if self.current_provider is None: return query = self.en_query.get_text() @self.obtain_podcasts_with def load_data(): if self.current_provider.kind == directory.Provider.PROVIDER_SEARCH: return self.current_provider.on_search(query) elif self.current_provider.kind == directory.Provider.PROVIDER_URL: return self.current_provider.on_url(query) <mask>: return self.current_provider.on_file(query)",True,elif self . current_provider . kind == directory . Provider . PROVIDER_FILE :,elif self . current_provider . kind == directory . Provider . PROVIDER_FILE :,0.8820016898747209,100.00000000000004
"def remove(self, name): for s in [self.__storage(self.__category), self.__storage(None)]: for i, b in enumerate(s): <mask>: del s[i] if b.persistent: self.__save() return raise KeyError(name)",True,if b . name == name :,if b . name == name :,0.8820016898747209,100.00000000000004
"def _wrapper(data, axis=None, keepdims=False): if not keepdims: return func(data, axis=axis) else: <mask>: axis = axis if isinstance(axis, int) else axis[0] out_shape = list(data.shape) out_shape[axis] = 1 else: out_shape = [1 for _ in range(len(data.shape))] return func(data, axis=axis).reshape(out_shape)",True,if axis is not None :,if axis is not None :,0.8820016898747209,100.00000000000004
"def authn_info(self): res = [] for astat in self.assertion.authn_statement: context = astat.authn_context try: authn_instant = astat.authn_instant except AttributeError: authn_instant = """" <mask>: try: aclass = context.authn_context_class_ref.text except AttributeError: aclass = """" try: authn_auth = [a.text for a in context.authenticating_authority] except AttributeError: authn_auth = [] res.append((aclass, authn_auth, authn_instant)) return res",True,if context :,if context :,0.8820016898747209,0.0
"def _persist_metadata(self, dirname, filename): metadata_path = ""{0}/{1}.json"".format(dirname, filename) if self.media_metadata or self.comments or self.include_location: if self.posts: <mask>: self.merge_json({""GraphImages"": self.posts}, metadata_path) else: self.save_json({""GraphImages"": self.posts}, metadata_path) if self.stories: if self.latest: self.merge_json({""GraphStories"": self.stories}, metadata_path) else: self.save_json({""GraphStories"": self.stories}, metadata_path)",True,if self . latest :,if self . latest :,0.8820016898747209,100.00000000000004
"def update_record_image_detail(input_image_record, updated_image_detail, session=None): if not session: session = db.Session image_record = {} image_record.update(input_image_record) image_record.pop(""created_at"", None) image_record.pop(""last_updated"", None) if image_record[""image_type""] == ""docker"": for tag_record in updated_image_detail: <mask>: image_record[""image_detail""].append(tag_record) return update_record(image_record, session=session) return image_record",False,"if tag_record not in image_record [ ""image_detail"" ] :","if tag_record [ ""image_type"" ] == ""docker"" :",0.8820016898747209,43.85068972747104
"def backup(self): for ds in [(""activedirectory"", ""AD""), (""ldap"", ""LDAP""), (""nis"", ""NIS"")]: <mask>: try: ds_cache = self.middleware.call_sync(""cache.get"", f""{ds[1]}_cache"") with open(f""/var/db/system/.{ds[1]}_cache_backup"", ""wb"") as f: pickle.dump(ds_cache, f) except KeyError: self.logger.debug(""No cache exists for directory service [%s]."", ds[0])",False,"if ( self . middleware . call_sync ( f""{ds[0]}.config"" ) ) [ ""enable"" ] :",if ds [ 0 ] in self . cache_service_names :,0.8820016898747209,8.01412806220908
"def parse_setup_cfg(self): # type: () -> Dict[STRING_TYPE, Any] if self.setup_cfg is not None and self.setup_cfg.exists(): contents = self.setup_cfg.read_text() base_dir = self.setup_cfg.absolute().parent.as_posix() try: parsed = setuptools_parse_setup_cfg(self.setup_cfg.as_posix()) except Exception: if six.PY2: contents = self.setup_cfg.read_bytes() parsed = parse_setup_cfg(contents, base_dir) <mask>: return {} return parsed return {}",False,if not parsed :,if parsed is None :,0.8820016898747209,14.058533129758727
"def parts(): for l in lists.leaves: head_name = l.get_head_name() <mask>: yield l.leaves elif head_name != ""System`Missing"": raise MessageException(""Catenate"", ""invrp"", l)",True,"if head_name == ""System`List"" :","if head_name == ""System`List"" :",0.8820016898747209,100.00000000000004
"def _get_callback_and_order(self, hook): if callable(hook): return hook, None elif isinstance(hook, tuple) and len(hook) == 2: callback, order = hook # test that callback is a callable <mask>: raise ValueError(""Hook callback is not a callable"") # test that number is an int try: int(order) except ValueError: raise ValueError(""Hook order is not a number"") return callback, order else: raise ValueError( ""Invalid hook definition, neither a callable nor a 2-tuple (callback, order): {!r}"".format( hook ) )",True,if not callable ( callback ) :,if not callable ( callback ) :,0.8820016898747209,100.00000000000004
"def _resize_masks(self, results): """"""Resize masks with ``results['scale']``"""""" for key in results.get(""mask_fields"", []): if results[key] is None: continue <mask>: results[key] = results[key].rescale(results[""scale""]) else: results[key] = results[key].resize(results[""img_shape""][:2])",False,if self . keep_ratio :,"if ""scale"" in results [ key ] :",0.8820016898747209,4.990049701936832
"def getDataMax(self): result = -Double.MAX_VALUE nCurves = self.chart.getNCurves() for i in range(nCurves): c = self.getSystemCurve(i) if not c.isVisible(): continue <mask>: nPoints = c.getNPoints() for j in range(nPoints): result = self.maxIgnoreNaNAndMaxValue(result, c.getPoint(j).getY()) if result == -Double.MAX_VALUE: return Double.NaN return result",False,if c . getYAxis ( ) == Y_AXIS :,if c . isPointInBounds ( ) :,0.8820016898747209,15.749996500436227
"def _check_token(self): if settings.app.sso_client_cache and self.server_auth_token: doc = self.sso_client_cache_collection.find_one( { ""user_id"": self.user.id, ""server_id"": self.server.id, ""device_id"": self.device_id, ""device_name"": self.device_name, ""auth_token"": self.server_auth_token, } ) <mask>: self.has_token = True",True,if doc :,if doc :,0.8820016898747209,0.0
"def parse_header(plyfile, ext): # Variables line = [] properties = [] num_points = None while b""end_header"" not in line and line != b"""": line = plyfile.readline() <mask>: line = line.split() num_points = int(line[2]) elif b""property"" in line: line = line.split() properties.append((line[2].decode(), ext + ply_dtypes[line[1]])) return num_points, properties",False,"if b""element"" in line :","if b""num_points"" in line :",0.8820016898747209,37.99178428257963
"def __codeanalysis_settings_changed(self, current_finfo): if self.data: run_pyflakes, run_pep8 = self.pyflakes_enabled, self.pep8_enabled for finfo in self.data: self.__update_editor_margins(finfo.editor) finfo.cleanup_analysis_results() <mask>: if current_finfo is not finfo: finfo.run_code_analysis(run_pyflakes, run_pep8)",False,if ( run_pyflakes or run_pep8 ) and current_finfo is not None :,if self . __codeanalysis_settings_changed ( finfo ) :,0.8820016898747209,3.589070884428298
"def __modules(self): raw_output = self.__module_avail_output().decode(""utf-8"") for line in StringIO(raw_output): line = line and line.strip() <mask>: continue line_modules = line.split() for module in line_modules: if module.endswith(self.default_indicator): module = module[0 : -len(self.default_indicator)].strip() module_parts = module.split(""/"") module_version = None if len(module_parts) == 2: module_version = module_parts[1] module_name = module_parts[0] yield module_name, module_version",False,"if not line or line . startswith ( ""-"" ) :",if not line :,0.8820016898747209,6.734410772670761
"def _set_trailing_size(self, size): if self.is_free(): next_chunk = self.next_chunk() <mask>: self.state.memory.store(next_chunk.base, size, self.state.arch.bytes)",False,if next_chunk is not None :,if next_chunk . base != 0 :,0.8820016898747209,27.77619034011791
"def _execute_for_all_tables(self, app, bind, operation, skip_tables=False): app = self.get_app(app) if bind == ""__all__"": binds = [None] + list(app.config.get(""SQLALCHEMY_BINDS"") or ()) elif isinstance(bind, string_types) or bind is None: binds = [bind] else: binds = bind for bind in binds: extra = {} <mask>: tables = self.get_tables_for_bind(bind) extra[""tables""] = tables op = getattr(self.Model.metadata, operation) op(bind=self.get_engine(app, bind), **extra)",False,if not skip_tables :,if skip_tables :,0.8820016898747209,57.89300674674101
"def getFileName(): extension = "".json"" file = ""%s-stats"" % self.clusterName counter = 0 while True: suffix = str(counter).zfill(3) + extension fullName = os.path.join(self.statsPath, file + suffix) <mask>: return fullName counter += 1",False,if not os . path . exists ( fullName ) :,if os . path . exists ( fullName ) :,0.8820016898747209,81.76129038784515
def logic(): # direction if goRight == ACTIVE: dir.next = DirType.RIGHT run.next = True elif goLeft == ACTIVE: dir.next = DirType.LEFT run.next = True # stop if stop == ACTIVE: run.next = False # counter action if run: <mask>: q.next[4:1] = q[3:] q.next[0] = not q[3] else: q.next[3:] = q[4:1] q.next[3] = not q[0],False,if dir == DirType . LEFT :,if q [ 3 ] == 0 :,0.8820016898747209,11.339582221952005
"def test_broadcast(self): """"""Test example broadcast functionality."""""" self.create_lang_connection(""1000000000"", ""en"") self.create_lang_connection(""1000000001"", ""en"") self.create_lang_connection(""1000000002"", ""en"") self.create_lang_connection(""1000000003"", ""es"") self.create_lang_connection(""1000000004"", ""es"") app.lang_broadcast() self.assertEqual(2, len(self.outbound)) for message in self.outbound: if message.text == ""hello"": self.assertEqual(3, len(message.connections)) <mask>: self.assertEqual(2, len(message.connections))",False,"elif message . text == ""hola"" :","elif message . text == ""hello2"" :",0.8820016898747209,70.71067811865478
"def get_ovf_env(dirname): env_names = (""ovf-env.xml"", ""ovf_env.xml"", ""OVF_ENV.XML"", ""OVF-ENV.XML"") for fname in env_names: full_fn = os.path.join(dirname, fname) <mask>: try: contents = util.load_file(full_fn) return (fname, contents) except Exception: util.logexc(LOG, ""Failed loading ovf file %s"", full_fn) return (None, False)",True,if os . path . isfile ( full_fn ) :,if os . path . isfile ( full_fn ) :,0.8820016898747209,100.00000000000004
"def _calc_offsets_children(self, offset, is_last): if self.elems: elem_last = self.elems[-1] for elem in self.elems: offset = elem._calc_offsets(offset, (elem is elem_last)) offset += _BLOCK_SENTINEL_LENGTH elif not self.props or self.id in _ELEMS_ID_ALWAYS_BLOCK_SENTINEL: <mask>: offset += _BLOCK_SENTINEL_LENGTH return offset",False,if not is_last :,if is_last :,0.8820016898747209,57.89300674674101
"def publish_state(cls, payload, state): try: <mask>: if state == action_constants.LIVEACTION_STATUS_REQUESTED: cls.process(payload) else: worker.get_worker().process(payload) except Exception: traceback.print_exc() print(payload)",False,"if isinstance ( payload , LiveActionDB ) :",if state != action_constants . LIVEACTION_STATUS_PENDING :,0.8820016898747209,3.377156414337854
"def log_predictive_density(self, x_test, y_test, Y_metadata=None): if isinstance(x_test, list): x_test, y_test, ind = util.multioutput.build_XY(x_test, y_test) <mask>: Y_metadata = {""output_index"": ind, ""trials"": np.ones(ind.shape)} return super(MultioutputGP, self).log_predictive_density(x_test, y_test, Y_metadata)",True,if Y_metadata is None :,if Y_metadata is None :,0.8820016898747209,100.00000000000004
"def minimalBases(classes): """"""Reduce a list of base classes to its ordered minimum equivalent"""""" if not __python3: # pragma: no cover classes = [c for c in classes if c is not ClassType] candidates = [] for m in classes: for n in classes: <mask>: break else: # m has no subclasses in 'classes' if m in candidates: candidates.remove(m) # ensure that we're later in the list candidates.append(m) return candidates",False,"if issubclass ( n , m ) and m is not n :",if n . __name__ == m :,0.8820016898747209,4.406306339938217
"def apply(self, operations, rotations=None, **kwargs): rotations = rotations or [] # apply the circuit operations for i, operation in enumerate(operations): <mask>: raise DeviceError( ""Operation {} cannot be used after other Operations have already been applied "" ""on a {} device."".format(operation.name, self.short_name) ) for operation in operations: self._apply_operation(operation) # store the pre-rotated state self._pre_rotated_state = self._state # apply the circuit rotations for operation in rotations: self._apply_operation(operation)",False,"if i > 0 and isinstance ( operation , ( QubitStateVector , BasisState ) ) :",if i == self . _max_operations :,0.8820016898747209,5.26526142111959
"def __str__(self): txt = str(self._called) if self.call_gas or self.call_value: gas = f""gas: {self.call_gas}"" if self.call_gas else """" value = f""value: {self.call_value}"" if self.call_value else """" salt = f""salt: {self.call_salt}"" if self.call_salt else """" <mask>: options = [gas, value, salt] txt += ""{"" + "","".join([o for o in options if o != """"]) + ""}"" return txt + ""("" + "","".join([str(a) for a in self._arguments]) + "")""",False,if gas or value or salt :,if self . _arguments :,0.8820016898747209,8.170609724417774
"def pop(self): """"""Pop a nonterminal. (Internal)"""""" popdfa, popstate, popnode = self.stack.pop() newnode = self.convert(self.grammar, popnode) if newnode is not None: <mask>: dfa, state, node = self.stack[-1] node.children.append(newnode) else: self.rootnode = newnode",False,if self . stack :,"if newnode . name == ""terminal"" :",0.8820016898747209,5.522397783539471
"def pollpacket(self, wait): self._stage0() if len(self.buffer) < self.bufneed: r, w, x = select.select([self.sock.fileno()], [], [], wait) <mask>: return None try: s = self.sock.recv(BUFSIZE) except socket.error: raise EOFError if len(s) == 0: raise EOFError self.buffer += s self._stage0() return self._stage1()",False,if len ( r ) == 0 :,if r == 0 :,0.8820016898747209,34.1077254951379
"def increaseToolReach(self): if self.draggingFace is not None: d = (1, -1)[self.draggingFace & 1] <mask>: # xxxxx y d = -d self.draggingY += d x, y, z = self.editor.mainViewport.cameraPosition pos = [x, y, z] pos[self.draggingFace >> 1] += d self.editor.mainViewport.cameraPosition = tuple(pos) else: self.cloneCameraDistance = self.editor._incrementReach(self.cloneCameraDistance) return True",False,if self . draggingFace >> 1 != 1 :,if d < 0 :,0.8820016898747209,3.8261660656802645
"def selectionToChunks(self, remove=False, add=False): box = self.selectionBox() if box: if box == self.level.bounds: self.selectedChunks = set(self.level.allChunks) return selectedChunks = self.selectedChunks boxedChunks = set(box.chunkPositions) if boxedChunks.issubset(selectedChunks): remove = True <mask>: selectedChunks.difference_update(boxedChunks) else: selectedChunks.update(boxedChunks) self.selectionTool.selectNone()",False,if remove and not add :,if add :,0.8820016898747209,0.0
"def __init__(self, *args, **kwargs): super(ProjectForm, self).__init__(*args, **kwargs) if self.instance.id: <mask>: self.fields[""localfiletype""].widget.attrs[""disabled""] = True self.fields[""localfiletype""].required = False if ( self.instance.treestyle != ""auto"" and self.instance.translationproject_set.count() and self.instance.treestyle == self.instance._detect_treestyle() ): self.fields[""treestyle""].widget.attrs[""disabled""] = True self.fields[""treestyle""].required = False",False,if Store . objects . filter ( translation_project__project = self . instance ) . count ( ) :,"if self . instance . localfiletype != ""auto"" and self . instance . localfiletype == self . instance . _detect_localfiletype ( ) :",0.8820016898747209,13.121950369400196
"def _infer_return_type(*args): """"""Look at the type of all args and divine their implied return type."""""" return_type = None for arg in args: if arg is None: continue if isinstance(arg, bytes): <mask>: raise TypeError(""Can't mix bytes and non-bytes in "" ""path components."") return_type = bytes else: if return_type is bytes: raise TypeError(""Can't mix bytes and non-bytes in "" ""path components."") return_type = str if return_type is None: return str # tempfile APIs return a str by default. return return_type",False,if return_type is str :,if return_type is None :,0.8820016898747209,64.34588841607616
"def deleteDuplicates(gadgets, callback=None): toReturn = [] inst = set() count = 0 added = False len_gadgets = len(gadgets) for i, gadget in enumerate(gadgets): inst.add(gadget._gadget) if len(inst) > count: count = len(inst) toReturn.append(gadget) added = True <mask>: callback(gadget, added, float(i + 1) / (len_gadgets)) added = False return toReturn",True,if callback :,if callback :,0.8820016898747209,0.0
"def send_all(self, data: bytes): with self._conflict_detector: <mask>: raise _core.ClosedResourceError(""this pipe is already closed"") if not data: await _core.checkpoint() return try: written = await _core.write_overlapped(self._handle_holder.handle, data) except BrokenPipeError as ex: raise _core.BrokenResourceError from ex # By my reading of MSDN, this assert is guaranteed to pass so long # as the pipe isn't in nonblocking mode, but... let's just # double-check. assert written == len(data)",True,if self . _handle_holder . closed :,if self . _handle_holder . closed :,0.8820016898747209,100.00000000000004
"def setup_parameter_node(self, param_node): if param_node.bl_idname == ""SvNumberNode"": <mask>: value = self.sv_get()[0][0] print(""V"", value) if isinstance(value, int): param_node.selected_mode = ""int"" param_node.int_ = value elif isinstance(value, float): param_node.selected_mode = ""float"" param_node.float_ = value",False,if self . use_prop or self . get_prop_name ( ) :,if self . sv_get ( ) :,0.8820016898747209,13.185679291149079
"def collect_active_inst_idx_list(inst_beams, word_prob, inst_idx_to_position_map): active_inst_idx_list = [] for inst_idx, inst_position in inst_idx_to_position_map.items(): is_inst_complete = inst_beams[inst_idx].advance(word_prob[inst_position]) <mask>: active_inst_idx_list += [inst_idx] return active_inst_idx_list",False,if not is_inst_complete :,if is_inst_complete :,0.8820016898747209,72.89545183625967
"def compare_member_req_resp_without_key(self, request, response): for user_response in resp_json(response)[""data""]: for user_request in request: <mask>: assert user_request[""role""] == user_response[""role""]",False,"if user_request [ ""user_id"" ] == user_response [ ""user_id"" ] :","if ""role"" in user_request :",0.8820016898747209,4.142904542920855
"def __init__(self, dir): self.module_names = set() for name in os.listdir(dir): if name.endswith("".py""): self.module_names.add(name[:-3]) <mask>: self.module_names.add(name)",False,"elif ""."" not in name :","elif name . endswith ( "".py"" ) :",0.8820016898747209,9.864703138979419
"def _read_filter(self, data): if data: if self.expected_inner_sha256: self.inner_sha.update(data) <mask>: self.inner_md5.update(data) return data",False,if self . expected_inner_md5sum :,elif self . expected_inner_md5 :,0.8820016898747209,61.04735835807847
"def _p_basicstr_content(s, content=_basicstr_re): res = [] while True: res.append(s.expect_re(content).group(0)) <mask>: break if s.consume_re(_newline_esc_re): pass elif s.consume_re(_short_uni_re) or s.consume_re(_long_uni_re): res.append(_chr(int(s.last().group(1), 16))) else: s.expect_re(_escapes_re) res.append(_escapes[s.last().group(0)]) return """".join(res)",False,"if not s . consume ( ""\\"" ) :",if s . consume_re ( _newline_esc_re ) :,0.8820016898747209,13.380161378318954
"def process_response(self, request, response): if ( response.status_code == 404 and request.path_info.endswith(""/"") and not is_valid_path(request.path_info) and is_valid_path(request.path_info[:-1]) ): # Use request.path because we munged app/locale in path_info. newurl = request.path[:-1] <mask>: with safe_query_string(request): newurl += ""?"" + request.META.get(""QUERY_STRING"", """") return HttpResponsePermanentRedirect(newurl) else: return response",False,if request . GET :,"if request . META . get ( ""QUERY_STRING"" , """" ) :",0.8820016898747209,9.147827112247601
"def convertDict(obj): obj = dict(obj) for k, v in obj.items(): del obj[k] <mask>: k = dumps(k) # Keep track of which keys need to be decoded when loading. if Types.KEYS not in obj: obj[Types.KEYS] = [] obj[Types.KEYS].append(k) obj[k] = convertObjects(v) return obj",False,"if not ( isinstance ( k , str ) or isinstance ( k , unicode ) ) :","if isinstance ( k , dict ) :",0.8820016898747209,12.241977696855177
"def __repr__(self): if self._in_repr: return ""<recursion>"" try: self._in_repr = True <mask>: status = ""computed, "" if self.error() is None: if self.value() is self: status += ""= self"" else: status += ""= "" + repr(self.value()) else: status += ""error = "" + repr(self.error()) else: status = ""isn't computed"" return ""%s (%s)"" % (type(self), status) finally: self._in_repr = False",False,if self . is_computed ( ) :,if self . computed :,0.8820016898747209,20.300727612812874
"def allocate_network(ipv=""ipv4""): global dtcd_uuid global network_pool global allocations network = None try: cx = httplib.HTTPConnection(""localhost:7623"") cx.request(""POST"", ""/v1/network/%s/"" % ipv, body=dtcd_uuid) resp = cx.getresponse() <mask>: network = netaddr.IPNetwork(resp.read().decode(""utf-8"")) cx.close() except Exception: pass if network is None: network = network_pool[ipv].pop() allocations[network] = True return network",False,if resp . status == 200 :,if resp . status_code == 200 :,0.8820016898747209,52.53819788848316
"def change_args_to_dict(string): if string is None: return None ans = [] strings = string.split(""\n"") ind = 1 start = 0 while ind <= len(strings): if ind < len(strings) and strings[ind].startswith("" ""): ind += 1 else: if start < ind: ans.append(""\n"".join(strings[start:ind])) start = ind ind += 1 d = {} for line in ans: <mask>: lines = line.split("":"") d[lines[0]] = lines[1].strip() return d",False,"if "":"" in line and len ( line ) > 0 :","if line . startswith ( ""#"" ) :",0.8820016898747209,4.5751368559350025
"def kill_members(members, sig, hosts=nodes): for member in sorted(members): try: <mask>: print(""killing %s"" % member) proc = hosts[member][""proc""] # Not sure if cygwin makes sense here... if sys.platform in (""win32"", ""cygwin""): os.kill(proc.pid, signal.CTRL_C_EVENT) else: os.kill(proc.pid, sig) except OSError: if ha_tools_debug: print(""%s already dead?"" % member)",True,if ha_tools_debug :,if ha_tools_debug :,0.8820016898747209,100.00000000000004
"def check(self): for path in self.paths: response = self.http_request( method=""GET"", path=path, ) <mask>: continue if any( map( lambda x: x in response.text, [ ""report.db.server.name"", ""report.db.server.sa.pass"", ""report.db.server.user.pass"", ], ) ): self.valid = path return True # target is vulnerable return False # target not vulnerable",True,if response is None :,if response is None :,0.8820016898747209,100.00000000000004
"def get_to_download_runs_ids(session, headers): last_date = 0 result = [] while 1: r = session.get(RUN_DATA_API.format(last_date=last_date), headers=headers) if r.ok: run_logs = r.json()[""data""][""records""] result.extend([i[""logs""][0][""stats""][""id""] for i in run_logs]) last_date = r.json()[""data""][""lastTimestamp""] since_time = datetime.utcfromtimestamp(last_date / 1000) print(f""pares keep ids data since {since_time}"") time.sleep(1) # spider rule <mask>: break return result",False,if not last_date :,"if not r . json ( ) [ ""data"" ] [ ""spider"" ] :",0.8820016898747209,4.753622060013117
"def button_press_cb(self, tdw, event): self._update_zone_and_cursors(tdw, event.x, event.y) if self._zone in (_EditZone.CREATE_FRAME, _EditZone.REMOVE_FRAME): button = event.button <mask>: self._click_info = (button, self._zone) return False return super(FrameEditMode, self).button_press_cb(tdw, event)",False,if button == 1 and event . type == Gdk . EventType . BUTTON_PRESS :,if button == _EditZone . DELETE_FRAME :,0.8820016898747209,13.040087180327404
"def first_timestep(): assignment = self.has_previous.assign( value=tf_util.constant(value=True, dtype=""bool""), read_value=False ) with tf.control_dependencies(control_inputs=(assignment,)): <mask>: current = x else: current = tf.expand_dims(input=x, axis=(self.axis + 1)) multiples = tuple( self.length if dims == self.axis + 1 else 1 for dims in range(self.output_spec().rank + 1) ) return tf.tile(input=current, multiples=multiples)",False,if self . concatenate :,if self . axis == 0 :,0.8820016898747209,22.089591134157878
"def main() -> None: onefuzz = Onefuzz() jobs = onefuzz.jobs.list() for job in jobs: print( ""job:"", str(job.job_id)[:8], "":"".join([job.config.project, job.config.name, job.config.build]), ) for task in onefuzz.tasks.list(job_id=job.job_id): <mask>: continue print( "" "", str(task.task_id)[:8], task.config.task.type, task.config.task.target_exe, )",False,"if task . state in [ ""stopped"" , ""stopping"" ] :",if task . config . task is None :,0.8820016898747209,9.791289611338179
"def update_stack(self, full_name, template_url, parameters, tags): """"""Updates an existing stack in CloudFormation."""""" try: logger.info(""Attempting to update stack %s."", full_name) self.conn.cloudformation.update_stack( full_name, template_url=template_url, parameters=parameters, tags=tags, capabilities=[""CAPABILITY_IAM""], ) return SUBMITTED except BotoServerError as e: <mask>: logger.info(""Stack %s did not change, not updating."", full_name) return SKIPPED raise",False,"if ""No updates are to be performed."" in e . message :","if e . response [ ""Error"" ] [ ""Code"" ] == ""StackAlreadyExists"" :",0.8820016898747209,5.039518688486958
"def header_tag_files(env, files, legal_header, script_files=False): """"""Apply the legal_header to the list of files"""""" try: import apply_legal_header except: xbc.cdie(""XED ERROR: mfile.py could not find scripts directory"") for g in files: print(""G: "", g) for f in mbuild.glob(g): print(""F: "", f) <mask>: apply_legal_header.apply_header_to_data_file(legal_header, f) else: apply_legal_header.apply_header_to_source_file(legal_header, f)",True,if script_files :,if script_files :,0.8820016898747209,100.00000000000004
"def cleanDataCmd(cmd): newcmd = ""AbracadabrA ** <?php "" if cmd[:6] != ""php://"": if reverseConn not in cmd: cmds = cmd.split(""&"") for c in cmds: <mask>: newcmd += ""system('%s');"" % c else: b64cmd = base64.b64encode(cmd) newcmd += ""system(base64_decode('%s'));"" % b64cmd else: newcmd += cmd[6:] newcmd += ""?> **"" return newcmd",False,if len ( c ) > 0 :,if c . startswith ( reverseConn ) :,0.8820016898747209,8.25791079503452
"def test_form(self): n_qubits = 6 random_operator = get_fermion_operator(random_interaction_operator(n_qubits)) chemist_operator = chemist_ordered(random_operator) for term, _ in chemist_operator.terms.items(): <mask>: pass else: self.assertTrue(term[0][1]) self.assertTrue(term[2][1]) self.assertFalse(term[1][1]) self.assertFalse(term[3][1]) self.assertTrue(term[0][0] > term[2][0]) self.assertTrue(term[1][0] > term[3][0])",False,if len ( term ) == 2 or not len ( term ) :,if term [ 0 ] == 0 :,0.8820016898747209,6.155947438501932
"def do(server, handler, config, modargs): data = [] clients = server.get_clients(handler.default_filter) if not clients: return for client in clients: tags = config.tags(client.node()) <mask>: tags.remove(*modargs.remove) if modargs.add: tags.add(*modargs.add) data.append({""ID"": client.node(), ""TAGS"": tags}) config.save(project=modargs.write_project, user=modargs.write_user) handler.display(Table(data))",True,if modargs . remove :,if modargs . remove :,0.8820016898747209,100.00000000000004
"def validate(self): if self.data.get(""state"") == ""enabled"": <mask>: raise PolicyValidationError( ( ""redshift logging enablement requires `bucket` "" ""and `prefix` specification on %s"" % (self.manager.data,) ) ) return self",False,"if ""bucket"" not in self . data :","if self . data [ ""bucket"" ] and self . data [ ""prefix"" ] :",0.8820016898747209,13.618796864073044
"def renumber(self, x1, y1, x2, y2, dx, dy): out = [] for part in re.split(""(\w+)"", self.formula): m = re.match(""^([A-Z]+)([1-9][0-9]*)$"", part) <mask>: sx, sy = m.groups() x = colname2num(sx) y = int(sy) if x1 <= x <= x2 and y1 <= y <= y2: part = cellname(x + dx, y + dy) out.append(part) return FormulaCell("""".join(out), self.fmt, self.alignment)",False,if m is not None :,if m :,0.8820016898747209,0.0
"def update_sysconfig_file(fn, adjustments, allow_empty=False): if not adjustments: return (exists, contents) = read_sysconfig_file(fn) updated_am = 0 for (k, v) in adjustments.items(): if v is None: continue v = str(v) if len(v) == 0 and not allow_empty: continue contents[k] = v updated_am += 1 if updated_am: lines = [ str(contents), ] <mask>: lines.insert(0, util.make_header()) util.write_file(fn, ""\n"".join(lines) + ""\n"", 0o644)",False,if not exists :,if exists :,0.8820016898747209,0.0
"def getElement(self, aboutUri, namespace, name): for desc in self.rdfRoot.getElementsByTagNameNS(RDF_NAMESPACE, ""Description""): if desc.getAttributeNS(RDF_NAMESPACE, ""about"") == aboutUri: attr = desc.getAttributeNodeNS(namespace, name) <mask>: yield attr for element in desc.getElementsByTagNameNS(namespace, name): yield element",False,if attr != None :,if attr :,0.8820016898747209,0.0
"def get_store_name_from_connection_string(connection_string): if is_valid_connection_string(connection_string): segments = dict(seg.split(""="", 1) for seg in connection_string.split("";"")) endpoint = segments.get(""Endpoint"") <mask>: return endpoint.split(""//"")[1].split(""."")[0] return None",True,if endpoint :,if endpoint :,0.8820016898747209,0.0
"def insertLoopTemplate(self, layout): col = layout.column(align=True) for socket in self.activeNode.outputs: <mask>: props = col.operator( ""an.insert_loop_for_iterator"", text=""Loop through {}"".format(repr(socket.getDisplayedName())), icon=""MOD_ARRAY"", ) props.nodeIdentifier = self.activeNode.identifier props.socketIndex = socket.getIndex()",False,if not socket . hide and isList ( socket . bl_idname ) :,if socket . isLoop ( ) :,0.8820016898747209,6.555660318294845
"def do_task(self, task): self.running_task += 1 result = yield gen.Task(self.fetcher.fetch, task) type, task, response = result.args self.processor.on_task(task, response) # do with message while not self.processor.inqueue.empty(): _task, _response = self.processor.inqueue.get() self.processor.on_task(_task, _response) # do with results while not self.processor.result_queue.empty(): _task, _result = self.processor.result_queue.get() <mask>: self.result_worker.on_result(_task, _result) self.running_task -= 1",False,if self . result_worker :,if _result is not None :,0.8820016898747209,9.287528999566801
"def _parse_config_result(data): command_list = "" ; "".join([x.strip() for x in data[0]]) config_result = data[1] if isinstance(config_result, list): result = """" <mask>: for key in config_result[0]: result += config_result[0][key] config_result = result else: config_result = config_result[0] return [command_list, config_result]",False,"if isinstance ( config_result [ 0 ] , dict ) :",if len ( config_result ) > 0 :,0.8820016898747209,23.142716255858215
"def load_api_handler(self, mod_name): for name, hdl in API_HANDLERS: name = name.lower() <mask>: handler = self.mods.get(name) if not handler: handler = hdl(self.emu) self.mods.update({name: handler}) return handler return None",False,if mod_name and name == mod_name . lower ( ) :,if name . startswith ( mod_name ) :,0.8820016898747209,13.41633219332672
def heal(self): if not self.doctors: return proc_ids = self._get_process_ids() for proc_id in proc_ids: # get proc every time for latest state proc = PipelineProcess.objects.get(id=proc_id) if not proc.is_alive or proc.is_frozen: continue for dr in self.doctors: <mask>: dr.cure(proc) break,False,if dr . confirm ( proc ) :,if dr . is_alive ( proc ) :,0.8820016898747209,37.99178428257963
"def __new__(cls, *args, **kwargs): if len(args) == 1: if len(kwargs): raise ValueError( ""You can either use {} with one positional argument or with keyword arguments, not both."".format( cls.__name__ ) ) <mask>: return super().__new__(cls) if isinstance(args[0], cls): return cls return super().__new__(cls, *args, **kwargs)",False,if not args [ 0 ] :,if args [ 0 ] is None :,0.8820016898747209,38.260294162784454
"def __lt__(self, other): # 0: clock 1: timestamp 3: process id try: A, B = self[0], other[0] # uses logical clock value first <mask>: # use logical clock if available if A == B: # equal clocks use lower process id return self[2] < other[2] return A < B return self[1] < other[1] # ... or use timestamp except IndexError: return NotImplemented",False,if A and B :,if A == B :,0.8820016898747209,22.957488466614336
"def _get_client(rp_mapping, resource_provider): for key, value in rp_mapping.items(): if str.lower(key) == str.lower(resource_provider): <mask>: return GeneralPrivateEndpointClient( key, value[""api_version""], value[""support_list_or_not""], value[""resource_get_api_version""], ) return value() raise CLIError( ""Resource type must be one of {}"".format("", "".join(rp_mapping.keys())) )",False,"if isinstance ( value , dict ) :","if value [ ""api_version"" ] == value [ ""api_version"" ] :",0.8820016898747209,2.5197593442434796
"def test_progressbar_format_pos(runner, pos, length): with _create_progress(length, length_known=length != 0, pos=pos) as progress: result = progress.format_pos() <mask>: assert result == f""{pos}/{length}"" else: assert result == str(pos)",False,if progress . length_known :,if runner . is_running ( ) :,0.8820016898747209,6.742555929751843
"def optimize(self, graph: Graph): MAX_TEXTURE_SIZE = config.WEBGL_MAX_TEXTURE_SIZE flag_changed = False for v in traverse.listup_variables(graph): if not Placeholder.check_resolved(v.size): continue height, width = TextureShape.get(v) if height <= MAX_TEXTURE_SIZE and width <= MAX_TEXTURE_SIZE: continue <mask>: flag_changed = True v.attributes.add(SplitTarget()) return graph, flag_changed",False,if not v . has_attribute ( SplitTarget ) :,if height != width :,0.8820016898747209,4.194930905450255
"def ant_map(m): tmp = ""rows %s\ncols %s\n"" % (len(m), len(m[0])) players = {} for row in m: tmp += ""m "" for col in row: <mask>: tmp += ""."" elif col == BARRIER: tmp += ""%"" elif col == FOOD: tmp += ""*"" elif col == UNSEEN: tmp += ""?"" else: players[col] = True tmp += chr(col + 97) tmp += ""\n"" tmp = (""players %s\n"" % len(players)) + tmp return tmp",False,if col == LAND :,if col == AUTOMATIC :,0.8820016898747209,53.7284965911771
"def reset(self): logger.debug(""Arctic.reset()"") with self._lock: if self.__conn is not None: self.__conn.close() self.__conn = None for _, l in self._library_cache.items(): <mask>: logger.debug(""Library reset() %s"" % l) l._reset() # the existence of _reset() is not guaranteed/enforced, it also triggers re-auth",False,"if hasattr ( l , ""_reset"" ) and callable ( l . _reset ) :",if l . _is_valid ( ) :,0.8820016898747209,8.98095755122353
"def add_cand_to_check(cands): for cand in cands: x = cand.creator <mask>: continue if x not in fan_out: # `len(fan_out)` is in order to avoid comparing `x` heapq.heappush(cand_funcs, (-x.rank, len(fan_out), x)) fan_out[x] += 1",True,if x is None :,if x is None :,0.8820016898747209,100.00000000000004
"def on_task_modify(self, task, config): for entry in task.entries: <mask>: size = entry[""torrent""].size / 1024 / 1024 log.debug(""%s size: %s MB"" % (entry[""title""], size)) entry[""content_size""] = size",True,"if ""torrent"" in entry :","if ""torrent"" in entry :",0.8820016898747209,100.00000000000004
"def get_measurements(self, pipeline, object_name, category): if self.get_categories(pipeline, object_name) == [category]: results = [] if self.do_corr_and_slope: if object_name == ""Image"": results += [""Correlation"", ""Slope""] else: results += [""Correlation""] if self.do_overlap: results += [""Overlap"", ""K""] if self.do_manders: results += [""Manders""] if self.do_rwc: results += [""RWC""] <mask>: results += [""Costes""] return results return []",True,if self . do_costes :,if self . do_costes :,0.8820016898747209,100.00000000000004
"def create_root(cls, site=None, title=""Root"", request=None, **kwargs): if not site: site = Site.objects.get_current() root_nodes = cls.objects.root_nodes().filter(site=site) if not root_nodes: article = Article() revision = ArticleRevision(title=title, **kwargs) <mask>: revision.set_from_request(request) article.add_revision(revision, save=True) article.save() root = cls.objects.create(site=site, article=article) article.add_object_relation(root) else: root = root_nodes[0] return root",True,if request :,if request :,0.8820016898747209,0.0
"def get(self, key): filename = self._get_filename(key) try: with open(filename, ""rb"") as f: pickle_time = pickle.load(f) <mask>: return pickle.load(f) else: os.remove(filename) return None except (IOError, OSError, pickle.PickleError): return None",False,if pickle_time == 0 or pickle_time >= time ( ) :,if pickle_time . st_mtime > self . _last_modified :,0.8820016898747209,16.542390175536816
"def build_message(self, options, target): message = multipart.MIMEMultipart() for name, value in list(options.items()): <mask>: self.add_body(message, value) elif name == ""EMAIL_ATTACHMENT"": self.add_attachment(message, value) else: # From, To, Subject, etc. self.set_option(message, name, value, target) return message",False,"if name == ""EMAIL_BODY"" :","if name == ""BODY"" :",0.8820016898747209,53.849523560640876
"def updateVar(name, data, mode=None): if mode: <mask>: core.config.globalVariables[name].append(data) elif mode == ""add"": core.config.globalVariables[name].add(data) else: core.config.globalVariables[name] = data",False,"if mode == ""append"" :","if mode == ""add"" :",0.8820016898747209,59.4603557501361
"def insert_errors( el, errors, form_id=None, form_index=None, error_class=""error"", error_creator=default_error_creator, ): el = _find_form(el, form_id=form_id, form_index=form_index) for name, error in errors.items(): <mask>: continue for error_el, message in _find_elements_for_name(el, name, error): assert isinstance(message, (basestring, type(None), ElementBase)), ( ""Bad message: %r"" % message ) _insert_error(error_el, message, error_class, error_creator)",False,if error is None :,"if name == ""error"" :",0.8820016898747209,7.267884212102741
"def read(self, item, recursive=False, sort=False): item = _normalize_path(item) if item in self._store: <mask>: del self._store[item] raise KeyError(item) return PathResult(item, value=self._store[item]) else: return self._read_dir(item, recursive=recursive, sort=sort)",False,if item in self . _expire_time and self . _expire_time [ item ] < datetime . now ( ) :,if not recursive :,0.8820016898747209,0.07763186945116722
"def _stash_splitter(states): keep, split = [], [] if state_func is not None: for s in states: ns = state_func(s) if isinstance(ns, SimState): split.append(ns) <mask>: split.extend(ns) else: split.append(s) if stash_func is not None: split = stash_func(states) if to_stash is not stash: keep = states return keep, split",False,"elif isinstance ( ns , ( list , tuple , set ) ) :","elif isinstance ( ns , list ) :",0.8820016898747209,29.040536047560465
"def run(self): while self.runflag: <mask>: with self.lock: tasks = list(self.queue) self.queue.clear() while len(tasks) > 0: pathname, remotepath = tasks.pop(0) self.bcloud_app.upload_page.add_bg_task(pathname, remotepath) self.last = time() else: sleep(1)",False,if time ( ) - self . last > 5 and self . qsize ( ) > 0 :,if self . last is time ( ) :,0.8820016898747209,10.558266591288652
"def _append_patch(self, patch_dir, patch_files): for patch in patch_files: <mask>: tmp = patch patch = {} for key in tmp.keys(): patch[os.path.join(patch_dir, key)] = tmp[key] self.patches.append(patch) else: self.patches.append(os.path.join(patch_dir, patch))",False,if type ( patch ) is dict :,"if isinstance ( patch , dict ) :",0.8820016898747209,14.535768424205482
"def __remote_port(self): port = 22 if self.git_has_remote: m = re.match(r""^(.*?)?@([^/:]*):?([0-9]+)?"", self.git_remote.url) if m: <mask>: port = m.group(3) return int(port)",False,if m . group ( 3 ) :,"if m . group ( 1 ) == ""port"" :",0.8820016898747209,31.455601883230702
"def _create_or_get_helper(self, infer_mode: Optional[bool] = None, **kwargs) -> Helper: # Prefer creating a new helper when at least one kwarg is specified. prefer_new = len(kwargs) > 0 kwargs.update(infer_mode=infer_mode) is_training = not infer_mode if infer_mode is not None else self.training helper = self._train_helper if is_training else self._infer_helper if prefer_new or helper is None: helper = self.create_helper(**kwargs) <mask>: self._train_helper = helper elif not is_training and self._infer_helper is None: self._infer_helper = helper return helper",True,if is_training and self . _train_helper is None :,if is_training and self . _train_helper is None :,0.8820016898747209,100.00000000000004
"def flushChangeClassifications(self, schedulerid, less_than=None): if less_than is not None: classifications = self.classifications.setdefault(schedulerid, {}) for changeid in list(classifications): <mask>: del classifications[changeid] else: self.classifications[schedulerid] = {} return defer.succeed(None)",False,if changeid < less_than :,if less_than < changeid :,0.8820016898747209,30.213753973567677
"def pid_from_name(name): processes = [] for pid in os.listdir(""/proc""): try: pid = int(pid) pname, cmdline = SunProcess._name_args(pid) if name in pname: return pid <mask>: return pid except: pass raise ProcessException(""No process with such name: %s"" % name)",False,"if name in cmdline . split ( "" "" , 1 ) [ 0 ] :",elif cmdline in pname :,0.8820016898747209,1.5577298727187734
"def spew(): seenUID = False start() for part in query: <mask>: seenUID = True if part.type == ""body"": yield self.spew_body(part, id, msg, write, flush) else: f = getattr(self, ""spew_"" + part.type) yield f(id, msg, write, flush) if part is not query[-1]: space() if uid and not seenUID: space() yield self.spew_uid(id, msg, write, flush) finish() flush()",False,"if part . type == ""uid"" :",if part . uid :,0.8820016898747209,16.62083000646927
"def rx(): while True: rx_i = rep.recv() <mask>: rep.send(b""done"") break rep.send(b""i"")",False,"if rx_i == b""1000"" :","if rx_i == b""done"" :",0.8820016898747209,74.19446627365011
"def test_search_incorrect_base_exception_1(self): self.connection_1c.bind() try: result = self.connection_1c.search( ""o=nonexistant"", ""(cn=*)"", search_scope=SUBTREE, attributes=[""cn"", ""sn""] ) <mask>: _, result = self.connection_1c.get_response(result) self.fail(""exception not raised"") except LDAPNoSuchObjectResult: pass",False,if not self . connection_1c . strategy . sync :,if result :,0.8820016898747209,0.0
"def value_from_datadict(self, data, files, prefix): count = int(data[""%s-count"" % prefix]) values_with_indexes = [] for i in range(0, count): <mask>: continue values_with_indexes.append( ( int(data[""%s-%d-order"" % (prefix, i)]), self.child_block.value_from_datadict( data, files, ""%s-%d-value"" % (prefix, i) ), ) ) values_with_indexes.sort() return [v for (i, v) in values_with_indexes]",False,"if data [ ""%s-%d-deleted"" % ( prefix , i ) ] :",if i >= count :,0.8820016898747209,1.707863452144561
"def _ensure_header_written(self, datasize): if not self._headerwritten: if not self._nchannels: raise Error(""# channels not specified"") if not self._sampwidth: raise Error(""sample width not specified"") <mask>: raise Error(""sampling rate not specified"") self._write_header(datasize)",False,if not self . _framerate :,if not self . _samprate :,0.8820016898747209,64.34588841607616
def wait_til_ready(cls): while True: now = time.time() next_iteration = now // 1.0 + 1 <mask>: break else: await cls._clock.run_til(next_iteration) await asyncio.sleep(1.0),False,if cls . connector . ready :,if next_iteration == 0 :,0.8820016898747209,6.567274736060395
"def lookup_actions(self, resp): actions = {} for action, conditions in self.actions.items(): for condition, opts in conditions: for key, val in condition: <mask>: if resp.match(key[:-1], val): break else: if not resp.match(key, val): break else: actions[action] = opts return actions",False,"if key [ - 1 ] == ""!"" :","if key . endswith ( ""_"" ) :",0.8820016898747209,8.639795714750207
"def close(self, wait=True, abort=False): """"""Close the socket connection."""""" if not self.closed and not self.closing: self.closing = True self.server._trigger_event(""disconnect"", self.sid, run_async=False) if not abort: self.send(packet.Packet(packet.CLOSE)) self.closed = True self.queue.put(None) <mask>: self.queue.join()",True,if wait :,if wait :,0.8820016898747209,0.0
"def model_parse(self): for name, submodel in self.model.named_modules(): for op_type in SUPPORTED_OP_TYPE: <mask>: self.target_layer[name] = submodel self.already_pruned[name] = 0",False,"if isinstance ( submodel , op_type ) :",if op_type in submodel . op_types :,0.8820016898747209,16.59038701421971
"def pack_identifier(self): """"""Return a combined identifier for the whole pack if this has more than one episode."""""" # Currently only supports ep mode if self.id_type == ""ep"": <mask>: return ""S%02dE%02d-E%02d"" % ( self.season, self.episode, self.episode + self.episodes - 1, ) else: return self.identifier else: return self.identifier",False,if self . episodes > 1 :,if self . episode > 0 :,0.8820016898747209,27.77619034011791
"def on_data(res): if terminate.is_set(): return if args.strings and not args.no_content: if type(res) == tuple: f, v = res if type(f) == unicode: f = f.encode(""utf-8"") <mask>: v = v.encode(""utf-8"") self.success(""{}: {}"".format(f, v)) elif not args.content_only: self.success(res) else: self.success(res)",False,if type ( v ) == unicode :,elif type ( v ) == unicode :,0.8820016898747209,86.33400213704509
"def _enable_contours_changed(self, value): """"""Turns on and off the contours."""""" if self.module_manager is None: return if value: self.actor.inputs = [self.contour] <mask>: self.actor.mapper.scalar_mode = ""use_cell_data"" else: self.actor.inputs = [self.grid_plane] self.actor.mapper.scalar_mode = ""default"" self.render()",False,if self . contour . filled_contours :,elif value :,0.8820016898747209,0.0
"def _apply_abs_paths(data, script_dir): for flag_data in data.values(): <mask>: continue default = flag_data.get(""default"") if ( not default or not isinstance(default, six.string_types) or os.path.sep not in default ): continue abs_path = os.path.join(script_dir, default) if os.path.exists(abs_path): flag_data[""default""] = abs_path",False,"if not isinstance ( flag_data , dict ) :","if flag_data . get ( ""flag"" ) is None :",0.8820016898747209,13.06511329838856
"def button_release(self, mapper): self.pressed = False if self.waiting_task and self.active is None and not self.action: # In HoldModifier, button released before timeout mapper.cancel_task(self.waiting_task) self.waiting_task = None <mask>: self.normalaction.button_press(mapper) mapper.schedule(0.02, self.normalaction.button_release) elif self.active: # Released held button self.active.button_release(mapper) self.active = None",True,if self . normalaction :,if self . normalaction :,0.8820016898747209,100.00000000000004
"def goToPrevMarkedHeadline(self, event=None): """"""Select the next marked node."""""" c = self p = c.p if not p: return p.moveToThreadBack() wrapped = False while 1: if p and p.isMarked(): break <mask>: p.moveToThreadBack() elif wrapped: break else: wrapped = True p = c.rootPosition() if not p: g.blue(""done"") c.treeSelectHelper(p) # Sets focus.",True,elif p :,elif p :,0.8820016898747209,0.0
"def status(self, name, error=""No matching script logs found""): with self.script_lock: if self.script_running and self.script_running[1] == name: return self.script_running[1:] <mask>: return self.script_last[1:] else: raise ValueError(error)",True,elif self . script_last and self . script_last [ 1 ] == name :,elif self . script_last and self . script_last [ 1 ] == name :,0.8820016898747209,100.00000000000004
"def _stderr_supports_color(): try: if hasattr(sys.stderr, ""isatty"") and sys.stderr.isatty(): <mask>: curses.setupterm() if curses.tigetnum(""colors"") > 0: return True elif colorama: if sys.stderr is getattr( colorama.initialise, ""wrapped_stderr"", object() ): return True except Exception: # Very broad exception handling because it's always better to # fall back to non-colored logs than to break at startup. pass return False",False,if curses :,if curses . isatty ( ) :,0.8820016898747209,14.535768424205482
"def main(): configFilename = ""twitterbot.ini"" if sys.argv[1:]: configFilename = sys.argv[1] try: <mask>: raise Exception() load_config(configFilename) except Exception as e: print(""Error while loading ini file %s"" % (configFilename), file=sys.stderr) print(e, file=sys.stderr) print(__doc__, file=sys.stderr) sys.exit(1) bot = TwitterBot(configFilename) return bot.run()",True,if not os . path . exists ( configFilename ) :,if not os . path . exists ( configFilename ) :,0.8820016898747209,100.00000000000004
def safe_to_kill(request): if os.path.exists(DRAIN_FILE): with open(DRAIN_FILE) as f: dt = datetime.datetime.fromtimestamp(float(f.read())) delta = datetime.datetime.now() - dt <mask>: return Response(status_int=200) else: return Response(status_int=400) else: return Response(status_int=400),False,if delta . seconds > 2 :,if delta > 0.01 :,0.8820016898747209,17.030578356760866
"def get_class_name(item): class_name, module_name = None, None for parent in reversed(item.listchain()): <mask>: class_name = parent.name elif isinstance(parent, pytest.Module): module_name = parent.module.__name__ break # heuristic: # - better to group gpu and task tests, since tests from those modules # are likely to share caching more # - split up the rest by class name because slow tests tend to be in # the same module if class_name and "".tasks."" not in module_name: return ""{}.{}"".format(module_name, class_name) else: return module_name",True,"if isinstance ( parent , pytest . Class ) :","if isinstance ( parent , pytest . Class ) :",0.8820016898747209,100.00000000000004
"def getAllFitsLite(): fits = eos.db.getFitListLite() shipMap = {f.shipID: None for f in fits} for shipID in shipMap: ship = eos.db.getItem(shipID) <mask>: shipMap[shipID] = (ship.name, ship.getShortName()) fitsToPurge = set() for fit in fits: try: fit.shipName, fit.shipNameShort = shipMap[fit.shipID] except (KeyError, TypeError): fitsToPurge.add(fit) for fit in fitsToPurge: fits.remove(fit) return fits",False,if ship is not None :,if ship :,0.8820016898747209,0.0
"def _process(self, event_data): self.machine.callbacks(self.machine.prepare_event, event_data) _LOGGER.debug( ""%sExecuted machine preparation callbacks before conditions."", self.machine.name ) try: for trans in self.transitions[event_data.state.name]: event_data.transition = trans <mask>: event_data.result = True break except Exception as err: event_data.error = err raise finally: self.machine.callbacks(self.machine.finalize_event, event_data) _LOGGER.debug(""%sExecuted machine finalize callbacks"", self.machine.name) return event_data.result",False,if trans . execute ( event_data ) :,if trans . is_active ( ) :,0.8820016898747209,21.721856265678966
"def fetch_comments(self, force=False, limit=None): comments = [] if (force is True) or (self.badges[""comments""] > 0): query_params = {""filter"": ""commentCard,copyCommentCard""} <mask>: query_params[""limit""] = limit comments = self.client.fetch_json( ""/cards/"" + self.id + ""/actions"", query_params=query_params ) return sorted(comments, key=lambda comment: comment[""date""]) return comments",True,if limit is not None :,if limit is not None :,0.8820016898747209,100.00000000000004
"def get_changed(self): if self._is_expression(): result = self._get_node_text(self.ast) if result == self.source: return None return result else: collector = codeanalyze.ChangeCollector(self.source) last_end = -1 for match in self.matches: start, end = match.get_region() if start < last_end: <mask>: continue last_end = end replacement = self._get_matched_text(match) collector.add_change(start, end, replacement) return collector.get_changed()",False,if not self . _is_expression ( ) :,if end == last_end :,0.8820016898747209,4.995138898472386
"def _replace_home(x): if xp.ON_WINDOWS: home = ( builtins.__xonsh__.env[""HOMEDRIVE""] + builtins.__xonsh__.env[""HOMEPATH""][0] ) <mask>: x = x.replace(home, ""~"", 1) if builtins.__xonsh__.env.get(""FORCE_POSIX_PATHS""): x = x.replace(os.sep, os.altsep) return x else: home = builtins.__xonsh__.env[""HOME""] if x.startswith(home): x = x.replace(home, ""~"", 1) return x",True,if x . startswith ( home ) :,if x . startswith ( home ) :,0.8820016898747209,100.00000000000004
"def project_review(plans): for plan in plans: print(""Inspecting {} plan"".format(plan)) branches = get_branches_from_plan(plan) for branch in branches: build_results = get_results_from_branch(branch) for build in build_results: build_key = build.get(""buildResultKey"") or None print(""Inspecting build - {}"".format(build_key)) <mask>: for status in STATUS_CLEANED_RESULTS: remove_build_result(build_key=build_key, status=status)",True,if build_key :,if build_key :,0.8820016898747209,100.00000000000004
"def _check_for_batch_clashes(xs): """"""Check that batch names do not overlap with sample names."""""" names = set([x[""description""] for x in xs]) dups = set([]) for x in xs: batches = tz.get_in((""metadata"", ""batch""), x) if batches: <mask>: batches = [batches] for batch in batches: if batch in names: dups.add(batch) if len(dups) > 0: raise ValueError( ""Batch names must be unique from sample descriptions.\n"" ""Clashing batch names: %s"" % sorted(list(dups)) )",False,"if not isinstance ( batches , ( list , tuple ) ) :","if not isinstance ( batches , list ) :",0.8820016898747209,43.624306402227546
"def _check_signal(self): """"""Checks if a signal was received and issues a message."""""" proc_signal = getattr(self.proc, ""signal"", None) if proc_signal is None: return sig, core = proc_signal sig_str = SIGNAL_MESSAGES.get(sig) if sig_str: if core: sig_str += "" (core dumped)"" print(sig_str, file=sys.stderr) <mask>: self.errors += sig_str + ""\n""",False,if self . errors is not None :,if self . errors :,0.8820016898747209,38.80684294761701
"def loadLabelFile(self, labelpath): labeldict = {} if not os.path.exists(labelpath): f = open(labelpath, ""w"", encoding=""utf-8"") else: with open(labelpath, ""r"", encoding=""utf-8"") as f: data = f.readlines() for each in data: file, label = each.split(""\t"") <mask>: label = label.replace(""false"", ""False"") label = label.replace(""true"", ""True"") labeldict[file] = eval(label) else: labeldict[file] = [] return labeldict",False,if label :,if self . _isBoolean ( label ) :,0.8820016898747209,6.27465531099474
"def exists_col_to_many(self, select_columns: List[str]) -> bool: for column in select_columns: <mask>: root_relation = get_column_root_relation(column) if self.is_relation_many_to_many( root_relation ) or self.is_relation_one_to_many(root_relation): return True return False",False,if is_column_dotted ( column ) :,if column . is_primary_key ( ) :,0.8820016898747209,12.605968092174914
"def check_sequence_matches(seq, template): i = 0 for pattern in template: <mask>: pattern = {pattern} got = set(seq[i : i + len(pattern)]) assert got == pattern i += len(got)",False,"if not isinstance ( pattern , set ) :","if not isinstance ( pattern , dict ) :",0.8820016898747209,66.06328636027612
"def load_modules( to_load, load, attr, modules_dict, excluded_aliases, loading_message=None ): if loading_message: print(loading_message) for name in to_load: module = load(name) if module is None or not hasattr(module, attr): continue cls = getattr(module, attr) if hasattr(cls, ""initialize"") and not cls.initialize(): continue <mask>: for alias in module.aliases(): if alias not in excluded_aliases: modules_dict[alias] = module else: modules_dict[name] = module if loading_message: print()",True,"if hasattr ( module , ""aliases"" ) :","if hasattr ( module , ""aliases"" ) :",0.8820016898747209,100.00000000000004
"def result(): # ""global"" does not work here... R, V = rays, virtual_rays if V is not None: <mask>: V = normalize_rays(V, lattice) if check: R = PointCollection(V, lattice) V = PointCollection(V, lattice) d = lattice.dimension() if len(V) != d - R.dim() or (R + V).dim() != d: raise ValueError( ""virtual rays must be linearly "" ""independent and with other rays span the ambient space."" ) return RationalPolyhedralFan(cones, R, lattice, is_complete, V)",True,if normalize :,if normalize :,0.8820016898747209,0.0
"def communicate(self, _input=None, _timeout=None) -> Tuple[bytes, bytes]: if parse_args().print_commands: <mask>: print_stderr( color_line(""=> "", 14) + "" "".join(str(arg) for arg in self.args) ) stdout, stderr = super().communicate(_input, _timeout) self.stdout_text = stdout.decode(""utf-8"") if stdout else None self.stderr_text = stderr.decode(""utf-8"") if stderr else None return stdout, stderr",False,if self . args != get_sudo_refresh_command ( ) :,if self . args :,0.8820016898747209,7.834966465489322
"def convert(data): result = [] for d in data: # noinspection PyCompatibility <mask>: result.append((d[0], None, d[1])) elif isinstance(d, basestring): result.append(d) return result",False,"if isinstance ( d , tuple ) and len ( d ) == 2 :","if isinstance ( d , tuple ) :",0.8820016898747209,31.984974287337113
"def validate(self, value): try: value = [ datetime.datetime.strptime(range, ""%Y-%m-%d %H:%M:%S"") for range in value.split("" to "") ] <mask>: return True else: return False except ValueError: return False",False,if ( len ( value ) == 2 ) and ( value [ 0 ] <= value [ 1 ] ) :,if value :,0.8820016898747209,0.0
"def rmdir(dirname): if dirname[-1] == os.sep: dirname = dirname[:-1] if os.path.islink(dirname): return # do not clear link - we can get out of dir for f in os.listdir(dirname): if f in (""."", ""..""): continue path = dirname + os.sep + f <mask>: rmdir(path) else: os.unlink(path) os.rmdir(dirname)",True,if os . path . isdir ( path ) :,if os . path . isdir ( path ) :,0.8820016898747209,100.00000000000004
"def onCompletion(self, text): res = [] for l in text.split(""\n""): if not l: continue l = l.split("":"") <mask>: continue res.append([l[0].strip(), l[1].strip()]) self.panel.setSlides(res)",True,if len ( l ) != 2 :,if len ( l ) != 2 :,0.8820016898747209,100.00000000000004
"def pytest_collection_modifyitems(items): for item in items: <mask>: if ""stage"" not in item.keywords: item.add_marker(pytest.mark.stage(""unit"")) if ""init"" not in item.keywords: item.add_marker(pytest.mark.init(rng_seed=123))",False,"if item . nodeid . startswith ( ""tests/infer"" ) :","if ""unit"" not in item . keywords :",0.8820016898747209,7.40354787297858
"def build_message(self, options, target): message = multipart.MIMEMultipart() for name, value in list(options.items()): if name == ""EMAIL_BODY"": self.add_body(message, value) <mask>: self.add_attachment(message, value) else: # From, To, Subject, etc. self.set_option(message, name, value, target) return message",True,"elif name == ""EMAIL_ATTACHMENT"" :","elif name == ""EMAIL_ATTACHMENT"" :",0.8820016898747209,100.00000000000004
def extend_with_zeroes(b): try: for x in b: x = to_constant(x) <mask>: yield (x) else: yield (0) for _ in range(32): yield (0) except Exception as e: return,False,"if isinstance ( x , int ) :",if x != 0 :,0.8820016898747209,7.654112967106117
"def _start_cluster(*, cleanup_atexit=True): global _default_cluster if _default_cluster is None: cluster_addr = os.environ.get(""EDGEDB_TEST_CLUSTER_ADDR"") <mask>: conn_spec = json.loads(cluster_addr) _default_cluster = edgedb_cluster.RunningCluster(**conn_spec) else: data_dir = os.environ.get(""EDGEDB_TEST_DATA_DIR"") _default_cluster = _init_cluster( data_dir=data_dir, cleanup_atexit=cleanup_atexit ) return _default_cluster",True,if cluster_addr :,if cluster_addr :,0.8820016898747209,100.00000000000004
"def preprocess_raw_enwik9(input_filename, output_filename): with open(input_filename, ""r"") as f1: with open(output_filename, ""w"") as f2: while True: line = f1.readline() <mask>: break line = list(enwik9_norm_transform([line]))[0] if line != "" "" and line != """": if line[0] == "" "": line = line[1:] f2.writelines(line + ""\n"")",True,if not line :,if not line :,0.8820016898747209,100.00000000000004
"def is_entirely_italic(line): style = subs.styles.get(line.style, SSAStyle.DEFAULT_STYLE) for fragment, sty in parse_tags(line.text, style, subs.styles): fragment = fragment.replace(r""\h"", "" "") fragment = fragment.replace(r""\n"", ""\n"") fragment = fragment.replace(r""\N"", ""\n"") <mask>: return False return True",False,if not sty . italic and fragment and not fragment . isspace ( ) :,"if fragment . startswith ( r""\t"" ) or fragment . startswith ( r""\t"" ) :",0.8820016898747209,5.538696232597744
def __get_all_nodes(self): nodes = [] next_level = [self.__tree.get_root()] while len(next_level) != 0: cur_level = next_level nodes += next_level next_level = [] for cur_node in cur_level: children = cur_node.get_children() <mask>: next_level += children return nodes,False,if children is not None :,if children :,0.8820016898747209,0.0
"def _openvpn_stdout(self): while True: line = self.process.stdout.readline() if not line: <mask>: return time.sleep(0.05) continue yield try: self.server.output.push_output(line) except: logger.exception( ""Failed to push vpn output"", ""server"", server_id=self.server.id, ) yield",False,if self . process . poll ( ) is not None or self . is_interrupted ( ) :,if self . server . output . is_running ( ) :,0.8820016898747209,15.706536634435272
"def payment_received_handler(event): if isinstance(event.message.action, types.MessageActionPaymentSentMe): payment: types.MessageActionPaymentSentMe = event.message.action # do something after payment was received if payment.payload.decode(""UTF-8"") == ""product A"": await bot.send_message( event.message.from_id, ""Thank you for buying product A!"" ) <mask>: await bot.send_message( event.message.from_id, ""Thank you for buying product B!"" ) raise events.StopPropagation",True,"elif payment . payload . decode ( ""UTF-8"" ) == ""product B"" :","elif payment . payload . decode ( ""UTF-8"" ) == ""product B"" :",0.8820016898747209,100.00000000000004
"def spaces_after(token, prev, next, min=-1, max=-1, min_desc=None, max_desc=None): if next is not None and token.end_mark.line == next.start_mark.line: spaces = next.start_mark.pointer - token.end_mark.pointer <mask>: return LintProblem( token.start_mark.line + 1, next.start_mark.column, max_desc ) elif min != -1 and spaces < min: return LintProblem( token.start_mark.line + 1, next.start_mark.column + 1, min_desc )",True,if max != - 1 and spaces > max :,if max != - 1 and spaces > max :,0.8820016898747209,100.00000000000004
"def seek_to_block(self, pos): baseofs = 0 ofs = 0 for b in self.blocks: <mask>: self.current_block = b break baseofs += b.compressed_size ofs += b.uncompressed_size else: self.current_block = None self.current_stream = BytesIO(b"""") return self.current_block_start = ofs self.stream.seek(self.basepos + baseofs) buf = BytesIO(self.stream.read(self.current_block.compressed_size)) self.current_stream = self.current_block.decompress(buf)",False,if ofs + b . uncompressed_size > pos :,if b . compressed_size == pos :,0.8820016898747209,13.66926756730835
"def rewrite_hunks(hunks): # type: (List[Hunk]) -> Iterator[Hunk] # Assumes `hunks` are sorted, and from the same file deltas = (hunk.b_length - hunk.a_length for hunk in hunks) offsets = accumulate(deltas, initial=0) for hunk, offset in zip(hunks, offsets): new_b = hunk.a_start + offset if hunk_of_additions_only(hunk): new_b += 1 <mask>: new_b -= 1 yield hunk._replace(b_start=new_b)",False,elif hunk_of_removals_only ( hunk ) :,if hunk_of_removes_only ( hunk ) :,0.8820016898747209,63.40466277046863
"def do_query(data, q): ret = [] if not q: return ret qkey = q[0] for key, value in iterate(data): <mask>: if key == qkey: ret.append(value) elif is_iterable(value): ret.extend(do_query(value, q)) else: if not is_iterable(value): continue if key == qkey: ret.extend(do_query(value, q[1:])) else: ret.extend(do_query(value, q)) return ret",False,if len ( q ) == 1 :,"if isinstance ( value , list ) :",0.8820016898747209,6.892168295481103
"def get_url(token, base_url): """"""Parse an <url> token."""""" if token.type == ""url"": return _get_url_tuple(token.value, base_url) elif token.type == ""function"": if token.name == ""attr"": return check_attr_function(token, ""url"") <mask>: # Ignore url modifiers # See https://drafts.csswg.org/css-values-3/#urls return _get_url_tuple(token.arguments[0].value, base_url)",False,"elif token . name == ""url"" and len ( token . arguments ) in ( 1 , 2 ) :","elif token . name == ""url"" :",0.8820016898747209,24.626027423291728
"def read(self, count): if self.closed: return self.upstream.read(count) try: while len(self.upstream) < count: <mask>: with self.buf_in: self.transport.downstream_recv(self.buf_in) else: break return self.upstream.read(count) except: logger.debug(traceback.format_exc())",False,if self . buf_in or self . _poll_read ( 10 ) :,if self . transport :,0.8820016898747209,3.876260268742648
"def get_timestamp_for_block( self, block_hash: HexBytes, max_tries: Optional[int] = 10 ) -> int: counter = 0 block: AttributeDict = None if block_hash in self._block_cache.keys(): block = self._block_cache.get(block_hash) else: while block is None: <mask>: raise ValueError(f""Block hash {block_hash.hex()} does not exist."") counter += 1 block = self._block_cache.get(block_hash) await asyncio.sleep(0.5) return block.get(""timestamp"")",False,if counter == max_tries :,if counter >= max_tries :,0.8820016898747209,59.4603557501361
"def reader(): batch_out = [] for video_name in self.video_list: video_idx = self.video_list.index(video_name) video_feat = self.load_file(video_name) batch_out.append((video_feat, video_idx)) <mask>: yield batch_out batch_out = []",False,if len ( batch_out ) == self . batch_size :,if batch_out :,0.8820016898747209,6.114461654585455
"def cleanup(): gscript.message(_(""Erasing temporary files..."")) for temp_map, maptype in temp_maps: <mask>: gscript.run_command( ""g.remove"", flags=""f"", type=maptype, name=temp_map, quiet=True )",False,"if gscript . find_file ( temp_map , element = maptype ) [ ""name"" ] :","if maptype in [ ""file"" , ""file_write"" ] :",0.8820016898747209,9.805484301731983
"def run(self): while True: try: with DelayedKeyboardInterrupt(): raw_inputs = self._parent_task_queue.get() <mask>: self._rq.put(raw_inputs, block=True) break if self._flow_type == BATCH: self._rq.put(raw_inputs, block=True) elif self._flow_type == REALTIME: try: self._rq.put(raw_inputs, block=False) except: pass except KeyboardInterrupt: continue",False,if self . _has_stop_signal ( raw_inputs ) :,if not raw_inputs :,0.8820016898747209,7.659936211474486
"def handle_sent(self, elt): sent = [] for child in elt: if child.tag in (""mw"", ""hi"", ""corr"", ""trunc""): sent += [self.handle_word(w) for w in child] <mask>: sent.append(self.handle_word(child)) elif child.tag not in self.tags_to_ignore: raise ValueError(""Unexpected element %s"" % child.tag) return BNCSentence(elt.attrib[""n""], sent)",False,"elif child . tag in ( ""w"" , ""c"" ) :","elif child . tag in ( ""mw"" , ""hi"" , ""corr"" , ""trunc"" ) :",0.8820016898747209,36.176403924259866
"def bind_subscribers_to_graphql_type(self, graphql_type): for field, subscriber in self._subscribers.items(): <mask>: raise ValueError(""Field %s is not defined on type %s"" % (field, self.name)) graphql_type.fields[field].subscribe = subscriber",False,if field not in graphql_type . fields :,"if not isinstance ( subscriber , GraphQLSubscriber ) :",0.8820016898747209,5.61480827173619
"def _get_from_json(self, *, name, version): url = urljoin(self.url, posixpath.join(name, str(version), ""json"")) async with aiohttp_session(auth=self.auth) as session: async with session.get(url) as response: <mask>: raise PackageNotFoundError(package=name, url=url) response.raise_for_status() response = await response.json() dist = response[""info""][""requires_dist""] or [] if dist: return dist # If no requires_dist then package metadata can be broken. # Let's check distribution files. return await self._get_from_files(response[""urls""])",False,if response . status == 404 :,"if ""info"" not in response :",0.8820016898747209,7.267884212102741
"def is_active(self): if not self.pk: log_level = get_setting(""LOG_MISSING_SWITCHES"") if log_level: logger.log(log_level, ""Switch %s not found"", self.name) <mask>: switch, _created = Switch.objects.get_or_create( name=self.name, defaults={""active"": get_setting(""SWITCH_DEFAULT"")} ) cache = get_cache() cache.set(self._cache_key(self.name), switch) return get_setting(""SWITCH_DEFAULT"") return self.active",False,"if get_setting ( ""CREATE_MISSING_SWITCHES"" ) :",if self . active :,0.8820016898747209,2.099844458473431
"def add_requirements(self, requirements): if self._legacy: self._legacy.add_requirements(requirements) else: run_requires = self._data.setdefault(""run_requires"", []) always = None for entry in run_requires: <mask>: always = entry break if always is None: always = {""requires"": requirements} run_requires.insert(0, always) else: rset = set(always[""requires""]) | set(requirements) always[""requires""] = sorted(rset)",False,"if ""environment"" not in entry and ""extra"" not in entry :","if entry [ ""requires"" ] == requirements :",0.8820016898747209,3.895748804295674
"def display_failures_for_single_test(result: TestResult) -> None: """"""Display a failure for a single method / endpoint."""""" display_subsection(result) checks = _get_unique_failures(result.checks) for idx, check in enumerate(checks, 1): message: Optional[str] if check.message: message = f""{idx}. {check.message}"" else: message = None example = cast(Case, check.example) # filtered in `_get_unique_failures` display_example(example, check.name, message, result.seed) # Display every time except the last check <mask>: click.echo(""\n"")",False,if idx != len ( checks ) :,"if check . name == ""test"" :",0.8820016898747209,5.522397783539471
"def __call__(self, frame: FrameType, event: str, arg: Any) -> ""CallTracer"": code = frame.f_code if ( event not in SUPPORTED_EVENTS or code.co_name == ""trace_types"" or self.should_trace and not self.should_trace(code) ): return self try: if event == EVENT_CALL: self.handle_call(frame) <mask>: self.handle_return(frame, arg) else: logger.error(""Cannot handle event %s"", event) except Exception: logger.exception(""Failed collecting trace"") return self",True,elif event == EVENT_RETURN :,elif event == EVENT_RETURN :,0.8820016898747209,100.00000000000004
"def get_maps(test): pages = set() for addr in test[""pre""][""memory""].keys(): pages.add(addr >> 12) for addr in test[""pos""][""memory""].keys(): pages.add(addr >> 12) maps = [] for p in sorted(pages): <mask>: maps[-1] = (maps[-1][0], maps[-1][1] + 0x1000) else: maps.append((p << 12, 0x1000)) return maps",False,if len ( maps ) > 0 and maps [ - 1 ] [ 0 ] + maps [ - 1 ] [ 1 ] == p << 12 :,if p == 0 :,0.8820016898747209,0.28037993823818674
"def process_rotate_aes_key(self): if hasattr(self.options, ""rotate_aes_key"") and isinstance( self.options.rotate_aes_key, six.string_types ): <mask>: self.options.rotate_aes_key = True elif self.options.rotate_aes_key.lower() == ""false"": self.options.rotate_aes_key = False",True,"if self . options . rotate_aes_key . lower ( ) == ""true"" :","if self . options . rotate_aes_key . lower ( ) == ""true"" :",0.8820016898747209,100.00000000000004
"def apply_figure(self, figure): super(legend_text_legend, self).apply_figure(figure) properties = self.properties.copy() with suppress(KeyError): del properties[""margin""] with suppress(KeyError): texts = figure._themeable[""legend_text_legend""] for text in texts: <mask>: # textarea text = text._text text.set(**properties)",False,"if not hasattr ( text , ""_x"" ) :","if isinstance ( text , TextArea ) :",0.8820016898747209,16.409149280404737
"def tearDown(self): for i in range(len(self.tree) - 1, -1, -1): s = os.path.join(self.root, self.tree[i]) <mask>: os.rmdir(s) else: os.remove(s) os.rmdir(self.root)",False,"if not ""."" in s :",if os . path . isdir ( s ) :,0.8820016898747209,5.934202609760488
"def _get_id(self, type, id): fields = id.split("":"") if len(fields) >= 3: if type != fields[-2]: logger.warning( ""Expected id of type %s but found type %s %s"", type, fields[-2], id ) return fields[-1] fields = id.split(""/"") if len(fields) >= 3: itype = fields[-2] <mask>: logger.warning( ""Expected id of type %s but found type %s %s"", type, itype, id ) return fields[-1].split(""?"")[0] return id",False,if type != itype :,if itype != fields [ - 1 ] :,0.8820016898747209,10.552670315936318
"def candidates() -> Generator[""Symbol"", None, None]: s = self if Symbol.debug_lookup: Symbol.debug_print(""searching in self:"") print(s.to_string(Symbol.debug_indent + 1), end="""") while True: <mask>: yield s if recurseInAnon: yield from s.children_recurse_anon else: yield from s._children if s.siblingAbove is None: break s = s.siblingAbove if Symbol.debug_lookup: Symbol.debug_print(""searching in sibling:"") print(s.to_string(Symbol.debug_indent + 1), end="""")",False,if matchSelf :,if s . siblingAbove is None :,0.8820016898747209,7.809849842300637
"def records(account_id): """"""Fetch locks data"""""" s = boto3.Session() table = s.resource(""dynamodb"").Table(""Sphere11.Dev.ResourceLocks"") results = table.scan() for r in results[""Items""]: if ""LockDate"" in r: r[""LockDate""] = datetime.fromtimestamp(r[""LockDate""]) <mask>: r[""RevisionDate""] = datetime.fromtimestamp(r[""RevisionDate""]) print(tabulate.tabulate(results[""Items""], headers=""keys"", tablefmt=""fancy_grid""))",True,"if ""RevisionDate"" in r :","if ""RevisionDate"" in r :",0.8820016898747209,100.00000000000004
"def _handle_errors(errors): """"""Log out and possibly reraise errors during import."""""" if not errors: return log_all = True # pylint: disable=unused-variable err_msg = ""T2T: skipped importing {num_missing} data_generators modules."" print(err_msg.format(num_missing=len(errors))) for module, err in errors: err_str = str(err) if log_all: print(""Did not import module: %s; Cause: %s"" % (module, err_str)) <mask>: print(""From module %s"" % module) raise err",False,"if not _is_import_err_msg ( err_str , module ) :","if not module . startswith ( ""data_generators."" ) :",0.8820016898747209,6.942470526671953
"def find_needle(self, tree, focused=None): if isinstance(tree, list): for el in tree: res = self.find_needle(el, focused) <mask>: return res elif isinstance(tree, dict): nodes = tree.get(""nodes"", []) + tree.get(""floating_nodes"", []) if focused: for node in nodes: if node[""id""] == focused[""id""]: return tree elif tree[""focused""]: return tree return self.find_needle(nodes, focused) return {}",True,if res :,if res :,0.8820016898747209,0.0
"def available_datasets(self): """"""Automatically determine datasets provided by this file"""""" res = self.resolution coordinates = [""pixel_longitude"", ""pixel_latitude""] for var_name, val in self.file_content.items(): if isinstance(val, netCDF4.Variable): ds_info = { ""file_type"": self.filetype_info[""file_type""], ""resolution"": res, } <mask>: ds_info[""coordinates""] = coordinates yield DatasetID(name=var_name, resolution=res), ds_info",False,if not self . is_geo :,if coordinates :,0.8820016898747209,0.0
"def get_subkeys(self, key): # TODO: once we revamp the registry emulation, # make this better parent_path = key.get_path() subkeys = [] for k in self.keys: test_path = k.get_path() <mask>: sub = test_path[len(parent_path) :] if sub.startswith(""\\""): sub = sub[1:] end_slash = sub.find(""\\"") if end_slash >= 0: sub = sub[:end_slash] if not sub: continue subkeys.append(sub) return subkeys",False,if test_path . lower ( ) . startswith ( parent_path . lower ( ) ) :,if test_path . startswith ( parent_path ) :,0.8820016898747209,39.58936700221689
"def default(self, o): try: if type(o) == datetime.datetime: return str(o) else: # remove unwanted attributes from the provider object during conversion to json if hasattr(o, ""profile""): del o.profile <mask>: del o.credentials if hasattr(o, ""metadata_path""): del o.metadata_path if hasattr(o, ""services_config""): del o.services_config return vars(o) except Exception as e: return str(o)",True,"if hasattr ( o , ""credentials"" ) :","if hasattr ( o , ""credentials"" ) :",0.8820016898747209,100.00000000000004
"def submit(self, fn, *args, **kwargs): with self._shutdown_lock: <mask>: raise RuntimeError(""cannot schedule new futures after shutdown"") f = _base.Future() w = _WorkItem(f, fn, args, kwargs) self._work_queue.put(w) self._adjust_thread_count() return f",False,if self . _shutdown :,if self . _shutdown_thread :,0.8820016898747209,54.10822690539397
"def __viewerKeyPress(viewer, event): view = viewer.view() if not isinstance(view, GafferSceneUI.SceneView): return False if event == __editSourceKeyPress: selectedPath = __sceneViewSelectedPath(view) <mask>: __editSourceNode(view.getContext(), view[""in""], selectedPath) return True elif event == __editTweaksKeyPress: selectedPath = __sceneViewSelectedPath(view) if selectedPath is not None: __editTweaksNode(view.getContext(), view[""in""], selectedPath) return True",True,if selectedPath is not None :,if selectedPath is not None :,0.8820016898747209,100.00000000000004
"def _split_to_option_groups_and_paths(self, args): opt_groups = [] current = [] for arg in args: <mask>: opts = self._arg_parser.parse_args(current)[0] opt_groups.append(opts) current = [] else: current.append(arg) if opt_groups: return opt_groups, current raise ValueError(""Nothing to split"")",False,"if arg . replace ( ""-"" , """" ) == """" and len ( arg ) >= 3 :",if arg in self . _arg_parser . parse_args ( current ) :,0.8820016898747209,3.907788852962481
"def _on_change(self): changed = False self.save() for key, value in self.data.items(): if isinstance(value, bool): <mask>: changed = True break if isinstance(value, int): if value != 1: changed = True break elif value is None: continue elif len(value) != 0: changed = True break self._reset_button.disabled = not changed",False,if value :,if value != 0 :,0.8820016898747209,17.965205598154213
"def wait_for_child(pid, timeout=1.0): deadline = mitogen.core.now() + timeout while timeout < mitogen.core.now(): try: target_pid, status = os.waitpid(pid, os.WNOHANG) if target_pid == pid: return except OSError: e = sys.exc_info()[1] <mask>: return time.sleep(0.05) assert False, ""wait_for_child() timed out""",False,if e . args [ 0 ] == errno . ECHILD :,if e . errno == errno . EINTR :,0.8820016898747209,29.10042507378281
"def _get_os_version_lsb_release(): try: output = subprocess.check_output(""lsb_release -sri"", shell=True) lines = output.strip().split() name, version = lines <mask>: version = """" return name, version except: return _get_os_version_uname()",False,"if version . lower ( ) == ""rolling"" :","if version == """" :",0.8820016898747209,17.58943312560456
"def _check_snapshot_status_healthy(self, snapshot_uuid): status = """" try: while True: status, locked = self._get_snapshot_status(snapshot_uuid) <mask>: break eventlet.sleep(2) except Exception: with excutils.save_and_reraise_exception(): LOG.exception(""Failed to get snapshot status. [%s]"", snapshot_uuid) LOG.debug( ""Lun [%(snapshot)s], status [%(status)s]."", {""snapshot"": snapshot_uuid, ""status"": status}, ) return status == ""Healthy""",False,if not locked :,if locked :,0.8820016898747209,0.0
"def CountButtons(self): """"""Returns the number of visible buttons in the docked pane."""""" n = 0 if self.HasCaption() or self.HasCaptionLeft(): if isinstance(wx.GetTopLevelParent(self.window), AuiFloatingFrame): return 1 <mask>: n += 1 if self.HasMaximizeButton(): n += 1 if self.HasMinimizeButton(): n += 1 if self.HasPinButton(): n += 1 return n",False,if self . HasCloseButton ( ) :,if self . HasMinimizeButton ( ) :,0.8820016898747209,41.11336169005196
"def _url_encode_impl(obj, charset, encode_keys, sort, key): from .datastructures import iter_multi_items iterable = iter_multi_items(obj) if sort: iterable = sorted(iterable, key=key) for key, value in iterable: <mask>: continue if not isinstance(key, bytes): key = text_type(key).encode(charset) if not isinstance(value, bytes): value = text_type(value).encode(charset) yield _fast_url_quote_plus(key) + ""="" + _fast_url_quote_plus(value)",False,if value is None :,if key in encode_keys :,0.8820016898747209,7.809849842300637
"def get_response(self, exc_fmt=None): self.callback = None if __debug__: self.parent._log(3, ""%s:%s.ready.wait"" % (self.name, self.tag)) self.ready.wait() if self.aborted is not None: typ, val = self.aborted <mask>: exc_fmt = ""%s - %%s"" % typ raise typ(exc_fmt % str(val)) return self.response",True,if exc_fmt is None :,if exc_fmt is None :,0.8820016898747209,100.00000000000004
"def extract_items(self): responses = self.fetch() items = [] for response in responses: page_key = response.meta.get(""page_key"") or response.url item = {""key"": page_key, ""items"": None, ""templates"": None} extracted_items = [ dict(i) for i in self.spider.parse(response) if not isinstance(i, Request) ] <mask>: item[""items""] = extracted_items item[""templates""] = [ i[""_template""] for i in extracted_items if i.get(""_template"") ] items.append(item) return items",False,if extracted_items :,if len ( extracted_items ) > 0 :,0.8820016898747209,17.747405280050266
"def fit_one(self, x): for i, xi in x.items(): if self.with_centering: self.median[i].update(xi) <mask>: self.iqr[i].update(xi) return self",False,if self . with_scaling :,elif self . with_iqr :,0.8820016898747209,43.47208719449914
"def find_word_bounds(self, text, index, allowed_chars): right = left = index done = False while not done: if left == 0: done = True <mask>: left -= 1 else: done = True done = False while not done: if right == len(text): done = True elif not self.word_boundary_char(text[right]): right += 1 else: done = True return left, right",False,elif not self . word_boundary_char ( text [ left - 1 ] ) :,if not self . word_boundary_char ( text [ left ] ) :,0.8820016898747209,71.9548353625319
"def _validate_duplicate_detection_history_time_window(namespace): if namespace.duplicate_detection_history_time_window: if iso8601pattern.match(namespace.duplicate_detection_history_time_window): pass <mask>: pass else: raise CLIError( ""--duplicate-detection-history-time-window Value Error : {0} value is not in ISO 8601 timespan / duration format. e.g. PT10M for duration of 10 min or 00:10:00 for duration of 10 min"".format( namespace.duplicate_detection_history_time_window ) )",False,elif timedeltapattern . match ( namespace . duplicate_detection_history_time_window ) :,elif durationpattern . match ( namespace . duplicate_detection_history_time_window ) :,0.8820016898747209,89.15993127600096
"def get_subkeys(self, key): # TODO: once we revamp the registry emulation, # make this better parent_path = key.get_path() subkeys = [] for k in self.keys: test_path = k.get_path() if test_path.lower().startswith(parent_path.lower()): sub = test_path[len(parent_path) :] if sub.startswith(""\\""): sub = sub[1:] end_slash = sub.find(""\\"") if end_slash >= 0: sub = sub[:end_slash] <mask>: continue subkeys.append(sub) return subkeys",False,if not sub :,"if sub == ""/"" :",0.8820016898747209,7.267884212102741
"def generator(self, data): <mask>: silent_vars = self._get_silent_vars() for task in data: for var, val in task.environment_variables(): if self._config.SILENT: if var in silent_vars: continue yield ( 0, [ int(task.UniqueProcessId), str(task.ImageFileName), Address(task.Peb.ProcessParameters.Environment), str(var), str(val), ], )",False,if self . _config . SILENT :,if self . _config . SLENT :,0.8820016898747209,70.71067811865478
"def start_requests(self): if self.fail_before_yield: 1 / 0 for s in range(100): qargs = {""total"": 10, ""seed"": s} url = self.mockserver.url(""/follow?%s"") % urlencode(qargs, doseq=1) yield Request(url, meta={""seed"": s}) <mask>: 2 / 0 assert self.seedsseen, ""All start requests consumed before any download happened""",False,if self . fail_yielding :,if self . fail_before_yield :,0.8820016898747209,46.713797772819994
"def populateGridlines(self): cTicks = self.getSystemCurve(self.ticksId) cGridlines = self.getSystemCurve(self.gridlinesId) cGridlines.clearPoints() nTicks = cTicks.getNPoints() for iTick in range(nTicks): <mask>: p = cTicks.getPoint(iTick) cGridlines.addPoint(p.getX(), p.getY())",False,if self . hasGridlines and ( iTick % self . ticksPerGridline ) == 0 :,if cGridlines . hasPoint ( iTick ) :,0.8820016898747209,5.34741036489421
"def handle_before_events(request, event_list): if not event_list: return """" if not hasattr(event_list, ""__iter__""): project = event_list.project event_list = [event_list] else: projects = set(e.project for e in event_list) <mask>: project = projects.pop() else: project = None for plugin in plugins.for_project(project): safe_execute(plugin.before_events, request, event_list) return """"",False,if len ( projects ) == 1 :,if projects :,0.8820016898747209,0.0
"def handle_parse_result(self, ctx, opts, args): if self.name in opts: <mask>: self._raise_exclusive_error() if self.multiple and len(set(opts[self.name])) > 1: self._raise_exclusive_error() return super(MutuallyExclusiveOption, self).handle_parse_result(ctx, opts, args)",False,if self . mutually_exclusive . intersection ( opts ) :,if self . multiple and len ( opts [ self . name ] ) > 1 :,0.8820016898747209,12.03921753741131
"def current_word(cursor_offset, line): """"""the object.attribute.attribute just before or under the cursor"""""" pos = cursor_offset start = pos end = pos word = None for m in current_word_re.finditer(line): <mask>: start = m.start(1) end = m.end(1) word = m.group(1) if word is None: return None return LinePart(start, end, word)",False,if m . start ( 1 ) < pos and m . end ( 1 ) >= pos :,"if m . group ( 0 ) == ""word"" :",0.8820016898747209,8.294578358343248
"def query_to_script_path(path, query): if path != ""*"": script = os.path.join(path, query.split("" "")[0]) <mask>: raise IOError(""Script '{}' not found in script directory"".format(query)) return os.path.join(path, query).split("" "") return query",True,if not os . path . exists ( script ) :,if not os . path . exists ( script ) :,0.8820016898747209,100.00000000000004
"def expand(self, pbegin): # TODO(b/151921205): we have to do an identity map for unmodified # PCollections below because otherwise we get an error from beam. identity_map = ""Identity"" >> beam.Map(lambda x: x) if self._dataset_key.is_flattened_dataset_key(): <mask>: return self._flat_pcollection | identity_map else: return list( self._pcollection_dict.values() ) | ""FlattenAnalysisInputs"" >> beam.Flatten(pipeline=pbegin.pipeline) else: return self._pcollection_dict[self._dataset_key] | identity_map",True,if self . _flat_pcollection :,if self . _flat_pcollection :,0.8820016898747209,100.00000000000004
"def processCoords(coords): newcoords = deque() for (x, y, z) in coords: for _dir, offsets in faceDirections: if _dir == FaceYIncreasing: continue dx, dy, dz = offsets p = (x + dx, y + dy, z + dz) if p not in box: continue nx, ny, nz = p <mask>: level.setBlockAt(nx, ny, nz, waterID) newcoords.append(p) return newcoords",False,"if level . blockAt ( nx , ny , nz ) == 0 :",if nx != ny :,0.8820016898747209,2.708196419295002
"def delete_byfilter(userId, remove=True, session=None, **dbfilter): if not session: session = db.Session ret = False results = session.query(ObjectStorageMetadata).filter_by(**dbfilter) if results: for result in results: <mask>: session.delete(result) else: result.update( { ""record_state_key"": ""to_delete"", ""record_state_val"": str(time.time()), } ) ret = True return ret",True,if remove :,if remove :,0.8820016898747209,0.0
"def fields(self, fields): fields_xml = """" for field in fields: field_dict = DEFAULT_FIELD.copy() field_dict.update(field) <mask>: field_dict[""required""] = ""true"" fields_xml += FIELD_XML_TEMPLATE % field_dict + ""\n"" self.xml = force_unicode( force_unicode(self.xml).replace( u""<!-- REPLACE FIELDS -->"", force_unicode(fields_xml) ) )",False,"if self . unique_key_field == field [ ""name"" ] :",if field . required :,0.8820016898747209,1.3704649608359576
"def get_all_users(self, access_token, timeout=None): if timeout is None: timeout = DEFAULT_TIMEOUT headers = self.retrieve_header(access_token) try: response = await self.standard_request( ""get"", ""/walkoff/api/users"", timeout=DEFAULT_TIMEOUT, headers=headers ) <mask>: resp = await response.json() return resp, ""Success"" else: return ""Invalid Credentials"" except asyncio.CancelledError: return False, ""TimedOut""",True,if response . status == 200 :,if response . status == 200 :,0.8820016898747209,100.00000000000004
"def set_val(): idx = 0 for idx in range(0, len(model)): row = model[idx] if value and row[0] == value: break <mask>: idx = -1 os_widget.set_active(idx) if idx == -1: os_widget.set_active(0) if idx >= 0: return row[1] if self.show_all_os: return None",False,if idx == len ( os_widget . get_model ( ) ) - 1 :,if idx == len ( model ) - 1 :,0.8820016898747209,34.72495173503991
"def translate_module_name(module: str, relative: int) -> Tuple[str, int]: for pkg in VENDOR_PACKAGES: for alt in ""six.moves"", ""six"": substr = ""{}.{}"".format(pkg, alt) if module.endswith(""."" + substr) or (module == substr and relative): return alt, 0 <mask>: return alt + ""."" + module.partition(""."" + substr + ""."")[2], 0 return module, relative",False,"if ""."" + substr + ""."" in module :","elif module . startswith ( ""."" + substr ) :",0.8820016898747209,32.80424072511114
"def escape(m): all, tail = m.group(0, 1) assert all.startswith(""\\"") esc = simple_escapes.get(tail) if esc is not None: return esc if tail.startswith(""x""): hexes = tail[1:] <mask>: raise ValueError(""invalid hex string escape ('\\%s')"" % tail) try: i = int(hexes, 16) except ValueError: raise ValueError(""invalid hex string escape ('\\%s')"" % tail) else: try: i = int(tail, 8) except ValueError: raise ValueError(""invalid octal string escape ('\\%s')"" % tail) return chr(i)",False,if len ( hexes ) < 2 :,"if hexes == ""0x"" :",0.8820016898747209,7.267884212102741
"def __get_k8s_container_name(self, job_wrapper): # These must follow a specific regex for Kubernetes. raw_id = job_wrapper.job_destination.id if isinstance(raw_id, str): cleaned_id = re.sub(""[^-a-z0-9]"", ""-"", raw_id) <mask>: cleaned_id = ""x%sx"" % cleaned_id return cleaned_id return ""job-container""",False,"if cleaned_id . startswith ( ""-"" ) or cleaned_id . endswith ( ""-"" ) :","if cleaned_id . startswith ( ""x"" ) :",0.8820016898747209,29.384508067883136
"def _power_exact(y, xc, yc, xe): yc, ye = y.int, y.exp while yc % 10 == 0: yc //= 10 ye += 1 if xc == 1: xe *= yc while xe % 10 == 0: xe //= 10 ye += 1 if ye < 0: return None exponent = xe * 10 ** ye <mask>: xc = exponent else: xc = 0 return 5",False,if y and xe :,if xc == 1 :,0.8820016898747209,9.652434877402245
"def lpush(key, *vals, **kwargs): ttl = kwargs.get(""ttl"") cap = kwargs.get(""cap"") if not ttl and not cap: _client.lpush(key, *vals) else: pipe = _client.pipeline() pipe.lpush(key, *vals) <mask>: pipe.ltrim(key, 0, cap) if ttl: pipe.expire(key, ttl) pipe.execute()",True,if cap :,if cap :,0.8820016898747209,0.0
"def render_headers(self) -> bytes: if not hasattr(self, ""_headers""): parts = [ b""Content-Disposition: form-data; "", format_form_param(""name"", self.name), ] if self.filename: filename = format_form_param(""filename"", self.filename) parts.extend([b""; "", filename]) <mask>: content_type = self.content_type.encode() parts.extend([b""\r\nContent-Type: "", content_type]) parts.append(b""\r\n\r\n"") self._headers = b"""".join(parts) return self._headers",False,if self . content_type is not None :,if self . content_type :,0.8820016898747209,54.77927682341229
"def validate_custom_field_data(field_type: int, field_data: ProfileFieldData) -> None: try: <mask>: # Choice type field must have at least have one choice if len(field_data) < 1: raise JsonableError(_(""Field must have at least one choice."")) validate_choice_field_data(field_data) elif field_type == CustomProfileField.EXTERNAL_ACCOUNT: validate_external_account_field_data(field_data) except ValidationError as error: raise JsonableError(error.message)",False,if field_type == CustomProfileField . CHOICE :,if field_type == CustomProfileField . Choice :,0.8820016898747209,78.25422900366438
"def get_data(self, path): """"""Gross hack to contort loader to deal w/ load_*()'s bad API."""""" if self.file and path == self.path: <mask>: file = self.file else: self.file = file = open(self.path, ""r"") with file: # Technically should be returning bytes, but # SourceLoader.get_code() just passed what is returned to # compile() which can handle str. And converting to bytes would # require figuring out the encoding to decode to and # tokenize.detect_encoding() only accepts bytes. return file.read() else: return super().get_data(path)",False,if not self . file . closed :,"if isinstance ( self . file , str ) :",0.8820016898747209,17.747405280050266
"def handle_read(self): """"""Called when there is data waiting to be read."""""" try: chunk = self.recv(self.ac_in_buffer_size) except RetryError: pass except socket.error: self.handle_error() else: self.tot_bytes_received += len(chunk) <mask>: self.transfer_finished = True # self.close() # <-- asyncore.recv() already do that... return if self._data_wrapper is not None: chunk = self._data_wrapper(chunk) try: self.file_obj.write(chunk) except OSError as err: raise _FileReadWriteError(err)",False,if not chunk :,if self . tot_bytes_received >= self . transfer_size :,0.8820016898747209,2.908317710573757
"def _swig_extract_dependency_files(self, src): dep = [] for line in open(src): <mask>: line = line.split("" "")[1].strip(""""""'""\r\n"""""") if not (""<"" in line or line in dep): dep.append(line) return [i for i in dep if os.path.exists(i)]",False,"if line . startswith ( ""#include"" ) or line . startswith ( ""%include"" ) :","if line . startswith ( ""dependency:"" ) :",0.8820016898747209,23.67900729725624
"def buffer(self, lines, scroll_end=True, scroll_if_editing=False): ""Add data to be displayed in the buffer."" self.values.extend(lines) if scroll_end: if not self.editing: self.start_display_at = len(self.values) - len(self._my_widgets) <mask>: self.start_display_at = len(self.values) - len(self._my_widgets)",False,elif scroll_if_editing :,if scroll_if_editing :,0.8820016898747209,80.91067115702207
"def test_getline(self): with tokenize.open(self.file_name) as fp: for index, line in enumerate(fp): <mask>: line += ""\n"" cached_line = linecache.getline(self.file_name, index + 1) self.assertEqual(line, cached_line)",False,"if not line . endswith ( ""\n"" ) :","if not line . startswith ( ""\n"" ) :",0.8820016898747209,73.48889200874659
"def selectRow(self, rowNumber, highlight=None): if rowNumber == ""h"": rowNumber = 0 else: rowNumber = int(rowNumber) + 1 if 1 > rowNumber >= len(self.cells) + 1: raise Exception(""Invalid row number."") else: selected = self.cells[rowNumber][0].selected for cell in self.cells[rowNumber]: <mask>: if selected: cell.deselect() else: cell.select() else: if highlight: cell.mouseEnter() else: cell.mouseLeave()",False,if highlight is None :,if cell . selected :,0.8820016898747209,12.703318703865365
"def put(self, session): with sess_lock: self.parent.put(session) # Do not store the session if skip paths for sp in self.skip_paths: if request.path.startswith(sp): return <mask>: try: del self._cache[session.sid] except Exception: pass self._cache[session.sid] = session self._normalize()",True,if session . sid in self . _cache :,if session . sid in self . _cache :,0.8820016898747209,100.00000000000004
"def TryMerge(self, d): while d.avail() > 0: tt = d.getVarInt32() if tt == 10: length = d.getVarInt32() tmp = ProtocolBuffer.Decoder(d.buffer(), d.pos(), d.pos() + length) d.skip(length) self.add_status().TryMerge(tmp) continue <mask>: self.add_doc_id(d.getPrefixedString()) continue if tt == 0: raise ProtocolBuffer.ProtocolBufferDecodeError d.skipData(tt)",True,if tt == 18 :,if tt == 18 :,0.8820016898747209,100.00000000000004
"def extract(self, zip): max_nb = maxNbFile(self) for index, field in enumerate(zip.array(""file"")): <mask>: self.warning( ""ZIP archive contains many files, but only first %s files are processed"" % max_nb ) break self.processFile(field)",False,if max_nb is not None and max_nb <= index :,if index >= max_nb :,0.8820016898747209,10.59106218302618
"def get_norm(norm, out_channels): if isinstance(norm, str): <mask>: return None norm = { ""BN"": BatchNorm2d, ""GN"": lambda channels: nn.GroupNorm(32, channels), ""nnSyncBN"": nn.SyncBatchNorm, # keep for debugging """": lambda x: x, }[norm] return norm(out_channels)",False,if len ( norm ) == 0 :,"if norm == ""nn"" :",0.8820016898747209,12.256200970377108
"def execute(self): if self._dirty or not self._qr: model_class = self.model_class query_meta = self.get_query_meta() if self._tuples: ResultWrapper = TuplesQueryResultWrapper elif self._dicts: ResultWrapper = DictQueryResultWrapper elif self._naive or not self._joins or self.verify_naive(): ResultWrapper = NaiveQueryResultWrapper <mask>: ResultWrapper = AggregateQueryResultWrapper else: ResultWrapper = ModelQueryResultWrapper self._qr = ResultWrapper(model_class, self._execute(), query_meta) self._dirty = False return self._qr else: return self._qr",False,elif self . _aggregate_rows :,elif self . _aggregate :,0.8820016898747209,56.98363775444274
"def emitIpToDomainsData(self, data, event): self.emitRawRirData(data, event) domains = data.get(""domains"") if isinstance(domains, list): for domain in domains: if self.checkForStop(): return None domain = domain.strip() <mask>: self.emitHostname(domain, event)",True,if domain :,if domain :,0.8820016898747209,0.0
"def delete(self): from weblate.trans.models import Change, Suggestion, Vote fast_deletes = [] for item in self.fast_deletes: <mask>: fast_deletes.append(Vote.objects.filter(suggestion__in=item)) fast_deletes.append(Change.objects.filter(suggestion__in=item)) fast_deletes.append(item) self.fast_deletes = fast_deletes return super().delete()",False,if item . model is Suggestion :,"if isinstance ( item , Suggestion ) :",0.8820016898747209,7.809849842300637
"def token(self): if not self._token: try: cookie_token = self.state[""request""].headers.cookie[CSRF_TOKEN].value except KeyError: cookie_token = """" <mask>: self._token = cookie_token else: self._token = get_random_string(TOKEN_LENGTH) return self._token",False,if len ( cookie_token ) == TOKEN_LENGTH :,if cookie_token :,0.8820016898747209,9.121704909091086
"def get_logs(last_file=None, last_time=None): try: response = client.get_logs(last_file=last_file, last_time=last_time) get_logs_streamer( show_timestamp=not hide_time, all_containers=all_containers, all_info=all_info, )(response) return response except (ApiException, HTTPError) as e: <mask>: handle_cli_error( e, message=""Could not get logs for run `{}`."".format(client.run_uuid), ) sys.exit(1)",False,if not follow :,if e . status_code == 404 :,0.8820016898747209,4.990049701936832
"def update(self, targets): Section.update(self, targets) outputNames = set() for target in targets: g = target.globals() outputNames.update([k for k in g.keys() if k.startswith(""output:"")]) rows = [] outputNames = sorted(outputNames) for outputName in outputNames: row = self.__rows.get(outputName) <mask>: row = _OutputRow(outputName) self.__rows[outputName] = row row.update(targets) row.setAlternate(len(rows) % 2) rows.append(row) self._mainColumn()[:] = rows",True,if row is None :,if row is None :,0.8820016898747209,100.00000000000004
"def getBranches(self): returned = [] for git_branch_line in self._executeGitCommandAssertSuccess(""branch"").stdout: if git_branch_line.startswith(""*""): git_branch_line = git_branch_line[1:] git_branch_line = git_branch_line.strip() <mask>: alias_name, aliased = git_branch_line.split(BRANCH_ALIAS_MARKER) returned.append(branch.LocalBranchAlias(self, alias_name, aliased)) else: returned.append(branch.LocalBranch(self, git_branch_line)) return returned",True,if BRANCH_ALIAS_MARKER in git_branch_line :,if BRANCH_ALIAS_MARKER in git_branch_line :,0.8820016898747209,100.00000000000004
"def has_bad_headers(self): headers = [self.sender, self.reply_to] + self.recipients for header in headers: if _has_newline(header): return True if self.subject: if _has_newline(self.subject): for linenum, line in enumerate(self.subject.split(""\r\n"")): if not line: return True <mask>: return True if _has_newline(line): return True if len(line.strip()) == 0: return True return False",False,"if linenum > 0 and line [ 0 ] not in ""\t "" :",if linenum == 0 :,0.8820016898747209,3.6462189126393114
"def resolve_references(self, note, reflist): assert len(note[""ids""]) == 1 id = note[""ids""][0] for ref in reflist: <mask>: continue ref.delattr(""refname"") ref[""refid""] = id assert len(ref[""ids""]) == 1 note.add_backref(ref[""ids""][0]) ref.resolved = 1 note.resolved = 1",False,if ref . resolved :,"if ref . get ( ""refname"" ) == id :",0.8820016898747209,12.35622127262679
"def pickPath(self, color): self.path[color] = () currentPos = self.starts[color] while True: minDist = None minGuide = None for guide in self.guides[color]: guideDist = dist(currentPos, guide) if minDist == None or guideDist < minDist: minDist = guideDist minGuide = guide <mask>: return if minGuide == None: return self.path[color] = self.path[color] + (minGuide,) currentPos = minGuide self.guides[color].remove(minGuide)",False,"if dist ( currentPos , self . ends [ color ] ) == 1 :",if minDist == None or minGuide == None :,0.8820016898747209,5.6578916063256015
"def __hierarchyViewKeyPress(hierarchyView, event): if event == __editSourceKeyPress: selectedPath = __hierarchyViewSelectedPath(hierarchyView) <mask>: __editSourceNode( hierarchyView.getContext(), hierarchyView.scene(), selectedPath ) return True elif event == __editTweaksKeyPress: selectedPath = __hierarchyViewSelectedPath(hierarchyView) if selectedPath is not None: __editTweaksNode( hierarchyView.getContext(), hierarchyView.scene(), selectedPath ) return True",True,if selectedPath is not None :,if selectedPath is not None :,0.8820016898747209,100.00000000000004
"def getSubsegments(self): for num, localdata in self.lfh.LocalData: for bucket, seginfo in localdata.SegmentInfo: <mask>: continue yield Win32Subsegment(self.trace, self.heap, seginfo.ActiveSubsegment)",True,if seginfo . ActiveSubsegment == 0 :,if seginfo . ActiveSubsegment == 0 :,0.8820016898747209,100.00000000000004
"def test_full_hd_bluray(self): cur_test = ""full_hd_bluray"" cur_qual = common.Quality.FULLHDBLURAY for name, tests in iteritems(self.test_cases): for test in tests: <mask>: self.assertEqual(cur_qual, common.Quality.name_quality(test)) else: self.assertNotEqual(cur_qual, common.Quality.name_quality(test))",True,if name == cur_test :,if name == cur_test :,0.8820016898747209,100.00000000000004
"def calc(self, arg): op = arg[""op""] if op == ""C"": self.clear() return str(self.current) num = decimal.Decimal(arg[""num""]) if self.op: if self.op == ""+"": self.current += num elif self.op == ""-"": self.current -= num elif self.op == ""*"": self.current *= num <mask>: self.current /= num self.op = op else: self.op = op self.current = num res = str(self.current) if op == ""="": self.clear() return res",True,"elif self . op == ""/"" :","elif self . op == ""/"" :",0.8820016898747209,100.00000000000004
"def strip_export_type(path): matched = re.search(r""#([a-zA-Z0-9\-]+\\+[a-zA-Z0-9\-]+)?$"", path.encode(""utf-8"")) mime_type = None if matched: fragment = matched.group(0) mime_type = matched.group(1) <mask>: mime_type = mime_type.replace(""+"", ""/"") path = path[: -len(fragment)] return (path, mime_type)",False,if mime_type is not None :,if fragment :,0.8820016898747209,0.0
"def _save_as_module(file, data, binary=False): if not data: return with open(file, ""w"") as f: f.write(""DATA="") <mask>: f.write('""') f.write(base64.b64encode(data).decode(""ascii"")) f.write('""') else: f.write(str(data).replace(""\\\\"", ""\\"")) f.flush()",True,if binary :,if binary :,0.8820016898747209,0.0
"def ProcessStringLiteral(self): if self._lastToken == None or self._lastToken.type == self.OpenBrace: text = super(JavaScriptBaseLexer, self).text <mask>: if len(self._scopeStrictModes) > 0: self._scopeStrictModes.pop() self._useStrictCurrent = True self._scopeStrictModes.append(self._useStrictCurrent)",False,"if text == '""use strict""' or text == ""'use strict'"" :",if text :,0.8820016898747209,0.0
"def run(self, ttl=None): self.zeroconf = zeroconf.Zeroconf() zeroconf.ServiceBrowser(self.zeroconf, self.domain, MDNSHandler(self)) if ttl: gobject.timeout_add(ttl * 1000, self.shutdown) self.__running = True self.__mainloop = gobject.MainLoop() context = self.__mainloop.get_context() while self.__running: try: <mask>: context.iteration(True) else: time.sleep(0.1) except KeyboardInterrupt: break self.zeroconf.close() logger.debug(""MDNSListener.run() quit"")",False,if context . pending ( ) :,if context . is_running ( ) :,0.8820016898747209,29.84745896009822
"def topology_change_notify(self, port_state): notice = False if port_state is PORT_STATE_FORWARD: for port in self.ports.values(): if port.role is DESIGNATED_PORT: notice = True break else: notice = True if notice: self.send_event(EventTopologyChange(self.dp)) <mask>: self._transmit_tc_bpdu() else: self._transmit_tcn_bpdu()",False,if self . is_root_bridge :,if port_state is PORT_STATE_FORWARD :,0.8820016898747209,5.604233375480572
def close_open_fds(keep=None): # noqa keep = [maybe_fileno(f) for f in (keep or []) if maybe_fileno(f) is not None] for fd in reversed(range(get_fdmax(default=2048))): if fd not in keep: try: os.close(fd) except OSError as exc: <mask>: raise,False,if exc . errno != errno . EBADF :,if exc . errno != errno . EAGAIN :,0.8820016898747209,78.25422900366438
"def collect_attributes(options, node, master_list): """"""Collect all attributes"""""" for ii in node.instructions: if field_check(ii, ""attributes""): s = getattr(ii, ""attributes"") if isinstance(s, list): for x in s: if x not in master_list: master_list.append(x) <mask>: master_list.append(s) for nxt in node.next.values(): collect_attributes(options, nxt, master_list)",False,elif s != None and s not in master_list :,"elif isinstance ( s , tuple ) :",0.8820016898747209,3.8902180856807296
"def remove_test_run_directories(expiry_time: int = 60 * 60) -> int: removed = 0 directories = glob.glob(os.path.join(UUID_VAR_DIR, ""test-backend"", ""run_*"")) for test_run in directories: <mask>: try: shutil.rmtree(test_run) removed += 1 except FileNotFoundError: pass return removed",False,if round ( time . time ( ) ) - os . path . getmtime ( test_run ) > expiry_time :,if time . time ( ) - expiry_time > 0 :,0.8820016898747209,17.673631000524313
"def read_work_titles(fields): found = [] if ""240"" in fields: for line in fields[""240""]: title = join_subfield_values(line, [""a"", ""m"", ""n"", ""p"", ""r""]) <mask>: found.append(title) if ""130"" in fields: for line in fields[""130""]: title = "" "".join(get_lower_subfields(line)) if title not in found: found.append(title) return {""work_titles"": found} if found else {}",True,if title not in found :,if title not in found :,0.8820016898747209,100.00000000000004
"def _process_v1_msg(prot, msg): header = None body = msg[1] if not isinstance(body, (binary_type, mmap, memoryview)): raise ValidationError(body, ""Body must be a bytestream."") if len(msg) > 2: header = msg[2] <mask>: raise ValidationError(header, ""Header must be a dict."") for k, v in header.items(): header[k] = msgpack.unpackb(v) ctx = MessagePackMethodContext(prot, MessagePackMethodContext.SERVER) ctx.in_string = [body] ctx.transport.in_header = header return ctx",True,"if not isinstance ( header , dict ) :","if not isinstance ( header , dict ) :",0.8820016898747209,100.00000000000004
"def find(self, node): typename = type(node).__name__ method = getattr(self, ""find_{}"".format(typename), None) if method is None: fields = getattr(node, ""_fields"", None) <mask>: return for field in fields: value = getattr(node, field) for result in self.find(value): yield result else: for result in method(node): yield result",True,if fields is None :,if fields is None :,0.8820016898747209,100.00000000000004
"def _str_param_list(self, name): out = [] if self[name]: out += self._str_header(name) for param in self[name]: parts = [] <mask>: parts.append(param.name) if param.type: parts.append(param.type) out += ["" : "".join(parts)] if param.desc and """".join(param.desc).strip(): out += self._str_indent(param.desc) out += [""""] return out",True,if param . name :,if param . name :,0.8820016898747209,100.00000000000004
"def _get_image(self, image_list, source): if source.startswith(""wx""): img = wx.ArtProvider_GetBitmap(source, wx.ART_OTHER, _SIZE) else: path = os.path.join(_BASE, source) <mask>: img = wx.Image(path, wx.BITMAP_TYPE_GIF).ConvertToBitmap() else: img = wx.Image(path, wx.BITMAP_TYPE_PNG).ConvertToBitmap() return image_list.Add(img)",False,"if source . endswith ( ""gif"" ) :",if os . path . isfile ( path ) :,0.8820016898747209,10.552670315936318
"def change_opacity_function(self, new_f): self.opacity_function = new_f dr = self.radius / self.num_levels sectors = [] for submob in self.submobjects: <mask>: sectors.append(submob) for (r, submob) in zip(np.arange(0, self.radius, dr), sectors): if type(submob) != AnnularSector: # it's the shadow, don't dim it continue alpha = self.opacity_function(r) submob.set_fill(opacity=alpha)",False,if type ( submob ) == AnnularSector :,if submob . get_shape ( ) == self . shape :,0.8820016898747209,13.06511329838856
"def _sqlite_post_configure_engine(url, engine, follower_ident): from sqlalchemy import event @event.listens_for(engine, ""connect"") def connect(dbapi_connection, connection_record): # use file DBs in all cases, memory acts kind of strangely # as an attached <mask>: dbapi_connection.execute('ATTACH DATABASE ""test_schema.db"" AS test_schema') else: dbapi_connection.execute( 'ATTACH DATABASE ""%s_test_schema.db"" AS test_schema' % follower_ident )",False,if not follower_ident :,"if follower_ident == ""file"" :",0.8820016898747209,17.747405280050266
"def apply_conf_file(fn, conf_filename): for env in LSF_CONF_ENV: conf_file = get_conf_file(conf_filename, env) if conf_file: with open(conf_file) as conf_handle: value = fn(conf_handle) <mask>: return value return None",False,if value :,if value is not None :,0.8820016898747209,17.965205598154213
"def test_call_extern_c_fn(self): global memcmp memcmp = cffi_support.ExternCFunction( ""memcmp"", (""int memcmp ( const uint8_t * ptr1, "" ""const uint8_t * ptr2, size_t num )""), ) @udf(BooleanVal(FunctionContext, StringVal, StringVal)) def fn(context, a, b): if a.is_null != b.is_null: return False if a is None: return True <mask>: return False if a.ptr == b.ptr: return True return memcmp(a.ptr, b.ptr, a.len) == 0",False,if len ( a ) != b . len :,if b is None :,0.8820016898747209,4.234348806659263
"def _get_initialized_app(app): """"""Returns a reference to an initialized App instance."""""" if app is None: return firebase_admin.get_app() if isinstance(app, firebase_admin.App): initialized_app = firebase_admin.get_app(app.name) <mask>: raise ValueError( ""Illegal app argument. App instance not "" ""initialized via the firebase module."" ) return app raise ValueError( ""Illegal app argument. Argument must be of type "" ' firebase_admin.App, but given ""{0}"".'.format(type(app)) )",False,if app is not initialized_app :,if initialized_app is not None :,0.8820016898747209,35.930411196308434
def compiled_query(self): <mask>: self.lazy_init_lock_.acquire() try: if self.compiled_query_ is None: self.compiled_query_ = CompiledQuery() finally: self.lazy_init_lock_.release() return self.compiled_query_,True,if self . compiled_query_ is None :,if self . compiled_query_ is None :,0.8820016898747209,100.00000000000004
"def clean_subevent(event, subevent): if event.has_subevents: <mask>: raise ValidationError(_(""Subevent cannot be null for event series."")) if event != subevent.event: raise ValidationError(_(""The subevent does not belong to this event."")) else: if subevent: raise ValidationError(_(""The subevent does not belong to this event.""))",False,if not subevent :,if subevent is None :,0.8820016898747209,14.058533129758727
"def get_blob_type_declaration_sql(self, column): length = column.get(""length"") if length: if length <= self.LENGTH_LIMIT_TINYBLOB: return ""TINYBLOB"" if length <= self.LENGTH_LIMIT_BLOB: return ""BLOB"" <mask>: return ""MEDIUMBLOB"" return ""LONGBLOB""",True,if length <= self . LENGTH_LIMIT_MEDIUMBLOB :,if length <= self . LENGTH_LIMIT_MEDIUMBLOB :,0.8820016898747209,100.00000000000004
"def decompress(self, data): if not data: return data if not self._first_try: return self._obj.decompress(data) self._data += data try: decompressed = self._obj.decompress(data) <mask>: self._first_try = False self._data = None return decompressed except zlib.error: self._first_try = False self._obj = zlib.decompressobj(-zlib.MAX_WBITS) try: return self.decompress(self._data) finally: self._data = None",False,if decompressed :,if decompressed is None :,0.8820016898747209,23.643540225079384
"def _record_event(self, path, fsevent_handle, filename, events, error): with self.lock: self.events[path].append(events) if events | pyuv.fs.UV_RENAME: <mask>: self.watches.pop(path).close()",False,if not os . path . exists ( path ) :,if self . watches [ path ] :,0.8820016898747209,5.367626065580593
"def __init__(self, duration, batch_shape, event_shape, validate_args=None): if duration is None: <mask>: # Infer duration from event_shape. duration = event_shape[0] elif duration != event_shape[0]: if event_shape[0] != 1: raise ValueError( ""duration, event_shape mismatch: {} vs {}"".format(duration, event_shape) ) # Infer event_shape from duration. event_shape = torch.Size((duration,) + event_shape[1:]) self._duration = duration super().__init__(batch_shape, event_shape, validate_args)",False,if event_shape [ 0 ] != 1 :,if event_shape is not None :,0.8820016898747209,25.124218547395092
"def _CheckPrerequisites(self): """"""Exits if any of the prerequisites is not met."""""" if not FLAGS.kubectl: raise Exception( ""Please provide path to kubectl tool using --kubectl "" ""flag. Exiting."" ) if not FLAGS.kubeconfig: raise Exception( ""Please provide path to kubeconfig using --kubeconfig "" ""flag. Exiting."" ) if self.disk_specs and self.disk_specs[0].disk_type == disk.STANDARD: <mask>: raise Exception( ""Please provide a list of Ceph Monitors using "" ""--ceph_monitors flag."" )",True,if not FLAGS . ceph_monitors :,if not FLAGS . ceph_monitors :,0.8820016898747209,100.00000000000004
"def invalidateDependentSlices(self, iFirstCurve): # only user defined curve can have slice dependency relationships if self.isSystemCurveIndex(iFirstCurve): return nCurves = self.getNCurves() for i in range(iFirstCurve, nCurves): c = self.getSystemCurve(i) <mask>: c.invalidate() elif i == iFirstCurve: # if first curve isn't a slice, break # there are no dependent slices",False,"if isinstance ( c . getSymbol ( ) . getSymbolType ( ) , SymbolType . PieSliceSymbolType ) :",if c . isDependent ( ) :,0.8820016898747209,4.726210391502949
"def find_backwards(self, offset): try: for _, token_type, token_value in reversed(self.tokens[self.offset : offset]): if token_type in (""comment"", ""linecomment""): try: prefix, comment = token_value.split(None, 1) except ValueError: continue <mask>: return [comment.rstrip()] return [] finally: self.offset = offset",False,if prefix in self . comment_tags :,"if prefix == ""linecomment"" :",0.8820016898747209,10.786826322527466
"def parse_column_definitions(self, elem): for column_elem in elem.findall(""column""): name = column_elem.get(""name"", None) assert name is not None, ""Required 'name' attribute missing from column def"" index = column_elem.get(""index"", None) assert index is not None, ""Required 'index' attribute missing from column def"" index = int(index) self.columns[name] = index <mask>: self.largest_index = index assert ""value"" in self.columns, ""Required 'value' column missing from column def"" if ""name"" not in self.columns: self.columns[""name""] = self.columns[""value""]",True,if index > self . largest_index :,if index > self . largest_index :,0.8820016898747209,100.00000000000004
"def __find_smallest(self): """"""Find the smallest uncovered value in the matrix."""""" minval = sys.maxsize for i in range(self.n): for j in range(self.n): if (not self.row_covered[i]) and (not self.col_covered[j]): <mask>: minval = self.C[i][j] return minval",False,if minval > self . C [ i ] [ j ] :,if self . C [ i ] [ j ] < minval :,0.8820016898747209,69.62269175492561
"def includes_tools_for_display_in_tool_panel(self): if self.includes_tools: tool_dicts = self.metadata[""tools""] for tool_dict in tool_dicts: <mask>: return True return False",False,"if tool_dict . get ( ""add_to_tool_panel"" , True ) :","if tool_dict [ ""display_in_tool_panel"" ] :",0.8820016898747209,32.09581132711523
"def commit(self, notify=False): if self.editing: text = self._text if text: try: value = self.type(text) except ValueError: return value = self.clamp_value(value) else: value = self.empty if value is NotImplemented: return self.value = value self.insertion_point = None <mask>: self.change_text(unicode(value)) else: self._text = unicode(value) self.editing = False else: self.insertion_point = None",True,if notify :,if notify :,0.8820016898747209,0.0
"def GeneratePageMetatadata(self, task): address_space = self.session.GetParameter(""default_address_space"") for vma in task.mm.mmap.walk_list(""vm_next""): start = vma.vm_start end = vma.vm_end # Skip the entire region. if end < self.plugin_args.start: continue # Done. <mask>: break for vaddr in utils.xrange(start, end, 0x1000): if self.plugin_args.start <= vaddr <= self.plugin_args.end: yield vaddr, self._CreateMetadata(address_space.describe_vtop(vaddr))",False,if start > self . plugin_args . end :,if start == self . plugin_args . end :,0.8820016898747209,67.0422683816333
"def _check_for_duplicate_host_entries(self, task_entries): non_host_statuses = ( models.HostQueueEntry.Status.PARSING, models.HostQueueEntry.Status.ARCHIVING, ) for task_entry in task_entries: using_host = ( task_entry.host is not None and task_entry.status not in non_host_statuses ) <mask>: self._assert_host_has_no_agent(task_entry)",True,if using_host :,if using_host :,0.8820016898747209,100.00000000000004
"def get_biggest_wall_time(jsons): lowest_wall = None for j in jsons: <mask>: lowest_wall = j[""wall_time""] if lowest_wall < j[""wall_time""]: lowest_wall = j[""wall_time""] return lowest_wall",True,if lowest_wall is None :,if lowest_wall is None :,0.8820016898747209,100.00000000000004
"def log_change_report(self, old_value, new_value, include_details=False): from octoprint.util import map_boolean with self._check_mutex: self._logger.info( ""Connectivity changed from {} to {}"".format( map_boolean(old_value, ""online"", ""offline""), map_boolean(new_value, ""online"", ""offline""), ) ) <mask>: self.log_details()",True,if include_details :,if include_details :,0.8820016898747209,100.00000000000004
"def _include_block(self, value, context=None): if hasattr(value, ""render_as_block""): <mask>: new_context = context.get_all() else: new_context = {} return jinja2.Markup(value.render_as_block(context=new_context)) return jinja2.Markup(value)",True,if context :,if context :,0.8820016898747209,0.0
"def __lt__(self, other): # 0: clock 1: timestamp 3: process id try: A, B = self[0], other[0] # uses logical clock value first if A and B: # use logical clock if available <mask>: # equal clocks use lower process id return self[2] < other[2] return A < B return self[1] < other[1] # ... or use timestamp except IndexError: return NotImplemented",True,if A == B :,if A == B :,0.8820016898747209,100.00000000000004
"def _get_port(): while True: port = 20000 + random.randint(1, 9999) for i in range(5): sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM) result = sock.connect_ex((""127.0.0.1"", port)) <mask>: continue else: return port",True,if result == 0 :,if result == 0 :,0.8820016898747209,100.00000000000004
"def fetch_all(self, api_client, fetchstatuslogger, q, targets): self.fetchstatuslogger = fetchstatuslogger if targets != None: # Ensure targets is a tuple if type(targets) != list and type(targets) != tuple: targets = tuple( targets, ) <mask>: targets = tuple(targets) for target in targets: self._fetch_targets(api_client, q, target)",False,elif type ( targets ) != tuple :,elif type ( targets ) == list :,0.8820016898747209,48.54917717073236
"def migrate_node_facts(facts): """"""Migrate facts from various roles into node"""""" params = { ""common"": (""dns_ip""), } if ""node"" not in facts: facts[""node""] = {} # pylint: disable=consider-iterating-dictionary for role in params.keys(): <mask>: for param in params[role]: if param in facts[role]: facts[""node""][param] = facts[role].pop(param) return facts",False,if role in facts :,if role in facts [ role ] :,0.8820016898747209,36.55552228545123
"def build_dimension_param(self, dimension, params): prefix = ""Dimensions.member"" i = 0 for dim_name in dimension: dim_value = dimension[dim_name] <mask>: if isinstance(dim_value, six.string_types): dim_value = [dim_value] for value in dim_value: params[""%s.%d.Name"" % (prefix, i + 1)] = dim_name params[""%s.%d.Value"" % (prefix, i + 1)] = value i += 1 else: params[""%s.%d.Name"" % (prefix, i + 1)] = dim_name i += 1",True,if dim_value :,if dim_value :,0.8820016898747209,100.00000000000004
"def add_if_unique(self, issuer, use, keys): if use in self.issuer_keys[issuer] and self.issuer_keys[issuer][use]: for typ, key in keys: flag = 1 for _typ, _key in self.issuer_keys[issuer][use]: <mask>: flag = 0 break if flag: self.issuer_keys[issuer][use].append((typ, key)) else: self.issuer_keys[issuer][use] = keys",False,if _typ == typ and key is _key :,if typ == key :,0.8820016898747209,14.628187563941417
"def run(self): while True: message = self.in_queue.get() <mask>: self.reset() elif message == EXIT: return else: index, transaction = message self.results_queue.put((index, self.validate(transaction)))",False,if message == RESET :,if message == ENTER :,0.8820016898747209,53.7284965911771
"def __run(self): threads = self.parameters()[""threads""].getTypedValue() with IECore.tbb_global_control( IECore.tbb_global_control.parameter.max_allowed_parallelism, IECore.hardwareConcurrency() if threads == 0 else threads, ): self._executeStartupFiles(self.root().getName()) # Append DEBUG message with process information to all messages defaultMessageHandler = IECore.MessageHandler.getDefaultHandler() <mask>: IECore.MessageHandler.setDefaultHandler( Gaffer.ProcessMessageHandler(defaultMessageHandler) ) return self._run(self.parameters().getValidatedValue())",False,"if not isinstance ( defaultMessageHandler , Gaffer . ProcessMessageHandler ) :",if defaultMessageHandler :,0.8820016898747209,0.0
"def adjust_uri(self, uri, relativeto): """"""Adjust the given ``uri`` based on the given relative URI."""""" key = (uri, relativeto) if key in self._uri_cache: return self._uri_cache[key] if uri[0] != ""/"": <mask>: v = self._uri_cache[key] = posixpath.join( posixpath.dirname(relativeto), uri ) else: v = self._uri_cache[key] = ""/"" + uri else: v = self._uri_cache[key] = uri return v",False,if relativeto is not None :,if relativeto :,0.8820016898747209,0.0
"def decoder(s): r = [] decode = [] for c in s: <mask>: decode.append(""&"") elif c == ""-"" and decode: if len(decode) == 1: r.append(""&"") else: r.append(modified_unbase64("""".join(decode[1:]))) decode = [] elif decode: decode.append(c) else: r.append(c) if decode: r.append(modified_unbase64("""".join(decode[1:]))) bin_str = """".join(r) return (bin_str, len(s))",False,"if c == ""&"" and not decode :","if c == ""+"" and decode :",0.8820016898747209,45.561621331146846
"def _process_file(self, content): args = [] for line in content.splitlines(): line = line.strip() if line.startswith(""-""): args.extend(self._split_option(line)) <mask>: args.append(line) return args",False,"elif line and not line . startswith ( ""#"" ) :","elif line . startswith ( ""#"" ) :",0.8820016898747209,68.94090358777787
"def _method_events_callback(self, values): try: previous_echoed = ( values[""child_result_list""][-1].decode().split(""\n"")[-2].strip() ) if previous_echoed.endswith(""foo1""): return ""echo foo2\n"" elif previous_echoed.endswith(""foo2""): return ""echo foo3\n"" <mask>: return ""exit\n"" else: raise Exception(""Unexpected output {0!r}"".format(previous_echoed)) except IndexError: return ""echo foo1\n""",True,"elif previous_echoed . endswith ( ""foo3"" ) :","elif previous_echoed . endswith ( ""foo3"" ) :",0.8820016898747209,100.00000000000004
"def __delete_hook(self, rpc): try: rpc.check_success() except apiproxy_errors.Error: return None result = [] for status in rpc.response.delete_status_list(): <mask>: result.append(DELETE_SUCCESSFUL) elif status == MemcacheDeleteResponse.NOT_FOUND: result.append(DELETE_ITEM_MISSING) else: result.append(DELETE_NETWORK_FAILURE) return result",False,if status == MemcacheDeleteResponse . DELETED :,if status == MemcacheDeleteResponse . SUCCESS :,0.8820016898747209,70.71067811865478
"def __createRandom(plug): node = plug.node() parentNode = node.ancestor(Gaffer.Node) with Gaffer.UndoScope(node.scriptNode()): randomNode = Gaffer.Random() parentNode.addChild(randomNode) <mask>: plug.setInput(randomNode[""outFloat""]) elif isinstance(plug, Gaffer.Color3fPlug): plug.setInput(randomNode[""outColor""]) GafferUI.NodeEditor.acquire(randomNode)",False,"if isinstance ( plug , ( Gaffer . FloatPlug , Gaffer . IntPlug ) ) :","if isinstance ( plug , Gaffer . Float3fPlug ) :",0.8820016898747209,30.861946272099846
"def escapeentities(self, line): ""Escape all Unicode characters to HTML entities."" result = """" pos = TextPosition(line) while not pos.finished(): if ord(pos.current()) > 128: codepoint = hex(ord(pos.current())) <mask>: codepoint = hex(ord(pos.next()) + 0xF800) result += ""&#"" + codepoint[1:] + "";"" else: result += pos.current() pos.skipcurrent() return result",False,"if codepoint == ""0xd835"" :",elif ord ( pos . next ( ) ) < 0x80 :,0.8820016898747209,3.3864985683445354
def get_and_set_all_aliases(self): all_aliases = [] for page in self.pages: <mask>: all_aliases.extend(page.relations.aliases_norm) if page.relations.aliases is not None: all_aliases.extend(page.relations.aliases) return set(all_aliases),True,if page . relations . aliases_norm is not None :,if page . relations . aliases_norm is not None :,0.8820016898747209,100.00000000000004
"def _list_cases(suite): for test in suite: <mask>: _list_cases(test) elif isinstance(test, unittest.TestCase): if support.match_test(test): print(test.id())",False,"if isinstance ( test , unittest . TestSuite ) :","if isinstance ( test , unittest . TestCase ) :",0.8820016898747209,70.71067811865478
"def get_next_requests(self, max_n_requests, **kwargs): next_pages = [] partitions = set(kwargs.pop(""partitions"", [])) for partition_id in range(0, self.queue_partitions): <mask>: continue results = self.queue.get_next_requests(max_n_requests, partition_id) next_pages.extend(results) self.logger.debug( ""Got %d requests for partition id %d"", len(results), partition_id ) return next_pages",True,if partition_id not in partitions :,if partition_id not in partitions :,0.8820016898747209,100.00000000000004
"def __iter__(self): if (self.query is not None) and sqlite.is_read_only_query(self.query): cur = self.connection.cursor() results = cur.execute(self.query) <mask>: yield [col[0] for col in cur.description] for i, row in enumerate(results): if i >= self.limit: break yield [val for val in row] else: yield",False,if self . headers :,if results :,0.8820016898747209,0.0
"def rollback(self): for operation, values in self.current_transaction_state[::-1]: if operation == ""insert"": values.remove() <mask>: old_value, new_value = values if new_value.full_filename != old_value.full_filename: os.unlink(new_value.full_filename) old_value.write() self._post_xact_cleanup()",False,"elif operation == ""update"" :","elif operation == ""delete"" :",0.8820016898747209,59.4603557501361
"def index(self, value): if self._growing: if self._start <= value < self._stop: q, r = divmod(value - self._start, self._step) <mask>: return int(q) else: if self._start >= value > self._stop: q, r = divmod(self._start - value, -self._step) if r == self._zero: return int(q) raise ValueError(""{} is not in numeric range"".format(value))",True,if r == self . _zero :,if r == self . _zero :,0.8820016898747209,100.00000000000004
"def validate_name_and_description(body, check_length=True): for attribute in [""name"", ""description"", ""display_name"", ""display_description""]: value = body.get(attribute) <mask>: if isinstance(value, six.string_types): body[attribute] = value.strip() if check_length: try: utils.check_string_length( body[attribute], attribute, min_length=0, max_length=255 ) except exception.InvalidInput as error: raise webob.exc.HTTPBadRequest(explanation=error.msg)",True,if value is not None :,if value is not None :,0.8820016898747209,100.00000000000004
"def printWiki(): firstHeading = False for m in protocol: if m[0] == """": <mask>: output(""|}"") __printWikiHeader(m[1], m[2]) firstHeading = True else: output(""|-"") output( '| <span style=""white-space:nowrap;""><tt>' + m[0] + ""</tt></span> || || "" + m[1] ) output(""|}"")",True,if firstHeading :,if firstHeading :,0.8820016898747209,0.0
"def _get_platforms(data): platform_list = [] for item in data: if item.startswith(""PlatformEdit.html?""): parameter_list = item.split(""PlatformEdit.html?"", 1)[1].split(""&"") for parameter in parameter_list: <mask>: platform_list.append(parameter.split(""="")[1]) return platform_list",False,"if parameter . startswith ( ""platformName"" ) :","if parameter . startswith ( ""PlatformEdit.html?"" ) :",0.8820016898747209,48.44273237963865
"def find_scintilla_constants(f): lexers = [] states = [] for name in f.order: v = f.features[name] if v[""Category""] != ""Deprecated"": if v[""FeatureType""] == ""val"": <mask>: states.append((name, v[""Value""])) elif name.startswith(""SCLEX_""): lexers.append((name, v[""Value""])) return (lexers, states)",False,"if name . startswith ( ""SCE_"" ) :","if name . startswith ( ""STATE_"" ) :",0.8820016898747209,70.16879391277372
"def get_operation_ast(document_ast, operation_name=None): operation = None for definition in document_ast.definitions: if isinstance(definition, ast.OperationDefinition): <mask>: # If no operation name is provided, only return an Operation if it is the only one present in the # document. This means that if we've encountered a second operation as we were iterating over the # definitions in the document, there are more than one Operation defined, and we should return None. if operation: return None operation = definition elif definition.name and definition.name.value == operation_name: return definition return operation",False,if not operation_name :,if operation_name is None :,0.8820016898747209,27.77619034011791
"def _insertNewItemAtParent(self, targetIndex): if not self.isContainer(targetIndex): return elif not self.isContainerOpen(targetIndex): uri = self._rows[targetIndex].uri modelNode = self.getNodeForURI(uri) <mask>: modelNode.markForRefreshing() return self.refreshView(targetIndex)",True,if modelNode :,if modelNode :,0.8820016898747209,0.0
"def _get_trace(self, model, guide, args, kwargs): model_trace, guide_trace = super()._get_trace(model, guide, args, kwargs) # Mark all sample sites with require_backward to gather enumerated # sites and adjust cond_indep_stack of all sample sites. for node in model_trace.nodes.values(): <mask>: log_prob = node[""packed""][""unscaled_log_prob""] require_backward(log_prob) self._saved_state = model, model_trace, guide_trace, args, kwargs return model_trace, guide_trace",False,"if node [ ""type"" ] == ""sample"" and not node [ ""is_observed"" ] :","if ""unscaled_log_prob"" in node :",0.8820016898747209,1.9703152795863839
"def _url_encode_impl(obj, charset, encode_keys, sort, key): from .datastructures import iter_multi_items iterable = iter_multi_items(obj) if sort: iterable = sorted(iterable, key=key) for key, value in iterable: if value is None: continue if not isinstance(key, bytes): key = text_type(key).encode(charset) <mask>: value = text_type(value).encode(charset) yield _fast_url_quote_plus(key) + ""="" + _fast_url_quote_plus(value)",True,"if not isinstance ( value , bytes ) :","if not isinstance ( value , bytes ) :",0.8820016898747209,100.00000000000004
"def handle_parse_result(self, ctx, opts, args): with augment_usage_errors(ctx, param=self): value = self.consume_value(ctx, opts) try: value = self.full_process_value(ctx, value) except Exception: <mask>: raise value = None if self.callback is not None: try: value = invoke_param_callback(self.callback, ctx, self, value) except Exception: if not ctx.resilient_parsing: raise if self.expose_value: ctx.params[self.name] = value return value, args",True,if not ctx . resilient_parsing :,if not ctx . resilient_parsing :,0.8820016898747209,100.00000000000004
"def word_pattern(pattern, str): dict = {} set_value = set() list_str = str.split() if len(list_str) != len(pattern): return False for i in range(len(pattern)): if pattern[i] not in dict: <mask>: return False dict[pattern[i]] = list_str[i] set_value.add(list_str[i]) else: if dict[pattern[i]] != list_str[i]: return False return True",False,if list_str [ i ] in set_value :,if set_value . add ( list_str [ i ] ) :,0.8820016898747209,43.33207865423753
"def create(self, path, wipe=False): # type: (Text, bool) -> bool _path = self.validatepath(path) with ftp_errors(self, path): <mask>: empty_file = io.BytesIO() self.ftp.storbinary( str(""STOR "") + _encode(_path, self.ftp.encoding), empty_file ) return True return False",False,if wipe or not self . isfile ( path ) :,if wipe :,0.8820016898747209,0.0
"def build_output_for_item(self, item): output = [] for field in self.fields: values = self._get_item(item, field) <mask>: values = [values] for value in values: if value: output.append(self.build_output_for_single_value(value)) return """".join(output)",True,"if not isinstance ( values , list ) :","if not isinstance ( values , list ) :",0.8820016898747209,100.00000000000004
"def get_resource_public_actions(resource_class): resource_class_members = inspect.getmembers(resource_class) resource_methods = {} for name, member in resource_class_members: if not name.startswith(""_""): if not name[0].isupper(): <mask>: if is_resource_action(member): resource_methods[name] = member return resource_methods",False,"if not name . startswith ( ""wait_until"" ) :","if member . __name__ == ""resource"" :",0.8820016898747209,4.6192151051305474
"def get_command(cls): ifconfig_cmd = ""ifconfig"" for path in [""/sbin"", ""/usr/sbin"", ""/bin"", ""/usr/bin""]: <mask>: ifconfig_cmd = os.path.join(path, ifconfig_cmd) break ifconfig_cmd = ifconfig_cmd + "" -a"" return ifconfig_cmd",False,"if os . path . exists ( os . path . join ( path , ifconfig_cmd ) ) :",if os . path . exists ( path ) :,0.8820016898747209,24.98609756475043
"def main(): base_dir = os.path.join(os.path.split(__file__)[0], "".."", "".."") for path in PATHS: path = os.path.join(base_dir, path) for root, _, files in os.walk(path): for file in files: extension = os.path.splitext(file)[1] <mask>: path = os.path.join(root, file) validate_header(path)",False,if extension in EXTENSIONS :,"if extension == "".py"" :",0.8820016898747209,10.552670315936318
"def auth_login(request): form = RegistrationForm(request.POST or None) if form.is_valid(): authed_user = authenticate( username=form.cleaned_data[""username""], password=form.cleaned_data[""password""], ) <mask>: login(request, authed_user) return HttpResponse(""Success"") raise Http404",True,if authed_user :,if authed_user :,0.8820016898747209,100.00000000000004
"def set(self, _key, _new_login=True): with self.lock: user = self.users.get(current_user.id, None) if user is None: self.users[current_user.id] = dict(session_count=1, key=_key) else: <mask>: user[""session_count""] += 1 user[""key""] = _key",True,if _new_login :,if _new_login :,0.8820016898747209,100.00000000000004
"def fetch(self, fingerprints): to_fetch = [f for f in fingerprints if f not in self._cache] self._logger.debug(""cache size %s"" % len(self._cache)) self._logger.debug(""to fetch %d from %d"" % (len(to_fetch), len(fingerprints))) [self._redis_pipeline.hgetall(key) for key in to_fetch] responses = self._redis_pipeline.execute() for index, key in enumerate(to_fetch): response = responses[index] <mask>: self._cache[key] = response[FIELD_STATE] else: self._cache[key] = self.NOT_CRAWLED",False,if len ( response ) > 0 and FIELD_STATE in response :,if response [ FIELD_STATE ] :,0.8820016898747209,11.547544133164605
"def _append_to_io_queue(self, data, stream_name): # Make sure ANSI CSI codes and object links are stored as separate events # TODO: try to complete previously submitted incomplete code parts = re.split(OUTPUT_SPLIT_REGEX, data) for part in parts: if part: # split may produce empty string in the beginning or start # split the data so that very long lines separated for block in re.split( ""(.{%d,})"" % (self._get_squeeze_threshold() + 1), part ): <mask>: self._queued_io_events.append((block, stream_name))",True,if block :,if block :,0.8820016898747209,0.0
"def find_file_at_path_with_indexes(self, path, url): if url.endswith(""/""): path = os.path.join(path, self.index_file) return self.get_static_file(path, url) elif url.endswith(""/"" + self.index_file): <mask>: return self.redirect(url, url[: -len(self.index_file)]) else: try: return self.get_static_file(path, url) except IsDirectoryError: if os.path.isfile(os.path.join(path, self.index_file)): return self.redirect(url, url + ""/"") raise MissingFileError(path)",False,if os . path . isfile ( path ) :,"if os . path . isfile ( os . path . join ( path , self . index_file ) ) :",0.8820016898747209,29.456425448249245
"def module_list(target, fast): """"""Find the list of modules to be compiled"""""" modules = [] native = native_modules(target) basedir = os.path.join(ouroboros_repo_folder(), ""ouroboros"") for name in os.listdir(basedir): module_name, ext = os.path.splitext(name) if ext == "".py"" or ext == """" and os.path.isdir(os.path.join(basedir, name)): if module_name not in IGNORE_MODULES and module_name not in native: <mask>: modules.append(module_name) return set(modules)",False,if not ( fast and module_name in KNOWN_PROBLEM_MODULES ) :,if fast :,0.8820016898747209,0.0
"def housenumber(self): if self.address: expression = r""\d+"" pattern = re.compile(expression) match = pattern.search(self.address) <mask>: return int(match.group(0))",True,if match :,if match :,0.8820016898747209,0.0
"def get_pip_version(import_path=BASE_IMPORT_PATH): try: pip = importlib.import_module(import_path) except ImportError: <mask>: return get_pip_version(import_path=""pip"") else: import subprocess version = subprocess.check_output([""pip"", ""--version""]) if version: version = version.decode(""utf-8"").split()[1] return version return ""0.0.0"" version = getattr(pip, ""__version__"", None) return version",False,"if import_path != ""pip"" :","if sys . platform == ""win32"" :",0.8820016898747209,12.549310621989482
"def __animate_progress(self): """"""Change the status message, mostly used to animate progress."""""" while True: sleep_time = ThreadPool.PROGRESS_IDLE_DELAY with self.__progress_lock: if not self.__progress_status: sleep_time = ThreadPool.PROGRESS_IDLE_DELAY <mask>: self.__progress_status.update_progress(self.__current_operation_name) sleep_time = ThreadPool.PROGRESS_UPDATE_DELAY else: self.__progress_status.show_as_ready() sleep_time = ThreadPool.PROGRESS_IDLE_DELAY # Allow some time for progress status to be updated. time.sleep(sleep_time)",False,elif self . __show_animation :,elif self . __current_operation_name :,0.8820016898747209,38.16330911371339
"def range_key_names(self): keys = [self.range_key_attr] for index in self.global_indexes: range_key = None for key in index.schema: <mask>: range_key = keys.append(key[""AttributeName""]) keys.append(range_key) return keys",False,"if key [ ""KeyType"" ] == ""RANGE"" :","if key [ ""AttributeName"" ] == self . range_key_attr :",0.8820016898747209,29.48993986902436
"def run(self): dist = self.distribution commands = dist.command_options.keys() settings = {} for cmd in commands: if cmd == ""saveopts"": continue # don't save our own options! for opt, (src, val) in dist.get_option_dict(cmd).items(): <mask>: settings.setdefault(cmd, {})[opt] = val edit_config(self.filename, settings, self.dry_run)",False,"if src == ""command line"" :","if src == ""save"" :",0.8820016898747209,52.47357977607325
"def parse_move(self, node): old, new = """", """" for child in node: tag, text = child.tag, child.text text = text.strip() if text else None if tag == ""Old"" and text: old = text <mask>: new = text return Move(old, new)",True,"elif tag == ""New"" and text :","elif tag == ""New"" and text :",0.8820016898747209,100.00000000000004
"def __codeanalysis_settings_changed(self, current_finfo): if self.data: run_pyflakes, run_pep8 = self.pyflakes_enabled, self.pep8_enabled for finfo in self.data: self.__update_editor_margins(finfo.editor) finfo.cleanup_analysis_results() if (run_pyflakes or run_pep8) and current_finfo is not None: <mask>: finfo.run_code_analysis(run_pyflakes, run_pep8)",False,if current_finfo is not finfo :,if finfo . is_codeanalysis ( ) :,0.8820016898747209,7.129384882260374
"def tchg(var, width): ""Convert time string to given length"" ret = ""%2dh%02d"" % (var / 60, var % 60) <mask>: ret = ""%2dh"" % (var / 60) if len(ret) > width: ret = ""%2dd"" % (var / 60 / 24) if len(ret) > width: ret = ""%2dw"" % (var / 60 / 24 / 7) return ret",True,if len ( ret ) > width :,if len ( ret ) > width :,0.8820016898747209,100.00000000000004
"def spider_log_activity(self, messages): for i in range(0, messages): <mask>: self.sp_sl_p.send( sha1(str(randint(1, 1000))), b""http://helloworld.com/way/to/the/sun/"" + b""0"", ) else: self.sp_sl_p.send( sha1(str(randint(1, 1000))), b""http://way.to.the.sun"" + b""0"" ) self.sp_sl_p.flush()",False,if i % 2 == 0 :,if i == messages - 1 :,0.8820016898747209,16.515821590069027
"def decode_serial(self, offset): serialnum = ( (self.cache[offset + 3] << 24) + (self.cache[offset + 2] << 16) + (self.cache[offset + 1] << 8) + self.cache[offset] ) serialstr = """" is_alnum = True for i in range(4): <mask>: is_alnum = False break serialstr += chr(self.cache[offset + 3 - i]) serial = serialstr if is_alnum else str(serialnum) self.ann_field(offset, offset + 3, ""Serial "" + serial)",False,if not chr ( self . cache [ offset + 3 - i ] ) . isalnum ( ) :,if self . cache [ offset + 3 - i ] == 0 :,0.8820016898747209,46.968008400977354
def gettext(rv): for child in rv.childNodes: if child.nodeType == child.TEXT_NODE: yield child.nodeValue <mask>: for item in gettext(child): yield item,False,if child . nodeType == child . ELEMENT_NODE :,elif child . nodeType == child . ELEMENT_NODE :,0.8820016898747209,90.36020036098445
"def determine_block_hints(self, text): hints = """" if text: if text[0] in "" \n\x85\u2028\u2029"": hints += str(self.best_indent) if text[-1] not in ""\n\x85\u2028\u2029"": hints += ""-"" <mask>: hints += ""+"" return hints",False,"elif len ( text ) == 1 or text [ - 2 ] in ""\n\x85\u2028\u2029"" :","if text [ - 1 ] not in ""\n\x85\u2028\u2029"" :",0.8820016898747209,48.17867859697942
"def _infer_return_type(*args): """"""Look at the type of all args and divine their implied return type."""""" return_type = None for arg in args: if arg is None: continue <mask>: if return_type is str: raise TypeError(""Can't mix bytes and non-bytes in "" ""path components."") return_type = bytes else: if return_type is bytes: raise TypeError(""Can't mix bytes and non-bytes in "" ""path components."") return_type = str if return_type is None: return str # tempfile APIs return a str by default. return return_type",False,"if isinstance ( arg , bytes ) :",if arg . is_bytes :,0.8820016898747209,8.051153633013374
"def as_iconbitmap(cls, rkey): """"""Get image path for use in iconbitmap property"""""" img = None if rkey in cls._stock: data = cls._stock[rkey] <mask>: fpath = data[""filename""] fname = os.path.basename(fpath) name, file_ext = os.path.splitext(fname) file_ext = str(file_ext).lower() if file_ext in TK_BITMAP_FORMATS: img = BITMAP_TEMPLATE.format(fpath) return img",False,"if data [ ""type"" ] not in ( ""stock"" , ""data"" , ""image"" ) :","if ""filename"" in data :",0.8820016898747209,1.0453215315463866
"def anonymize_ip(ip): if ip: match = RE_FIRST_THREE_OCTETS_OF_IP.findall(str(ip)) <mask>: return ""%s%s"" % (match[0][0], ""0"") return """"",True,if match :,if match :,0.8820016898747209,0.0
"def serialize_tail(self): msg = bytearray() for v in self.info: <mask>: value = v[""value""].encode(""utf-8"") elif v[""type""] == BMP_TERM_TYPE_REASON: value = struct.pack(""!H"", v[""value""]) v[""len""] = len(value) msg += struct.pack(self._TLV_PACK_STR, v[""type""], v[""len""]) msg += value return msg",False,"if v [ ""type"" ] == BMP_TERM_TYPE_STRING :","if v [ ""type"" ] == BMP_TERM_TYPE_TEXT :",0.8820016898747209,88.43946454355333
"def get_django_comment(text: str, i: int) -> str: end = i + 4 unclosed_end = 0 while end <= len(text): <mask>: return text[i:end] if not unclosed_end and text[end] == ""<"": unclosed_end = end end += 1 raise TokenizationException(""Unclosed comment"", text[i:unclosed_end])",False,"if text [ end - 2 : end ] == ""#}"" :","if text [ end ] == ""<>"" :",0.8820016898747209,40.39488481131525
"def ComboBoxDroppedHeightTest(windows): ""Check if each combobox height is the same as the reference"" bugs = [] for win in windows: if not win.ref: continue if win.Class() != ""ComboBox"" or win.ref.Class() != ""ComboBox"": continue <mask>: bugs.append( ( [ win, ], {}, testname, 0, ) ) return bugs",False,if win . DroppedRect ( ) . height ( ) != win . ref . DroppedRect ( ) . height ( ) :,"if win . ref . Class ( ) == ""ComboBoxDroppedHeight"" :",0.8820016898747209,13.403300919489984
"def testBadModeArgument(self): # verify that we get a sensible error message for bad mode argument bad_mode = ""qwerty"" try: f = self.open(TESTFN, bad_mode) except ValueError as msg: <mask>: s = str(msg) if TESTFN in s or bad_mode not in s: self.fail(""bad error message for invalid mode: %s"" % s) # if msg.args[0] == 0, we're probably on Windows where there may be # no obvious way to discover why open() failed. else: f.close() self.fail(""no error for invalid mode: %s"" % bad_mode)",False,if msg . args [ 0 ] != 0 :,if msg . args [ 0 ] == 0 :,0.8820016898747209,70.16879391277372
"def command_group_expired(self, command_group_name): try: deprecate_info = self._command_loader.command_group_table[ command_group_name ].group_kwargs.get(""deprecate_info"", None) <mask>: return deprecate_info.expired() except AttributeError: # Items with only token presence in the command table will not have any data. They can't be expired. pass return False",True,if deprecate_info :,if deprecate_info :,0.8820016898747209,100.00000000000004
"def test_non_uniform_probabilities_over_elements(self): param = iap.Choice([0, 1], p=[0.25, 0.75]) samples = param.draw_samples((10000,)) unique, counts = np.unique(samples, return_counts=True) assert len(unique) == 2 for val, count in zip(unique, counts): if val == 0: assert 2500 - 500 < count < 2500 + 500 <mask>: assert 7500 - 500 < count < 7500 + 500 else: assert False",True,elif val == 1 :,elif val == 1 :,0.8820016898747209,100.00000000000004
"def get_labels(directory): cache = get_labels.__cache if directory not in cache: l = {} for t in get_visual_configs(directory)[0][LABEL_SECTION]: <mask>: Messager.warning( ""In configuration, labels for '%s' defined more than once. Only using the last set."" % t.storage_form(), -1, ) # first is storage for, rest are labels. l[t.storage_form()] = t.terms[1:] cache[directory] = l return cache[directory]",True,if t . storage_form ( ) in l :,if t . storage_form ( ) in l :,0.8820016898747209,100.00000000000004
"def try_split(self, split_text: List[str]): ret = [] for i in split_text: if len(i) == 0: continue val = int(i, 2) <mask>: return None ret.append(val) if len(ret) != 0: ret = bytes(ret) logger.debug(f""binary successful, returning {ret.__repr__()}"") return ret",False,if val > 255 or val < 0 :,if val == 0 :,0.8820016898747209,13.924420625000767
"def setCellValue(self, row_idx, col, value): assert col.id == ""repls-marked"" with self._lock: rgroup = self.events[row_idx] <mask>: return rgroup._marked = value == ""true"" and True or False if self._tree: self._tree.invalidateCell(row_idx, col)",False,"if not isinstance ( rgroup , findlib2 . ReplaceHitGroup ) :",if rgroup . _marked :,0.8820016898747209,4.988641679706251
"def create(cls, settlement_manager, resource_id): """"""Create a production chain that can produce the given resource."""""" resource_producer = {} for abstract_building in AbstractBuilding.buildings.values(): for resource, production_line in abstract_building.lines.items(): <mask>: resource_producer[resource] = [] resource_producer[resource].append((production_line, abstract_building)) return ProductionChain(settlement_manager, resource_id, resource_producer)",True,if resource not in resource_producer :,if resource not in resource_producer :,0.8820016898747209,100.00000000000004
def get_all_partition_sets(self): partition_sets = [] if self.partitions_handle: partition_sets.extend(self.partitions_handle.get_partition_sets()) if self.scheduler_handle: partition_sets.extend( [ schedule_def.get_partition_set() for schedule_def in self.scheduler_handle.all_schedule_defs() <mask>: ] ) return partition_sets,False,"if isinstance ( schedule_def , PartitionScheduleDefinition )",if schedule_def . is_active ( ),0.8820016898747209,18.575057999133595
"def _sendDatapointsNow(self, datapoints): metrics = {} payload_pb = Payload() for metric, datapoint in datapoints: <mask>: metric_pb = payload_pb.metrics.add() metric_pb.metric = metric metrics[metric] = metric_pb else: metric_pb = metrics[metric] point_pb = metric_pb.points.add() point_pb.timestamp = int(datapoint[0]) point_pb.value = datapoint[1] self.sendString(payload_pb.SerializeToString())",True,if metric not in metrics :,if metric not in metrics :,0.8820016898747209,100.00000000000004
"def execute(self): if self._dirty or not self._qr: model_class = self.model_class query_meta = self.get_query_meta() if self._tuples: ResultWrapper = TuplesQueryResultWrapper <mask>: ResultWrapper = DictQueryResultWrapper elif self._naive or not self._joins or self.verify_naive(): ResultWrapper = NaiveQueryResultWrapper elif self._aggregate_rows: ResultWrapper = AggregateQueryResultWrapper else: ResultWrapper = ModelQueryResultWrapper self._qr = ResultWrapper(model_class, self._execute(), query_meta) self._dirty = False return self._qr else: return self._qr",True,elif self . _dicts :,elif self . _dicts :,0.8820016898747209,100.00000000000004
"def get_metrics(): classifier, feature_labels = load_classifier() available_metrics = ImgageMetrics.get_metric_classes() # todo review: DONE IN DOCS # effective_metrics isn't used after filling it with values # in the loops below effective_metrics = [] for metric in available_metrics: for label in feature_labels: for label_part in metric.get_labels(): <mask>: effective_metrics.append(metric) return (classifier, feature_labels, available_metrics)",False,if label_part == label and metric not in effective_metrics :,if label_part . get_label ( ) == label :,0.8820016898747209,25.491833774890388
"def test_nic_names(self): p = subprocess.Popen([""ipconfig"", ""/all""], stdout=subprocess.PIPE) out = p.communicate()[0] if PY3: out = str(out, sys.stdout.encoding) nics = psutil.net_io_counters(pernic=True).keys() for nic in nics: if ""pseudo-interface"" in nic.replace("" "", ""-"").lower(): continue <mask>: self.fail(""%r nic wasn't found in 'ipconfig /all' output"" % nic)",False,if nic not in out :,if out != nic :,0.8820016898747209,11.478744233307168
"def convert_with_key(self, key, value, replace=True): result = self.configurator.convert(value) # If the converted value is different, save for next time if value is not result: if replace: self[key] = result <mask>: result.parent = self result.key = key return result",False,"if type ( result ) in ( ConvertingDict , ConvertingList , ConvertingTuple ) :",elif result is not None :,0.8820016898747209,2.544354209531657
"def _EvaluateFile(self, test_list, file): (name, ext) = os.path.splitext(file) if ext == "".cc"" or ext == "".cpp"" or ext == "".c"": <mask>: logger.SilentLog(""Found native test file %s"" % file) test_list.append(name)",False,"if re . search ( ""_test$|_test_$|_unittest$|_unittest_$|^test_|Tests$"" , name ) :",if name not in test_list :,0.8820016898747209,0.47522448365275494
"def leading_whitespace(self, inputstring): """"""Get leading whitespace."""""" leading_ws = [] for i, c in enumerate(inputstring): <mask>: leading_ws.append(c) else: break if self.indchar is None: self.indchar = c elif c != self.indchar: self.strict_err_or_warn(""found mixing of tabs and spaces"", inputstring, i) return """".join(leading_ws)",False,if c in legal_indent_chars :,if c in self . leading_whitespace :,0.8820016898747209,20.164945583740657
"def ident_values(self): value = self._ident_values if value is False: value = None # XXX: how will this interact with orig_prefix ? # not exposing attrs for now if orig_prefix is set. <mask>: wrapped = self.wrapped idents = getattr(wrapped, ""ident_values"", None) if idents: value = [self._wrap_hash(ident) for ident in idents] ##else: ## ident = self.ident ## if ident is not None: ## value = [ident] self._ident_values = value return value",False,if not self . orig_prefix :,if self . wrapped :,0.8820016898747209,13.943458243384402
"def _available_symbols(self, scoperef, expr): cplns = [] found_names = set() while scoperef: elem = self._elem_from_scoperef(scoperef) for child in elem: name = child.get(""name"", """") <mask>: if name not in found_names: found_names.add(name) ilk = child.get(""ilk"") or child.tag cplns.append((ilk, name)) scoperef = self.parent_scoperef_from_scoperef(scoperef) if not scoperef: break return sorted(cplns, key=operator.itemgetter(1))",False,if name . startswith ( expr ) :,if name and child . tag == expr :,0.8820016898747209,10.552670315936318
"def pid_from_name(name): # quick and dirty, works with all linux not depending on ps output for pid in os.listdir(""/proc""): try: int(pid) except: continue pname = """" with open(""/proc/%s/cmdline"" % pid, ""r"") as f: pname = f.read() <mask>: return int(pid) raise ProcessException(""No process with such name: %s"" % name)",False,if name in pname :,if pname == name :,0.8820016898747209,11.478744233307168
"def touch(self): if not self.exists(): try: self.parent().touch() except ValueError: pass node = self._fs.touch(self.pathnames, {}) <mask>: raise AssertionError(""Not a folder: %s"" % self.path) if self.watcher: self.watcher.emit(""created"", self)",False,if not node . isdir :,if not node :,0.8820016898747209,38.75385825373298
"def setUp(self): BaseTestCase.setUp(self) self.rawData = [] self.dataByKey = {} for i in range(1, 11): stringCol = ""String %d"" % i fixedCharCol = (""Fixed Char %d"" % i).ljust(40) rawCol = ""Raw %d"" % i <mask>: nullableCol = ""Nullable %d"" % i else: nullableCol = None dataTuple = (i, stringCol, rawCol, fixedCharCol, nullableCol) self.rawData.append(dataTuple) self.dataByKey[i] = dataTuple",False,if i % 2 :,if i % 2 == 0 :,0.8820016898747209,36.55552228545123
"def GenerateVector(self, hits, vector, level): """"""Generate possible hit vectors which match the rules."""""" for item in hits.get(level, []): if vector: if item < vector[-1]: continue if item > self.max_separation + vector[-1]: break new_vector = vector + [item] <mask>: yield new_vector elif level + 1 < len(hits): for result in self.GenerateVector(hits, new_vector, level + 1): yield result",False,if level + 1 == len ( hits ) :,if level == 0 :,0.8820016898747209,10.54969271144651
"def __repr__(self): attrs = [] for k in self.keydata: <mask>: attrs.append(""p(%d)"" % (self.size() + 1,)) elif hasattr(self.key, k): attrs.append(k) if self.has_private(): attrs.append(""private"") return ""<%s @0x%x %s>"" % (self.__class__.__name__, id(self), "","".join(attrs))",False,"if k == ""p"" :","if k == ""size"" :",0.8820016898747209,59.4603557501361
"def autoload(self): if self._app.config.THEME == ""auto"": <mask>: if get_osx_theme() == 1: theme = DARK else: theme = LIGHT else: theme = self.guess_system_theme() if theme == Dark: theme = MacOSDark else: # user settings have highest priority theme = self._app.config.THEME self.load_theme(theme)",False,"if sys . platform == ""darwin"" :",if self . _app . config . USE_OSX :,0.8820016898747209,4.456882760699063
"def _get_matching_bracket(self, s, pos): if s[pos] != ""{"": return None end = len(s) depth = 1 pos += 1 while pos != end: c = s[pos] if c == ""{"": depth += 1 elif c == ""}"": depth -= 1 <mask>: break pos += 1 if pos < end and s[pos] == ""}"": return pos return None",False,if depth == 0 :,elif depth == 0 :,0.8820016898747209,75.98356856515926
"def update_meter(self, output, target, meters={""accuracy""}): output = self.__to_tensor(output) target = self.__to_tensor(target) for meter in meters: <mask>: self.__addmeter(meter) if meter in [""ap"", ""map"", ""confusion""]: target_th = self._ver2tensor(target) self.meter[meter].add(output, target_th) else: self.meter[meter].add(output, target)",False,if meter not in self . meter . keys ( ) :,if meter not in self . meter :,0.8820016898747209,52.734307450329375
"def _reinit_optimizers_with_oss(self): optimizers = self.lightning_module.trainer.optimizers for x, optimizer in enumerate(optimizers): if is_lightning_optimizer(optimizer): optimizer = optimizer._optimizer <mask>: optim_class = type(optimizer) zero_optimizer = OSS( params=optimizer.param_groups, optim=optim_class, **optimizer.defaults ) optimizers[x] = zero_optimizer del optimizer trainer = self.lightning_module.trainer trainer.optimizers = optimizers trainer.convert_to_lightning_optimizers()",False,"if not isinstance ( optimizer , OSS ) :","if isinstance ( optimizer , OSS ) :",0.8820016898747209,76.72796459606589
"def OnSelChanged(self, event): self.item = event.GetItem() if self.item: self.log.write(""OnSelChanged: %s"" % self.GetItemText(self.item)) <mask>: self.log.write( "", BoundingRect: %s\n"" % self.GetBoundingRect(self.item, True) ) else: self.log.write(""\n"") event.Skip()",False,"if wx . Platform == ""__WXMSW__"" :",if self . item . GetBoundingRect ( ) :,0.8820016898747209,3.600104948631532
"def parse_batch(args): errmsg = ""Invalid batch definition: batch entry has to be defined as RULE=BATCH/BATCHES (with integers BATCH <= BATCHES, BATCH >= 1)."" if args.batch is not None: rule, batchdef = parse_key_value_arg(args.batch, errmsg=errmsg) try: batch, batches = batchdef.split(""/"") batch = int(batch) batches = int(batches) except ValueError: raise ValueError(errmsg) <mask>: raise ValueError(errmsg) return Batch(rule, batch, batches) return None",False,if batch > batches or batch < 1 :,if len ( batch ) != 1 :,0.8820016898747209,11.339582221952005
"def get_foreign_key_columns(self, engine, table_name): foreign_keys = set() table = db_utils.get_table(engine, table_name) inspector = reflection.Inspector.from_engine(engine) for column_dict in inspector.get_columns(table_name): column_name = column_dict[""name""] column = getattr(table.c, column_name) <mask>: foreign_keys.add(column_name) return foreign_keys",False,if column . foreign_keys :,if column . foreign_key :,0.8820016898747209,64.34588841607616
"def update(self, t): l = int(t * self.nr_of_tiles) for i in range(self.nr_of_tiles): t = self.tiles_order[i] <mask>: self.turn_off_tile(t) else: self.turn_on_tile(t)",False,if i < l :,if t < l :,0.8820016898747209,42.72870063962342
"def read(self, amt=None): # the _rbuf test is only in this first if for speed. It's not # logically necessary if self._rbuf and not amt is None: L = len(self._rbuf) <mask>: amt -= L else: s = self._rbuf[:amt] self._rbuf = self._rbuf[amt:] return s s = self._rbuf + self._raw_read(amt) self._rbuf = b"""" return s",True,if amt > L :,if amt > L :,0.8820016898747209,100.00000000000004
"def draw_menu_button(self, context, layout, node, text): if ( hasattr(node.id_data, ""sv_show_socket_menus"") and node.id_data.sv_show_socket_menus ): <mask>: layout.menu(""SV_MT_SocketOptionsMenu"", text="""", icon=""TRIA_DOWN"")",False,if self . is_output or self . is_linked or not self . use_prop :,"if text == ""Socket Options"" :",0.8820016898747209,1.6701777430699283
"def __enter__(self): with DB.connection_context(): session_record = SessionRecord() session_record.f_session_id = self._session_id session_record.f_engine_name = self._engine_name session_record.f_engine_type = EngineType.STORAGE # TODO: engine address session_record.f_engine_address = {} session_record.f_create_time = current_timestamp() rows = session_record.save(force_insert=True) <mask>: raise Exception(f""create session record {self._session_id} failed"") LOGGER.debug(f""save session {self._session_id} record"") self.create() return self",False,if rows != 1 :,if not rows :,0.8820016898747209,12.750736437345598
"def tearDown(self): """"""Shutdown the server."""""" try: if self.server: self.server.stop(2.0) <mask>: self.root_logger.removeHandler(self.sl_hdlr) self.sl_hdlr.close() finally: BaseTest.tearDown(self)",True,if self . sl_hdlr :,if self . sl_hdlr :,0.8820016898747209,100.00000000000004
"def _dec_device(self, srcdev, dstdev): if srcdev: self.srcdevs[srcdev] -= 1 <mask>: del self.srcdevs[srcdev] self._set_limits(""read"", self.srcdevs) if dstdev: self.dstdevs[dstdev] -= 1 if self.dstdevs[dstdev] == 0: del self.dstdevs[dstdev] self._set_limits(""write"", self.dstdevs)",True,if self . srcdevs [ srcdev ] == 0 :,if self . srcdevs [ srcdev ] == 0 :,0.8820016898747209,100.00000000000004
"def array_for(self, i): if 0 <= i < self._cnt: <mask>: return self._tail node = self._root level = self._shift while level > 0: assert isinstance(node, Node) node = node._array[(i >> level) & 0x01F] level -= 5 return node._array affirm(False, u""Index out of Range"")",False,if i >= self . tailoff ( ) :,if i >= self . _cnt :,0.8820016898747209,54.627576446464936
"def convert_tensor(self, offsets, sizes): results = [] for b, batch in enumerate(offsets): utterances = [] for p, utt in enumerate(batch): size = sizes[b][p] <mask>: utterances.append(utt[0:size]) else: utterances.append(torch.tensor([], dtype=torch.int)) results.append(utterances) return results",False,if sizes [ b ] [ p ] > 0 :,if size > 0 :,0.8820016898747209,12.869637315183779
"def _predict_proba(self, X, preprocess=True): if preprocess: X = self.preprocess(X) if self.problem_type == REGRESSION: return self.model.predict(X) y_pred_proba = self.model.predict_proba(X) if self.problem_type == BINARY: if len(y_pred_proba.shape) == 1: return y_pred_proba <mask>: return y_pred_proba[:, 1] else: return y_pred_proba elif y_pred_proba.shape[1] > 2: return y_pred_proba else: return y_pred_proba[:, 1]",False,elif y_pred_proba . shape [ 1 ] > 1 :,elif len ( y_pred_proba . shape ) == 2 :,0.8820016898747209,42.61082723917019
def timeout(self): now = ptime.time() dt = now - self.lastPlayTime if dt < 0: return n = int(self.playRate * dt) if n != 0: self.lastPlayTime += float(n) / self.playRate <mask>: self.play(0) self.jumpFrames(n),False,"if self . currentIndex + n > self . image . shape [ self . axes [ ""t"" ] ] :",if n == 1 :,0.8820016898747209,0.6282878523720715
"def __init__(self, data, weights=None, ddof=0): self.data = np.asarray(data) if weights is None: self.weights = np.ones(self.data.shape[0]) else: self.weights = np.asarray(weights).astype(float) # TODO: why squeeze? <mask>: self.weights = self.weights.squeeze() self.ddof = ddof",False,if len ( self . weights . shape ) > 1 and len ( self . weights ) > 1 :,if self . weights . ndim == 1 :,0.8820016898747209,10.807256086619267
"def writerow(self, row): unicode_row = [] for col in row: <mask>: unicode_row.append(col.encode(""utf-8"").strip()) else: unicode_row.append(col) self.writer.writerow(unicode_row) # Fetch UTF-8 output from the queue ... data = self.queue.getvalue() data = data.decode(""utf-8"") # ... and reencode it into the target encoding data = self.encoder.encode(data) # write to the target stream self.stream.write(data) # empty queue self.queue.truncate(0)",False,if type ( col ) == str or type ( col ) == unicode :,"if isinstance ( col , unicode ) :",0.8820016898747209,4.719073083867901
"def __init__(self, choices, allow_blank=False, **kwargs): self.choiceset = choices self.allow_blank = allow_blank self._choices = dict() # Unpack grouped choices for k, v in choices: <mask>: for k2, v2 in v: self._choices[k2] = v2 else: self._choices[k] = v super().__init__(**kwargs)",False,"if type ( v ) in [ list , tuple ] :","if isinstance ( v , ( list , tuple ) ) :",0.8820016898747209,17.827531042796263
"def simp_ext(_, expr): if expr.op.startswith(""zeroExt_""): arg = expr.args[0] <mask>: return arg return ExprCompose(arg, ExprInt(0, expr.size - arg.size)) if expr.op.startswith(""signExt_""): arg = expr.args[0] add_size = expr.size - arg.size new_expr = ExprCompose( arg, ExprCond( arg.msb(), ExprInt(size2mask(add_size), add_size), ExprInt(0, add_size) ), ) return new_expr return expr",False,if expr . size == arg . size :,if arg . size == 0 :,0.8820016898747209,36.827215283744195
"def mark_differences(value: str, compare_against: str): result = [] for i, char in enumerate(value): try: <mask>: result.append('<font color=""red"">{}</font>'.format(char)) else: result.append(char) except IndexError: result.append(char) return """".join(result)",False,if char != compare_against [ i ] :,if compare_against [ i ] :,0.8820016898747209,59.755798910891144
"def run_query(self, query, user): url = ""%s%s"" % (self.base_url, ""&"".join(query.split(""\n""))) error = None data = None try: response = requests.get(url, auth=self.auth, verify=self.verify) <mask>: data = _transform_result(response) else: error = ""Failed getting results (%d)"" % response.status_code except Exception as ex: data = None error = str(ex) return data, error",True,if response . status_code == 200 :,if response . status_code == 200 :,0.8820016898747209,100.00000000000004
"def on_enter(self): """"""Fired when mouse enter the bbox of the widget."""""" if hasattr(self, ""md_bg_color"") and self.focus_behavior: <mask>: self.md_bg_color = self.theme_cls.bg_normal else: if not self.focus_color: self.md_bg_color = App.get_running_app().theme_cls.bg_normal else: self.md_bg_color = self.focus_color",False,"if hasattr ( self , ""theme_cls"" ) and not self . focus_color :","if self . focus_color == ""normal"" :",0.8820016898747209,20.491364558458866
"def tearDown(self): if not self.is_playback(): try: <mask>: self.sms.delete_hosted_service(self.hosted_service_name) except: pass try: if self.storage_account_name is not None: self.sms.delete_storage_account(self.storage_account_name) except: pass try: self.sms.delete_affinity_group(self.affinity_group_name) except: pass return super(LegacyMgmtAffinityGroupTest, self).tearDown()",True,if self . hosted_service_name is not None :,if self . hosted_service_name is not None :,0.8820016898747209,100.00000000000004
"def name2cp(k): if k == ""apos"": return ord(""'"") if hasattr(htmlentitydefs, ""name2codepoint""): # requires Python 2.3 return htmlentitydefs.name2codepoint[k] else: k = htmlentitydefs.entitydefs[k] <mask>: return int(k[2:-1]) # not in latin-1 return ord(codecs.latin_1_decode(k)[0])",False,"if k . startswith ( ""&#"" ) and k . endswith ( "";"" ) :","if k [ : 2 ] == ""latin-1"" :",0.8820016898747209,4.372564651695467
"def _para_set(self, params, part): if len(params) == 0: result = suggest([i.get_name() for i in self._options], part) return result elif len(params) == 1: paramName = params[0] if paramName not in self._options: return [] opt = self._options[paramName] paramType = opt.get_type() <mask>: values = [opt.get_default_value() == ""True"" and ""False"" or ""True""] else: values = self._memory[paramName] return suggest(values, part) else: return []",True,"if paramType == ""boolean"" :","if paramType == ""boolean"" :",0.8820016898747209,100.00000000000004
"def hexcmp(x, y): try: a = int(x, 16) b = int(y, 16) <mask>: return -1 if a > b: return 1 return 0 except: return cmp(x, y)",True,if a < b :,if a < b :,0.8820016898747209,100.00000000000004
"def execute(self, statement, arguments=None): while True: try: if arguments: self.cursor.execute(statement, arguments) else: self.cursor.execute(statement) except sqlite3.OperationalError as ex: <mask>: raise else: break if statement.lstrip().upper().startswith(""SELECT""): return self.cursor.fetchall()",False,"if ""locked"" not in getSafeExString ( ex ) :",if ex . errno != errno . EEXIST :,0.8820016898747209,4.996872151825361
"def _test_forever(self, tests): while True: for test_name in tests: yield test_name <mask>: return if self.ns.fail_env_changed and self.environment_changed: return",False,if self . bad :,if self . ns . fail_test_changed and self . test_changed :,0.8820016898747209,9.147827112247601
"def removeUser(self, username): hideFromOSD = not constants.SHOW_DIFFERENT_ROOM_OSD if username in self._users: user = self._users[username] if user.room: <mask>: hideFromOSD = not constants.SHOW_SAME_ROOM_OSD if username in self._users: self._users.pop(username) message = getMessage(""left-notification"").format(username) self.ui.showMessage(message, hideFromOSD) self._client.lastLeftTime = time.time() self._client.lastLeftUser = username self.userListChange()",False,if self . isRoomSame ( user . room ) :,if user . room . is_active ( ) :,0.8820016898747209,19.72940627795883
"def AutoTest(): with open(sys.argv[1], ""rb"") as f: for line in f.read().split(b""\n""): line = BYTES2SYSTEMSTR(line.strip()) if not line: continue elif line.startswith(""#""): print(line) else: print("">>> "" + line) os.system(line) sys.stdout.write(""\npress enter to continue..."") <mask>: input() else: raw_input() sys.stdout.write(""\n"")",False,if PY3 :,"if sys . argv [ 0 ] == b""input"" :",0.8820016898747209,3.377156414337854
"def get_first_field(layout, clz): for layout_object in layout.fields: <mask>: return layout_object elif hasattr(layout_object, ""get_field_names""): gf = get_first_field(layout_object, clz) if gf: return gf",False,"if issubclass ( layout_object . __class__ , clz ) :","if hasattr ( layout_object , ""get_field_names"" ) :",0.8820016898747209,20.68720601025941
"def sanitize_event_keys(kwargs, valid_keys): # Sanity check: Don't honor keys that we don't recognize. for key in list(kwargs.keys()): if key not in valid_keys: kwargs.pop(key) # Truncate certain values over 1k for key in [""play"", ""role"", ""task"", ""playbook""]: if isinstance(kwargs.get(""event_data"", {}).get(key), str): <mask>: kwargs[""event_data""][key] = Truncator(kwargs[""event_data""][key]).chars( 1024 )",False,"if len ( kwargs [ ""event_data"" ] [ key ] ) > 1024 :","if kwargs [ ""event_data"" ] [ key ] . isdigit ( ) :",0.8820016898747209,62.174479032359464
"def visit_productionlist(self, node): self.new_state() names = [] for production in node: names.append(production[""tokenname""]) maxlen = max(len(name) for name in names) for production in node: <mask>: self.add_text(production[""tokenname""].ljust(maxlen) + "" ::="") lastname = production[""tokenname""] else: self.add_text(""%s "" % ("" "" * len(lastname))) self.add_text(production.astext() + self.nl) self.end_state(wrap=False) raise nodes.SkipNode",True,"if production [ ""tokenname"" ] :","if production [ ""tokenname"" ] :",0.8820016898747209,100.00000000000004
"def uuid(self): if not getattr(self, ""_uuid"", None): <mask>: self._uuid = self.repository._kp_uuid( self.path ) # Use repository UUID (even if None) else: self._uuid = str(uuid.uuid4()) return self._uuid",False,if self . repository is not None :,if self . repository :,0.8820016898747209,38.80684294761701
"def remove(self, values): if not isinstance(values, (list, tuple, set)): values = [values] for v in values: v = str(v) <mask>: self._definition.pop(v, None) elif self._definition == ""ANY"": if v == ""ANY"": self._definition = [] elif v in self._definition: self._definition.remove(v) if ( self._value is not None and self._value not in self._definition and self._not_any() ): raise ConanException(bad_value_msg(self._name, self._value, self.values_range))",False,"if isinstance ( self . _definition , dict ) :",if self . _definition :,0.8820016898747209,24.439253249722206
"def make(self): pygments_dir = join(self.dir, ""externals"", ""pygments"") if exists(pygments_dir): run_in_dir(""hg pull"", pygments_dir, self.log.info) run_in_dir(""hg update"", pygments_dir, self.log.info) else: <mask>: os.makedirs(dirname(pygments_dir)) run_in_dir( ""hg clone http://dev.pocoo.org/hg/pygments-main %s"" % basename(pygments_dir), dirname(pygments_dir), self.log.info, )",True,if not exists ( dirname ( pygments_dir ) ) :,if not exists ( dirname ( pygments_dir ) ) :,0.8820016898747209,100.00000000000004
def set_field(self): i = 0 for string in self.display_string: <mask>: self.config[self.field + str(i)] = self.conversion_fn(self.str[i]) else: self.config[self.field + str(i)] = self.str[i] i = i + 1,True,if self . conversion_fn :,if self . conversion_fn :,0.8820016898747209,100.00000000000004
"def cleanup(self): with self.lock: for proc in self.processes: <mask>: continue proc.join() self.processes.remove(proc) log.debug(""Subprocess %s cleaned up"", proc.name)",True,if proc . is_alive ( ) :,if proc . is_alive ( ) :,0.8820016898747209,100.00000000000004
"def setup(self, gen): Node.setup(self, gen) for c in self.children: c.setup(gen) if not self.accepts_epsilon: # If it's not already accepting epsilon, it might now do so. for c in self.children: # any non-epsilon means all is non-epsilon <mask>: break else: self.accepts_epsilon = 1 gen.changed()",False,if not c . accepts_epsilon :,if c . is_epsilon ( gen ) :,0.8820016898747209,13.134549472120794
"def __call__(self, message): with self._lock: self._pending_ack += 1 self.max_pending_ack = max(self.max_pending_ack, self._pending_ack) self.seen_message_ids.append(int(message.attributes[""seq_num""])) time.sleep(self._processing_time) with self._lock: self._pending_ack -= 1 message.ack() self.completed_calls += 1 if self.completed_calls >= self._resolve_at_msg_count: <mask>: self.done_future.set_result(None)",False,if not self . done_future . done ( ) :,if self . done_future is not None :,0.8820016898747209,36.17085516890759
"def build_canned_image_list(path): layers_path = get_bitbake_var(""BBLAYERS"") canned_wks_layer_dirs = [] if layers_path is not None: for layer_path in layers_path.split(): for wks_path in (WIC_DIR, SCRIPTS_CANNED_IMAGE_DIR): cpath = os.path.join(layer_path, wks_path) <mask>: canned_wks_layer_dirs.append(cpath) cpath = os.path.join(path, CANNED_IMAGE_DIR) canned_wks_layer_dirs.append(cpath) return canned_wks_layer_dirs",True,if os . path . isdir ( cpath ) :,if os . path . isdir ( cpath ) :,0.8820016898747209,100.00000000000004
"def _recv_loop(self) -> None: async with self._ws as connection: self._connected = True self.connection = connection while self._connected: try: resp = await self.connection.recv() <mask>: await self._on_message(resp) except (websockets.ConnectionClosed, ConnectionResetError): logger.info(""connection closed"") break await asyncio.sleep(0) if self._connected: self._loop.create_task(self.dispose())",True,if resp :,if resp :,0.8820016898747209,0.0
"def _get_between(content, start, end=None): should_yield = False for line in content.split(""\n""): if start in line: should_yield = True continue <mask>: return if should_yield and line: yield line.strip().split("" "")[0]",False,if end and end in line :,if end is None :,0.8820016898747209,15.848738972120703
"def handle_parse_result(self, ctx, opts, args): if self.name in opts: if self.mutually_exclusive.intersection(opts): self._raise_exclusive_error() <mask>: self._raise_exclusive_error() return super(MutuallyExclusiveOption, self).handle_parse_result(ctx, opts, args)",False,if self . multiple and len ( set ( opts [ self . name ] ) ) > 1 :,elif self . exclusive_exclusive . intersection ( opts ) :,0.8820016898747209,5.656212892915904
"def write(self, s): if self.interactive: <mask>: self.active_mode.write(s) else: component.get(""CmdLine"").add_line(s, False) self.events.append(s) else: print(colors.strip_colors(s))",False,"if isinstance ( self . active_mode , deluge . ui . console . modes . cmdline . CmdLine ) :",if self . active_mode :,0.8820016898747209,7.845605491344882
"def findfiles(path): files = [] for name in os.listdir(path): # ignore hidden files/dirs and other unwanted files if name.startswith(""."") or name == ""lastsnap.jpg"": continue pathname = os.path.join(path, name) st = os.lstat(pathname) mode = st.st_mode <mask>: files.extend(findfiles(pathname)) elif stat.S_ISREG(mode): files.append((pathname, name, st)) return files",True,if stat . S_ISDIR ( mode ) :,if stat . S_ISDIR ( mode ) :,0.8820016898747209,100.00000000000004
"def _get_documented_completions(self, table, startswith=None): names = [] for key, command in table.items(): if getattr(command, ""_UNDOCUMENTED"", False): # Don't tab complete undocumented commands/params continue if startswith is not None and not key.startswith(startswith): continue <mask>: continue names.append(key) return names",False,"if getattr ( command , ""positional_arg"" , False ) :",if key in self . _documented_commands :,0.8820016898747209,3.701773936489291
"def fix_newlines(lines): """"""Convert newlines to unix."""""" for i, line in enumerate(lines): <mask>: lines[i] = line[:-2] + ""\n"" elif line.endswith(""\r""): lines[i] = line[:-1] + ""\n""",False,"if line . endswith ( ""\r\n"" ) :","if line . endswith ( ""\n"" ) :",0.8820016898747209,75.33808072882876
"def GeneratePageMetatadata(self, task): address_space = self.session.GetParameter(""default_address_space"") for vma in task.mm.mmap.walk_list(""vm_next""): start = vma.vm_start end = vma.vm_end # Skip the entire region. if end < self.plugin_args.start: continue # Done. if start > self.plugin_args.end: break for vaddr in utils.xrange(start, end, 0x1000): <mask>: yield vaddr, self._CreateMetadata(address_space.describe_vtop(vaddr))",False,if self . plugin_args . start <= vaddr <= self . plugin_args . end :,if vaddr in address_space . describe_vtop ( vaddr ) :,0.8820016898747209,2.6957787672008857
"def get_shape_at_node(self, node, assumptions): for k, v in assumptions.items(): <mask>: return v if node.inputs: return node.container.shape( input_shapes=[ self.get_shape_at_node(input_node, assumptions) for input_node in node.inputs ] ) else: return node.container.shape(None)",False,if k in node . names :,"if k == ""shape"" :",0.8820016898747209,12.22307556087252
"def fix_doc(self, doc): type = doc.get(""type"", {}).get(""key"") if type == ""/type/work"": <mask>: # some record got empty author records because of an error # temporary hack to fix doc[""authors""] = [ a for a in doc[""authors""] if ""author"" in a and ""key"" in a[""author""] ] elif type == ""/type/edition"": # get rid of title_prefix. if ""title_prefix"" in doc: title = doc[""title_prefix""].strip() + "" "" + doc.get(""title"", """") doc[""title""] = title.strip() del doc[""title_prefix""] return doc",False,"if doc . get ( ""authors"" ) :","if ""authors"" in doc :",0.8820016898747209,18.938334565508196
"def modify_column(self, column: List[Optional[""Cell""]]): for i in range(len(column)): gate = column[i] <mask>: continue elif isinstance(gate, ParityControlCell): # The first parity control to modify the column must merge all # of the other parity controls into itself. column[i] = None self._basis_change += gate._basis_change self.qubits += gate.qubits elif gate is not None: column[i] = gate.controlled_by(self.qubits[0])",False,if gate is self :,if gate is None :,0.8820016898747209,42.72870063962342
"def onSync(self, auto=False, reload=True): if not auto or ( self.pm.profile[""syncKey""] and self.pm.profile[""autoSync""] and not self.safeMode ): from aqt.sync import SyncManager if not self.unloadCollection(): return # set a sync state so the refresh timer doesn't fire while deck # unloaded self.state = ""sync"" self.syncer = SyncManager(self, self.pm) self.syncer.sync() if reload: <mask>: self.loadCollection()",False,if not self . col :,if self . loadCollection :,0.8820016898747209,20.80119537801062
"def _has_url_match(self, match, request_url): url = match[""url""] if _is_string(url): <mask>: return self._has_strict_url_match(url, request_url) else: url_without_qs = request_url.split(""?"", 1)[0] return url == url_without_qs elif isinstance(url, re._pattern_type) and url.match(request_url): return True else: return False",False,"if match [ ""match_querystring"" ] :",if re . _pattern_type in url :,0.8820016898747209,5.522397783539471
"def pool_image(self, image): if self.count < self.pool_size: self.pool.append(image) self.count += 1 return image else: p = random.random() <mask>: random_id = random.randint(0, self.pool_size - 1) temp = self.pool[random_id] self.pool[random_id] = image return temp else: return image",False,if p > 0.5 :,if p < self . pool_size :,0.8820016898747209,10.552670315936318
"def get_target_dimensions(self): width, height = self.engine.size for operation in self.operations: <mask>: width = operation[""right""] - operation[""left""] height = operation[""bottom""] - operation[""top""] if operation[""type""] == ""resize"": width = operation[""width""] height = operation[""height""] return (width, height)",False,"if operation [ ""type"" ] == ""crop"" :","if operation [ ""type"" ] == ""resize"" :",0.8820016898747209,79.10665071754353
"def validate_matrix(matrix): if not matrix: return None for key, value in matrix.items(): <mask>: raise ValidationError( ""`{}` defines a non uniform distribution, "" ""and it cannot be used with bayesian optimization."".format(key) ) return matrix",False,if value . is_distribution and not value . is_uniform :,if not is_uniform_distribution ( value ) :,0.8820016898747209,15.468856017774625
"def scm_to_conandata(self): try: scm_to_conandata = get_env(""CONAN_SCM_TO_CONANDATA"") <mask>: scm_to_conandata = self.get_item(""general.scm_to_conandata"") return scm_to_conandata.lower() in (""1"", ""true"") except ConanException: return False",True,if scm_to_conandata is None :,if scm_to_conandata is None :,0.8820016898747209,100.00000000000004
"def _link_vrf_table(self, vrf_table, rt_list): route_family = vrf_table.route_family for rt in rt_list: rt_rf_id = rt + "":"" + str(route_family) table_set = self._tables_for_rt.get(rt_rf_id) <mask>: table_set = set() self._tables_for_rt[rt_rf_id] = table_set table_set.add(vrf_table) LOG.debug(""Added VrfTable %s to import RT table list: %s"", vrf_table, rt)",True,if table_set is None :,if table_set is None :,0.8820016898747209,100.00000000000004
"def add_tags( self, cve_results: Dict[str, Dict[str, Dict[str, str]]], file_object: FileObject ): # results structure: {'component': {'cve_id': {'score2': '6.4', 'score3': 'N/A'}}} for component in cve_results: for cve_id in cve_results[component]: entry = cve_results[component][cve_id] <mask>: self.add_analysis_tag( file_object, ""CVE"", ""critical CVE"", TagColor.RED, True ) return",False,if self . _entry_has_critical_rating ( entry ) :,"if entry [ ""cve_id"" ] == ""score2"" :",0.8820016898747209,3.716499092256817
"def _validate(self): try: super(CustomClassifier, self)._validate() except UnsupportedDataType: if self.dtype in FACTOR_DTYPES: raise UnsupportedDataType( typename=type(self).__name__, dtype=self.dtype, hint=""Did you mean to create a CustomFactor?"", ) <mask>: raise UnsupportedDataType( typename=type(self).__name__, dtype=self.dtype, hint=""Did you mean to create a CustomFilter?"", ) raise",False,elif self . dtype in FILTER_DTYPES :,if self . dtype in FILTER_DTYPES :,0.8820016898747209,86.33400213704509
"def formatMessage(self, record): recordcopy = copy(record) levelname = recordcopy.levelname seperator = "" "" * (8 - len(recordcopy.levelname)) if self.use_colors: levelname = self.color_level_name(levelname, recordcopy.levelno) <mask>: recordcopy.msg = recordcopy.__dict__[""color_message""] recordcopy.__dict__[""message""] = recordcopy.getMessage() recordcopy.__dict__[""levelprefix""] = levelname + "":"" + seperator return super().formatMessage(recordcopy)",False,"if ""color_message"" in recordcopy . __dict__ :",if levelname in self . colors :,0.8820016898747209,2.961853899298388
"def dumpregs(self): for reg in ( list(self.regs.retaddr) + list(self.regs.misc) + list(self.regs.common) + list(self.regs.flags) ): enum = self.get_reg_enum(reg) <mask>: debug(""# Could not dump register %r"" % reg) continue name = ""U.x86_const.UC_X86_REG_%s"" % reg.upper() value = self.uc.reg_read(enum) debug(""uc.reg_read(%(name)s) ==> %(value)x"" % locals())",False,if not reg or enum is None :,if enum is None :,0.8820016898747209,38.80684294761701
"def filter(self, lexer, stream): current_type = None current_value = None for ttype, value in stream: if ttype is current_type: current_value += value else: <mask>: yield current_type, current_value current_type = ttype current_value = value if current_type is not None: yield current_type, current_value",True,if current_type is not None :,if current_type is not None :,0.8820016898747209,100.00000000000004
"def _get_between(content, start, end=None): should_yield = False for line in content.split(""\n""): <mask>: should_yield = True continue if end and end in line: return if should_yield and line: yield line.strip().split("" "")[0]",False,if start in line :,if start and start in line :,0.8820016898747209,46.713797772820016
"def parse_git_config(path): """"""Parse git config file."""""" config = dict() section = None with open(os.path.join(path, ""config""), ""r"") as f: for line in f: line = line.strip() if line.startswith(""[""): section = line[1:-1].strip() config[section] = dict() <mask>: key, value = line.replace("" "", """").split(""="") config[section][key] = value return config",False,elif section :,"elif line . startswith ( ""="" ) :",0.8820016898747209,4.990049701936832
"def test_has_arg(fn, name, accept_all, expected): if isinstance(fn, str): context = dict() try: exec(""def {}: pass"".format(fn), context) except SyntaxError: <mask>: raise pytest.skip(""Function is not compatible with Python 2"") # Sometimes exec adds builtins to the context context.pop(""__builtins__"", None) (fn,) = context.values() assert has_arg(fn, name, accept_all) is expected",False,"if sys . version_info >= ( 3 , ) :","if sys . version_info [ 0 ] < ( 3 , 0 ) :",0.8820016898747209,41.69392927528885
"def ObjectExpression(self, properties, **kwargs): data = [] for prop in properties: self.emit(prop[""value""]) <mask>: raise NotImplementedError( ""ECMA 5.1 does not support computed object properties!"" ) data.append((to_key(prop[""key""]), prop[""kind""][0])) self.emit(""LOAD_OBJECT"", tuple(data))",False,"if prop [ ""computed"" ] :","if ""kind"" not in prop :",0.8820016898747209,8.25791079503452
"def run(self): for domain, locale, po in self.locales: <mask>: path = os.path.join(""locale"", locale, ""LC_MESSAGES"") else: path = os.path.join(self.build_dir, locale, ""LC_MESSAGES"") mo = os.path.join(path, ""%s.mo"" % domain) self.mkpath(path) self.spawn([""msgfmt"", ""-o"", mo, po])",False,if self . inplace :,"if domain == ""default"" :",0.8820016898747209,6.567274736060395
"def _compute_map(self, first_byte, second_byte=None): if first_byte != 0x0F: return ""XED_ILD_MAP0"" else: if second_byte == None: return ""XED_ILD_MAP1"" if second_byte == 0x38: return ""XED_ILD_MAP2"" <mask>: return ""XED_ILD_MAP3"" if second_byte == 0x0F and self.amd_enabled: return ""XED_ILD_MAPAMD"" die(""Unhandled escape {} / map {} bytes"".format(first_byte, second_byte))",False,if second_byte == 0x3A :,if second_byte == 0x0F and self . mip_enabled :,0.8820016898747209,36.362270465000705
"def parse_tag(self): buf = [] escaped = False for c in self.get_next_chars(): <mask>: buf.append(c) elif c == ""\\"": escaped = True elif c == "">"": return """".join(buf) else: buf.append(c) raise Exception(""Unclosed tag "" + """".join(buf))",True,if escaped :,if escaped :,0.8820016898747209,0.0
"def print_pairs(attrs=None, offset_y=0): fmt = "" ({0}:{1}) "" fmt_len = len(fmt) for bg, fg in get_fg_bg(): try: color = curses.color_pair(pair_number(fg, bg)) <mask>: for attr in attrs: color |= attr screen.addstr(offset_y + bg, fg * fmt_len, fmt.format(fg, bg), color) pass except curses.error: pass",False,if not attrs is None :,if attrs :,0.8820016898747209,0.0
"def _impl(inputs, input_types): data = inputs[0] axis = None keepdims = False if len(inputs) > 2: # default, torch have only data, axis=None, keepdims=False <mask>: axis = int(inputs[1]) elif _is_int_seq(inputs[1]): axis = inputs[1] else: axis = list(_infer_shape(inputs[1])) keepdims = bool(inputs[2]) return get_relay_op(name)(data, axis=axis, keepdims=keepdims)",False,"if isinstance ( inputs [ 1 ] , int ) :",if _is_int_tensor ( inputs [ 1 ] ) :,0.8820016898747209,32.55964126200301
"def run(self, args, **kwargs): # Filtering options if args.trace_tag: kwargs[""trace_tag""] = args.trace_tag if args.trigger_instance: kwargs[""trigger_instance""] = args.trigger_instance if args.execution: kwargs[""execution""] = args.execution if args.rule: kwargs[""rule""] = args.rule if args.sort_order: <mask>: kwargs[""sort_asc""] = True elif args.sort_order in [""desc"", ""descending""]: kwargs[""sort_desc""] = True return self.manager.query_with_count(limit=args.last, **kwargs)",False,"if args . sort_order in [ ""asc"" , ""ascending"" ] :","if args . sort_order in [ ""asc"" , ""descending"" ] :",0.8820016898747209,82.82477531331043
def retaddr(): sp = pwndbg.regs.sp stack = pwndbg.vmmap.find(sp) # Enumerate all return addresses frame = gdb.newest_frame() addresses = [] while frame: addresses.append(frame.pc()) frame = frame.older() # Find all of them on the stack start = stack.vaddr stop = start + stack.memsz while addresses and start < sp < stop: value = pwndbg.memory.u(sp) <mask>: index = addresses.index(value) del addresses[:index] print(pwndbg.chain.format(sp)) sp += pwndbg.arch.ptrsize,False,if value in addresses :,if value :,0.8820016898747209,0.0
"def update_from_dictio(self, dictio_item): for index, dictio_payload in enumerate(dictio_item, 1): fuzz_payload = None for fuzz_payload in self.payloads[index]: fuzz_payload.content = dictio_payload.content fuzz_payload.type = dictio_payload.type # payload generated not used in seed but in filters <mask>: self.add( {""full_marker"": None, ""word"": None, ""index"": index, ""field"": None}, dictio_item[index - 1], )",False,if fuzz_payload is None :,if fuzz_payload is not None :,0.8820016898747209,59.4603557501361
"def check_expected(result, expected, contains=False): if sys.version_info[0] >= 3: <mask>: result = result.encode(""ascii"") if isinstance(expected, str): expected = expected.encode(""ascii"") resultlines = result.splitlines() expectedlines = expected.splitlines() if len(resultlines) != len(expectedlines): return False for rline, eline in zip(resultlines, expectedlines): if contains: if eline not in rline: return False else: if not rline.endswith(eline): return False return True",True,"if isinstance ( result , str ) :","if isinstance ( result , str ) :",0.8820016898747209,100.00000000000004
"def execute_sql(self, sql, params=None, commit=True): try: cursor = super(RetryOperationalError, self).execute_sql(sql, params, commit) except OperationalError: if not self.is_closed(): self.close() with __exception_wrapper__: cursor = self.cursor() cursor.execute(sql, params or ()) <mask>: self.commit() return cursor",False,if commit and not self . in_transaction ( ) :,if not self . is_closed ( ) :,0.8820016898747209,23.11111848486415
"def get_operation_ast(document_ast, operation_name=None): operation = None for definition in document_ast.definitions: if isinstance(definition, ast.OperationDefinition): if not operation_name: # If no operation name is provided, only return an Operation if it is the only one present in the # document. This means that if we've encountered a second operation as we were iterating over the # definitions in the document, there are more than one Operation defined, and we should return None. <mask>: return None operation = definition elif definition.name and definition.name.value == operation_name: return definition return operation",False,if operation :,if definition . name . value == operation_name :,0.8820016898747209,4.456882760699063
"def removeTrailingWs(self, aList): i = 0 while i < len(aList): if self.is_ws(aList[i]): j = i i = self.skip_ws(aList, i) assert j < i <mask>: # print ""removing trailing ws:"", `i-j` del aList[j:i] i = j else: i += 1",False,"if i >= len ( aList ) or aList [ i ] == ""\n"" :",if aList [ j ] == aList [ j ] :,0.8820016898747209,9.152959627456559
"def _process_filter(self, query, host_state): """"""Recursively parse the query structure."""""" if not query: return True cmd = query[0] method = self.commands[cmd] cooked_args = [] for arg in query[1:]: <mask>: arg = self._process_filter(arg, host_state) elif isinstance(arg, basestring): arg = self._parse_string(arg, host_state) if arg is not None: cooked_args.append(arg) result = method(self, cooked_args) return result",False,"if isinstance ( arg , list ) :","if isinstance ( arg , ( list , tuple ) ) :",0.8820016898747209,37.70063804549471
"def handle_sent(self, elt): sent = [] for child in elt: if child.tag in (""mw"", ""hi"", ""corr"", ""trunc""): sent += [self.handle_word(w) for w in child] elif child.tag in (""w"", ""c""): sent.append(self.handle_word(child)) <mask>: raise ValueError(""Unexpected element %s"" % child.tag) return BNCSentence(elt.attrib[""n""], sent)",False,elif child . tag not in self . tags_to_ignore :,"elif child . tag != ""n"" :",0.8820016898747209,18.61893718748252
"def get_display_price( base: Union[TaxedMoney, TaxedMoneyRange], display_gross: bool = False ) -> Money: """"""Return the price amount that should be displayed based on settings."""""" if not display_gross: display_gross = display_gross_prices() if isinstance(base, TaxedMoneyRange): <mask>: base = MoneyRange(start=base.start.gross, stop=base.stop.gross) else: base = MoneyRange(start=base.start.net, stop=base.stop.net) if isinstance(base, TaxedMoney): base = base.gross if display_gross else base.net return base",True,if display_gross :,if display_gross :,0.8820016898747209,100.00000000000004
"def check_classes(self, node): if isinstance(node, nodes.Element): for class_value in node[""classes""][:]: if class_value in self.strip_classes: node[""classes""].remove(class_value) <mask>: return 1",False,if class_value in self . strip_elements :,"if node [ ""class"" ] == self . strip_classes :",0.8820016898747209,18.92240568795936
"def validate(outfile=sys.stdout, silent_success=False): ""Validates all installed models."" try: num_errors = get_validation_errors(outfile) <mask>: return outfile.write( ""%s error%s found.\n"" % (num_errors, num_errors != 1 and ""s"" or """") ) except ImproperlyConfigured: outfile.write(""Skipping validation because things aren't configured properly."")",False,if silent_success and num_errors == 0 :,if silent_success :,0.8820016898747209,17.437038542312457
"def check_basename_conflicts(self, targets): """"""Apps' basenames are used as bundle directory names. Ensure they are all unique."""""" basename_seen = {} for target in targets: <mask>: raise self.BasenameConflictError( ""Basename must be unique, found two targets use "" ""the same basename: {}'\n\t{} and \n\t{}"".format( target.basename, basename_seen[target.basename].address.spec, target.address.spec, ) ) basename_seen[target.basename] = target",True,if target . basename in basename_seen :,if target . basename in basename_seen :,0.8820016898747209,100.00000000000004
"def __init__(self, api_version_str): try: self.latest = self.preview = False self.yyyy = self.mm = self.dd = None <mask>: self.latest = True else: if ""preview"" in api_version_str: self.preview = True parts = api_version_str.split(""-"") self.yyyy = int(parts[0]) self.mm = int(parts[1]) self.dd = int(parts[2]) except (ValueError, TypeError): raise ValueError( ""The API version {} is not in a "" ""supported format"".format(api_version_str) )",False,"if api_version_str == ""latest"" :","if ""latest"" in api_version_str :",0.8820016898747209,45.305163015763085
"def _osp2ec(self, bytes): compressed = self._from_bytes(bytes) y = compressed >> self._bits x = compressed & (1 << self._bits) - 1 if x == 0: y = self._curve.b else: result = self.sqrtp( x ** 3 + self._curve.a * x + self._curve.b, self._curve.field.p ) <mask>: y = result[0] elif len(result) == 2: y1, y2 = result y = y1 if (y1 & 1 == y) else y2 else: return None return ec.Point(self._curve, x, y)",True,if len ( result ) == 1 :,if len ( result ) == 1 :,0.8820016898747209,100.00000000000004
"def _visit_import_alike(self, node: Union[cst.Import, cst.ImportFrom]) -> bool: names = node.names if isinstance(names, cst.ImportStar): return False # make sure node.names is Sequence[ImportAlias] for name in names: self.provider.set_metadata(name, self.scope) asname = name.asname <mask>: name_values = _gen_dotted_names(cst.ensure_type(asname.name, cst.Name)) else: name_values = _gen_dotted_names(name.name) for name_value, _ in name_values: self.scope.record_assignment(name_value, node) return False",False,if asname is not None :,if asname :,0.8820016898747209,0.0
"def test_sanity_no_unmatched_parentheses(CorpusType: Type[ColumnCorpus]): corpus = CorpusType() unbalanced_entities = [] for sentence in corpus.get_all_sentences(): entities = sentence.get_spans(""ner"") for entity in entities: entity_text = """".join(t.text for t in entity.tokens) <mask>: unbalanced_entities.append(entity_text) assert unbalanced_entities == []",False,if not has_balanced_parantheses ( entity_text ) :,if entity_text not in unbalanced_entities :,0.8820016898747209,14.301399262246576
"def _learn_rate_adjust(self): if self.learn_rate_decays == 1.0: return learn_rate_decays = self._vp(self.learn_rate_decays) learn_rate_minimums = self._vp(self.learn_rate_minimums) for index, decay in enumerate(learn_rate_decays): new_learn_rate = self.net_.learnRates[index] * decay <mask>: self.net_.learnRates[index] = new_learn_rate if self.verbose >= 2: print(""Learn rates: {}"".format(self.net_.learnRates))",False,if new_learn_rate >= learn_rate_minimums [ index ] :,if new_learn_rate < self . net_learnRates [ index ] :,0.8820016898747209,44.05136963304349
"def set_attr_from_xmp_tag(self, attr, xmp_tags, tags, cast=None): v = self.get_xmp_tag(xmp_tags, tags) if v is not None: <mask>: setattr(self, attr, v) else: # Handle fractions if (cast == float or cast == int) and ""/"" in v: v = self.try_parse_fraction(v) setattr(self, attr, cast(v))",True,if cast is None :,if cast is None :,0.8820016898747209,100.00000000000004
"def _merge_scientific_float_tokens(tokens: Iterable[str]) -> List[str]: tokens = list(tokens) i = 0 while ""e"" in tokens[i + 1 :]: i = tokens.index(""e"", i + 1) s = i - 1 e = i + 1 <mask>: continue if re.match(""[+-]"", str(tokens[e])): e += 1 if re.match(""[0-9]"", str(tokens[e])): e += 1 tokens[s:e] = ["""".join(tokens[s:e])] i -= 1 return tokens",False,"if not re . match ( ""[0-9]"" , str ( tokens [ s ] ) ) :",if s == 0 :,0.8820016898747209,0.6751392346890166
"def anypython(request): name = request.param executable = getexecutable(name) if executable is None: <mask>: executable = winpymap.get(name, None) if executable: executable = py.path.local(executable) if executable.check(): return executable pytest.skip(""no suitable %s found"" % (name,)) return executable",False,"if sys . platform == ""win32"" :",if name in winpymap :,0.8820016898747209,4.673289785800722
"def set_meta(self, dataset, overwrite=True, **kwd): super().set_meta(dataset, overwrite=overwrite, **kwd) try: <mask>: with tarfile.open(dataset.file_name, ""r"") as temptar: dataset.metadata.fast5_count = sum( 1 for f in temptar if f.name.endswith("".fast5"") ) except Exception as e: log.warning(""%s, set_meta Exception: %s"", self, e)",False,if dataset and tarfile . is_tarfile ( dataset . file_name ) :,if dataset . file_name :,0.8820016898747209,19.548182395144593
"def run(self): for k in list(iterkeys(self.objs)): <mask>: continue v = self.objs[k] if v[""_class""] == ""User"": self.split_user(k, v) elif v[""_class""] in [ ""Message"", ""PrintJob"", ""Question"", ""Submission"", ""UserTest"", ]: v[""participation""] = v[""user""] del v[""user""] return self.objs",False,"if k . startswith ( ""_"" ) :",if k not in self . objs :,0.8820016898747209,10.229197414177778
"def _findInTree(t, n): ret = [] if type(t) is dict: <mask>: ret.append(t) for k, v in t.items(): ret += _findInTree(v, n) if type(t) is list: for v in t: ret += _findInTree(v, n) return ret",False,"if ""_name"" in t and t [ ""_name"" ] == n :",if n . is_dict ( t ) :,0.8820016898747209,2.551084474780675
"def parseArrayPattern(self): node = Node() elements = [] self.expect(""["") while not self.match(""]""): <mask>: self.lex() elements.append(null) else: if self.match(""...""): restNode = Node() self.lex() rest = self.parseVariableIdentifier() elements.append(restNode.finishRestElement(rest)) break else: elements.append(self.parsePatternWithDefault()) if not self.match(""]""): self.expect("","") self.expect(""]"") return node.finishArrayPattern(elements)",False,"if self . match ( "","" ) :","if self . match ( ""null"" ) :",0.8820016898747209,65.80370064762461
"def _set_log_writer(self): if self.config[""logging""]: config = self.config[""log_writer_config""] <mask>: self.log_writer = LogWriter(**config) elif config[""writer""] == ""tensorboard"": self.log_writer = TensorBoardWriter(**config) else: raise ValueError(f""Unrecognized writer option: {config['writer']}"") else: self.log_writer = None",False,"if config [ ""writer"" ] == ""json"" :","if config [ ""writer"" ] == ""log"" :",0.8820016898747209,79.10665071754353
"def _parse(self, contents): entries = [] hostnames_found = set() for line in contents.splitlines(): <mask>: entries.append((""blank"", [line])) continue (head, tail) = chop_comment(line.strip(), ""#"") if not len(head): entries.append((""all_comment"", [line])) continue entries.append((""hostname"", [head, tail])) hostnames_found.add(head) if len(hostnames_found) > 1: raise IOError(""Multiple hostnames (%s) found!"" % (hostnames_found)) return entries",False,if not len ( line . strip ( ) ) :,if not line . strip ( ) :,0.8820016898747209,44.22423924264936
"def get_all_values(self, project): if isinstance(project, models.Model): project_id = project.id else: project_id = project if project_id not in self.__cache: cache_key = self._make_key(project_id) result = cache.get(cache_key) <mask>: result = self.reload_cache(project_id) else: self.__cache[project_id] = result return self.__cache.get(project_id, {})",True,if result is None :,if result is None :,0.8820016898747209,100.00000000000004
"def needed_libraries(self): for cmd in self.load_commands_of_type(0xC): # LC_LOAD_DYLIB tname = self._get_typename(""dylib_command"") dylib_command = cmd.cast(tname) name_addr = cmd.obj_offset + dylib_command.name dylib_name = self.obj_vm.read(name_addr, 256) <mask>: idx = dylib_name.find(""\x00"") if idx != -1: dylib_name = dylib_name[:idx] yield dylib_name",True,if dylib_name :,if dylib_name :,0.8820016898747209,100.00000000000004
"def compress(self, data_list): warn_untested() if data_list: <mask>: error = self.error_messages[""invalid_year""] raise forms.ValidationError(error) if data_list[0] in forms.fields.EMPTY_VALUES: error = self.error_messages[""invalid_month""] raise forms.ValidationError(error) year = int(data_list[1]) month = int(data_list[0]) # find last day of the month day = monthrange(year, month)[1] return date(year, month, day) return None",True,if data_list [ 1 ] in forms . fields . EMPTY_VALUES :,if data_list [ 1 ] in forms . fields . EMPTY_VALUES :,0.8820016898747209,100.00000000000004
"def put(self, obj, block=True, timeout=None): assert not self._closed if not self._sem.acquire(block, timeout): raise Full with self._notempty: with self._cond: <mask>: self._start_thread() self._buffer.append(obj) self._unfinished_tasks.release() self._notempty.notify()",False,if self . _thread is None :,if self . _finished_tasks . is_set ( ) :,0.8820016898747209,19.67497981115564
"def has_module(self, module, version): has_module = False for directory in self.directories: module_directory = join(directory, module) has_module_directory = isdir(module_directory) if not version: has_module = has_module_directory or exists( module_directory ) # could be a bare modulefile else: modulefile = join(module_directory, version) has_modulefile = exists(modulefile) has_module = has_module_directory and has_modulefile <mask>: break return has_module",False,if has_module :,if not has_module :,0.8820016898747209,53.7284965911771
"def expanduser(path): if path[:1] == ""~"": c = path[1:2] <mask>: return gethome() if c == os.sep: return asPyString(File(gethome(), path[2:]).getPath()) return path",False,if not c :,if c == os . path . expanduser ( path [ 2 : ] ) :,0.8820016898747209,3.0098043843528286
"def mock_touch(self, bearer, version=None, revision=None, **kwargs): if version: <mask>: try: return self.versions[int(version) - 1] except (IndexError, ValueError): return None else: return None return file_models.FileVersion()",False,if self . versions :,if version > 0 :,0.8820016898747209,12.703318703865365
"def _get_field_value(self, test, key, match): if test.ver == ofproto_v1_0.OFP_VERSION: members = inspect.getmembers(match) for member in members: if member[0] == key: field_value = member[1] <mask>: wildcards = member[1] if key == ""nw_src"": field_value = test.nw_src_to_str(wildcards, field_value) elif key == ""nw_dst"": field_value = test.nw_dst_to_str(wildcards, field_value) else: field_value = match[key] return field_value",True,"elif member [ 0 ] == ""wildcards"" :","elif member [ 0 ] == ""wildcards"" :",0.8820016898747209,100.00000000000004
"def check_expected(result, expected, contains=False): if sys.version_info[0] >= 3: if isinstance(result, str): result = result.encode(""ascii"") if isinstance(expected, str): expected = expected.encode(""ascii"") resultlines = result.splitlines() expectedlines = expected.splitlines() if len(resultlines) != len(expectedlines): return False for rline, eline in zip(resultlines, expectedlines): <mask>: if eline not in rline: return False else: if not rline.endswith(eline): return False return True",True,if contains :,if contains :,0.8820016898747209,0.0
"def OnKeyUp(self, event): if self._properties.modifiable: if event.GetKeyCode() == wx.WXK_ESCAPE: self._cancel_editing() <mask>: self._update_value() elif event.GetKeyCode() == wx.WXK_DELETE: self.SetValue("""") if event.GetKeyCode() != wx.WXK_RETURN: # Don't send skip event if enter key is pressed # On some platforms this event is sent too late and causes crash event.Skip()",False,elif event . GetKeyCode ( ) == wx . WXK_RETURN :,elif event . GetKeyCode ( ) == wx . WXK_ENTER :,0.8820016898747209,85.5526185871245
"def load_modules( to_load, load, attr, modules_dict, excluded_aliases, loading_message=None ): if loading_message: print(loading_message) for name in to_load: module = load(name) <mask>: continue cls = getattr(module, attr) if hasattr(cls, ""initialize"") and not cls.initialize(): continue if hasattr(module, ""aliases""): for alias in module.aliases(): if alias not in excluded_aliases: modules_dict[alias] = module else: modules_dict[name] = module if loading_message: print()",False,"if module is None or not hasattr ( module , attr ) :",if not module :,0.8820016898747209,2.3809737623256155
def eventIterator(): while True: yield eventmodule.wait() while True: event = eventmodule.poll() <mask>: break else: yield event,False,if event . type == NOEVENT :,if event is None :,0.8820016898747209,12.975849993980741
"def _get_state_without_padding(self, state_with_padding, padding): lean_state = {} for key, value in state_with_padding.items(): <mask>: lean_length = value.numel() - padding lean_state[key] = value[:lean_length] else: lean_state[key] = value return lean_state",False,if torch . is_tensor ( value ) :,"if isinstance ( value , ( list , tuple ) ) :",0.8820016898747209,10.127993013562818
"def _get_validate(data): """"""Retrieve items to validate, from single samples or from combined joint calls."""""" if data.get(""vrn_file"") and tz.get_in([""config"", ""algorithm"", ""validate""], data): return utils.deepish_copy(data) elif ""group_orig"" in data: for sub in multi.get_orig_items(data): <mask>: sub_val = utils.deepish_copy(sub) sub_val[""vrn_file""] = data[""vrn_file""] return sub_val return None",False,"if ""validate"" in sub [ ""config"" ] [ ""algorithm"" ] :","if sub . get ( ""vrn_file"" ) :",0.8820016898747209,3.3383922484634225
"def OnPopup(self, form, popup_handle): for num, action_name, menu_name, shortcut in self.actions: <mask>: ida_kernwin.attach_action_to_popup(form, popup_handle, None) else: handler = command_handler_t(self, num, 2) desc = ida_kernwin.action_desc_t(action_name, menu_name, handler, shortcut) ida_kernwin.attach_dynamic_action_to_popup(form, popup_handle, desc)",False,if menu_name is None :,if num == 0 :,0.8820016898747209,8.170609724417774
"def show(self, indent=0): """"""Pretty print this structure."""""" if indent == 0: print(""struct {}"".format(self.name)) for field in self.fields: <mask>: offset = ""0x??"" else: offset = ""0x{:02x}"".format(field.offset) print(""{}+{} {} {}"".format("" "" * indent, offset, field.name, field.type)) if isinstance(field.type, Structure): field.type.show(indent + 1)",False,if field . offset is None :,if field . offset == 0 :,0.8820016898747209,36.55552228545123
"def get_operation_ast(document_ast, operation_name=None): operation = None for definition in document_ast.definitions: <mask>: if not operation_name: # If no operation name is provided, only return an Operation if it is the only one present in the # document. This means that if we've encountered a second operation as we were iterating over the # definitions in the document, there are more than one Operation defined, and we should return None. if operation: return None operation = definition elif definition.name and definition.name.value == operation_name: return definition return operation",False,"if isinstance ( definition , ast . OperationDefinition ) :",if definition . name and definition . name . value == operation_name :,0.8820016898747209,3.4585921141027365
"def getSubMenu(self, callingWindow, context, mainItem, selection, rootMenu, i, pitem): msw = True if ""wxMSW"" in wx.PlatformInfo else False self.context = context self.abilityIds = {} sub = wx.Menu() for ability in self.fighter.abilities: <mask>: continue menuItem = self.addAbility(rootMenu if msw else sub, ability) sub.Append(menuItem) menuItem.Check(ability.active) return sub",False,if not ability . effect . isImplemented :,if ability . active :,0.8820016898747209,13.943458243384402
"def consume(self, event: Dict[str, Any]) -> None: with self.lock: logging.debug(""Received missedmessage_emails event: %s"", event) # When we process an event, just put it into the queue and ensure we have a timer going. user_profile_id = event[""user_profile_id""] <mask>: self.batch_start_by_recipient[user_profile_id] = time.time() self.events_by_recipient[user_profile_id].append(event) self.ensure_timer()",False,if user_profile_id not in self . batch_start_by_recipient :,if user_profile_id not in self . events_by_recipient :,0.8820016898747209,71.9548353625319
"def __init__(self, start_enabled=False, use_hardware=True): self._use_hardware = use_hardware if use_hardware: self._button = Button(BUTTON_GPIO_PIN) self._enabled = start_enabled <mask>: self._button.when_pressed = self._enable",False,if not start_enabled :,if self . _enabled :,0.8820016898747209,32.46679154750991
"def execute(cls, ctx, op: ""DataFrameGroupByAgg""): try: pd.set_option(""mode.use_inf_as_na"", op.use_inf_as_na) <mask>: cls._execute_map(ctx, op) elif op.stage == OperandStage.combine: cls._execute_combine(ctx, op) elif op.stage == OperandStage.agg: cls._execute_agg(ctx, op) else: # pragma: no cover raise ValueError(""Aggregation operand not executable"") finally: pd.reset_option(""mode.use_inf_as_na"")",True,if op . stage == OperandStage . map :,if op . stage == OperandStage . map :,0.8820016898747209,100.00000000000004
"def load_package(name, path): if os.path.isdir(path): extensions = machinery.SOURCE_SUFFIXES[:] + machinery.BYTECODE_SUFFIXES[:] for extension in extensions: init_path = os.path.join(path, ""__init__"" + extension) <mask>: path = init_path break else: raise ValueError(""{!r} is not a package"".format(path)) spec = util.spec_from_file_location(name, path, submodule_search_locations=[]) if name in sys.modules: return _exec(spec, sys.modules[name]) else: return _load(spec)",False,if os . path . exists ( init_path ) :,if os . path . isfile ( init_path ) :,0.8820016898747209,73.48889200874659
def setup(level=None): from pipeline.logging import pipeline_logger as logger from pipeline.log.handlers import EngineLogHandler if level in set(logging._levelToName.values()): logger.setLevel(level) logging._acquireLock() try: for hdl in logger.handlers: <mask>: break else: hdl = EngineLogHandler() hdl.setLevel(logger.level) logger.addHandler(hdl) finally: logging._releaseLock(),False,"if isinstance ( hdl , EngineLogHandler ) :",if hdl . isEnabledFor ( logging . level ) :,0.8820016898747209,10.552670315936318
"def find_approximant(x): c = 1e-4 it = sympy.ntheory.continued_fraction_convergents( sympy.ntheory.continued_fraction_iterator(x) ) for i in it: p, q = i.as_numer_denom() tol = c / q ** 2 if abs(i - x) <= tol: return i <mask>: break return x",False,if tol < machine_epsilon :,if abs ( i - x ) >= tol :,0.8820016898747209,4.9323515694897075
"def resolve( self, debug: bool = False, silent: bool = False, level: Optional[int] = None ) -> bool: if silent: spinner = nullcontext(type(""Mock"", (), {})) else: spinner = yaspin(text=""resolving..."") with spinner as spinner: while True: resolved = self._resolve( debug=debug, silent=silent, level=level, spinner=spinner ) <mask>: continue self.graph.clear() # remove unused deps from graph return resolved",True,if resolved is None :,if resolved is None :,0.8820016898747209,100.00000000000004
"def canonicalize_instruction_name(instr): name = instr.insn_name().upper() # XXX bypass a capstone bug that incorrectly labels some insns as mov if name == ""MOV"": <mask>: return ""LSR"" elif instr.mnemonic.startswith(""lsl""): return ""LSL"" elif instr.mnemonic.startswith(""asr""): return ""ASR"" return OP_NAME_MAP.get(name, name)",True,"if instr . mnemonic . startswith ( ""lsr"" ) :","if instr . mnemonic . startswith ( ""lsr"" ) :",0.8820016898747209,100.00000000000004
"def run_all(rule_list, defined_variables, defined_actions, stop_on_first_trigger=False): rule_was_triggered = False for rule in rule_list: result = run(rule, defined_variables, defined_actions) if result: rule_was_triggered = True <mask>: return True return rule_was_triggered",True,if stop_on_first_trigger :,if stop_on_first_trigger :,0.8820016898747209,100.00000000000004
"def get_filters(self, request): filter_specs = [] if self.lookup_opts.admin.list_filter and not self.opts.one_to_one_field: filter_fields = [ self.lookup_opts.get_field(field_name) for field_name in self.lookup_opts.admin.list_filter ] for f in filter_fields: spec = FilterSpec.create(f, request, self.params, self.model) <mask>: filter_specs.append(spec) return filter_specs, bool(filter_specs)",False,if spec and spec . has_output ( ) :,if spec . is_valid ( ) :,0.8820016898747209,20.88702936655045
"def get_type(type_ref): kind = type_ref.get(""kind"") if kind == TypeKind.LIST: item_ref = type_ref.get(""ofType"") <mask>: raise Exception(""Decorated type deeper than introspection query."") return GraphQLList(get_type(item_ref)) elif kind == TypeKind.NON_NULL: nullable_ref = type_ref.get(""ofType"") if not nullable_ref: raise Exception(""Decorated type deeper than introspection query."") return GraphQLNonNull(get_type(nullable_ref)) return get_named_type(type_ref[""name""])",True,if not item_ref :,if not item_ref :,0.8820016898747209,100.00000000000004
"def _1_0_cloud_ips_cip_jsjc5_map(self, method, url, body, headers): if method == ""POST"": body = json.loads(body) <mask>: return self.test_response(httplib.ACCEPTED, """") else: data = '{""error_name"":""bad destination"", ""errors"": [""Bad destination""]}' return self.test_response(httplib.BAD_REQUEST, data)",False,"if ""destination"" in body :","if body [ ""destination"" ] == ""1.0.cloud_ips.cip.jsjc5"" :",0.8820016898747209,8.039313477786735
"def _get_prefixed_values(data, prefix): """"""Collect lines which start with prefix; with trimming"""""" matches = [] for line in data.splitlines(): line = line.strip() <mask>: match = line[len(prefix) :] match = match.strip() matches.append(match) return matches",True,if line . startswith ( prefix ) :,if line . startswith ( prefix ) :,0.8820016898747209,100.00000000000004
"def _power_exact(y, xc, yc, xe): yc, ye = y.int, y.exp while yc % 10 == 0: yc //= 10 ye += 1 if xc == 1: xe *= yc while xe % 10 == 0: xe //= 10 ye += 1 <mask>: return None exponent = xe * 10 ** ye if y and xe: xc = exponent else: xc = 0 return 5",False,if ye < 0 :,if xc == 1 :,0.8820016898747209,9.652434877402245
"def init(self, view, items=None): selections = [] if view.sel(): for region in view.sel(): selections.append(view.substr(region)) values = [] for idx, index in enumerate(map(int, items)): if idx >= len(selections): break i = index - 1 if i >= 0 and i < len(selections): values.append(selections[i]) else: values.append(None) # fill up for idx, value in enumerate(selections): <mask>: values.append(value) self.stack = values",False,if len ( values ) + 1 < idx :,if idx == 0 and value != 0 :,0.8820016898747209,4.9323515694897075
"def toggleFactorReload(self, value=None): self.serviceFittingOptions[""useGlobalForceReload""] = ( value if value is not None else not self.serviceFittingOptions[""useGlobalForceReload""] ) fitIDs = set() for fit in set(self._loadedFits): if fit is None: continue <mask>: fit.factorReload = self.serviceFittingOptions[""useGlobalForceReload""] fit.clearFactorReloadDependentData() fitIDs.add(fit.ID) return fitIDs",False,if fit . calculated :,if fit . factorReload is None :,0.8820016898747209,26.269098944241588
"def init_weights(self): """"""Initialize model weights."""""" for m in self.predict_layers.modules(): if isinstance(m, nn.Conv2d): kaiming_init(m) <mask>: constant_init(m, 1) elif isinstance(m, nn.Linear): normal_init(m, std=0.01)",False,"elif isinstance ( m , nn . BatchNorm2d ) :","elif isinstance ( m , nn . Conv2d ) :",0.8820016898747209,70.71067811865478
"def _unzip_file(self, filepath, ext): try: <mask>: zf = zipfile.ZipFile(filepath) zf.extractall(os.path.dirname(filepath)) zf.close() elif ext == "".tar"": tf = tarfile.open(filepath) tf.extractall(os.path.dirname(filepath)) tf.close() except Exception as e: raise ValueError(""Error reading file %r!\n%s"" % (filepath, e))",True,"if ext == "".zip"" :","if ext == "".zip"" :",0.8820016898747209,100.00000000000004
"def add_multiple_tasks(data, parent): data = json.loads(data) new_doc = { ""doctype"": ""Task"", ""parent_task"": parent if parent != ""All Tasks"" else """", } new_doc[""project""] = frappe.db.get_value(""Task"", {""name"": parent}, ""project"") or """" for d in data: <mask>: continue new_doc[""subject""] = d.get(""subject"") new_task = frappe.get_doc(new_doc) new_task.insert()",False,"if not d . get ( ""subject"" ) :","if d . get ( ""doctype"" ) != ""Task"" :",0.8820016898747209,30.130404892785695
"def filterSimilarKeywords(keyword, kwdsIterator): """"""Return a sorted list of keywords similar to the one given."""""" seenDict = {} kwdSndx = soundex(keyword.encode(""ascii"", ""ignore"")) matches = [] matchesappend = matches.append checkContained = False if len(keyword) > 4: checkContained = True for movieID, key in kwdsIterator: <mask>: continue seenDict[key] = None if checkContained and keyword in key: matchesappend(key) continue if kwdSndx == soundex(key.encode(""ascii"", ""ignore"")): matchesappend(key) return _sortKeywords(keyword, matches)",True,if key in seenDict :,if key in seenDict :,0.8820016898747209,100.00000000000004
"def visit_If(self, node): self.newline() self.write(""if "") self.visit(node.test) self.write("":"") self.body(node.body) while True: else_ = node.orelse <mask>: node = else_[0] self.newline() self.write(""elif "") self.visit(node.test) self.write("":"") self.body(node.body) else: self.newline() self.write(""else:"") self.body(else_) break",False,"if len ( else_ ) == 1 and isinstance ( else_ [ 0 ] , If ) :","if isinstance ( else_ , list ) :",0.8820016898747209,10.05403140773399
"def _eyeLinkHardwareAndSoftwareVersion(self): try: tracker_software_ver = 0 eyelink_ver = self._eyelink.getTrackerVersion() <mask>: tvstr = self._eyelink.getTrackerVersionString() vindex = tvstr.find(""EYELINK CL"") tracker_software_ver = int( float(tvstr[(vindex + len(""EYELINK CL"")) :].strip()) ) return eyelink_ver, tracker_software_ver except Exception: print2err(""EYELINK Error during _eyeLinkHardwareAndSoftwareVersion:"") printExceptionDetailsToStdErr() return EyeTrackerConstants.EYETRACKER_ERROR",False,if eyelink_ver == 3 :,if eyelink_ver :,0.8820016898747209,38.80684294761701
"def execute(self, context): for monad in context.blend_data.node_groups: if monad.bl_idname == ""SverchGroupTreeType"": <mask>: try: monad.update_cls() except Exception as err: print(err) print(""{} group class could not be created"".format(monad.name)) return {""FINISHED""}",False,"if not getattr ( bpy . types , monad . cls_bl_idname , None ) :","if monad . bl_idname == ""SverchGroupTreeType"" :",0.8820016898747209,9.621765470834335
"def word_pattern(pattern, str): dict = {} set_value = set() list_str = str.split() if len(list_str) != len(pattern): return False for i in range(len(pattern)): if pattern[i] not in dict: if list_str[i] in set_value: return False dict[pattern[i]] = list_str[i] set_value.add(list_str[i]) else: <mask>: return False return True",False,if dict [ pattern [ i ] ] != list_str [ i ] :,if list_str [ i ] not in set_value :,0.8820016898747209,30.06435681188122
"def decorator_handle(tokens): """"""Process decorators."""""" defs = [] decorates = [] for i, tok in enumerate(tokens): if ""simple"" in tok and len(tok) == 1: decorates.append(""@"" + tok[0]) <mask>: varname = decorator_var + ""_"" + str(i) defs.append(varname + "" = "" + tok[0]) decorates.append(""@"" + varname) else: raise CoconutInternalException(""invalid decorator tokens"", tok) return ""\n"".join(defs + decorates) + ""\n""",False,"elif ""test"" in tok and len ( tok ) == 1 :","elif ""decorator"" in tok and tok [ 0 ] == ""decorator"" :",0.8820016898747209,20.455163269401236
"def wait_impl(self, cpid): for i in range(10): # wait3() shouldn't hang, but some of the buildbots seem to hang # in the forking tests. This is an attempt to fix the problem. spid, status, rusage = os.wait3(os.WNOHANG) <mask>: break time.sleep(1.0) self.assertEqual(spid, cpid) self.assertEqual(status, 0, ""cause = %d, exit = %d"" % (status & 0xFF, status >> 8)) self.assertTrue(rusage)",False,if spid == cpid :,if spid == 0 :,0.8820016898747209,53.7284965911771
"def test_non_uniform_probabilities_over_elements(self): param = iap.Choice([0, 1], p=[0.25, 0.75]) samples = param.draw_samples((10000,)) unique, counts = np.unique(samples, return_counts=True) assert len(unique) == 2 for val, count in zip(unique, counts): <mask>: assert 2500 - 500 < count < 2500 + 500 elif val == 1: assert 7500 - 500 < count < 7500 + 500 else: assert False",True,if val == 0 :,if val == 0 :,0.8820016898747209,100.00000000000004
"def dispatch_return(self, frame, arg): if self.stop_here(frame) or frame == self.returnframe: # Ignore return events in generator except when stepping. if self.stopframe and frame.f_code.co_flags & CO_GENERATOR: return self.trace_dispatch try: self.frame_returning = frame self.user_return(frame, arg) finally: self.frame_returning = None <mask>: raise BdbQuit # The user issued a 'next' or 'until' command. if self.stopframe is frame and self.stoplineno != -1: self._set_stopinfo(None, None) return self.trace_dispatch",False,if self . quitting :,if self . stopframe :,0.8820016898747209,42.72870063962342
"def mouse(self, button, mods, x, y): if button == 1: for i in range(4): <mask>: self.hit = i elif button == -1: self.hit = None elif self.hit != None: self.coords[self.hit] = (x, y) self.view.dirty()",False,"if hypot ( x - self . coords [ i ] [ 0 ] , y - self . coords [ i ] [ 1 ] ) < 4 :",if mods [ i ] == x :,0.8820016898747209,2.046626413521275
"def __init__(self, *commands): self.all_cmds = list( map(lambda cmd: cmd[0] if isinstance(cmd, list) else cmd, commands) ) for command in commands: self.cmd = command if isinstance(command, list) else [command] self.cmd_path = pwndbg.which.which(self.cmd[0]) <mask>: break",False,if self . cmd_path :,"if self . cmd_path == ""all"" :",0.8820016898747209,43.36189090348677
"def _recv_obj(self, suppress_error=False): """"""Receive a (picklable) object"""""" if self.conn.closed: raise OSError(""handle is closed"") try: buf = self.conn.recv_bytes() except (ConnectionError, EOFError) as e: <mask>: return logger.debug(""receive has failed"", exc_info=e) try: self._set_remote_close_cause(e) raise PipeShutdownError() finally: self._close() obj = RemoteObjectUnpickler.loads(buf, self) logger.debug(""received %r"", obj) return obj",True,if suppress_error :,if suppress_error :,0.8820016898747209,100.00000000000004
"def act(self, obs): with chainer.no_backprop_mode(): batch_obs = self.batch_states([obs], self.xp, self.phi) action_distrib = self.model(batch_obs) <mask>: return chainer.cuda.to_cpu(action_distrib.most_probable.array)[0] else: return chainer.cuda.to_cpu(action_distrib.sample().array)[0]",False,if self . act_deterministically :,if self . is_most_probable :,0.8820016898747209,20.164945583740657
"def _classify(nodes_by_level): missing, invalid, downloads = [], [], [] for level in nodes_by_level: for node in level: if node.binary == BINARY_MISSING: missing.append(node) elif node.binary == BINARY_INVALID: invalid.append(node) <mask>: downloads.append(node) return missing, invalid, downloads",False,"elif node . binary in ( BINARY_UPDATE , BINARY_DOWNLOAD ) :",elif node . binary == BINARY_DOWNLOAD :,0.8820016898747209,23.825412935547586
"def persist(self, *_): for key, obj in self._objects.items(): try: state = obj.get_state() <mask>: continue md5 = hashlib.md5(state).hexdigest() if self._last_state.get(key) == md5: continue self._persist_provider.store(key, state) except Exception as e: system_log.exception(""PersistHelper.persist fail"") else: self._last_state[key] = md5",True,if not state :,if not state :,0.8820016898747209,100.00000000000004
"def enter(self, doc, **kwds): """"""Enters the mode, arranging for necessary grabs ASAP"""""" super(ColorPickMode, self).enter(doc, **kwds) if self._started_from_key_press: # Pick now using the last recorded event position doc = self.doc tdw = self.doc.tdw t, x, y = doc.get_last_event_info(tdw) <mask>: self._pick_color_mode(tdw, x, y, self._pickmode) # Start the drag when possible self._start_drag_on_next_motion_event = True self._needs_drag_start = True",False,"if None not in ( x , y ) :",if t == self . _last_event_position :,0.8820016898747209,3.673526562988939
"def on_profiles_loaded(self, profiles): cb = self.builder.get_object(""cbProfile"") model = cb.get_model() model.clear() for f in profiles: name = f.get_basename() <mask>: continue if name.endswith("".sccprofile""): name = name[0:-11] model.append((name, f, None)) cb.set_active(0)",False,"if name . endswith ( "".mod"" ) :","if name == ""profile"" :",0.8820016898747209,9.545138913210204
"def subprocess_post_check( completed_process: subprocess.CompletedProcess, raise_error: bool = True ) -> None: if completed_process.returncode: if completed_process.stdout is not None: print(completed_process.stdout, file=sys.stdout, end="""") <mask>: print(completed_process.stderr, file=sys.stderr, end="""") if raise_error: raise PipxError( f""{' '.join([str(x) for x in completed_process.args])!r} failed"" ) else: logger.info(f""{' '.join(completed_process.args)!r} failed"")",True,if completed_process . stderr is not None :,if completed_process . stderr is not None :,0.8820016898747209,100.00000000000004
"def test_connect( ipaddr, port, device, partition, method, path, headers=None, query_string=None ): if path == ""/a"": for k, v in headers.iteritems(): <mask>: break else: test_errors.append(""%s: %s not in %s"" % (test_header, test_value, headers))",False,if k . lower ( ) == test_header . lower ( ) and v == test_value :,if k == test_header :,0.8820016898747209,9.428081681249603
"def test_stat_result_pickle(self): result = os.stat(self.fname) for proto in range(pickle.HIGHEST_PROTOCOL + 1): p = pickle.dumps(result, proto) self.assertIn(b""stat_result"", p) <mask>: self.assertIn(b""cos\nstat_result\n"", p) unpickled = pickle.loads(p) self.assertEqual(result, unpickled)",False,if proto < 4 :,if proto == pickle . HIGHEST_PROTOCOL :,0.8820016898747209,9.287528999566801
"def run_sql(sql): table = sql.split("" "")[5] logger.info(""Updating table {}"".format(table)) with transaction.atomic(): with connection.cursor() as cursor: cursor.execute(sql) rows = cursor.fetchall() <mask>: raise Exception(""Sentry notification that {} is migrated"".format(table))",False,if not rows :,if rows :,0.8820016898747209,0.0
"def countbox(self): self.box = [1000, 1000, -1000, -1000] for x, y in self.body: if x < self.box[0]: self.box[0] = x <mask>: self.box[2] = x if y < self.box[1]: self.box[1] = y if y > self.box[3]: self.box[3] = y",True,if x > self . box [ 2 ] :,if x > self . box [ 2 ] :,0.8820016898747209,100.00000000000004
"def _packageFocusOutViaKeyPress(self, row, column, txt): if txt: self._set_current_cell(row + 1, column) else: widget = self.cellWidget(row + 1, column) <mask>: self._delete_cell(row, column) new_request = self.get_request() self.context_model.set_request(new_request) self._update_request_column(column, self.context_model)",False,"if widget and isinstance ( widget , PackageSelectWidget ) :",if widget . is_focusable ( ) :,0.8820016898747209,12.759307794697138
"def parse_bash_set_output(output): """"""Parse Bash-like 'set' output"""""" if not sys.platform.startswith(""win""): # Replace ""\""-continued lines in *Linux* environment dumps. # Cannot do this on Windows because a ""\"" at the end of the # line does not imply a continuation. output = output.replace(""\\\n"", """") environ = {} for line in output.splitlines(0): line = line.rstrip() <mask>: continue # skip black lines item = _ParseBashEnvStr(line) if item: environ[item[0]] = item[1] return environ",True,if not line :,if not line :,0.8820016898747209,100.00000000000004
"def _get(self, domain): with self.lock: try: record = self.cache[domain] time_now = time.time() <mask>: record = None except KeyError: record = None if not record: record = {""r"": ""unknown"", ""dns"": {}, ""g"": 1, ""query_count"": 0} # self.cache[domain] = record return record",False,"if time_now - record [ ""update"" ] > self . ttl :",if time_now > self . cache_timeout :,0.8820016898747209,22.269344484776322
"def test_filehash(self): """"""tests the hashes of the files in data/"""""" fp = self.get_data_path() for fn in os.listdir(fp): <mask>: # file used for something else continue expected_hash = fn fullp = os.path.join(fp, fn) output = self.run_command(""sha1sum "" + fullp, exitcode=0) result = output.split("" "")[0] self.assertEqual(result, expected_hash)",False,"if ""."" in fn :","if fn . endswith ( "".md"" ) :",0.8820016898747209,9.864703138979419
"def test_new_vs_reference_code_stream_read_during_iter(read_idx, read_len, bytecode): reference = SlowCodeStream(bytecode) latest = CodeStream(bytecode) for index, (actual, expected) in enumerate(zip(latest, reference)): assert actual == expected if index == read_idx: readout_actual = latest.read(read_len) readout_expected = reference.read(read_len) assert readout_expected == readout_actual <mask>: assert latest.program_counter >= len(reference) else: assert latest.program_counter == reference.program_counter",False,if reference . program_counter >= len ( reference ) :,if index == read_idx :,0.8820016898747209,4.18031138310865
"def setup_logging(): try: logconfig = config.get(""logging_config_file"") <mask>: logging.config.fileConfig(logconfig, disable_existing_loggers=False) logger.info(""logging initialized"") logger.debug(""debug"") except Exception as e: print(""Unable to set logging configuration:"", str(e), file=sys.stderr) raise",False,if logconfig and os . path . exists ( logconfig ) :,if logconfig :,0.8820016898747209,0.0
"def all_words(filename): start_char = True for c in characters(filename): <mask>: word = """" if c.isalnum(): # We found the start of a word word = c.lower() start_char = False else: pass else: if c.isalnum(): word += c.lower() else: # We found end of word, emit it start_char = True yield word",False,if start_char == True :,if start_char :,0.8820016898747209,38.80684294761701
"def _get_nonce(self, url, new_nonce_url): if not self._nonces: logger.debug(""Requesting fresh nonce"") <mask>: response = self.head(url) else: # request a new nonce from the acme newNonce endpoint response = self._check_response(self.head(new_nonce_url), content_type=None) self._add_nonce(response) return self._nonces.pop()",True,if new_nonce_url is None :,if new_nonce_url is None :,0.8820016898747209,100.00000000000004
"def paragraph_is_fully_commented(lines, comment, main_language): """"""Is the paragraph fully commented?"""""" for i, line in enumerate(lines): if line.startswith(comment): <mask>: continue if is_magic(line, main_language): return False continue return i > 0 and _BLANK_LINE.match(line) return True",False,if line [ len ( comment ) : ] . lstrip ( ) . startswith ( comment ) :,if i == 0 :,0.8820016898747209,1.1057717812699017
"def gvariant_args(args: List[Any]) -> str: """"""Convert args into gvariant."""""" gvariant = """" for arg in args: if isinstance(arg, bool): gvariant += "" {}"".format(str(arg).lower()) <mask>: gvariant += f"" {arg}"" elif isinstance(arg, str): gvariant += f' ""{arg}""' else: gvariant += f"" {arg!s}"" return gvariant.lstrip()",False,"elif isinstance ( arg , ( int , float ) ) :","elif isinstance ( arg , int ) :",0.8820016898747209,37.28878639930421
"def _SkipGroup(buffer, pos, end): """"""Skip sub-group. Returns the new position."""""" while 1: (tag_bytes, pos) = ReadTag(buffer, pos) new_pos = SkipField(buffer, pos, end, tag_bytes) <mask>: return pos pos = new_pos",False,if new_pos == - 1 :,if new_pos == pos :,0.8820016898747209,62.401954419369176
"def update_participants(self, refresh=True): for participant in list(self.participants_dict): if participant is None or participant == self.simulator_config.broadcast_part: continue self.removeItem(self.participants_dict[participant]) self.participant_items.remove(self.participants_dict[participant]) del self.participants_dict[participant] for participant in self.simulator_config.participants: <mask>: self.participants_dict[participant].refresh() else: self.insert_participant(participant) if refresh: self.update_view()",True,if participant in self . participants_dict :,if participant in self . participants_dict :,0.8820016898747209,100.00000000000004
"def feature_reddit(layer_data, graph): feature = {} times = {} indxs = {} for _type in layer_data: if len(layer_data[_type]) == 0: continue idxs = np.array(list(layer_data[_type].keys())) tims = np.array(list(layer_data[_type].values()))[:, 1] feature[_type] = np.array( list(graph.node_feature[_type].loc[idxs, ""emb""]), dtype=np.float ) times[_type] = tims indxs[_type] = idxs <mask>: attr = feature[_type] return feature, times, indxs, attr",False,"if _type == ""def"" :",if len ( feature [ _type ] ) == 1 :,0.8820016898747209,9.669265690880861
"def _get_sort_map(tags): """"""See TAG_TO_SORT"""""" tts = {} for name, tag in tags.items(): if tag.has_sort: <mask>: tts[name] = ""%ssort"" % name if tag.internal: tts[""~%s"" % name] = ""~%ssort"" % name return tts",False,if tag . user :,if tag . internal :,0.8820016898747209,42.72870063962342
"def max_radius(iterator): radius_result = dict() for k, v in iterator: if v[0] not in radius_result: radius_result[v[0]] = v[1] <mask>: radius_result[v[0]] = v[1] return radius_result",False,elif v [ 1 ] >= radius_result [ v [ 0 ] ] :,if v [ 1 ] not in radius_result :,0.8820016898747209,20.334154400185707
"def run(self): pwd_found = [] if constant.user_dpapi and constant.user_dpapi.unlocked: main_vault_directory = os.path.join( constant.profile[""APPDATA""], u"".."", u""Local"", u""Microsoft"", u""Vault"" ) if os.path.exists(main_vault_directory): for vault_directory in os.listdir(main_vault_directory): cred = constant.user_dpapi.decrypt_vault( os.path.join(main_vault_directory, vault_directory) ) <mask>: pwd_found.append(cred) return pwd_found",True,if cred :,if cred :,0.8820016898747209,0.0
"def disconnect_sync(self, connection, close_connection=False): key = id(connection) ts = self.in_use.pop(key) if close_connection: self.connections_map.pop(key) self._connection_close_sync(connection) else: <mask>: self.connections_map.pop(key) self._connection_close_sync(connection) else: with self._lock_sync: heapq.heappush(self.connections_sync, (ts, key))",False,if self . stale_timeout and self . is_stale ( ts ) :,if ts == self . connections_sync :,0.8820016898747209,6.061512325492642
"def _populate_tree(self, element, d): """"""Populates an etree with attributes & elements, given a dict."""""" for k, v in d.iteritems(): if isinstance(v, dict): self._populate_dict(element, k, v) elif isinstance(v, list): self._populate_list(element, k, v) elif isinstance(v, bool): self._populate_bool(element, k, v) elif isinstance(v, basestring): self._populate_str(element, k, v) <mask>: self._populate_number(element, k, v)",False,"elif type ( v ) in [ int , float , long , complex ] :","elif isinstance ( v , int ) :",0.8820016898747209,5.55750946374376
"def readframes(self, nframes): if self._ssnd_seek_needed: self._ssnd_chunk.seek(0) dummy = self._ssnd_chunk.read(8) pos = self._soundpos * self._framesize <mask>: self._ssnd_chunk.seek(pos + 8) self._ssnd_seek_needed = 0 if nframes == 0: return """" data = self._ssnd_chunk.read(nframes * self._framesize) if self._convert and data: data = self._convert(data) self._soundpos = self._soundpos + len(data) / (self._nchannels * self._sampwidth) return data",True,if pos :,if pos :,0.8820016898747209,0.0
"def target_glob(tgt, hosts): ret = {} for host in hosts: <mask>: ret[host] = copy.deepcopy(__opts__.get(""roster_defaults"", {})) ret[host].update({""host"": host}) if __opts__.get(""ssh_user""): ret[host].update({""user"": __opts__[""ssh_user""]}) return ret",False,"if fnmatch . fnmatch ( tgt , host ) :",if host not in ret :,0.8820016898747209,5.484411595600381
"def get_attribute_value(self, nodeid, attr): with self._lock: self.logger.debug(""get attr val: %s %s"", nodeid, attr) if nodeid not in self._nodes: dv = ua.DataValue() dv.StatusCode = ua.StatusCode(ua.StatusCodes.BadNodeIdUnknown) return dv node = self._nodes[nodeid] if attr not in node.attributes: dv = ua.DataValue() dv.StatusCode = ua.StatusCode(ua.StatusCodes.BadAttributeIdInvalid) return dv attval = node.attributes[attr] <mask>: return attval.value_callback() return attval.value",False,if attval . value_callback :,"if hasattr ( attval , ""value_callback"" ) :",0.8820016898747209,14.991106946711685
"def remove_property(self, key): # type: (str) -> None with self.secure() as config: keys = key.split(""."") current_config = config for i, key in enumerate(keys): <mask>: return if i == len(keys) - 1: del current_config[key] break current_config = current_config[key]",False,if key not in current_config :,if key in current_config :,0.8820016898747209,61.29752413741059
"def _class_browser(parent): # Wrapper for htest try: file = __file__ except NameError: file = sys.argv[0] <mask>: file = sys.argv[1] else: file = sys.argv[0] dir, file = os.path.split(file) name = os.path.splitext(file)[0] flist = PyShell.PyShellFileList(parent) global file_open file_open = flist.open ClassBrowser(flist, name, [dir], _htest=True)",False,if sys . argv [ 1 : ] :,if sys . argv [ 1 ] :,0.8820016898747209,67.5291821812656
"def get_only_text_part(self, msg): count = 0 only_text_part = None for part in msg.walk(): if part.is_multipart(): continue count += 1 mimetype = part.get_content_type() or ""text/plain"" <mask>: return False else: only_text_part = part return only_text_part",False,"if mimetype != ""text/plain"" or count != 1 :","if mimetype != ""text/plain"" :",0.8820016898747209,54.80623193671364
"def should_keep_alive(commit_msg): result = False ci = get_current_ci() or """" for line in commit_msg.splitlines(): parts = line.strip(""# "").split("":"", 1) (key, val) = parts if len(parts) > 1 else (parts[0], """") if key == ""CI_KEEP_ALIVE"": ci_names = val.replace("","", "" "").lower().split() if val else [] <mask>: result = True return result",False,if len ( ci_names ) == 0 or ci . lower ( ) in ci_names :,if ci in ci_names :,0.8820016898747209,9.050415858572288
"def _calc_block_io(self, blkio): """"""Calculate block IO stats."""""" for stats in blkio[""io_service_bytes_recursive""]: if stats[""op""] == ""Read"": self._blk_read += stats[""value""] <mask>: self._blk_write += stats[""value""]",True,"elif stats [ ""op"" ] == ""Write"" :","elif stats [ ""op"" ] == ""Write"" :",0.8820016898747209,100.00000000000004
"def value_to_db_datetime(self, value): if value is None: return None # Oracle doesn't support tz-aware datetimes if timezone.is_aware(value): <mask>: value = value.astimezone(timezone.utc).replace(tzinfo=None) else: raise ValueError( ""Oracle backend does not support timezone-aware datetimes when USE_TZ is False."" ) return six.text_type(value)",False,if settings . USE_TZ :,if self . USE_TZ :,0.8820016898747209,64.34588841607616
"def load_state_dict(self, state_dict): for module_name, module_state_dict in state_dict.items(): if module_name in self.module_pool: <mask>: self.module_pool[module_name].module.load_state_dict(module_state_dict) else: self.module_pool[module_name].load_state_dict(module_state_dict) else: logging.info(f""Missing {module_name} in module_pool, skip it.."")",False,"if self . config [ ""dataparallel"" ] :","if isinstance ( self . module_pool [ module_name ] . module , Module ) :",0.8820016898747209,5.32864224277779
"def _unpack_scales(scales, vidxs): scaleData = [None, None, None] for i in range(3): if i >= min(len(scales), len(vidxs) // 2): break scale = scales[i] <mask>: vidx1, vidx2 = vidxs[i * 2], vidxs[i * 2 + 1] scaleData[i] = (int(vidx1), int(vidx2), float(scale)) return scaleData",False,if not math . isnan ( scale ) :,if scale != 0 :,0.8820016898747209,6.4790667469036025
"def __init__(self, factors, contrast_matrices, num_columns): self.factors = tuple(factors) factor_set = frozenset(factors) if not isinstance(contrast_matrices, dict): raise ValueError(""contrast_matrices must be dict"") for factor, contrast_matrix in six.iteritems(contrast_matrices): <mask>: raise ValueError(""Unexpected factor in contrast_matrices dict"") if not isinstance(contrast_matrix, ContrastMatrix): raise ValueError(""Expected a ContrastMatrix, not %r"" % (contrast_matrix,)) self.contrast_matrices = contrast_matrices if not isinstance(num_columns, six.integer_types): raise ValueError(""num_columns must be an integer"") self.num_columns = num_columns",True,if factor not in factor_set :,if factor not in factor_set :,0.8820016898747209,100.00000000000004
"def app(scope, receive, send): while True: message = await receive() <mask>: await send({""type"": ""websocket.accept""}) elif message[""type""] == ""websocket.receive"": pass elif message[""type""] == ""websocket.disconnect"": break",False,"if message [ ""type"" ] == ""websocket.connect"" :","if message [ ""type"" ] == ""websocket.accept"" :",0.8820016898747209,82.42367502646057
"def value__set(self, value): for i, (option, checked) in enumerate(self.options): <mask>: self.selectedIndex = i break else: raise ValueError( ""Option %r not found (from %s)"" % (value, "", "".join([repr(o) for o, c in self.options])) )",False,if option == str ( value ) :,if value == option :,0.8820016898747209,12.9581334938072
"def init_links(self): links = LinkCallback.find_links(self) callbacks = [] for link, src_plot, tgt_plot in links: cb = Link._callbacks[""bokeh""][type(link)] <mask>: continue callbacks.append(cb(self.root, link, src_plot, tgt_plot)) return callbacks",False,if src_plot is None or ( link . _requires_target and tgt_plot is None ) :,if not cb :,0.8820016898747209,0.21102530003841274
"def _validate_scalar_extensions(self) -> List[str]: errors = [] for extension in [ x for x in self.extensions if isinstance(x, GraphQLScalarTypeExtension) ]: extended = self.type_definitions.get(extension.name) ext_errors = _validate_extension( extended, extension.name, GraphQLScalarType, ""SCALAR"" ) errors.extend(ext_errors) <mask>: errors.extend(_validate_extension_directives(extension, extended, ""SCALAR"")) return errors",False,if not ext_errors :,if extension . directives :,0.8820016898747209,10.400597689005304
"def copy_tcltk(src, dest, symlink): """"""copy tcl/tk libraries on Windows (issue #93)"""""" for libversion in ""8.5"", ""8.6"": for libname in ""tcl"", ""tk"": srcdir = join(src, ""tcl"", libname + libversion) destdir = join(dest, ""tcl"", libname + libversion) # Only copy the dirs from the above combinations that exist <mask>: copyfileordir(srcdir, destdir, symlink)",False,if os . path . exists ( srcdir ) and not os . path . exists ( destdir ) :,if os . path . isdir ( srcdir ) :,0.8820016898747209,20.687381245863396
"def parse(self, response): try: content = response.content.decode(""utf-8"", ""ignore"") content = json.loads(content, strict=False) except: self.logger.error(""Fail to parse the response in json format"") return for item in content[""data""]: if ""objURL"" in item: img_url = self._decode_url(item[""objURL""]) <mask>: img_url = item[""hoverURL""] else: continue yield dict(file_url=img_url)",False,"elif ""hoverURL"" in item :","if ""hoverURL"" in item :",0.8820016898747209,80.91067115702207
"def check_and_reload(self): # Check if tables have been modified, if so reload for table_name, table_version in self._table_versions.items(): table = self.app.tool_data_tables.get(table_name, None) <mask>: return self.reload_genomes()",False,if table is not None and not table . is_current_version ( table_version ) :,if table and table . version == table_version :,0.8820016898747209,10.399769989211284
"def _get_query_defaults(self, query_defns): defaults = {} for k, v in query_defns.items(): try: <mask>: defaults[k] = self._get_default_obj(v[""schema""]) else: defaults[k] = v[""schema""][""default""] except KeyError: pass return defaults",False,"if v [ ""schema"" ] [ ""type"" ] == ""object"" :","if ""default"" in v :",0.8820016898747209,2.0401632902006996
"def ftp_login(host, port, username=None, password=None, anonymous=False): ret = False try: ftp = ftplib.FTP() ftp.connect(host, port, timeout=6) <mask>: ftp.login() else: ftp.login(username, password) ret = True ftp.quit() except Exception: pass return ret",True,if anonymous :,if anonymous :,0.8820016898747209,0.0
"def _getVolumeScalar(self): if self._volumeScalar is not None: return self._volumeScalar # use default elif self._value in dynamicStrToScalar: return dynamicStrToScalar[self._value] else: thisDynamic = self._value # ignore leading s like in sf if ""s"" in thisDynamic: thisDynamic = thisDynamic[1:] # ignore closing z like in fz if thisDynamic[-1] == ""z"": thisDynamic = thisDynamic[:-1] <mask>: return dynamicStrToScalar[thisDynamic] else: return dynamicStrToScalar[None]",True,if thisDynamic in dynamicStrToScalar :,if thisDynamic in dynamicStrToScalar :,0.8820016898747209,100.00000000000004
"def processCoords(coords): newcoords = deque() for (x, y, z) in coords: for _dir, offsets in faceDirections: if _dir == FaceYIncreasing: continue dx, dy, dz = offsets p = (x + dx, y + dy, z + dz) <mask>: continue nx, ny, nz = p if level.blockAt(nx, ny, nz) == 0: level.setBlockAt(nx, ny, nz, waterID) newcoords.append(p) return newcoords",False,if p not in box :,if p == 0 :,0.8820016898747209,17.965205598154213
"def _set_property(self, target_widget, pname, value): if pname == ""text"": wstate = str(target_widget[""state""]) <mask>: # change state temporarily target_widget[""state""] = ""normal"" target_widget.delete(""0"", tk.END) target_widget.insert(""0"", value) target_widget[""state""] = wstate else: super(EntryBaseBO, self)._set_property(target_widget, pname, value)",False,"if wstate != ""normal"" :",if wstate == tk . START :,0.8820016898747209,13.134549472120788
"def teardown(): try: time.sleep(1) except KeyboardInterrupt: return while launchers: p = launchers.pop() <mask>: try: p.stop() except Exception as e: print(e) pass if p.poll() is None: try: time.sleep(0.25) except KeyboardInterrupt: return if p.poll() is None: try: print(""cleaning up test process..."") p.signal(SIGKILL) except: print(""couldn't shutdown process: "", p)",False,if p . poll ( ) is None :,if p . poll ( ) is not None :,0.8820016898747209,70.71067811865478
"def checkAndRemoveDuplicate(self, node): for bucket in self.buckets: for n in bucket.getNodes(): <mask>: self.removeContact(n)",False,"if ( n . ip , n . port ) == ( node . ip , node . port ) and n . id != node . id :","if n . get ( ""node"" ) == node . get ( ""node"" ) :",0.8820016898747209,9.037249848601194
"def toString(): flags = u"""" try: if this.glob: flags += u""g"" <mask>: flags += u""i"" if this.multiline: flags += u""m"" except: pass v = this.value if this.value else ""(?:)"" return u""/%s/"" % v + flags",False,if this . ignore_case :,if this . iglob :,0.8820016898747209,28.641904579795423
"def import_submodules(package_name): package = sys.modules[package_name] results = {} for loader, name, is_pkg in pkgutil.iter_modules(package.__path__): full_name = package_name + ""."" + name module = importlib.import_module(full_name) setattr(sys.modules[__name__], name, module) results[full_name] = module if is_pkg: valid_pkg = import_submodules(full_name) <mask>: results.update(valid_pkg) return results",True,if valid_pkg :,if valid_pkg :,0.8820016898747209,100.00000000000004
"def _call(self, cmd): what = cmd[""command""] if what == ""list"": name = cmd[""properties""].get(""name"") <mask>: return {""watchers"": [""one"", ""two"", ""three""]} return {""pids"": [123, 456]} elif what == ""dstats"": return {""info"": {""pid"": 789}} elif what == ""listsockets"": return { ""status"": ""ok"", ""sockets"": [{""path"": self._unix, ""fd"": 5, ""name"": ""XXXX"", ""backlog"": 2048}], ""time"": 1369647058.967524, } raise NotImplementedError(cmd)",False,if name is None :,"if name == ""watchers"" :",0.8820016898747209,12.22307556087252
"def select(self): e = xlib.XEvent() while xlib.XPending(self._display): xlib.XNextEvent(self._display, e) # Key events are filtered by the xlib window event # handler so they get a shot at the prefiltered event. if e.xany.type not in (xlib.KeyPress, xlib.KeyRelease): <mask>: continue try: dispatch = self._window_map[e.xany.window] except KeyError: continue dispatch(e)",False,"if xlib . XFilterEvent ( e , e . xany . window ) :",if e . xany . window not in self . _window_map :,0.8820016898747209,27.668736912821906
"def translate(self, line): parsed = self.RE_LINE_PARSER.match(line) if parsed: value = parsed.group(3) stage = parsed.group(1) <mask>: # query string is rendered here return ""\n# HTTP Request:\n"" + self.stripslashes(value) elif stage == ""reply"": return ""\n\n# HTTP Response:\n"" + self.stripslashes(value) elif stage == ""header"": return value + ""\n"" else: return value return line",False,"if stage == ""send"" :","if stage == ""query"" :",0.8820016898747209,59.4603557501361
"def toString(): flags = u"""" try: <mask>: flags += u""g"" if this.ignore_case: flags += u""i"" if this.multiline: flags += u""m"" except: pass v = this.value if this.value else ""(?:)"" return u""/%s/"" % v + flags",False,if this . glob :,if this . ignoreCase :,0.8820016898747209,42.72870063962342
"def __exit__(self, *exc_info): super(WarningsChecker, self).__exit__(*exc_info) # only check if we're not currently handling an exception if all(a is None for a in exc_info): if self.expected_warning is not None: <mask>: __tracebackhide__ = True pytest.fail(""DID NOT WARN"")",False,if not any ( r . category in self . expected_warning for r in self ) :,if not self . expected_warning . is_warn ( ) :,0.8820016898747209,25.070170904599163
"def run(self): for k, v in iteritems(self.objs): if k.startswith(""_""): continue <mask>: if v[""email""] == """": v[""email""] = None if v[""ip""] == ""0.0.0.0"": v[""ip""] = None return self.objs",False,"if v [ ""_class"" ] == ""User"" :","if v [ ""type"" ] == ""email"" :",0.8820016898747209,48.764850158827386
"def list_stuff(self, upto=10, start_after=-1): for i in range(upto): if i <= start_after: continue <mask>: self.count += 1 raise TemporaryProblem if i == 7 and self.count < 4: self.count += 1 raise TemporaryProblem yield i",False,if i == 2 and self . count < 1 :,if i == 6 and self . count < 4 :,0.8820016898747209,54.52469119630866
"def check(self): tcp_client = self.tcp_create() if tcp_client.connect(): tcp_client.send(b""ABCDE"") response = tcp_client.recv(5) tcp_client.close() if response: <mask>: self.endianness = "">"" # BE elif response.startswith(b""ScMM""): self.endianness = ""<"" # LE return True # target is vulnerable return False # target is not vulnerable",False,"if response . startswith ( b""MMcS"" ) :","if response . startswith ( b""BM"" ) :",0.8820016898747209,70.16879391277372
"def copy_tree(self, src_dir, dst_dir, skip_variables=False): for src_root, _, files in os.walk(src_dir): if src_root != src_dir: rel_root = os.path.relpath(src_root, src_dir) else: rel_root = """" if skip_variables and rel_root.startswith(""variables""): continue dst_root = os.path.join(dst_dir, rel_root) <mask>: os.makedirs(dst_root) for f in files: shutil.copy(os.path.join(src_root, f), os.path.join(dst_root, f))",False,if not os . path . exists ( dst_root ) :,if not os . path . isdir ( dst_root ) :,0.8820016898747209,76.11606003349888
"def _set_hostport(self, host, port): if port is None: i = host.rfind("":"") j = host.rfind(""]"") # ipv6 addresses have [...] <mask>: try: port = int(host[i + 1 :]) except ValueError: raise InvalidURL(""nonnumeric port: '%s'"" % host[i + 1 :]) host = host[:i] else: port = self.default_port if host and host[0] == ""["" and host[-1] == ""]"": host = host[1:-1] self.host = host self.port = port",False,if i > j :,if i and j > 0 :,0.8820016898747209,16.515821590069034
"def _get_field_value(self, test, key, match): if test.ver == ofproto_v1_0.OFP_VERSION: members = inspect.getmembers(match) for member in members: if member[0] == key: field_value = member[1] elif member[0] == ""wildcards"": wildcards = member[1] <mask>: field_value = test.nw_src_to_str(wildcards, field_value) elif key == ""nw_dst"": field_value = test.nw_dst_to_str(wildcards, field_value) else: field_value = match[key] return field_value",False,"if key == ""nw_src"" :","elif key == ""nw_src"" :",0.8820016898747209,88.01117367933934
"def _clear_storage(): """"""Clear old files from storage."""""" hacs = get_hacs() storagefiles = [""hacs""] for s_f in storagefiles: path = f""{hacs.core.config_path}/.storage/{s_f}"" <mask>: hacs.log.info(f""Cleaning up old storage file {path}"") os.remove(path)",False,if os . path . isfile ( path ) :,if os . path . exists ( path ) :,0.8820016898747209,65.80370064762461
"def action_delete(self, ids): try: count = 0 # TODO: Optimize me for pk in ids: <mask>: count += 1 flash( ngettext( ""Record was successfully deleted."", ""%(count)s records were successfully deleted."", count, count=count, ), ""success"", ) except Exception as ex: flash(gettext(""Failed to delete records. %(error)s"", error=str(ex)), ""error"")",False,if self . delete_model ( self . get_one ( pk ) ) :,if pk . delete ( ) :,0.8820016898747209,5.119988697048061
"def test_inclusion(all_values): for values in [{""guid_2"", ""guid_1""}, {""guid_5"", ""guid_XXX""}, {""guid_2""}]: test_predicate = in_set(values, ""volume_guid"") included_values = set() for val in all_values: <mask>: included_values.add(val) assert included_values == all_values.intersection(values)",False,"if test_predicate . do_include ( { ""volume_guid"" : val } ) :",if test_predicate ( val ) :,0.8820016898747209,9.857708014755968
"def _get_attr(sdk_path, mod_attr_path, checked=True): try: attr_mod, attr_path = ( mod_attr_path.split(""#"") if ""#"" in mod_attr_path else (mod_attr_path, """") ) full_mod_path = ""{}.{}"".format(sdk_path, attr_mod) if attr_mod else sdk_path op = import_module(full_mod_path) if attr_path: # Only load attributes if needed for part in attr_path.split("".""): op = getattr(op, part) return op except (ImportError, AttributeError) as ex: <mask>: return None raise ex",True,if checked :,if checked :,0.8820016898747209,0.0
"def __exit__(self, exc_type, exc_val, exc_tb): if self.fusefat is not None: self.fusefat.send_signal(signal.SIGINT) # Allow 1s to return without sending terminate for count in range(10): time.sleep(0.1) <mask>: break else: self.fusefat.terminate() time.sleep(self.delay) assert not os.path.exists(self.canary) self.dev_null.close() shutil.rmtree(self.tmpdir)",False,if self . fusefat . poll ( ) is not None :,if count == 1 :,0.8820016898747209,3.550932348642477
"def check_context_processors(output): with output.section(""Context processors"") as section: processors = list( chain( *[ template[""OPTIONS""].get(""context_processors"", []) for template in settings.TEMPLATES ] ) ) required_processors = (""cms.context_processors.cms_settings"",) for processor in required_processors: <mask>: section.error( ""%s context processor must be in TEMPLATES option context_processors"" % processor )",True,if processor not in processors :,if processor not in processors :,0.8820016898747209,100.00000000000004
"def test_converters(self): response = self._get(""datatypes/converters"") self._assert_status_code_is(response, 200) converters_list = response.json() found_fasta_to_tabular = False for converter in converters_list: self._assert_has_key(converter, ""source"", ""target"", ""tool_id"") <mask>: found_fasta_to_tabular = True assert found_fasta_to_tabular",False,"if converter [ ""source"" ] == ""fasta"" and converter [ ""target"" ] == ""tabular"" :","if converter . get ( ""target"" ) == ""fasta"" :",0.8820016898747209,19.65746616918952
"def remove_pid(self, watcher, pid): if pid in self._pids[watcher]: logger.debug(""Removing %d from %s"" % (pid, watcher)) self._pids[watcher].remove(pid) <mask>: logger.debug(""Stopping the periodic callback for {0}"".format(watcher)) self._callbacks[watcher].stop()",False,if len ( self . _pids [ watcher ] ) == 0 :,if self . _callbacks [ watcher ] :,0.8820016898747209,16.466920669770428
"def _fc_layer(self, sess, bottom, name, trainable=True, relu=True): with tf.variable_scope(name) as scope: shape = bottom.get_shape().as_list() dim = 1 for d in shape[1:]: dim *= d x = tf.reshape(bottom, [-1, dim]) weight = self._get_fc_weight(sess, name, trainable=trainable) bias = self._get_bias(sess, name, trainable=trainable) fc = tf.nn.bias_add(tf.matmul(x, weight), bias) <mask>: fc = tf.nn.relu(fc) return fc",True,if relu :,if relu :,0.8820016898747209,0.0
"def get_drive(self, root_path="""", volume_guid_path=""""): for drive in self.drives: if root_path: config_root_path = drive.get(""root_path"") <mask>: return drive elif volume_guid_path: config_volume_guid_path = drive.get(""volume_guid_path"") if config_volume_guid_path and config_volume_guid_path == volume_guid_path: return drive",False,if config_root_path and root_path == config_root_path :,if config_root_path and config_root_path == root_path :,0.8820016898747209,79.38047857077986
"def rewire_init(expr): new_args = [] if expr[0] == HySymbol(""setv""): pairs = expr[1:] while len(pairs) > 0: k, v = (pairs.pop(0), pairs.pop(0)) <mask>: v.append(HySymbol(""None"")) new_args.append(k) new_args.append(v) expr = HyExpression([HySymbol(""setv"")] + new_args).replace(expr) return expr",False,"if k == HySymbol ( ""__init__"" ) :","if k == ""None"" :",0.8820016898747209,16.5759081092516
"def doDir(elem): for child in elem.childNodes: if not isinstance(child, minidom.Element): continue <mask>: doDir(child) elif child.tagName == ""Component"": for grandchild in child.childNodes: if not isinstance(grandchild, minidom.Element): continue if grandchild.tagName != ""File"": continue files.add(grandchild.getAttribute(""Source"").replace(os.sep, ""/""))",True,"if child . tagName == ""Directory"" :","if child . tagName == ""Directory"" :",0.8820016898747209,100.00000000000004
"def _v2_common(self, cfg): LOG.debug(""v2_common: handling config:\n%s"", cfg) if ""nameservers"" in cfg: search = cfg.get(""nameservers"").get(""search"", []) dns = cfg.get(""nameservers"").get(""addresses"", []) name_cmd = {""type"": ""nameserver""} <mask>: name_cmd.update({""search"": search}) if len(dns) > 0: name_cmd.update({""addresses"": dns}) LOG.debug(""v2(nameserver) -> v1(nameserver):\n%s"", name_cmd) self.handle_nameserver(name_cmd)",True,if len ( search ) > 0 :,if len ( search ) > 0 :,0.8820016898747209,100.00000000000004
"def __start_element_handler(self, name, attrs): if name == ""mime-type"": if self.type: for extension in self.extensions: self[extension] = self.type self.type = attrs[""type""].lower() self.extensions = [] elif name == ""glob"": pattern = attrs[""pattern""] <mask>: self.extensions.append(pattern[1:].lower())",False,"if pattern . startswith ( ""*."" ) :",if pattern :,0.8820016898747209,0.0
"def get_attr_by_data_model(self, dmodel, exclude_record=False): if exclude_record: return list( filter( lambda x: x.data_model == dmodel and x.value == """" <mask>: else False, self._inferred_intent, ) ) else: return list( filter( lambda x: x.data_model == dmodel and x.value == """" if hasattr(x, ""data_model"") else False, self._inferred_intent, ) )",False,"if x . attribute != ""Record"" and hasattr ( x , ""data_model"" )","if hasattr ( x , ""data_model"" )",0.8820016898747209,40.35921041825187
"def general(metadata, value): if metadata.get(""commands"") and value: if not metadata.get(""nargs""): v = quote(value) else: v = value return u""{0} {1}"".format(metadata[""commands""][0], v) else: if not value: return None <mask>: return quote(value) else: return value",False,"elif not metadata . get ( ""nargs"" ) :","if metadata . get ( ""nargs"" ) :",0.8820016898747209,79.6358031503278
"def get_images(self): images = [] try: tag = MP4(self[""~filename""]) except Exception: return [] for cover in tag.get(""covr"", []): <mask>: mime = ""image/jpeg"" elif cover.imageformat == MP4Cover.FORMAT_PNG: mime = ""image/png"" else: mime = ""image/"" f = get_temp_cover_file(cover) images.append(EmbeddedImage(f, mime)) return images",True,if cover . imageformat == MP4Cover . FORMAT_JPEG :,if cover . imageformat == MP4Cover . FORMAT_JPEG :,0.8820016898747209,100.00000000000004
"def run_cmd(self, util, value): state = util.state if not state.argument_supplied: state.argument_supplied = True if value == ""by_four"": state.argument_value = 4 <mask>: state.argument_negative = True else: state.argument_value = value elif value == ""by_four"": state.argument_value *= 4 elif isinstance(value, int): state.argument_value *= 10 state.argument_value += value elif value == ""negative"": state.argument_value = -state.argument_value",True,"elif value == ""negative"" :","elif value == ""negative"" :",0.8820016898747209,100.00000000000004
"def finish_character_data(self): if self.character_data: <mask>: line, column = self.character_pos token = XmlToken( XML_CHARACTER_DATA, self.character_data, None, line, column ) self.tokens.append(token) self.character_data = """"",False,if not self . skip_ws or not self . character_data . isspace ( ) :,if self . character_pos :,0.8820016898747209,8.194094675927117
"def check_syntax(filename, raise_error=False): """"""Return True if syntax is okay."""""" with autopep8.open_with_encoding(filename) as input_file: try: compile(input_file.read(), ""<string>"", ""exec"", dont_inherit=True) return True except (SyntaxError, TypeError, UnicodeDecodeError): <mask>: raise else: return False",True,if raise_error :,if raise_error :,0.8820016898747209,100.00000000000004
"def write(self, file): if not self._been_written: self._been_written = True for attribute, value in self.__dict__.items(): <mask>: self.write_recursive(value, file) w = file.write w(""\t%s = {\n"" % self._id) w(""\t\tisa = %s;\n"" % self.__class__.__name__) for attribute, value in self.__dict__.items(): if attribute[0] != ""_"": w(""\t\t%s = %s;\n"" % (attribute, self.tostring(value))) w(""\t};\n\n"")",False,"if attribute [ 0 ] != ""_"" :","if attribute [ 0 ] == ""_recursive"" :",0.8820016898747209,46.59538415189962
"def update_service_key(kid, name=None, metadata=None): try: with db_transaction(): key = db_for_update(ServiceKey.select().where(ServiceKey.kid == kid)).get() if name is not None: key.name = name <mask>: key.metadata.update(metadata) key.save() except ServiceKey.DoesNotExist: raise ServiceKeyDoesNotExist",True,if metadata is not None :,if metadata is not None :,0.8820016898747209,100.00000000000004
"def fill_buf(self, db, len_=None): with open(""/dev/urandom"", ""rb"") as rfh: first = True for (id_,) in db.query(""SELECT id FROM test""): if len_ is None and first: val = b"""" # We always want to check this case first = False <mask>: val = rfh.read(random.randint(0, 140)) else: val = rfh.read(len_) db.execute(""UPDATE test SET buf=? WHERE id=?"", (val, id_))",True,elif len_ is None :,elif len_ is None :,0.8820016898747209,100.00000000000004
"def load_category_from_parser(self, parser): for cate in parser.keys(): id = parser.get_id(cate) <mask>: self._data[""cates""][id] = 0 else: self._data[""cates""][id] = self.count_unread(id) self._is_init = False self.save()",False,if self . _is_init :,"if id == ""cates"" :",0.8820016898747209,6.567274736060395
"def after_insert(self): if self.prescription: frappe.db.set_value( ""Lab Prescription"", self.prescription, ""lab_test_created"", 1 ) <mask>: self.invoiced = True if not self.lab_test_name and self.template: self.load_test_from_template() self.reload()",False,"if frappe . db . get_value ( ""Lab Prescription"" , self . prescription , ""invoiced"" ) :",if not self . invoiced :,0.8820016898747209,1.2005861999569136
"def sync_terminology(self): if self.is_source: return store = self.store missing = [] for source in self.component.get_all_sources(): <mask>: continue try: _unit, add = store.find_unit(source.context, source.source) except UnitNotFound: add = True # Unit is already present if not add: continue missing.append((source.context, source.source, """")) if missing: self.add_units(None, missing)",False,"if ""terminology"" not in source . all_flags :",if not source . context :,0.8820016898747209,7.509307647752128
def refresh(self): if self._obj: base = self._db.get_media_from_handle(self._obj.get_reference_handle()) <mask>: self._title = base.get_description() self._value = base.get_path(),True,if base :,if base :,0.8820016898747209,0.0
"def _set_parse_context(self, tag, tag_attrs): # special case: script or style parse context if not self._wb_parse_context: if tag == ""style"": self._wb_parse_context = ""style"" <mask>: if self._allow_js_type(tag_attrs): self._wb_parse_context = ""script""",True,"elif tag == ""script"" :","elif tag == ""script"" :",0.8820016898747209,100.00000000000004
"def can_read(self): if hasattr(self.file, ""__iter__""): iterator = iter(self.file) head = next(iterator, None) if head is None: self.repaired = [] return True <mask>: self.repaired = itertools.chain([head], iterator) return True else: # We may have mangled a generator at this point, so just abort raise IOSourceError( ""Could not open source: %r (mode: %r)"" % (self.file, self.options[""mode""]) ) return False",False,"if isinstance ( head , str ) :","if isinstance ( head , list ) :",0.8820016898747209,59.4603557501361
"def wrapped_request_method(*args, **kwargs): """"""Modifies HTTP headers to include a specified user-agent."""""" if kwargs.get(""headers"") is not None: if kwargs[""headers""].get(""user-agent""): <mask>: # Save the existing user-agent header and tack on our own. kwargs[""headers""][""user-agent""] = ( f""{user_agent} "" f'{kwargs[""headers""][""user-agent""]}' ) else: kwargs[""headers""][""user-agent""] = user_agent else: kwargs[""headers""] = {""user-agent"": user_agent} return request_method(*args, **kwargs)",False,"if user_agent not in kwargs [ ""headers"" ] [ ""user-agent"" ] :","if user_agent in kwargs [ ""headers"" ] :",0.8820016898747209,48.35695637717113
"def execute(self): if self._dirty or not self._qr: model_class = self.model_class query_meta = self.get_query_meta() if self._tuples: ResultWrapper = TuplesQueryResultWrapper elif self._dicts: ResultWrapper = DictQueryResultWrapper <mask>: ResultWrapper = NaiveQueryResultWrapper elif self._aggregate_rows: ResultWrapper = AggregateQueryResultWrapper else: ResultWrapper = ModelQueryResultWrapper self._qr = ResultWrapper(model_class, self._execute(), query_meta) self._dirty = False return self._qr else: return self._qr",False,elif self . _naive or not self . _joins or self . verify_naive ( ) :,elif self . _naive :,0.8820016898747209,7.711896755350043
"def populate_data(apps, schema_editor): Menu = apps.get_model(""menu"", ""Menu"") for menu in Menu.objects.all(): <mask>: json_str = menu.json_content while isinstance(json_str, str): json_str = json.loads(json_str) menu.json_content_new = json_str menu.save()",False,"if isinstance ( menu . json_content , str ) :","if isinstance ( menu , str ) :",0.8820016898747209,41.938051117049184
"def virtualenv_exists(self): if os.path.exists(self.virtualenv_location): <mask>: extra = [""Scripts"", ""activate.bat""] else: extra = [""bin"", ""activate""] return os.path.isfile(os.sep.join([self.virtualenv_location] + extra)) return False",False,"if os . name == ""nt"" :",if os . path . isdir ( self . virtualenv_location ) :,0.8820016898747209,11.359354890271161
"def get_minkowski_function(name, variable): fn_name = name + get_postfix(variable) if hasattr(MEB, fn_name): return getattr(MEB, fn_name) else: <mask>: raise ValueError( f""Function {fn_name} not available. Please compile MinkowskiEngine with `torch.cuda.is_available()` is `True`."" ) else: raise ValueError(f""Function {fn_name} not available."")",False,if variable . is_cuda :,if torch . cuda . is_available ( ) :,0.8820016898747209,16.59038701421971
"def build_temp_workspace(files): tempdir = tempfile.mkdtemp(prefix=""yamllint-tests-"") for path, content in files.items(): path = os.path.join(tempdir, path).encode(""utf-8"") <mask>: os.makedirs(os.path.dirname(path)) if type(content) is list: os.mkdir(path) else: mode = ""wb"" if isinstance(content, bytes) else ""w"" with open(path, mode) as f: f.write(content) return tempdir",True,if not os . path . exists ( os . path . dirname ( path ) ) :,if not os . path . exists ( os . path . dirname ( path ) ) :,0.8820016898747209,100.00000000000004
"def clean_form(self, request, user, form, cleaned_data): for field in self.get_fields(): <mask>: continue try: cleaned_data[field.fieldname] = field.clean( request, user, cleaned_data[field.fieldname] ) except ValidationError as e: form.add_error(field.fieldname, e) return cleaned_data",False,if field . fieldname not in cleaned_data :,if field . is_required ( ) :,0.8820016898747209,18.04438612975343
"def setUp(self): self.realm = service.InMemoryWordsRealm(""realmname"") self.checker = checkers.InMemoryUsernamePasswordDatabaseDontUse() self.portal = portal.Portal(self.realm, [self.checker]) self.factory = service.IRCFactory(self.realm, self.portal) c = [] for nick in self.STATIC_USERS: <mask>: nick = nick.decode(""utf-8"") c.append(self.realm.createUser(nick)) self.checker.addUser(nick, nick + ""_password"") return DeferredList(c)",True,"if isinstance ( nick , bytes ) :","if isinstance ( nick , bytes ) :",0.8820016898747209,100.00000000000004
"def __call__(self, message): with self._lock: self._pending_ack += 1 self.max_pending_ack = max(self.max_pending_ack, self._pending_ack) self.seen_message_ids.append(int(message.attributes[""seq_num""])) time.sleep(self._processing_time) with self._lock: self._pending_ack -= 1 message.ack() self.completed_calls += 1 <mask>: if not self.done_future.done(): self.done_future.set_result(None)",False,if self . completed_calls >= self . _resolve_at_msg_count :,if self . done_calls == self . max_pending_ack :,0.8820016898747209,15.616719133682372
"def fill_in_standard_formats(book): for x in std_format_code_types.keys(): <mask>: ty = std_format_code_types[x] # Note: many standard format codes (mostly CJK date formats) have # format strings that vary by locale; xlrd does not (yet) # handle those; the type (date or numeric) is recorded but the fmt_str will be None. fmt_str = std_format_strings.get(x) fmtobj = Format(x, ty, fmt_str) book.format_map[x] = fmtobj",False,if x not in book . format_map :,if x in book . format_map :,0.8820016898747209,71.89393375176813
"def FetchFn(bigger_than_3_only=None, less_than_7_only=None, even_only=None): result = [] for i in range(10): # This line introduces a bug. if bigger_than_3_only and less_than_7_only and i == 4: continue if bigger_than_3_only and i <= 3: continue <mask>: continue if even_only and i % 2 != 0: continue result.append(i) return result",True,if less_than_7_only and i >= 7 :,if less_than_7_only and i >= 7 :,0.8820016898747209,100.00000000000004
"def next_instruction_is_function_or_class(lines): """"""Is the first non-empty, non-commented line of the cell either a function or a class?"""""" parser = StringParser(""python"") for i, line in enumerate(lines): <mask>: parser.read_line(line) continue parser.read_line(line) if not line.strip(): # empty line if i > 0 and not lines[i - 1].strip(): return False continue if line.startswith(""def "") or line.startswith(""class ""): return True if line.startswith((""#"", ""@"", "" "", "")"")): continue return False return False",False,if parser . is_quoted ( ) :,"if line . startswith ( ""#"" ) :",0.8820016898747209,10.552670315936318
"def __getattr__(self, key): for tag in self.tag.children: if tag.name not in (""input"",): continue <mask>: from thug.DOM.W3C.Core.DOMImplementation import DOMImplementation return DOMImplementation.createHTMLElement(self.doc, tag) raise AttributeError",False,"if ""name"" in tag . attrs and tag . attrs [ ""name"" ] in ( key , ) :",if key in tag . name :,0.8820016898747209,3.072780845849199
"def process_signature(app, what, name, obj, options, signature, return_annotation): if signature: # replace Mock function names signature = re.sub(""<Mock name='([^']+)'.*>"", ""\g<1>"", signature) signature = re.sub(""tensorflow"", ""tf"", signature) # add scope name to layer signatures: <mask>: if obj.use_scope: signature = signature[0] + ""variable_scope_name, "" + signature[1:] elif obj.use_scope is None: signature = signature[0] + ""[variable_scope_name,] "" + signature[1:] # signature: arg list return signature, return_annotation",False,"if hasattr ( obj , ""use_scope"" ) :",if obj . scope_name :,0.8820016898747209,4.807478402503058
"def countbox(self): self.box = [1000, 1000, -1000, -1000] for x, y in self.body: if x < self.box[0]: self.box[0] = x if x > self.box[2]: self.box[2] = x if y < self.box[1]: self.box[1] = y <mask>: self.box[3] = y",True,if y > self . box [ 3 ] :,if y > self . box [ 3 ] :,0.8820016898747209,100.00000000000004
"def find_shell(): global DEFAULT_SHELL if not DEFAULT_SHELL: for shell in propose_shell(): <mask>: DEFAULT_SHELL = shell break if not DEFAULT_SHELL: DEFAULT_SHELL = ""/bin/sh"" return DEFAULT_SHELL",False,"if os . path . isfile ( shell ) and os . access ( shell , os . X_OK ) :",if shell . is_file ( ) :,0.8820016898747209,2.7528805823809215
"def addAggregators(sheet, cols, aggrnames): ""Add each aggregator in list of *aggrnames* to each of *cols*."" for aggrname in aggrnames: aggrs = vd.aggregators.get(aggrname) aggrs = aggrs if isinstance(aggrs, list) else [aggrs] for aggr in aggrs: for c in cols: if not hasattr(c, ""aggregators""): c.aggregators = [] <mask>: c.aggregators += [aggr]",False,if aggr and aggr not in c . aggregators :,"if not hasattr ( c , ""aggregators"" ) :",0.8820016898747209,5.604233375480572
"def run(self, paths=[]): items = [] for item in SideBarSelection(paths).getSelectedItems(): items.append(item.pathAbsoluteFromProjectEncoded()) if len(items) > 0: sublime.set_clipboard(""\n"".join(items)) <mask>: sublime.status_message(""Items copied"") else: sublime.status_message(""Item copied"")",False,if len ( items ) > 1 :,if len ( items ) == 1 :,0.8820016898747209,51.33450480401705
"def social_user(backend, uid, user=None, *args, **kwargs): provider = backend.name social = backend.strategy.storage.user.get_social_auth(provider, uid) if social: <mask>: msg = ""This account is already in use."" raise AuthAlreadyAssociated(backend, msg) elif not user: user = social.user return { ""social"": social, ""user"": user, ""is_new"": user is None, ""new_association"": social is None, }",False,if user and social . user != user :,if user is not None :,0.8820016898747209,9.22364410103253
"def _text(bitlist): out = """" for typ, text in bitlist: if not typ: out += text elif typ == ""em"": out += ""\\fI%s\\fR"" % text <mask>: out += ""\\fB%s\\fR"" % text else: raise ValueError(""unexpected tag %r inside text"" % (typ,)) out = out.strip() out = re.sub(re.compile(r""^\s+"", re.M), """", out) return out",False,"elif typ in [ ""strong"" , ""code"" ] :","elif typ == ""b"" :",0.8820016898747209,7.433761660133445
"def OnRadioSelect(self, event): fitID = self.mainFrame.getActiveFit() if fitID is not None: self.mainFrame.command.Submit( cmd.GuiChangeImplantLocationCommand( fitID=fitID, source=ImplantLocation.FIT <mask>: else ImplantLocation.CHARACTER, ) )",False,if self . rbFit . GetValue ( ),if fitID == ImplantLocation . FIT,0.8820016898747209,6.770186228657864
"def hexdump(data): """"""yield lines with hexdump of data"""""" values = [] ascii = [] offset = 0 for h, a in sixteen(data): <mask>: yield (offset, "" "".join(["""".join(values), """".join(ascii)])) del values[:] del ascii[:] offset += 0x10 else: values.append(h) ascii.append(a)",False,if h is None :,if h == 0x20 :,0.8820016898747209,17.965205598154213
"def submit(self): bot_token = self.config[""bot_token""] chat_ids = self.config[""chat_id""] chat_ids = [chat_ids] if isinstance(chat_ids, str) else chat_ids text = ""\n"".join(super().submit()) if not text: logger.debug(""Not calling telegram API (no changes)"") return result = None for chunk in chunkstring(text, self.MAX_LENGTH, numbering=True): for chat_id in chat_ids: res = self.submitToTelegram(bot_token, chat_id, chunk) <mask>: result = res return result",False,if res . status_code != requests . codes . ok or res is None :,if res :,0.8820016898747209,0.0
"def onMessage(self, payload, isBinary): if not isBinary: self.result = ""Expected binary message with payload, but got binary."" else: <mask>: self.result = ( ""Expected binary message with payload of length %d, but got %d."" % (self.DATALEN, len(payload)) ) else: ## FIXME : check actual content ## self.behavior = Case.OK self.result = ""Received binary message of length %d."" % len(payload) self.p.createWirelog = True self.p.sendClose(self.p.CLOSE_STATUS_CODE_NORMAL)",False,if len ( payload ) != self . DATALEN :,if self . DATALEN != len ( payload ) :,0.8820016898747209,39.76353643835254
"def verify_output(actual, expected): actual = _read_file(actual, ""Actual"") expected = _read_file(join(CURDIR, expected), ""Expected"") if len(expected) != len(actual): raise AssertionError( ""Lengths differ. Expected %d lines but got %d"" % (len(expected), len(actual)) ) for exp, act in zip(expected, actual): tester = fnmatchcase if ""*"" in exp else eq <mask>: raise AssertionError( ""Lines differ.\nExpected: %s\nActual: %s"" % (exp, act) )",False,"if not tester ( act . rstrip ( ) , exp . rstrip ( ) ) :",if not tester ( act ) :,0.8820016898747209,16.94588729863642
"def _in_out_vector_helper(self, name1, name2, ceil): vector = [] stats = self.record if ceil is None: ceil = self._get_max_rate(name1, name2) maxlen = self.config.get_stats_history_length() for n in [name1, name2]: for i in range(maxlen + 1): <mask>: vector.append(float(stats[i][n]) / ceil) else: vector.append(0.0) return vector",False,if i < len ( stats ) :,if stats [ i ] [ n ] > ceil :,0.8820016898747209,5.300156689756295
"def _init_param(param, mode): if isinstance(param, str): param = _resolve(param) elif isinstance(param, (list, tuple)): param = [_init_param(p, mode) for p in param] elif isinstance(param, dict): <mask>: param = from_params(param, mode=mode) else: param = {k: _init_param(v, mode) for k, v in param.items()} return param",False,"if { ""ref"" , ""class_name"" , ""config_path"" } . intersection ( param . keys ( ) ) :","if isinstance ( param , dict ) :",0.8820016898747209,1.4189245065793463
"def link_pantsrefs(soups, precomputed): """"""Transorm soups: <a pantsref=""foo""> becomes <a href=""../foo_page.html#foo"">"""""" for (page, soup) in soups.items(): for a in soup.find_all(""a""): if not a.has_attr(""pantsref""): continue pantsref = a[""pantsref""] <mask>: raise TaskError( f'Page {page} has pantsref ""{pantsref}"" and I cannot find pantsmark for it' ) a[""href""] = rel_href(page, precomputed.pantsref[pantsref])",True,if pantsref not in precomputed . pantsref :,if pantsref not in precomputed . pantsref :,0.8820016898747209,100.00000000000004
"def _gridconvvalue(self, value): if isinstance(value, (str, _tkinter.Tcl_Obj)): try: svalue = str(value) if not svalue: return None <mask>: return getdouble(svalue) else: return getint(svalue) except ValueError: pass return value",False,"elif ""."" in svalue :","if isinstance ( svalue , float ) :",0.8820016898747209,6.567274736060395
"def default(self, o): try: <mask>: return str(o) else: # remove unwanted attributes from the provider object during conversion to json if hasattr(o, ""profile""): del o.profile if hasattr(o, ""credentials""): del o.credentials if hasattr(o, ""metadata_path""): del o.metadata_path if hasattr(o, ""services_config""): del o.services_config return vars(o) except Exception as e: return str(o)",False,if type ( o ) == datetime . datetime :,"if isinstance ( o , str ) :",0.8820016898747209,9.545138913210204
"def transform_kwarg(self, name, value, split_single_char_options): if len(name) == 1: <mask>: return [""-%s"" % name] elif value not in (False, None): if split_single_char_options: return [""-%s"" % name, ""%s"" % value] else: return [""-%s%s"" % (name, value)] else: if value is True: return [""--%s"" % dashify(name)] elif value is not False and value is not None: return [""--%s=%s"" % (dashify(name), value)] return []",False,if value is True :,"if value in ( True , None ) :",0.8820016898747209,11.339582221952005
"def handle(self, context, sign, *args): if context.rounding in (ROUND_HALF_UP, ROUND_HALF_EVEN, ROUND_HALF_DOWN, ROUND_UP): return Infsign[sign] if sign == 0: <mask>: return Infsign[sign] return Decimal((sign, (9,) * context.prec, context.Emax - context.prec + 1)) if sign == 1: if context.rounding == ROUND_FLOOR: return Infsign[sign] return Decimal((sign, (9,) * context.prec, context.Emax - context.prec + 1))",False,if context . rounding == ROUND_CEILING :,if context . rounding == ROUND_FLOOR :,0.8820016898747209,78.25422900366438
"def OnLeftUp(self, event): # Stop Drawing if self.Drawing: self.Drawing = False <mask>: world_rect = ( self.Canvas.PixelToWorld(self.RBRect[0]), self.Canvas.ScalePixelToWorld(self.RBRect[1]), ) wx.CallAfter(self.CallBack, world_rect) self.RBRect = None",True,if self . RBRect :,if self . RBRect :,0.8820016898747209,100.00000000000004
"def _map_answers(answers): result = [] for a in answers.split(""|""): user_answers = [] result.append(dict(sourcerAnswers=user_answers)) for r in a.split("",""): <mask>: user_answers.append(dict(noAnswer=True)) else: start_, end_ = map(int, r.split("":"")) user_answers.append(dict(s=start_, e=end_)) return result",False,"if r == ""None"" :","if r == """" :",0.8820016898747209,61.29752413741059
"def parse_edges(self, pcb): edges = [] drawings = list(pcb.GetDrawings()) bbox = None for m in pcb.GetModules(): for g in m.GraphicalItems(): drawings.append(g) for d in drawings: <mask>: parsed_drawing = self.parse_drawing(d) if parsed_drawing: edges.append(parsed_drawing) if bbox is None: bbox = d.GetBoundingBox() else: bbox.Merge(d.GetBoundingBox()) if bbox: bbox.Normalize() return edges, bbox",False,if d . GetLayer ( ) == pcbnew . Edge_Cuts :,if d . GetDrawing ( ) :,0.8820016898747209,11.835764736093042
"def get_size(self): size = self.start_size for operation in self.ran_operations: <mask>: size = operation[1][0] elif operation[0] == ""crop"": crop = operation[1][0] size = crop[2] - crop[0], crop[3] - crop[1] return size",False,"if operation [ 0 ] == ""resize"" :","if operation [ 0 ] == ""size"" :",0.8820016898747209,74.19446627365011
"def migrate_account_metadata(account_id): from inbox.models.session import session_scope from inbox.models import Account with session_scope(versioned=False) as db_session: account = db_session.query(Account).get(account_id) <mask>: create_categories_for_easfoldersyncstatuses(account, db_session) else: create_categories_for_folders(account, db_session) if account.discriminator == ""gmailaccount"": set_labels_for_imapuids(account, db_session) db_session.commit()",False,"if account . discriminator == ""easaccount"" :","if account . discriminator == ""easfoldersyncstatuses"" :",0.8820016898747209,70.71067811865478
"def OnEndDrag(self, event): self.StopDragging() dropTarget = event.GetItem() if not dropTarget: dropTarget = self.GetRootItem() if self.IsValidDropTarget(dropTarget): self.UnselectAll() <mask>: self.SelectItem(dropTarget) self.OnDrop(dropTarget, self._dragItem)",False,if dropTarget != self . GetRootItem ( ) :,if self . IsValidDropTarget ( dropTarget ) :,0.8820016898747209,13.991316187881289
"def validate(self, frame, value): if self.sep and isinstance(value, string_types): value = value.split(self.sep) if isinstance(value, list): <mask>: return [self.specs[0].validate(frame, v) for v in value] else: return [ [s.validate(frame, v) for (v, s) in izip(val, self.specs)] for val in value ] raise ValueError(""Invalid MultiSpec data: %r"" % value)",True,if len ( self . specs ) == 1 :,if len ( self . specs ) == 1 :,0.8820016898747209,100.00000000000004
"def __init__(self, action_space=None, network=None, network_kwargs=None, hparams=None): QNetBase.__init__(self, hparams=hparams) with tf.variable_scope(self.variable_scope): <mask>: action_space = Space(low=0, high=self._hparams.action_space, dtype=np.int32) self._action_space = action_space self._append_output_layer()",True,if action_space is None :,if action_space is None :,0.8820016898747209,100.00000000000004
"def n_weights(self): """"""Return the number of weights (parameters) in this network."""""" n_weights = 0 for i, w in enumerate(self.all_weights): n = 1 # for s in p.eval().shape: for s in w.get_shape(): try: s = int(s) except: s = 1 <mask>: n = n * s n_weights = n_weights + n # print(""num of weights (parameters) %d"" % n_weights) return n_weights",False,if s :,if s > 0 :,0.8820016898747209,23.643540225079384
"def _arg_desc(name, ctx): for param in ctx.command.params: if param.name == name: desc = param.opts[-1] <mask>: desc = param.human_readable_name return desc raise AssertionError(name)",False,"if desc [ 0 ] != ""-"" :",if param . human_readable_name :,0.8820016898747209,4.540013809283726
"def walk(directory, path_so_far): for name in sorted(os.listdir(directory)): if any(fnmatch(name, pattern) for pattern in basename_ignore): continue path = path_so_far + ""/"" + name if path_so_far else name if any(fnmatch(path, pattern) for pattern in path_ignore): continue full_name = os.path.join(directory, name) if os.path.isdir(full_name): for file_path in walk(full_name, path): yield file_path <mask>: yield path",False,elif os . path . isfile ( full_name ) :,elif os . path . isfile ( path ) :,0.8820016898747209,57.89300674674101
"def cache_dst(self): final_dst = None final_linenb = None for linenb, assignblk in enumerate(self): for dst, src in viewitems(assignblk): if dst.is_id(""IRDst""): <mask>: raise ValueError(""Multiple destinations!"") final_dst = src final_linenb = linenb self._dst = final_dst self._dst_linenb = final_linenb return final_dst",False,if final_dst is not None :,if len ( src ) > 1 :,0.8820016898747209,6.567274736060395
"def run(self, args, **kwargs): if args.resource_ref or args.policy_type: filters = {} <mask>: filters[""resource_ref""] = args.resource_ref if args.policy_type: filters[""policy_type""] = args.policy_type filters.update(**kwargs) return self.manager.query(**filters) else: return self.manager.get_all(**kwargs)",True,if args . resource_ref :,if args . resource_ref :,0.8820016898747209,100.00000000000004
"def __init__(self, folders): self.folders = folders self.duplicates = {} for folder, path in folders.items(): duplicates = [] for other_folder, other_path in folders.items(): if other_folder == folder: continue if other_path == path: duplicates.append(other_folder) <mask>: self.duplicates[folder] = duplicates",False,if len ( duplicates ) :,if duplicates :,0.8820016898747209,0.0
"def limit_clause(self, select, **kw): text = """" if select._limit_clause is not None: text += ""\n LIMIT "" + self.process(select._limit_clause, **kw) if select._offset_clause is not None: <mask>: text += ""\n LIMIT "" + self.process(sql.literal(-1)) text += "" OFFSET "" + self.process(select._offset_clause, **kw) else: text += "" OFFSET "" + self.process(sql.literal(0), **kw) return text",False,if select . _limit_clause is None :,if select . _offset_clause is not None :,0.8820016898747209,37.81790427652475
"def _get_activation(self, act): """"""Get activation block based on the name."""""" if isinstance(act, str): if act.lower() == ""gelu"": return GELU() <mask>: return GELU(approximate=True) else: return gluon.nn.Activation(act) assert isinstance(act, gluon.Block) return act",False,"elif act . lower ( ) == ""approx_gelu"" :","elif act . lower ( ) == ""gelu_approximate"" :",0.8820016898747209,67.25080050576685
"def __eq__(self, other): try: if self.type != other.type: return False if self.type == ""ASK"": return self.askAnswer == other.askAnswer <mask>: return self.vars == other.vars and self.bindings == other.bindings else: return self.graph == other.graph except: return False",False,"elif self . type == ""SELECT"" :","elif self . type == ""VARIABLE"" :",0.8820016898747209,70.71067811865478
"def _get_text_nodes(nodes, html_body): text = [] open_tags = 0 for node in nodes: if isinstance(node, HtmlTag): <mask>: open_tags += 1 elif node.tag_type == CLOSE_TAG: open_tags -= 1 elif ( isinstance(node, HtmlDataFragment) and node.is_text_content and open_tags == 0 ): text.append(html_body[node.start : node.end]) return text",True,if node . tag_type == OPEN_TAG :,if node . tag_type == OPEN_TAG :,0.8820016898747209,100.00000000000004
"def test_do_change(self): """"""Test if VTK object changes when trait is changed."""""" p = Prop() p.edge_visibility = not p.edge_visibility p.representation = ""p"" p.opacity = 0.5 p.color = (0, 1, 0) p.diffuse_color = (1, 1, 1) p.specular_color = (1, 1, 0) for t, g in p._updateable_traits_: val = getattr(p._vtk_obj, g)() <mask>: self.assertEqual(val, getattr(p, t + ""_"")) else: self.assertEqual(val, getattr(p, t))",False,"if t == ""representation"" :","if t in ( ""p"" , ""p"" ) :",0.8820016898747209,7.768562846380176
"def update_item(source_doc, target_doc, source_parent): target_doc.t_warehouse = """" if source_doc.material_request_item and source_doc.material_request: add_to_transit = frappe.db.get_value( ""Stock Entry"", source_name, ""add_to_transit"" ) <mask>: warehouse = frappe.get_value( ""Material Request Item"", source_doc.material_request_item, ""warehouse"" ) target_doc.t_warehouse = warehouse target_doc.s_warehouse = source_doc.t_warehouse target_doc.qty = source_doc.qty - source_doc.transferred_qty",True,if add_to_transit :,if add_to_transit :,0.8820016898747209,100.00000000000004
"def get_drive(self, root_path="""", volume_guid_path=""""): for drive in self.drives: <mask>: config_root_path = drive.get(""root_path"") if config_root_path and root_path == config_root_path: return drive elif volume_guid_path: config_volume_guid_path = drive.get(""volume_guid_path"") if config_volume_guid_path and config_volume_guid_path == volume_guid_path: return drive",True,if root_path :,if root_path :,0.8820016898747209,100.00000000000004
"def f_freeze(_): repos = utils.get_repos() for name, path in repos.items(): url = """" cp = subprocess.run([""git"", ""remote"", ""-v""], cwd=path, capture_output=True) <mask>: url = cp.stdout.decode(""utf-8"").split(""\n"")[0].split()[1] print(f""{url},{name},{path}"")",True,if cp . returncode == 0 :,if cp . returncode == 0 :,0.8820016898747209,100.00000000000004
"def conj(self): dtype = self.dtype if issubclass(self.dtype.type, np.complexfloating): <mask>: raise RuntimeError( ""only contiguous arrays may "" ""be used as arguments to this operation"" ) if self.flags.f_contiguous: order = ""F"" else: order = ""C"" result = self._new_like_me(order=order) func = elementwise.get_conj_kernel(dtype) func.prepared_async_call( self._grid, self._block, None, self.gpudata, result.gpudata, self.mem_size ) return result else: return self",False,if not self . flags . forc :,if self . flags . contiguous :,0.8820016898747209,39.44243648327556
"def detect_reentrancy(self, contract): for function in contract.functions_and_modifiers_declared: if function.is_implemented: <mask>: continue self._explore(function.entry_point, []) function.context[self.KEY] = True",True,if self . KEY in function . context :,if self . KEY in function . context :,0.8820016898747209,100.00000000000004
"def test_default_configuration_no_encoding(self): transformations = [] for i in range(2): transformation, original = _test_preprocessing(NoEncoding) self.assertEqual(transformation.shape, original.shape) self.assertTrue((transformation == original).all()) transformations.append(transformation) <mask>: self.assertTrue((transformations[-1] == transformations[-2]).all())",True,if len ( transformations ) > 1 :,if len ( transformations ) > 1 :,0.8820016898747209,100.00000000000004
"def main(): """"""main function"""""" # todo: lookuo real description parser = argparse.ArgumentParser(description=""Let a cow speak for you"") parser.add_argument(""text"", nargs=""*"", default=None, help=""text to say"") ns = parser.parse_args() if (ns.text is None) or (len(ns.text) == 0): text = """" while True: inp = sys.stdin.read(4096) if inp.endswith(""\n""): inp = inp[:-1] <mask>: break text += inp else: text = "" "".join(ns.text) cow = get_cow(text) print(cow)",True,if not inp :,if not inp :,0.8820016898747209,100.00000000000004
"def prehook(self, emu, op, eip): if op in self.badops: emu.stopEmu() raise v_exc.BadOpBytes(op.va) if op.mnem in STOS: if self.arch == ""i386"": reg = emu.getRegister(envi.archs.i386.REG_EDI) elif self.arch == ""amd64"": reg = emu.getRegister(envi.archs.amd64.REG_RDI) <mask>: self.vw.makePointer(reg, follow=True)",False,if self . vw . isValidPointer ( reg ) and self . vw . getLocation ( reg ) is None :,"elif self . arch == ""vw"" :",0.8820016898747209,3.322086503981984
"def get_boarding_status(project): status = ""Pending"" if project: doc = frappe.get_doc(""Project"", project) if flt(doc.percent_complete) > 0.0 and flt(doc.percent_complete) < 100.0: status = ""In Process"" <mask>: status = ""Completed"" return status",False,elif flt ( doc . percent_complete ) == 100.0 :,elif doc . percent_complete > 0.0 and doc . percent_complete < 100.0 :,0.8820016898747209,25.459845316736796
"def set_weights(self, new_weights): weights = self.get_weights() if len(weights) != len(new_weights): raise ValueError(""len of lists mismatch"") tuples = [] for w, new_w in zip(weights, new_weights): <mask>: new_w = new_w.reshape(w.shape) tuples.append((w, new_w)) nn.batch_set_value(tuples)",False,if len ( w . shape ) != new_w . shape :,"if isinstance ( new_w , nn . Tensor ) :",0.8820016898747209,12.545696183524145
"def reload_json_api_settings(*args, **kwargs): django_setting = kwargs[""setting""] setting = django_setting.replace(JSON_API_SETTINGS_PREFIX, """") value = kwargs[""value""] if setting in DEFAULTS.keys(): <mask>: setattr(json_api_settings, setting, value) elif hasattr(json_api_settings, setting): delattr(json_api_settings, setting)",False,if value is not None :,if value :,0.8820016898747209,0.0
"def knamn(self, sup, cdict): cname = cdict[sup].class_name if not cname: (namesp, tag) = cdict[sup].name.split(""."") <mask>: ctag = self.root.modul[namesp].factory(tag).__class__.__name__ cname = ""%s.%s"" % (namesp, ctag) else: cname = tag + ""_"" return cname",False,if namesp :,if namesp in self . root . modul :,0.8820016898747209,10.552670315936318
"def setdefault(self, key, default=None): try: o = self.data[key]() except KeyError: o = None if o is None: <mask>: self._commit_removals() self.data[key] = KeyedRef(default, self._remove, key) return default else: return o",False,if self . _pending_removals :,if self . _remove :,0.8820016898747209,38.49815007763549
"def __on_item_activated(self, event): if self.__module_view: module = self.get_event_module(event) self.__module_view.set_selection(module.module_num) <mask>: self.input_list_ctrl.deactivate_active_item() else: self.list_ctrl.deactivate_active_item() for index in range(self.list_ctrl.GetItemCount()): if self.list_ctrl.IsSelected(index): self.list_ctrl.Select(index, False) self.__controller.enable_module_controls_panel_buttons()",False,if event . EventObject is self . list_ctrl :,if self . __controller . GetActiveItem ( ) :,0.8820016898747209,9.864703138979419
"def _create_valid_graph(graph): nodes = graph.nodes() for i in range(len(nodes)): for j in range(len(nodes)): if i == j: continue edge = (nodes[i], nodes[j]) <mask>: graph.del_edge(edge) graph.add_edge(edge, 1)",False,if graph . has_edge ( edge ) :,if edge not in graph . edges ( ) :,0.8820016898747209,13.650604313545333
"def _parse_param_value(name, datatype, default): if datatype == ""bool"": <mask>: return True elif default.lower() == ""false"": return False else: _s = ""{}: Invalid default value '{}' for bool parameter {}"" raise SyntaxError(_s.format(self.name, default, p)) elif datatype == ""int"": if type(default) == int: return default else: return int(default, 0) elif datatype == ""real"": if type(default) == float: return default else: return float(default) else: return str(default)",True,"if default . lower ( ) == ""true"" :","if default . lower ( ) == ""true"" :",0.8820016898747209,100.00000000000004
"def get_size(self, shape_info): # The size is the data, that have constant size. state = np.random.RandomState().get_state() size = 0 for elem in state: <mask>: size += len(elem) elif isinstance(elem, np.ndarray): size += elem.size * elem.itemsize elif isinstance(elem, int): size += np.dtype(""int"").itemsize elif isinstance(elem, float): size += np.dtype(""float"").itemsize else: raise NotImplementedError() return size",False,"if isinstance ( elem , str ) :","if isinstance ( elem , ( list , tuple ) ) :",0.8820016898747209,36.462858619364674
"def _merge_substs(self, subst, new_substs): subst = subst.copy() for new_subst in new_substs: for name, var in new_subst.items(): if name not in subst: subst[name] = var <mask>: subst[name].PasteVariable(var) return subst",False,elif subst [ name ] is not var :,"if isinstance ( subst [ name ] , PasteVariable ) :",0.8820016898747209,24.808415001701817
"def _load_weights_if_possible(self, model, init_weight_path=None): """"""Loads model weights when it is provided."""""" if init_weight_path: logging.info(""Load weights: {}"".format(init_weight_path)) <mask>: checkpoint = tf.train.Checkpoint( model=model, optimizer=self._create_optimizer() ) checkpoint.restore(init_weight_path) else: model.load_weights(init_weight_path) else: logging.info(""Weights not loaded from path:{}"".format(init_weight_path))",False,if self . use_tpu :,if self . _is_checkpoint_enabled ( ) :,0.8820016898747209,14.323145079400492
"def _cleanup_inactive_receivexlogs(self, site): if site in self.receivexlogs: if not self.receivexlogs[site].running: <mask>: self.receivexlogs[site].join() del self.receivexlogs[site]",False,if self . receivexlogs [ site ] . is_alive ( ) :,if self . receiptexlogs [ site ] . is_active ( ) :,0.8820016898747209,53.33505353503043
"def get_asset(self, path): """"""Loads an asset by path."""""" clean_path = cleanup_path(path).strip(""/"") nodes = [self.asset_root] + self.theme_asset_roots for node in nodes: for piece in clean_path.split(""/""): node = node.get_child(piece) <mask>: break if node is not None: return node return None",True,if node is None :,if node is None :,0.8820016898747209,100.00000000000004
"def palindromic_substrings(s): if not s: return [[]] results = [] for i in range(len(s), 0, -1): sub = s[:i] <mask>: for rest in palindromic_substrings(s[i:]): results.append([sub] + rest) return results",False,if sub == sub [ : : - 1 ] :,if sub :,0.8820016898747209,0.0
"def debug_tree(tree): l = [] for elt in tree: <mask>: l.append(_names.get(elt, elt)) elif isinstance(elt, str): l.append(elt) else: l.append(debug_tree(elt)) return l",False,"if isinstance ( elt , ( int , long ) ) :","if isinstance ( elt , ( tuple , list ) ) :",0.8820016898747209,54.52469119630866
"def shared_username(account): username = os.environ.get(""SHARED_USERNAME"", ""PKKid"") for user in account.users(): <mask>: return username elif ( user.username and user.email and user.id and username.lower() in (user.username.lower(), user.email.lower(), str(user.id)) ): return username pytest.skip(""Shared user %s wasn`t found in your MyPlex account"" % username)",False,if user . title . lower ( ) == username . lower ( ) :,"if user . username and username . lower ( ) in ( user . username , user . email ) :",0.8820016898747209,26.153117750218023
"def process_schema_element(self, e): if e.name is None: return self.debug1(""adding element: %s"", e.name) t = self.get_type(e.type) if t: <mask>: del self.pending_elements[e.name] self.retval[self.tns].elements[e.name] = e else: self.pending_elements[e.name] = e",True,if e . name in self . pending_elements :,if e . name in self . pending_elements :,0.8820016898747209,100.00000000000004
"def __setitem__(self, key, value): with self._lock: try: link = self._get_link_and_move_to_front_of_ll(key) except KeyError: <mask>: self._set_key_and_add_to_front_of_ll(key, value) else: evicted = self._set_key_and_evict_last_in_ll(key, value) super(LRI, self).__delitem__(evicted) super(LRI, self).__setitem__(key, value) else: link[VALUE] = value",False,if len ( self ) < self . max_size :,if LINK_KEY_IN_LL :,0.8820016898747209,4.4959869933858485
"def __delattr__(self, name): if name == ""__dict__"": raise AttributeError( ""%r object attribute '__dict__' is read-only"" % self.__class__.__name__ ) if name in self._local_type_vars: <mask>: # A data descriptor, like a property or a slot. type_attr = getattr(self._local_type, name, _marker) type(type_attr).__delete__(type_attr, self) return # Otherwise it goes directly in the dict # Begin inlined function _get_dict() dct = _local_get_dict(self) try: del dct[name] except KeyError: raise AttributeError(name)",False,if name in self . _local_type_del_descriptors :,if name in self . _local_type_vars :,0.8820016898747209,69.963138207288
"def update_participants(self, refresh=True): for participant in list(self.participants_dict): <mask>: continue self.removeItem(self.participants_dict[participant]) self.participant_items.remove(self.participants_dict[participant]) del self.participants_dict[participant] for participant in self.simulator_config.participants: if participant in self.participants_dict: self.participants_dict[participant].refresh() else: self.insert_participant(participant) if refresh: self.update_view()",False,if participant is None or participant == self . simulator_config . broadcast_part :,if participant in self . participants_dict :,0.8820016898747209,5.490133261314248
"def insert_bigger_b_add(node): if node.op == theano.tensor.add: inputs = list(node.inputs) <mask>: inputs[-1] = theano.tensor.concatenate((inputs[-1], inputs[-1])) return [node.op(*inputs)] return False",False,if inputs [ - 1 ] . owner is None :,if len ( inputs ) > 1 :,0.8820016898747209,5.367626065580593
"def _activate_cancel_status(self, cancel_status): if self._cancel_status is not None: self._cancel_status._tasks.remove(self) self._cancel_status = cancel_status if self._cancel_status is not None: self._cancel_status._tasks.add(self) <mask>: self._attempt_delivery_of_any_pending_cancel()",False,if self . _cancel_status . effectively_cancelled :,if self . _cancel_status . is_active ( ) :,0.8820016898747209,54.3742768222752
"def writeLibraryGeometry(fp, meshes, config, shapes=None): progress = Progress(len(meshes), None) fp.write(""\n <library_geometries>\n"") for mIdx, mesh in enumerate(meshes): <mask>: shape = None else: shape = shapes[mIdx] writeGeometry(fp, mesh, config, shape) progress.step() fp.write("" </library_geometries>\n"")",True,if shapes is None :,if shapes is None :,0.8820016898747209,100.00000000000004
"def init_module_config(module_json, config, config_path=default_config_path): if ""config"" in module_json[""meta""]: if module_json[""meta""][""config""]: if module_json[""name""] not in config: config.add_section(module_json[""name""]) for config_var in module_json[""meta""][""config""]: <mask>: config.set(module_json[""name""], config_var, """") return config",False,"if config_var not in config [ module_json [ ""name"" ] ] :",if config_var . startswith ( config_path ) :,0.8820016898747209,14.789754718862433
"def get_const_defines(flags, prefix=""""): defs = [] for k, v in globals().items(): if isinstance(v, int): if v & flags: <mask>: if k.startswith(prefix): defs.append(k) else: defs.append(k) return defs",False,if prefix :,"if k . startswith ( ""_"" ) :",0.8820016898747209,4.990049701936832
"def __init__(self, source, encoding=DEFAULT_ENCODING): self.data = {} with open(source, encoding=encoding) as file_: for line in file_: line = line.strip() <mask>: continue k, v = line.split(""="", 1) k = k.strip() v = v.strip() if len(v) >= 2 and ( (v[0] == ""'"" and v[-1] == ""'"") or (v[0] == '""' and v[-1] == '""') ): v = v.strip(""'\"""") self.data[k] = v",False,"if not line or line . startswith ( ""#"" ) or ""="" not in line :",if not line :,0.8820016898747209,1.2951112459987986
"def __detect_console_logger(self): logger = self.log while logger: for handler in logger.handlers[:]: <mask>: if handler.stream in (sys.stdout, sys.stderr): self.logger_handlers.append(handler) if logger.root == logger: break else: logger = logger.root",False,"if isinstance ( handler , StreamHandler ) :","if isinstance ( handler , logging . StreamHandler ) :",0.8820016898747209,52.53819788848316
"def check_heuristic_in_sql(): heurs = set() excluded = [""Equal assembly or pseudo-code"", ""All or most attributes""] for heur in HEURISTICS: name = heur[""name""] <mask>: continue sql = heur[""sql""] if sql.lower().find(name.lower()) == -1: print((""SQL command not correctly associated to %s"" % repr(name))) print(sql) assert sql.find(name) != -1 heurs.add(name) print(""Heuristics:"") import pprint pprint.pprint(heurs)",True,if name in excluded :,if name in excluded :,0.8820016898747209,100.00000000000004
"def read(self, size=-1): buf = bytearray() while size != 0 and self.cursor < self.maxpos: <mask>: self.seek_to_block(self.cursor) part = self.current_stream.read(size) if size > 0: if len(part) == 0: raise EOFError() size -= len(part) self.cursor += len(part) buf += part return bytes(buf)",False,if not self . in_current_block ( self . cursor ) :,if self . current_stream . eof ( ) :,0.8820016898747209,9.987617065555241
"def get_project_dir(env): project_file = workon_home / env / "".project"" if project_file.exists(): with project_file.open() as f: project_dir = f.readline().strip() <mask>: return project_dir else: err( ""Corrupted or outdated:"", project_file, ""\nDirectory"", project_dir, ""doesn't exist."", )",False,if os . path . exists ( project_dir ) :,if os . path . isdir ( project_dir ) :,0.8820016898747209,73.48889200874659
"def _cache_mem(curr_out, prev_mem, mem_len, reuse_len=None): """"""cache hidden states into memory."""""" if mem_len is None or mem_len == 0: return None else: <mask>: curr_out = curr_out[:reuse_len] if prev_mem is None: new_mem = curr_out[-mem_len:] else: new_mem = tf.concat([prev_mem, curr_out], 0)[-mem_len:] return tf.keras.backend.stop_gradient(new_mem)",False,if reuse_len is not None and reuse_len > 0 :,if reuse_len is not None :,0.8820016898747209,41.06951993704473
"def cleanup_channel(self, to_cleanup): public_key, id_ = to_cleanup # TODO: Maybe run it threaded? try: with db_session: channel = self.session.mds.ChannelMetadata.get_for_update( public_key=public_key, id_=id_ ) <mask>: return channel.local_version = 0 channel.contents.delete(bulk=True) except Exception as e: self._logger.warning(""Exception while cleaning unsubscribed channel: %"", str(e))",True,if not channel :,if not channel :,0.8820016898747209,100.00000000000004
"def best_image(width, height): # A heuristic for finding closest sized image to required size. image = images[0] for img in images: if img.width == width and img.height == height: # Exact match always used return img <mask>: # At least wide enough, and largest area image = img return image",False,elif img . width >= width and img . width * img . height > image . width * image . height :,if img . width > width and img . height > height :,0.8820016898747209,24.892286909879342
"def add_peer_to_blob(self, contact: ""KademliaPeer"", key: bytes) -> None: now = self.loop.time() if key in self._data_store: current = list(filter(lambda x: x[0] == contact, self._data_store[key])) <mask>: self._data_store[key][self._data_store[key].index(current[0])] = ( contact, now, ) else: self._data_store[key].append((contact, now)) else: self._data_store[key] = [(contact, now)]",False,if len ( current ) > 0 :,if current :,0.8820016898747209,0.0
"def dump(self): self.ql.log.info(""[*] Dumping object: %s"" % (self.sf_name)) for field in self._fields_: if isinstance(getattr(self, field[0]), POINTER64): self.ql.log.info(""%s: 0x%x"" % (field[0], getattr(self, field[0]).value)) elif isinstance(getattr(self, field[0]), int): self.ql.log.info(""%s: %d"" % (field[0], getattr(self, field[0]))) <mask>: self.ql.log.info(""%s: %s"" % (field[0], getattr(self, field[0]).decode()))",True,"elif isinstance ( getattr ( self , field [ 0 ] ) , bytes ) :","elif isinstance ( getattr ( self , field [ 0 ] ) , bytes ) :",0.8820016898747209,100.00000000000004
"def GeneratePageMetatadata(self, task): address_space = self.session.GetParameter(""default_address_space"") for vma in task.mm.mmap.walk_list(""vm_next""): start = vma.vm_start end = vma.vm_end # Skip the entire region. <mask>: continue # Done. if start > self.plugin_args.end: break for vaddr in utils.xrange(start, end, 0x1000): if self.plugin_args.start <= vaddr <= self.plugin_args.end: yield vaddr, self._CreateMetadata(address_space.describe_vtop(vaddr))",False,if end < self . plugin_args . start :,if start == self . plugin_args . start :,0.8820016898747209,63.15552371794033
"def _available_symbols(self, scoperef, expr): cplns = [] found_names = set() while scoperef: elem = self._elem_from_scoperef(scoperef) for child in elem: name = child.get(""name"", """") if name.startswith(expr): if name not in found_names: found_names.add(name) ilk = child.get(""ilk"") or child.tag cplns.append((ilk, name)) scoperef = self.parent_scoperef_from_scoperef(scoperef) <mask>: break return sorted(cplns, key=operator.itemgetter(1))",True,if not scoperef :,if not scoperef :,0.8820016898747209,100.00000000000004
"def get_xenapi_host(self): """"""Return the xenapi host on which nova-compute runs on."""""" with self._get_session() as session: <mask>: return session.xenapi.host.get_by_uuid(self.host_uuid) else: return session.xenapi.session.get_this_host(session.handle)",True,if self . host_uuid :,if self . host_uuid :,0.8820016898747209,100.00000000000004
"def stream_docker_log(log_stream): async for line in log_stream: <mask>: logger.debug(line[""stream""].strip()) elif ""status"" in line: logger.debug(line[""status""].strip()) elif ""error"" in line: logger.error(line[""error""].strip()) raise DockerBuildError",False,"if ""stream"" in line and line [ ""stream"" ] . strip ( ) :","if ""stream"" in line :",0.8820016898747209,17.469470584451173
"def test_wildcard_import(): bonobo = __import__(""bonobo"") assert bonobo.__version__ for name in dir(bonobo): # ignore attributes starting by underscores <mask>: continue attr = getattr(bonobo, name) if inspect.ismodule(attr): continue assert name in bonobo.__all__",True,"if name . startswith ( ""_"" ) :","if name . startswith ( ""_"" ) :",0.8820016898747209,100.00000000000004
"def _coerce_to_bool(self, node, var, true_val=True): """"""Coerce the values in a variable to bools."""""" bool_var = self.program.NewVariable() for b in var.bindings: v = b.data if isinstance(v, mixin.PythonConstant) and isinstance(v.pyval, bool): const = v.pyval is true_val <mask>: const = not true_val elif not compare.compatible_with(v, False): const = true_val else: const = None bool_var.AddBinding(self.convert.bool_values[const], {b}, node) return bool_var",False,"elif not compare . compatible_with ( v , True ) :","elif isinstance ( v , mixin . Constant ) :",0.8820016898747209,15.8270999899438
"def _parse_policies(self, policies_yaml): for item in policies_yaml: id_ = required_key(item, ""id"") controls_ids = required_key(item, ""controls"") if not isinstance(controls_ids, list): <mask>: msg = ""Policy {id_} contains invalid controls list {controls}."".format( id_=id_, controls=str(controls_ids) ) raise ValueError(msg) self.policies[id_] = controls_ids",False,"if controls_ids != ""all"" :",if controls_ids not in self . policies :,0.8820016898747209,27.77619034011791
"def pong(self, payload: Union[str, bytes] = """") -> None: if self.trace_enabled and self.ping_pong_trace_enabled: <mask>: payload = payload.decode(""utf-8"") self.logger.debug( ""Sending a pong data frame "" f""(session id: {self.session_id}, payload: {payload})"" ) data = _build_data_frame_for_sending(payload, FrameHeader.OPCODE_PONG) with self.sock_send_lock: self.sock.send(data)",True,"if isinstance ( payload , bytes ) :","if isinstance ( payload , bytes ) :",0.8820016898747209,100.00000000000004
"def _extract_curve_feature_log(arg): """"""extract sampled curve feature for log items"""""" try: inp, res = arg config = inp.config with inp.target: sch, args = inp.task.instantiate(config) fea = feature.get_buffer_curve_sample_flatten(sch, args, sample_n=20) x = np.concatenate((fea, list(config.get_other_option().values()))) <mask>: y = inp.task.flop / np.mean(res.costs) else: y = 0.0 return x, y except Exception: # pylint: disable=broad-except return None",False,if res . error_no == 0 :,if res . costs :,0.8820016898747209,15.719010513286515
"def messageSourceStamps(self, source_stamps): text = """" for ss in source_stamps: source = """" if ss[""branch""]: source += ""[branch %s] "" % ss[""branch""] <mask>: source += str(ss[""revision""]) else: source += ""HEAD"" if ss[""patch""] is not None: source += "" (plus patch)"" discriminator = """" if ss[""codebase""]: discriminator = "" '%s'"" % ss[""codebase""] text += ""Build Source Stamp%s: %s\n"" % (discriminator, source) return text",True,"if ss [ ""revision"" ] :","if ss [ ""revision"" ] :",0.8820016898747209,100.00000000000004
"def find_repository(): orig_path = path = os.path.realpath(""."") drive, path = os.path.splitdrive(path) while path: current_path = os.path.join(drive, path) current_repo = LocalRepository(current_path) if current_repo.isValid(): return current_repo path, path_tail = os.path.split(current_path) <mask>: raise CannotFindRepository(""Cannot find repository for %s"" % (orig_path,))",False,if not path_tail :,if path_tail != orig_path :,0.8820016898747209,17.747405280050266
"def compute_indices(text: str, tokens): indices = [] for i, token in enumerate(tokens): <mask>: current_index = indices[-1] + len(tokens[i - 1][0]) indices.append(current_index + text[current_index:].find(token[0])) else: indices.append(text.find(token[0])) return indices",False,if 1 <= i :,if i > 0 :,0.8820016898747209,11.51015341649912
"def _add_defaults_data_files(self): # getting distribution.data_files if self.distribution.has_data_files(): for item in self.distribution.data_files: if isinstance(item, str): # plain file item = convert_path(item) <mask>: self.filelist.append(item) else: # a (dirname, filenames) tuple dirname, filenames = item for f in filenames: f = convert_path(f) if os.path.isfile(f): self.filelist.append(f)",True,if os . path . isfile ( item ) :,if os . path . isfile ( item ) :,0.8820016898747209,100.00000000000004
"def libcxx_define(settings): compiler = _base_compiler(settings) libcxx = settings.get_safe(""compiler.libcxx"") if not compiler or not libcxx: return """" if str(compiler) in GCC_LIKE: <mask>: return ""_GLIBCXX_USE_CXX11_ABI=0"" elif str(libcxx) == ""libstdc++11"": return ""_GLIBCXX_USE_CXX11_ABI=1"" return """"",False,"if str ( libcxx ) == ""libstdc++"" :","if str ( libcxx ) == ""libstdc++10"" :",0.8820016898747209,80.91067115702207
"def _populate_tree(self, element, d): """"""Populates an etree with attributes & elements, given a dict."""""" for k, v in d.iteritems(): if isinstance(v, dict): self._populate_dict(element, k, v) elif isinstance(v, list): self._populate_list(element, k, v) elif isinstance(v, bool): self._populate_bool(element, k, v) <mask>: self._populate_str(element, k, v) elif type(v) in [int, float, long, complex]: self._populate_number(element, k, v)",False,"elif isinstance ( v , basestring ) :","elif isinstance ( v , str ) :",0.8820016898747209,59.4603557501361
"def test_seek(self): <mask>: print(""create large file via seek (may be sparse file) ..."") with self.open(TESTFN, ""wb"") as f: f.write(b""z"") f.seek(0) f.seek(size) f.write(b""a"") f.flush() if verbose: print(""check file size with os.fstat"") self.assertEqual(os.fstat(f.fileno())[stat.ST_SIZE], size + 1)",True,if verbose :,if verbose :,0.8820016898747209,0.0
"def serialize_review_url_field(self, obj, **kwargs): if obj.review_ui: review_request = obj.get_review_request() <mask>: local_site_name = review_request.local_site.name else: local_site_name = None return local_site_reverse( ""file-attachment"", local_site_name=local_site_name, kwargs={ ""review_request_id"": review_request.display_id, ""file_attachment_id"": obj.pk, }, ) return """"",False,if review_request . local_site_id :,if review_request . local_site :,0.8820016898747209,71.19674182275
"def on_item_down_clicked(self, button): model = self.treeview.get_model() for s in self._get_selected(): <mask>: # XXX need model.swap old = model.get_iter(s[0]) iter = model.insert(s[0] + 2) for i in range(3): model.set_value(iter, i, model.get_value(old, i)) model.remove(old) self.treeview.get_selection().select_iter(iter) self._update_filter_string()",False,if s [ 0 ] < len ( model ) - 1 :,if s [ 0 ] == 0 :,0.8820016898747209,29.95197100101507
"def writer(self): """"""loop forever and copy socket->serial"""""" while self.alive: try: data = self.socket.recv(1024) <mask>: break self.serial.write(b"""".join(self.rfc2217.filter(data))) except socket.error as msg: self.log.error(""{}"".format(msg)) # probably got disconnected break self.stop()",True,if not data :,if not data :,0.8820016898747209,100.00000000000004
"def __getitem__(self, key): if key == 1: return self.get_value() elif key == 0: return self.cell[0] elif isinstance(key, slice): s = list(self.cell.__getitem__(key)) <mask>: s[s.index(self.cell[1])] = self.get_value() return s else: raise IndexError(key)",False,if self . cell [ 1 ] in s :,if len ( s ) > 1 :,0.8820016898747209,6.082317172853824
"def test_error_stream(environ, start_response): writer = start_response(""200 OK"", []) wsgi_errors = environ[""wsgi.errors""] error_msg = None for method in [ ""flush"", ""write"", ""writelines"", ]: <mask>: error_msg = ""wsgi.errors has no '%s' attr"" % method if not error_msg and not callable(getattr(wsgi_errors, method)): error_msg = ""wsgi.errors.%s attr is not callable"" % method if error_msg: break return_msg = error_msg or ""success"" writer(return_msg) return []",False,"if not hasattr ( wsgi_errors , method ) :","if not error_msg and not hasattr ( wsgi_errors , method ) :",0.8820016898747209,61.28081331864041
"def job_rule_modules(app): rules_module_list = [] for rules_module_name in __job_rule_module_names(app): rules_module = sys.modules.get(rules_module_name, None) <mask>: # if using a non-default module, it's not imported until a JobRunnerMapper is instantiated when the first # JobWrapper is created rules_module = importlib.import_module(rules_module_name) rules_module_list.append(rules_module) return rules_module_list",False,if not rules_module :,if rules_module is None :,0.8820016898747209,27.77619034011791
"def discover_hdfstore(f): d = dict() for key in f.keys(): d2 = d key2 = key.lstrip(""/"") while ""/"" in key2: group, key2 = key2.split(""/"", 1) <mask>: d2[group] = dict() d2 = d2[group] d2[key2] = f.get_storer(key) return discover(d)",True,if group not in d2 :,if group not in d2 :,0.8820016898747209,100.00000000000004
"def test_update_zone(self): zone = self.driver.list_zones()[0] updated_zone = self.driver.update_zone(zone=zone, domain="""", extra={""paused"": True}) self.assertEqual(zone.id, updated_zone.id) self.assertEqual(zone.domain, updated_zone.domain) self.assertEqual(zone.type, updated_zone.type) self.assertEqual(zone.ttl, updated_zone.ttl) for key in set(zone.extra) | set(updated_zone.extra): <mask>: self.assertNotEqual(zone.extra[key], updated_zone.extra[key]) else: self.assertEqual(zone.extra[key], updated_zone.extra[key])",False,"if key in ( ""paused"" , ""modified_on"" ) :","if key in ( ""paused"" , ""paused"" ) :",0.8820016898747209,65.26220818377338
"def ESP(phrase): for num, name in enumerate(devname): <mask>: dev = devid[num] if custom_action_keyword[""Dict""][""On""] in phrase: ctrl = ""=ON"" say(""Turning On "" + name) elif custom_action_keyword[""Dict""][""Off""] in phrase: ctrl = ""=OFF"" say(""Turning Off "" + name) rq = requests.head(""https://"" + ip + dev + ctrl, verify=False)",False,if name . lower ( ) in phrase :,if num in devid :,0.8820016898747209,6.316906128202129
"def filter_ports(self, dpid, in_port, nw_id, allow_nw_id_external=None): assert nw_id != self.nw_id_unknown ret = [] for port in self.get_ports(dpid): nw_id_ = port.network_id if port.port_no == in_port: continue if nw_id_ == nw_id: ret.append(port.port_no) <mask>: ret.append(port.port_no) return ret",False,elif allow_nw_id_external is not None and nw_id_ == allow_nw_id_external :,if allow_nw_id_external :,0.8820016898747209,13.057133736325447
"def tail(filename): if os.path.isfile(filename): file = open(filename, ""r"") st_results = os.stat(filename) st_size = st_results[6] file.seek(st_size) while 1: where = file.tell() line = file.readline() <mask>: time.sleep(1) file.seek(where) else: print( line, ) # already has newline else: print_error(""File not found, cannot tail."")",False,if not line :,if line :,0.8820016898747209,0.0
"def proc_day_of_week(d): if expanded[4][0] != ""*"": diff_day_of_week = nearest_diff_method(d.isoweekday() % 7, expanded[4], 7) if diff_day_of_week is not None and diff_day_of_week != 0: <mask>: d += relativedelta(days=diff_day_of_week, hour=23, minute=59, second=59) else: d += relativedelta(days=diff_day_of_week, hour=0, minute=0, second=0) return True, d return False, d",False,if is_prev :,"if expanded [ 4 ] [ 0 ] == ""*"" :",0.8820016898747209,3.377156414337854
"def __call__(self): """"""Run all check_* methods."""""" if self.on: oldformatwarning = warnings.formatwarning warnings.formatwarning = self.formatwarning try: for name in dir(self): if name.startswith(""check_""): method = getattr(self, name) <mask>: method() finally: warnings.formatwarning = oldformatwarning",False,if method and callable ( method ) :,"if hasattr ( method , ""__call__"" ) :",0.8820016898747209,8.493098745313148
"def get(self, request, *args, **kwargs): if self.revision: <mask>: try: return send_file( request, self.revision.file.path, self.revision.created, self.attachment.original_filename, ) except OSError: pass else: return HttpResponseRedirect(self.revision.file.url) raise Http404",False,if settings . USE_LOCAL_PATH :,if self . attachment :,0.8820016898747209,6.316906128202129
"def _close(self): super(Recording, self)._close() if self._log_n is not None: for i in range(self.n): <mask>: self._log_n[i].close() self._log_n[i] = None",True,if self . _log_n [ i ] is not None :,if self . _log_n [ i ] is not None :,0.8820016898747209,100.00000000000004
"def addTags(self, rpcObjects=None): hosts = self._getOnlyHostObjects(rpcObjects) if hosts: title = ""Add Tags"" body = ""What tags should be added?\n\nUse a comma or space between each"" (tags, choice) = self.getText(title, body, """") <mask>: tags = str(tags).replace("" "", "","").split("","") for host in hosts: self.cuebotCall( host.addTags, ""Add Tags to %s Failed"" % host.data.name, tags ) self._update()",True,if choice :,if choice :,0.8820016898747209,0.0
"def available_datasets(self): """"""Automatically determine datasets provided by this file"""""" res = self.resolution coordinates = [""pixel_longitude"", ""pixel_latitude""] for var_name, val in self.file_content.items(): <mask>: ds_info = { ""file_type"": self.filetype_info[""file_type""], ""resolution"": res, } if not self.is_geo: ds_info[""coordinates""] = coordinates yield DatasetID(name=var_name, resolution=res), ds_info",False,"if isinstance ( val , netCDF4 . Variable ) :","if val == ""pixel_longitude"" :",0.8820016898747209,5.522397783539471
"def extract_from_file(fname: PathIsh) -> Iterator[Extraction]: path = Path(fname) fallback_dt = file_mtime(path) p = Parser(path) for r in p.walk(): <mask>: yield r else: yield Visit( url=r.url, dt=fallback_dt, locator=Loc.file(fname), # TODO line number context=r.context, )",False,"if isinstance ( r , Exception ) :",if r . url is None :,0.8820016898747209,7.492442692259767
"def init_module_config(module_json, config, config_path=default_config_path): if ""config"" in module_json[""meta""]: if module_json[""meta""][""config""]: <mask>: config.add_section(module_json[""name""]) for config_var in module_json[""meta""][""config""]: if config_var not in config[module_json[""name""]]: config.set(module_json[""name""], config_var, """") return config",True,"if module_json [ ""name"" ] not in config :","if module_json [ ""name"" ] not in config :",0.8820016898747209,100.00000000000004
"def _create_entities(parsed_entities, sidx, eidx): entities = [] for k, vs in parsed_entities.items(): <mask>: vs = [vs] for value in vs: entities.append( { ""entity"": k, ""start"": sidx, ""end"": eidx, # can't be more specific ""value"": value, } ) return entities",False,"if not isinstance ( vs , list ) :","if not isinstance ( vs , ( list , tuple ) ) :",0.8820016898747209,44.08231875586728
"def _telegram_upload_stream(self, stream, **kwargs): """"""Perform upload defined in a stream."""""" msg = None try: stream.accept() msg = self._telegram_special_message( chat_id=stream.identifier.id, content=stream.raw, msg_type=stream.stream_type, **kwargs, ) except Exception: log.exception(f""Upload of {stream.name} to {stream.identifier} failed."") else: <mask>: stream.error() else: stream.success()",True,if msg is None :,if msg is None :,0.8820016898747209,100.00000000000004
"def readlines(self, size=-1): if self._nbr == self._size: return [] # leave all additional logic to our readline method, we just check the size out = [] nbr = 0 while True: line = self.readline() <mask>: break out.append(line) if size > -1: nbr += len(line) if nbr > size: break # END handle size constraint # END readline loop return out",True,if not line :,if not line :,0.8820016898747209,100.00000000000004
"def clean_permissions( cls, requestor: ""User"", group: auth_models.Group, errors: Dict[Optional[str], List[ValidationError]], cleaned_input: dict, ): field = ""add_permissions"" permission_items = cleaned_input.get(field) if permission_items: cleaned_input[field] = get_permissions(permission_items) <mask>: cls.ensure_can_manage_permissions( requestor, errors, field, permission_items )",False,if not requestor . is_superuser :,if errors :,0.8820016898747209,0.0
"def _bwd(subj=None, obj=None, seen=None): seen.add(obj) for s, o in evalPath(graph, (None, self.path, obj)): <mask>: yield s, o if self.more: if s in seen: continue for s2, o2 in _bwd(None, s, seen): yield s2, o",False,if not subj or subj == s :,if s in seen :,0.8820016898747209,6.316906128202129
"def generate_data(self, request): """"""Generate data for the widget."""""" uptime = {} cache_stats = get_cache_stats() if cache_stats: for hosts, stats in cache_stats: <mask>: uptime[""value""] = stats[""uptime""] / 60 / 60 / 24 uptime[""unit""] = _(""days"") elif stats[""uptime""] > 3600: uptime[""value""] = stats[""uptime""] / 60 / 60 uptime[""unit""] = _(""hours"") else: uptime[""value""] = stats[""uptime""] / 60 uptime[""unit""] = _(""minutes"") return {""cache_stats"": cache_stats, ""uptime"": uptime}",False,"if stats [ ""uptime"" ] > 86400 :","if stats [ ""uptime"" ] > 24 :",0.8820016898747209,78.25422900366438
def refresh(self): if self._handle: source = self._db.get_repository_from_handle(self._handle) <mask>: self._title = str(source.get_type()) self._value = source.get_name(),True,if source :,if source :,0.8820016898747209,0.0
"def _gridconvvalue(self, value): if isinstance(value, (str, _tkinter.Tcl_Obj)): try: svalue = str(value) <mask>: return None elif ""."" in svalue: return getdouble(svalue) else: return getint(svalue) except ValueError: pass return value",False,if not svalue :,"if svalue == """" :",0.8820016898747209,8.643019616048525
"def parseGrants(self, tree): for grant in tree.findall("".//Grant""): grantee = Grantee() g = grant.find("".//Grantee"") grantee.xsi_type = g.attrib[""{http://www.w3.org/2001/XMLSchema-instance}type""] grantee.permission = grant.find(""Permission"").text for el in g: <mask>: grantee.display_name = el.text else: grantee.tag = el.tag grantee.name = el.text self.grantees.append(grantee)",False,"if el . tag == ""DisplayName"" :","if el . tag == ""display"" :",0.8820016898747209,70.71067811865478
"def __init__(self, name: Optional[str] = None, order: int = 0): if name is None: if order == 0: name = ""std_dev"" <mask>: name = ""sample_std_dev"" else: name = f""std_dev{order})"" super().__init__(name=name, order=order) self.order = order",True,elif order == 1 :,elif order == 1 :,0.8820016898747209,100.00000000000004
"def _shouldRollover(self): if self.maxBytes > 0: # are we rolling over? try: self.stream.seek(0, 2) # due to non-posix-compliant Windows feature except IOError: return True <mask>: return True else: self._degrade(False, ""Rotation done or not needed at this time"") return False",False,if self . stream . tell ( ) >= self . maxBytes :,if self . stream . tell ( ) == self . maxBytes :,0.8820016898747209,78.25422900366432
"def userfullname(): """"""Get the user's full name."""""" global _userfullname if not _userfullname: uid = os.getuid() entry = pwd_from_uid(uid) <mask>: _userfullname = entry[4].split("","")[0] or entry[0] if not _userfullname: _userfullname = ""user%d"" % uid return _userfullname",True,if entry :,if entry :,0.8820016898747209,0.0
"def drop(self): # mssql sql = ""if object_id('%s') is not null drop table %s"" % (self.tname, self.tname) try: self.execute(sql) except Exception as e: self.conn.rollback() <mask>: raise # sqlite sql = ""drop table if exists %s"" % self.tname self.execute(sql)",False,"if ""syntax error"" not in str ( e ) :","if e . args [ 0 ] != ""Database not found"" :",0.8820016898747209,4.112982349983277
"def _find_delimiter(f, block_size=2 ** 16): delimiter = b""\n"" if f.tell() == 0: return 0 while True: b = f.read(block_size) <mask>: return f.tell() elif delimiter in b: return f.tell() - len(b) + b.index(delimiter) + 1",False,if not b :,if b == b :,0.8820016898747209,17.965205598154213
"def _convert(container): if _value_marker in container: force_list = False values = container.pop(_value_marker) <mask>: force_list = True values.extend(_convert(x[1]) for x in sorted(container.items())) if not force_list and len(values) == 1: values = values[0] if not container: return values return _convert(container) elif container.pop(_list_marker, False): return [_convert(x[1]) for x in sorted(container.items())] return dict_cls((k, _convert(v)) for k, v in iteritems(container))",False,"if container . pop ( _list_marker , False ) :",if values :,0.8820016898747209,0.0
"def fitting(self, value): self._fitting = value if self._fitting is not None: <mask>: try: os.makedirs(dirname(self.checkpoint_path())) except FileExistsError as ex: pass # race to create if not os.path.exists(dirname(self.tensorboard_path())): try: os.makedirs(dirname(self.tensorboard_path())) except FileExistsError as ex: pass # race to create",True,if not os . path . exists ( dirname ( self . checkpoint_path ( ) ) ) :,if not os . path . exists ( dirname ( self . checkpoint_path ( ) ) ) :,0.8820016898747209,100.00000000000004
"def _make_headers(self): libraries = self._df.columns.to_list() columns = [] for library in libraries: version = self._package_versions[library] library_description = self._libraries_description.get(library) <mask>: library += "" {}"".format(library_description) columns.append( ""{library}<br><small>{version}</small>"".format( library=library, version=version ) ) return [""""] + columns",True,if library_description :,if library_description :,0.8820016898747209,100.00000000000004
"def plugin_on_song_ended(self, song, stopped): if song is not None: poll = self.rating_box.poll_vote() <mask>: ups = int(song.get(""~#wins"") or 0) downs = int(song.get(""~#losses"") or 0) ups += poll[0] downs += poll[1] song[""~#wins""] = ups song[""~#losses""] = downs song[""~#rating""] = ups / max((ups + downs), 2) # note: ^^^ Look into implementing w/ confidence intervals! song[""~#score""] = ups - downs",False,if poll [ 0 ] >= 1 or poll [ 1 ] >= 1 :,if poll is not None :,0.8820016898747209,2.8722725093023906
"def submit(self, pig_script, params): workflow = None try: workflow = self._create_workflow(pig_script, params) mapping = dict( [(param[""name""], param[""value""]) for param in workflow.get_parameters()] ) oozie_wf = _submit_workflow(self.user, self.fs, self.jt, workflow, mapping) finally: <mask>: workflow.delete(skip_trash=True) return oozie_wf",True,if workflow :,if workflow :,0.8820016898747209,0.0
"def test_parse(self): correct = 0 for example in EXAMPLES: try: schema.parse(example.schema_string) <mask>: correct += 1 else: self.fail(""Invalid schema was parsed: "" + example.schema_string) except: if not example.valid: correct += 1 else: self.fail(""Valid schema failed to parse: "" + example.schema_string) fail_msg = ""Parse behavior correct on %d out of %d schemas."" % ( correct, len(EXAMPLES), ) self.assertEqual(correct, len(EXAMPLES), fail_msg)",False,if example . valid :,if not example . valid :,0.8820016898747209,53.7284965911771
"def handle_sent(self, elt): sent = [] for child in elt: if child.tag in (""wf"", ""punc""): itm = self.handle_word(child) <mask>: sent.extend(itm) else: sent.append(itm) else: raise ValueError(""Unexpected element %s"" % child.tag) return SemcorSentence(elt.attrib[""snum""], sent)",False,"if self . _unit == ""word"" :","if isinstance ( itm , list ) :",0.8820016898747209,4.513617516969122
"def _set_property(self, target_widget, pname, value): if pname == ""text"": state = target_widget.cget(""state"") <mask>: target_widget.configure(state=tk.NORMAL) target_widget.insert(""0.0"", value) target_widget.configure(state=tk.DISABLED) else: target_widget.insert(""0.0"", value) else: super(TKText, self)._set_property(target_widget, pname, value)",False,if state == tk . DISABLED :,if state == tk . NORMAL :,0.8820016898747209,70.71067811865478
"def get_vrf_tables(self, vrf_rf=None): vrf_tables = {} for (scope_id, table_id), table in self._tables.items(): if scope_id is None: continue <mask>: continue vrf_tables[(scope_id, table_id)] = table return vrf_tables",False,if vrf_rf is not None and table_id != vrf_rf :,if vrf_rf is None or table_id == vrf_rf :,0.8820016898747209,51.417085326632545
"def new_f(self, *args, **kwargs): for obj in f(self, *args, **kwargs): if self.protected == False: if ""user"" in obj and obj[""user""][""protected""]: continue <mask>: continue yield obj",False,"elif ""protected"" in obj and obj [ ""protected"" ] :","if ""protected"" in obj and obj [ ""protected"" ] :",0.8820016898747209,91.93227152249175
"def draw(self, context): col = self.layout.column() col.operator(""node.sv_show_latest_commits"") if context.scene.sv_new_version: col_alert = self.layout.column() col_alert.alert = True col_alert.operator(""node.sverchok_update_addon"", text=""Upgrade Sverchok addon"") else: col.operator(""node.sverchok_check_for_upgrades_wsha"", text=""Check for updates"") with sv_preferences() as prefs: <mask>: col.operator(""node.sv_run_pydoc"")",False,if prefs . developer_mode :,"if prefs . get ( ""version"" ) == context . scene . sv_new_version :",0.8820016898747209,7.681104116622756
"def generate_tag_1_data(ids): if len(ids) != SAMPLE_NUM: raise ValueError(""len ids should equal to sample number"") counter = 0 for sample_i in range(SAMPLE_NUM): one_data = [ids[sample_i]] valid_set = [x for x in range(TAG_INTERVAL[0], TAG_INTERVAL[1])] features = np.random.choice(valid_set, FEATURE_NUM, replace=False) one_data += ["":"".join([x, ""1.0""]) for x in features] counter += 1 <mask>: print(""generate data {}"".format(counter)) yield one_data",False,if counter % 10000 == 0 :,if counter % 2 == 0 :,0.8820016898747209,50.000000000000014
"def handle_api_languages(self, http_context): mgr = PluginManager.get(aj.context) languages = set() for id in mgr: locale_dir = mgr.get_content_path(id, ""locale"") <mask>: for lang in os.listdir(locale_dir): if lang != ""app.pot"": languages.add(lang) return sorted(list(languages))",True,if os . path . isdir ( locale_dir ) :,if os . path . isdir ( locale_dir ) :,0.8820016898747209,100.00000000000004
"def update(self, t): # direction right - up for i in range(self.grid.x): for j in range(self.grid.y): distance = self.test_func(i, j, t) <mask>: self.turn_off_tile(i, j) elif distance < 1: self.transform_tile(i, j, distance) else: self.turn_on_tile(i, j)",False,if distance == 0 :,if distance > 1 :,0.8820016898747209,19.3576934939088
"def _handle_autocomplete_request_for_text(text): if not hasattr(text, ""autocompleter""): if isinstance(text, (CodeViewText, ShellText)) and text.is_python_text(): if isinstance(text, CodeViewText): text.autocompleter = Completer(text) <mask>: text.autocompleter = ShellCompleter(text) text.bind(""<1>"", text.autocompleter.on_text_click) else: return text.autocompleter.handle_autocomplete_request()",True,"elif isinstance ( text , ShellText ) :","elif isinstance ( text , ShellText ) :",0.8820016898747209,100.00000000000004
"def test_create_repository(repo_name, expected_status, client): with client_with_identity(""devtable"", client) as cl: body = { ""namespace"": ""devtable"", ""repository"": repo_name, ""visibility"": ""public"", ""description"": ""foo"", } result = conduct_api_call( client, RepositoryList, ""post"", None, body, expected_code=expected_status ).json <mask>: assert result[""name""] == repo_name assert ( model.repository.get_repository(""devtable"", repo_name).name == repo_name )",False,if expected_status == 201 :,"if ""name"" in result :",0.8820016898747209,6.770186228657864
"def _apply_filter(filter_item, filter_list): for filter_method in filter_list: try: <mask>: return False except Exception as e: raise MessageException( ""Toolbox filter exception from '{}': {}."".format( filter_method.__name__, unicodify(e) ) ) return True",False,"if not filter_method ( context , filter_item ) :",if filter_method ( filter_item ) :,0.8820016898747209,50.04968472345046
"def printsumfp(fp, filename, out=sys.stdout): m = md5() try: while 1: data = fp.read(bufsize) if not data: break <mask>: data = data.encode(fp.encoding) m.update(data) except IOError as msg: sys.stderr.write(""%s: I/O error: %s\n"" % (filename, msg)) return 1 out.write(""%s %s\n"" % (m.hexdigest(), filename)) return 0",False,"if isinstance ( data , str ) :",if fp . encoding :,0.8820016898747209,6.9717291216921975
"def get_block_loc_keys(block): """"""Extract loc_keys used by @block"""""" symbols = set() for instr in block.lines: <mask>: if isinstance(instr.raw, list): for expr in instr.raw: symbols.update(get_expr_locs(expr)) else: for arg in instr.args: symbols.update(get_expr_locs(arg)) return symbols",False,"if isinstance ( instr , AsmRaw ) :","if isinstance ( instr , ast . Expr ) :",0.8820016898747209,45.180100180492246
"def get_operations(cls, info, operations: List[ProductAttributeAssignInput]): """"""Resolve all passed global ids into integer PKs of the Attribute type."""""" product_attrs_pks = [] variant_attrs_pks = [] for operation in operations: pk = from_global_id_strict_type( operation.id, only_type=Attribute, field=""operations"" ) <mask>: product_attrs_pks.append(pk) else: variant_attrs_pks.append(pk) return product_attrs_pks, variant_attrs_pks",False,if operation . type == ProductAttributeType . PRODUCT :,if info . product_attribute_assign_type == ProductAttributeAssignInput . product_attribute_assign_type :,0.8820016898747209,7.946357815712818
"def _collect_manual_intervention_nodes(pipeline_tree): for act in pipeline_tree[""activities""].values(): if act[""type""] == ""SubProcess"": _collect_manual_intervention_nodes(act[""pipeline""]) <mask>: manual_intervention_nodes.add(act[""id""])",False,"elif act [ ""component"" ] [ ""code"" ] in MANUAL_INTERVENTION_COMP_CODES :","elif act [ ""type"" ] == ""Intervention"" :",0.8820016898747209,13.75452701110242
"def prompt_authorization(self, stacks: List[Stack]): auth_required_per_resource = auth_per_resource(stacks) for resource, authorization_required in auth_required_per_resource: <mask>: auth_confirm = confirm( f""\t{self.start_bold}{resource} may not have authorization defined, Is this okay?{self.end_bold}"", default=False, ) if not auth_confirm: raise GuidedDeployFailedError(msg=""Security Constraints Not Satisfied!"")",False,if not authorization_required :,if authorization_required :,0.8820016898747209,57.89300674674101
"def get_cloud_credential(self): """"""Return the credential which is directly tied to the inventory source type."""""" credential = None for cred in self.credentials.all(): if self.source in CLOUD_PROVIDERS: <mask>: credential = cred break else: # these need to be returned in the API credential field if cred.credential_type.kind != ""vault"": credential = cred break return credential",False,"if cred . kind == self . source . replace ( ""ec2"" , ""aws"" ) :","if cred . credential_type . kind == ""vault"" :",0.8820016898747209,17.04960194455123
"def validate_party_details(self): if self.party: if not frappe.db.exists(self.party_type, self.party): frappe.throw(_(""Invalid {0}: {1}"").format(self.party_type, self.party)) <mask>: self.validate_account_type( self.party_account, [erpnext.get_party_account_type(self.party_type)] )",False,"if self . party_account and self . party_type in ( ""Customer"" , ""Supplier"" ) :",if self . party_account :,0.8820016898747209,8.552033621493605
"def __iter__(self): it = DiskHashMerger.__iter__(self) direct_upstreams = self.direct_upstreams for k, groups in it: t = list([[] for _ in range(self.size)]) for i, g in enumerate(groups): <mask>: if i in direct_upstreams: t[i] = g else: g.sort(key=itemgetter(0)) g1 = [] for _, vs in g: g1.extend(vs) t[i] = g1 yield k, tuple(t)",False,if g :,"if isinstance ( g , list ) :",0.8820016898747209,7.267884212102741
"def _unpack_scales(scales, vidxs): scaleData = [None, None, None] for i in range(3): <mask>: break scale = scales[i] if not math.isnan(scale): vidx1, vidx2 = vidxs[i * 2], vidxs[i * 2 + 1] scaleData[i] = (int(vidx1), int(vidx2), float(scale)) return scaleData",False,"if i >= min ( len ( scales ) , len ( vidxs ) // 2 ) :",if i >= len ( scales ) :,0.8820016898747209,18.821655523327284
"def _make_ext_obj(self, obj): ext = self._get_ext_class(obj.objname)() for name, val in obj.body: <mask>: raise Exception( ""Error val should be a list, this is a python-opcua bug"", name, type(val), val, ) else: for attname, v in val: self._set_attr(ext, attname, v) return ext",False,"if not isinstance ( val , list ) :","if not isinstance ( val , ( list , tuple ) ) :",0.8820016898747209,44.08231875586728
"def insertLine(self, refnum, linenum, line): i = -1 for i, row in enumerate(self.rows): if row[0] == linenum: if row[refnum + 1] is None: row[refnum + 1] = line return # else keep looking <mask>: break self.rows.insert(i, self.newRow(linenum, refnum, line))",False,elif row [ 0 ] > linenum :,if i == len ( self . rows ) - 1 :,0.8820016898747209,3.0890553181566975
"def valid_localparts(strip_delimiters=False): for line in ABRIDGED_LOCALPART_VALID_TESTS.split(""\n""): # strip line, skip over empty lines line = line.strip() if line == """": continue # skip over comments or empty lines match = COMMENT.match(line) if match: continue # skip over localparts with delimiters if strip_delimiters: <mask>: continue yield line",False,"if "","" in line or "";"" in line :","if not line . endswith ( ""\n"" ) :",0.8820016898747209,4.659101701766641
"def encodingChanged(self, idx): encoding = str(self.mode_combo.currentText()) validator = None if encoding == ""hex"": # only clear the box if there are non-hex chars # before setting the validator. txt = str(self.data_edit.text()) <mask>: self.data_edit.setText("""") regex = QtCore.QRegExp(""^[0-9A-Fa-f]+$"") validator = QtGui.QRegExpValidator(regex) self.data_edit.setValidator(validator) self.renderMemory()",False,if not all ( c in string . hexdigits for c in txt ) :,"if txt == ""0"" :",0.8820016898747209,3.0297048914466935
"def _compare_single_run(self, compares_done): try: compare_id, redo = self.in_queue.get( timeout=float(self.config[""ExpertSettings""][""block_delay""]) ) except Empty: pass else: if self._decide_whether_to_process(compare_id, redo, compares_done): <mask>: self.db_interface.delete_old_compare_result(compare_id) compares_done.add(compare_id) self._process_compare(compare_id) if self.callback: self.callback()",False,if redo :,if self . db_interface :,0.8820016898747209,7.809849842300637
"def _transform_bin(self, X: DataFrame): if self._bin_map: <mask>: X = X.copy(deep=True) with pd.option_context(""mode.chained_assignment"", None): # Pandas complains about SettingWithCopyWarning, but this should be valid. for column in self._bin_map: X[column] = binning.bin_column( series=X[column], mapping=self._bin_map[column], dtype=self._astype_map[column], ) return X",False,if not self . inplace :,"if isinstance ( X , pd . DataFrame ) :",0.8820016898747209,5.522397783539471
"def escape(text, newline=False): """"""Escape special html characters."""""" if isinstance(text, str): if ""&"" in text: text = text.replace(""&"", ""&amp;"") if "">"" in text: text = text.replace("">"", ""&gt;"") if ""<"" in text: text = text.replace(""<"", ""&lt;"") if '""' in text: text = text.replace('""', ""&quot;"") <mask>: text = text.replace(""'"", ""&quot;"") if newline: if ""\n"" in text: text = text.replace(""\n"", ""<br>"") return text",False,"if ""'"" in text :","if '""' in text :",0.8820016898747209,34.57207846419409
"def read(self): """"""Reads the robots.txt URL and feeds it to the parser."""""" try: f = urllib.request.urlopen(self.url) except urllib.error.HTTPError as err: <mask>: self.disallow_all = True elif err.code >= 400 and err.code < 500: self.allow_all = True else: raw = f.read() self.parse(raw.decode(""utf-8"").splitlines())",False,"if err . code in ( 401 , 403 ) :",if err . code == 404 and err . code == 404 :,0.8820016898747209,17.395797375642243
"def post_create(self, user, billing=None): from weblate.trans.models import Change if billing: billing.projects.add(self) <mask>: self.access_control = Project.ACCESS_PRIVATE else: self.access_control = Project.ACCESS_PUBLIC self.save() if not user.is_superuser: self.add_user(user, ""@Administration"") Change.objects.create( action=Change.ACTION_CREATE_PROJECT, project=self, user=user, author=user )",False,if billing . plan . change_access_control :,if user . is_superuser :,0.8820016898747209,5.244835934727967
"def visitConst(self, node): if self.documentable: <mask>: self.documentable.append(make_docstring(node.value, node.lineno)) else: self.documentable = None",False,"if type ( node . value ) in ( StringType , UnicodeType ) :",if node . value :,0.8820016898747209,7.468220329575271
"def requires(self): requires = copy.deepcopy(self._requires) # Auto add dependencies when parameters reference the Ouptuts of # another stack. parameters = self.parameters for value in parameters.values(): if isinstance(value, basestring) and ""::"" in value: stack_name, _ = value.split(""::"") else: continue <mask>: requires.add(stack_name) return requires",True,if stack_name not in requires :,if stack_name not in requires :,0.8820016898747209,100.00000000000004
"def __load_protos(): g = globals() for k, v in g.items(): <mask>: name = k[4:] modname = name.lower() try: mod = __import__(modname, g, level=1) PPP.set_p(v, getattr(mod, name)) except (ImportError, AttributeError): continue",False,"if k . startswith ( ""PPP_"" ) :","if k . startswith ( ""protos_"" ) :",0.8820016898747209,70.16879391277372
"def init_weights(self): """"""Initialize model weights."""""" for m in self.predict_layers.modules(): if isinstance(m, nn.Conv2d): kaiming_init(m) elif isinstance(m, nn.BatchNorm2d): constant_init(m, 1) <mask>: normal_init(m, std=0.01)",False,"elif isinstance ( m , nn . Linear ) :","elif isinstance ( m , nn . BatchNorm2d ) :",0.8820016898747209,70.71067811865478
"def get_data(self): """"""get all data from sockets"""""" si = self.inputs parameters = [] for socket in si: <mask>: parameters.append(socket.sv_get()) else: parameters.append(socket.sv_get(default=[[]])) return match_long_repeat(parameters)",False,if len ( socket . prop_name ) > 0 :,if socket . is_socket :,0.8820016898747209,8.085182710148953
"def test_parse_query_params_comparable_field(self): query_params = {""filter[int_field][gt]"": 42, ""filter[int_field][lte]"": 9000} fields = self.view.parse_query_params(query_params) for key, field_name in fields.items(): if field_name[""int_field""][""op""] == ""gt"": assert_equal(field_name[""int_field""][""value""], 42) <mask>: assert_equal(field_name[""int_field""][""value""], 9000) else: self.fail()",True,"elif field_name [ ""int_field"" ] [ ""op"" ] == ""lte"" :","elif field_name [ ""int_field"" ] [ ""op"" ] == ""lte"" :",0.8820016898747209,100.00000000000004
"def _create_examples(self, lines, set_type): """"""Creates examples for the training and dev sets."""""" examples = [] for (i, line) in enumerate(lines): <mask>: continue guid = ""%s-%s"" % (set_type, i) text = line[0] bbox = line[1] label = line[2] examples.append( DocExample(guid=guid, text_a=text, text_b=None, bbox=bbox, label=label) ) return examples",False,if i == 0 :,if not line :,0.8820016898747209,11.521590992286539
"def _get_attr(sdk_path, mod_attr_path, checked=True): try: attr_mod, attr_path = ( mod_attr_path.split(""#"") if ""#"" in mod_attr_path else (mod_attr_path, """") ) full_mod_path = ""{}.{}"".format(sdk_path, attr_mod) if attr_mod else sdk_path op = import_module(full_mod_path) <mask>: # Only load attributes if needed for part in attr_path.split("".""): op = getattr(op, part) return op except (ImportError, AttributeError) as ex: if checked: return None raise ex",False,if attr_path :,"if op . __name__ == ""Attribute"" :",0.8820016898747209,3.737437943747671
"def _load_ui_modules(self, modules: Any) -> None: if isinstance(modules, types.ModuleType): self._load_ui_modules(dict((n, getattr(modules, n)) for n in dir(modules))) elif isinstance(modules, list): for m in modules: self._load_ui_modules(m) else: assert isinstance(modules, dict) for name, cls in modules.items(): try: <mask>: self.ui_modules[name] = cls except TypeError: pass",False,"if issubclass ( cls , UIModule ) :","if isinstance ( cls , types . ModuleType ) :",0.8820016898747209,20.556680845025987
"def _remove_obsolete_leafs(input_dict): if not isinstance(input_dict, dict): return if input_dict[LEAF_MARKER]: bottom_leafs = input_dict[LEAF_MARKER] for leaf in bottom_leafs: <mask>: input_dict[LEAF_MARKER].remove(leaf) for subtree in input_dict.keys(): _remove_obsolete_leafs(input_dict[subtree])",False,if leaf in input_dict :,if leaf in input_dict [ LEAF_MARKER ] :,0.8820016898747209,43.36189090348677
"def decode(self, value, force=False): ""Return a unicode string from the bytes-like representation"" if self.decode_responses or force: <mask>: value = value.tobytes() if isinstance(value, bytes): value = value.decode(self.encoding, self.encoding_errors) return value",False,"if isinstance ( value , memoryview ) :","if isinstance ( value , bytes ) :",0.8820016898747209,59.4603557501361
"def audit(self, directive): value = _get_value(directive) if not value: return server_side = directive.name.startswith(""proxy_"") for var in compile_script(value): char = """" <mask>: char = ""\\n"" elif not server_side and var.can_contain(""\r""): char = ""\\r"" else: continue reason = 'At least variable ""${var}"" can contain ""{char}""'.format( var=var.name, char=char ) self.add_issue(directive=[directive] + var.providers, reason=reason)",False,"if var . can_contain ( ""\n"" ) :","if not server_side and var . can_contain ( ""\n"" ) :",0.8820016898747209,64.70107100770988
"def checkFilename(filename): # useful in case of drag and drop while True: if filename[0] == ""'"": filename = filename[1:] <mask>: filename = filename[:-1] if os.path.exists(filename): return filename filename = input( ""[!] Cannot find '%s'.\n[*] Enter a valid name of the file containing the paths to test -> "" % filename )",False,"if filename [ len ( filename ) - 1 ] == ""'"" :","elif filename [ - 1 ] == '""' :",0.8820016898747209,31.070152518033044
"def findfiles(self, dir, base, rec): try: names = os.listdir(dir or os.curdir) except os.error as msg: print(msg) return [] list = [] subdirs = [] for name in names: fn = os.path.join(dir, name) <mask>: subdirs.append(fn) else: if fnmatch.fnmatch(name, base): list.append(fn) if rec: for subdir in subdirs: list.extend(self.findfiles(subdir, base, rec)) return list",False,if os . path . isdir ( fn ) :,"if fnmatch . fnmatch ( name , base ) :",0.8820016898747209,10.552670315936318
"def loop(handler, obj): handler.response.write(""<table>"") for k, v in obj.__dict__.items(): <mask>: style = ""color: red"" if not v else """" handler.response.write( '<tr style=""{}""><td>{}:</td><td>{}</td></tr>'.format(style, k, v) ) handler.response.write(""</table>"")",False,"if not k in ( ""data"" , ""gae_user"" , ""credentials"" , ""content"" , ""config"" ) :","if isinstance ( v , dict ) :",0.8820016898747209,1.140004272922865
"def anypython(request): name = request.param executable = getexecutable(name) if executable is None: if sys.platform == ""win32"": executable = winpymap.get(name, None) if executable: executable = py.path.local(executable) <mask>: return executable pytest.skip(""no suitable %s found"" % (name,)) return executable",False,if executable . check ( ) :,if not os . path . exists ( executable ) :,0.8820016898747209,9.864703138979419
"def __init__(self, socketpath=None): if socketpath is None: <mask>: socketpath = ""/var/run/usbmuxd"" else: socketpath = ""/var/run/usbmuxd"" self.socketpath = socketpath self.listener = MuxConnection(socketpath, BinaryProtocol) try: self.listener.listen() self.version = 0 self.protoclass = BinaryProtocol except MuxVersionError: self.listener = MuxConnection(socketpath, PlistProtocol) self.listener.listen() self.protoclass = PlistProtocol self.version = 1 self.devices = self.listener.devices",False,"if sys . platform == ""darwin"" :","if sys . platform == ""win32"" :",0.8820016898747209,70.71067811865478
"def _validate_distinct_on_different_types_and_field_orders( self, collection, query, expected_results, get_mock_result ): self.count = 0 self.get_mock_result = get_mock_result query_iterable = collection.query_items(query, enable_cross_partition_query=True) results = list(query_iterable) for i in range(len(expected_results)): <mask>: self.assertDictEqual(results[i], expected_results[i]) elif isinstance(results[i], list): self.assertListEqual(results[i], expected_results[i]) else: self.assertEqual(results[i], expected_results[i]) self.count = 0",True,"if isinstance ( results [ i ] , dict ) :","if isinstance ( results [ i ] , dict ) :",0.8820016898747209,100.00000000000004
"def getRootId(self, id): with self.connect() as cu: while True: stmt = ""select parent_path_id from hierarchy where path_id = ?"" cu.execute(stmt, (id,)) parent_id = cu.fetchone()[0] <mask>: return id id = parent_id",False,if parent_id is None or parent_id == id :,if parent_id == id :,0.8820016898747209,47.23665527410149
"def add(self, path): with self.get_lock(path): <mask>: self.entries[path] = {} self.entries[path][""lock""] = self.new_locks[path] del self.new_locks[path] self.lru.append(path)",False,if not path in self . entries :,if path not in self . entries :,0.8820016898747209,58.14307369682194
"def _get_coordinates_for_dataset_key(self, dsid): """"""Get the coordinate dataset keys for *dsid*."""""" ds_info = self.ids[dsid] cids = [] for cinfo in ds_info.get(""coordinates"", []): if not isinstance(cinfo, dict): cinfo = {""name"": cinfo} cinfo[""resolution""] = ds_info[""resolution""] <mask>: cinfo[""polarization""] = ds_info[""polarization""] cid = DatasetID(**cinfo) cids.append(self.get_dataset_key(cid)) return cids",False,"if ""polarization"" in ds_info :","if ds_info [ ""polarization"" ] :",0.8820016898747209,28.227983861579556
"def build_from_gdobj(cls, gdobj, steal=False): # Avoid calling cls.__init__ by first instanciating a placeholder, then # overloading it __class__ to turn it into an instance of the right class ret = BuiltinInitPlaceholder() if steal: assert ffi.typeof(gdobj).kind == ""pointer"" ret._gd_ptr = gdobj else: <mask>: ret._gd_ptr = cls._copy_gdobj(gdobj) else: ret._gd_ptr = cls._copy_gdobj(ffi.addressof(gdobj)) ret.__class__ = cls return ret",False,"if ffi . typeof ( gdobj ) . kind == ""pointer"" :","if isinstance ( gdobj , ( int , long ) ) :",0.8820016898747209,6.632729312157198
"def _listen_output(self): ""NB! works in background thread"" try: while True: chars = self._proc.read(1) <mask>: as_bytes = chars.encode(self.encoding) self._make_output_available(as_bytes) else: self._error = ""EOF"" break except Exception as e: self._error = str(e)",False,if len ( chars ) > 0 :,if chars :,0.8820016898747209,0.0
"def result( metrics: Dict[metric_types.MetricKey, Any] ) -> Dict[metric_types.AttributionsKey, Dict[Text, Union[float, np.ndarray]]]: """"""Returns mean attributions."""""" total_attributions = metrics[total_attributions_key] weighted_count = metrics[weighted_example_count_key] attributions = {} for k, v in total_attributions.items(): <mask>: attributions[k] = float(""nan"") else: attributions[k] = v / weighted_count return {key: attributions}",False,"if np . isclose ( weighted_count , 0.0 ) :",if weighted_count is None :,0.8820016898747209,13.597602315271134
"def write_if_changed(path, data): if isinstance(data, str): data = data.encode() changed = False with open(os.open(path, os.O_CREAT | os.O_RDWR), ""wb+"") as f: f.seek(0) current = f.read() <mask>: changed = True f.seek(0) f.write(data) f.truncate() os.fsync(f) return changed",True,if current != data :,if current != data :,0.8820016898747209,100.00000000000004
"def detect_ssl_option(self): for option in self.ssl_options(): if scan_argv(self.argv, option) is not None: for other_option in self.ssl_options(): <mask>: if scan_argv(self.argv, other_option) is not None: raise ConfigurationError( ""Cannot give both %s and %s"" % (option, other_option) ) return option",False,if option != other_option :,if other_option != option :,0.8820016898747209,39.28146509005134
"def _infer_return_type(*args): """"""Look at the type of all args and divine their implied return type."""""" return_type = None for arg in args: <mask>: continue if isinstance(arg, bytes): if return_type is str: raise TypeError(""Can't mix bytes and non-bytes in "" ""path components."") return_type = bytes else: if return_type is bytes: raise TypeError(""Can't mix bytes and non-bytes in "" ""path components."") return_type = str if return_type is None: return str # tempfile APIs return a str by default. return return_type",True,if arg is None :,if arg is None :,0.8820016898747209,100.00000000000004
"def _get_app(self, body=None): app = self._app if app is None: try: tasks = self.tasks.tasks # is a group except AttributeError: tasks = self.tasks <mask>: app = tasks[0]._app if app is None and body is not None: app = body._app return app if app is not None else current_app",False,if len ( tasks ) :,if len ( tasks ) == 1 :,0.8820016898747209,46.713797772819994
"def add_field(self, field): self.remove_field(field.name) self.fields[field.name] = field self.columns[field.db_column] = field self._sorted_field_list.insert(field) self._update_field_lists() if field.default is not None: self.defaults[field] = field.default <mask>: self._default_callables[field] = field.default self._default_callable_list.append((field.name, field.default)) else: self._default_dict[field] = field.default self._default_by_name[field.name] = field.default",False,if callable ( field . default ) :,if field . name in self . _default_callables :,0.8820016898747209,8.516593018819643
"def _get_families(self): families = [] for name, ext in self._get_family_dirs(): <mask>: # is a directory family = self.get_resource( FileSystemPackageFamilyResource.key, location=self.location, name=name ) else: family = self.get_resource( FileSystemCombinedPackageFamilyResource.key, location=self.location, name=name, ext=ext, ) families.append(family) return families",False,if ext is None :,"if ext == ""."" :",0.8820016898747209,12.22307556087252
"def test(model, data_loader, device=None): device = device or torch.device(""cpu"") model.eval() correct = 0 total = 0 with torch.no_grad(): for batch_idx, (data, target) in enumerate(data_loader): <mask>: break data, target = data.to(device), target.to(device) outputs = model(data) _, predicted = torch.max(outputs.data, 1) total += target.size(0) correct += (predicted == target).sum().item() return correct / total",False,if batch_idx * len ( data ) > TEST_SIZE :,if batch_idx == 0 :,0.8820016898747209,17.267606045625936
"def __animate_progress(self): """"""Change the status message, mostly used to animate progress."""""" while True: sleep_time = ThreadPool.PROGRESS_IDLE_DELAY with self.__progress_lock: <mask>: sleep_time = ThreadPool.PROGRESS_IDLE_DELAY elif self.__show_animation: self.__progress_status.update_progress(self.__current_operation_name) sleep_time = ThreadPool.PROGRESS_UPDATE_DELAY else: self.__progress_status.show_as_ready() sleep_time = ThreadPool.PROGRESS_IDLE_DELAY # Allow some time for progress status to be updated. time.sleep(sleep_time)",False,if not self . __progress_status :,if self . __show_progress :,0.8820016898747209,34.13065354365521
"def _parse_subtitles(self, video_data, url_key): subtitles = {} for translation in video_data.get(""translations"", []): vtt_path = translation.get(url_key) <mask>: continue lang = translation.get(""language_w3c"") or ISO639Utils.long2short( translation[""language_medium""] ) subtitles.setdefault(lang, []).append( { ""ext"": ""vtt"", ""url"": vtt_path, } ) return subtitles",True,if not vtt_path :,if not vtt_path :,0.8820016898747209,100.00000000000004
"def postprocess_message(self, msg): if msg[""type""] == ""sample"" and msg[""value""] is not None: fn, value = msg[""fn""], msg[""value""] value_batch_ndims = jnp.ndim(value) - fn.event_dim fn_batch_ndim = len(fn.batch_shape) <mask>: prepend_shapes = (1,) * (value_batch_ndims - fn_batch_ndim) msg[""fn""] = tree_map( lambda x: jnp.reshape(x, prepend_shapes + jnp.shape(x)), fn )",False,if fn_batch_ndim < value_batch_ndims :,if fn_batch_ndim > 0 :,0.8820016898747209,39.14236894465539
"def TryMerge(self, d): while d.avail() > 0: tt = d.getVarInt32() <mask>: self.set_filename(d.getPrefixedString()) continue if tt == 0: raise ProtocolBuffer.ProtocolBufferDecodeError d.skipData(tt)",True,if tt == 10 :,if tt == 10 :,0.8820016898747209,100.00000000000004
"def createError(self, line, pos, description): global ENABLE_PYIMPORT msg = ""Line "" + unicode(line) + "": "" + unicode(description) if ENABLE_JS2PY_ERRORS: <mask>: import js2py.base return js2py.base.MakeError(""SyntaxError"", msg) else: return ENABLE_JS2PY_ERRORS(msg) else: return JsSyntaxError(msg)",False,"if isinstance ( ENABLE_JS2PY_ERRORS , bool ) :","if isinstance ( ENABLE_JS2PY_ERRORS , str ) :",0.8820016898747209,76.91605673134588
"def extract(self, page, start_index=0, end_index=None): items = [] for extractor in self.extractors: extracted = extractor.extract( page, start_index, end_index, self.template.ignored_regions ) for item in arg_to_iter(extracted): <mask>: if isinstance(item, (ItemProcessor, dict)): item[u""_template""] = self.template.id items.append(item) return items",True,if item :,if item :,0.8820016898747209,0.0
"def create_volume(self, volume): """"""Create a volume."""""" try: cmd = [""volume"", ""create"", volume[""name""], ""%sG"" % (volume[""size""])] <mask>: cmd.append(""pool"") cmd.append(self.configuration.eqlx_pool) if self.configuration.san_thin_provision: cmd.append(""thin-provision"") out = self._eql_execute(*cmd) self.add_multihost_access(volume) return self._get_volume_data(out) except Exception: with excutils.save_and_reraise_exception(): LOG.error('Failed to create volume ""%s"".', volume[""name""])",False,"if self . configuration . eqlx_pool != ""default"" :",if self . configuration . eqlx_pool :,0.8820016898747209,51.01469472683877
"def clean(self): # TODO: check for clashes if the random code is already taken if not self.code: self.code = u""static-%s"" % uuid.uuid4() if not self.site: placeholders = StaticPlaceholder.objects.filter( code=self.code, site__isnull=True ) <mask>: placeholders = placeholders.exclude(pk=self.pk) if placeholders.exists(): raise ValidationError( _(""A static placeholder with the same site and code already exists"") )",True,if self . pk :,if self . pk :,0.8820016898747209,100.00000000000004
"def spawnMenu(self, event): clickedPos = self.getRowByAbs(event.Position) self.ensureSelection(clickedPos) selection = self.getSelectedBoosters() mainBooster = None if clickedPos != -1: try: booster = self.boosters[clickedPos] except IndexError: pass else: <mask>: mainBooster = booster itemContext = None if mainBooster is None else _t(""Booster"") menu = ContextMenu.getMenu( self, mainBooster, selection, (""boosterItem"", itemContext), (""boosterItemMisc"", itemContext), ) if menu: self.PopupMenu(menu)",False,if booster in self . original :,if booster :,0.8820016898747209,0.0
"def init_errorhandler(): # http error handling for ex in default_exceptions: <mask>: app.register_error_handler(ex, error_http) elif ex == 500: app.register_error_handler(ex, internal_error) if services.ldap: # Only way of catching the LDAPException upon logging in with LDAP server down @app.errorhandler(services.ldap.LDAPException) def handle_exception(e): log.debug(""LDAP server not accessible while trying to login to opds feed"") return error_http(FailedDependency())",False,if ex < 500 :,if ex == 400 :,0.8820016898747209,17.965205598154213
"def reloadCols(self): self.columns = [] for i, (name, fmt, *shape) in enumerate(self.npy.dtype.descr): <mask>: t = anytype elif ""M"" in fmt: self.addColumn(Column(name, type=date, getter=lambda c, r, i=i: str(r[i]))) continue elif ""i"" in fmt: t = int elif ""f"" in fmt: t = float else: t = anytype self.addColumn(ColumnItem(name, i, type=t))",False,if shape :,"if ""D"" in fmt :",0.8820016898747209,7.809849842300637
"def Proc2(IntParIO): IntLoc = IntParIO + 10 while True: if Char1Glob == ""A"": IntLoc = IntLoc - 1 IntParIO = IntLoc - IntGlob EnumLoc = Ident1 <mask>: break return IntParIO",False,if EnumLoc == Ident1 :,if EnumLoc == Ident2 :,0.8820016898747209,53.7284965911771
"def opengroup(self, name=None): gid = self.groups self.groupwidths.append(None) if self.groups > MAXGROUPS: raise error(""too many groups"") if name is not None: ogid = self.groupdict.get(name, None) <mask>: raise error( ""redefinition of group name %r as group %d; "" ""was group %d"" % (name, gid, ogid) ) self.groupdict[name] = gid return gid",True,if ogid is not None :,if ogid is not None :,0.8820016898747209,100.00000000000004
"def __setattr__(self, name: str, val: Any): if name.startswith(""COMPUTED_""): if name in self: old_val = self[name] <mask>: return raise KeyError( ""Computed attributed '{}' already exists "" ""with a different value! old={}, new={}."".format(name, old_val, val) ) self[name] = val else: super().__setattr__(name, val)",False,if old_val == val :,if old_val != val :,0.8820016898747209,50.000000000000014
"def get_all_function_symbols(self, module=""kernel""): """"""Gets all the function tuples for the given module"""""" ret = [] symtable = self.type_map if module in symtable: mod = symtable[module] for (addr, (name, _sym_types)) in mod.items(): <mask>: addr = addr + self.shift_address ret.append([name, addr]) else: debug.info(""All symbols requested for non-existent module %s"" % module) return ret",False,if self . shift_address and addr :,if addr . startswith ( self . shift_address ) :,0.8820016898747209,35.65506208559251
"def __call__(self, frame: FrameType, event: str, arg: Any) -> ""CallTracer"": code = frame.f_code if ( event not in SUPPORTED_EVENTS or code.co_name == ""trace_types"" or self.should_trace and not self.should_trace(code) ): return self try: <mask>: self.handle_call(frame) elif event == EVENT_RETURN: self.handle_return(frame, arg) else: logger.error(""Cannot handle event %s"", event) except Exception: logger.exception(""Failed collecting trace"") return self",True,if event == EVENT_CALL :,if event == EVENT_CALL :,0.8820016898747209,100.00000000000004
"def test_update_topic(self): async with self.chat_client: await self._create_thread() topic = ""update topic"" async with self.chat_thread_client: await self.chat_thread_client.update_topic(topic=topic) # delete chat threads <mask>: await self.chat_client.delete_chat_thread(self.thread_id)",False,if not self . is_playback ( ) :,if self . thread_id :,0.8820016898747209,10.759051250985632
"def render_observation(self): x = self.read_head_position label = ""Observation Grid : "" x_str = """" for j in range(-1, self.rows + 1): if j != -1: x_str += "" "" * len(label) for i in range(-2, self.input_width + 2): <mask>: x_str += colorize(self._get_str_obs((i, j)), ""green"", highlight=True) else: x_str += self._get_str_obs((i, j)) x_str += ""\n"" x_str = label + x_str return x_str",False,if i == x [ 0 ] and j == x [ 1 ] :,if i == j :,0.8820016898747209,8.990698828155159
"def build(opt): dpath = os.path.join(opt[""datapath""], ""QA-ZRE"") version = None if not build_data.built(dpath, version_string=version): print(""[building data: "" + dpath + ""]"") <mask>: # An older version exists, so remove these outdated files. build_data.remove_dir(dpath) build_data.make_dir(dpath) # Download the data. for downloadable_file in RESOURCES: downloadable_file.download_file(dpath) # Mark the data as built. build_data.mark_done(dpath, version_string=version)",True,if build_data . built ( dpath ) :,if build_data . built ( dpath ) :,0.8820016898747209,100.00000000000004
"def git_pull(args): if len(args) <= 1: repo = _get_repo() _confirm_dangerous() url = args[0] if len(args) == 1 else repo.remotes.get(""origin"", """") <mask>: origin = url url = repo.remotes.get(origin) if url: repo.pull(origin_uri=url) else: print(""No pull URL."") else: print(command_help[""git pull""])",False,if url in repo . remotes :,if not origin :,0.8820016898747209,8.9730240870212
"def FindAndDelete(script, sig): """"""Consensus critical, see FindAndDelete() in Satoshi codebase"""""" r = b"""" last_sop_idx = sop_idx = 0 skip = True for (opcode, data, sop_idx) in script.raw_iter(): <mask>: r += script[last_sop_idx:sop_idx] last_sop_idx = sop_idx if script[sop_idx : sop_idx + len(sig)] == sig: skip = True else: skip = False if not skip: r += script[last_sop_idx:] return CScript(r)",False,if not skip :,"if opcode == ""FindAndDelete"" :",0.8820016898747209,6.567274736060395
"def get_ip_info(ipaddress): """"""Returns device information by IP address"""""" result = {} try: ip = IPAddress.objects.select_related().get(address=ipaddress) except IPAddress.DoesNotExist: pass else: if ip.venture is not None: result[""venture_id""] = ip.venture.id if ip.device is not None: result[""device_id""] = ip.device.id <mask>: result[""venture_id""] = ip.device.venture.id return result",False,if ip . device . venture is not None :,if ip . eventure is not None :,0.8820016898747209,38.940039153570254
"def restore(self, state): """"""Restore the state of a mesh previously saved using save()"""""" import pickle state = pickle.loads(state) for k in state: if isinstance(state[k], list): <mask>: state[k] = [[v.x(), v.y(), v.z()] for v in state[k]] state[k] = np.array(state[k]) setattr(self, k, state[k])",False,"if isinstance ( state [ k ] [ 0 ] , QtGui . QVector3D ) :","if isinstance ( state [ k ] , np . ndarray ) :",0.8820016898747209,44.92903001150463
"def get_extra_lines(tup): ext_name, pyopencl_ver = tup if ext_name is not None: <mask>: # capital letters -> CL version, not extension yield """" yield "" Available with OpenCL %s."" % (ext_name[3:]) yield """" else: yield """" yield "" Available with the ``%s`` extension."" % ext_name yield """" if pyopencl_ver is not None: yield """" yield "" .. versionadded:: %s"" % pyopencl_ver yield """"",False,"if ext_name . startswith ( ""CL_"" ) :","if ext_name [ : 3 ] == ""_"" :",0.8820016898747209,22.718709780542323
"def _gen_remote_uri( fileobj: IO[bytes], remote_uri: Optional[ParseResult], remote_path_prefix: Optional[str], remote_path_suffix: Optional[str], sha256sum: Optional[str], ) -> ParseResult: if remote_uri is None: assert remote_path_prefix is not None and remote_path_suffix is not None <mask>: sha256sum = _hash_fileobj(fileobj) return urlparse( os.path.join(remote_path_prefix, f""{sha256sum}{remote_path_suffix}"") ) else: return remote_uri",True,if sha256sum is None :,if sha256sum is None :,0.8820016898747209,100.00000000000004
"def queries(self): if DEV: cmd = ShellCommand(""docker"", ""ps"", ""-qf"", ""name=%s"" % self.path.k8s) if not cmd.check(f""docker check for {self.path.k8s}""): <mask>: log_cmd = ShellCommand( ""docker"", ""logs"", self.path.k8s, stderr=subprocess.STDOUT ) if log_cmd.check(f""docker logs for {self.path.k8s}""): print(cmd.stdout) pytest.exit(f""container failed to start for {self.path.k8s}"") return ()",False,if not cmd . stdout . strip ( ) :,if DEV :,0.8820016898747209,0.0
"def get_range(self): present = self.xml.find(""{%s}range"" % self.namespace) if present is not None: attributes = present.attrib return_value = dict() <mask>: return_value[""minimum""] = attributes[""min""] if ""max"" in attributes: return_value[""maximum""] = attributes[""max""] return return_value return False",True,"if ""min"" in attributes :","if ""min"" in attributes :",0.8820016898747209,100.00000000000004
"def _configuredOn(self, workerid, builderid=None, masterid=None): cfg = [] for cs in itervalues(self.configured): <mask>: continue bid, mid = self.db.builders.builder_masters[cs[""buildermasterid""]] if builderid is not None and bid != builderid: continue if masterid is not None and mid != masterid: continue cfg.append({""builderid"": bid, ""masterid"": mid}) return cfg",True,"if cs [ ""workerid"" ] != workerid :","if cs [ ""workerid"" ] != workerid :",0.8820016898747209,100.00000000000004
"def __exit__(self, type, value, traceback): try: if type is not None: return self.exception_handler(type, value, traceback) finally: final_contexts = _state.contexts _state.contexts = self.old_contexts <mask>: raise StackContextInconsistentError( ""stack_context inconsistency (may be caused by yield "" 'within a ""with StackContext"" block)' ) # Break up a reference to itself to allow for faster GC on CPython. self.new_contexts = None",False,if final_contexts is not self . new_contexts :,if final_contexts :,0.8820016898747209,20.73625029909464
"def del_(self, key): initial_hash = hash_ = self.hash(key) while True: <mask>: # That key was never assigned return None elif self._keys[hash_] == key: # key found, assign with deleted sentinel self._keys[hash_] = self._deleted self._values[hash_] = self._deleted self._len -= 1 return hash_ = self._rehash(hash_) if initial_hash == hash_: # table is full and wrapped around return None",False,if self . _keys [ hash_ ] is self . _empty :,if hash_ == initial_hash :,0.8820016898747209,6.155947438501932
"def TryMerge(self, d): while d.avail() > 0: tt = d.getVarInt32() <mask>: self.set_logout_url(d.getPrefixedString()) continue if tt == 0: raise ProtocolBuffer.ProtocolBufferDecodeError d.skipData(tt)",True,if tt == 10 :,if tt == 10 :,0.8820016898747209,100.00000000000004
"def data_generator(): i = 0 max_batch_index = len(X_train) // batch_size tot = 0 while 1: <mask>: yield ( np.ones([batch_size, input_dim]) * np.nan, np.ones([batch_size, num_classes]) * np.nan, ) else: yield ( X_train[i * batch_size : (i + 1) * batch_size], y_train[i * batch_size : (i + 1) * batch_size], ) i += 1 tot += 1 i = i % max_batch_index",False,if tot > 3 * len ( X_train ) :,if tot == batch_size :,0.8820016898747209,7.966506956353643
"def title(self): ret = theme[""title""] if isinstance(self.name, six.string_types): width = self.statwidth() return ( ret + self.name[0:width].center(width).replace("" "", ""-"") + theme[""default""] ) for i, name in enumerate(self.name): width = self.colwidth() ret = ret + name[0:width].center(width).replace("" "", ""-"") <mask>: if op.color: ret = ret + theme[""frame""] + char[""dash""] + theme[""title""] else: ret = ret + char[""space""] return ret",False,if i + 1 != len ( self . vars ) :,if i == 0 :,0.8820016898747209,6.011598678897526
"def get_container_from_dport(dport, docker_client): for container in docker_client.containers(): try: ports = container[""Ports""] for port in ports: <mask>: if port[""PublicPort""] == int(dport): return container except KeyError: print(ports) pass",False,"if ""PublicPort"" in port :","if port [ ""Port"" ] == int ( dport ) :",0.8820016898747209,4.246549372656572
"def _get_parents_data(self, data): parents = 0 if data[COLUMN_PARENT]: family = self.db.get_family_from_handle(data[COLUMN_PARENT][0]) if family.get_father_handle(): parents += 1 <mask>: parents += 1 return parents",False,if family . get_mother_handle ( ) :,elif family . get_father_handle ( ) :,0.8820016898747209,58.77283725105324
"def wrapper(filename): mtime = getmtime(filename) with lock: if filename in cache: old_mtime, result = cache.pop(filename) if old_mtime == mtime: # Move to the end cache[filename] = old_mtime, result return result result = function(filename) with lock: cache[filename] = mtime, result # at the end <mask>: cache.popitem(last=False) return result",False,if len ( cache ) > max_size :,if filename in cache :,0.8820016898747209,5.171845311465849
"def execute(cls, ctx, op: ""DataFrameGroupByAgg""): try: pd.set_option(""mode.use_inf_as_na"", op.use_inf_as_na) if op.stage == OperandStage.map: cls._execute_map(ctx, op) elif op.stage == OperandStage.combine: cls._execute_combine(ctx, op) <mask>: cls._execute_agg(ctx, op) else: # pragma: no cover raise ValueError(""Aggregation operand not executable"") finally: pd.reset_option(""mode.use_inf_as_na"")",True,elif op . stage == OperandStage . agg :,elif op . stage == OperandStage . agg :,0.8820016898747209,100.00000000000004
"def FindAndDelete(script, sig): """"""Consensus critical, see FindAndDelete() in Satoshi codebase"""""" r = b"""" last_sop_idx = sop_idx = 0 skip = True for (opcode, data, sop_idx) in script.raw_iter(): if not skip: r += script[last_sop_idx:sop_idx] last_sop_idx = sop_idx <mask>: skip = True else: skip = False if not skip: r += script[last_sop_idx:] return CScript(r)",False,if script [ sop_idx : sop_idx + len ( sig ) ] == sig :,if sig == data :,0.8820016898747209,1.9794320051780008
"def extractall(zip: typing.Any, path: str) -> NoneType: for name in zip.namelist(): member = zip.getinfo(name) extracted_path = zip._extract_member(member, path, None) attr = member.external_attr >> 16 <mask>: os.chmod(extracted_path, attr)",False,if attr != 0 :,if extracted_path :,0.8820016898747209,10.400597689005304
"def find_all_gyptest_files(directory): result = [] for root, dirs, files in os.walk(directory): <mask>: dirs.remove("".svn"") result.extend([os.path.join(root, f) for f in files if is_test_name(f)]) result.sort() return result",True,"if "".svn"" in dirs :","if "".svn"" in dirs :",0.8820016898747209,100.00000000000004
"def load(cls, storefile, template_store): # Did we get file or filename? if not hasattr(storefile, ""read""): storefile = open(storefile, ""rb"") # Adjust store to have translations store = cls.convertfile(storefile, template_store) for unit in store.units: if unit.isheader(): continue # HTML does this properly on loading, others need it <mask>: unit.target = unit.source unit.rich_target = unit.rich_source return store",False,if cls . needs_target_sync :,if unit . is_html ( ) :,0.8820016898747209,6.742555929751843
"def postOptions(self): _BasicOptions.postOptions(self) if self[""jobs""]: conflicts = [""debug"", ""profile"", ""debug-stacktraces"", ""exitfirst""] for option in conflicts: if self[option]: raise usage.UsageError( ""You can't specify --%s when using --jobs"" % option ) if self[""nopm""]: <mask>: raise usage.UsageError(""You must specify --debug when using "" ""--nopm "") failure.DO_POST_MORTEM = False",False,"if not self [ ""debug"" ] :","if self [ ""--debug"" ] :",0.8820016898747209,31.708476589333063
"def filterTokenLocation(): i = None entry = None token = None tokens = [] i = 0 while 1: if not (i < len(extra.tokens)): break entry = extra.tokens[i] token = jsdict( { ""type"": entry.type, ""value"": entry.value, } ) if extra.range: token.range = entry.range <mask>: token.loc = entry.loc tokens.append(token) i += 1 extra.tokens = tokens",True,if extra . loc :,if extra . loc :,0.8820016898747209,100.00000000000004
"def on_rebalance_end(self) -> None: """"""Call when rebalancing is done."""""" self.rebalancing = False if self._rebalancing_span: self._rebalancing_span.finish() self._rebalancing_span = None sensor_state = self._rebalancing_sensor_state try: <mask>: self.log.warning( ""Missing sensor state for rebalance #%s"", self.rebalancing_count ) else: self.sensors.on_rebalance_end(self, sensor_state) finally: self._rebalancing_sensor_state = None",False,if not sensor_state :,if sensor_state is None :,0.8820016898747209,27.77619034011791
"def decorator(request, *args, **kwargs): if CALENDAR_VIEW_PERM: user = request.user if not user: return HttpResponseRedirect(settings.LOGIN_URL) occurrence, event, calendar = get_objects(request, **kwargs) if calendar: allowed = CHECK_CALENDAR_PERM_FUNC(calendar, user) <mask>: return HttpResponseRedirect(settings.LOGIN_URL) # all checks passed return function(request, *args, **kwargs) return HttpResponseNotFound(""<h1>Page not found</h1>"") return function(request, *args, **kwargs)",False,if not allowed :,if allowed :,0.8820016898747209,0.0
"def reduce_arguments(self, args): assert isinstance(args, nodes.Arguments) if args.incorrect_order(): raise InvalidArguments( ""All keyword arguments must be after positional arguments."" ) reduced_pos = [self.reduce_single(arg) for arg in args.arguments] reduced_kw = {} for key in args.kwargs.keys(): <mask>: raise InvalidArguments(""Keyword argument name is not a string."") a = args.kwargs[key] reduced_kw[key] = self.reduce_single(a) return (reduced_pos, reduced_kw)",True,"if not isinstance ( key , str ) :","if not isinstance ( key , str ) :",0.8820016898747209,100.00000000000004
"def _encode(n, nbytes, little_endian=False): retval = [] n = long(n) for i in range(nbytes): <mask>: retval.append(chr(n & 0xFF)) else: retval.insert(0, chr(n & 0xFF)) n >>= 8 return """".join(retval)",True,if little_endian :,if little_endian :,0.8820016898747209,100.00000000000004
"def copy_shell(self): cls = self.__class__ old_id = cls.id new_i = cls() # create a new group new_i.id = self.id # with the same id cls.id = old_id # Reset the Class counter # Copy all properties for prop in cls.properties: if prop is not ""members"": <mask>: val = getattr(self, prop) setattr(new_i, prop, val) # but no members new_i.members = [] return new_i",False,if self . has ( prop ) :,"if hasattr ( self , prop ) :",0.8820016898747209,24.446151121745064
"def dataspec(config): master = yield fakemaster.make_master() data = connector.DataConnector() data.setServiceParent(master) if config[""out""] != ""--"": dirs = os.path.dirname(config[""out""]) <mask>: os.makedirs(dirs) f = open(config[""out""], ""w"") else: f = sys.stdout if config[""global""] is not None: f.write(""window."" + config[""global""] + ""="") f.write(json.dumps(data.allEndpoints(), indent=2)) f.close() defer.returnValue(0)",False,if dirs and not os . path . exists ( dirs ) :,if not os . path . exists ( dirs ) :,0.8820016898747209,76.26264731696685
"def _parseSCDOCDC(self, src): """"""[S|CDO|CDC]*"""""" while 1: src = src.lstrip() <mask>: src = src[4:] elif src.startswith(""-->""): src = src[3:] else: break return src",True,"if src . startswith ( ""<!--"" ) :","if src . startswith ( ""<!--"" ) :",0.8820016898747209,100.00000000000004
"def command(filenames, dirnames, fix): for filename in gather_files(dirnames, filenames): visitor = process_file(filename) if visitor.needs_fix(): print(""%s: %s"" % (filename, visitor.get_stats())) <mask>: print(""Fixing: %s"" % filename) fix_file(filename)",True,if fix :,if fix :,0.8820016898747209,0.0
"def shutdown(self): """"""Shutdown host system."""""" self._check_dbus(MANAGER) use_logind = self.sys_dbus.logind.is_connected _LOGGER.info(""Initialize host power off %s"", ""logind"" if use_logind else ""systemd"") try: await self.sys_core.shutdown() finally: <mask>: await self.sys_dbus.logind.power_off() else: await self.sys_dbus.systemd.power_off()",True,if use_logind :,if use_logind :,0.8820016898747209,100.00000000000004
"def _run_split_on_punc(self, text, never_split=None): """"""Splits punctuation on a piece of text."""""" if never_split is not None and text in never_split: return [text] chars = list(text) i = 0 start_new_word = True output = [] while i < len(chars): char = chars[i] if _is_punctuation(char): output.append([char]) start_new_word = True else: <mask>: output.append([]) start_new_word = False output[-1].append(char) i += 1 return ["""".join(x) for x in output]",True,if start_new_word :,if start_new_word :,0.8820016898747209,100.00000000000004
"def _terminal_messenger(tp=""write"", msg="""", out=sys.stdout): try: if tp == ""write"": out.write(msg) <mask>: out.flush() elif tp == ""write_flush"": out.write(msg) out.flush() elif tp == ""print"": print(msg, file=out) else: raise ValueError(""Unsupported type: "" + tp) except IOError as e: logger.critical(""{}: {}"".format(type(e).__name__, ucd(e))) pass",True,"elif tp == ""flush"" :","elif tp == ""flush"" :",0.8820016898747209,100.00000000000004
"def checkClassDeclation(file): localResult = [] with open(file, ""rb"") as f: lineNumber = 0 for line in f: m = re.search(""class\s+[^\(]*:"", line) <mask>: localResult.append( ""Old class definition found on {0}"".format(m.group()) ) return localResult",True,if m :,if m :,0.8820016898747209,0.0
"def _evaluate_local_single(self, iterator): for batch in iterator: in_arrays = convert._call_converter(self.converter, batch, self.device) with function.no_backprop_mode(): if isinstance(in_arrays, tuple): results = self.calc_local(*in_arrays) <mask>: results = self.calc_local(**in_arrays) else: results = self.calc_local(in_arrays) if self._progress_hook: self._progress_hook(batch) yield results",False,"elif isinstance ( in_arrays , dict ) :","elif isinstance ( in_arrays , list ) :",0.8820016898747209,70.71067811865478
"def check_billing_view(user, permission, obj): if hasattr(obj, ""all_projects""): <mask>: return True # This is a billing object return any(check_permission(user, permission, prj) for prj in obj.all_projects) return check_permission(user, permission, obj)",False,if user . is_superuser or obj . owners . filter ( pk = user . pk ) . exists ( ) :,if obj . all_projects [ 0 ] . is_billing :,0.8820016898747209,7.07824597648233
"def ensure_output_spaces_contain_the_same_data(self, y, y_ensured): stride = y.shape[1] self.assertEqual(y.shape[0] * y.shape[1], y_ensured.shape[0]) self.assertEqual(len(y_ensured.shape), 1) for row in range(y.shape[0]): for column in range(y.shape[1]): <mask>: self.assertEqual(y[row, column], y_ensured[row * stride + column]) else: self.assertEqual(y[row][column], y_ensured[row * stride + column])",False,if sp . issparse ( y ) :,if row == 0 :,0.8820016898747209,6.916271812933183
"def train( self, training_data: TrainingData, config: Optional[RasaNLUModelConfig] = None, **kwargs: Any, ) -> None: """"""Tokenize all training data."""""" for example in training_data.training_examples: for attribute in MESSAGE_ATTRIBUTES: if example.get(attribute) is not None and not example.get(attribute) == """": <mask>: tokens = self._split_name(example, attribute) else: tokens = self.tokenize(example, attribute) example.set(TOKENS_NAMES[attribute], tokens)",False,"if attribute in [ INTENT , ACTION_NAME , INTENT_RESPONSE_KEY ] :",if attribute in TOKENS_NAMES :,0.8820016898747209,6.656592803413299
"def refresh_token(self, strategy, *args, **kwargs): token = self.extra_data.get(""refresh_token"") or self.extra_data.get(""access_token"") backend = self.get_backend(strategy) if token and backend and hasattr(backend, ""refresh_token""): backend = backend(strategy=strategy) response = backend.refresh_token(token, *args, **kwargs) extra_data = backend.extra_data(self, self.uid, response, self.extra_data) <mask>: self.save()",False,if self . set_extra_data ( extra_data ) :,if extra_data :,0.8820016898747209,7.468220329575271
"def _verify_environ(_collected_environ): try: yield finally: new_environ = dict(os.environ) current_test = new_environ.pop(""PYTEST_CURRENT_TEST"", None) old_environ = dict(_collected_environ) old_environ.pop(""PYTEST_CURRENT_TEST"", None) <mask>: raise DirtyTest( ""Left over environment variables"", current_test, _compare_eq_dict(new_environ, old_environ, verbose=2), )",False,if new_environ != old_environ :,if current_test != old_environ :,0.8820016898747209,55.55238068023578
"def clean_len(self, line): """"""Calculate wisible length of string"""""" if isinstance(line, basestring): return len(self.screen.markup.clean_markup(line)) elif isinstance(line, tuple) or isinstance(line, list): markups = self.screen.markup.get_markup_vars() length = 0 for i in line: <mask>: length += len(i) return length",True,if i not in markups :,if i not in markups :,0.8820016898747209,100.00000000000004
"def _build_merged_dataset_args(datasets): merged_dataset_args = [] for dataset in datasets: dataset_code_column = _parse_dataset_code(dataset) arg = dataset_code_column[""code""] column_index = dataset_code_column[""column_index""] <mask>: arg = (dataset_code_column[""code""], {""column_index"": [column_index]}) merged_dataset_args.append(arg) return merged_dataset_args",False,if column_index is not None :,if column_index :,0.8820016898747209,38.80684294761701
"def update_watch_data_table_paths(self): if hasattr(self.tool_data_watcher, ""monitored_dirs""): for tool_data_table_path in self.tool_data_paths: <mask>: self.tool_data_watcher.watch_directory(tool_data_table_path)",False,if tool_data_table_path not in self . tool_data_watcher . monitored_dirs :,"if tool_data_table_path . endswith ( "".py"" ) :",0.8820016898747209,33.41737053054215
"def getsource(obj): """"""Wrapper around inspect.getsource"""""" try: try: src = encoding.to_unicode(inspect.getsource(obj)) except TypeError: <mask>: src = encoding.to_unicode(inspect.getsource(obj.__class__)) else: # Bindings like VTK or ITK require this case src = getdoc(obj) return src except (TypeError, IOError): return",False,"if hasattr ( obj , ""__class__"" ) :",if inspect . isclass ( obj ) :,0.8820016898747209,7.80152171018653
"def __iter__(self): for model in self.app_config.get_models(): admin_model = AdminModel(model, **self.options) for model_re in self.model_res: if model_re.search(admin_model.name): break else: <mask>: continue yield admin_model",False,if self . model_res :,if not admin_model . is_active ( ) :,0.8820016898747209,5.063996506781411
"def run(self): while True: try: with DelayedKeyboardInterrupt(): raw_inputs = self._parent_task_queue.get() if self._has_stop_signal(raw_inputs): self._rq.put(raw_inputs, block=True) break if self._flow_type == BATCH: self._rq.put(raw_inputs, block=True) <mask>: try: self._rq.put(raw_inputs, block=False) except: pass except KeyboardInterrupt: continue",False,elif self . _flow_type == REALTIME :,if self . _flow_type == RESTART :,0.8820016898747209,69.89307622784945
"def dump(self): self.ql.log.info(""[*] Dumping object: %s"" % (self.sf_name)) for field in self._fields_: <mask>: self.ql.log.info(""%s: 0x%x"" % (field[0], getattr(self, field[0]).value)) elif isinstance(getattr(self, field[0]), int): self.ql.log.info(""%s: %d"" % (field[0], getattr(self, field[0]))) elif isinstance(getattr(self, field[0]), bytes): self.ql.log.info(""%s: %s"" % (field[0], getattr(self, field[0]).decode()))",False,"if isinstance ( getattr ( self , field [ 0 ] ) , POINTER64 ) :","if isinstance ( getattr ( self , field [ 0 ] ) , int ) :",0.8820016898747209,83.7117009877792
"def validate_configuration(self, configuration: Optional[ExpectationConfiguration]): """"""Validating that user has inputted a value set and that configuration has been initialized"""""" super().validate_configuration(configuration) try: assert ""value_set"" in configuration.kwargs, ""value_set is required"" assert isinstance( configuration.kwargs[""value_set""], (list, set, dict) ), ""value_set must be a list or a set"" <mask>: assert ( ""$PARAMETER"" in configuration.kwargs[""value_set""] ), 'Evaluation Parameter dict for value_set kwarg must have ""$PARAMETER"" key' except AssertionError as e: raise InvalidExpectationConfigurationError(str(e)) return True",False,"if isinstance ( configuration . kwargs [ ""value_set"" ] , dict ) :","if ""value_set"" in configuration . kwargs :",0.8820016898747209,28.75683693213116
def test_one_dead_branch(): with deterministic_PRNG(): seen = set() @run_to_buffer def x(data): i = data.draw_bytes(1)[0] if i > 0: data.mark_invalid() i = data.draw_bytes(1)[0] if len(seen) < 255: seen.add(i) <mask>: data.mark_interesting(),False,elif i not in seen :,if seen :,0.8820016898747209,0.0
"def __on_item_activated(self, event): if self.__module_view: module = self.get_event_module(event) self.__module_view.set_selection(module.module_num) if event.EventObject is self.list_ctrl: self.input_list_ctrl.deactivate_active_item() else: self.list_ctrl.deactivate_active_item() for index in range(self.list_ctrl.GetItemCount()): <mask>: self.list_ctrl.Select(index, False) self.__controller.enable_module_controls_panel_buttons()",False,if self . list_ctrl . IsSelected ( index ) :,if self . list_ctrl . GetItem ( index ) == self . __module_view . GetSelectedItem ( index ) :,0.8820016898747209,31.348206659605648
"def prime(self, callback): <mask>: # import pdb # pdb.set_trace() self.cbhdl = simulator.register_rwsynch_callback(callback, self) if self.cbhdl is None: raise_error(self, ""Unable set up %s Trigger"" % (str(self))) Trigger.prime(self)",True,if self . cbhdl is None :,if self . cbhdl is None :,0.8820016898747209,100.00000000000004
"def fstab_configuration(middleware): for command in ( [ [""systemctl"", ""daemon-reload""], [""systemctl"", ""restart"", ""local-fs.target""], ] if osc.IS_LINUX else [[""mount"", ""-uw"", ""/""]] ): ret = subprocess.run(command, capture_output=True) <mask>: middleware.logger.debug( f'Failed to execute ""{"" "".join(command)}"": {ret.stderr.decode()}' )",False,if ret . returncode :,if ret . returncode != 0 :,0.8820016898747209,36.55552228545123
"def _generate_table(self, fromdesc, todesc, diffs): if fromdesc or todesc: yield ( simple_colorize(fromdesc, ""description""), simple_colorize(todesc, ""description""), ) for i, line in enumerate(diffs): <mask>: # mdiff yields None on separator lines; skip the bogus ones # generated for the first line if i > 0: yield ( simple_colorize(""---"", ""separator""), simple_colorize(""---"", ""separator""), ) else: yield line",False,if line is None :,"if line . strip ( ) == """" :",0.8820016898747209,8.29519350710986
"def update_completion(self): """"""Update completion model with exist tags"""""" orig_text = self.widget.text() text = "", "".join(orig_text.replace("", "", "","").split("","")[:-1]) tags = [] for tag in self.tags_list: if "","" in orig_text: <mask>: tags.append(""%s,%s"" % (text, tag)) tags.append(""%s, %s"" % (text, tag)) else: tags.append(tag) if tags != self.completer_model.stringList(): self.completer_model.setStringList(tags)",False,"if orig_text [ - 1 ] not in ( "","" , "" "" ) :",if tag in self . completer_model . stringList ( ) :,0.8820016898747209,5.124982513794918
"def cart_number_checksum_validation(cls, number): digits = [] even = False if not number.isdigit(): return False for digit in reversed(number): digit = ord(digit) - ord(""0"") <mask>: digit *= 2 if digit >= 10: digit = digit % 10 + digit // 10 digits.append(digit) even = not even return sum(digits) % 10 == 0 if digits else False",True,if even :,if even :,0.8820016898747209,0.0
"def __get_param_string__(params): params_string = [] for key in sorted(params.keys()): <mask>: return value = params[key] params_string.append("""" if value == ""null"" else str(value)) return ""|"".join(params_string)",False,"if ""REFUND"" in params [ key ] or ""|"" in params [ key ] :","if key == ""null"" :",0.8820016898747209,2.0879268276081806
"def _map_handlers(self, session, event_class, mapfn): for event in DOC_EVENTS: event_handler_name = event.replace(""-"", ""_"") <mask>: event_handler = getattr(self, event_handler_name) format_string = DOC_EVENTS[event] num_args = len(format_string.split(""."")) - 2 format_args = (event_class,) + (""*"",) * num_args event_string = event + format_string % format_args unique_id = event_class + event_handler_name mapfn(event_string, event_handler, unique_id)",False,"if hasattr ( self , event_handler_name ) :",if event_handler_name in session . _event_handlers :,0.8820016898747209,28.917849332325716
"def _create_param_lr(self, param_and_grad): # create learning rate variable for every parameter param = param_and_grad[0] param_lr = param.optimize_attr[""learning_rate""] if type(param_lr) == Variable: return param_lr else: <mask>: return self._global_learning_rate() else: with default_main_program()._lr_schedule_guard( is_with_opt=True ), framework.name_scope(""scale_with_param_lr""): return self._global_learning_rate() * param_lr",False,if param_lr == 1.0 :,if type ( param_lr ) == Variable :,0.8820016898747209,19.081654556856684
"def __getitem__(self, key): try: return self._clsmap[key] except KeyError as e: <mask>: self._mutex.acquire() try: if not self.initialized: self._init() self.initialized = True return self._clsmap[key] finally: self._mutex.release() raise e",False,if not self . initialized :,if self . _mutex is not None :,0.8820016898747209,11.99014838091355
"def save(self, force=False): if not force: <mask>: return if time.time() - self.last_save_time < 10: return with self.lock: with open(self.file_path, ""w"") as fd: for ip in self.cache: record = self.cache[ip] rule = record[""r""] connect_time = record[""c""] update_time = record[""update""] fd.write(""%s %s %d %d\n"" % (ip, rule, connect_time, update_time)) self.last_save_time = time.time() self.need_save = False",False,if not self . need_save :,if self . need_save :,0.8820016898747209,72.89545183625967
"def pick(items, sel): for x, s in zip(items, sel): <mask>: yield x elif not x.is_atom() and not s.is_atom(): yield x.restructure(x.head, pick(x.leaves, s.leaves), evaluation)",False,if match ( s ) :,if x . is_atom ( ) and s . is_atom ( ) :,0.8820016898747209,5.751391809950023
"def isValidFloat(config_param_name, value, constraints): if isinstance(value, float): constraints.setdefault(""min"", MIN_VALID_FLOAT_VALUE) constraints.setdefault(""max"", MAX_VALID_FLOAT_VALUE) minv = float(constraints.get(""min"")) maxv = float(constraints.get(""max"")) <mask>: if value <= maxv: return value raise FloatValueError(config_param_name, value, constraints)",False,if value >= minv :,if minv <= value <= maxv :,0.8820016898747209,7.129384882260374
"def get_files(d): f = [] for root, dirs, files in os.walk(d): for name in files: if ""meta-environment"" in root or ""cross-canadian"" in root: continue <mask>: continue if ""do_build"" not in name and ""do_populate_sdk"" not in name: f.append(os.path.join(root, name)) return f",False,"if ""qemux86copy-"" in root or ""qemux86-"" in root :","if ""build"" in root and ""build_sdk"" in root :",0.8820016898747209,27.22589423069701
"def __get_photo(self, person_or_marriage): """"""returns the first photo in the media list or None"""""" media_list = person_or_marriage.get_media_list() for media_ref in media_list: media_handle = media_ref.get_reference_handle() media = self.database.get_media_from_handle(media_handle) mime_type = media.get_mime_type() <mask>: return media return None",False,"if mime_type and mime_type . startswith ( ""image"" ) :","if mime_type == ""photo"" :",0.8820016898747209,16.581659750776073
"def filter(this, args): array = to_object(this, args.space) callbackfn = get_arg(args, 0) arr_len = js_arr_length(array) if not is_callable(callbackfn): raise MakeError(""TypeError"", ""callbackfn must be a function"") _this = get_arg(args, 1) k = 0 res = [] while k < arr_len: if array.has_property(unicode(k)): kValue = array.get(unicode(k)) <mask>: res.append(kValue) k += 1 return args.space.ConstructArray(res)",False,"if to_boolean ( callbackfn . call ( _this , ( kValue , float ( k ) , array ) ) ) :","if kValue is not None and callbackfn ( kValue , _this ) :",0.8820016898747209,7.540938828497274
"def optimize(self, graph: Graph): for v in graph.inputs: if not v.has_attribute(SplitTarget): continue <mask>: DumpGraph().optimize(graph) raise NotImplementedError( f""Input Variable {v} is too large to handle in WebGL backend"" ) return graph, False",False,if flags . DEBUG :,"if v . get_attribute ( ""size"" ) > self . _max_size :",0.8820016898747209,2.6643211213888947
"def detach_volume(self, volume): # We need to find the node using this volume for node in self.list_nodes(): if type(node.image) is not list: # This node has only one associated image. It is not the one we # are after. continue for disk in node.image: <mask>: # Node found. We can now detach the volume disk_id = disk.extra[""disk_id""] return self._do_detach_volume(node.id, disk_id) return False",False,if disk . id == volume . id :,"if disk . extra [ ""volume"" ] == volume :",0.8820016898747209,20.098339913206324
"def Yield(value, level=1): g = greenlet.getcurrent() while level != 0: if not isinstance(g, genlet): raise RuntimeError(""yield outside a genlet"") <mask>: g.parent.set_child(g) g = g.parent level -= 1 g.switch(value)",False,if level > 1 :,if g . parent :,0.8820016898747209,12.703318703865365
"def get_all_pipeline_nodes( pipeline: pipeline_pb2.Pipeline, ) -> List[pipeline_pb2.PipelineNode]: """"""Returns all pipeline nodes in the given pipeline."""""" result = [] for pipeline_or_node in pipeline.nodes: which = pipeline_or_node.WhichOneof(""node"") # TODO(goutham): Handle sub-pipelines. # TODO(goutham): Handle system nodes. <mask>: result.append(pipeline_or_node.pipeline_node) else: raise NotImplementedError(""Only pipeline nodes supported."") return result",False,"if which == ""pipeline_node"" :","if which in ( ""pipeline"" , ""system"" ) :",0.8820016898747209,9.669265690880861
"def __init__(self, **settings): default_settings = self.get_default_settings() for name, value in default_settings.items(): if not hasattr(self, name): setattr(self, name, value) for name, value in settings.items(): <mask>: raise ImproperlyConfigured( ""Invalid setting '{}' for {}"".format( name, self.__class__.__name__, ) ) setattr(self, name, value)",False,if name not in default_settings :,"if not hasattr ( self , name ) :",0.8820016898747209,6.742555929751843
"def _check_choice(self): if self.type == ""choice"": <mask>: raise OptionError(""must supply a list of choices for type 'choice'"", self) elif type(self.choices) not in (types.TupleType, types.ListType): raise OptionError( ""choices must be a list of strings ('%s' supplied)"" % str(type(self.choices)).split(""'"")[1], self, ) elif self.choices is not None: raise OptionError(""must not supply choices for type %r"" % self.type, self)",False,if self . choices is None :,"if not isinstance ( self . choices , list ) :",0.8820016898747209,15.851165692617148
"def prepare(self, size=None): if _is_seekable(self.file): start_pos = self.file.tell() self.file.seek(0, 2) end_pos = self.file.tell() self.file.seek(start_pos) fsize = end_pos - start_pos <mask>: self.remain = fsize else: self.remain = min(fsize, size) return self.remain",True,if size is None :,if size is None :,0.8820016898747209,100.00000000000004
"def _setSitemapTargets(): if not conf.sitemapUrl: return infoMsg = ""parsing sitemap '%s'"" % conf.sitemapUrl logger.info(infoMsg) found = False for item in parseSitemap(conf.sitemapUrl): <mask>: found = True kb.targets.add((item.strip(), None, None, None, None)) if not found and not conf.forms and not conf.crawlDepth: warnMsg = ""no usable links found (with GET parameters)"" logger.warn(warnMsg)",False,"if re . match ( r""[^ ]+\?(.+)"" , item , re . I ) :","if item . strip ( ) == ""links"" :",0.8820016898747209,1.7735625000229587
"def test_CY_decomposition(self, tol): """"""Tests that the decomposition of the CY gate is correct"""""" op = qml.CY(wires=[0, 1]) res = op.decomposition(op.wires) mats = [] for i in reversed(res): <mask>: mats.append(np.kron(i.matrix, np.eye(2))) else: mats.append(i.matrix) decomposed_matrix = np.linalg.multi_dot(mats) assert np.allclose(decomposed_matrix, op.matrix, atol=tol, rtol=0)",False,if len ( i . wires ) == 1 :,if i . is_kron :,0.8820016898747209,8.820727472213227
"def _line_ranges(statements, lines): """"""Produce a list of ranges for `format_lines`."""""" statements = sorted(statements) lines = sorted(lines) pairs = [] start = None lidx = 0 for stmt in statements: if lidx >= len(lines): break <mask>: lidx += 1 if not start: start = stmt end = stmt elif start: pairs.append((start, end)) start = None if start: pairs.append((start, end)) return pairs",False,if stmt == lines [ lidx ] :,if stmt in lines :,0.8820016898747209,11.415938068117505
"def init_params(net): """"""Init layer parameters."""""" for module in net.modules(): if isinstance(module, nn.Conv2d): init.kaiming_normal(module.weight, mode=""fan_out"") <mask>: init.constant(module.bias, 0) elif isinstance(module, nn.BatchNorm2d): init.constant(module.weight, 1) init.constant(module.bias, 0) elif isinstance(module, nn.Linear): init.normal(module.weight, std=1e-3) if module.bias: init.constant(module.bias, 0)",False,if module . bias :,"elif isinstance ( module , nn . Conv2d ) :",0.8820016898747209,5.522397783539471
"def _get_directory_size_in_bytes(directory): total = 0 try: for entry in os.scandir(directory): <mask>: # if it's a file, use stat() function total += entry.stat().st_size elif entry.is_dir(): # if it's a directory, recursively call this function total += _get_directory_size_in_bytes(entry.path) except NotADirectoryError: # if `directory` isn't a directory, get the file size then return os.path.getsize(directory) except PermissionError: # if for whatever reason we can't open the folder, return 0 return 0 return total",True,if entry . is_file ( ) :,if entry . is_file ( ) :,0.8820016898747209,100.00000000000004
"def run_cmd(self, util, to, always_push_mark=False): if to == ""bof"": util.push_mark_and_goto_position(0) elif to == ""eof"": util.push_mark_and_goto_position(self.view.size()) elif to in (""eow"", ""bow""): visible = self.view.visible_region() pos = visible.a if to == ""bow"" else visible.b <mask>: util.push_mark_and_goto_position(pos) else: util.set_cursors([sublime.Region(pos)])",True,if always_push_mark :,if always_push_mark :,0.8820016898747209,100.00000000000004
"def parse_results(cwd): optimal_dd = None optimal_measure = numpy.inf for tup in tools.find_conf_files(cwd): dd = tup[1] if ""results.train_y_misclass"" in dd: if dd[""results.train_y_misclass""] < optimal_measure: optimal_measure = dd[""results.train_y_misclass""] optimal_dd = dd print(""Optimal results.train_y_misclass:"", str(optimal_measure)) for key, value in optimal_dd.items(): <mask>: print(key + "": "" + str(value))",False,"if ""hyper_parameters"" in key :","if key != ""results"" :",0.8820016898747209,7.287580698437859
"def clean_vc_position(self): vc_position = self.cleaned_data[""vc_position""] if self.validate_vc_position: conflicting_members = Device.objects.filter( virtual_chassis=self.instance.virtual_chassis, vc_position=vc_position ) <mask>: raise forms.ValidationError( ""A virtual chassis member already exists in position {}."".format( vc_position ) ) return vc_position",False,if conflicting_members . exists ( ) :,if conflicting_members :,0.8820016898747209,31.772355751081438
"def cal_pads(auto_pad, pad_shape): spatial_size = len(pad_shape) pads = [0] * spatial_size * 2 for i in range(spatial_size): if auto_pad == ""SAME_LOWER"": pads[i + spatial_size] = pad_shape[i] // 2 pads[i] = pad_shape[i] - pads[i + spatial_size] <mask>: pads[i] = pad_shape[i] // 2 pads[i + spatial_size] = pad_shape[i] - pads[i] return pads",True,"elif auto_pad == ""SAME_UPPER"" :","elif auto_pad == ""SAME_UPPER"" :",0.8820016898747209,100.00000000000004
"def TryMerge(self, d): while d.avail() > 0: tt = d.getVarInt32() <mask>: length = d.getVarInt32() tmp = ProtocolBuffer.Decoder(d.buffer(), d.pos(), d.pos() + length) d.skip(length) self.add_presence_response().TryMerge(tmp) continue if tt == 0: raise ProtocolBuffer.ProtocolBufferDecodeError d.skipData(tt)",False,if tt == 10 :,if tt == 1 :,0.8820016898747209,53.7284965911771
"def test_cwl_rnaseq(self, install_test_files): with install_cwl_test_files() as work_dir: with utils.chdir(os.path.join(work_dir, ""rnaseq"")): <mask>: shutil.rmtree(""cromwell_work"") subprocess.check_call( [""bcbio_vm.py"", ""cwlrun"", ""cromwell"", ""rnaseq-workflow""] )",True,"if os . path . exists ( ""cromwell_work"" ) :","if os . path . exists ( ""cromwell_work"" ) :",0.8820016898747209,100.00000000000004
"def files_per_version(self): xpath = ""./files/file"" files = self.root.findall(xpath) versions = {} for file in files: vfile = file.findall(""version"") for version in vfile: nb = version.attrib[""nb""] <mask>: versions[nb] = [] versions[nb].append(file.attrib[""url""]) return versions",False,if not nb in versions :,if nb not in versions :,0.8820016898747209,35.930411196308434
"def value_to_db_datetime(self, value): if value is None: return None # SQLite doesn't support tz-aware datetimes if timezone.is_aware(value): <mask>: value = value.astimezone(timezone.utc).replace(tzinfo=None) else: raise ValueError( ""SQLite backend does not support timezone-aware datetimes when USE_TZ is False."" ) return six.text_type(value)",False,if settings . USE_TZ :,if USE_TZ :,0.8820016898747209,47.39878501170795
"def _toplevelTryFunc(func, *args, status=status, **kwargs): with ThreadProfiler(threading.current_thread()) as prof: t = threading.current_thread() t.name = func.__name__ try: t.status = func(*args, **kwargs) except EscapeException as e: # user aborted t.status = ""aborted by user"" <mask>: status(""%s aborted"" % t.name, priority=2) except Exception as e: t.exception = e t.status = ""exception"" vd.exceptionCaught(e) if t.sheet: t.sheet.currentThreads.remove(t)",True,if status :,if status :,0.8820016898747209,0.0
"def ESP(phrase): for num, name in enumerate(devname): if name.lower() in phrase: dev = devid[num] <mask>: ctrl = ""=ON"" say(""Turning On "" + name) elif custom_action_keyword[""Dict""][""Off""] in phrase: ctrl = ""=OFF"" say(""Turning Off "" + name) rq = requests.head(""https://"" + ip + dev + ctrl, verify=False)",True,"if custom_action_keyword [ ""Dict"" ] [ ""On"" ] in phrase :","if custom_action_keyword [ ""Dict"" ] [ ""On"" ] in phrase :",0.8820016898747209,100.00000000000004
"def _table_schema(self, table): rows = self.db.execute_sql(""PRAGMA table_info('%s')"" % table).fetchall() # Build list of fields from table information result = {} for _, name, data_type, not_null, _, primary_key in rows: parts = [data_type] <mask>: parts.append(""PRIMARY KEY"") if not_null: parts.append(""NOT NULL"") result[name] = "" "".join(parts) return result",True,if primary_key :,if primary_key :,0.8820016898747209,100.00000000000004
"def _validate_forward_input(x, n_in): if n_in != 1: if not isinstance(x, (tuple, list)): raise TypeError( f""Expected input to be a tuple or list; instead got {type(x)}."" ) <mask>: raise ValueError( f""Input tuple length ({len(x)}) does not equal required "" f""number of inputs ({n_in})."" )",True,if len ( x ) != n_in :,if len ( x ) != n_in :,0.8820016898747209,100.00000000000004
"def _table_reprfunc(self, row, col, val): if self._table.column_names[col].endswith(""Size""): if isinstance(val, compat.string_types): return "" %s"" % val <mask>: return "" %.1f KB"" % (val / 1024.0 ** 1) elif val < 1024 ** 3: return "" %.1f MB"" % (val / 1024.0 ** 2) else: return "" %.1f GB"" % (val / 1024.0 ** 3) if col in (0, """"): return str(val) else: return "" %s"" % val",False,elif val < 1024 ** 2 :,elif val < 1024 ** 1 :,0.8820016898747209,70.71067811865478
"def get_path_name(self): if self.is_root(): return ""@"" + self.name else: parent_name = self.parent.get_path_name() <mask>: return ""/"".join([parent_name, ""@"" + self.name]) else: return ""@"" + self.name",True,if parent_name :,if parent_name :,0.8820016898747209,100.00000000000004
"def parse(cls, api, json): lst = List(api) setattr(lst, ""_json"", json) for k, v in json.items(): <mask>: setattr(lst, k, User.parse(api, v)) elif k == ""created_at"": setattr(lst, k, parse_datetime(v)) else: setattr(lst, k, v) return lst",True,"if k == ""user"" :","if k == ""user"" :",0.8820016898747209,100.00000000000004
"def _bytecode_filenames(self, py_filenames): bytecode_files = [] for py_file in py_filenames: if not py_file.endswith("".py""): continue <mask>: bytecode_files.append(py_file + ""c"") if self.optimize > 0: bytecode_files.append(py_file + ""o"") return bytecode_files",False,if self . compile :,if self . optimize > 0 :,0.8820016898747209,26.269098944241588
"def to_json_dict(self): d = super().to_json_dict() d[""bullet_list""] = RenderedContent.rendered_content_list_to_json(self.bullet_list) if self.header is not None: <mask>: d[""header""] = self.header.to_json_dict() else: d[""header""] = self.header if self.subheader is not None: if isinstance(self.subheader, RenderedContent): d[""subheader""] = self.subheader.to_json_dict() else: d[""subheader""] = self.subheader return d",True,"if isinstance ( self . header , RenderedContent ) :","if isinstance ( self . header , RenderedContent ) :",0.8820016898747209,100.00000000000004
"def makeSomeFiles(pathobj, dirdict): pathdict = {} for (key, value) in dirdict.items(): child = pathobj.child(key) if isinstance(value, bytes): pathdict[key] = child child.setContent(value) <mask>: child.createDirectory() pathdict[key] = makeSomeFiles(child, value) else: raise ValueError(""only strings and dicts allowed as values"") return pathdict",True,"elif isinstance ( value , dict ) :","elif isinstance ( value , dict ) :",0.8820016898747209,100.00000000000004
"def Restore(self): picker, obj = self._window, self._pObject value = obj.RestoreValue(PERSIST_FILEDIRPICKER_PATH) if value is not None: <mask>: if type(value) == list: value = value[-1] picker.SetPath(value) return True return False",False,"if issubclass ( picker . __class__ , wx . FileDialog ) :","if value . endswith ( "".py"" ) :",0.8820016898747209,6.261486344344754
"def recv(self, buffer_size): try: return super(SSLConnection, self).recv(buffer_size) except ssl.SSLError as err: <mask>: return b"""" if err.args[0] in (ssl.SSL_ERROR_EOF, ssl.SSL_ERROR_ZERO_RETURN): self.handle_close() return b"""" raise",False,"if err . args [ 0 ] in ( ssl . SSL_ERROR_WANT_READ , ssl . SSL_ERROR_WANT_WRITE ) :",if err . args [ 0 ] == ssl . SSL_ERROR_NO_DATA :,0.8820016898747209,32.40587283786134
"def IncrementErrorCount(self, category): """"""Bumps the module's error statistic."""""" self.error_count += 1 if self.counting in (""toplevel"", ""detailed""): <mask>: category = category.split(""/"")[0] if category not in self.errors_by_category: self.errors_by_category[category] = 0 self.errors_by_category[category] += 1",False,"if self . counting != ""detailed"" :","if category . startswith ( ""/"" ) :",0.8820016898747209,6.27465531099474
"def _get_y(self, data_inst): if self.stratified: y = [v for i, v in data_inst.mapValues(lambda v: v.label).collect()] <mask>: y = self.transform_regression_label(data_inst) else: # make dummy y y = [0] * (data_inst.count()) return y",False,if self . need_transform :,if self . regression :,0.8820016898747209,28.641904579795423
"def test_all_project_files(self): if sys.platform.startswith(""win""): # XXX something with newlines goes wrong on Windows. return for filepath in support.all_project_files(): with open(filepath, ""rb"") as fp: encoding = tokenize.detect_encoding(fp.readline)[0] self.assertIsNotNone(encoding, ""can't detect encoding for %s"" % filepath) with open(filepath, ""r"") as fp: source = fp.read() source = source.decode(encoding) tree = driver.parse_string(source) new = unicode(tree) <mask>: self.fail(""Idempotency failed: %s"" % filepath)",False,"if diff ( filepath , new , encoding ) :","if new != ""utf-8"" :",0.8820016898747209,5.660233915657916
"def test_resource_arn_override_generator(self): overrides = set() for k, v in manager.resources.items(): arn_gen = bool(v.__dict__.get(""get_arns"") or v.__dict__.get(""generate_arn"")) <mask>: overrides.add(k) overrides = overrides.difference( { ""account"", ""s3"", ""hostedzone"", ""log-group"", ""rest-api"", ""redshift-snapshot"", ""rest-stage"", } ) if overrides: raise ValueError(""unknown arn overrides in %s"" % ("", "".join(overrides)))",True,if arn_gen :,if arn_gen :,0.8820016898747209,100.00000000000004
"def _check_dsl_runner(self) -> None: """"""Checks if runner in dsl is Kubeflow V2 runner."""""" with open(self.flags_dict[labels.PIPELINE_DSL_PATH], ""r"") as f: dsl_contents = f.read() <mask>: raise RuntimeError(""KubeflowV2DagRunner not found in dsl."")",False,"if ""KubeflowV2DagRunner"" not in dsl_contents :",if not dsl_contents :,0.8820016898747209,28.871566309219904
"def create_warehouse(warehouse_name, properties=None, company=None): if not company: company = ""_Test Company"" warehouse_id = erpnext.encode_company_abbr(warehouse_name, company) if not frappe.db.exists(""Warehouse"", warehouse_id): warehouse = frappe.new_doc(""Warehouse"") warehouse.warehouse_name = warehouse_name warehouse.parent_warehouse = ""All Warehouses - _TCUV"" warehouse.company = company warehouse.account = get_warehouse_account(warehouse_name, company) <mask>: warehouse.update(properties) warehouse.save() return warehouse.name else: return warehouse_id",True,if properties :,if properties :,0.8820016898747209,0.0
"def _parse(self, contents): entries = [] hostnames_found = set() for line in contents.splitlines(): if not len(line.strip()): entries.append((""blank"", [line])) continue (head, tail) = chop_comment(line.strip(), ""#"") <mask>: entries.append((""all_comment"", [line])) continue entries.append((""hostname"", [head, tail])) hostnames_found.add(head) if len(hostnames_found) > 1: raise IOError(""Multiple hostnames (%s) found!"" % (hostnames_found)) return entries",False,if not len ( head ) :,"if head == ""all_comment"" :",0.8820016898747209,5.522397783539471
"def _get_omega(self): if self._omega is None: n = self.get_drift_dim() // 2 omg = sympl.calc_omega(n) if self.oper_dtype == Qobj: self._omega = Qobj(omg, dims=self.dyn_dims) self._omega_qobj = self._omega <mask>: self._omega = sp.csr_matrix(omg) else: self._omega = omg return self._omega",True,elif self . oper_dtype == sp . csr_matrix :,elif self . oper_dtype == sp . csr_matrix :,0.8820016898747209,100.00000000000004
"def get_in_inputs(key, data): if isinstance(data, dict): for k, v in data.items(): if k == key: return v <mask>: out = get_in_inputs(key, v) if out: return out elif isinstance(data, (list, tuple)): out = [get_in_inputs(key, x) for x in data] out = [x for x in out if x] if out: return out[0]",False,"elif isinstance ( v , ( list , tuple , dict ) ) :","elif isinstance ( v , ( list , tuple ) ) :",0.8820016898747209,70.63486135430557
def visit_binary(binary): if binary.operator == operators.eq: cols = util.column_set(chain(*[c.proxy_set for c in columns.difference(omit)])) <mask>: for c in reversed(columns): if c.shares_lineage(binary.right) and ( not only_synonyms or c.name == binary.left.name ): omit.add(c) break,False,if binary . left in cols and binary . right in cols :,if not only_synonyms and not cols . issubset ( binary . left ) :,0.8820016898747209,11.633270842295033
"def wait_tasks_or_abort(futures, timeout=60, kill_switch_ev=None): try: LazySingletonTasksCoordinator.wait_tasks( futures, return_when=FIRST_EXCEPTION, raise_exceptions=True ) except Exception as e: <mask>: # Used when we want to keep both raise the exception and wait for all tasks to finish kill_switch_ev.set() LazySingletonTasksCoordinator.wait_tasks( futures, return_when=ALL_COMPLETED, raise_exceptions=False, timeout=timeout, ) raise e",True,if kill_switch_ev is not None :,if kill_switch_ev is not None :,0.8820016898747209,100.00000000000004
"def is_valid(sample): if sample is None: return False if isinstance(sample, tuple): for s in sample: <mask>: return False elif isinstance(s, np.ndarray) and s.size == 0: return False elif isinstance(s, collections.abc.Sequence) and len(s) == 0: return False return True",False,if s is None :,"if isinstance ( s , str ) and s . size == 0 :",0.8820016898747209,3.4585921141027356
"def setVaName(self, va, parent=None): if parent is None: parent = self curname = self.vw.getName(va) if curname is None: curname = """" name, ok = QInputDialog.getText(parent, ""Enter..."", ""Name"", text=curname) if ok: name = str(name) <mask>: raise Exception(""Duplicate Name: %s"" % name) self.vw.makeName(va, name)",False,if self . vw . vaByName ( name ) :,if name in self . vw . names :,0.8820016898747209,30.719343730842187
"def generic_tag_compiler(params, defaults, name, node_class, parser, token): ""Returns a template.Node subclass."" bits = token.split_contents()[1:] bmax = len(params) def_len = defaults and len(defaults) or 0 bmin = bmax - def_len if len(bits) < bmin or len(bits) > bmax: <mask>: message = ""%s takes %s arguments"" % (name, bmin) else: message = ""%s takes between %s and %s arguments"" % (name, bmin, bmax) raise TemplateSyntaxError(message) return node_class(bits)",False,if bmin == bmax :,if bmin < bmax :,0.8820016898747209,24.736929544091932
"def extract_segmentation_mask(annotation): poly_specs = annotation[DensePoseDataRelative.S_KEY] if isinstance(poly_specs, torch.Tensor): # data is already given as mask tensors, no need to decode return poly_specs import pycocotools.mask as mask_utils segm = torch.zeros((DensePoseDataRelative.MASK_SIZE,) * 2, dtype=torch.float32) for i in range(DensePoseDataRelative.N_BODY_PARTS): poly_i = poly_specs[i] <mask>: mask_i = mask_utils.decode(poly_i) segm[mask_i > 0] = i + 1 return segm",False,if poly_i :,"if isinstance ( poly_i , torch . Tensor ) :",0.8820016898747209,14.323145079400492
"def module_list(target, fast): """"""Find the list of modules to be compiled"""""" modules = [] native = native_modules(target) basedir = os.path.join(ouroboros_repo_folder(), ""ouroboros"") for name in os.listdir(basedir): module_name, ext = os.path.splitext(name) <mask>: if module_name not in IGNORE_MODULES and module_name not in native: if not (fast and module_name in KNOWN_PROBLEM_MODULES): modules.append(module_name) return set(modules)",False,"if ext == "".py"" or ext == """" and os . path . isdir ( os . path . join ( basedir , name ) ) :","if ext == "".py"" :",0.8820016898747209,6.178049532397588
"def filelist_from_patterns(pats, rootdir=None): if rootdir is None: rootdir = ""."" # filelist = [] fileset = set([]) lines = [line.strip() for line in pats] for line in lines: pat = line[2:] newfiles = glob(osp.join(rootdir, pat)) if line.startswith(""+""): fileset.update(newfiles) <mask>: fileset.difference_update(newfiles) else: raise ValueError(""line must start with + or -"") filelist = list(fileset) return filelist",True,"elif line . startswith ( ""-"" ) :","elif line . startswith ( ""-"" ) :",0.8820016898747209,100.00000000000004
"def get_upstream_statuses_events(self, upstream: Set) -> Dict[str, V1Statuses]: statuses_by_refs = {u: [] for u in upstream} events = self.events or [] # type: List[V1EventTrigger] for e in events: entity_ref = contexts_refs.get_entity_ref(e.ref) if not entity_ref: continue if entity_ref not in statuses_by_refs: continue for kind in e.kinds: status = V1EventKind.events_statuses_mapping.get(kind) <mask>: statuses_by_refs[entity_ref].append(status) return statuses_by_refs",True,if status :,if status :,0.8820016898747209,0.0
"def __setitem__(self, key, value): if isinstance(value, (tuple, list)): info, reference = value <mask>: self._reverse_infos[info] = len(self._infos) self._infos.append(info) if reference not in self._reverse_references: self._reverse_references[reference] = len(self._references) self._references.append(reference) self._trails[key] = ""%d,%d"" % ( self._reverse_infos[info], self._reverse_references[reference], ) else: raise Exception(""unsupported type '%s'"" % type(value))",True,if info not in self . _reverse_infos :,if info not in self . _reverse_infos :,0.8820016898747209,100.00000000000004
"def ChangeStyle(self, combos): style = 0 for combo in combos: <mask>: if combo.GetLabel() == ""TR_VIRTUAL"": style = style | HTL.TR_VIRTUAL else: try: style = style | eval(""wx."" + combo.GetLabel()) except: style = style | eval(""HTL."" + combo.GetLabel()) if self.GetAGWWindowStyleFlag() != style: self.SetAGWWindowStyleFlag(style)",False,if combo . GetValue ( ) == 1 :,"if combo . GetLabel ( ) != ""TR_VIRTUAL"" :",0.8820016898747209,14.458924666162856
"def _parse_csrf(self, response): for d in response: if d.startswith(""Set-Cookie:""): for c in d.split("":"", 1)[1].split("";""): <mask>: self._CSRFtoken = c.strip("" \r\n"") log.verbose(""Got new cookie: %s"", self._CSRFtoken) break if self._CSRFtoken != None: break",False,"if c . strip ( ) . startswith ( ""CSRF-Token-"" ) :","if c . startswith ( ""CSRF-Token"" ) :",0.8820016898747209,31.697477944241424
"def test_page_size_matching_max_returned_rows( app_client_returned_rows_matches_page_size, ): fetched = [] path = ""/fixtures/no_primary_key.json"" while path: response = app_client_returned_rows_matches_page_size.get(path) fetched.extend(response.json[""rows""]) assert len(response.json[""rows""]) in (1, 50) path = response.json[""next_url""] <mask>: path = path.replace(""http://localhost"", """") assert 201 == len(fetched)",False,if path :,"if path . startswith ( ""http://"" ) :",0.8820016898747209,6.837203339116283
"def get_mapping_exception_message(mappings: List[Tuple[Text, Text]]): """"""Return a message given a list of duplicates."""""" message = """" for name, action_name in mappings: <mask>: message += ""\n"" message += ( ""Intent '{}' is set to trigger action '{}', which is "" ""not defined in the domain."".format(name, action_name) ) return message",True,if message :,if message :,0.8820016898747209,0.0
def cut(sentence): sentence = strdecode(sentence) blocks = re_han.split(sentence) for blk in blocks: if re_han.match(blk): for word in __cut(blk): if word not in Force_Split_Words: yield word else: for c in word: yield c else: tmp = re_skip.split(blk) for x in tmp: <mask>: yield x,False,if x :,if x not in Force_Split_Words :,0.8820016898747209,9.287528999566801
"def chop(expr, delta=10.0 ** (-10.0)): if isinstance(expr, Real): if -delta < expr.get_float_value() < delta: return Integer(0) elif isinstance(expr, Complex) and expr.is_inexact(): real, imag = expr.real, expr.imag if -delta < real.get_float_value() < delta: real = Integer(0) <mask>: imag = Integer(0) return Complex(real, imag) elif isinstance(expr, Expression): return Expression(chop(expr.head), *[chop(leaf) for leaf in expr.leaves]) return expr",False,if - delta < imag . get_float_value ( ) < delta :,elif - delta < imag . get_float_value ( ) < delta :,0.8820016898747209,93.06048591020995
"def make_row(self): res = [] for i in range(self.num_cols): t = sqlite3_column_type(self.stmnt, i) # print(""type"", t) if t == SQLITE_INTEGER: res.append(sqlite3_column_int(self.stmnt, i)) elif t == SQLITE_FLOAT: res.append(sqlite3_column_double(self.stmnt, i)) <mask>: res.append(sqlite3_column_text(self.stmnt, i)) else: raise NotImplementedError return tuple(res)",True,elif t == SQLITE_TEXT :,elif t == SQLITE_TEXT :,0.8820016898747209,100.00000000000004
"def try_convert(self, string): string = string.strip() try: return int(string) except: try: return float(string) except: if string == ""True"": return True <mask>: return False return string",False,"if string == ""False"" :","elif string == ""False"" :",0.8820016898747209,84.08964152537145
"def configure_create_table_epilogue(store): for val in ["""", "" ENGINE=InnoDB""]: store.config[""create_table_epilogue""] = val store._set_sql_flavour() <mask>: store.log.info(""create_table_epilogue='%s'"", val) return raise Exception(""Can not create a transactional table."")",False,if store . _test_transaction ( ) :,"if store . config [ ""create_table_epilogue"" ] :",0.8820016898747209,12.571192676522521
"def _check_rule(self, match, target_dict, cred_dict): """"""Recursively checks credentials based on the brains rules."""""" try: new_match_list = self.rules[match] except KeyError: <mask>: new_match_list = (""rule:%s"" % self.default_rule,) else: return False return self.check(new_match_list, target_dict, cred_dict)",False,if self . default_rule and match != self . default_rule :,if self . default_rule :,0.8820016898747209,27.645304662956455
"def get_civil_names(self): congresspeople_ids = self.get_all_congresspeople_ids() for i, congress_id in enumerate(congresspeople_ids): if not np.math.isnan(float(congress_id)): percentage = i / self.total * 100 msg = ""Processed {} out of {} ({:.2f}%)"" print(msg.format(i, self.total, percentage), end=""\r"") data = self.fetch_data_repository(congress_id) <mask>: yield dict(data)",False,if data is not None :,if data :,0.8820016898747209,0.0
"def parse_network_whitelist(self, network_whitelist_location): networks = [] with open(network_whitelist_location, ""r"") as text_file: for line in text_file: line = line.strip().strip(""'"").strip('""') <mask>: networks.append(line) return networks",False,if isIPv4 ( line ) or isIPv6 ( line ) :,if line :,0.8820016898747209,0.0
"def _pick(self, cum): if self._isleaf(): return self.bd[0], self.s else: <mask>: return self.left._pick(cum) else: return self.right._pick(cum - self.left.s)",False,if cum < self . left . s :,if self . left . s == 0 :,0.8820016898747209,42.7287006396234
"def serialize_content_range(value): if isinstance(value, (tuple, list)): if len(value) not in (2, 3): raise ValueError( ""When setting content_range to a list/tuple, it must "" ""be length 2 or 3 (not %r)"" % value ) <mask>: begin, end = value length = None else: begin, end, length = value value = ContentRange(begin, end, length) value = str(value).strip() if not value: return None return value",False,if len ( value ) == 2 :,"if isinstance ( value , ( list , tuple ) ) :",0.8820016898747209,8.516593018819643
"def make_index_fields(rec): fields = {} for k, v in rec.iteritems(): <mask>: fields[k] = v continue if k == ""full_title"": fields[""title""] = [read_short_title(v)] return fields",False,"if k in ( ""lccn"" , ""oclc"" , ""isbn"" ) :","if k == ""id"" :",0.8820016898747209,4.5088043638672195
"def _sample_translation(reference, max_len): translation = reference[:] while np.random.uniform() < 0.8 and 1 < len(translation) < max_len: trans_len = len(translation) ind = np.random.randint(trans_len) action = np.random.choice(actions) if action == ""deletion"": del translation[ind] <mask>: ind_rep = np.random.randint(trans_len) translation[ind] = translation[ind_rep] else: ind_insert = np.random.randint(trans_len) translation.insert(ind, translation[ind_insert]) return translation",False,"elif action == ""replacement"" :","elif action == ""insert"" :",0.8820016898747209,59.4603557501361
"def __call__(self, text: str) -> str: for t in self.cleaner_types: if t == ""tacotron"": text = tacotron_cleaner.cleaners.custom_english_cleaners(text) elif t == ""jaconv"": text = jaconv.normalize(text) <mask>: if vietnamese_cleaners is None: raise RuntimeError(""Please install underthesea"") text = vietnamese_cleaners.vietnamese_cleaner(text) else: raise RuntimeError(f""Not supported: type={t}"") return text",True,"elif t == ""vietnamese"" :","elif t == ""vietnamese"" :",0.8820016898747209,100.00000000000004
"def hook_GetVariable(ql, address, params): if params[""VariableName""] in ql.env: var = ql.env[params[""VariableName""]] read_len = read_int64(ql, params[""DataSize""]) <mask>: write_int64(ql, params[""Attributes""], 0) write_int64(ql, params[""DataSize""], len(var)) if read_len < len(var): return EFI_BUFFER_TOO_SMALL if params[""Data""] != 0: ql.mem.write(params[""Data""], var) return EFI_SUCCESS return EFI_NOT_FOUND",False,"if params [ ""Attributes"" ] != 0 :",if read_len == 0 :,0.8820016898747209,15.181939159382823
"def test_setupapp(self, overrideRootMenu): ""Call setupApp with each possible graphics type."" root = self.root flist = FileList(root) for tktype in alltypes: with self.subTest(tktype=tktype): macosx._tk_type = tktype macosx.setupApp(root, flist) <mask>: self.assertTrue(overrideRootMenu.called) overrideRootMenu.reset_mock()",False,"if tktype in ( ""carbon"" , ""cocoa"" ) :",if overrideRootMenu :,0.8820016898747209,0.0
"def names(self, persistent=None): u = set() result = [] for s in [ self.__storage(None), self.__storage(self.__category), ]: for b in s: if persistent is not None and b.persistent != persistent: continue <mask>: continue if b.name not in u: result.append(b.name) u.add(b.name) return result",False,"if b . name . startswith ( ""__"" ) :",if b . persistent is None and b . persistent != persistent :,0.8820016898747209,12.011055432195764
"def _check_extra_specs(key, value=None): extra_specs = diff.get(""extra_specs"") specific_type = extra_specs.get(key) if extra_specs else None old_type = None new_type = None if specific_type: old_type, new_type = specific_type <mask>: old_type = True if old_type and old_type.upper() == value else False new_type = True if new_type and new_type.upper() == value else False return old_type, new_type",True,if value :,if value :,0.8820016898747209,0.0
"def _write_lock_file(self, repo, force=True): # type: (Repository, bool) -> None if force or (self._update and self._write_lock): updated_lock = self._locker.set_lock_data(self._package, repo.packages) <mask>: self._io.write_line("""") self._io.write_line(""<info>Writing lock file</>"")",True,if updated_lock :,if updated_lock :,0.8820016898747209,100.00000000000004
"def process_message(self, msg): if msg[""type""] == ""sample"": batch_shape = msg[""fn""].batch_shape <mask>: batch_shape = [1] * (-self.dim - len(batch_shape)) + list(batch_shape) batch_shape[self.dim] = self.size msg[""fn""] = msg[""fn""].expand(torch.Size(batch_shape))",False,if len ( batch_shape ) < - self . dim or batch_shape [ self . dim ] != self . size :,if self . dim > 0 :,0.8820016898747209,1.5952593645088629
"def _test_reducibility(self): # make a copy of the graph graph = networkx.DiGraph(self._graph) # preprocess: make it a super graph self._make_supergraph(graph) while True: changed = False # find a node with a back-edge, remove the edge (deleting the loop), and replace it with a MultiNode changed |= self._remove_self_loop(graph) # find a node that has only one predecessor, and merge it with its predecessor (replace them with a # MultiNode) changed |= self._merge_single_entry_node(graph) <mask>: # a fixed-point is reached break",False,if not changed :,if changed :,0.8820016898747209,0.0
"def __init__(self, roberta, num_classes=2, dropout=0.0, prefix=None, params=None): super(RoBERTaClassifier, self).__init__(prefix=prefix, params=params) self.roberta = roberta self._units = roberta._units with self.name_scope(): self.classifier = nn.HybridSequential(prefix=prefix) <mask>: self.classifier.add(nn.Dropout(rate=dropout)) self.classifier.add(nn.Dense(units=self._units, activation=""tanh"")) if dropout: self.classifier.add(nn.Dropout(rate=dropout)) self.classifier.add(nn.Dense(units=num_classes))",True,if dropout :,if dropout :,0.8820016898747209,0.0
"def get_object_from_name(self, name, check_symlinks=True): if not name: return None name = name.rstrip(""\\"") for a, o in self.objects.items(): if not o.name: continue <mask>: return o if check_symlinks: m = [sl[1] for sl in self.symlinks if name.lower() == sl[0].lower()] if m: name = m[0] return self.get_object_from_name(name, False)",False,if o . name . lower ( ) == name . lower ( ) :,if name . lower ( ) == a . lower ( ) :,0.8820016898747209,61.88604784965421
"def __call__(self): """"""Run all check_* methods."""""" if self.on: oldformatwarning = warnings.formatwarning warnings.formatwarning = self.formatwarning try: for name in dir(self): <mask>: method = getattr(self, name) if method and callable(method): method() finally: warnings.formatwarning = oldformatwarning",True,"if name . startswith ( ""check_"" ) :","if name . startswith ( ""check_"" ) :",0.8820016898747209,100.00000000000004
"def __print__(self, defaults=False): if defaults: print_func = str else: print_func = repr pieces = [] default_values = self.__defaults__ for k in self.__fields__: value = getattr(self, k) if not defaults and value == default_values[k]: continue <mask>: print_func = repr # keep quotes around strings pieces.append(""%s=%s"" % (k, print_func(value))) if pieces or self.__base__: return ""%s(%s)"" % (self.__class__.__name__, "", "".join(pieces)) else: return """"",False,"if isinstance ( value , basestring ) :",if print_func is str :,0.8820016898747209,6.770186228657864
"def apply(self, **kwargs: Any) -> None: for node in self.document.traverse(nodes.target): <mask>: continue if ( ""ismod"" in node and node.parent.__class__ is nodes.section and # index 0 is the section title node node.parent.index(node) == 1 ): node.parent[""ids""][0:0] = node[""ids""] node.parent.remove(node)",False,"if not node [ ""ids"" ] :",if node . __class__ is nodes . section :,0.8820016898747209,4.065425428798724
"def add_special_token_2d( values: List[List[int]], special_token: int = 0, use_first_value: bool = False ) -> List[List[int]]: results = torch.jit.annotate(List[List[int]], []) for value in values: result = torch.jit.annotate(List[int], []) <mask>: special_token = value[0] result.append(special_token) result.extend(value) result.append(special_token) results.append(result) return results",False,if use_first_value and len ( value ) > 0 :,if use_first_value :,0.8820016898747209,30.93485033266056
"def test_import(self): TIMEOUT = 5 # Test for a deadlock when importing a module that runs the # ThreadedResolver at import-time. See resolve_test.py for # full explanation. command = [sys.executable, ""-c"", ""import tornado.test.resolve_test_helper""] start = time.time() popen = Popen(command, preexec_fn=lambda: signal.alarm(TIMEOUT)) while time.time() - start < TIMEOUT: return_code = popen.poll() <mask>: self.assertEqual(0, return_code) return # Success. time.sleep(0.05) self.fail(""import timed out"")",False,if return_code is not None :,if return_code :,0.8820016898747209,38.80684294761701
"def find_item_for_key(self, e): for item in self._items: if item.keycode == e.key and item.shift == e.shift and item.alt == e.alt: focus = get_focus() <mask>: return self._items.index(item) else: return -1 return -1",False,"if self . command_is_enabled ( item , focus ) :",if focus :,0.8820016898747209,0.0
"def check_app_config_brackets(self): for sn, app in cherrypy.tree.apps.items(): if not isinstance(app, cherrypy.Application): continue <mask>: continue for key in app.config.keys(): if key.startswith(""["") or key.endswith(""]""): warnings.warn( ""The application mounted at %r has config "" ""section names with extraneous brackets: %r. "" ""Config *files* need brackets; config *dicts* "" ""(e.g. passed to tree.mount) do not."" % (sn, key) )",True,if not app . config :,if not app . config :,0.8820016898747209,100.00000000000004
"def got_arbiter_module_type_defined(self, mod_type): for a in self.arbiters: # Do like the linkify will do after.... for m in getattr(a, ""modules"", []): # So look at what the arbiter try to call as module m = m.strip() # Ok, now look in modules... for mod in self.modules: # try to see if this module is the good type <mask>: # if so, the good name? if getattr(mod, ""module_name"", """").strip() == m: return True return False",False,"if getattr ( mod , ""module_type"" , """" ) . strip ( ) == mod_type . strip ( ) :",if mod_type == mod_type :,0.8820016898747209,8.16474474946068
"def write_config_to_file(self, folder, filename, config): do_not_write = [""hyperparameter_search_space_updates""] with open(os.path.join(folder, filename), ""w"") as f: f.write( ""\n"".join( [ (key + ""="" + str(value)) for (key, value) in sorted(config.items(), key=lambda x: x[0]) <mask>: ] ) )",False,if not key in do_not_write,if key not in do_not_write,0.8820016898747209,65.00593260343696
"def parsing(self, parsing): # type: (bool) -> None self._parsed = parsing for k, v in self._body: <mask>: v.value.parsing(parsing) elif isinstance(v, AoT): for t in v.body: t.value.parsing(parsing)",False,"if isinstance ( v , Table ) :","if isinstance ( v , AoT ) :",0.8820016898747209,59.4603557501361
"def test_crashers_crash(self): for fname in glob.glob(CRASHER_FILES): <mask>: continue # Some ""crashers"" only trigger an exception rather than a # segfault. Consider that an acceptable outcome. if test.support.verbose: print(""Checking crasher:"", fname) assert_python_failure(fname)",False,if os . path . basename ( fname ) in infinite_loops :,"if not fname . startswith ( ""crasher_"" ) :",0.8820016898747209,4.662759254079381
"def __getitem__(self, k) -> ""SimMemView"": if isinstance(k, slice): if k.step is not None: raise ValueError(""Slices with strides are not supported"") elif k.start is None: raise ValueError(""Must specify start index"") <mask>: raise ValueError(""Slices with stop index are not supported"") else: addr = k.start elif self._type is not None and self._type._can_refine_int: return self._type._refine(self, k) else: addr = k return self._deeper(addr=addr)",False,elif k . stop is not None :,elif k . stop is None :,0.8820016898747209,61.29752413741059
"def get_lowest_wall_time(jsons): lowest_wall = None for j in jsons: <mask>: lowest_wall = j[""wall_time""] if lowest_wall > j[""wall_time""]: lowest_wall = j[""wall_time""] return lowest_wall",True,if lowest_wall is None :,if lowest_wall is None :,0.8820016898747209,100.00000000000004
"def extract_wav_headers(data): # def search_subchunk(data, subchunk_id): pos = 12 # The size of the RIFF chunk descriptor subchunks = [] while pos + 8 <= len(data) and len(subchunks) < 10: subchunk_id = data[pos : pos + 4] subchunk_size = struct.unpack_from(""<I"", data[pos + 4 : pos + 8])[0] subchunks.append(WavSubChunk(subchunk_id, pos, subchunk_size)) <mask>: # 'data' is the last subchunk break pos += subchunk_size + 8 return subchunks",False,"if subchunk_id == b""data"" :",if subchunk_id == data [ pos : pos + 4 ] :,0.8820016898747209,34.79159475128448
"def _any_targets_have_native_sources(self, targets): # TODO(#5949): convert this to checking if the closure of python requirements has any # platform-specific packages (maybe find the platforms there too?). for tgt in targets: for type_constraint, target_predicate in self._native_target_matchers.items(): <mask>: return True return False",False,if type_constraint . satisfied_by ( tgt ) and target_predicate ( tgt ) :,"if target_predicate ( tgt , type_constraint ) :",0.8820016898747209,26.628862802997862
"def validate_memory(self, value): for k, v in value.viewitems(): if v is None: # use NoneType to unset a value continue <mask>: raise serializers.ValidationError(""Process types can only contain [a-z]"") if not re.match(MEMLIMIT_MATCH, str(v)): raise serializers.ValidationError( ""Limit format: <number><unit>, where unit = B, K, M or G"" ) return value",False,"if not re . match ( PROCTYPE_MATCH , k ) :","if k . startswith ( ""_"" ) :",0.8820016898747209,8.503662878579146
"def cart_number_checksum_validation(cls, number): digits = [] even = False if not number.isdigit(): return False for digit in reversed(number): digit = ord(digit) - ord(""0"") if even: digit *= 2 <mask>: digit = digit % 10 + digit // 10 digits.append(digit) even = not even return sum(digits) % 10 == 0 if digits else False",False,if digit >= 10 :,if digit % 10 == 0 :,0.8820016898747209,13.888095170058955
"def transform(a, cmds): buf = a.split(""\n"") for cmd in cmds: ctype, line, col, char = cmd if ctype == ""D"": <mask>: buf[line] = buf[line][:col] + buf[line][col + len(char) :] else: buf[line] = buf[line] + buf[line + 1] del buf[line + 1] elif ctype == ""I"": buf[line] = buf[line][:col] + char + buf[line][col:] buf = ""\n"".join(buf).split(""\n"") return ""\n"".join(buf)",False,"if char != ""\n"" :",if char :,0.8820016898747209,0.0
"def get_partners(self) -> Dict[AbstractNode, Set[int]]: partners = {} # type: Dict[AbstractNode, Set[int]] for edge in self.edges: if edge.is_dangling(): raise ValueError(""Cannot contract copy tensor with dangling edges"") if self._is_my_trace(edge): continue partner_node, shared_axis = self._get_partner(edge) <mask>: partners[partner_node] = set() partners[partner_node].add(shared_axis) return partners",True,if partner_node not in partners :,if partner_node not in partners :,0.8820016898747209,100.00000000000004
"def _bind_interactive_rez(self): if config.set_prompt and self.settings.prompt: stored_prompt = os.getenv(""REZ_STORED_PROMPT_CMD"") curr_prompt = stored_prompt or os.getenv(""PROMPT"", """") <mask>: self.setenv(""REZ_STORED_PROMPT_CMD"", curr_prompt) new_prompt = ""%%REZ_ENV_PROMPT%%"" new_prompt = ( (new_prompt + "" %s"") if config.prefix_prompt else (""%s "" + new_prompt) ) new_prompt = new_prompt % curr_prompt self._addline(""set PROMPT=%s"" % new_prompt)",False,if not stored_prompt :,if curr_prompt :,0.8820016898747209,34.98330125272253
"def __listingColumns(self): columns = [] for name in self.__getColumns(): definition = column(name) if not definition: IECore.msg( IECore.Msg.Level.Error, ""GafferImageUI.CatalogueUI"", ""No column registered with name '%s'"" % name, ) continue <mask>: c = GafferUI.PathListingWidget.IconColumn(definition.title(), """", name) else: c = GafferUI.PathListingWidget.StandardColumn(definition.title(), name) columns.append(c) return columns",False,"if isinstance ( definition , IconColumn ) :",if definition . icon :,0.8820016898747209,7.715486568024961
"def _check_invalid_keys(self, section_name, section): for key in section: key_name = str(key) valid_key_names = [s[0] for s in self.keys] is_valid_key = key_name in valid_key_names <mask>: err_msg = ( ""'{0}' is not a valid key name for '{1}'. Must "" ""be one of these: {2}"" ).format(key_name, section_name, "", "".join(valid_key_names)) raise InvalidConfig(err_msg)",True,if not is_valid_key :,if not is_valid_key :,0.8820016898747209,100.00000000000004
"def _get_startup_packages(lib_path: Path, packages) -> Set[str]: names = set() for path in lib_path.iterdir(): name = path.name if name == ""__pycache__"": continue if name.endswith("".py""): names.add(name.split(""."")[0]) <mask>: names.add(name) if packages: packages = {package.lower().replace(""-"", ""_"") for package in packages} if len(names & packages) == len(packages): return packages return names",False,"elif path . is_dir ( ) and ""."" not in name :","elif name . startswith ( ""py"" ) :",0.8820016898747209,3.8729615016773113
"def sortkeypicker(keynames): negate = set() for i, k in enumerate(keynames): <mask>: keynames[i] = k[1:] negate.add(k[1:]) def getit(adict): composite = [adict[k] for k in keynames] for i, (k, v) in enumerate(zip(keynames, composite)): if k in negate: composite[i] = -v return composite return getit",False,"if k [ : 1 ] == ""-"" :","if k [ 0 ] == ""-"" :",0.8820016898747209,64.07117598241614
"def iter_symbols(code): """"""Yield names and strings used by `code` and its nested code objects"""""" for name in code.co_names: yield name for const in code.co_consts: if isinstance(const, six.string_types): yield const <mask>: for name in iter_symbols(const): yield name",False,"elif isinstance ( const , CodeType ) :","elif isinstance ( const , ( list , tuple ) ) :",0.8820016898747209,36.462858619364674
"def set_study_directions( self, study_id: int, directions: Sequence[StudyDirection] ) -> None: with self._lock: <mask>: current_directions = self._studies[study_id].directions if directions == current_directions: return elif ( len(current_directions) == 1 and current_directions[0] == StudyDirection.NOT_SET ): self._studies[study_id].directions = list(directions) self._backend.set_study_directions(study_id, directions) return self._backend.set_study_directions(study_id, directions)",True,if study_id in self . _studies :,if study_id in self . _studies :,0.8820016898747209,100.00000000000004
"def PreprocessConditionalStatement(self, IfList, ReplacedLine): while self: <mask>: x = 1 elif not IfList: if self <= 2: continue RegionSizeGuid = 3 if not RegionSizeGuid: RegionLayoutLine = 5 continue RegionLayoutLine = self.CurrentLineNumber return 1",False,if self . __Token :,if ReplacedLine :,0.8820016898747209,0.0
"def _check_blocking(self, current_time): if self._switch_flag is False: active_greenlet = self._active_greenlet <mask>: self._notify_greenlet_blocked(active_greenlet, current_time) self._switch_flag = False",False,if active_greenlet is not None and active_greenlet != self . _hub :,if active_greenlet is not None :,0.8820016898747209,24.909923021496894
"def detect(get_page): retval = False for vector in WAF_ATTACK_VECTORS: page, headers, code = get_page(get=vector) retval = ( re.search(r""BlockDos\.net"", headers.get(HTTP_HEADER.SERVER, """"), re.I) is not None ) <mask>: break return retval",True,if retval :,if retval :,0.8820016898747209,0.0
"def _fastqc_data_section(self, section_name): out = [] in_section = False data_file = os.path.join(self._dir, ""fastqc_data.txt"") if os.path.exists(data_file): with open(data_file) as in_handle: for line in in_handle: <mask>: in_section = True elif in_section: if line.startswith("">>END""): break out.append(line.rstrip(""\r\n"")) return out",False,"if line . startswith ( "">>%s"" % section_name ) :",if line . startswith ( section_name ) :,0.8820016898747209,39.01319655022955
"def shortcut(self, input, ch_out, stride, is_first, name): ch_in = input.shape[1] if ch_in != ch_out or stride != 1: <mask>: return self.conv_bn_layer(input, ch_out, 1, stride, name=name) else: return self.conv_bn_layer_new(input, ch_out, 1, stride, name=name) elif is_first: return self.conv_bn_layer(input, ch_out, 1, stride, name=name) else: return input",False,if is_first or stride == 1 :,if is_first :,0.8820016898747209,26.013004751144457
"def get_value_from_string(self, string_value): """"""Return internal representation starting from CFN/user-input value."""""" param_value = self.get_default_value() try: if string_value is not None: string_value = str(string_value).strip() <mask>: param_value = int(string_value) except ValueError: self.pcluster_config.warn( ""Unable to convert the value '{0}' to an Integer. "" ""Using default value for parameter '{1}'"".format(string_value, self.key) ) return param_value",False,"if string_value != ""NONE"" :","if isinstance ( string_value , int ) :",0.8820016898747209,17.747405280050266
"def get_running(workers): running = [] for worker in workers: current_test_name = worker.current_test_name <mask>: continue dt = time.monotonic() - worker.start_time if dt >= PROGRESS_MIN_TIME: text = ""%s (%s)"" % (current_test_name, format_duration(dt)) running.append(text) return running",False,if not current_test_name :,"if current_test_name == ""test"" :",0.8820016898747209,34.48444257953326
"def generate_data(self, request): """"""Generate data for the widget."""""" uptime = {} cache_stats = get_cache_stats() if cache_stats: for hosts, stats in cache_stats: if stats[""uptime""] > 86400: uptime[""value""] = stats[""uptime""] / 60 / 60 / 24 uptime[""unit""] = _(""days"") <mask>: uptime[""value""] = stats[""uptime""] / 60 / 60 uptime[""unit""] = _(""hours"") else: uptime[""value""] = stats[""uptime""] / 60 uptime[""unit""] = _(""minutes"") return {""cache_stats"": cache_stats, ""uptime"": uptime}",True,"elif stats [ ""uptime"" ] > 3600 :","elif stats [ ""uptime"" ] > 3600 :",0.8820016898747209,100.00000000000004
"def add_actors(self): """"""Adds `self.actors` to the scene."""""" if not self._actors_added: self.reader.render_window = self.scene.render_window self._update_reader() self._actors_added = True <mask>: self._visible_changed(self.visible) self.scene.render()",False,if not self . visible :,if self . visible :,0.8820016898747209,57.89300674674101
"def _add_uniqu_suffix(self, titles): counters = dict() titles_with_suffix = [] for title in titles: counters[title] = counters[title] + 1 if title in counters else 1 <mask>: title = f""{title} ({counters[title]})"" titles_with_suffix.append(title) return titles_with_suffix",False,if counters [ title ] > 1 :,if title in counters :,0.8820016898747209,8.290829875388036
"def _verify_udf_resources(self, job, config): udf_resources = config.get(""userDefinedFunctionResources"", ()) self.assertEqual(len(job.udf_resources), len(udf_resources)) for found, expected in zip(job.udf_resources, udf_resources): <mask>: self.assertEqual(found.udf_type, ""resourceUri"") self.assertEqual(found.value, expected[""resourceUri""]) else: self.assertEqual(found.udf_type, ""inlineCode"") self.assertEqual(found.value, expected[""inlineCode""])",True,"if ""resourceUri"" in expected :","if ""resourceUri"" in expected :",0.8820016898747209,100.00000000000004
"def __init__( self, layout, value=None, string=None, *, dtype: np.dtype = np.float64 ) -> None: """"""Constructor."""""" self.layout = layout if value is None: <mask>: self.value = np.zeros((self.layout.gaDims,), dtype=dtype) else: self.value = layout.parse_multivector(string).value else: self.value = np.array(value) if self.value.shape != (self.layout.gaDims,): raise ValueError( ""value must be a sequence of length %s"" % self.layout.gaDims )",True,if string is None :,if string is None :,0.8820016898747209,100.00000000000004
"def read_file(filename, print_error=True): """"""Returns the contents of a file."""""" try: for encoding in [""utf-8"", ""latin1""]: try: with io.open(filename, encoding=encoding) as fp: return fp.read() except UnicodeDecodeError: pass except IOError as exception: <mask>: print(exception, file=sys.stderr) return None",True,if print_error :,if print_error :,0.8820016898747209,100.00000000000004
"def get_albums_for_iter(self, iter_): obj = self.get_value(iter_) if isinstance(obj, AlbumNode): return {obj.album} albums = set() for child_iter, value in self.iterrows(iter_): <mask>: albums.add(value.album) else: albums.update(self.get_albums_for_iter(child_iter)) return albums",True,"if isinstance ( value , AlbumNode ) :","if isinstance ( value , AlbumNode ) :",0.8820016898747209,100.00000000000004
"def wait_til_ready(cls, connector=None): if connector is None: connector = cls.connector while True: now = time.time() next_iteration = now // 1.0 + 1 <mask>: break else: await cls._clock.run_til(next_iteration) await asyncio.sleep(1.0)",False,if connector . ready :,if next_iteration == 0 :,0.8820016898747209,6.567274736060395
"def remove_property(self, key): # type: (str) -> None with self.secure() as config: keys = key.split(""."") current_config = config for i, key in enumerate(keys): if key not in current_config: return <mask>: del current_config[key] break current_config = current_config[key]",True,if i == len ( keys ) - 1 :,if i == len ( keys ) - 1 :,0.8820016898747209,100.00000000000004
"def get(self, hash160, default=None): v = self.p2s_for_hash(hash160) <mask>: return v if hash160 not in self._secret_exponent_cache: v = self.path_for_hash160(hash160) if v: fingerprint, path = v for key in self._secrets.get(fingerprint, []): subkey = key.subkey_for_path(path) self._add_key_to_cache(subkey) return self._secret_exponent_cache.get(hash160, default)",True,if v :,if v :,0.8820016898747209,0.0
"def fetch_all(self, api_client, fetchstatuslogger, q, targets): self.fetchstatuslogger = fetchstatuslogger if targets != None: # Ensure targets is a tuple <mask>: targets = tuple( targets, ) elif type(targets) != tuple: targets = tuple(targets) for target in targets: self._fetch_targets(api_client, q, target)",False,if type ( targets ) != list and type ( targets ) != tuple :,if type ( targets ) == list :,0.8820016898747209,22.472736255949545
"def dgl_mp_batchify_fn(data): if isinstance(data[0], tuple): data = zip(*data) return [dgl_mp_batchify_fn(i) for i in data] for dt in data: <mask>: if isinstance(dt, dgl.DGLGraph): return [d for d in data if isinstance(d, dgl.DGLGraph)] elif isinstance(dt, nd.NDArray): pad = Pad(axis=(1, 2), num_shards=1, ret_length=False) data_list = [dt for dt in data if dt is not None] return pad(data_list)",False,if dt is not None :,"if isinstance ( dt , ( list , tuple ) ) :",0.8820016898747209,4.456882760699063
"def capture_server(evt, buf, serv): try: serv.listen(5) conn, addr = serv.accept() except socket.timeout: pass else: n = 200 while n > 0: r, w, e = select.select([conn], [], []) <mask>: data = conn.recv(10) # keep everything except for the newline terminator buf.write(data.replace(""\n"", """")) if ""\n"" in data: break n -= 1 time.sleep(0.01) conn.close() finally: serv.close() evt.set()",False,if r :,if r == 0 :,0.8820016898747209,17.965205598154213
"def elem(): if ints_only: return random.randint(0, 10000000000) else: t = random.randint(0, 2) if t == 0: return random.randint(0, 10000000000) elif t == 1: return float(random.randint(0, 10000000000)) <mask>: return strings[random.randint(0, len(strings) - 1)] return random_string(random.randint(100, 1000))",False,elif strings is not None :,elif t == 2 :,0.8820016898747209,9.652434877402245
"def has_changed(self, initial, data): if self.disabled: return False if initial is None: initial = ["""" for x in range(0, len(data))] else: <mask>: initial = self.widget.decompress(initial) for field, initial, data in zip(self.fields, initial, data): try: initial = field.to_python(initial) except ValidationError: return True if field.has_changed(initial, data): return True return False",False,"if not isinstance ( initial , list ) :","if isinstance ( initial , str ) :",0.8820016898747209,37.70794596593207
"def _load_testfile(filename, package, module_relative): if module_relative: package = _normalize_module(package, 3) filename = _module_relative_path(package, filename) <mask>: if hasattr(package.__loader__, ""get_data""): file_contents = package.__loader__.get_data(filename) # get_data() opens files as 'rb', so one must do the equivalent # conversion as universal newlines would do. return file_contents.replace(os.linesep, ""\n""), filename return open(filename).read(), filename",False,"if hasattr ( package , ""__loader__"" ) :",if os . path . isfile ( filename ) :,0.8820016898747209,6.68986069184485
"def release(self): tid = _thread.get_ident() with self.lock: if self.owner != tid: raise RuntimeError(""cannot release un-acquired lock"") assert self.count > 0 self.count -= 1 if self.count == 0: self.owner = None <mask>: self.waiters -= 1 self.wakeup.release()",False,if self . waiters :,if self . waiters > 0 :,0.8820016898747209,43.47208719449914
"def stage( self, x, num_modules, num_blocks, channels, multi_scale_output=True, name=None ): out = x for i in range(num_modules): <mask>: out = self.high_resolution_module( out, num_blocks, channels, multi_scale_output=False, name=name + ""_"" + str(i + 1), ) else: out = self.high_resolution_module( out, num_blocks, channels, name=name + ""_"" + str(i + 1) ) return out",False,if i == num_modules - 1 and multi_scale_output == False :,if multi_scale_output :,0.8820016898747209,12.043498774973072
"def changeFrontAlteration(intV, alter): # fati = front alteration transpose interval fati = self.frontAlterationTransposeInterval if fati: newFati = interval.add([fati, intV]) self.frontAlterationTransposeInterval = newFati self.frontAlterationAccidental.alter = ( self.frontAlterationAccidental.alter + alter ) <mask>: self.frontAlterationTransposeInterval = None self.frontAlterationAccidental = None else: self.frontAlterationTransposeInterval = intV self.frontAlterationAccidental = pitch.Accidental(alter)",False,if self . frontAlterationAccidental . alter == 0 :,if intV == 0 :,0.8820016898747209,27.585129929794586
"def set_to_train(self): for T in self.trainable_attributes(): for k, v in T.items(): <mask>: c_f.set_requires_grad(v, requires_grad=False) v.eval() else: v.train() self.maybe_freeze_trunk_batchnorm()",False,if k in self . freeze_these :,"if k == ""requires_grad"" :",0.8820016898747209,9.980099403873663
"def _migrate(self, sig=None, compact=True): with self.lock: sig = sig or self.sig <mask>: return if sig in self.WORDS and len(self.WORDS[sig]) > 0: PostingList.Append( self.session, sig, self.WORDS[sig], sig=sig, compact=compact ) del self.WORDS[sig]",False,if sig in GPL_NEVER_MIGRATE :,if not sig :,0.8820016898747209,6.023021415818187
"def on_prediction_step(self, args, state, control, eval_dataloader=None, **kwargs): if self.prediction_bar is None: <mask>: self.prediction_bar = self.training_tracker.add_child(len(eval_dataloader)) else: self.prediction_bar = NotebookProgressBar(len(eval_dataloader)) self.prediction_bar.update(1) else: self.prediction_bar.update(self.prediction_bar.value + 1)",False,if self . training_tracker is not None :,if eval_dataloader is not None :,0.8820016898747209,29.797147054518835
"def show(self, indent=0): """"""Pretty print this structure."""""" if indent == 0: print(""struct {}"".format(self.name)) for field in self.fields: if field.offset is None: offset = ""0x??"" else: offset = ""0x{:02x}"".format(field.offset) print(""{}+{} {} {}"".format("" "" * indent, offset, field.name, field.type)) <mask>: field.type.show(indent + 1)",False,"if isinstance ( field . type , Structure ) :",if field . type is not None :,0.8820016898747209,18.190371142855746
"def __exit__(self, exc, value, tb): for key in self.overrides.keys(): old_value = self.old[key] <mask>: delattr(self.instance, key) else: setattr(self.instance, key, old_value) self.instance.save()",False,if old_value is NULL :,if old_value is None :,0.8820016898747209,64.34588841607616
"def complete(self, block): with self._condition: <mask>: return False if self._complete(): self._calculate_state_root_if_not_already_done() return True if block: self._condition.wait_for(self._complete) self._calculate_state_root_if_not_already_done() return True return False",False,if not self . _final :,if self . _state_root_if_not_already_done :,0.8820016898747209,10.82597837309053
"def parseArguments(self): args = [] self.expect(""("") if not self.match("")""): while self.startIndex < self.length: args.append(self.isolateCoverGrammar(self.parseAssignmentExpression)) <mask>: break self.expectCommaSeparator() self.expect("")"") return args",False,"if self . match ( "")"" ) :","if self . match ( "","" ) :",0.8820016898747209,65.80370064762461
"def isValidDateString(config_param_name, value, valid_value): try: if value == ""DD-MM-YYYY"": return value day, month, year = value.split(""-"") <mask>: raise DateStringValueError(config_param_name, value) if int(month) < 1 or int(month) > 12: raise DateStringValueError(config_param_name, value) if int(year) < 1900 or int(year) > 2013: raise DateStringValueError(config_param_name, value) return value except Exception: raise DateStringValueError(config_param_name, value)",False,if int ( day ) < 1 or int ( day ) > 31 :,if int ( day ) < 1 or int ( day ) > 10 :,0.8820016898747209,86.66415730847507
"def build_tree(path): tree = Tree() for basename, entry in trees[path].items(): <mask>: mode = stat.S_IFDIR sha = build_tree(pathjoin(path, basename)) else: (mode, sha) = entry tree.add(basename, mode, sha) object_store.add_object(tree) return tree.id",False,"if isinstance ( entry , dict ) :",if entry is None :,0.8820016898747209,7.715486568024961
"def get_quarantine_count(self): """"""get obj/container/account quarantine counts"""""" qcounts = {""objects"": 0, ""containers"": 0, ""accounts"": 0} qdir = ""quarantined"" for device in os.listdir(self.devices): for qtype in qcounts: qtgt = os.path.join(self.devices, device, qdir, qtype) <mask>: linkcount = os.lstat(qtgt).st_nlink if linkcount > 2: qcounts[qtype] += linkcount - 2 return qcounts",True,if os . path . exists ( qtgt ) :,if os . path . exists ( qtgt ) :,0.8820016898747209,100.00000000000004
"def _is_static_shape(self, shape): if shape is None or not isinstance(shape, list): return False for dim_value in shape: <mask>: return False if dim_value < 0: raise Exception(""Negative dimension is illegal: %d"" % dim_value) return True",False,"if not isinstance ( dim_value , int ) :",if dim_value == 0 :,0.8820016898747209,16.0529461904344
"def BraceDetectAll(words): # type: (List[compound_word]) -> List[word_t] """"""Return a new list of words, possibly with BracedTree instances."""""" out = [] # type: List[word_t] for w in words: # The shortest possible brace expansion is {,}. This heuristic prevents # a lot of garbage from being created, since otherwise nearly every word # would be checked. We could be even more precise but this is cheap. if len(w.parts) >= 3: brace_tree = _BraceDetect(w) <mask>: out.append(brace_tree) continue out.append(w) return out",True,if brace_tree :,if brace_tree :,0.8820016898747209,100.00000000000004
"def __init__(original, self, *args, **kwargs): data = args[0] if len(args) > 0 else kwargs.get(""data"") if data is not None: try: <mask>: raise Exception( ""cannot gather example input when dataset is loaded from a file."" ) input_example_info = _InputExampleInfo( input_example=deepcopy(data[:INPUT_EXAMPLE_SAMPLE_ROWS]) ) except Exception as e: input_example_info = _InputExampleInfo(error_msg=str(e)) setattr(self, ""input_example_info"", input_example_info) original(self, *args, **kwargs)",False,"if isinstance ( data , str ) :","if not isinstance ( data [ INPUT_EXAMPLE_SAMPLE_ROWS ] , str ) :",0.8820016898747209,20.105373454060025
"def setRow(self, row, vals): if row > self.rowCount() - 1: self.setRowCount(row + 1) for col in range(len(vals)): val = vals[col] item = self.itemClass(val, row) item.setEditable(self.editable) sortMode = self.sortModes.get(col, None) <mask>: item.setSortMode(sortMode) format = self._formats.get(col, self._formats[None]) item.setFormat(format) self.items.append(item) self.setItem(row, col, item) item.setValue(val) # Required--the text-change callback is invoked",False,if sortMode is not None :,if sortMode :,0.8820016898747209,0.0
"def wakeUp(self): """"""Write one byte to the pipe, and flush it."""""" # We don't use fdesc.writeToFD since we need to distinguish # between EINTR (try again) and EAGAIN (do nothing). if self.o is not None: try: util.untilConcludes(os.write, self.o, b""x"") except OSError as e: # XXX There is no unit test for raising the exception # for other errnos. See #4285. <mask>: raise",False,if e . errno != errno . EAGAIN :,if e . errno != errno . EINTR :,0.8820016898747209,78.25422900366438
"def _setup(self, field_name, owner_model): # Resolve possible name-based model references. resolved_classes = [] for m in self.model_classes: <mask>: if m == owner_model.__name__: resolved_classes.append(owner_model) else: raise Exception( ""PolyModelType: Unable to resolve model '{}'."".format(m) ) else: resolved_classes.append(m) self.model_classes = tuple(resolved_classes) super(PolyModelType, self)._setup(field_name, owner_model)",False,"if isinstance ( m , string_type ) :","if isinstance ( m , str ) :",0.8820016898747209,46.307771619910305
"def _wrap_forwarded(self, key, value): if isinstance(value, SourceCode) and value.late_binding: # get cached return value if present value_ = self._late_binding_returnvalues.get(key, KeyError) if value_ is KeyError: # evaluate the late-bound function value_ = self._eval_late_binding(value) schema = self.late_bind_schemas.get(key) <mask>: value_ = schema.validate(value_) # cache result of late bound func self._late_binding_returnvalues[key] = value_ return value_ else: return value",False,if schema is not None :,if schema :,0.8820016898747209,0.0
"def convert(self, ctx, argument): arg = argument.replace(""0x"", """").lower() if arg[0] == ""#"": arg = arg[1:] try: value = int(arg, base=16) <mask>: raise BadColourArgument(arg) return discord.Colour(value=value) except ValueError: arg = arg.replace("" "", ""_"") method = getattr(discord.Colour, arg, None) if arg.startswith(""from_"") or method is None or not inspect.ismethod(method): raise BadColourArgument(arg) return method()",False,if not ( 0 <= value <= 0xFFFFFF ) :,if value < 0 :,0.8820016898747209,6.624642068265613
"def get_versions(*, all=False, quiet=None): import bonobo from bonobo.util.pkgs import bonobo_packages yield _format_version(bonobo, quiet=quiet) if all: for name in sorted(bonobo_packages): <mask>: try: mod = __import__(name.replace(""-"", ""_"")) try: yield _format_version(mod, name=name, quiet=quiet) except Exception as exc: yield ""{} ({})"".format(name, exc) except ImportError as exc: yield ""{} is not importable ({})."".format(name, exc)",False,"if name != ""bonobo"" :","if name . startswith ( ""bonobo_"" ) :",0.8820016898747209,11.731175160263996
"def assertOperationsInjected(self, plan, **kwargs): for migration, _backward in plan: operations = iter(migration.operations) for operation in operations: <mask>: next_operation = next(operations) self.assertIsInstance( next_operation, contenttypes_management.RenameContentType ) self.assertEqual(next_operation.app_label, migration.app_label) self.assertEqual(next_operation.old_model, operation.old_name_lower) self.assertEqual(next_operation.new_model, operation.new_name_lower)",False,"if isinstance ( operation , migrations . RenameModel ) :",if operation . name == migration . name :,0.8820016898747209,5.934202609760488
"def valid_localparts(strip_delimiters=False): for line in ABRIDGED_LOCALPART_VALID_TESTS.split(""\n""): # strip line, skip over empty lines line = line.strip() if line == """": continue # skip over comments or empty lines match = COMMENT.match(line) if match: continue # skip over localparts with delimiters <mask>: if "","" in line or "";"" in line: continue yield line",True,if strip_delimiters :,if strip_delimiters :,0.8820016898747209,100.00000000000004
"def read_lccn(line, is_marc8=False): found = [] for k, v in get_raw_subfields(line, [""a""]): lccn = v.strip() if re_question.match(lccn): continue m = re_lccn.search(lccn) if not m: continue # remove letters and bad chars lccn = re_letters_and_bad.sub("""", m.group(1)).strip() <mask>: found.append(lccn) return found",False,if lccn :,if is_marc8 and lccn not in found :,0.8820016898747209,5.522397783539471
"def test_named_parameters_and_constraints(self): likelihood = gpytorch.likelihoods.GaussianLikelihood() model = ExactGPModel(None, None, likelihood) for name, _param, constraint in model.named_parameters_and_constraints(): if name == ""likelihood.noise_covar.raw_noise"": self.assertIsInstance(constraint, gpytorch.constraints.GreaterThan) elif name == ""mean_module.constant"": self.assertIsNone(constraint) elif name == ""covar_module.raw_outputscale"": self.assertIsInstance(constraint, gpytorch.constraints.Positive) <mask>: self.assertIsInstance(constraint, gpytorch.constraints.Positive)",False,"elif name == ""covar_module.base_kernel.raw_lengthscale"" :","elif name == ""covar_module.raw_outputscale_constant"" :",0.8820016898747209,56.60216224646277
"def _cleanupSocket(self): """"""Close the Connection's socket."""""" try: self._sock.shutdown(socket.SHUT_WR) except: return try: while True: r, w, e = select.select([self._sock], [], []) <mask>: break except: pass self._sock.close()",False,if not r or not self . _sock . recv ( 1024 ) :,if r == w :,0.8820016898747209,2.383515454163372
"def fadeIn(self, acts=None, t=None, duration=None): """"""Gradually switch on the input list of meshes by increasing opacity."""""" if self.bookingMode: acts, t, duration, rng = self._parse(acts, t, duration) for tt in rng: alpha = linInterpolate(tt, [t, t + duration], [0, 1]) self.events.append((tt, self.fadeIn, acts, alpha)) else: for a in self._performers: <mask>: continue a.alpha(self._inputvalues) return self",False,if a . alpha ( ) >= self . _inputvalues :,if a . alpha is None :,0.8820016898747209,18.448373350246094
"def get_config_updates_recursive(self): config_updates = self.config_updates.copy() for sr_path, subrunner in self.subrunners.items(): <mask>: continue update = subrunner.get_config_updates_recursive() if update: config_updates[rel_path(self.path, sr_path)] = update return config_updates",False,"if not is_prefix ( self . path , sr_path ) :",if not subrunner :,0.8820016898747209,2.260191208300767
"def setArgs(self, **kwargs): """"""See GridSearchCostGamma"""""" for key, value in list(kwargs.items()): if key in (""folds"", ""nfolds""): self._n_folds = int(value) <mask>: self._validator_kwargs[""max_epochs""] = value else: GridSearchDOE.setArgs(self, **{key: value})",False,"elif key in ( ""max_epochs"" ) :","elif key == ""max_epochs"" :",0.8820016898747209,42.2683921634124
"def _parse_composite_axis(composite_axis_name: str): axes_names = [axis for axis in composite_axis_name.split("" "") if len(axis) > 0] for axis in axes_names: <mask>: continue assert ""a"" <= axis[0] <= ""z"" for letter in axis: assert str.isdigit(letter) or ""a"" <= letter <= ""z"" return axes_names",False,"if axis == ""_"" :",if len ( axis ) == 1 :,0.8820016898747209,11.99014838091355
"def visit_For(self, node, for_branch=""body"", **kwargs): if for_branch == ""body"": self.sym_visitor.visit(node.target, store_as_param=True) branch = node.body elif for_branch == ""else"": branch = node.else_ elif for_branch == ""test"": self.sym_visitor.visit(node.target, store_as_param=True) <mask>: self.sym_visitor.visit(node.test) return else: raise RuntimeError(""Unknown for branch"") for item in branch or (): self.sym_visitor.visit(item)",False,if node . test is not None :,"elif for_branch == ""test_if"" :",0.8820016898747209,4.456882760699063
def contains_only_whitespace(node): if is_tag(node): <mask>: if not any([unicode(s).strip() for s in node.contents]): return True return False,False,if not any ( [ not is_text ( s ) for s in node . contents ] ) :,"if node . type == ""text"" :",0.8820016898747209,3.5126788039742776
"def dir_tag_click(event): mouse_index = self.path_bar.index(""@%d,%d"" % (event.x, event.y)) lineno = int(float(mouse_index)) if lineno == 1: self.request_focus_into("""") else: assert lineno == 2 dir_range = get_dir_range(event) if dir_range: _, end_index = dir_range path = self.path_bar.get(""2.0"", end_index) <mask>: path += ""\\"" self.request_focus_into(path)",False,"if path . endswith ( "":"" ) :",if path :,0.8820016898747209,0.0
"def validate_employee_id(self): if self.employee: sales_person = frappe.db.get_value(""Sales Person"", {""employee"": self.employee}) <mask>: frappe.throw( _(""Another Sales Person {0} exists with the same Employee id"").format( sales_person ) )",False,if sales_person and sales_person != self . name :,if sales_person :,0.8820016898747209,11.688396478408103
"def pytest_collection_modifyitems(items): for item in items: if item.nodeid.startswith(""tests/infer""): if ""stage"" not in item.keywords: item.add_marker(pytest.mark.stage(""unit"")) <mask>: item.add_marker(pytest.mark.init(rng_seed=123))",False,"if ""init"" not in item . keywords :","elif ""init"" not in item . keywords :",0.8820016898747209,88.01117367933934
"def poll(self, timeout): if timeout < 0: timeout = None # kqueue behaviour events = self._kqueue.control(None, KqueueLoop.MAX_EVENTS, timeout) results = defaultdict(lambda: POLL_NULL) for e in events: fd = e.ident if e.filter == select.KQ_FILTER_READ: results[fd] |= POLL_IN <mask>: results[fd] |= POLL_OUT return results.items()",True,elif e . filter == select . KQ_FILTER_WRITE :,elif e . filter == select . KQ_FILTER_WRITE :,0.8820016898747209,100.00000000000004
"def _read_dimensions(self, *dimnames, **kwargs): path = kwargs.get(""path"", ""/"") try: <mask>: return [self.rootgrp.dimensions[dname] for dname in dimnames] group = self.path2group[path] return [group.dimensions[dname] for dname in dimnames] except KeyError: raise self.Error( ""In file %s:\nError while reading dimensions: `%s` with kwargs: `%s`"" % (self.path, dimnames, kwargs) )",False,"if path == ""/"" :",if path in self . path2group :,0.8820016898747209,12.600736402830258
"def spam_to_me(address): sock = eventlet.connect(address) while True: try: sock.sendall(b""hello world"") # Arbitrary delay to not use all available CPU, keeps the test # running quickly and reliably under a second time.sleep(0.001) except socket.error as e: <mask>: return raise",False,if get_errno ( e ) == errno . EPIPE :,if e . errno == errno . EINTR :,0.8820016898747209,23.142716255858215
"def has_hash_of(self, destpath, code, package_level): """"""Determine if a file has the hash of the code."""""" if destpath is not None and os.path.isfile(destpath): with univ_open(destpath, ""r"") as opened: compiled = readfile(opened) hashash = gethash(compiled) <mask>: return True return False",False,"if hashash is not None and hashash == self . comp . genhash ( code , package_level ) :",if hashash == code :,0.8820016898747209,2.7629077445603603
"def insert(self, index, item): if len(self.lists) == 1: self.lists[0].insert(index, item) self._balance_list(0) else: list_idx, rel_idx = self._translate_index(index) <mask>: raise IndexError() self.lists[list_idx].insert(rel_idx, item) self._balance_list(list_idx) return",False,if list_idx is None :,if list_idx == - 1 :,0.8820016898747209,31.55984539112946
"def _parse_class_simplified(symbol): results = {} name = symbol.name + ""("" name += "", "".join([analyzer.expand_attribute(base) for base in symbol.bases]) name += "")"" for sym in symbol.body: if isinstance(sym, ast.FunctionDef): result = _parse_function_simplified(sym, symbol.name) results.update(result) <mask>: result = _parse_class_simplified(sym) results.update(result) lineno = symbol.lineno for decorator in symbol.decorator_list: lineno += 1 results[lineno] = (name, ""c"") return results",True,"elif isinstance ( sym , ast . ClassDef ) :","elif isinstance ( sym , ast . ClassDef ) :",0.8820016898747209,100.00000000000004
"def append_vars(pairs, result): for name, value in sorted(pairs.items()): if isinstance(value, list): value = ""[%s]"" % "","".join(value) <mask>: result.append(""%s:%s=%s"" % (package, name, value)) else: result.append(""%s=%s"" % (name, value))",False,if package :,"elif isinstance ( value , str ) :",0.8820016898747209,5.522397783539471
"def nextEditable(self): """"""Moves focus of the cursor to the next editable window"""""" if self.currentEditable is None: <mask>: self._currentEditableRef = self._editableChildren[0] else: for ref in weakref.getweakrefs(self.currentEditable): if ref in self._editableChildren: cei = self._editableChildren.index(ref) nei = cei + 1 if nei >= len(self._editableChildren): nei = 0 self._currentEditableRef = self._editableChildren[nei] return self.currentEditable",False,if len ( self . _editableChildren ) :,if len ( self . _editableChildren ) == 1 :,0.8820016898747209,63.15552371794033
"def everythingIsUnicode(d): """"""Takes a dictionary, recursively verifies that every value is unicode"""""" for k, v in d.iteritems(): if isinstance(v, dict) and k != ""headers"": if not everythingIsUnicode(v): return False elif isinstance(v, list): for i in v: if isinstance(i, dict) and not everythingIsUnicode(i): return False elif isinstance(i, _bytes): return False <mask>: return False return True",False,"elif isinstance ( v , _bytes ) :","elif isinstance ( i , unicode ) :",0.8820016898747209,23.87517132417733
"def is_valid(sample): if sample is None: return False if isinstance(sample, tuple): for s in sample: if s is None: return False <mask>: return False elif isinstance(s, collections.abc.Sequence) and len(s) == 0: return False return True",False,"elif isinstance ( s , np . ndarray ) and s . size == 0 :","elif isinstance ( s , collections . abc . Mapping ) and s . __class__ . __name__ == ""Sequence"" :",0.8820016898747209,21.31922460941227
"def scan_resource_conf(self, conf): if ""properties"" in conf: if ""attributes"" in conf[""properties""]: if ""exp"" in conf[""properties""][""attributes""]: <mask>: return CheckResult.PASSED return CheckResult.FAILED",False,"if conf [ ""properties"" ] [ ""attributes"" ] [ ""exp"" ] :","if ""exp"" in conf [ ""properties"" ] [ ""attributes"" ] [ ""exp"" ] :",0.8820016898747209,76.6119563549595
"def encode(self): if self.expr in gpregs.expr: self.value = gpregs.expr.index(self.expr) self.parent.rot2.value = 0 elif isinstance(self.expr, ExprOp) and self.expr.op == allshifts[3]: reg, value = self.expr.args <mask>: return False self.value = gpregs.expr.index(reg) if not isinstance(value, ExprInt): return False value = int(value) if not value in [8, 16, 24]: return False self.parent.rot2.value = value // 8 return True",True,if reg not in gpregs . expr :,if reg not in gpregs . expr :,0.8820016898747209,100.00000000000004
"def validate_transaction_reference(self): bank_account = self.paid_to if self.payment_type == ""Receive"" else self.paid_from bank_account_type = frappe.db.get_value(""Account"", bank_account, ""account_type"") if bank_account_type == ""Bank"": <mask>: frappe.throw( _(""Reference No and Reference Date is mandatory for Bank transaction"") )",False,if not self . reference_no or not self . reference_date :,if self . reference_date is None :,0.8820016898747209,24.925978674400294
"def monad(self): if not self.cls_bl_idname: return None for monad in bpy.data.node_groups: if hasattr(monad, ""cls_bl_idname""): <mask>: return monad return None",True,if monad . cls_bl_idname == self . cls_bl_idname :,if monad . cls_bl_idname == self . cls_bl_idname :,0.8820016898747209,100.00000000000004
"def _create_mask(self, plen): mask = [] for i in range(16): if plen >= 8: mask.append(0xFF) <mask>: mask.append(0xFF >> (8 - plen) << (8 - plen)) else: mask.append(0x00) plen -= 8 return mask",False,elif plen > 0 :,elif plen < 8 :,0.8820016898747209,23.643540225079384
"def dataset_to_stream(dataset, input_name): """"""Takes a tf.Dataset and creates a numpy stream of ready batches."""""" # All input-pipeline processing should be on CPU. for example in fastmath.dataset_as_numpy(dataset): features = example[0] inp, out = features[input_name], example[1] mask = features[""mask""] if ""mask"" in features else None # Some accelerators don't handle uint8 well, cast to int. <mask>: inp = inp.astype(np.int32) if isinstance(out, np.uint8): out = out.astype(np.int32) yield (inp, out) if mask is None else (inp, out, mask)",True,"if isinstance ( inp , np . uint8 ) :","if isinstance ( inp , np . uint8 ) :",0.8820016898747209,100.00000000000004
"def _idle_redraw_cb(self): assert self._idle_redraw_src_id is not None queue = self._idle_redraw_queue if len(queue) > 0: bbox = queue.pop(0) <mask>: super(CanvasRenderer, self).queue_draw() else: super(CanvasRenderer, self).queue_draw_area(*bbox) if len(queue) == 0: self._idle_redraw_src_id = None return False return True",True,if bbox is None :,if bbox is None :,0.8820016898747209,100.00000000000004
"def mutated(self, indiv): """"""mutate some genes of the given individual"""""" res = indiv.copy() # to avoid having a child identical to one of the currentpopulation''' for i in range(self.numParameters): <mask>: if self.xBound is None: res[i] = indiv[i] + gauss(0, self.mutationStdDev) else: res[i] = max( min(indiv[i] + gauss(0, self.mutationStdDev), self.maxs[i]), self.mins[i], ) return res",False,if random ( ) < self . mutationProb :,if self . xBound is not None :,0.8820016898747209,11.59119922599073
"def _justifyDrawParaLine(tx, offset, extraspace, words, last=0): setXPos(tx, offset) text = b"" "".join(words) if last: # last one, left align tx._textOut(text, 1) else: nSpaces = len(words) - 1 <mask>: tx.setWordSpace(extraspace / float(nSpaces)) tx._textOut(text, 1) tx.setWordSpace(0) else: tx._textOut(text, 1) setXPos(tx, -offset) return offset",False,if nSpaces :,if nSpaces > 0 :,0.8820016898747209,23.643540225079384
"def _read_0(self, stream): r = b"""" while True: c = stream.read(2) <mask>: raise EOFError() if c == b""\x00\x00"": break r += c return r.decode(self.encoding)",False,if len ( c ) != 2 :,"if c == b""\x00"" :",0.8820016898747209,5.934202609760488
"def run(self, app, editor, args): line_nums = [] for cursor in editor.cursors: <mask>: line_nums.append(cursor.y) data = editor.lines[cursor.y].get_data().upper() editor.lines[cursor.y].set_data(data)",True,if cursor . y not in line_nums :,if cursor . y not in line_nums :,0.8820016898747209,100.00000000000004
"def create_default_energy_point_rules(): for rule in get_default_energy_point_rules(): # check if any rule for ref. doctype exists rule_exists = frappe.db.exists( ""Energy Point Rule"", {""reference_doctype"": rule.get(""reference_doctype"")} ) <mask>: continue doc = frappe.get_doc(rule) doc.insert(ignore_permissions=True)",True,if rule_exists :,if rule_exists :,0.8820016898747209,100.00000000000004
"def __new__(cls, *nodes): if not nodes: raise TypeError(""DisjunctionNode() requires at least one node"") elif len(nodes) == 1: return nodes[0] self = super(DisjunctionNode, cls).__new__(cls) self.__nodes = [] # TODO: Remove duplicates? for node in nodes: if not isinstance(node, Node): raise TypeError( ""DisjunctionNode() expects Node instances as arguments;"" "" received a non-Node instance %r"" % node ) <mask>: self.__nodes.extend(node.__nodes) else: self.__nodes.append(node) return self",False,"if isinstance ( node , DisjunctionNode ) :","if isinstance ( node , list ) :",0.8820016898747209,59.4603557501361
def dfs(v: str) -> Iterator[Set[str]]: index[v] = len(stack) stack.append(v) boundaries.append(index[v]) for w in edges[v]: if w not in index: yield from dfs(w) <mask>: while index[w] < boundaries[-1]: boundaries.pop() if boundaries[-1] == index[v]: boundaries.pop() scc = set(stack[index[v] :]) del stack[index[v] :] identified.update(scc) yield scc,False,elif w not in identified :,if boundaries [ - 1 ] < index [ v ] :,0.8820016898747209,3.3864985683445354
"def unpack_item_obj(map_uuid_global_id, misp_obj): obj_meta = get_object_metadata(misp_obj) obj_id = None io_content = None for attribute in misp_obj.attributes: <mask>: obj_id = attribute.value # # TODO: sanitize io_content = attribute.data # # TODO: check if type == io if obj_id and io_content: res = Item.create_item(obj_id, obj_meta, io_content) map_uuid_global_id[misp_obj.uuid] = get_global_id(""item"", obj_id)",False,"if attribute . object_relation == ""raw-data"" :","if attribute . type == ""id"" :",0.8820016898747209,24.437032551865375
"def parse(self, response): soup = BeautifulSoup(response.content.decode(""utf-8"", ""ignore""), ""lxml"") image_divs = soup.find_all(""div"", class_=""imgpt"") pattern = re.compile(r""murl\"":\""(.*?)\.jpg"") for div in image_divs: href_str = html_parser.HTMLParser().unescape(div.a[""m""]) match = pattern.search(href_str) <mask>: name = match.group(1) if six.PY3 else match.group(1).encode(""utf-8"") img_url = ""{}.jpg"".format(name) yield dict(file_url=img_url)",True,if match :,if match :,0.8820016898747209,0.0
"def filter_errors(self, errors: List[str]) -> List[str]: real_errors: List[str] = list() current_file = __file__ current_path = os.path.split(current_file) for line in errors: line = line.strip() if not line: continue fn, lno, lvl, msg = self.parse_trace_line(line) <mask>: _path = os.path.split(fn) if _path[-1] != current_path[-1]: continue real_errors.append(line) return real_errors",False,if fn is not None :,if fn :,0.8820016898747209,0.0
"def decompileFormat1(self, reader, otFont): self.classDefs = classDefs = [] startGlyphID = reader.readUShort() glyphCount = reader.readUShort() for i in range(glyphCount): glyphName = otFont.getglyphName(startGlyphID + i) classValue = reader.readUShort() <mask>: classDefs.append((glyphName, classValue))",False,if classValue :,if classValue != 0 :,0.8820016898747209,17.965205598154213
"def compress(self, data_list): if len(data_list) == 2: value, lookup_expr = data_list <mask>: if lookup_expr not in EMPTY_VALUES: return Lookup(value=value, lookup_expr=lookup_expr) else: raise forms.ValidationError( self.error_messages[""lookup_required""], code=""lookup_required"" ) return None",True,if value not in EMPTY_VALUES :,if value not in EMPTY_VALUES :,0.8820016898747209,100.00000000000004
"def open_compat(path, mode=""r""): if mode in [""r"", ""rb""] and not os.path.exists(path): raise FileNotFoundError(u'The file ""%s"" could not be found' % path) if sys.version_info >= (3,): encoding = ""utf-8"" errors = ""replace"" <mask>: encoding = None errors = None return open(path, mode, encoding=encoding, errors=errors) else: return open(path, mode)",False,"if mode in [ ""rb"" , ""wb"" , ""ab"" ] :","if sys . version_info >= ( 3 , ) :",0.8820016898747209,2.988662868962178
"def filter_errors(self, errors: List[str]) -> List[str]: real_errors: List[str] = list() current_file = __file__ current_path = os.path.split(current_file) for line in errors: line = line.strip() if not line: continue fn, lno, lvl, msg = self.parse_trace_line(line) if fn is not None: _path = os.path.split(fn) <mask>: continue real_errors.append(line) return real_errors",False,if _path [ - 1 ] != current_path [ - 1 ] :,if _path [ - 1 ] != current_path :,0.8820016898747209,68.41262339661338
"def filter_by_level(record, level_per_module): name = record[""name""] level = 0 if name in level_per_module: level = level_per_module[name] elif name is not None: lookup = """" if """" in level_per_module: level = level_per_module[""""] for n in name.split("".""): lookup += n <mask>: level = level_per_module[lookup] lookup += ""."" if level is False: return False return record[""level""].no >= level",False,if lookup in level_per_module :,elif lookup in level_per_module :,0.8820016898747209,86.33400213704509
"def CountButtons(self): """"""Returns the number of visible buttons in the docked pane."""""" n = 0 if self.HasCaption() or self.HasCaptionLeft(): if isinstance(wx.GetTopLevelParent(self.window), AuiFloatingFrame): return 1 if self.HasCloseButton(): n += 1 <mask>: n += 1 if self.HasMinimizeButton(): n += 1 if self.HasPinButton(): n += 1 return n",False,if self . HasMaximizeButton ( ) :,if self . HasOpenButton ( ) :,0.8820016898747209,41.11336169005196
"def search(a, b, desired): if a == b: return a if abs(b - a) < 0.005: ca = count(a) cb = count(b) dista = abs(desired - ca) distb = abs(desired - cb) <mask>: return a else: return b m = (a + b) / 2.0 cm = count(m) if desired < cm: return search(m, b, desired) else: return search(a, m, desired)",True,if dista < distb :,if dista < distb :,0.8820016898747209,100.00000000000004
"def force_ipv4(self, *args): """"""only ipv4 localhost in /etc/hosts"""""" logg.debug(""checking /etc/hosts for '::1 localhost'"") lines = [] for line in open(self.etc_hosts()): <mask>: newline = re.sub(""\\slocalhost\\s"", "" "", line) if line != newline: logg.info(""/etc/hosts: '%s' => '%s'"", line.rstrip(), newline.rstrip()) line = newline lines.append(line) f = open(self.etc_hosts(), ""w"") for line in lines: f.write(line) f.close()",False,"if ""::1"" in line :","if line . startswith ( ""localhost"" ) :",0.8820016898747209,6.27465531099474
"def aiter_cogs(cls) -> AsyncIterator[Tuple[str, str]]: yield ""Core"", ""0"" for _dir in data_manager.cog_data_path().iterdir(): fpath = _dir / ""settings.json"" <mask>: continue with fpath.open() as f: try: data = json.load(f) except json.JSONDecodeError: continue if not isinstance(data, dict): continue cog_name = _dir.stem for cog_id, inner in data.items(): if not isinstance(inner, dict): continue yield cog_name, cog_id",True,if not fpath . exists ( ) :,if not fpath . exists ( ) :,0.8820016898747209,100.00000000000004
"def _get_dbutils(): try: import IPython ip_shell = IPython.get_ipython() <mask>: raise _NoDbutilsError return ip_shell.ns_table[""user_global""][""dbutils""] except ImportError: raise _NoDbutilsError except KeyError: raise _NoDbutilsError",False,if ip_shell is None :,"if ip_shell . ns_table [ ""user_global"" ] [ ""dbutils"" ] is None :",0.8820016898747209,14.90896080339584
"def _bytecode_filenames(self, py_filenames): bytecode_files = [] for py_file in py_filenames: # Since build_py handles package data installation, the # list of outputs can contain more than just .py files. # Make sure we only report bytecode for the .py files. ext = os.path.splitext(os.path.normcase(py_file))[1] <mask>: continue if self.compile: bytecode_files.append(py_file + ""c"") if self.optimize > 0: bytecode_files.append(py_file + ""o"") return bytecode_files",False,if ext != PYTHON_SOURCE_EXTENSION :,"if ext == "".py"" :",0.8820016898747209,10.147104008451905
"def compute_distances_mu(line, pts, result, gates, tolerance): """"""calculate all distances with mathuutils"""""" line_origin = V(line[0]) line_end = V(line[-1]) local_result = [[], [], [], [], []] for point in pts: data = compute_distance(V(point), line_origin, line_end, tolerance) for i, res in enumerate(local_result): res.append(data[i]) for i, res in enumerate(result): <mask>: res.append(local_result[i])",False,if gates [ i ] :,if i < gates :,0.8820016898747209,12.368464772045972
"def _get_next_segment(self, segment_path, page_size, segment_cursor=None): if segment_path: <mask>: return None return Segment(self.client, segment_path, page_size, segment_cursor) return None",False,if self . end_time and self . _is_later_than_end_time ( segment_path ) :,if not self . client . get_segment ( segment_path ) :,0.8820016898747209,20.244901473881153
"def _check_number_of_sessions(): nb_desktop_sessions = sessions.get_number_of_desktop_sessions(ignore_gdm=True) if nb_desktop_sessions > 1: print( ""WARNING : There are %d other desktop sessions open. The GPU switch will not become effective until you have manually"" "" logged out from ALL desktop sessions.\n"" ""Continue ? (y/N)"" % (nb_desktop_sessions - 1) ) confirmation = ask_confirmation() <mask>: sys.exit(0)",False,if not confirmation :,if confirmation is None :,0.8820016898747209,14.058533129758727
"def delete_compute_environment(self, compute_environment_name): if compute_environment_name is None: raise InvalidParameterValueException(""Missing computeEnvironment parameter"") compute_env = self.get_compute_environment(compute_environment_name) if compute_env is not None: # Pop ComputeEnvironment self._compute_environments.pop(compute_env.arn) # Delete ECS cluster self.ecs_backend.delete_cluster(compute_env.ecs_name) <mask>: # Delete compute environment instance_ids = [instance.id for instance in compute_env.instances] self.ec2_backend.terminate_instances(instance_ids)",False,"if compute_env . env_type == ""MANAGED"" :",if compute_env . instances is not None :,0.8820016898747209,27.559110500755533
"def run(self): results = {} for func_name in [ # Execute every function starting with check_* fn for fn in self.check_functions # if the user does not specify any name if not self.args.get(""check"") # of if specify the current function name or self.args.get(""check"") == fn ]: function = getattr(self, func_name) log.warn(function.__doc__) result = function() <mask>: log.info(""\n"".join(result)) results.update({func_name: result}) return results",True,if result :,if result :,0.8820016898747209,0.0
"def invalidate(self, layers=None): if layers is None: layers = Layer.AllLayers if layers: layers = set(layers) self.invalidLayers.update(layers) blockRenderers = [ br for br in self.blockRenderers <mask>: ] if len(blockRenderers) < len(self.blockRenderers): self.forgetDisplayLists() self.blockRenderers = blockRenderers if self.renderer.showRedraw and Layer.Blocks in layers: self.needsRedisplay = True",False,if br . layer is Layer . Blocks or br . layer not in layers,if br . isDisplayList ( ),0.8820016898747209,6.741599762807414
"def get_library_dirs(platform, arch=None): if platform == ""win32"": jre_home = get_jre_home(platform) jdk_home = JAVA_HOME <mask>: jre_home = jre_home.decode(""utf-8"") return [join(jdk_home, ""lib""), join(jdk_home, ""bin"", ""server"")] elif platform == ""android"": return [""libs/{}"".format(arch)] return []",True,"if isinstance ( jre_home , bytes ) :","if isinstance ( jre_home , bytes ) :",0.8820016898747209,100.00000000000004
"def save_plugin_options(self): for name, option_widgets in self._plugin_option_widgets.items(): <mask>: self.config[""plugins""][name] = {} plugin_config = self.config[""plugins""][ name ] # use or instead of get incase the value is actually None for option_name, option_widget in option_widgets.items(): plugin_config[option_name] = option_widget.option.get_widget_value( option_widget.widget )",True,"if name not in self . config [ ""plugins"" ] :","if name not in self . config [ ""plugins"" ] :",0.8820016898747209,100.00000000000004
"def _select_block(str_in, start_tag, end_tag): """"""Select first block delimited by start_tag and end_tag"""""" start_pos = str_in.find(start_tag) if start_pos < 0: raise ValueError(""start_tag not found"") depth = 0 for pos in range(start_pos, len(str_in)): if str_in[pos] == start_tag: depth += 1 elif str_in[pos] == end_tag: depth -= 1 <mask>: break sel = str_in[start_pos + 1 : pos] return sel",False,if depth == 0 :,elif depth == 0 :,0.8820016898747209,75.98356856515926
"def _coerce_to_bool(self, node, var, true_val=True): """"""Coerce the values in a variable to bools."""""" bool_var = self.program.NewVariable() for b in var.bindings: v = b.data if isinstance(v, mixin.PythonConstant) and isinstance(v.pyval, bool): const = v.pyval is true_val elif not compare.compatible_with(v, True): const = not true_val <mask>: const = true_val else: const = None bool_var.AddBinding(self.convert.bool_values[const], {b}, node) return bool_var",False,"elif not compare . compatible_with ( v , False ) :","elif compare . compatible_with ( v , False ) :",0.8820016898747209,84.96364166597652
def multiline_indentation(self): if self._multiline_indentation is None: offset = 0 <mask>: offset = 2 indentation = make_indentation(3 * self.indent_size + offset) self._multiline_indentation = indentation if self.current_rule: indent_extra = make_indentation(self.indent_size) return self._multiline_indentation + indent_extra return self._multiline_indentation,False,if self . show_aligned_keywords :,if self . current_rule :,0.8820016898747209,20.873176328735713
"def __call__(self, event, data=None): datatype, delta = event self.midi_ctrl.delta += delta if TIMING_CLOCK in datatype and not self.played: self.midi_ctrl.pulse += 1 <mask>: t_master = 60.0 self.midi_ctrl.bpm = round(60.0 / self.midi_ctrl.delta, 0) self.midi_ctrl.pulse = 0 self.midi_ctrl.delta = 0.0",False,if self . midi_ctrl . pulse == self . midi_ctrl . ppqn :,if self . midi_ctrl . pulse > 60.0 :,0.8820016898747209,36.988348418257935
"def handle_sent(self, elt): sent = [] for child in elt: <mask>: itm = self.handle_word(child) if self._unit == ""word"": sent.extend(itm) else: sent.append(itm) else: raise ValueError(""Unexpected element %s"" % child.tag) return SemcorSentence(elt.attrib[""snum""], sent)",False,"if child . tag in ( ""wf"" , ""punc"" ) :","if child . tag == ""sentence"" :",0.8820016898747209,18.32556812998321
"def _handle_def_errors(testdef): # If the test generation had an error, raise if testdef.error: <mask>: if isinstance(testdef.exception, Exception): raise testdef.exception else: raise Exception(testdef.exception) else: raise Exception(""Test parse failure"")",False,if testdef . exception :,"if isinstance ( testdef . exception , TestParseFailure ) :",0.8820016898747209,17.747405280050266
"def _authorized_sid(self, jid, sid, ifrom, iq): with self._preauthed_sids_lock: <mask>: del self._preauthed_sids[(jid, sid, ifrom)] return True return False",True,"if ( jid , sid , ifrom ) in self . _preauthed_sids :","if ( jid , sid , ifrom ) in self . _preauthed_sids :",0.8820016898747209,100.00000000000004
"def wait(self, timeout=None): if self.returncode is None: <mask>: msecs = _subprocess.INFINITE else: msecs = max(0, int(timeout * 1000 + 0.5)) res = _subprocess.WaitForSingleObject(int(self._handle), msecs) if res == _subprocess.WAIT_OBJECT_0: code = _subprocess.GetExitCodeProcess(self._handle) if code == TERMINATE: code = -signal.SIGTERM self.returncode = code return self.returncode",True,if timeout is None :,if timeout is None :,0.8820016898747209,100.00000000000004
"def _gen_legal_y_s_t(self): while True: y = self._gen_random_scalar() s = self.tec_arithmetic.mul( scalar=y, a=self.tec_arithmetic.get_generator() ) # S = yG t = self._hash_tec_element(s) <mask>: # Both S and T are legal LOGGER.info(""randomly generated y, S, T"") return y, s, t",False,if self . tec_arithmetic . is_in_group ( s ) and type ( t ) != int :,if t != s :,0.8820016898747209,1.063668370007461
"def write_out(): while True: <mask>: time.sleep(0.1) continue data_str = self.instrument_queue.get() data_str = data_str.splitlines() tb.write("""") # position cursor to end for line in data_str: tb.write(line) tb.write(""\n"")",False,if self . instrument_queue . empty ( ) :,if self . instrument_queue is None :,0.8820016898747209,48.88290318657944
"def _parse_preamble(self): """"""Parse metadata about query (PRIVATE)."""""" meta = {} while self.line: regx = re.search(_RE_QUERY, self.line) if regx: self.query_id = regx.group(1) <mask>: self.seq_len = int(self.line.strip().split()[1]) self.line = self.handle.readline().strip() return meta",False,"if self . line . startswith ( ""Match_columns"" ) :","if self . query_id == ""PRIVATE"" :",0.8820016898747209,13.188274750399428
"def init_sequence(self, coll_name, seq_config): if not isinstance(seq_config, list): raise Exception('""sequence"" config must be a list') handlers = [] for entry in seq_config: <mask>: raise Exception('""sequence"" entry must be a dict') name = entry.get(""name"", """") handler = self.load_coll(name, entry) handlers.append(handler) return HandlerSeq(handlers)",True,"if not isinstance ( entry , dict ) :","if not isinstance ( entry , dict ) :",0.8820016898747209,100.00000000000004
"def change_args_to_dict(string): if string is None: return None ans = [] strings = string.split(""\n"") ind = 1 start = 0 while ind <= len(strings): if ind < len(strings) and strings[ind].startswith("" ""): ind += 1 else: <mask>: ans.append(""\n"".join(strings[start:ind])) start = ind ind += 1 d = {} for line in ans: if "":"" in line and len(line) > 0: lines = line.split("":"") d[lines[0]] = lines[1].strip() return d",False,if start < ind :,if start < len ( strings ) :,0.8820016898747209,22.089591134157878
"def wait(self): while True: return_code = self._process.poll() if return_code is not None: line = self._process.stdout.readline().decode(""utf-8"") <mask>: break log.debug(line.strip(""\n"")) return True",False,"if line == """" :",if not line :,0.8820016898747209,9.930283522141846
"def __getattr__(self, key): for tag in self.tag.children: <mask>: continue if ""name"" in tag.attrs and tag.attrs[""name""] in (key,): from thug.DOM.W3C.Core.DOMImplementation import DOMImplementation return DOMImplementation.createHTMLElement(self.doc, tag) raise AttributeError",False,"if tag . name not in ( ""input"" , ) :","if tag . tag_type != ""tag"" :",0.8820016898747209,13.792484215432934
"def compare_hash(hash_of_gold, path_to_file): with open(path_to_file, ""rb"") as f: hash_of_file = hashlib.sha256(f.read()).hexdigest() <mask>: print( ""########## Hash sum of"", path_to_file, ""differs from the target, the topology will be deleted !!! ##########"", ) shutil.rmtree(os.path.dirname(path_to_file))",False,if hash_of_file != hash_of_gold :,if hash_of_gold != hash_of_file :,0.8820016898747209,79.71755824799465
def on_completed2(): doner[0] = True if not qr: <mask>: observer.on_next(False) observer.on_completed() elif donel[0]: observer.on_next(True) observer.on_completed(),False,if len ( ql ) > 0 :,if donel [ 1 ] :,0.8820016898747209,6.916271812933183
"def get_other(self, data, items): is_tuple = False if type(data) == tuple: data = list(data) is_tuple = True if type(data) == list: m_items = items.copy() for idx, item in enumerate(items): <mask>: m_items[idx] = len(data) - abs(item) for i in sorted(set(m_items), reverse=True): if i < len(data) and i > -1: del data[i] if is_tuple: return tuple(data) else: return data else: return None",False,if item < 0 :,if abs ( item ) < abs ( data ) :,0.8820016898747209,5.300156689756295
"def _open_url(cls, url): if config.browser: cmd = [config.browser, url] <mask>: print(""running command: %s"" % "" "".join(cmd)) p = Popen(cmd) p.communicate() else: if not config.quiet: print(""opening URL in browser: %s"" % url) webbrowser.open_new(url)",True,if not config . quiet :,if not config . quiet :,0.8820016898747209,100.00000000000004
"def setLabel(self, s, protect=False): """"""Set the label of the minibuffer."""""" c, k, w = self.c, self, self.w if w: # Support for the curses gui. if hasattr(g.app.gui, ""set_minibuffer_label""): g.app.gui.set_minibuffer_label(c, s) w.setAllText(s) n = len(s) w.setSelectionRange(n, n, insert=n) <mask>: k.mb_prefix = s",True,if protect :,if protect :,0.8820016898747209,0.0
"def __init__(self, path): self.symcaches = [] for path in path.split("";""): if os.path.isdir(path): self.symcaches.append(SymbolCache(dirname=path)) continue <mask>: import cobra self.symcaches.append(cobra.CobraProxy(path)) continue",False,"if path . startswith ( ""cobra://"" ) or path . startswith ( ""cobrassl://"" ) :",if os . path . isfile ( path ) :,0.8820016898747209,3.1492572798507688
"def init_params(net): """"""Init layer parameters."""""" for module in net.modules(): if isinstance(module, nn.Conv2d): init.kaiming_normal(module.weight, mode=""fan_out"") if module.bias: init.constant(module.bias, 0) <mask>: init.constant(module.weight, 1) init.constant(module.bias, 0) elif isinstance(module, nn.Linear): init.normal(module.weight, std=1e-3) if module.bias: init.constant(module.bias, 0)",False,"elif isinstance ( module , nn . BatchNorm2d ) :","elif isinstance ( module , nn . Conv2d ) :",0.8820016898747209,70.71067811865478
"def _diff_dict(self, old, new): diff = {} removed = [] added = [] for key, value in old.items(): <mask>: removed.append(key) elif old[key] != new[key]: # modified is indicated by a remove and add removed.append(key) added.append(key) for key, value in new.items(): if key not in old: added.append(key) if removed: diff[""removed""] = sorted(removed) if added: diff[""added""] = sorted(added) return diff",False,if key not in new :,if key in new :,0.8820016898747209,40.93653765389909
"def __init__(self, *args, **kwargs): _kwargs = { ""max_length"": 20, ""widget"": forms.TextInput(attrs={""autocomplete"": ""off""}), ""label"": _(""Card number""), } if ""types"" in kwargs: self.accepted_cards = set(kwargs.pop(""types"")) difference = self.accepted_cards - VALID_CARDS <mask>: raise ImproperlyConfigured( ""The following accepted_cards are "" ""unknown: %s"" % difference ) _kwargs.update(kwargs) super().__init__(*args, **_kwargs)",False,if difference :,if difference != VALID_CARDS :,0.8820016898747209,12.22307556087252
"def dumps(self): sections = [] for name, env_info in self._dependencies_.items(): sections.append(""[ENV_%s]"" % name) for var, values in sorted(env_info.vars.items()): tmp = ""%s="" % var <mask>: tmp += ""[%s]"" % "","".join(['""%s""' % val for val in values]) else: tmp += ""%s"" % values sections.append(tmp) return ""\n"".join(sections)",True,"if isinstance ( values , list ) :","if isinstance ( values , list ) :",0.8820016898747209,100.00000000000004
"def air_quality(self): aqi_data = self._get_aqi_data() if aqi_data: if aqi_data.get(""status"") == ""ok"": aqi_data = self._organize(aqi_data) aqi_data = self._manipulate(aqi_data) <mask>: self.py3.error(aqi_data.get(""data"")) return { ""cached_until"": self.py3.time_in(self.cache_timeout), ""full_text"": self.py3.safe_format(self.format, aqi_data), }",False,"elif aqi_data . get ( ""status"" ) == ""error"" :","if ""data"" in aqi_data :",0.8820016898747209,8.676664929103568
"def _blend(x, y): # pylint: disable=invalid-name """"""Implements the ""blend"" strategy for `deep_merge`."""""" if isinstance(x, (dict, OrderedDict)): if not isinstance(y, (dict, OrderedDict)): return y return _merge(x, y, recursion_func=_blend) if isinstance(x, (list, tuple)): <mask>: return y result = [_blend(*i) for i in zip(x, y)] if len(x) > len(y): result += x[len(y) :] elif len(x) < len(y): result += y[len(x) :] return result return y",False,"if not isinstance ( y , ( list , tuple ) ) :","if not isinstance ( y , list ) :",0.8820016898747209,43.624306402227546
"def _rate(cls, sample1, sample2): ""Simple rate"" try: interval = sample2[0] - sample1[0] <mask>: raise Infinity() delta = sample2[1] - sample1[1] if delta < 0: raise UnknownValue() return (sample2[0], delta / interval, sample2[2], sample2[3]) except Infinity: raise except UnknownValue: raise except Exception as e: raise NaN(e)",False,if interval == 0 :,if interval < 0 :,0.8820016898747209,24.736929544091932
"def wrapped_request_method(*args, **kwargs): """"""Modifies HTTP headers to include a specified user-agent."""""" if kwargs.get(""headers"") is not None: <mask>: if user_agent not in kwargs[""headers""][""user-agent""]: # Save the existing user-agent header and tack on our own. kwargs[""headers""][""user-agent""] = ( f""{user_agent} "" f'{kwargs[""headers""][""user-agent""]}' ) else: kwargs[""headers""][""user-agent""] = user_agent else: kwargs[""headers""] = {""user-agent"": user_agent} return request_method(*args, **kwargs)",False,"if kwargs [ ""headers"" ] . get ( ""user-agent"" ) :","if ""user-agent"" in kwargs :",0.8820016898747209,9.271103732443692
"def remove_addons(auth, resource_object_list): for config in AbstractNode.ADDONS_AVAILABLE: try: settings_model = config.node_settings except LookupError: settings_model = None <mask>: addon_list = settings_model.objects.filter( owner__in=resource_object_list, is_deleted=False ) for addon in addon_list: addon.after_delete(auth.user)",True,if settings_model :,if settings_model :,0.8820016898747209,100.00000000000004
"def Decorator(*args, **kwargs): delay = 0.2 num_attempts = 15 cur_attempt = 0 while True: try: return f(*args, **kwargs) except exceptions.WebDriverException as e: logging.warning(""Selenium raised %s"", utils.SmartUnicode(e)) cur_attempt += 1 <mask>: raise time.sleep(delay)",False,if cur_attempt == num_attempts :,if cur_attempt >= num_attempts :,0.8820016898747209,65.80370064762461
"def _cleanup_parts_dir(parts_dir, local_plugins_dir, parts): if os.path.exists(parts_dir): logger.info(""Cleaning up parts directory"") for subdirectory in os.listdir(parts_dir): path = os.path.join(parts_dir, subdirectory) <mask>: try: shutil.rmtree(path) except NotADirectoryError: os.remove(path) for part in parts: part.mark_cleaned(steps.BUILD) part.mark_cleaned(steps.PULL)",False,if path != local_plugins_dir :,if os . path . exists ( path ) :,0.8820016898747209,5.522397783539471
"def traverse_trees(node_pos, sample, trees: List[HeteroDecisionTreeGuest]): if node_pos[""reach_leaf_node""].all(): return node_pos for t_idx, tree in enumerate(trees): cur_node_idx = node_pos[""node_pos""][t_idx] # reach leaf <mask>: continue rs, reach_leaf = HeteroSecureBoostingTreeGuest.traverse_a_tree( tree, sample, cur_node_idx ) if reach_leaf: node_pos[""reach_leaf_node""][t_idx] = True node_pos[""node_pos""][t_idx] = rs return node_pos",False,if cur_node_idx == - 1 :,if cur_node_idx == 0 :,0.8820016898747209,70.80735452207037
"def get_measurements(self, pipeline, object_name, category): if self.get_categories(pipeline, object_name) == [category]: results = [] if self.do_corr_and_slope: if object_name == ""Image"": results += [""Correlation"", ""Slope""] else: results += [""Correlation""] if self.do_overlap: results += [""Overlap"", ""K""] <mask>: results += [""Manders""] if self.do_rwc: results += [""RWC""] if self.do_costes: results += [""Costes""] return results return []",True,if self . do_manders :,if self . do_manders :,0.8820016898747209,100.00000000000004
"def create_connection(self, infos, f2, laddr_infos, protocol): for family in infos: try: <mask>: for laddr in laddr_infos: try: break except OSError: protocol = ""foo"" else: continue except OSError: protocol = ""bar"" else: break else: raise return protocol",False,if f2 :,if family == f2 :,0.8820016898747209,17.965205598154213
"def app_middleware(next, root, info, **kwargs): app_auth_header = ""HTTP_AUTHORIZATION"" prefix = ""bearer"" request = info.context if request.path == API_PATH: if not hasattr(request, ""app""): request.app = None auth = request.META.get(app_auth_header, """").split() if len(auth) == 2: auth_prefix, auth_token = auth <mask>: request.app = SimpleLazyObject(lambda: get_app(auth_token)) return next(root, info, **kwargs)",False,if auth_prefix . lower ( ) == prefix :,if auth_prefix == prefix :,0.8820016898747209,41.938051117049184
"def when(self, matches, context): ret = [] for episode in matches.named(""episode"", lambda match: len(match.initiator) == 1): group = matches.markers.at_match( episode, lambda marker: marker.name == ""group"", index=0 ) <mask>: if not matches.range( *group.span, predicate=lambda match: match.name == ""title"" ): ret.append(episode) return ret",True,if group :,if group :,0.8820016898747209,0.0
def locate_via_pep514(spec): with _PY_LOCK: if not _PY_AVAILABLE: from . import pep514 _PY_AVAILABLE.extend(pep514.discover_pythons()) _PY_AVAILABLE.append(CURRENT) for cur_spec in _PY_AVAILABLE: <mask>: return cur_spec.path,False,if cur_spec . satisfies ( spec ) :,if cur_spec . spec == spec :,0.8820016898747209,42.7287006396234
"def setCorkImageDefault(self): if settings.corkBackground[""image""] != """": i = self.cmbCorkImage.findData(settings.corkBackground[""image""]) <mask>: self.cmbCorkImage.setCurrentIndex(i)",True,if i != - 1 :,if i != - 1 :,0.8820016898747209,100.00000000000004
"def _split_key(key): if isinstance(key, util.string_types): # coerce fooload('*') into ""default loader strategy"" if key == _WILDCARD_TOKEN: return (_DEFAULT_TOKEN,) # coerce fooload("".*"") into ""wildcard on default entity"" <mask>: key = key[1:] return key.split(""."") else: return (key,)",False,"elif key . startswith ( ""."" + _WILDCARD_TOKEN ) :","if key . startswith ( ""*"" ) :",0.8820016898747209,28.333350580831056
"def detach_volume(self, volume): # We need to find the node using this volume for node in self.list_nodes(): <mask>: # This node has only one associated image. It is not the one we # are after. continue for disk in node.image: if disk.id == volume.id: # Node found. We can now detach the volume disk_id = disk.extra[""disk_id""] return self._do_detach_volume(node.id, disk_id) return False",False,if type ( node . image ) is not list :,if node . image is None :,0.8820016898747209,16.417223692914014
"def create(self, private=False): try: <mask>: log.info(""Creating private channel %s."", self) self._bot.api_call( ""conversations.create"", data={""name"": self.name, ""is_private"": True} ) else: log.info(""Creating channel %s."", self) self._bot.api_call(""conversations.create"", data={""name"": self.name}) except SlackAPIResponseError as e: if e.error == ""user_is_bot"": raise RoomError(f""Unable to create channel. {USER_IS_BOT_HELPTEXT}"") else: raise RoomError(e)",True,if private :,if private :,0.8820016898747209,0.0
"def test_dataset_has_valid_etag(self, dataset_name): py_script_path = list(filter(lambda x: x, dataset_name.split(""/"")))[-1] + "".py"" dataset_url = hf_bucket_url(dataset_name, filename=py_script_path, dataset=True) etag = None try: response = requests.head( dataset_url, allow_redirects=True, proxies=None, timeout=10 ) <mask>: etag = response.headers.get(""Etag"") except (EnvironmentError, requests.exceptions.Timeout): pass self.assertIsNotNone(etag)",True,if response . status_code == 200 :,if response . status_code == 200 :,0.8820016898747209,100.00000000000004
"def set_dir_modes(self, dirname, mode): if not self.is_chmod_supported(): return for dirpath, dirnames, fnames in os.walk(dirname): if os.path.islink(dirpath): continue log.info(""changing mode of %s to %o"", dirpath, mode) <mask>: os.chmod(dirpath, mode)",False,if not self . dry_run :,if os . path . isdir ( dirpath ) and mode != os . S_IWUSR :,0.8820016898747209,2.8629993657668873
"def _clean(self): logger.info(""Cleaning up..."") if self._process is not None: <mask>: for _ in range(3): self._process.terminate() time.sleep(0.5) if self._process.poll() is not None: break else: self._process.kill() self._process.wait() logger.error(""KILLED"") if os.path.exists(self._tmp_dir): shutil.rmtree(self._tmp_dir) self._process = None self._ws = None logger.info(""Cleanup complete"")",False,if self . _process . poll ( ) is None :,if self . _process . returncode == 0 :,0.8820016898747209,43.817713423777185
"def iter_chars_to_words(self, chars): current_word = [] for char in chars: if not self.keep_blank_chars and char[""text""].isspace(): <mask>: yield current_word current_word = [] elif current_word and self.char_begins_new_word(current_word, char): yield current_word current_word = [char] else: current_word.append(char) if current_word: yield current_word",False,if current_word :,"if current_word and self . char_begins_new_word ( current_word , char ) :",0.8820016898747209,11.433361115787452
"def _lookup(components, specs, provided, name, i, l): if i < l: for spec in specs[i].__sro__: comps = components.get(spec) if comps: r = _lookup(comps, specs, provided, name, i + 1, l) <mask>: return r else: for iface in provided: comps = components.get(iface) if comps: r = comps.get(name) if r is not None: return r return None",True,if r is not None :,if r is not None :,0.8820016898747209,100.00000000000004
"def run(cmd, task=None): process = subprocess.Popen( cmd, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, close_fds=True ) output_lines = [] while True: line = process.stdout.readline() <mask>: break line = line.decode(""utf-8"") output_lines += [line] logger.info(line.rstrip(""\n"")) process.stdout.close() exit_code = process.wait() if exit_code: output = """".join(output_lines) raise subprocess.CalledProcessError(exit_code, cmd, output=output)",True,if not line :,if not line :,0.8820016898747209,100.00000000000004
"def process_response(self, request, response): if ( response.status_code == 404 and request.path_info.endswith(""/"") and not is_valid_path(request.path_info) and is_valid_path(request.path_info[:-1]) ): # Use request.path because we munged app/locale in path_info. newurl = request.path[:-1] <mask>: with safe_query_string(request): newurl += ""?"" + request.META[""QUERY_STRING""] return HttpResponsePermanentRedirect(newurl) return response",False,if request . GET :,"if request . META . get ( ""QUERY_STRING"" ) :",0.8820016898747209,11.359354890271161
"def dependencies(self): deps = [] midx = None if self.ref is not None: query = TypeQuery(self.ref) super = query.execute(self.schema) if super is None: log.debug(self.schema) raise TypeNotFound(self.ref) <mask>: deps.append(super) midx = 0 return (midx, deps)",False,if not super . builtin ( ) :,if super . is_dependency ( ) :,0.8820016898747209,23.356898886410015
"def _get_vtkjs(self): if self._vtkjs is None and self.object is not None: if isinstance(self.object, string_types) and self.object.endswith("".vtkjs""): <mask>: with open(self.object, ""rb"") as f: vtkjs = f.read() else: data_url = urlopen(self.object) vtkjs = data_url.read() elif hasattr(self.object, ""read""): vtkjs = self.object.read() self._vtkjs = vtkjs return self._vtkjs",False,if isfile ( self . object ) :,"if hasattr ( self . object , ""open"" ) :",0.8820016898747209,26.20251007173262
"def _save(self): fd, tempname = tempfile.mkstemp() fd = os.fdopen(fd, ""w"") json.dump(self._cache, fd, indent=2, separators=("","", "": "")) fd.close() # Silently ignore errors try: <mask>: os.makedirs(os.path.dirname(self.filename)) shutil.move(tempname, self.filename) except (IOError, OSError): os.remove(tempname)",False,if not os . path . exists ( os . path . dirname ( self . filename ) ) :,if not os . path . isdir ( os . path . dirname ( self . filename ) ) :,0.8820016898747209,85.78928092681438
"def refiner_configs(self): rv = {} for refiner in refiner_manager: <mask>: rv[refiner.name] = {k: v for k, v in self.config.items(refiner.name)} return rv",False,if self . config . has_section ( refiner . name ) :,"if isinstance ( refiner , Refiner ) :",0.8820016898747209,7.80152171018653
"def com_slice(self, primary, node, assigning): # short_slice: [lower_bound] "":"" [upper_bound] lower = upper = None if len(node.children) == 2: <mask>: upper = self.com_node(node.children[1]) else: lower = self.com_node(node.children[0]) elif len(node.children) == 3: lower = self.com_node(node.children[0]) upper = self.com_node(node.children[2]) return Slice(primary, assigning, lower, upper, lineno=extractLineNo(node))",False,if node . children [ 0 ] . type == token . COLON :,"if node . children [ 0 ] == ""upper"" :",0.8820016898747209,45.823488902304994
"def close(self, *args, **kwargs): super(mytqdm, self).close(*args, **kwargs) # If it was not run in a notebook, sp is not assigned, check for it if hasattr(self, ""sp""): # Try to detect if there was an error or KeyboardInterrupt # in manual mode: if n < total, things probably got wrong if self.total and self.n < self.total: self.sp(bar_style=""danger"") else: <mask>: self.sp(bar_style=""success"") else: self.sp(close=True)",False,if self . leave :,if self . n < self . total :,0.8820016898747209,19.070828081828378
"def test_alloc(self): b = bytearray() alloc = b.__alloc__() self.assertTrue(alloc >= 0) seq = [alloc] for i in range(100): b += b""x"" alloc = b.__alloc__() self.assertTrue(alloc >= len(b)) <mask>: seq.append(alloc)",False,if alloc not in seq :,if len ( seq ) == 0 :,0.8820016898747209,6.27465531099474
"def flush_file(self, key, f): f.flush() <mask>: f.compress = zlib.compressobj( 9, zlib.DEFLATED, -zlib.MAX_WBITS, zlib.DEF_MEM_LEVEL, 0 ) if len(self.files) > self.MAX_OPEN_FILES: if self.compress: open_files = sum(1 for f in self.files.values() if f.fileobj is not None) if open_files > self.MAX_OPEN_FILES: f.fileobj.close() f.fileobj = None else: f.close() self.files.pop(key)",False,if self . compress :,if not f . compress :,0.8820016898747209,32.46679154750991
"def _run(self): # Low-level run method to do the actual scheduling loop. self.running = True while self.running: try: self.sched.run() except Exception as x: logging.error( ""Error during scheduler execution: %s"" % str(x), exc_info=True ) # queue is empty; sleep a short while before checking again <mask>: time.sleep(5)",False,if self . running :,if self . queue is None :,0.8820016898747209,26.269098944241588
"def TryMerge(self, d): while d.avail() > 0: tt = d.getVarInt32() if tt == 10: self.set_app_id(d.getPrefixedString()) continue <mask>: self.set_max_rows(d.getVarInt32()) continue if tt == 0: raise ProtocolBuffer.ProtocolBufferDecodeError d.skipData(tt)",False,if tt == 16 :,if tt == 18 :,0.8820016898747209,53.7284965911771
"def check(dbdef): ""drop script must clear the database"" for version in dbdef: connector = MemConnector().bound(None) create(dbdef, version, connector) drop(dbdef, version, connector) remaining = connector.execute( ""SELECT * FROM sqlite_master WHERE name NOT LIKE 'sqlite_%'"" ).fetchall() <mask>: yield ""{0}:drop.sql"".format(version), remaining",True,if remaining :,if remaining :,0.8820016898747209,0.0
"def test_open_overwrite_offset_size(self, sftp): """"""Test writing data at a specific offset"""""" f = None try: self._create_file(""file"", ""xxxxyyyy"") f = yield from sftp.open(""file"", ""r+"") yield from f.write(""zz"", 3) yield from f.close() with open(""file"") as localf: self.assertEqual(localf.read(), ""xxxzzyyy"") finally: <mask>: # pragma: no branch yield from f.close() remove(""file"")",True,if f :,if f :,0.8820016898747209,0.0
"def pump(): import sys as _sys while self.countdown_active(): if not (self.connected(""send"") and other.connected(""recv"")): break try: data = other.recv(timeout=0.05) except EOFError: break <mask>: return if not data: continue try: self.send(data) except EOFError: break if not _sys: return self.shutdown(""send"") other.shutdown(""recv"")",True,if not _sys :,if not _sys :,0.8820016898747209,100.00000000000004
"def parse_results(cwd): optimal_dd = None optimal_measure = numpy.inf for tup in tools.find_conf_files(cwd): dd = tup[1] if ""results.train_y_misclass"" in dd: <mask>: optimal_measure = dd[""results.train_y_misclass""] optimal_dd = dd print(""Optimal results.train_y_misclass:"", str(optimal_measure)) for key, value in optimal_dd.items(): if ""hyper_parameters"" in key: print(key + "": "" + str(value))",False,"if dd [ ""results.train_y_misclass"" ] < optimal_measure :","if ""results.train_y_misclass"" in dd :",0.8820016898747209,47.3930294822252
"def valid(self): valid = True <mask>: return valid else: try: with io.open(self.pathfile, ""w"", encoding=""utf-8"") as f: f.close() # do nothing except OSError: valid = False if os.path.exists(self.pathfile): os.remove(self.pathfile) return valid",False,if os . path . exists ( self . pathfile ) :,if self . pathfile is None :,0.8820016898747209,13.597602315271134
"def __getitem__(self, key): try: value = self.cache[key] except KeyError: f = BytesIO(self.dict[key.encode(self.keyencoding)]) value = Unpickler(f).load() <mask>: self.cache[key] = value return value",False,if self . writeback :,if value is not None :,0.8820016898747209,9.652434877402245
"def hasMenu(cls, callingWindow, mainItem, selection, *fullContexts): for i, fullContext in enumerate(fullContexts): srcContext = fullContext[0] for menuHandler in cls.menus: m = menuHandler() <mask>: return True return False",False,"if m . _baseDisplay ( callingWindow , srcContext , mainItem , selection ) :","if m . hasMenu ( callingWindow , mainItem , selection , srcContext ) :",0.8820016898747209,34.78396609202866
"def lr_read_tables(module=tab_module, optimize=0): global _lr_action, _lr_goto, _lr_productions, _lr_method try: exec(""import %s as parsetab"" % module) global parsetab # declare the name of the imported module <mask>: _lr_action = parsetab._lr_action _lr_goto = parsetab._lr_goto _lr_productions = parsetab._lr_productions _lr_method = parsetab._lr_method return 1 else: return 0 except (ImportError, AttributeError): return 0",False,if ( optimize ) or ( Signature . digest ( ) == parsetab . _lr_signature ) :,if optimize :,0.8820016898747209,0.0
"def _Determine_Do(self): if sys.platform.startswith(""win""): self.applicable = 1 for opt, optarg in self.chosenOptions: if opt == ""--moz-tools"": self.value = os.path.abspath(os.path.normpath(optarg)) break else: <mask>: self.value = os.environ[self.name] else: self.value = None else: self.applicable = 0 self.determined = 1",False,if os . environ . has_key ( self . name ) :,if self . name in os . environ :,0.8820016898747209,18.402097851927994
"def parse_chunked(self, unreader): (size, rest) = self.parse_chunk_size(unreader) while size > 0: while size > len(rest): size -= len(rest) yield rest rest = unreader.read() if not rest: raise NoMoreData() yield rest[:size] # Remove \r\n after chunk rest = rest[size:] while len(rest) < 2: rest += unreader.read() <mask>: raise ChunkMissingTerminator(rest[:2]) (size, rest) = self.parse_chunk_size(unreader, data=rest[2:])",False,"if rest [ : 2 ] != b""\r\n"" :","if rest [ : 2 ] == ""\r\n"" :",0.8820016898747209,69.01228050062707
"def _scroll_down(self, cli): ""Scroll window down."" info = self.render_info if self.vertical_scroll < info.content_height - info.window_height: <mask>: self.content.move_cursor_down(cli) self.vertical_scroll += 1",False,if info . cursor_position . y <= info . configured_scroll_offsets . top :,if self . vertical_scroll > info . content_height - info . window_height :,0.8820016898747209,7.741936972574765
"def _add_defaults_data_files(self): # getting distribution.data_files if self.distribution.has_data_files(): for item in self.distribution.data_files: if isinstance(item, str): # plain file item = convert_path(item) if os.path.isfile(item): self.filelist.append(item) else: # a (dirname, filenames) tuple dirname, filenames = item for f in filenames: f = convert_path(f) <mask>: self.filelist.append(f)",True,if os . path . isfile ( f ) :,if os . path . isfile ( f ) :,0.8820016898747209,100.00000000000004
"def list_stuff(self, upto=10, start_after=-1): for i in range(upto): <mask>: continue if i == 2 and self.count < 1: self.count += 1 raise TemporaryProblem if i == 7 and self.count < 4: self.count += 1 raise TemporaryProblem yield i",False,if i <= start_after :,if i == start_after :,0.8820016898747209,59.4603557501361
"def is_open(self): if self.signup_code: return True else: <mask>: if self.messages.get(""invalid_signup_code""): messages.add_message( self.request, self.messages[""invalid_signup_code""][""level""], self.messages[""invalid_signup_code""][""text""].format( **{ ""code"": self.get_code(), } ), ) return settings.ACCOUNT_OPEN_SIGNUP",False,if self . signup_code_present :,if self . get_code ( ) :,0.8820016898747209,23.356898886410015
"def on_delete_from_disk(self, widget, data=None): model, iter = self.get_selection().get_selected() if iter: path = model.get_value(iter, COLUMN_PATH) <mask>: ErrorDialog(_(""Can't delete system item from disk."")).launch() else: os.remove(path) self.update_items()",False,if self . is_defaultitem ( path ) :,if not os . path . exists ( path ) :,0.8820016898747209,25.965358893403383
"def get_detections_for_batch(self, images): images = images[..., ::-1] detected_faces = self.face_detector.detect_from_batch(images.copy()) results = [] for i, d in enumerate(detected_faces): <mask>: results.append(None) continue d = d[0] d = np.clip(d, 0, None) x1, y1, x2, y2 = map(int, d[:-1]) results.append((x1, y1, x2, y2)) return results",False,if len ( d ) == 0 :,if i == len ( detected_faces ) - 1 :,0.8820016898747209,10.04916995660316
def on_update(self): # # Calculate maximum # of planes per well # self.max_per_well = 0 for pd in list(self.plate_well_site.values()): for wd in list(pd.values()): nplanes = sum([len(x) for x in list(wd.values())]) <mask>: self.max_per_well = nplanes for registrant in self.registrants: registrant(),True,if nplanes > self . max_per_well :,if nplanes > self . max_per_well :,0.8820016898747209,100.00000000000004
"def is_writable(self, path): result = False while not result: if os.path.exists(path): result = os.access(path, os.W_OK) break parent = os.path.dirname(path) <mask>: break path = parent return result",True,if parent == path :,if parent == path :,0.8820016898747209,100.00000000000004
"def _check_seed(self, seed): if seed is not None: <mask>: self._raise_error( ""The random number generator seed value, seed, should be integer type or None."" ) if seed < 0: self._raise_error( ""The random number generator seed value, seed, should be non-negative integer or None."" )",False,if type ( seed ) != int :,"if not isinstance ( seed , int ) :",0.8820016898747209,12.549310621989482
"def write(self, x): # try to use backslash and surrogate escape strategies before failing self._errors = ""backslashescape"" if self.encoding != ""mbcs"" else ""surrogateescape"" try: return io.TextIOWrapper.write(self, to_text(x, errors=self._errors)) except UnicodeDecodeError: <mask>: self._errors = ""surrogateescape"" else: self._errors = ""replace"" return io.TextIOWrapper.write(self, to_text(x, errors=self._errors))",False,"if self . _errors != ""surrogateescape"" :","if self . encoding == ""mbcs"" :",0.8820016898747209,20.772794588721627
"def post(self, request, *args, **kwargs): validated_session = [] for session_id in request.data: session = get_object_or_none(Session, id=session_id) <mask>: validated_session.append(session_id) self.model.objects.create( name=""kill_session"", args=session.id, terminal=session.terminal, ) return Response({""ok"": validated_session})",False,if session and not session . is_finished :,if session :,0.8820016898747209,0.0
"def _has_list_or_dict_var_value_before(self, arg_index): for idx, value in enumerate(self.args): <mask>: return False if variablematcher.is_list_variable( value ) and not variablematcher.is_list_variable_subitem(value): return True if robotapi.is_dict_var(value) and not variablematcher.is_dict_var_access( value ): return True return False",False,if idx > arg_index :,if idx == arg_index :,0.8820016898747209,41.11336169005198
"def test_return_correct_type(self): for proto in protocols: # Protocol 0 supports only ASCII strings. <mask>: self._check_return_correct_type(""abc"", 0) else: for obj in [b""abc\n"", ""abc\n"", -1, -1.1 * 0.1, str]: self._check_return_correct_type(obj, proto)",False,if proto == 0 :,"if proto == ""0"" :",0.8820016898747209,38.260294162784454
"def backward_impl(self, inputs, outputs, prop_down, accum): # inputs: [inputs_fwd_graph] + [inputs_bwd_graph] or # [inputs_fwd_graph] + [outputs_fwd_graph] + [inputs_bwd_graph] # Args axis = self.forward_func.info.args[""axis""] # Compute ## w.r.t. dy if prop_down[-1]: g_dy = inputs[-1].grad g_dy_ = F.stack(*[o.grad for o in outputs], axis=axis) <mask>: g_dy += g_dy_ else: g_dy.copy_from(g_dy_)",True,if accum [ - 1 ] :,if accum [ - 1 ] :,0.8820016898747209,100.00000000000004
"def remove(self, url): try: i = self.items.index(url) except (ValueError, IndexError): pass else: was_selected = i in self.selectedindices() self.list.delete(i) del self.items[i] if not self.items: self.mp.hidepanel(self.name) elif was_selected: <mask>: i = len(self.items) - 1 self.list.select_set(i)",False,if i >= len ( self . items ) :,if self . items [ i ] . url == url :,0.8820016898747209,14.211672443220438
"def prepend(self, value): """"""prepend value to nodes"""""" root, root_text = self._get_root(value) for i, tag in enumerate(self): if not tag.text: tag.text = """" if len(root) > 0: root[-1].tail = tag.text tag.text = root_text else: tag.text = root_text + tag.text <mask>: root = deepcopy(list(root)) tag[:0] = root root = tag[: len(root)] return self",False,if i > 0 :,if i == len ( root ) :,0.8820016898747209,10.552670315936318
"def _get_tracks_compositors_list(): tracks_list = [] tracks = current_sequence().tracks compositors = current_sequence().compositors for track_index in range(1, len(tracks) - 1): track_compositors = [] for j in range(0, len(compositors)): comp = compositors[j] <mask>: track_compositors.append(comp) tracks_list.append(track_compositors) return tracks_list",False,if comp . transition . b_track == track_index :,if comp . track_index == track_index :,0.8820016898747209,47.26710158823674
"def __getattr__(self, name): if name in self._sections: return ""\n"".join(self._sections[name]) else: <mask>: return """" else: raise ConanException(""ConfigParser: Unrecognized field '%s'"" % name)",False,if self . _allowed_fields and name in self . _allowed_fields :,"if name == ""default"" :",0.8820016898747209,2.3595365419339505
"def get_first_param_index(self, group_id, param_group, partition_id): for index, param in enumerate(param_group): param_id = self.get_param_id(param) <mask>: return index return None",False,if partition_id in self . param_to_partition_ids [ group_id ] [ param_id ] :,if param_id == group_id and param_id == partition_id :,0.8820016898747209,14.272129722834098
"def handle_uv_sockets(self, context): u_socket = self.inputs[""U""] v_socket = self.inputs[""V""] if self.cast_mode == ""Sphere"": u_socket.hide_safe = True v_socket.hide_safe = True elif self.cast_mode in [""Cylinder"", ""Prism""]: v_socket.hide_safe = True <mask>: u_socket.hide_safe = False else: if u_socket.hide_safe: u_socket.hide_safe = False if v_socket.hide_safe: v_socket.hide_safe = False",False,if u_socket . hide_safe :,"elif self . cast_mode == ""U"" :",0.8820016898747209,4.456882760699063
"def _scrub_generated_timestamps(self, target_workdir): """"""Remove the first line of comment from each file if it contains a timestamp."""""" for root, _, filenames in safe_walk(target_workdir): for filename in filenames: source = os.path.join(root, filename) with open(source, ""r"") as f: lines = f.readlines() <mask>: return with open(source, ""w"") as f: if not self._COMMENT_WITH_TIMESTAMP_RE.match(lines[0]): f.write(lines[0]) for line in lines[1:]: f.write(line)",False,if len ( lines ) < 1 :,if not lines :,0.8820016898747209,7.733712583165139
"def inner(request, *args, **kwargs): page = request.current_page if page: if page.login_required and not request.user.is_authenticated: return redirect_to_login( urlquote(request.get_full_path()), settings.LOGIN_URL ) site = get_current_site() <mask>: return _handle_no_page(request) return func(request, *args, **kwargs)",False,"if not user_can_view_page ( request . user , page , site ) :",if not site . is_active :,0.8820016898747209,3.6752178130875106
"def flush(self, *args, **kwargs): with self._lock: self._last_updated = time.time() try: <mask>: self._locked_flush_without_tempfile() else: mailbox.mbox.flush(self, *args, **kwargs) except OSError: if ""_create_temporary"" in traceback.format_exc(): self._locked_flush_without_tempfile() else: raise self._last_updated = time.time()",False,"if kwargs . get ( ""in_place"" , False ) :","if ""_create_temporary"" in traceback . format_exc ( ) :",0.8820016898747209,8.47178590796544
"def sanitize_event_keys(kwargs, valid_keys): # Sanity check: Don't honor keys that we don't recognize. for key in list(kwargs.keys()): if key not in valid_keys: kwargs.pop(key) # Truncate certain values over 1k for key in [""play"", ""role"", ""task"", ""playbook""]: <mask>: if len(kwargs[""event_data""][key]) > 1024: kwargs[""event_data""][key] = Truncator(kwargs[""event_data""][key]).chars( 1024 )",False,"if isinstance ( kwargs . get ( ""event_data"" , { } ) . get ( key ) , str ) :","if key in kwargs [ ""event_data"" ] :",0.8820016898747209,12.428640864054127
"def parse_auth(val): if val is not None: authtype, params = val.split("" "", 1) <mask>: if authtype == ""Basic"" and '""' not in params: # this is the ""Authentication: Basic XXXXX=="" case pass else: params = parse_auth_params(params) return authtype, params return val",False,if authtype in known_auth_schemes :,if authtype :,0.8820016898747209,0.0
"def _memoized(*args): now = time.time() try: value, last_update = self.cache[args] age = now - last_update if self._call_count > self.ctl or age > self.ttl: self._call_count = 0 raise AttributeError if self.ctl: self._call_count += 1 return value except (KeyError, AttributeError): value = func(*args) <mask>: self.cache[args] = (value, now) return value except TypeError: return func(*args)",False,if value :,if value is not None :,0.8820016898747209,17.965205598154213
"def _get_md_bg_color_down(self): t = self.theme_cls c = self.md_bg_color # Default to no change on touch # Material design specifies using darker hue when on Dark theme if t.theme_style == ""Dark"": if self.md_bg_color == t.primary_color: c = t.primary_dark <mask>: c = t.accent_dark return c",True,elif self . md_bg_color == t . accent_color :,elif self . md_bg_color == t . accent_color :,0.8820016898747209,100.00000000000004
def _init_table_h(): _table_h = [] for i in range(256): part_l = i part_h = 0 for j in range(8): rflag = part_l & 1 part_l >>= 1 <mask>: part_l |= 1 << 31 part_h >>= 1 if rflag: part_h ^= 0xD8000000 _table_h.append(part_h) return _table_h,False,if part_h & 1 :,if rflag :,0.8820016898747209,0.0
"def migrate_Stats(self): for old_obj in self.session_old.query(self.model_from[""Stats""]): if not old_obj.summary: self.entries_count[""Stats""] -= 1 continue new_obj = self.model_to[""Stats""]() for key in new_obj.__table__.columns._data.keys(): <mask>: continue setattr(new_obj, key, getattr(old_obj, key)) self.session_new.add(new_obj)",False,if key not in old_obj . __table__ . columns :,if key in old_obj . __table__ . columns . _data :,0.8820016898747209,68.53234406569368
"def get_in_turn_repetition(pred, is_cn=False): """"""Get in-turn repetition."""""" if len(pred) == 0: return 1.0 if isinstance(pred[0], str): pred = [tok.lower() for tok in pred] if is_cn: pred = """".join(pred) tri_grams = set() for i in range(len(pred) - 2): tri_gram = tuple(pred[i : i + 3]) <mask>: return 1.0 tri_grams.add(tri_gram) return 0.0",False,if tri_gram in tri_grams :,if tri_gram not in tri_grams :,0.8820016898747209,65.80370064762461
"def translate(): assert Lex.next() is AttributeList reader.read() # Discard attribute list from reader. attrs = {} d = AttributeList.match.groupdict() for k, v in d.items(): if v is not None: <mask>: v = subs_attrs(v) if v: parse_attributes(v, attrs) else: AttributeList.attrs[k] = v AttributeList.subs(attrs) AttributeList.attrs.update(attrs)",False,"if k == ""attrlist"" :","if isinstance ( v , str ) :",0.8820016898747209,6.567274736060395
"def _parse(self, engine): """"""Parse the layer."""""" if isinstance(self.args, dict): if ""axis"" in self.args: self.axis = engine.evaluate(self.args[""axis""], recursive=True) <mask>: raise ParsingError('""axis"" must be an integer.') if ""momentum"" in self.args: self.momentum = engine.evaluate(self.args[""momentum""], recursive=True) if not isinstance(self.momentum, (int, float)): raise ParsingError('""momentum"" must be numeric.')",False,"if not isinstance ( self . axis , int ) :","if not isinstance ( self . axis , ( int , float ) ) :",0.8820016898747209,53.2800971987552
"def __getattr__(self, attrname): if attrname in (""visamp"", ""visamperr"", ""visphi"", ""visphierr""): return ma.masked_array(self.__dict__[""_"" + attrname], mask=self.flag) elif attrname in (""cflux"", ""cfluxerr""): <mask>: return ma.masked_array(self.__dict__[""_"" + attrname], mask=self.flag) else: return None else: raise AttributeError(attrname)",False,"if self . __dict__ [ ""_"" + attrname ] != None :",if self . flag :,0.8820016898747209,2.5983349617896914
"def draw(self, context): layout = self.layout presets.draw_presets_ops(layout, context=context) for category in presets.get_category_names(): <mask>: if category in preset_category_menus: class_name = preset_category_menus[category].__name__ layout.menu(class_name)",True,if category in preset_category_menus :,if category in preset_category_menus :,0.8820016898747209,100.00000000000004
"def __setitem__(self, key, value): if isinstance(value, (tuple, list)): info, reference = value if info not in self._reverse_infos: self._reverse_infos[info] = len(self._infos) self._infos.append(info) <mask>: self._reverse_references[reference] = len(self._references) self._references.append(reference) self._trails[key] = ""%d,%d"" % ( self._reverse_infos[info], self._reverse_references[reference], ) else: raise Exception(""unsupported type '%s'"" % type(value))",True,if reference not in self . _reverse_references :,if reference not in self . _reverse_references :,0.8820016898747209,100.00000000000004
"def format_bpe_text(symbols, delimiter=b""@@""): """"""Convert a sequence of bpe words into sentence."""""" words = [] word = b"""" if isinstance(symbols, str): symbols = symbols.encode() delimiter_len = len(delimiter) for symbol in symbols: <mask>: word += symbol[:-delimiter_len] else: # end of a word word += symbol words.append(word) word = b"""" return b"" "".join(words)",False,if len ( symbol ) >= delimiter_len and symbol [ - delimiter_len : ] == delimiter :,if symbol . endswith ( delimiter ) :,0.8820016898747209,1.325449986659188
"def output_type(data, request, response): accept = request.accept if accept in ("""", ""*"", ""/""): handler = default or handlers and next(iter(handlers.values())) else: handler = default accepted = [accept_quality(accept_type) for accept_type in accept.split("","")] accepted.sort(key=itemgetter(0)) for _quality, accepted_content_type in reversed(accepted): <mask>: handler = handlers[accepted_content_type] break if not handler: raise falcon.HTTPNotAcceptable(error) response.content_type = handler.content_type return handler(data, request=request, response=response)",False,if accepted_content_type in handlers :,if _quality == accepted_quality :,0.8820016898747209,11.99014838091355
"def _render_raw_list(bytes_items): flatten_items = [] for item in bytes_items: if item is None: flatten_items.append(b"""") elif isinstance(item, bytes): flatten_items.append(item) <mask>: flatten_items.append(str(item).encode()) elif isinstance(item, list): flatten_items.append(_render_raw_list(item)) return b""\n"".join(flatten_items)",False,"elif isinstance ( item , int ) :","elif isinstance ( item , str ) :",0.8820016898747209,59.4603557501361
"def TryMerge(self, d): while d.avail() > 0: tt = d.getVarInt32() <mask>: self.set_mime_type(d.getVarInt32()) continue if tt == 16: self.set_quality(d.getVarInt32()) continue if tt == 0: raise ProtocolBuffer.ProtocolBufferDecodeError d.skipData(tt)",True,if tt == 8 :,if tt == 8 :,0.8820016898747209,100.00000000000004
"def delete(self, waiters): # Delete flow. msgs = self.ofctl.get_all_flow(waiters) for msg in msgs: for stats in msg.body: vlan_id = VlanRouter._cookie_to_id(REST_VLANID, stats.cookie) <mask>: self.ofctl.delete_flow(stats) assert len(self.packet_buffer) == 0",True,if vlan_id == self . vlan_id :,if vlan_id == self . vlan_id :,0.8820016898747209,100.00000000000004
def missing_push_allowance(push_allowances: List[PushAllowance]) -> bool: for push_allowance in push_allowances: # a null databaseId indicates this is not a GitHub App. if push_allowance.actor.databaseId is None: continue <mask>: return False return True,False,if str ( push_allowance . actor . databaseId ) == str ( app_config . GITHUB_APP_ID ) :,"if push_allowance . actor . databaseId == ""GitHub App"" :",0.8820016898747209,22.364623267658008
"def _cluster_page(self, htmlpage): template_cluster, preferred = _CLUSTER_NA, None if self.clustering: self.clustering.add_page(htmlpage) <mask>: clt = self.clustering.classify(htmlpage) if clt != -1: template_cluster = preferred = self.template_names[clt] else: template_cluster = _CLUSTER_OUTLIER return template_cluster, preferred",False,if self . clustering . is_fit :,if self . template_names :,0.8820016898747209,20.873176328735713
"def readlines(self, size=-1): if self._nbr == self._size: return [] # leave all additional logic to our readline method, we just check the size out = [] nbr = 0 while True: line = self.readline() if not line: break out.append(line) if size > -1: nbr += len(line) <mask>: break # END handle size constraint # END readline loop return out",False,if nbr > size :,if nbr == self . _size :,0.8820016898747209,13.485111859503691
"def post_mortem(t=None): # handling the default <mask>: # sys.exc_info() returns (type, value, traceback) if an exception is # being handled, otherwise it returns None t = sys.exc_info()[2] if t is None: raise ValueError( ""A valid traceback must be passed if no exception is being handled."" ) p = BPdb() p.reset() p.interaction(None, t)",True,if t is None :,if t is None :,0.8820016898747209,100.00000000000004
"def fixup(m): txt = m.group(0) if txt[:2] == ""&#"": # character reference try: <mask>: return unichr(int(txt[3:-1], 16)) else: return unichr(int(txt[2:-1])) except ValueError: pass else: # named entity try: txt = unichr(htmlentitydefs.name2codepoint[txt[1:-1]]) except KeyError: pass return txt # leave as is",False,"if txt [ : 3 ] == ""&#x"" :","if txt [ 3 : - 1 ] == ""&#x"" :",0.8820016898747209,62.628449627654696
"def parse_converter_args(argstr: str) -> t.Tuple[t.Tuple, t.Dict[str, t.Any]]: argstr += "","" args = [] kwargs = {} for item in _converter_args_re.finditer(argstr): value = item.group(""stringval"") <mask>: value = item.group(""value"") value = _pythonize(value) if not item.group(""name""): args.append(value) else: name = item.group(""name"") kwargs[name] = value return tuple(args), kwargs",True,if value is None :,if value is None :,0.8820016898747209,100.00000000000004
"def IT(cpu): cc = cpu.instruction.cc true_case = cpu._evaluate_conditional(cc) # this is incredibly hacky--how else does capstone expose this? # TODO: find a better way than string parsing the mnemonic -GR, 2017-07-13 for c in cpu.instruction.mnemonic[1:]: <mask>: cpu._it_conditional.append(true_case) elif c == ""e"": cpu._it_conditional.append(not true_case)",False,"if c == ""t"" :","if c == ""g"" :",0.8820016898747209,59.4603557501361
"def flatten(self): # this is similar to fill_messages except it uses a list instead # of a queue to place the messages in. result = [] channel = await self.messageable._get_channel() self.channel = channel while self._get_retrieve(): data = await self._retrieve_messages(self.retrieve) <mask>: self.limit = 0 # terminate the infinite loop if self.reverse: data = reversed(data) if self._filter: data = filter(self._filter, data) for element in data: result.append(self.state.create_message(channel=channel, data=element)) return result",False,if len ( data ) < 100 :,if not data :,0.8820016898747209,7.733712583165139
"def _get_beta_accumulators(self): with tf.init_scope(): <mask>: graph = None else: graph = tf.get_default_graph() return ( self._get_non_slot_variable(""beta1_power"", graph=graph), self._get_non_slot_variable(""beta2_power"", graph=graph), )",False,if tf . executing_eagerly ( ) :,if self . _is_train :,0.8820016898747209,6.892168295481103
"def prefixed(self, prefix: _StrType) -> typing.Iterator[""Env""]: """"""Context manager for parsing envvars with a common prefix."""""" try: old_prefix = self._prefix <mask>: self._prefix = prefix else: self._prefix = f""{old_prefix}{prefix}"" yield self finally: # explicitly reset the stored prefix on completion and exceptions self._prefix = None self._prefix = old_prefix",True,if old_prefix is None :,if old_prefix is None :,0.8820016898747209,100.00000000000004
"def decode_content(self): """"""Return the best possible representation of the response body."""""" ct = self.headers.get(""content-type"") if ct: ct, options = parse_options_header(ct) charset = options.get(""charset"") if ct in JSON_CONTENT_TYPES: return self.json(charset) <mask>: return self.text(charset) elif ct == FORM_URL_ENCODED: return parse_qsl(self.content.decode(charset), keep_blank_values=True) return self.content",False,"elif ct . startswith ( ""text/"" ) :",elif ct == TEXT_CONTENT_TYPES :,0.8820016898747209,8.403703759902122
"def test_incrementaldecoder(self): UTF8Writer = codecs.getwriter(""utf-8"") for sizehint in [None, -1] + list(range(1, 33)) + [64, 128, 256, 512, 1024]: istream = BytesIO(self.tstring[0]) ostream = UTF8Writer(BytesIO()) decoder = self.incrementaldecoder() while 1: data = istream.read(sizehint) <mask>: break else: u = decoder.decode(data) ostream.write(u) self.assertEqual(ostream.getvalue(), self.tstring[1])",True,if not data :,if not data :,0.8820016898747209,100.00000000000004
"def delete_all(path): ppath = os.getcwd() os.chdir(path) for fn in glob.glob(""*""): fn_full = os.path.join(path, fn) <mask>: delete_all(fn_full) elif fn.endswith("".png""): os.remove(fn_full) elif fn.endswith("".md""): os.remove(fn_full) elif DELETE_ALL_OLD: os.remove(fn_full) os.chdir(ppath) os.rmdir(path)",False,if os . path . isdir ( fn ) :,"if fn . endswith ( "".py"" ) :",0.8820016898747209,10.252286118120933
"def _delete_reason(self): for i in range(_lib.X509_REVOKED_get_ext_count(self._revoked)): ext = _lib.X509_REVOKED_get_ext(self._revoked, i) obj = _lib.X509_EXTENSION_get_object(ext) <mask>: _lib.X509_EXTENSION_free(ext) _lib.X509_REVOKED_delete_ext(self._revoked, i) break",False,if _lib . OBJ_obj2nid ( obj ) == _lib . NID_crl_reason :,if obj . _revoked == self . _revoked :,0.8820016898747209,4.659755815573173
"def hexcmp(x, y): try: a = int(x, 16) b = int(y, 16) if a < b: return -1 <mask>: return 1 return 0 except: return cmp(x, y)",True,if a > b :,if a > b :,0.8820016898747209,100.00000000000004
"def get_indentation_count(view, start): indent_count = 0 i = start - 1 while i > 0: ch = view.substr(i) scope = view.scope_name(i) # Skip preprocessors, strings, characaters and comments if ""string.quoted"" in scope or ""comment"" in scope or ""preprocessor"" in scope: extent = view.extract_scope(i) i = extent.a - 1 continue else: i -= 1 <mask>: indent_count -= 1 elif ch == ""{"": indent_count += 1 return indent_count",False,"if ch == ""}"" :","if ch == ""{"" :",0.8820016898747209,59.4603557501361
"def set(self, name, value, ex=None, px=None, nx=False, xx=False): if ( (not nx and not xx) or (nx and self._db.get(name, None) is None) or (xx and not self._db.get(name, None) is None) ): if ex > 0: self._db.expire(name, datetime.now() + timedelta(seconds=ex)) <mask>: self._db.expire(name, datetime.now() + timedelta(milliseconds=px)) self._db[name] = str(value) return True else: return None",True,elif px > 0 :,elif px > 0 :,0.8820016898747209,100.00000000000004
"def _get_between(content, start, end=None): should_yield = False for line in content.split(""\n""): if start in line: should_yield = True continue if end and end in line: return <mask>: yield line.strip().split("" "")[0]",False,if should_yield and line :,if should_yield :,0.8820016898747209,47.39878501170795
"def iter_event_handlers( self, resource: resources_.Resource, event: bodies.RawEvent, ) -> Iterator[handlers.ResourceWatchingHandler]: warnings.warn( ""SimpleRegistry.iter_event_handlers() is deprecated; use "" ""ResourceWatchingRegistry.iter_handlers()."", DeprecationWarning, ) cause = _create_watching_cause(resource, event) for handler in self._handlers: <mask>: pass elif registries.match(handler=handler, cause=cause, ignore_fields=True): yield handler",False,"if not isinstance ( handler , handlers . ResourceWatchingHandler ) :",if handler is None :,0.8820016898747209,4.234348806659263
"def __enter__(self): if log_timer: <mask>: self.logger.debug(""%s starting"" % self.name) else: print((""[%s starting]..."" % self.name)) self.tstart = time.time()",True,if self . logger :,if self . logger :,0.8820016898747209,100.00000000000004
"def _handle_errors(errors): """"""Log out and possibly reraise errors during import."""""" if not errors: return log_all = True # pylint: disable=unused-variable err_msg = ""T2T: skipped importing {num_missing} data_generators modules."" print(err_msg.format(num_missing=len(errors))) for module, err in errors: err_str = str(err) <mask>: print(""Did not import module: %s; Cause: %s"" % (module, err_str)) if not _is_import_err_msg(err_str, module): print(""From module %s"" % module) raise err",True,if log_all :,if log_all :,0.8820016898747209,100.00000000000004
"def _ungroup(sequence, groups=None): for v in sequence: if isinstance(v, (list, tuple)): <mask>: groups.append(list(_ungroup(v, groups=None))) for v in _ungroup(v, groups): yield v else: yield v",True,if groups is not None :,if groups is not None :,0.8820016898747209,100.00000000000004
def run(self): while not self.completed: if self.block: time.sleep(self.period) else: self._completed.wait(self.period) self.counter += 1 try: self.callback(self.counter) except Exception: self.stop() if self.timeout is not None: dt = time.time() - self._start_time <mask>: self.stop() if self.counter == self.count: self.stop(),False,if dt > self . timeout :,if dt < self . timeout :,0.8820016898747209,48.892302243490086
"def dont_let_stderr_buffer(): while True: line = context.daemon.stderr.readline() <mask>: return if DEAD_DEPLOYD_WORKER_MESSAGE.encode(""utf-8"") in line: context.num_workers_crashed += 1 print(f""deployd stderr: {line}"")",True,if not line :,if not line :,0.8820016898747209,100.00000000000004
"def mergeHiLo(self, x_stats): """"""Merge the highs and lows of another accumulator into myself."""""" if x_stats.firsttime is not None: if self.firsttime is None or x_stats.firsttime < self.firsttime: self.firsttime = x_stats.firsttime self.first = x_stats.first if x_stats.lasttime is not None: <mask>: self.lasttime = x_stats.lasttime self.last = x_stats.last",False,if self . lasttime is None or x_stats . lasttime >= self . lasttime :,if self . lasttime is None :,0.8820016898747209,17.469470584451173
"def test_rlimit_get(self): import resource p = psutil.Process(os.getpid()) names = [x for x in dir(psutil) if x.startswith(""RLIMIT"")] assert names for name in names: value = getattr(psutil, name) self.assertGreaterEqual(value, 0) <mask>: self.assertEqual(value, getattr(resource, name)) self.assertEqual(p.rlimit(value), resource.getrlimit(value)) else: ret = p.rlimit(value) self.assertEqual(len(ret), 2) self.assertGreaterEqual(ret[0], -1) self.assertGreaterEqual(ret[1], -1)",False,if name in dir ( resource ) :,"if hasattr ( resource , name ) :",0.8820016898747209,17.286039232097043
"def _calculate_writes_for_built_in_indices(self, entity): writes = 0 for prop_name in entity.keys(): if not prop_name in entity.unindexed_properties(): prop_vals = entity[prop_name] <mask>: num_prop_vals = len(prop_vals) else: num_prop_vals = 1 writes += 2 * num_prop_vals return writes",False,"if isinstance ( prop_vals , ( list ) ) :","if isinstance ( prop_vals , list ) :",0.8820016898747209,61.455883305931245
"def check_value_check(self, x_data, t_data, use_cudnn): x = chainer.Variable(x_data) t = chainer.Variable(t_data) with chainer.using_config(""use_cudnn"", use_cudnn): <mask>: # Check if it throws nothing functions.softmax_cross_entropy( x, t, enable_double_backprop=self.enable_double_backprop ) else: with self.assertRaises(ValueError): functions.softmax_cross_entropy( x, t, enable_double_backprop=self.enable_double_backprop )",False,if self . valid :,if self . use_cudnn :,0.8820016898747209,26.269098944241588
"def get_note_title_file(note): mo = note_title_re.match(note.get(""content"", """")) if mo: fn = mo.groups()[0] fn = fn.replace("" "", ""_"") fn = fn.replace(""/"", ""_"") <mask>: return """" if isinstance(fn, str): fn = unicode(fn, ""utf-8"") else: fn = unicode(fn) if note_markdown(note): fn += "".mkdn"" else: fn += "".txt"" return fn else: return """"",True,if not fn :,if not fn :,0.8820016898747209,100.00000000000004
"def _parseparam(s): plist = [] while s[:1] == "";"": s = s[1:] end = s.find("";"") while end > 0 and (s.count('""', 0, end) - s.count('\\""', 0, end)) % 2: end = s.find("";"", end + 1) if end < 0: end = len(s) f = s[:end] <mask>: i = f.index(""="") f = f[:i].strip().lower() + ""="" + f[i + 1 :].strip() plist.append(f.strip()) s = s[end:] return plist",True,"if ""="" in f :","if ""="" in f :",0.8820016898747209,100.00000000000004
"def doDir(elem): for child in elem.childNodes: if not isinstance(child, minidom.Element): continue if child.tagName == ""Directory"": doDir(child) elif child.tagName == ""Component"": for grandchild in child.childNodes: if not isinstance(grandchild, minidom.Element): continue <mask>: continue files.add(grandchild.getAttribute(""Source"").replace(os.sep, ""/""))",False,"if grandchild . tagName != ""File"" :","if grandchild . getAttribute ( ""Source"" ) == """" :",0.8820016898747209,15.537125692760354
"def date_to_format(value, target_format): """"""Convert date to specified format"""""" if target_format == str: <mask>: ret = value.strftime(""%d/%m/%y"") elif isinstance(value, datetime.datetime): ret = value.strftime(""%d/%m/%y"") elif isinstance(value, datetime.time): ret = value.strftime(""%H:%M:%S"") else: ret = value return ret",True,"if isinstance ( value , datetime . date ) :","if isinstance ( value , datetime . date ) :",0.8820016898747209,100.00000000000004
"def __listingColumns(self): columns = [] for name in self.__getColumns(): definition = column(name) <mask>: IECore.msg( IECore.Msg.Level.Error, ""GafferImageUI.CatalogueUI"", ""No column registered with name '%s'"" % name, ) continue if isinstance(definition, IconColumn): c = GafferUI.PathListingWidget.IconColumn(definition.title(), """", name) else: c = GafferUI.PathListingWidget.StandardColumn(definition.title(), name) columns.append(c) return columns",False,if not definition :,if definition is None :,0.8820016898747209,14.058533129758727
"def metrics_to_scalars(self, metrics): new_metrics = {} for k, v in metrics.items(): <mask>: v = v.item() if isinstance(v, dict): v = self.metrics_to_scalars(v) new_metrics[k] = v return new_metrics",False,"if isinstance ( v , torch . Tensor ) :","if isinstance ( v , dict ) :",0.8820016898747209,46.307771619910305
"def start(self, connection): try: if self.client_name: creds = gssapi.Credentials(name=gssapi.Name(self.client_name)) else: creds = None hostname = self.get_hostname(connection) name = gssapi.Name( b""@"".join([self.service, hostname]), gssapi.NameType.hostbased_service ) context = gssapi.SecurityContext(name=name, creds=creds) return context.step(None) except gssapi.raw.misc.GSSError: <mask>: return NotImplemented else: raise",False,if self . fail_soft :,if self . service is None :,0.8820016898747209,26.269098944241588
"def nanmax(self, axis=None, dtype=None, keepdims=None): ret = self._reduction( ""nanmax"", axis=axis, dtype=dtype, keepdims=keepdims, todense=True ) if not issparse(ret): <mask>: return ret xps = get_sparse_module(self.spmatrix) ret = SparseNDArray(xps.csr_matrix(ret)) return ret return ret",False,if get_array_module ( ret ) . isscalar ( ret ) :,if self . spmatrix is None :,0.8820016898747209,2.75631563063758
"def utterance_to_sample(query_data, tagging_scheme, language): tokens, tags = [], [] current_length = 0 for chunk in query_data: chunk_tokens = tokenize(chunk[TEXT], language) tokens += [ Token(t.value, current_length + t.start, current_length + t.end) for t in chunk_tokens ] current_length += len(chunk[TEXT]) <mask>: tags += negative_tagging(len(chunk_tokens)) else: tags += positive_tagging( tagging_scheme, chunk[SLOT_NAME], len(chunk_tokens) ) return {TOKENS: tokens, TAGS: tags}",False,if SLOT_NAME not in chunk :,"if tagging_scheme == ""negative"" :",0.8820016898747209,5.522397783539471
"def use_index( self, term: Union[str, Index], *terms: Union[str, Index] ) -> ""QueryBuilder"": for t in (term, *terms): if isinstance(t, Index): self._use_indexes.append(t) <mask>: self._use_indexes.append(Index(t))",True,"elif isinstance ( t , str ) :","elif isinstance ( t , str ) :",0.8820016898747209,100.00000000000004
"def reconfigServiceWithBuildbotConfig(self, new_config): if new_config.manhole != self.manhole: if self.manhole: yield self.manhole.disownServiceParent() self.manhole = None <mask>: self.manhole = new_config.manhole yield self.manhole.setServiceParent(self) # chain up yield service.ReconfigurableServiceMixin.reconfigServiceWithBuildbotConfig( self, new_config )",False,if new_config . manhole :,if self . manhole is not None :,0.8820016898747209,13.134549472120788
"def cleanup_folder(target_folder): for file in os.listdir(target_folder): file_path = os.path.join(target_folder, file) try: <mask>: os.remove(file_path) except Exception as e: logging.error(e)",False,if os . path . isfile ( file_path ) :,if os . path . exists ( file_path ) :,0.8820016898747209,73.48889200874659
"def to_key(literal_or_identifier): """"""returns string representation of this object"""""" if literal_or_identifier[""type""] == ""Identifier"": return literal_or_identifier[""name""] elif literal_or_identifier[""type""] == ""Literal"": k = literal_or_identifier[""value""] <mask>: return unicode(float_repr(k)) elif ""regex"" in literal_or_identifier: return compose_regex(k) elif isinstance(k, bool): return ""true"" if k else ""false"" elif k is None: return ""null"" else: return unicode(k)",True,"if isinstance ( k , float ) :","if isinstance ( k , float ) :",0.8820016898747209,100.00000000000004
"def decompile(decompiler): for pos, next_pos, opname, arg in decompiler.instructions: if pos in decompiler.targets: decompiler.process_target(pos) method = getattr(decompiler, opname, None) <mask>: throw(DecompileError(""Unsupported operation: %s"" % opname)) decompiler.pos = pos decompiler.next_pos = next_pos x = method(*arg) if x is not None: decompiler.stack.append(x)",True,if method is None :,if method is None :,0.8820016898747209,100.00000000000004
"def shutdown(self, timeout, callback=None): logger.debug(""background worker got shutdown request"") with self._lock: if self.is_alive: self._queue.put_nowait(_TERMINATOR) <mask>: self._wait_shutdown(timeout, callback) self._thread = None self._thread_for_pid = None logger.debug(""background worker shut down"")",False,if timeout > 0.0 :,if callback :,0.8820016898747209,0.0
"def getDOMImplementation(features=None): if features: <mask>: features = domreg._parse_feature_string(features) for f, v in features: if not Document.implementation.hasFeature(f, v): return None return Document.implementation",True,"if isinstance ( features , str ) :","if isinstance ( features , str ) :",0.8820016898747209,100.00000000000004
"def validate_subevent(self, subevent): if self.context[""event""].has_subevents: <mask>: raise ValidationError(""You need to set a subevent."") if subevent.event != self.context[""event""]: raise ValidationError( ""The specified subevent does not belong to this event."" ) elif subevent: raise ValidationError(""You cannot set a subevent for this event."") return subevent",True,if not subevent :,if not subevent :,0.8820016898747209,100.00000000000004
"def einsum(job_id, idx, einsum_expr, data_list): _, all_parties = session_init(job_id, idx) with SPDZ(): <mask>: x = FixedPointTensor.from_source(""x"", data_list[0]) y = FixedPointTensor.from_source(""y"", all_parties[1]) else: x = FixedPointTensor.from_source(""x"", all_parties[0]) y = FixedPointTensor.from_source(""y"", data_list[1]) return x.einsum(y, einsum_expr).get()",False,if idx == 0 :,if len ( all_parties ) == 2 :,0.8820016898747209,8.913765521398126
"def slowSorted(qq): ""Reference sort peformed by insertion using only <"" rr = list() for q in qq: i = 0 for i in range(len(rr)): <mask>: rr.insert(i, q) break else: rr.append(q) return rr",False,if q < rr [ i ] :,if rr [ i ] [ 0 ] < q [ 0 ] :,0.8820016898747209,22.718709780542323
"def _format_entry(entry, src): if entry: result = [] for x in entry.split("",""): x = x.strip() if os.path.exists(os.path.join(src, x)): result.append(relpath(os.path.join(src, x), src)) <mask>: result.append(relpath(os.path.abspath(x), src)) else: raise RuntimeError(""No entry script %s found"" % x) return "","".join(result)",False,elif os . path . exists ( x ) :,elif os . path . isabs ( os . path . abspath ( x ) ) :,0.8820016898747209,30.648595997659086
"def reloadCols(self): self.columns = [] for i, (name, fmt, *shape) in enumerate(self.npy.dtype.descr): if shape: t = anytype elif ""M"" in fmt: self.addColumn(Column(name, type=date, getter=lambda c, r, i=i: str(r[i]))) continue elif ""i"" in fmt: t = int <mask>: t = float else: t = anytype self.addColumn(ColumnItem(name, i, type=t))",True,"elif ""f"" in fmt :","elif ""f"" in fmt :",0.8820016898747209,100.00000000000004
"def tool_lineages(self, trans): rval = [] for id, tool in self.app.toolbox.tools(): <mask>: lineage_dict = tool.lineage.to_dict() else: lineage_dict = None entry = dict(id=id, lineage=lineage_dict) rval.append(entry) return rval",False,"if hasattr ( tool , ""lineage"" ) :",if tool . lineage :,0.8820016898747209,5.557509463743763
"def item(self, tensor): numel = 0 if len(tensor.shape) > 0: numel = fct.reduce(op.mul, tensor.shape) <mask>: raise ValueError( f""expected tensor with one element, "" f""got {tensor.shape}"" ) if numel == 1: return tensor[0] return tensor",True,if numel != 1 :,if numel != 1 :,0.8820016898747209,100.00000000000004
"def get_host_metadata(self): meta = {} if self.agent_url: try: resp = requests.get( self.agent_url + ECS_AGENT_METADATA_PATH, timeout=1 ).json() if ""Version"" in resp: match = AGENT_VERSION_EXP.search(resp.get(""Version"")) <mask>: meta[""ecs_version""] = match.group(1) except Exception as e: self.log.debug(""Error getting ECS version: %s"" % str(e)) return meta",False,if match is not None and len ( match . groups ( ) ) == 1 :,if match :,0.8820016898747209,0.0
"def generate(): for leaf in u.leaves: if isinstance(leaf, Integer): val = leaf.get_int_value() if val in (0, 1): yield val else: raise _NoBoolVector elif isinstance(leaf, Symbol): <mask>: yield 1 elif leaf == SymbolFalse: yield 0 else: raise _NoBoolVector else: raise _NoBoolVector",True,if leaf == SymbolTrue :,if leaf == SymbolTrue :,0.8820016898747209,100.00000000000004
"def _test_set_metadata(self, metadata, mask=None): header = ofproto.OXM_OF_METADATA match = OFPMatch() if mask is None: match.set_metadata(metadata) else: <mask>: header = ofproto.OXM_OF_METADATA_W match.set_metadata_masked(metadata, mask) metadata &= mask self._test_serialize_and_parser(match, header, metadata, mask)",False,if ( mask + 1 ) >> 64 != 1 :,if mask is not None :,0.8820016898747209,3.3264637832151163
"def pixbufrenderer(self, column, crp, model, it): tok = model.get_value(it, 0) if tok.type == ""class"": icon = ""class"" else: if tok.visibility == ""private"": icon = ""method_priv"" <mask>: icon = ""method_prot"" else: icon = ""method"" crp.set_property(""pixbuf"", imagelibrary.pixbufs[icon])",False,"elif tok . visibility == ""protected"" :","elif tok . visibility == ""prot"" :",0.8820016898747209,70.71067811865478
"def path_sum2(root, s): if root is None: return [] res = [] stack = [(root, [root.val])] while stack: node, ls = stack.pop() if node.left is None and node.right is None and sum(ls) == s: res.append(ls) <mask>: stack.append((node.left, ls + [node.left.val])) if node.right is not None: stack.append((node.right, ls + [node.right.val])) return res",False,if node . left is not None :,elif node . left is not None :,0.8820016898747209,84.08964152537145
"def clear_slot(self, slot_id, trigger_changed): if self.slots[slot_id] is not None: old_resource_id = self.slots[slot_id].resource_id <mask>: del self.sell_list[old_resource_id] else: del self.buy_list[old_resource_id] self.slots[slot_id] = None if trigger_changed: self._changed()",False,if self . slots [ slot_id ] . selling :,if old_resource_id in self . buy_list :,0.8820016898747209,9.669265690880861
"def OnRightUp(self, event): self.HandleMouseEvent(event) self.Unbind(wx.EVT_RIGHT_UP, handler=self.OnRightUp) self.Unbind(wx.EVT_MOUSE_CAPTURE_LOST, handler=self.OnRightUp) self._right = False if not self._left: self.Unbind(wx.EVT_MOTION, handler=self.OnMotion) self.SendChangeEvent() self.SetToolTip(wx.ToolTip(self._tooltip)) <mask>: self.ReleaseMouse()",False,if self . HasCapture ( ) :,if self . _left :,0.8820016898747209,27.482545710800192
"def __init__(self, *args, **kwargs): for arg in args: for k, v in arg.items(): <mask>: arg[k] = AttrDict(v) else: arg[k] = v super(AttrDict, self).__init__(*args, **kwargs)",True,"if isinstance ( v , dict ) :","if isinstance ( v , dict ) :",0.8820016898747209,100.00000000000004
"def _toplevelTryFunc(func, *args, status=status, **kwargs): with ThreadProfiler(threading.current_thread()) as prof: t = threading.current_thread() t.name = func.__name__ try: t.status = func(*args, **kwargs) except EscapeException as e: # user aborted t.status = ""aborted by user"" if status: status(""%s aborted"" % t.name, priority=2) except Exception as e: t.exception = e t.status = ""exception"" vd.exceptionCaught(e) <mask>: t.sheet.currentThreads.remove(t)",True,if t . sheet :,if t . sheet :,0.8820016898747209,100.00000000000004
"def comboSelectionChanged(self, index): text = self.comboBox.cb.itemText(index) for i in range(self.labelList.count()): if text == """": self.labelList.item(i).setCheckState(2) <mask>: self.labelList.item(i).setCheckState(0) else: self.labelList.item(i).setCheckState(2)",False,elif text != self . labelList . item ( i ) . text ( ) :,"elif text == "" "" :",0.8820016898747209,3.7432772183239766
"def __attempt_add_to_linked_match( self, input_name, hdca, collection_type_description, subcollection_type ): structure = get_structure( hdca, collection_type_description, leaf_subcollection_type=subcollection_type ) if not self.linked_structure: self.linked_structure = structure self.collections[input_name] = hdca self.subcollection_types[input_name] = subcollection_type else: <mask>: raise exceptions.MessageException(CANNOT_MATCH_ERROR_MESSAGE) self.collections[input_name] = hdca self.subcollection_types[input_name] = subcollection_type",False,if not self . linked_structure . can_match ( structure ) :,if self . linked_structure . match ( structure ) :,0.8820016898747209,58.491831377324175
"def _wait_for_bot_presense(self, online): for _ in range(10): time.sleep(2) <mask>: break if not online and not self._is_testbot_online(): break else: raise AssertionError( ""test bot is still {}"".format(""offline"" if online else ""online"") )",False,if online and self . _is_testbot_online ( ) :,if online and self . _is_testbot_offline ( ) :,0.8820016898747209,78.25422900366432
"def find(self, path): if os.path.isfile(path) or os.path.islink(path): self.num_files = self.num_files + 1 <mask>: self.files.append(path) elif os.path.isdir(path): for content in os.listdir(path): file = os.path.join(path, content) if os.path.isfile(file) or os.path.islink(file): self.num_files = self.num_files + 1 if self.match_function(file): self.files.append(file) else: self.find(file)",True,if self . match_function ( path ) :,if self . match_function ( path ) :,0.8820016898747209,100.00000000000004
"def optimize(self, graph: Graph): MAX_TEXTURE_SIZE = config.WEBGL_MAX_TEXTURE_SIZE flag_changed = False for v in traverse.listup_variables(graph): if not Placeholder.check_resolved(v.size): continue height, width = TextureShape.get(v) <mask>: continue if not v.has_attribute(SplitTarget): flag_changed = True v.attributes.add(SplitTarget()) return graph, flag_changed",False,if height <= MAX_TEXTURE_SIZE and width <= MAX_TEXTURE_SIZE :,if height > MAX_TEXTURE_SIZE :,0.8820016898747209,21.74757062134855
"def brightness_func(args): device = _get_device_from_filter(args) if args.set is None: # Get brightness if args.raw: print(str(device.brightness)) else: print(""Brightness: {0}%"".format(device.brightness)) else: brightness_value = float(_clamp_u8(args.set)) <mask>: print(""Setting brightness to {0}%"".format(brightness_value)) device.brightness = brightness_value",False,if not args . raw :,if args . raw :,0.8820016898747209,57.89300674674101
"def _setup(self, field_name, owner_model): # Resolve possible name-based model reference. if not self.model_class: <mask>: self.model_class = owner_model else: raise Exception( ""ModelType: Unable to resolve model '{}'."".format(self.model_name) ) super(ModelType, self)._setup(field_name, owner_model)",False,if self . model_name == owner_model . __name__ :,if owner_model :,0.8820016898747209,3.355687704487231
"def build_json_schema_object(cls, parent_builder=None): builder = builders.ObjectBuilder(cls, parent_builder) if builder.count_type(builder.type) > 1: return builder for _, name, field in cls.iterate_with_name(): if isinstance(field, fields.EmbeddedField): builder.add_field(name, field, _parse_embedded(field, builder)) <mask>: builder.add_field(name, field, _parse_list(field, builder)) else: builder.add_field(name, field, _create_primitive_field_schema(field)) return builder",True,"elif isinstance ( field , fields . ListField ) :","elif isinstance ( field , fields . ListField ) :",0.8820016898747209,100.00000000000004
"def filter_module(mod, type_req=None, subclass_req=None): for name in dir(mod): val = getattr(mod, name) if type_req is not None and not isinstance(val, type_req): continue <mask>: continue yield name, val",False,"if subclass_req is not None and not issubclass ( val , subclass_req ) :",if subclass_req is not None and not subclass_req . is_subclass ( val ) :,0.8820016898747209,53.05903666964891
"def get_icon(self): if self.icon is not None: # Load it from an absolute filename if os.path.exists(self.icon): try: return GdkPixbuf.Pixbuf.new_from_file_at_size(self.icon, 24, 24) except GObject.GError as ge: pass # Load it from the current icon theme (icon_name, extension) = os.path.splitext(os.path.basename(self.icon)) theme = Gtk.IconTheme() <mask>: return theme.load_icon(icon_name, 24, 0)",False,if theme . has_icon ( icon_name ) :,"if extension == ""png"" :",0.8820016898747209,3.983253478176822
"def sysctlTestAndSet(name, limit): ""Helper function to set sysctl limits"" # convert non-directory names into directory names if ""/"" not in name: name = ""/proc/sys/"" + name.replace(""."", ""/"") # read limit with open(name, ""r"") as readFile: oldLimit = readFile.readline() if isinstance(limit, int): # compare integer limits before overriding <mask>: with open(name, ""w"") as writeFile: writeFile.write(""%d"" % limit) else: # overwrite non-integer limits with open(name, ""w"") as writeFile: writeFile.write(limit)",False,if int ( oldLimit ) < limit :,if limit != oldLimit :,0.8820016898747209,8.22487964923291
"def _wait_for_bot_presense(self, online): for _ in range(10): time.sleep(2) if online and self._is_testbot_online(): break <mask>: break else: raise AssertionError( ""test bot is still {}"".format(""offline"" if online else ""online"") )",False,if not online and not self . _is_testbot_online ( ) :,elif online and self . _is_testbot_offline ( ) :,0.8820016898747209,49.68271843768451
"def handle(self, context, sign, *args): if context.rounding in (ROUND_HALF_UP, ROUND_HALF_EVEN, ROUND_HALF_DOWN, ROUND_UP): return Infsign[sign] if sign == 0: if context.rounding == ROUND_CEILING: return Infsign[sign] return Decimal((sign, (9,) * context.prec, context.Emax - context.prec + 1)) if sign == 1: <mask>: return Infsign[sign] return Decimal((sign, (9,) * context.prec, context.Emax - context.prec + 1))",False,if context . rounding == ROUND_FLOOR :,if context . rounding == ROUND_CEILING :,0.8820016898747209,78.25422900366438
"def _get_item_columns_panel(items, rows): hbox = Gtk.HBox(False, 4) n_item = 0 col_items = 0 vbox = Gtk.VBox() hbox.pack_start(vbox, False, False, 0) while n_item < len(items): item = items[n_item] vbox.pack_start(item, False, False, 0) n_item += 1 col_items += 1 <mask>: vbox = Gtk.VBox() hbox.pack_start(vbox, False, False, 0) col_items = 0 return hbox",False,if col_items > rows :,if col_items == rows :,0.8820016898747209,41.11336169005198
"def _changed(self): if self.gtk_range.get_sensitive(): <mask>: self.timer.cancel() self.timer = _Timer(0.5, lambda: GLib.idle_add(self._write)) self.timer.start()",True,if self . timer :,if self . timer :,0.8820016898747209,100.00000000000004
"def unlock_graph(result, callback, interval=1, propagate=False, max_retries=None): if result.ready(): second_level_res = result.get() <mask>: with allow_join_result(): signature(callback).delay( list(joinall(second_level_res, propagate=propagate)) ) else: unlock_graph.retry(countdown=interval, max_retries=max_retries)",False,if second_level_res . ready ( ) :,if second_level_res :,0.8820016898747209,47.486944442513455
"def update(self, other=None, /, **kwargs): if self._pending_removals: self._commit_removals() d = self.data if other is not None: <mask>: other = dict(other) for key, o in other.items(): d[key] = KeyedRef(o, self._remove, key) for key, o in kwargs.items(): d[key] = KeyedRef(o, self._remove, key)",False,"if not hasattr ( other , ""items"" ) :","if isinstance ( other , dict ) :",0.8820016898747209,18.594002123233256
"def default(self, o): try: if type(o) == datetime.datetime: return str(o) else: # remove unwanted attributes from the provider object during conversion to json if hasattr(o, ""profile""): del o.profile if hasattr(o, ""credentials""): del o.credentials if hasattr(o, ""metadata_path""): del o.metadata_path <mask>: del o.services_config return vars(o) except Exception as e: return str(o)",True,"if hasattr ( o , ""services_config"" ) :","if hasattr ( o , ""services_config"" ) :",0.8820016898747209,100.00000000000004
"def read(self, count=True, timeout=None, ignore_non_errors=True, ignore_timeouts=True): try: return self._read(count, timeout) except usb.USBError as e: <mask>: log.info( ""read: e.errno=%s e.strerror=%s e.message=%s repr=%s"" % (e.errno, e.strerror, e.message, repr(e)) ) if ignore_timeouts and is_timeout(e): return [] if ignore_non_errors and is_noerr(e): return [] raise",False,if DEBUG_COMM :,if log . isEnabledFor ( logging . INFO ) :,0.8820016898747209,4.990049701936832
def heal(self): if not self.doctors: return proc_ids = self._get_process_ids() for proc_id in proc_ids: # get proc every time for latest state proc = PipelineProcess.objects.get(id=proc_id) <mask>: continue for dr in self.doctors: if dr.confirm(proc): dr.cure(proc) break,False,if not proc . is_alive or proc . is_frozen :,if not proc :,0.8820016898747209,5.2447643832804935
"def to_value(self, value): # Tip: 'value' is the object returned by # taiga.projects.history.models.HistoryEntry.values_diff() ret = {} for key, val in value.items(): <mask>: ret[key] = val elif key == ""points"": ret[key] = {k: {""from"": v[0], ""to"": v[1]} for k, v in val.items()} else: ret[key] = {""from"": val[0], ""to"": val[1]} return ret",False,"if key in [ ""attachments"" , ""custom_attributes"" , ""description_diff"" ] :","if key == ""items"" :",0.8820016898747209,2.7347280853315854
"def default_generator( self, dataset, epochs=1, mode=""fit"", deterministic=True, pad_batches=True ): for epoch in range(epochs): for (X_b, y_b, w_b, ids_b) in dataset.iterbatches( batch_size=self.batch_size, deterministic=deterministic, pad_batches=pad_batches, ): <mask>: dropout = np.array(0.0) else: dropout = np.array(1.0) yield ([X_b, dropout], [y_b], [w_b])",False,"if mode == ""predict"" :","if mode == ""fit"" :",0.8820016898747209,59.4603557501361
"def _cygwin_hack_find_addresses(target): addresses = [] for h in [ target, ""localhost"", ""127.0.0.1"", ]: try: addr = get_local_ip_for(h) <mask>: addresses.append(addr) except socket.gaierror: pass return defer.succeed(addresses)",False,if addr not in addresses :,if addr :,0.8820016898747209,0.0
"def _get_notify(self, action_node): if action_node.name not in self._skip_notify_tasks: if action_node.notify: task_notify = NotificationsHelper.to_model(action_node.notify) return task_notify <mask>: return self._chain_notify return None",False,elif self . _chain_notify :,if self . _chain_notify :,0.8820016898747209,84.08964152537145
"def filterTokenLocation(): i = None entry = None token = None tokens = [] i = 0 while 1: <mask>: break entry = extra.tokens[i] token = jsdict( { ""type"": entry.type, ""value"": entry.value, } ) if extra.range: token.range = entry.range if extra.loc: token.loc = entry.loc tokens.append(token) i += 1 extra.tokens = tokens",False,if not ( i < len ( extra . tokens ) ) :,if i is None :,0.8820016898747209,2.8383688870107915
"def read(self, size=-1): buf = bytearray() while size != 0 and self.cursor < self.maxpos: if not self.in_current_block(self.cursor): self.seek_to_block(self.cursor) part = self.current_stream.read(size) <mask>: if len(part) == 0: raise EOFError() size -= len(part) self.cursor += len(part) buf += part return bytes(buf)",False,if size > 0 :,if part :,0.8820016898747209,0.0
"def get_properties_from_model(model_class): """"""Show properties from a model"""""" properties = [] attr_names = [name for (name, value) in inspect.getmembers(model_class, isprop)] for attr_name in attr_names: <mask>: attr_names.remove(attr_name) else: properties.append( dict(label=attr_name, name=attr_name.strip(""_"").replace(""_"", "" "")) ) return sorted(properties, key=lambda k: k[""label""])",False,"if attr_name . endswith ( ""pk"" ) :",if attr_name in model_class . __dict__ :,0.8820016898747209,18.20705281109213
"def __getitem__(self, name, set=set, getattr=getattr, id=id): visited = set() mydict = self.basedict while 1: value = mydict[name] if value is not None: return value myid = id(mydict) assert myid not in visited visited.add(myid) mydict = mydict.Parent <mask>: return",False,if mydict is None :,if myid not in visited :,0.8820016898747209,9.652434877402245
"def multicolumn(self, list, format, cols=4): """"""Format a list of items into a multi-column list."""""" result = """" rows = (len(list) + cols - 1) // cols for col in range(cols): result = result + '<td width=""%d%%"" valign=top>' % (100 // cols) for i in range(rows * col, rows * col + rows): <mask>: result = result + format(list[i]) + ""<br>\n"" result = result + ""</td>"" return '<table width=""100%%"" summary=""list""><tr>%s</tr></table>' % result",False,if i < len ( list ) :,if list [ i ] :,0.8820016898747209,8.22487964923291
"def format_exc(exc=None): """"""Return exc (or sys.exc_info if None), formatted."""""" try: <mask>: exc = _exc_info() if exc == (None, None, None): return """" import traceback return """".join(traceback.format_exception(*exc)) finally: del exc",True,if exc is None :,if exc is None :,0.8820016898747209,100.00000000000004
"def assert_counts(res, lang, files, blank, comment, code): for line in res: fields = line.split() if len(fields) >= 5: <mask>: self.assertEqual(files, int(fields[1])) self.assertEqual(blank, int(fields[2])) self.assertEqual(comment, int(fields[3])) self.assertEqual(code, int(fields[4])) return self.fail(""Found no output line for {}"".format(lang))",True,if fields [ 0 ] == lang :,if fields [ 0 ] == lang :,0.8820016898747209,100.00000000000004
"def __iter__(self): for name, value in self.__class__.__dict__.items(): <mask>: continue if isinstance(value, flag_value): yield (name, self._has_flag(value.flag))",False,"if isinstance ( value , alias_flag_value ) :",if name in self . _ignore_names :,0.8820016898747209,4.85851417160653
"def optimize_models(args, use_cuda, models): """"""Optimize ensemble for generation"""""" for model in models: model.make_generation_fast_( beamable_mm_beam_size=None if args.no_beamable_mm else args.beam, need_attn=args.print_alignment, ) if args.fp16: model.half() <mask>: model.cuda()",False,if use_cuda :,elif use_cuda :,0.8820016898747209,66.87403049764218
"def convertstore(self, mydict): targetheader = self.mypofile.header() targetheader.addnote(""extracted from web2py"", ""developer"") for source_str in mydict.keys(): target_str = mydict[source_str] if target_str == source_str: # a convention with new (untranslated) web2py files target_str = u"""" <mask>: # an older convention target_str = u"""" pounit = self.convertunit(source_str, target_str) self.mypofile.addunit(pounit) return self.mypofile",False,"elif target_str . startswith ( u""*** "" ) :",elif target_str > source_str :,0.8820016898747209,16.203364897827715
"def __sparse_values_set(instances, static_col_indexes: list): tmp_result = {idx: set() for idx in static_col_indexes} for _, instance in instances: data_generator = instance.features.get_all_data() for idx, value in data_generator: <mask>: continue tmp_result[idx].add(value) result = [tmp_result[x] for x in static_col_indexes] return result",False,if idx not in tmp_result :,if value is None :,0.8820016898747209,6.9717291216921975
def puts(self): <mask>: self.lazy_init_lock_.acquire() try: if self.puts_ is None: self.puts_ = PutRequest() finally: self.lazy_init_lock_.release() return self.puts_,True,if self . puts_ is None :,if self . puts_ is None :,0.8820016898747209,100.00000000000004
"def run(self, args, **kwargs): if args.resource_ref or args.policy_type: filters = {} if args.resource_ref: filters[""resource_ref""] = args.resource_ref <mask>: filters[""policy_type""] = args.policy_type filters.update(**kwargs) return self.manager.query(**filters) else: return self.manager.get_all(**kwargs)",True,if args . policy_type :,if args . policy_type :,0.8820016898747209,100.00000000000004
"def Get_Gene(self, id): """"""Retreive the gene name (GN)."""""" entry = self.Get(id) if not entry: return None GN = """" for line in string.split(entry, ""\n""): if line[0:5] == ""GN "": GN = string.strip(line[5:]) <mask>: GN = GN[0:-1] return GN if line[0:2] == ""//"": break return GN",False,"if GN [ - 1 ] == ""."" :","if GN . endswith ( ""/"" ) :",0.8820016898747209,9.042713792226897
"def processMovie(self, atom): for field in atom: <mask>: self.processTrack(field[""track""]) if ""movie_hdr"" in field: self.processMovieHeader(field[""movie_hdr""])",True,"if ""track"" in field :","if ""track"" in field :",0.8820016898747209,100.00000000000004
"def get_next_video_frame(self, skip_empty_frame=True): if not self.video_format: return while True: # We skip video packets which are not video frames # This happens in mkv files for the first few frames. video_packet = self._get_video_packet() <mask>: self._decode_video_packet(video_packet) if video_packet.image is not None or not skip_empty_frame: break if _debug: print(""Returning"", video_packet) return video_packet.image",False,if video_packet . image == 0 :,if video_packet :,0.8820016898747209,26.013004751144457
"def get_devices(display=None): base = ""/dev/input"" for filename in os.listdir(base): if filename.startswith(""event""): path = os.path.join(base, filename) <mask>: continue try: _devices[path] = EvdevDevice(display, path) except OSError: pass return list(_devices.values())",False,if path in _devices :,if not os . path . exists ( path ) :,0.8820016898747209,4.9323515694897075
"def _ensure_header_written(self, datasize): if not self._headerwritten: if not self._nchannels: raise Error(""# channels not specified"") <mask>: raise Error(""sample width not specified"") if not self._framerate: raise Error(""sampling rate not specified"") self._write_header(datasize)",False,if not self . _sampwidth :,if not self . _samplewidth :,0.8820016898747209,64.34588841607616
"def process(self, fuzzresult): base_url = urljoin(fuzzresult.url, "".."") for line in fuzzresult.history.content.splitlines(): record = line.split(""/"") if len(record) == 6 and record[1]: self.queue_url(urljoin(base_url, record[1])) # Directory <mask>: self.queue_url(urljoin(base_url, record[1])) self.queue_url(urljoin(base_url, ""%s/CVS/Entries"" % (record[1])))",False,"if record [ 0 ] == ""D"" :",elif len ( record ) == 4 and record [ 1 ] :,0.8820016898747209,8.889175589171739
"def tearDown(self): """"""Shutdown the UDP server."""""" try: <mask>: self.server.stop(2.0) if self.sock_hdlr: self.root_logger.removeHandler(self.sock_hdlr) self.sock_hdlr.close() finally: BaseTest.tearDown(self)",True,if self . server :,if self . server :,0.8820016898747209,100.00000000000004
"def get_backend(find_library=None): try: global _lib, _ctx <mask>: _lib = _load_library(find_library) _setup_prototypes(_lib) _ctx = _Context() _logger.warning( ""OpenUSB backend deprecated (https://github.com/pyusb/pyusb/issues/284)"" ) return _OpenUSB() except usb.libloader.LibraryException: # exception already logged (if any) _logger.error(""Error loading OpenUSB backend"", exc_info=False) return None except Exception: _logger.error(""Error loading OpenUSB backend"", exc_info=True) return None",False,if _lib is None :,if find_library :,0.8820016898747209,11.51015341649912
"def __init__(self, event, event_info, fields=[]): _wmi_object.__init__(self, event, fields=fields) _set(self, ""event_type"", None) _set(self, ""timestamp"", None) _set(self, ""previous"", None) if event_info: event_type = self.event_type_re.match(event_info.Path_.Class).group(1).lower() _set(self, ""event_type"", event_type) if hasattr(event_info, ""TIME_CREATED""): _set(self, ""timestamp"", from_1601(event_info.TIME_CREATED)) <mask>: _set(self, ""previous"", event_info.PreviousInstance)",True,"if hasattr ( event_info , ""PreviousInstance"" ) :","if hasattr ( event_info , ""PreviousInstance"" ) :",0.8820016898747209,100.00000000000004
"def _getListNextPackagesReadyToBuild(): for pkg in Scheduler.listOfPackagesToBuild: <mask>: continue if constants.rpmCheck or Scheduler._checkNextPackageIsReadyToBuild(pkg): Scheduler.listOfPackagesNextToBuild.put((-Scheduler._getPriority(pkg), pkg)) Scheduler.logger.debug(""Adding "" + pkg + "" to the schedule list"")",False,if pkg in Scheduler . listOfPackagesCurrentlyBuilding :,if pkg in Scheduler . listOfPackagesNextToBuild :,0.8820016898747209,64.34588841607616
"def process_all(self, lines, times=1): gap = False for _ in range(times): for line in lines: <mask>: self.write("""") self.process(line) if not is_command(line): gap = True return 0",True,if gap :,if gap :,0.8820016898747209,0.0
"def diff(old, new, display=True): """"""Nice colored diff implementation"""""" if not isinstance(old, list): old = decolorize(str(old)).splitlines() if not isinstance(new, list): new = decolorize(str(new)).splitlines() line_types = {"" "": ""%Reset"", ""-"": ""%Red"", ""+"": ""%Green"", ""?"": ""%Pink""} if display: for line in difflib.Differ().compare(old, new): <mask>: continue print(colorize(line_types[line[0]], line)) return old != new",False,"if line . startswith ( ""?"" ) :",if not line :,0.8820016898747209,4.690733795095046
"def get_limit(self, request): if self.limit_query_param: try: limit = int(request.query_params[self.limit_query_param]) if limit < 0: raise ValueError() # Enforce maximum page size, if defined if settings.MAX_PAGE_SIZE: <mask>: return settings.MAX_PAGE_SIZE else: return min(limit, settings.MAX_PAGE_SIZE) return limit except (KeyError, ValueError): pass return self.default_limit",True,if limit == 0 :,if limit == 0 :,0.8820016898747209,100.00000000000004
"def slice_fill(self, slice_): ""Fills the slice with zeroes for the dimensions that have single elements and squeeze_dims true"" if isinstance(self.indexes, int): new_slice_ = [0] offset = 0 else: new_slice_ = [slice_[0]] offset = 1 for i in range(1, len(self.nums)): if self.squeeze_dims[i]: new_slice_.append(0) <mask>: new_slice_.append(slice_[offset]) offset += 1 new_slice_ += slice_[offset:] return new_slice_",False,elif offset < len ( slice_ ) :,elif self . indices [ i ] :,0.8820016898747209,5.795599612995366
"def wrapper(*args, **kw): instance = args[0] try: <mask>: ret_dict = instance._create_ret_object( instance.FAILURE, None, True, instance.MUST_JSON ) instance.logger.error(instance.MUST_JSON) return jsonify(ret_dict), 400 except BadRequest: ret_dict = instance._create_ret_object( instance.FAILURE, None, True, instance.MUST_JSON ) instance.logger.error(instance.MUST_JSON) return jsonify(ret_dict), 400 instance.logger.debug(""JSON is valid"") return f(*args, **kw)",False,if request . get_json ( ) is None :,if instance . status == 404 :,0.8820016898747209,4.995138898472386
"def add_css(self, data): if data: for medium, paths in data.items(): for path in paths: <mask>: self._css.setdefault(medium, []).append(path)",False,if not self . _css . get ( medium ) or path not in self . _css [ medium ] :,"if path not in self . _css . get ( medium , [ ] ) :",0.8820016898747209,47.08533500459053
"def mangle_template(template: str, template_vars: Set[str]) -> str: if TEMPLATE_PREFIX in template or TEMPLATE_SUFFIX in template: raise Exception(""Cannot parse a template containing reserved strings"") for var in template_vars: original = f""{{{var}}}"" <mask>: raise Exception( f'Template string is missing a reference to ""{var}"" referred to in kwargs' ) template = template.replace(original, mangled_name(var)) return template",False,if original not in template :,if not original :,0.8820016898747209,13.70155798417842
"def filterSimilarKeywords(keyword, kwdsIterator): """"""Return a sorted list of keywords similar to the one given."""""" seenDict = {} kwdSndx = soundex(keyword.encode(""ascii"", ""ignore"")) matches = [] matchesappend = matches.append checkContained = False if len(keyword) > 4: checkContained = True for movieID, key in kwdsIterator: if key in seenDict: continue seenDict[key] = None if checkContained and keyword in key: matchesappend(key) continue <mask>: matchesappend(key) return _sortKeywords(keyword, matches)",False,"if kwdSndx == soundex ( key . encode ( ""ascii"" , ""ignore"" ) ) :",if kwdSndx in key :,0.8820016898747209,1.26492199361622
"def GetInfo(self): for k, v in sorted(self.memory_parameters.items()): <mask>: continue if not v: continue print(""%s: \t%#08x (%s)"" % (k, v, v)) print(""Memory ranges:"") print(""Start\t\tEnd\t\tLength"") for start, length in self.runs: print(""0x%X\t\t0x%X\t\t0x%X"" % (start, start + length, length))",False,"if k . startswith ( ""Pad"" ) :",if not k :,0.8820016898747209,4.690733795095046
"def Children(self): """"""Returns a list of all of this object's owned (strong) children."""""" children = [] for property, attributes in self._schema.iteritems(): (is_list, property_type, is_strong) = attributes[0:3] if is_strong and property in self._properties: <mask>: children.append(self._properties[property]) else: children.extend(self._properties[property]) return children",False,if not is_list :,if is_list :,0.8820016898747209,57.89300674674101
"def normalize_res_identifier(self, emu, cw, val): mask = (16 ** (emu.get_ptr_size() // 2) - 1) << 16 if val & mask: # not an INTRESOURCE name = emu.read_mem_string(val, cw) <mask>: try: name = int(name[1:]) except Exception: return 0 else: name = val return name",False,"if name [ 0 ] == ""#"" :","if name . startswith ( ""INTRESOURCE"" ) :",0.8820016898747209,9.548450962056531
"def _optimize(self, solutions): best_a = None best_silhouette = None best_k = None for a, silhouette, k in solutions(): if best_silhouette is None: pass <mask>: break best_silhouette = silhouette best_a = a best_k = k return best_a, best_silhouette, best_k",False,elif silhouette <= best_silhouette :,if silhouette is None :,0.8820016898747209,6.9717291216921975
"def find_commit_type(sha): try: o = obj_store[sha] except KeyError: <mask>: raise else: if isinstance(o, Commit): commits.add(sha) elif isinstance(o, Tag): tags.add(sha) commits.add(o.object[1]) else: raise KeyError(""Not a commit or a tag: %s"" % sha)",False,if not ignore_unknown :,if not o :,0.8820016898747209,21.444097124017667
"def on_search_entry_keypress(self, widget, event): key = Gdk.keyval_name(event.keyval) if key == ""Escape"": self.hide_search_box() elif key == ""Return"": # Combine with Shift? <mask>: self.search_prev = False self.do_search(None) else: self.search_prev = True",False,if event . state & Gdk . ModifierType . SHIFT_MASK :,if self . search_prev :,0.8820016898747209,3.9413751108533592
"def process_webhook_prop(namespace): if not isinstance(namespace.webhook_properties, list): return result = {} for each in namespace.webhook_properties: <mask>: if ""="" in each: key, value = each.split(""="", 1) else: key, value = each, """" result[key] = value namespace.webhook_properties = result",False,if each :,"if isinstance ( each , str ) :",0.8820016898747209,7.267884212102741
"def run(self): global WAITING_BEFORE_START time.sleep(WAITING_BEFORE_START) while self.keep_alive: path_id, module, resolve = self.queue_receive.get() <mask>: continue self.lock.acquire() self.modules[path_id] = module self.lock.release() if resolve: resolution = self._resolve_with_other_modules(resolve) self._relations[path_id] = [] for package in resolution: self._relations[path_id].append(resolution[package]) self.queue_send.put((path_id, module, False, resolution))",True,if path_id is None :,if path_id is None :,0.8820016898747209,100.00000000000004
"def _get_download_link(self, url, download_type=""torrent""): links = { ""torrent"": """", ""magnet"": """", } try: data = self.session.get(url).text with bs4_parser(data) as html: downloads = html.find(""div"", {""class"": ""download""}) if downloads: for download in downloads.findAll(""a""): link = download[""href""] <mask>: links[""magnet""] = link else: links[""torrent""] = urljoin(self.urls[""base_url""], link) except Exception: pass return links[download_type]",False,"if link . startswith ( ""magnet"" ) :","if download_type == ""magnet"" :",0.8820016898747209,17.747405280050266
"def _parse_fields(cls, read): read = unicode_to_str(read) if type(read) is not str: _wrong_type_for_arg(read, ""str"", ""read"") fields = {} while read and read[0] != "";"": <mask>: DeserializeError(read, ""does not separate fields with commas"") read = read[1:] key, _type, value, read = cls._parse_field(read) fields[key] = (_type, value) if read: # read[0] == ';' read = read[1:] return fields, read",False,"if read and read [ 0 ] != "","" :","if read [ 0 ] == "","" :",0.8820016898747209,52.01772843458376
"def _convertDict(self, d): r = {} for k, v in d.items(): <mask>: v = str(v, ""utf-8"") elif isinstance(v, list) or isinstance(v, tuple): v = self._convertList(v) elif isinstance(v, dict): v = self._convertDict(v) if isinstance(k, bytes): k = str(k, ""utf-8"") r[k] = v return r",True,"if isinstance ( v , bytes ) :","if isinstance ( v , bytes ) :",0.8820016898747209,100.00000000000004
"def wrapper(filename): mtime = getmtime(filename) with lock: if filename in cache: old_mtime, result = cache.pop(filename) <mask>: # Move to the end cache[filename] = old_mtime, result return result result = function(filename) with lock: cache[filename] = mtime, result # at the end if len(cache) > max_size: cache.popitem(last=False) return result",False,if old_mtime == mtime :,if old_mtime != mtime :,0.8820016898747209,50.000000000000014
def isFinished(self): # returns true if episode timesteps has reached episode length and resets the task if self.count > self.epiLen: self.res() return True else: <mask>: self.pertGlasPos(0) if self.count == self.epiLen / 2 + 1: self.env.reset() self.pertGlasPos(1) self.count += 1 return False,False,if self . count == 1 :,if self . count == self . eposLen / 2 + 1 :,0.8820016898747209,39.34995962231127
"def _check_vulnerabilities(self, processed_analysis): matched_vulnerabilities = list() for vulnerability in self._rule_base_vulnerabilities: <mask>: vulnerability_data = vulnerability.get_dict() name = vulnerability_data.pop(""short_name"") matched_vulnerabilities.append((name, vulnerability_data)) return matched_vulnerabilities",False,"if evaluate ( processed_analysis , vulnerability . rule ) :",if vulnerability . get_name ( ) == processed_analysis . get_name ( ) :,0.8820016898747209,11.154288433080788
"def _table_reprfunc(self, row, col, val): if self._table.column_names[col].endswith(""Size""): <mask>: return "" %s"" % val elif val < 1024 ** 2: return "" %.1f KB"" % (val / 1024.0 ** 1) elif val < 1024 ** 3: return "" %.1f MB"" % (val / 1024.0 ** 2) else: return "" %.1f GB"" % (val / 1024.0 ** 3) if col in (0, """"): return str(val) else: return "" %s"" % val",False,"if isinstance ( val , compat . string_types ) :",if val < 1024 ** 1 :,0.8820016898747209,4.408194605881708
"def serve_until_stopped(self) -> None: while True: rd, wr, ex = select.select([self.socket.fileno()], [], [], self.timeout) if rd: self.handle_request() <mask>: break",False,if self . event is not None and self . event . is_set ( ) :,if ex :,0.8820016898747209,0.0
"def resize(self, *e): bold = (""helvetica"", -self._size.get(), ""bold"") helv = (""helvetica"", -self._size.get()) xspace = self._size.get() yspace = self._size.get() for widget in self._widgets: widget[""node_font""] = bold widget[""leaf_font""] = helv widget[""xspace""] = xspace widget[""yspace""] = yspace if self._size.get() < 20: widget[""line_width""] = 1 <mask>: widget[""line_width""] = 2 else: widget[""line_width""] = 3 self._layout()",False,elif self . _size . get ( ) < 30 :,elif self . _size . get ( ) < 40 :,0.8820016898747209,82.651681837938
"def __assertTilesChangedInRegion(self, t1, t2, region): for tileOriginTuple in t1.keys(): tileOrigin = imath.V2i(*tileOriginTuple) tileRegion = imath.Box2i( tileOrigin, tileOrigin + imath.V2i(GafferImage.ImagePlug.tileSize()) ) <mask>: self.assertNotEqual(t1[tileOriginTuple], t2[tileOriginTuple]) else: self.assertEqual(t1[tileOriginTuple], t2[tileOriginTuple])",False,"if GafferImage . BufferAlgo . intersects ( tileRegion , region ) :",if region == tileRegion :,0.8820016898747209,4.222794013898957
"def grouped_by_prefix(args, prefixes): """"""Group behave args by (directory) scope into multiple test-runs."""""" group_args = [] current_scope = None for arg in args.strip().split(): assert not arg.startswith(""-""), ""REQUIRE: arg, not options"" scope = select_prefix_for(arg, prefixes) if scope != current_scope: <mask>: # -- DETECTED GROUP-END: yield "" "".join(group_args) group_args = [] current_scope = scope group_args.append(arg) if group_args: yield "" "".join(group_args)",True,if group_args :,if group_args :,0.8820016898747209,100.00000000000004
"def __print__(self, defaults=False): if defaults: print_func = str else: print_func = repr pieces = [] default_values = self.__defaults__ for k in self.__fields__: value = getattr(self, k) <mask>: continue if isinstance(value, basestring): print_func = repr # keep quotes around strings pieces.append(""%s=%s"" % (k, print_func(value))) if pieces or self.__base__: return ""%s(%s)"" % (self.__class__.__name__, "", "".join(pieces)) else: return """"",False,if not defaults and value == default_values [ k ] :,if value in default_values :,0.8820016898747209,10.694820729788422
"def setInnerHTML(self, html): log.HTMLClassifier.classify( log.ThugLogging.url if log.ThugOpts.local else log.last_url, html ) self.tag.clear() for node in bs4.BeautifulSoup(html, ""html.parser"").contents: self.tag.append(node) name = getattr(node, ""name"", None) <mask>: continue handler = getattr(log.DFT, ""handle_%s"" % (name,), None) if handler: handler(node)",True,if name is None :,if name is None :,0.8820016898747209,100.00000000000004
"def createFields(self): yield Enum(Bits(self, ""class"", 2), self.CLASS_DESC) yield Enum(Bit(self, ""form""), self.FORM_DESC) if self[""class""].value == 0: yield Enum(Bits(self, ""type"", 5), self.TYPE_DESC) else: yield Bits(self, ""type"", 5) yield ASNInteger(self, ""size"", ""Size in bytes"") size = self[""size""].value if size: <mask>: for field in self._handler(self, size): yield field else: yield RawBytes(self, ""raw"", size)",True,if self . _handler :,if self . _handler :,0.8820016898747209,100.00000000000004
"def _process_service_request(self, pkttype, pktid, packet): """"""Process a service request"""""" # pylint: disable=unused-argument service = packet.get_string() packet.check_end() if service == self._next_service: self.logger.debug2(""Accepting request for service %s"", service) self._next_service = None self.send_packet(MSG_SERVICE_ACCEPT, String(service)) <mask>: # pragma: no branch self._auth_in_progress = True self._send_deferred_packets() else: raise DisconnectError( DISC_SERVICE_NOT_AVAILABLE, ""Unexpected service request received"" )",False,if self . is_server ( ) and service == _USERAUTH_SERVICE :,elif service == self . _next_service :,0.8820016898747209,11.434735332025594
"def _read_fixed_body( self, content_length: int, delegate: httputil.HTTPMessageDelegate ) -> None: while content_length > 0: body = await self.stream.read_bytes( min(self.params.chunk_size, content_length), partial=True ) content_length -= len(body) <mask>: with _ExceptionLoggingContext(app_log): ret = delegate.data_received(body) if ret is not None: await ret",False,if not self . _write_finished or self . is_client :,if body is not None :,0.8820016898747209,2.5612540390806937
"def wait_for_child(pid, timeout=1.0): deadline = mitogen.core.now() + timeout while timeout < mitogen.core.now(): try: target_pid, status = os.waitpid(pid, os.WNOHANG) <mask>: return except OSError: e = sys.exc_info()[1] if e.args[0] == errno.ECHILD: return time.sleep(0.05) assert False, ""wait_for_child() timed out""",False,if target_pid == pid :,if status == 0 :,0.8820016898747209,13.83254362586636
"def execute(cls, ctx, op: ""DataFrameGroupByAgg""): try: pd.set_option(""mode.use_inf_as_na"", op.use_inf_as_na) if op.stage == OperandStage.map: cls._execute_map(ctx, op) <mask>: cls._execute_combine(ctx, op) elif op.stage == OperandStage.agg: cls._execute_agg(ctx, op) else: # pragma: no cover raise ValueError(""Aggregation operand not executable"") finally: pd.reset_option(""mode.use_inf_as_na"")",True,elif op . stage == OperandStage . combine :,elif op . stage == OperandStage . combine :,0.8820016898747209,100.00000000000004
def cut(sentence): sentence = strdecode(sentence) blocks = re_han.split(sentence) for blk in blocks: if re_han.match(blk): for word in __cut(blk): <mask>: yield word else: for c in word: yield c else: tmp = re_skip.split(blk) for x in tmp: if x: yield x,False,if word not in Force_Split_Words :,if word :,0.8820016898747209,0.0
"def _iter_tags(self, type=None): """"""Yield all raw tags (limit to |type| if specified)"""""" for n in itertools.count(): tag = self._get_tag(n) <mask>: yield tag if tag[""d_tag""] == ""DT_NULL"": break",False,"if type is None or tag [ ""d_tag"" ] == type :","if type is not None and tag [ ""d_tag"" ] == type :",0.8820016898747209,72.76817202342096
"def reverse_search_history(self, searchfor, startpos=None): if startpos is None: startpos = self.history_cursor if _ignore_leading_spaces: res = [ (idx, line.lstrip()) for idx, line in enumerate(self.history[startpos:0:-1]) if line.lstrip().startswith(searchfor.lstrip()) ] else: res = [ (idx, line) for idx, line in enumerate(self.history[startpos:0:-1]) <mask>: ] if res: self.history_cursor -= res[0][0] return res[0][1].get_line_text() return """"",False,if line . startswith ( searchfor ),if line . startswith ( searchfor . lstrip ( ) ),0.8820016898747209,47.987820666906615
"def value_to_db_datetime(self, value): if value is None: return None # Oracle doesn't support tz-aware datetimes if timezone.is_aware(value): <mask>: value = value.astimezone(timezone.utc).replace(tzinfo=None) else: raise ValueError( ""Oracle backend does not support timezone-aware datetimes when USE_TZ is False."" ) return unicode(value)",False,if settings . USE_TZ :,if self . USE_TZ :,0.8820016898747209,64.34588841607616
"def _sniff(filename, oxlitype): try: with open(filename, ""rb"") as fileobj: header = fileobj.read(4) if header == b""OXLI"": fileobj.read(1) # skip the version number ftype = fileobj.read(1) <mask>: return True return False except OSError: return False",False,if binascii . hexlify ( ftype ) == oxlitype :,"if oxlitype == b""OXL"" :",0.8820016898747209,9.600960275119885
"def unget(self, char): # Only one character is allowed to be ungotten at once - it must # be consumed again before any further call to unget if char is not EOF: <mask>: # unget is called quite rarely, so it's a good idea to do # more work here if it saves a bit of work in the frequently # called char and charsUntil. # So, just prepend the ungotten character onto the current # chunk: self.chunk = char + self.chunk self.chunkSize += 1 else: self.chunkOffset -= 1 assert self.chunk[self.chunkOffset] == char",True,if self . chunkOffset == 0 :,if self . chunkOffset == 0 :,0.8820016898747209,100.00000000000004
"def scan(rule, extensions, paths, ignore_paths=None): """"""The libsast scan."""""" try: options = { ""match_rules"": rule, ""match_extensions"": extensions, ""ignore_paths"": ignore_paths, ""show_progress"": False, } scanner = Scanner(options, paths) res = scanner.scan() <mask>: return format_findings(res[""pattern_matcher""], paths[0]) except Exception: logger.exception(""libsast scan"") return {}",True,if res :,if res :,0.8820016898747209,0.0
"def _getPatternTemplate(pattern, key=None): if key is None: key = pattern <mask>: key = pattern.upper() template = DD_patternCache.get(key) if not template: if key in (""EPOCH"", ""{^LN-BEG}EPOCH"", ""^EPOCH""): template = DateEpoch(lineBeginOnly=(key != ""EPOCH"")) elif key in (""TAI64N"", ""{^LN-BEG}TAI64N"", ""^TAI64N""): template = DateTai64n(wordBegin=(""start"" if key != ""TAI64N"" else False)) else: template = DatePatternRegex(pattern) DD_patternCache.set(key, template) return template",False,"if ""%"" not in pattern :","if pattern . startswith ( ""_"" ) :",0.8820016898747209,6.27465531099474
"def _forward_response(self, src, dst): """"""Forward an SCP response between two remote SCP servers"""""" # pylint: disable=no-self-use try: exc = yield from src.await_response() <mask>: dst.send_error(exc) return exc else: dst.send_ok() return None except OSError as exc: return exc",False,if exc :,if exc is not None :,0.8820016898747209,17.965205598154213
"def _maybe_signal_recovery_end() -> None: if self.in_recovery and not self.active_remaining_total(): # apply anything stuck in the buffers self.flush_buffers() self._set_recovery_ended() <mask>: self._actives_span.set_tag(""Actives-Ready"", True) self.signal_recovery_end.set()",False,if self . _actives_span is not None :,if self . _actives_span :,0.8820016898747209,59.755798910891144
"def main(): tmpdir = None try: # Create a temporary working directory tmpdir = tempfile.mkdtemp() # Unpack the zipfile into the temporary directory pip_zip = os.path.join(tmpdir, ""pip.zip"") with open(pip_zip, ""wb"") as fp: fp.write(b85decode(DATA.replace(b""\n"", b""""))) # Add the zipfile to sys.path so that we can import it sys.path.insert(0, pip_zip) # Run the bootstrap bootstrap(tmpdir=tmpdir) finally: # Clean up our temporary working directory <mask>: shutil.rmtree(tmpdir, ignore_errors=True)",True,if tmpdir :,if tmpdir :,0.8820016898747209,0.0
"def __init__(self, api_version_str): try: self.latest = self.preview = False self.yyyy = self.mm = self.dd = None if api_version_str == ""latest"": self.latest = True else: <mask>: self.preview = True parts = api_version_str.split(""-"") self.yyyy = int(parts[0]) self.mm = int(parts[1]) self.dd = int(parts[2]) except (ValueError, TypeError): raise ValueError( ""The API version {} is not in a "" ""supported format"".format(api_version_str) )",False,"if ""preview"" in api_version_str :","if api_version_str . startswith ( ""-preview"" ) :",0.8820016898747209,30.79300751569293
"def _merge(self, items, map_id, dep_id, use_disk, meminfo, mem_limit): combined = self.combined merge_combiner = self.aggregator.mergeCombiners for k, v in items: o = combined.get(k) combined[k] = merge_combiner(o, v) if o is not None else v <mask>: mem_limit = self._rotate()",False,if use_disk and meminfo . rss > mem_limit :,if mem_limit is None :,0.8820016898747209,11.787460936700446
"def TryMerge(self, d): while d.avail() > 0: tt = d.getVarInt32() <mask>: self.set_value(d.getVarInt32()) continue if tt == 0: raise ProtocolBuffer.ProtocolBufferDecodeError d.skipData(tt)",True,if tt == 8 :,if tt == 8 :,0.8820016898747209,100.00000000000004
"def nice(deltat): # singular,plural times = _( ""second,seconds:minute,minutes:hour,hours:day,days:week,weeks:month,months:year,years"" ).split("":"") d = abs(int(deltat)) for div, time in zip((60, 60, 24, 7, 4, 12, 100), times): <mask>: return ""%s%i %s"" % (deltat < 0 and ""-"" or """", d, time.split("","")[d != 1]) d /= div",False,if d < div * 5 :,if d :,0.8820016898747209,0.0
"def after_get_object(self, event, view_kwargs): if event and event.state == ""draft"": <mask>: raise ObjectNotFound({""parameter"": ""{id}""}, ""Event: not found"")",False,"if not is_logged_in ( ) or not has_access ( ""is_coorganizer"" , event_id = event . id ) :","if not self . get_object ( event . object , view_kwargs ) :",0.8820016898747209,4.189895797853235
def daemonize_if_required(self): if self.options.daemon: <mask>: # Stop the logging queue listener for the current process # We'll restart it once forked log.shutdown_multiprocessing_logging_listener(daemonizing=True) # Late import so logging works correctly salt.utils.process.daemonize() # Setup the multiprocessing log queue listener if enabled self._setup_mp_logging_listener(),False,if self . _setup_mp_logging_listener_ is True :,if self . options . daemon :,0.8820016898747209,8.377387908310832
"def iter_modules(self, by_clients=False, clients_filter=None): """"""iterate over all modules"""""" clients = None if by_clients: clients = self.get_clients(clients_filter) if not clients: return self._refresh_modules() for module_name in self.modules: try: module = self.get_module(module_name) except PupyModuleDisabled: continue if clients is not None: for client in clients: <mask>: yield module break else: yield module",False,if module . is_compatible_with ( client ) :,if client . is_active ( ) :,0.8820016898747209,17.983947973543398
"def _incremental_avg_dp(self, avg, new_el, idx): for attr in [""coarse_segm"", ""fine_segm"", ""u"", ""v""]: setattr( avg, attr, (getattr(avg, attr) * idx + getattr(new_el, attr)) / (idx + 1) ) <mask>: # Deletion of the > 0 index intermediary values to prevent GPU OOM setattr(new_el, attr, None) return avg",False,if idx :,if idx > 0 :,0.8820016898747209,23.643540225079384
"def run(self, paths=[]): collapsed = False for item in SideBarSelection(paths).getSelectedDirectories(): for view in item.views(): <mask>: Window().focus_view(view) self.collapse_sidebar_folder() collapsed = True view.close()",False,if not collapsed :,if collapsed :,0.8820016898747209,0.0
"def test_reductions(expr, rdd): result = compute(expr, rdd) expected = compute(expr, data) if not result == expected: print(result) print(expected) <mask>: assert abs(result - expected) < 0.001 else: assert result == expected",False,"if isinstance ( result , float ) :","elif isinstance ( result , float ) :",0.8820016898747209,84.08964152537145
"def deltask(task, d): if task[:3] != ""do_"": task = ""do_"" + task bbtasks = d.getVar(""__BBTASKS"", False) or [] if task in bbtasks: bbtasks.remove(task) d.delVarFlag(task, ""task"") d.setVar(""__BBTASKS"", bbtasks) d.delVarFlag(task, ""deps"") for bbtask in d.getVar(""__BBTASKS"", False) or []: deps = d.getVarFlag(bbtask, ""deps"", False) or [] <mask>: deps.remove(task) d.setVarFlag(bbtask, ""deps"", deps)",True,if task in deps :,if task in deps :,0.8820016898747209,100.00000000000004
"def _apply_weightnorm(self, list_layers): """"""Try apply weightnorm for all layer in list_layers."""""" for i in range(len(list_layers)): try: layer_name = list_layers[i].name.lower() <mask>: list_layers[i] = WeightNormalization(list_layers[i]) except Exception: pass",False,"if ""conv1d"" in layer_name or ""dense"" in layer_name :","if layer_name == ""weightnorm"" :",0.8820016898747209,9.586514611843183
"def __init__(self, execution_context, aggregate_operators): super(_QueryExecutionAggregateEndpointComponent, self).__init__(execution_context) self._local_aggregators = [] self._results = None self._result_index = 0 for operator in aggregate_operators: if operator == ""Average"": self._local_aggregators.append(_AverageAggregator()) elif operator == ""Count"": self._local_aggregators.append(_CountAggregator()) <mask>: self._local_aggregators.append(_MaxAggregator()) elif operator == ""Min"": self._local_aggregators.append(_MinAggregator()) elif operator == ""Sum"": self._local_aggregators.append(_SumAggregator())",True,"elif operator == ""Max"" :","elif operator == ""Max"" :",0.8820016898747209,100.00000000000004
"def _conv_layer(self, sess, bottom, name, trainable=True, padding=""SAME"", relu=True): with tf.variable_scope(name) as scope: filt = self._get_conv_filter(sess, name, trainable=trainable) conv_biases = self._get_bias(sess, name, trainable=trainable) conv = tf.nn.conv2d(bottom, filt, [1, 1, 1, 1], padding=padding) bias = tf.nn.bias_add(conv, conv_biases) <mask>: bias = tf.nn.relu(bias) return bias",True,if relu :,if relu :,0.8820016898747209,0.0
"def get_partners(self) -> Dict[AbstractNode, Set[int]]: partners = {} # type: Dict[AbstractNode, Set[int]] for edge in self.edges: if edge.is_dangling(): raise ValueError(""Cannot contract copy tensor with dangling edges"") <mask>: continue partner_node, shared_axis = self._get_partner(edge) if partner_node not in partners: partners[partner_node] = set() partners[partner_node].add(shared_axis) return partners",False,if self . _is_my_trace ( edge ) :,if edge . is_edge_in_dangling ( ) :,0.8820016898747209,10.986406937498575
"def close(self): with self._lock: """"""Close this _MultiFileWatcher object forever."""""" <mask>: self._folder_handlers = {} LOGGER.debug( ""Stopping observer thread even though there is a non-zero "" ""number of event observers!"" ) else: LOGGER.debug(""Stopping observer thread"") self._observer.stop() self._observer.join(timeout=5)",False,if len ( self . _folder_handlers ) != 0 :,if len ( self . _folder_handlers ) == 0 :,0.8820016898747209,78.25422900366432
"def comboSelectionChanged(self, index): text = self.comboBox.cb.itemText(index) for i in range(self.labelList.count()): <mask>: self.labelList.item(i).setCheckState(2) elif text != self.labelList.item(i).text(): self.labelList.item(i).setCheckState(0) else: self.labelList.item(i).setCheckState(2)",False,"if text == """" :",if text == self . labelList . item ( i ) . text ( ) :,0.8820016898747209,15.13851459876605
"def _get_messages(self): r = [] try: self._connect() self._login() for message in self._fetch(): <mask>: r.append(message) self._connection.expunge() self._connection.close() self._connection.logout() except MailFetcherError as e: self.log(""error"", str(e)) return r",False,if message :,if message . is_valid ( ) :,0.8820016898747209,10.552670315936318
"def get_current_user(self): try: <mask>: return config.get(""json_authentication_override"") tkn_header = self.request.headers[""authorization""] except KeyError: raise WebAuthNError(reason=""Missing Authorization Header"") else: tkn_str = tkn_header.split("" "")[-1] try: tkn = self.jwt_validator(tkn_str) except AuthenticationError as e: raise WebAuthNError(reason=e.message) else: return tkn",False,"if config . get ( ""development"" ) and config . get ( ""json_authentication_override"" ) :","if config . get ( ""json_authentication_override"" ) :",0.8820016898747209,52.578802442578
def _get_data(self): formdata = self._formdata if formdata: data = [] # TODO: Optimize? for item in formdata: model = self.loader.get_one(item) if item else None <mask>: data.append(model) else: self._invalid_formdata = True self._set_data(data) return self._data,True,if model :,if model :,0.8820016898747209,0.0
"def _getSubstrings(self, va, size, ltyp): # rip through the desired memory range to populate any substrings subs = set() end = va + size for offs in range(va, end, 1): loc = self.getLocation(offs, range=True) if loc and loc[L_LTYPE] == LOC_STRING and loc[L_VA] > va: subs.add((loc[L_VA], loc[L_SIZE])) <mask>: subs = subs.union(set(loc[L_TINFO])) return list(subs)",False,if loc [ L_TINFO ] :,elif loc [ L_LTYPE ] == LTYPE_STRING :,0.8820016898747209,21.401603033752977
def monad(self): if not self.cls_bl_idname: return None for monad in bpy.data.node_groups: <mask>: if monad.cls_bl_idname == self.cls_bl_idname: return monad return None,False,"if hasattr ( monad , ""cls_bl_idname"" ) :",if monad . cls_bl_idname is not None :,0.8820016898747209,30.181358455294284
"def _set_peer_statuses(self): """"""Set peer statuses."""""" cutoff = time.time() - STALE_SECS for peer in self.peers: if peer.bad: peer.status = PEER_BAD <mask>: peer.status = PEER_GOOD elif peer.last_good: peer.status = PEER_STALE else: peer.status = PEER_NEVER",False,elif peer . last_good > cutoff :,elif cutoff > 0.5 :,0.8820016898747209,6.787957387517878
"def title_by_index(self, trans, index, context): d_type = self.get_datatype(trans, context) for i, (composite_name, composite_file) in enumerate(d_type.writable_files.items()): if i == index: rval = composite_name <mask>: rval = ""{} ({})"".format(rval, composite_file.description) if composite_file.optional: rval = ""%s [optional]"" % rval return rval if index < self.get_file_count(trans, context): return ""Extra primary file"" return None",True,if composite_file . description :,if composite_file . description :,0.8820016898747209,100.00000000000004
"def testUiViewServerDump_windowIntM1(self): device = None try: device = MockDevice(version=15, startviewserver=True) vc = ViewClient(device, device.serialno, adb=TRUE, autodump=False) vc.dump(window=-1) vc.findViewByIdOrRaise(""id/home"") finally: <mask>: device.shutdownMockViewServer()",True,if device :,if device :,0.8820016898747209,0.0
"def _convertDict(self, d): r = {} for k, v in d.items(): if isinstance(v, bytes): v = str(v, ""utf-8"") elif isinstance(v, list) or isinstance(v, tuple): v = self._convertList(v) <mask>: v = self._convertDict(v) if isinstance(k, bytes): k = str(k, ""utf-8"") r[k] = v return r",False,"elif isinstance ( v , dict ) :","elif isinstance ( v , dict ) or isinstance ( v , dict ) :",0.8820016898747209,47.587330964125236
"def _testSendmsgTimeout(self): try: self.cli_sock.settimeout(0.03) try: while True: self.sendmsgToServer([b""a"" * 512]) except socket.timeout: pass except OSError as exc: <mask>: raise # bpo-33937 the test randomly fails on Travis CI with # ""OSError: [Errno 12] Cannot allocate memory"" else: self.fail(""socket.timeout not raised"") finally: self.misc_event.set()",False,if exc . errno != errno . ENOMEM :,if exc . errno != 12 :,0.8820016898747209,55.0695314903184
"def addError(self, test, err): if err[0] is SkipTest: if self.showAll: self.stream.writeln(str(err[1])) <mask>: self.stream.write(""s"") self.stream.flush() return _org_AddError(self, test, err)",False,elif self . dots :,if self . showAll :,0.8820016898747209,23.643540225079384
"def mouse_down(self, event): if event.button == 1: if self.scrolling: p = event.local if self.scroll_up_rect().collidepoint(p): self.scroll_up() return <mask>: self.scroll_down() return if event.button == 4: self.scroll_up() if event.button == 5: self.scroll_down() GridView.mouse_down(self, event)",False,elif self . scroll_down_rect ( ) . collidepoint ( p ) :,if self . scroll_down_rect ( ) . collidepoint ( p ) :,0.8820016898747209,93.06048591020995
"def find_file_copyright_notices(fname): ret = set() f = open(fname) lines = f.readlines() for l in lines[:80]: # hmmm, assume copyright to be in first 80 lines idx = l.lower().find(""copyright"") if idx < 0: continue copyright = l[idx + 9 :].strip() <mask>: continue copyright = sanitise(copyright) # hmm, do a quick check to see if there's a year, # if not, skip it if not copyright.find(""200"") >= 0 and not copyright.find(""199"") >= 0: continue ret.add(copyright) return ret",True,if not copyright :,if not copyright :,0.8820016898747209,100.00000000000004
"def get_selectable_values(self, request): shop = lfs.core.utils.get_default_shop(request) countries = [] for country in shop.shipping_countries.all(): <mask>: selected = True else: selected = False countries.append( { ""id"": country.id, ""name"": country.name, ""selected"": selected, } ) return countries",False,if country in self . value . all ( ) :,if country . selected :,0.8820016898747209,7.652332131360532
"def _addItemToLayout(self, sample, label): col = self.layout.columnCount() row = self.layout.rowCount() if row: row -= 1 nCol = self.columnCount * 2 # FIRST ROW FULL if col == nCol: for col in range(0, nCol, 2): # FIND RIGHT COLUMN if not self.layout.itemAt(row, col): break <mask>: # MAKE NEW ROW col = 0 row += 1 self.layout.addItem(sample, row, col) self.layout.addItem(label, row, col + 1)",False,if col + 2 == nCol :,if col == nCol :,0.8820016898747209,43.29820146406896
def contains_only_whitespace(node): if is_tag(node): if not any([not is_text(s) for s in node.contents]): <mask>: return True return False,False,if not any ( [ unicode ( s ) . strip ( ) for s in node . contents ] ) :,if not is_whitespace ( node . contents ) :,0.8820016898747209,7.799274724594511
"def tokenize_generator(cw): ret = [] done = {} for op in ops: ch = op.symbol[0] <mask>: continue sops = start_symbols[ch] cw.write(""case '%s':"" % ch) for t in gen_tests(sops, 1): cw.write(t) done[ch] = True return ret",True,if ch in done :,if ch in done :,0.8820016898747209,100.00000000000004
"def _convertNbCharsInNbBits(self, nbChars): nbMinBit = None nbMaxBit = None if nbChars is not None: <mask>: nbMinBit = nbChars * 8 nbMaxBit = nbMinBit else: if nbChars[0] is not None: nbMinBit = nbChars[0] * 8 if nbChars[1] is not None: nbMaxBit = nbChars[1] * 8 return (nbMinBit, nbMaxBit)",False,"if isinstance ( nbChars , int ) :",if nbChars < 8 :,0.8820016898747209,7.715486568024961
"def init(self, *args, **kwargs): if ""_state"" not in kwargs: state = {} # Older versions have the _state entries as individual kwargs for arg in (""children"", ""windowState"", ""detachedPanels""): if arg in kwargs: state[arg] = kwargs[arg] del kwargs[arg] <mask>: kwargs[""_state""] = state originalInit(self, *args, **kwargs)",False,if state :,elif state :,0.8820016898747209,0.0
"def spm_decode(tokens: List[str]) -> List[str]: words = [] pieces: List[str] = [] for t in tokens: <mask>: if len(pieces) > 0: words.append("""".join(pieces)) pieces = [t[1:]] else: pieces.append(t) if len(pieces) > 0: words.append("""".join(pieces)) return words",False,if t [ 0 ] == DecodeMixin . spm_bos_token :,"if t . startswith ( "" "" ) :",0.8820016898747209,5.821935635427797
"def _compare_dirs(self, dir1: str, dir2: str) -> List[str]: # check that dir1 and dir2 are equivalent, # return the diff diff = [] # type: List[str] for root, dirs, files in os.walk(dir1): for file_ in files: path = os.path.join(root, file_) target_path = os.path.join(dir2, os.path.split(path)[-1]) <mask>: diff.append(file_) return diff",False,if not os . path . exists ( target_path ) :,if os . path . isfile ( target_path ) :,0.8820016898747209,59.74178044844197
"def credentials(self): """"""The session credentials as a dict"""""" creds = {} if self._creds: <mask>: # pragma: no branch creds[""aws_access_key_id""] = self._creds.access_key if self._creds.secret_key: # pragma: no branch creds[""aws_secret_access_key""] = self._creds.secret_key if self._creds.token: creds[""aws_session_token""] = self._creds.token if self._session.region_name: creds[""aws_region""] = self._session.region_name if self.requester_pays: creds[""aws_request_payer""] = ""requester"" return creds",True,if self . _creds . access_key :,if self . _creds . access_key :,0.8820016898747209,100.00000000000004
"def got_arbiter_module_type_defined(self, mod_type): for a in self.arbiters: # Do like the linkify will do after.... for m in getattr(a, ""modules"", []): # So look at what the arbiter try to call as module m = m.strip() # Ok, now look in modules... for mod in self.modules: # try to see if this module is the good type if getattr(mod, ""module_type"", """").strip() == mod_type.strip(): # if so, the good name? <mask>: return True return False",False,"if getattr ( mod , ""module_name"" , """" ) . strip ( ) == m :","if getattr ( mod , ""name"" , """" ) . strip ( ) == m :",0.8820016898747209,82.57183305096686
"def find_file_at_path_with_indexes(self, path, url): if url.endswith(""/""): path = os.path.join(path, self.index_file) return self.get_static_file(path, url) elif url.endswith(""/"" + self.index_file): if os.path.isfile(path): return self.redirect(url, url[: -len(self.index_file)]) else: try: return self.get_static_file(path, url) except IsDirectoryError: <mask>: return self.redirect(url, url + ""/"") raise MissingFileError(path)",False,"if os . path . isfile ( os . path . join ( path , self . index_file ) ) :",if os . path . isdir ( path ) :,0.8820016898747209,13.722896630276955
def _use_full_params(self) -> None: for p in self.params: if not p._is_sharded: <mask>: assert p._fp16_shard.storage().size() != 0 p.data = p._fp16_shard else: assert p._full_param_padded.storage().size() != 0 p.data = p._full_param_padded[: p._orig_size.numel()].view(p._orig_size),False,if self . mixed_precision :,if p . _fp16_shard :,0.8820016898747209,7.809849842300637
"def _attrdata(self, cont, name, *val): if not name: return None, False if isinstance(name, Mapping): if val: raise TypeError(""Cannot set a value to %s"" % name) return name, True else: if val: <mask>: return {name: val[0]}, True else: raise TypeError(""Too may arguments"") else: cont = self._extra.get(cont) return cont.get(name) if cont else None, False",True,if len ( val ) == 1 :,if len ( val ) == 1 :,0.8820016898747209,100.00000000000004
"def evaluate(env, net, device=""cpu""): obs = env.reset() reward = 0.0 steps = 0 while True: obs_v = ptan.agent.default_states_preprocessor([obs]).to(device) action_v = net(obs_v) action = action_v.data.cpu().numpy()[0] obs, r, done, _ = env.step(action) reward += r steps += 1 <mask>: break return reward, steps",True,if done :,if done :,0.8820016898747209,0.0
"def convert_html_js_files(app: Sphinx, config: Config) -> None: """"""This converts string styled html_js_files to tuple styled one."""""" html_js_files = [] # type: List[Tuple[str, Dict]] for entry in config.html_js_files: <mask>: html_js_files.append((entry, {})) else: try: filename, attrs = entry html_js_files.append((filename, attrs)) except Exception: logger.warning(__(""invalid js_file: %r, ignored""), entry) continue config.html_js_files = html_js_files # type: ignore",True,"if isinstance ( entry , str ) :","if isinstance ( entry , str ) :",0.8820016898747209,100.00000000000004
"def _check_duplications(self, regs): """"""n^2 loop which verifies that each reg exists only once."""""" for reg in regs: count = 0 for r in regs: <mask>: count += 1 if count > 1: genutil.die(""reg %s defined more than once"" % reg)",False,if reg == r :,if r [ reg ] == reg :,0.8820016898747209,12.549310621989482
"def PyJsHoisted_vault_(key, forget, this, arguments, var=var): var = Scope( {u""this"": this, u""forget"": forget, u""key"": key, u""arguments"": arguments}, var ) var.registers([u""forget"", u""key""]) if PyJsStrictEq(var.get(u""key""), var.get(u""passkey"")): return ( var.put(u""secret"", var.get(u""null"")) <mask>: else ( var.get(u""secret"") or var.put(u""secret"", var.get(u""secretCreatorFn"")(var.get(u""object""))) ) )",False,"if var . get ( u""forget"" )","if PyJsStrictEq ( var . get ( u""key"" ) , var . get ( u""passkey"" ) )",0.8820016898747209,23.4986979900135
"def sort_nested_dictionary_lists(d): for k, v in d.items(): if isinstance(v, list): for i in range(0, len(v)): if isinstance(v[i], dict): v[i] = await sort_nested_dictionary_lists(v[i]) d[k] = sorted(v) <mask>: d[k] = await sort_nested_dictionary_lists(v) return d",False,"if isinstance ( v , dict ) :","elif isinstance ( v , dict ) :",0.8820016898747209,84.08964152537145
"def transceiver(self, data): out = [] for t in range(8): if data[t] == 0: continue value = data[t] for b in range(8): if value & 0x80: <mask>: out.append(""(unknown)"") else: out.append(TRANSCEIVER[t][b]) value <<= 1 self.annotate(""Transceiver compliance"", "", "".join(out))",False,if len ( TRANSCEIVER [ t ] ) < b + 1 :,if t == 0 :,0.8820016898747209,3.3264637832151163
"def process_string(self, remove_repetitions, sequence): string = """" for i, char in enumerate(sequence): if char != self.int_to_char[self.blank_index]: # if this char is a repetition and remove_repetitions=true, # skip. <mask>: pass elif char == self.labels[self.space_index]: string += "" "" else: string = string + char return string",False,if remove_repetitions and i != 0 and char == sequence [ i - 1 ] :,if remove_repetitions :,0.8820016898747209,3.520477365831487
"def clean(self): username = self.cleaned_data.get(""username"") password = self.cleaned_data.get(""password"") if username and password: self.user_cache = authenticate(username=username, password=password) <mask>: raise forms.ValidationError(self.error_messages[""invalid_login""]) elif not self.user_cache.is_active: raise forms.ValidationError(self.error_messages[""inactive""]) self.check_for_test_cookie() return self.cleaned_data",False,if self . user_cache is None :,if not self . user_cache :,0.8820016898747209,49.62644776757999
"def is_listening_for_message(conversation_id: Text, endpoint: EndpointConfig) -> bool: """"""Check if the conversation is in need for a user message."""""" tracker = await retrieve_tracker(endpoint, conversation_id, EventVerbosity.APPLIED) for i, e in enumerate(reversed(tracker.get(""events"", []))): if e.get(""event"") == UserUttered.type_name: return False <mask>: return e.get(""name"") == ACTION_LISTEN_NAME return False",False,"elif e . get ( ""event"" ) == ActionExecuted . type_name :",if i == 0 :,0.8820016898747209,2.8722725093023906
"def getReferences(view, name=""""): """"""Find all reference definitions."""""" # returns {name -> Region} refs = [] name = re.escape(name) if name == """": refs.extend(view.find_all(r""(?<=^\[)([^\]]+)(?=\]:)"", 0)) else: refs.extend(view.find_all(r""(?<=^\[)(%s)(?=\]:)"" % name, 0)) regions = refs ids = {} for reg in regions: name = view.substr(reg).strip() key = name.lower() <mask>: ids[key].regions.append(reg) else: ids[key] = Obj(regions=[reg], label=name) return ids",True,if key in ids :,if key in ids :,0.8820016898747209,100.00000000000004
"def _get_header(self, requester, header_name): hits = sum([header_name in headers for _, headers in requester.requests]) self.assertEquals(hits, 2 if self.revs_enabled else 1) for url, headers in requester.requests: <mask>: if self.revs_enabled: self.assertTrue(url.endswith(""/latest""), msg=url) else: self.assertTrue(url.endswith(""/download_urls""), msg=url) return headers.get(header_name)",False,if header_name in headers :,"if url . startswith ( ""/download"" ) :",0.8820016898747209,4.456882760699063
"def TryMerge(self, d): while d.avail() > 0: tt = d.getVarInt32() <mask>: self.set_shuffle_name(d.getPrefixedString()) continue if tt == 0: raise ProtocolBuffer.ProtocolBufferDecodeError d.skipData(tt)",True,if tt == 10 :,if tt == 10 :,0.8820016898747209,100.00000000000004
"def make_release_tree(self, base_dir, files): """"""Make the release tree."""""" self.mkpath(base_dir) create_tree(base_dir, files, dry_run=self.dry_run) if not files: self.log.warning(""no files to distribute -- empty manifest?"") else: self.log.info(""copying files to %s..."", base_dir) for filename in files: <mask>: self.log.warning(""'%s' not a regular file -- skipping"", filename) else: dest = os.path.join(base_dir, filename) self.copy_file(filename, dest) self.distribution.metadata.write_pkg_info(base_dir)",True,if not os . path . isfile ( filename ) :,if not os . path . isfile ( filename ) :,0.8820016898747209,100.00000000000004
"def _parse_names_set(feature_names): """"""Helping function of `_parse_feature_names` that parses a set of feature names."""""" feature_collection = OrderedDict() for feature_name in feature_names: <mask>: feature_collection[feature_name] = ... else: raise ValueError(""Failed to parse {}, expected string"".format(feature_name)) return feature_collection",True,"if isinstance ( feature_name , str ) :","if isinstance ( feature_name , str ) :",0.8820016898747209,100.00000000000004
"def get_connection(self, url, proxies=None): with self.pools.lock: pool = self.pools.get(url) <mask>: return pool pool = NpipeHTTPConnectionPool( self.npipe_path, self.timeout, maxsize=self.max_pool_size ) self.pools[url] = pool return pool",True,if pool :,if pool :,0.8820016898747209,0.0
"def _parse_dimensions(dimensions): arrays = [] names = [] for key in dimensions: values = [v[""name""] for v in key[""values""]] role = key.get(""role"", None) <mask>: values = [_fix_quarter_values(v) for v in values] values = pd.DatetimeIndex(values) arrays.append(values) names.append(key[""name""]) midx = pd.MultiIndex.from_product(arrays, names=names) if len(arrays) == 1 and isinstance(midx, pd.MultiIndex): # Fix for pandas >= 0.21 midx = midx.levels[0] return midx",False,"if role in ( ""time"" , ""TIME_PERIOD"" ) :",if role is not None :,0.8820016898747209,4.008579202215618
"def _add_trials(self, name, spec): """"""Add trial by invoking TrialRunner."""""" resource = {} resource[""trials""] = [] trial_generator = BasicVariantGenerator() trial_generator.add_configurations({name: spec}) while not trial_generator.is_finished(): trial = trial_generator.next_trial() <mask>: break runner.add_trial(trial) resource[""trials""].append(self._trial_info(trial)) return resource",True,if not trial :,if not trial :,0.8820016898747209,100.00000000000004
"def _retrieve_key(self): url = ""http://www.canadapost.ca/cpo/mc/personal/postalcode/fpc.jsf"" text = """" try: r = requests.get(url, timeout=self.timeout, proxies=self.proxies) text = r.text except: self.error = ""ERROR - URL Connection"" if text: expression = r""'(....-....-....-....)';"" pattern = re.compile(expression) match = pattern.search(text) <mask>: self.key = match.group(1) return self.key else: self.error = ""ERROR - No API Key""",True,if match :,if match :,0.8820016898747209,0.0
"def test_net(net, env, count=10, device=""cpu""): rewards = 0.0 steps = 0 for _ in range(count): obs = env.reset() while True: obs_v = ptan.agent.float32_preprocessor([obs]).to(device) mu_v = net(obs_v)[0] action = mu_v.squeeze(dim=0).data.cpu().numpy() action = np.clip(action, -1, 1) obs, reward, done, _ = env.step(action) rewards += reward steps += 1 <mask>: break return rewards / count, steps / count",True,if done :,if done :,0.8820016898747209,0.0
"def compile(self, filename, obfuscate=False, raw=False, magic=""\x00"" * 8): body = marshal.dumps(compile(self.visit(self._source_ast), filename, ""exec"")) if obfuscate: body_len = len(body) offset = 0 if raw else 8 output = bytearray(body_len + 8) for i, x in enumerate(body): output[i + offset] = ord(x) ^ ((2 ** ((65535 - i) % 65535)) % 251) <mask>: for i in xrange(8): output[i] = 0 return output elif raw: return body else: return magic + body",True,if raw :,if raw :,0.8820016898747209,0.0
"def _map_saslprep(s): """"""Map stringprep table B.1 to nothing and C.1.2 to ASCII space"""""" r = [] for c in s: if stringprep.in_table_c12(c): r.append("" "") <mask>: r.append(c) return """".join(r)",False,elif not stringprep . in_table_b1 ( c ) :,elif stringprep . in_table_b11 ( c ) :,0.8820016898747209,59.74178044844197
"def ensemble(self, pairs, other_preds): """"""Ensemble the dict with statistical model predictions."""""" lemmas = [] assert len(pairs) == len(other_preds) for p, pred in zip(pairs, other_preds): w, pos = p if (w, pos) in self.composite_dict: lemma = self.composite_dict[(w, pos)] <mask>: lemma = self.word_dict[w] else: lemma = pred if lemma is None: lemma = w lemmas.append(lemma) return lemmas",False,elif w in self . word_dict :,"elif ( w , pos ) in self . word_dict :",0.8820016898747209,50.08718428920986
"def quiet_f(*args): vars = {arg_name: Real(arg) for arg_name, arg in zip(arg_names, args)} value = dynamic_scoping(quiet_expr.evaluate, vars, evaluation) if expect_list: <mask>: value = [extract_pyreal(item) for item in value.leaves] if any(item is None for item in value): return None return value else: return None else: value = extract_pyreal(value) if value is None or isinf(value) or isnan(value): return None return value",False,"if value . has_form ( ""List"" , None ) :","if isinstance ( value , list ) :",0.8820016898747209,6.866210821983635
"def _copy_package_apps( local_bin_dir: Path, app_paths: List[Path], suffix: str = """" ) -> None: for src_unresolved in app_paths: src = src_unresolved.resolve() app = src.name dest = Path(local_bin_dir / add_suffix(app, suffix)) if not dest.parent.is_dir(): mkdir(dest.parent) <mask>: logger.warning(f""{hazard} Overwriting file {str(dest)} with {str(src)}"") dest.unlink() if src.exists(): shutil.copy(src, dest)",True,if dest . exists ( ) :,if dest . exists ( ) :,0.8820016898747209,100.00000000000004
"def assert_readback(vehicle, values): i = 10 while i > 0: time.sleep(0.1) i -= 0.1 for k, v in values.items(): <mask>: continue break if i <= 0: raise Exception(""Did not match in channels readback %s"" % values)",False,if vehicle . channels [ k ] != v :,if v != vehicle :,0.8820016898747209,9.284908374907788
"def _get_linode_client(self): api_key = self.credentials.conf(""key"") api_version = self.credentials.conf(""version"") if api_version == """": api_version = None if not api_version: api_version = 3 # Match for v4 api key regex_v4 = re.compile(""^[0-9a-f]{64}$"") regex_match = regex_v4.match(api_key) <mask>: api_version = 4 else: api_version = int(api_version) return _LinodeLexiconClient(api_key, api_version)",False,if regex_match :,if not regex_match :,0.8820016898747209,53.7284965911771
"def mergeHiLo(self, x_stats): """"""Merge the highs and lows of another accumulator into myself."""""" if x_stats.firsttime is not None: <mask>: self.firsttime = x_stats.firsttime self.first = x_stats.first if x_stats.lasttime is not None: if self.lasttime is None or x_stats.lasttime >= self.lasttime: self.lasttime = x_stats.lasttime self.last = x_stats.last",False,if self . firsttime is None or x_stats . firsttime < self . firsttime :,if self . firsttime is None or x_stats . firsttime >= self . firsttime :,0.8820016898747209,77.7811122305422
"def _check_good_input(self, X, y=None): if isinstance(X, dict): lengths = [len(X1) for X1 in X.values()] <mask>: raise ValueError(""Not all values of X are of equal length."") x_len = lengths[0] else: x_len = len(X) if y is not None: if len(y) != x_len: raise ValueError(""X and y are not of equal length."") if self.regression and y is not None and y.ndim == 1: y = y.reshape(-1, 1) return X, y",False,if len ( set ( lengths ) ) > 1 :,if len ( lengths ) != 1 :,0.8820016898747209,26.264048972269727
"def set(self, obj, **kwargs): """"""Check for missing event functions and substitute these with"""""" """"""the ignore method"""""" ignore = getattr(self, ""ignore"") for k, v in kwargs.iteritems(): setattr(self, k, getattr(obj, v)) if k in self.combinations: for k1 in self.combinations[k]: <mask>: setattr(self, k1, ignore)",True,"if not hasattr ( self , k1 ) :","if not hasattr ( self , k1 ) :",0.8820016898747209,100.00000000000004
"def _parse_list(self, tokens): # Process left to right, allow descending in sub lists assert tokens[0] in (""["", ""("") delim = ""]"" if tokens.pop(0) == ""["" else "")"" expr = ExpressionList() while tokens and tokens[0] != delim: item = self._parse(tokens) <mask>: if tokens.pop(0) != "","": raise ExpressionSyntaxError('Expected: "",""') expr.append(item) if not tokens or tokens[0] != delim: raise ExpressionSyntaxError('Missing: ""%s""' % delim) else: tokens.pop(0) return expr",False,if tokens and tokens [ 0 ] != delim :,if item :,0.8820016898747209,0.0
"def param_value(self): # This is part of the ""handle quoted extended parameters"" hack. for token in self: if token.token_type == ""value"": return token.stripped_value <mask>: for token in token: if token.token_type == ""bare-quoted-string"": for token in token: if token.token_type == ""value"": return token.stripped_value return """"",False,"if token . token_type == ""quoted-string"" :",if self . is_quoted_string ( ) :,0.8820016898747209,4.839576869824698
"def paragraph_is_fully_commented(lines, comment, main_language): """"""Is the paragraph fully commented?"""""" for i, line in enumerate(lines): <mask>: if line[len(comment) :].lstrip().startswith(comment): continue if is_magic(line, main_language): return False continue return i > 0 and _BLANK_LINE.match(line) return True",False,if line . startswith ( comment ) :,if comment :,0.8820016898747209,0.0
"def lots_connected_to_existing_roads(model): set = [] for h in model.HarvestCells: for (i, j) in model.ExistingRoads: <mask>: if h not in set: set.append(h) return set",False,if ( i in model . COriginNodeForCell [ h ] ) or ( j in model . COriginNodeForCell [ h ] ) :,if i == j :,0.8820016898747209,0.6751392346890166
"def detect(get_page): retval = False for vector in WAF_ATTACK_VECTORS: page, headers, code = get_page(get=vector) retval = ( re.search( r""\Abarra_counter_session="", headers.get(HTTP_HEADER.SET_COOKIE, """"), re.I, ) is not None ) retval |= ( re.search( r""(\A|\b)barracuda_"", headers.get(HTTP_HEADER.SET_COOKIE, """"), re.I ) is not None ) <mask>: break return retval",True,if retval :,if retval :,0.8820016898747209,0.0
"def test_files(self): # get names of files to test dist_dir = os.path.join(os.path.dirname(__file__), os.pardir, os.pardir) names = [] for d in self.test_directories: test_dir = os.path.join(dist_dir, d) for n in os.listdir(test_dir): <mask>: names.append(os.path.join(test_dir, n)) for filename in names: if test_support.verbose: print(""Testing %s"" % filename) source = read_pyfile(filename) self.check_roundtrip(source)",False,"if n . endswith ( "".py"" ) and not n . startswith ( ""bad"" ) :","if n . endswith ( "".py"" ) :",0.8820016898747209,38.966271115357685
"def test_calibrate_target(create_target): mod, params = testing.synthetic.get_workload() dataset = get_calibration_dataset(mod, ""data"") with relay.quantize.qconfig(calibrate_mode=""kl_divergence""): <mask>: with tvm.target.Target(""llvm""): relay.quantize.quantize(mod, params, dataset) else: # current_target = None relay.quantize.quantize(mod, params, dataset)",True,if create_target :,if create_target :,0.8820016898747209,100.00000000000004
"def _cleanSubmodule(self, _=None): rc = RC_SUCCESS if self.submodules: command = [ ""submodule"", ""foreach"", ""--recursive"", ""git"", ""clean"", ""-f"", ""-f"", ""-d"", ] <mask>: command.append(""-x"") rc = yield self._dovccmd(command) defer.returnValue(rc)",False,"if self . mode == ""full"" and self . method == ""fresh"" :",if self . _isX86 ( ) :,0.8820016898747209,5.585123293852589
"def screen_length_to_bytes_count(string, screen_length_limit, encoding): bytes_count = 0 screen_length = 0 for unicode_char in string: screen_length += screen_len(unicode_char) char_bytes_count = len(unicode_char.encode(encoding)) bytes_count += char_bytes_count <mask>: bytes_count -= char_bytes_count break return bytes_count",True,if screen_length > screen_length_limit :,if screen_length > screen_length_limit :,0.8820016898747209,100.00000000000004
"def tamper(payload, **kwargs): junk_chars = ""!#$%&()*~+-_.,:;?@[/|\]^`"" retval = """" for i, char in enumerate(payload, start=1): amount = random.randint(10, 15) if char == "">"": retval += "">"" for _ in range(amount): retval += random.choice(junk_chars) <mask>: retval += ""<"" for _ in range(amount): retval += random.choice(junk_chars) elif char == "" "": for _ in range(amount): retval += random.choice(junk_chars) else: retval += char return retval",True,"elif char == ""<"" :","elif char == ""<"" :",0.8820016898747209,100.00000000000004
"def test_parse(self): correct = 0 for example in EXAMPLES: try: schema.parse(example.schema_string) if example.valid: correct += 1 else: self.fail(""Invalid schema was parsed: "" + example.schema_string) except: <mask>: correct += 1 else: self.fail(""Valid schema failed to parse: "" + example.schema_string) fail_msg = ""Parse behavior correct on %d out of %d schemas."" % ( correct, len(EXAMPLES), ) self.assertEqual(correct, len(EXAMPLES), fail_msg)",False,if not example . valid :,if example . valid :,0.8820016898747209,57.89300674674101
"def _on_change(self): changed = False self.save() for key, value in self.data.items(): <mask>: if value: changed = True break if isinstance(value, int): if value != 1: changed = True break elif value is None: continue elif len(value) != 0: changed = True break self._reset_button.disabled = not changed",False,"if isinstance ( value , bool ) :","if isinstance ( key , str ) :",0.8820016898747209,27.054113452696992
"def normalize(d: Dict[Any, Any]) -> Dict[str, Any]: first_exception = None for normalizer in normalizers: try: normalized = normalizer(d) except KeyError as e: <mask>: first_exception = e else: return normalized assert first_exception is not None raise first_exception",False,if not first_exception :,if first_exception is None :,0.8820016898747209,27.77619034011791
"def gather_callback_args(self, obj, callbacks): session = sa.orm.object_session(obj) for callback in callbacks: backref = callback.backref root_objs = getdotattr(obj, backref) if backref else obj if root_objs: if not isinstance(root_objs, Iterable): root_objs = [root_objs] with session.no_autoflush: for root_obj in root_objs: <mask>: args = self.get_callback_args(root_obj, callback) if args: yield args",False,if root_obj :,"if isinstance ( root_obj , sa . orm . object ) :",0.8820016898747209,12.011055432195764
"def test_opdm_to_oqdm(self): for file in filter(lambda x: x.endswith("".hdf5""), os.listdir(DATA_DIRECTORY)): molecule = MolecularData(filename=os.path.join(DATA_DIRECTORY, file)) <mask>: test_oqdm = map_one_pdm_to_one_hole_dm(molecule.fci_one_rdm) true_oqdm = numpy.eye(molecule.n_qubits) - molecule.fci_one_rdm assert numpy.allclose(test_oqdm, true_oqdm)",True,if molecule . fci_one_rdm is not None :,if molecule . fci_one_rdm is not None :,0.8820016898747209,100.00000000000004
"def emitSubDomainData(self, subDomainData, event): self.emitRawRirData(subDomainData, event) for subDomainElem in subDomainData: if self.checkForStop(): return None subDomain = subDomainElem.get(""subdomain"", """").strip() <mask>: self.emitHostname(subDomain, event)",True,if subDomain :,if subDomain :,0.8820016898747209,0.0
"def download_cve( download_path: str, years: Optional[List[int]] = None, update: bool = False ): if update: process_url(CVE_URL.format(""modified""), download_path) else: all_cve_urls = get_cve_links(CVE_URL, years) <mask>: raise CveLookupException(""Error: No CVE links found"") for url in all_cve_urls: process_url(url, download_path)",True,if not all_cve_urls :,if not all_cve_urls :,0.8820016898747209,100.00000000000004
"def is_special(s, i, directive): """"""Return True if the body text contains the @ directive."""""" # j = skip_line(s,i) ; trace(s[i:j],':',directive) assert directive and directive[0] == ""@"" # 10/23/02: all directives except @others must start the line. skip_flag = directive in (""@others"", ""@all"") while i < len(s): if match_word(s, i, directive): return True, i else: i = skip_line(s, i) <mask>: i = skip_ws(s, i) return False, -1",True,if skip_flag :,if skip_flag :,0.8820016898747209,100.00000000000004
"def run_async(self, nuke_cursors): # type: (bool) -> None interface_type = self.view.settings().get(""git_savvy.interface"") for cls in subclasses: if cls.interface_type == interface_type: vid = self.view.id() interface = interfaces.get(vid, None) <mask>: interface = interfaces[vid] = cls(view=self.view) interface.render(nuke_cursors=nuke_cursors) # type: ignore[union-attr] break",False,if not interface :,if interface is None :,0.8820016898747209,14.058533129758727
"def scan_resource_conf(self, conf): if ""properties"" in conf: <mask>: if str(conf[""properties""][""sslEnforcement""]).lower() == ""enabled"": return CheckResult.PASSED return CheckResult.FAILED",True,"if ""sslEnforcement"" in conf [ ""properties"" ] :","if ""sslEnforcement"" in conf [ ""properties"" ] :",0.8820016898747209,100.00000000000004
"def do_shorts( opts: List[Tuple[str, str]], optstring: str, shortopts: str, args: List[str] ) -> Tuple[List[Tuple[str, str]], List[str]]: while optstring != """": opt, optstring = optstring[0], optstring[1:] if short_has_arg(opt, shortopts): if optstring == """": <mask>: raise GetoptError(""option -%s requires argument"" % opt, opt) optstring, args = args[0], args[1:] optarg, optstring = optstring, """" else: optarg = """" opts.append((""-"" + opt, optarg)) return opts, args",False,if not args :,if args is None :,0.8820016898747209,14.058533129758727
"def release(self): tid = _thread.get_ident() with self.lock: <mask>: raise RuntimeError(""cannot release un-acquired lock"") assert self.count > 0 self.count -= 1 if self.count == 0: self.owner = None if self.waiters: self.waiters -= 1 self.wakeup.release()",False,if self . owner != tid :,if tid == self . owner :,0.8820016898747209,25.40663740773074
"def _summarize_kraken(fn): """"""get the value at species level"""""" kraken = {} list_sp, list_value = [], [] with open(fn) as handle: for line in handle: cols = line.strip().split(""\t"") sp = cols[5].strip() <mask>: list_sp.append(sp) list_value.append(cols[0]) kraken = {""kraken_sp"": list_sp, ""kraken_value"": list_value} return kraken",False,"if len ( sp . split ( "" "" ) ) > 1 and not sp . startswith ( ""cellular"" ) :",if sp :,0.8820016898747209,0.0
"def _sync_remote_run(remote_run): assert remote_run.remote remote_name = remote_run.remote.name pull_args = click_util.Args(remote=remote_name, delete=False) try: remote_impl_support.pull_runs([remote_run], pull_args) except Exception as e: <mask>: log.exception(""pull %s from %s"", remote_run.id, remote_name) else: log.error(""error pulling %s from %s: %s"", remote_run.id, remote_name, e)",False,if log . getEffectiveLevel ( ) <= logging . DEBUG :,"if e . args [ 0 ] == ""No results found"" :",0.8820016898747209,3.716499092256817
"def group_by_sign(seq, slop=sin(pi / 18), key=lambda x: x): sign = None subseq = [] for i in seq: ki = key(i) <mask>: subseq.append(i) if ki != 0: sign = ki / abs(ki) else: subseq.append(i) if sign * ki < -slop: sign = ki / abs(ki) yield subseq subseq = [i] if subseq: yield subseq",True,if sign is None :,if sign is None :,0.8820016898747209,100.00000000000004
"def import_til(self): log(""Importing type libraries..."") cur = self.db_cursor() sql = ""select name from diff.program_data where type = 'til'"" cur.execute(sql) for row in cur.fetchall(): til = row[""name""] <mask>: til = til.decode(""utf-8"") try: add_default_til(til) except: log(""Error loading til %s: %s"" % (row[""name""], str(sys.exc_info()[1]))) cur.close() auto_wait()",False,if type ( til ) is bytes :,"if isinstance ( til , bytes ) :",0.8820016898747209,14.535768424205482
"def getBranches(self): returned = [] for git_branch_line in self._executeGitCommandAssertSuccess(""branch"").stdout: <mask>: git_branch_line = git_branch_line[1:] git_branch_line = git_branch_line.strip() if BRANCH_ALIAS_MARKER in git_branch_line: alias_name, aliased = git_branch_line.split(BRANCH_ALIAS_MARKER) returned.append(branch.LocalBranchAlias(self, alias_name, aliased)) else: returned.append(branch.LocalBranch(self, git_branch_line)) return returned",False,"if git_branch_line . startswith ( ""*"" ) :","if git_branch_line . startswith ( ""#"" ) :",0.8820016898747209,78.25422900366432
"def add_include_dirs(self, args): ids = [] for a in args: # FIXME same hack, forcibly unpack from holder. <mask>: a = a.includedirs if not isinstance(a, IncludeDirs): raise InvalidArguments( ""Include directory to be added is not an include directory object."" ) ids.append(a) self.include_dirs += ids",False,"if hasattr ( a , ""includedirs"" ) :","if isinstance ( a , File ) :",0.8820016898747209,21.069764742263047
"def _serialize_feature(self, feature): name = feature.unique_name() <mask>: self._features_dict[feature.unique_name()] = feature.to_dictionary() for dependency in feature.get_dependencies(deep=True): name = dependency.unique_name() if name not in self._features_dict: self._features_dict[name] = dependency.to_dictionary()",True,if name not in self . _features_dict :,if name not in self . _features_dict :,0.8820016898747209,100.00000000000004
"def generate_io(chart_type, race_configs, environment): # output JSON structures structures = [] for race_config in race_configs: <mask>: title = chart_type.format_title( environment, race_config.track, es_license=race_config.es_license, suffix=""%s-io"" % race_config.label, ) structures.append(chart_type.io(title, environment, race_config)) return structures",False,"if ""io"" in race_config . charts :",if race_config . label :,0.8820016898747209,25.694343649393552
"def format_partition(partition, partition_schema): tokens = [] if isinstance(partition, dict): for name in partition_schema: <mask>: tok = _format_partition_kv( name, partition[name], partition_schema[name] ) else: # dynamic partitioning tok = name tokens.append(tok) else: for name, value in zip(partition_schema, partition): tok = _format_partition_kv(name, value, partition_schema[name]) tokens.append(tok) return ""PARTITION ({})"".format("", "".join(tokens))",True,if name in partition :,if name in partition :,0.8820016898747209,100.00000000000004
"def to_dict(self, validate=True, ignore=(), context=None): context = context or {} condition = getattr(self, ""condition"", Undefined) copy = self # don't copy unless we need to if condition is not Undefined: if isinstance(condition, core.SchemaBase): pass <mask>: kwds = parse_shorthand(condition[""field""], context.get(""data"", None)) copy = self.copy(deep=[""condition""]) copy.condition.update(kwds) return super(ValueChannelMixin, copy).to_dict( validate=validate, ignore=ignore, context=context )",False,"elif ""field"" in condition and ""type"" not in condition :","elif condition [ ""field"" ] is not Undefined :",0.8820016898747209,13.126509735412402
"def _checkForCommand(self): prompt = b""cftp> "" if self._expectingCommand and self._lineBuffer == prompt: buf = b""\n"".join(self._linesReceived) <mask>: buf = buf[len(prompt) :] self.clearBuffer() d, self._expectingCommand = self._expectingCommand, None d.callback(buf)",True,if buf . startswith ( prompt ) :,if buf . startswith ( prompt ) :,0.8820016898747209,100.00000000000004
"def schedule_logger(job_id=None, delete=False): if not job_id: return getLogger(""fate_flow_schedule"") else: if delete: with LoggerFactory.lock: try: for key in LoggerFactory.schedule_logger_dict.keys(): if job_id in key: del LoggerFactory.schedule_logger_dict[key] except: pass return True key = job_id + ""schedule"" <mask>: return LoggerFactory.schedule_logger_dict[key] return LoggerFactory.get_schedule_logger(job_id)",False,if key in LoggerFactory . schedule_logger_dict :,if key in LoggerFactory . schedule_logger_dict . keys ( ) :,0.8820016898747209,64.1386525898168
"def halfMultipartScore(nzb_name): try: wrong_found = 0 for nr in [1, 2, 3, 4, 5, ""i"", ""ii"", ""iii"", ""iv"", ""v"", ""a"", ""b"", ""c"", ""d"", ""e""]: for wrong in [""cd"", ""part"", ""dis"", ""disc"", ""dvd""]: if ""%s%s"" % (wrong, nr) in nzb_name.lower(): wrong_found += 1 <mask>: return -30 return 0 except: log.error(""Failed doing halfMultipartScore: %s"", traceback.format_exc()) return 0",False,if wrong_found == 1 :,if wrong_found == 2 :,0.8820016898747209,70.71067811865478
"def parse_converter_args(argstr: str) -> t.Tuple[t.Tuple, t.Dict[str, t.Any]]: argstr += "","" args = [] kwargs = {} for item in _converter_args_re.finditer(argstr): value = item.group(""stringval"") if value is None: value = item.group(""value"") value = _pythonize(value) <mask>: args.append(value) else: name = item.group(""name"") kwargs[name] = value return tuple(args), kwargs",False,"if not item . group ( ""name"" ) :","if isinstance ( value , t . Tuple ) :",0.8820016898747209,9.548450962056531
"def leaves(self, unique=True): """"""Get the leaves of the tree starting at this root."""""" if not self.children: return [self] else: res = list() for child in self.children: for sub_child in child.leaves(unique=unique): <mask>: res.append(sub_child) return res",False,if not unique or sub_child not in res :,if sub_child not in res :,0.8820016898747209,59.755798910891144
"def to_tree(self, tagname=None, idx=None, namespace=None): axIds = set((ax.axId for ax in self._axes)) for chart in self._charts: for id, axis in chart._axes.items(): <mask>: setattr(self, axis.tagname, axis) axIds.add(id) return super(PlotArea, self).to_tree(tagname)",True,if id not in axIds :,if id not in axIds :,0.8820016898747209,100.00000000000004
"def update_neighbor(neigh_ip_address, changes): rets = [] for k, v in changes.items(): if k == neighbors.MULTI_EXIT_DISC: rets.append(_update_med(neigh_ip_address, v)) if k == neighbors.ENABLED: rets.append(update_neighbor_enabled(neigh_ip_address, v)) <mask>: rets.append(_update_connect_mode(neigh_ip_address, v)) return all(rets)",True,if k == neighbors . CONNECT_MODE :,if k == neighbors . CONNECT_MODE :,0.8820016898747209,100.00000000000004
"def close_all_connections(): global _managers, _lock, _in_use, _timer _lock.acquire() try: <mask>: _timer.cancel() _timer = None for domain, managers in _managers.items(): for manager in managers: manager.close() _managers = {} finally: _lock.release()",True,if _timer :,if _timer :,0.8820016898747209,100.00000000000004
"def _instrument_model(self, model): for key, value in list( model.__dict__.items() ): # avoid ""dictionary keys changed during iteration"" if isinstance(value, tf.keras.layers.Layer): new_layer = self._instrument(value) if new_layer is not value: setattr(model, key, new_layer) elif isinstance(value, list): for i, item in enumerate(value): <mask>: value[i] = self._instrument(item) return model",True,"if isinstance ( item , tf . keras . layers . Layer ) :","if isinstance ( item , tf . keras . layers . Layer ) :",0.8820016898747209,100.00000000000004
"def target_glob(tgt, hosts): ret = {} for host in hosts: if fnmatch.fnmatch(tgt, host): ret[host] = copy.deepcopy(__opts__.get(""roster_defaults"", {})) ret[host].update({""host"": host}) <mask>: ret[host].update({""user"": __opts__[""ssh_user""]}) return ret",False,"if __opts__ . get ( ""ssh_user"" ) :","if __opts__ [ ""ssh_user"" ] :",0.8820016898747209,54.59709700160347
"def write(self, data): if mock_target._mirror_on_stderr: if self._write_line: sys.stderr.write(fn + "": "") <mask>: sys.stderr.write(data.decode(""utf8"")) else: sys.stderr.write(data) if (data[-1]) == ""\n"": self._write_line = True else: self._write_line = False super(Buffer, self).write(data)",False,if bytes :,"if isinstance ( data , bytes ) :",0.8820016898747209,7.267884212102741
"def task_thread(): while not task_queue.empty(): host, port, username, password = task_queue.get() logger.info( ""try burst {}:{} use username:{} password:{}"".format( host, port, username, password ) ) <mask>: with task_queue.mutex: task_queue.queue.clear() result_queue.put((username, password))",False,"if telnet_login ( host , port , username , password ) :",if not task_queue . empty ( ) :,0.8820016898747209,7.073666451977357
"def _format_results(name, ppl, scores, metrics): """"""Format results."""""" result_str = """" if ppl: result_str = ""%s ppl %.2f"" % (name, ppl) if scores: for metric in metrics: <mask>: result_str += "", %s %s %.1f"" % (name, metric, scores[metric]) else: result_str = ""%s %s %.1f"" % (name, metric, scores[metric]) return result_str",False,if result_str :,if metric in scores :,0.8820016898747209,12.703318703865365
"def info_query(self, query): """"""Send a query which only returns 1 row"""""" self._cmysql.query(query) first_row = () if self._cmysql.have_result_set: first_row = self._cmysql.fetch_row() <mask>: self._cmysql.free_result() raise errors.InterfaceError(""Query should not return more than 1 row"") self._cmysql.free_result() return first_row",False,if self . _cmysql . fetch_row ( ) :,if len ( first_row ) > 1 :,0.8820016898747209,9.042713792226897
"def reset_class(self): for f in self.fields_order: <mask>: f.value = int(f.strbits, 2) elif ""default_val"" in f.kargs: f.value = int(f.kargs[""default_val""], 2) else: f.value = None if f.fname: setattr(self, f.fname, f)",False,if f . strbits and isbin ( f . strbits ) :,if f . bits :,0.8820016898747209,10.536767850900915
"def _walk_map_list(self, access_func): seen = [] cur = self while cur: <mask>: break yield cur seen.append(cur.obj_offset) # check for signs of infinite looping if len(seen) > 1024: break cur = access_func(cur)",True,if cur . obj_offset in seen :,if cur . obj_offset in seen :,0.8820016898747209,100.00000000000004
def bgdel(): q = bgdelq while True: name = q.get() while os.path.exists(name): try: <mask>: os.remove(name) else: shutil.rmtree(name) except: pass if os.path.exists(name): time.sleep(0.1),False,if os . path . isfile ( name ) :,if os . path . isdir ( name ) :,0.8820016898747209,65.80370064762461
"def _find_all_variables(transfer_variable): d = {} for _k, _v in transfer_variable.__dict__.items(): if isinstance(_v, Variable): d[_v._name] = _v <mask>: d.update(_find_all_variables(_v)) return d",False,"elif isinstance ( _v , BaseTransferVariables ) :","elif isinstance ( _v , dict ) :",0.8820016898747209,66.06328636027612
"def set_val(): idx = 0 for idx in range(0, len(model)): row = model[idx] <mask>: break if idx == len(os_widget.get_model()) - 1: idx = -1 os_widget.set_active(idx) if idx == -1: os_widget.set_active(0) if idx >= 0: return row[1] if self.show_all_os: return None",False,if value and row [ 0 ] == value :,if row [ 0 ] == self . _model_value :,0.8820016898747209,40.52587697205425
"def _make_cache_key(group, window, rate, value, methods): count, period = _split_rate(rate) safe_rate = ""%d/%ds"" % (count, period) parts = [group, safe_rate, value, str(window)] if methods is not None: if methods == ALL: methods = """" <mask>: methods = """".join(sorted([m.upper() for m in methods])) parts.append(methods) prefix = getattr(settings, ""RATELIMIT_CACHE_PREFIX"", ""rl:"") return prefix + hashlib.md5(u"""".join(parts).encode(""utf-8"")).hexdigest()",False,"elif isinstance ( methods , ( list , tuple ) ) :",elif methods :,0.8820016898747209,0.0
"def findfiles(path): files = [] for name in os.listdir(path): # ignore hidden files/dirs and other unwanted files <mask>: continue pathname = os.path.join(path, name) st = os.lstat(pathname) mode = st.st_mode if stat.S_ISDIR(mode): files.extend(findfiles(pathname)) elif stat.S_ISREG(mode): files.append((pathname, name, st)) return files",False,"if name . startswith ( ""."" ) or name == ""lastsnap.jpg"" :","if name . startswith ( ""_"" ) :",0.8820016898747209,24.345633712861616
"def __getitem__(self, key): if isinstance(key, str_types): keys = self.get_keys() <mask>: raise KeyError(' ""{0}"" is an invalid key'.format(key)) else: return self[keys.index(key)] else: return list.__getitem__(self, key)",False,if key not in keys :,if len ( keys ) < 1 :,0.8820016898747209,7.267884212102741
"def test_assert_set_equal(estimate: tp.Iterable[int], message: str) -> None: reference = {1, 2, 3} try: testing.assert_set_equal(estimate, reference) except AssertionError as error: if not message: raise AssertionError( ""An error has been raised while it should not."" ) from error np.testing.assert_equal(error.args[0].split(""\n"")[1:], message) else: <mask>: raise AssertionError(""An error should have been raised."")",False,if message :,"if not np . testing . assert_equal ( estimate , reference ) :",0.8820016898747209,3.1251907639724417
"def get_directory_info(prefix, pth, recursive): res = [] directory = os.listdir(pth) directory.sort() for p in directory: <mask>: subp = os.path.join(pth, p) p = os.path.join(prefix, p) if recursive and os.path.isdir(subp): res.append([p, get_directory_info(prefix, subp, 1)]) else: res.append([p, None]) return res",False,"if p [ 0 ] != ""."" :",if os . path . isdir ( p ) :,0.8820016898747209,5.369488567517933
"def check(self, runner, script, info): if isinstance(info, ast.FunctionDef): for arg in info.args.args: <mask>: if arg.id in script.modelVars: self.problem( ""Function {0} may shadow model variable {1}"".format( info.name, arg.id ), lineno=info.lineno, )",False,"if isinstance ( arg , ast . Name ) :","if isinstance ( arg , ast . VariableDef ) :",0.8820016898747209,70.71067811865478
"def db_lookup(field, key, publish_year=None): sql = ""select sum(ebook_count) as num from subjects where field=$field and key=$key"" if publish_year: <mask>: sql += "" and publish_year between $y1 and $y2"" (y1, y2) = publish_year else: sql += "" and publish_year=$publish_year"" return list(ebook_count_db.query(sql, vars=locals()))[0].num",False,"if isinstance ( publish_year , ( tuple , list ) ) :",if publish_year < date ( ) :,0.8820016898747209,13.927628237847681
"def put(self, session): with sess_lock: self.parent.put(session) # Do not store the session if skip paths for sp in self.skip_paths: <mask>: return if session.sid in self._cache: try: del self._cache[session.sid] except Exception: pass self._cache[session.sid] = session self._normalize()",False,if request . path . startswith ( sp ) :,if sp . is_session :,0.8820016898747209,6.050259138270144
"def summarize(self): if self.bad_commit and self.good_commit: for subresult in self.subresults.values(): sub = subresult.summarize() <mask>: return sub return ""Detected bad commit in {} repository:\n{} {}"".format( self.repo_name, self.bad_commit, get_message(self.suite, self.bad_commit) ) return """"",True,if sub :,if sub :,0.8820016898747209,0.0
def compute_nullable_nonterminals(self): nullable = {} num_nullable = 0 while 1: for p in self.grammar.Productions[1:]: if p.len == 0: nullable[p.name] = 1 continue for t in p.prod: if not t in nullable: break else: nullable[p.name] = 1 <mask>: break num_nullable = len(nullable) return nullable,False,if len ( nullable ) == num_nullable :,if num_nullable == 0 :,0.8820016898747209,19.324558191221733
"def _cast_float64_to_float32(self, feeds): for input_name, input_type in self.inputs: <mask>: feed = feeds.get(input_name) if feed is not None and feed.dtype == np.float64: feeds[input_name] = feed.astype(np.float32) return feeds",False,"if input_type == ""tensor(float)"" :","if input_type == ""float64"" :",0.8820016898747209,52.38375874705952
"def proc_minute(d): if expanded[0][0] != ""*"": diff_min = nearest_diff_method(d.minute, expanded[0], 60) <mask>: if is_prev: d += relativedelta(minutes=diff_min, second=59) else: d += relativedelta(minutes=diff_min, second=0) return True, d return False, d",False,if diff_min is not None and diff_min != 0 :,if diff_min :,0.8820016898747209,9.569649651041097
"def detype(self): if self._detyped is not None: return self._detyped ctx = {} for key, val in self._d.items(): if not isinstance(key, str): key = str(key) detyper = self.get_detyper(key) if detyper is None: # cannot be detyped continue deval = detyper(val) <mask>: # cannot be detyped continue ctx[key] = deval self._detyped = ctx return ctx",True,if deval is None :,if deval is None :,0.8820016898747209,100.00000000000004
"def get_or_create_user(request, user_data): try: user = User.objects.get(sso_id=user_data[""id""]) <mask>: update_user(user, user_data) return user except User.DoesNotExist: user = User.objects.create_user( user_data[""username""], user_data[""email""], is_active=user_data.get(""is_active"", True), sso_id=user_data[""id""], ) user.update_acl_key() setup_new_user(request.settings, user) return user",False,"if user_needs_updating ( user , user_data ) :",if user . is_active :,0.8820016898747209,5.746166391236874
"def _populate_tree(self, element, d): """"""Populates an etree with attributes & elements, given a dict."""""" for k, v in d.iteritems(): if isinstance(v, dict): self._populate_dict(element, k, v) elif isinstance(v, list): self._populate_list(element, k, v) <mask>: self._populate_bool(element, k, v) elif isinstance(v, basestring): self._populate_str(element, k, v) elif type(v) in [int, float, long, complex]: self._populate_number(element, k, v)",True,"elif isinstance ( v , bool ) :","elif isinstance ( v , bool ) :",0.8820016898747209,100.00000000000004
"def load(cls): if not cls._loaded: cls.log.debug(""Loading action_sets..."") <mask>: cls._find_action_sets(PATHS.ACTION_SETS_DIRECTORY) else: cls.action_sets = JsonDecoder.load(PATHS.ACTION_SETS_JSON_FILE) cls.log.debug(""Done!"") cls._loaded = True",False,if not horizons . globals . fife . use_atlases :,if not os . path . exists ( PATHS . ACTION_SETS_DIRECTORY ) :,0.8820016898747209,6.256118460580956
"def Resolve(self, updater=None): if len(self.Conflicts): for setting, edge in self.Conflicts: answer = self.AskUser(self.Setting, setting) <mask>: value = setting.Value.split(""|"") value.remove(edge) setting.Value = ""|"".join(value) if updater: updater.UpdateSetting(setting) if answer == Gtk.ResponseType.NO: return False return True",False,if answer == Gtk . ResponseType . YES :,if answer == Gtk . ResponseType . NO :,0.8820016898747209,78.25422900366438
"def read_tsv(input_file, quotechar=None): """"""Reads a tab separated value file."""""" with open(input_file, ""r"", encoding=""utf-8-sig"") as f: reader = csv.reader(f, delimiter=""\t"", quotechar=quotechar) lines = [] for line in reader: <mask>: line = list(str(cell, ""utf-8"") for cell in line) # noqa: F821 lines.append(line) return lines",False,if sys . version_info [ 0 ] == 2 :,if line :,0.8820016898747209,0.0
"def devd_devfs_hook(middleware, data): if data.get(""subsystem"") != ""CDEV"": return if data[""type""] == ""CREATE"": disks = await middleware.run_in_thread( lambda: sysctl.filter(""kern.disks"")[0].value.split() ) # Device notified about is not a disk if data[""cdev""] not in disks: return await added_disk(middleware, data[""cdev""]) elif data[""type""] == ""DESTROY"": # Device notified about is not a disk <mask>: return await remove_disk(middleware, data[""cdev""])",False,"if not RE_ISDISK . match ( data [ ""cdev"" ] ) :","if data [ ""cdev"" ] not in disks :",0.8820016898747209,32.43475052602429
"def on_edit_button_clicked(self, event=None, a=None, col=None): tree, tree_id = self.treeView.get_selection().get_selected() watchdir_id = str(self.store.get_value(tree_id, 0)) if watchdir_id: <mask>: if self.watchdirs[watchdir_id][""enabled""]: client.autoadd.disable_watchdir(watchdir_id) else: client.autoadd.enable_watchdir(watchdir_id) else: self.opts_dialog.show(self.watchdirs[watchdir_id], watchdir_id)",False,"if col and col . get_title ( ) == _ ( ""Active"" ) :","if self . watchdirs [ watchdir_id ] [ ""enabled"" ] :",0.8820016898747209,3.150249737045819
"def _execute(self, options, args): if len(args) < 1: raise CommandError(_(""Not enough arguments"")) paths = args songs = [self.load_song(p) for p in paths] for song in songs: <mask>: raise CommandError( _(""Image editing not supported for %(file_name)s "" ""(%(file_format)s)"") % {""file_name"": song(""~filename""), ""file_format"": song(""~format"")} ) for song in songs: try: song.clear_images() except AudioFileError as e: raise CommandError(e)",False,if not song . can_change_images :,if not song . is_image ( ) :,0.8820016898747209,29.071536848410968
"def filter_pricing_rule_based_on_condition(pricing_rules, doc=None): filtered_pricing_rules = [] if doc: for pricing_rule in pricing_rules: <mask>: try: if frappe.safe_eval(pricing_rule.condition, None, doc.as_dict()): filtered_pricing_rules.append(pricing_rule) except: pass else: filtered_pricing_rules.append(pricing_rule) else: filtered_pricing_rules = pricing_rules return filtered_pricing_rules",False,if pricing_rule . condition :,"if frappe . safe_eval ( pricing_rule . condition , None , doc . as_dict ( ) ) :",0.8820016898747209,16.020720994064927
"def ProcessStringLiteral(self): if self._lastToken == None or self._lastToken.type == self.OpenBrace: text = super(JavaScriptBaseLexer, self).text if text == '""use strict""' or text == ""'use strict'"": <mask>: self._scopeStrictModes.pop() self._useStrictCurrent = True self._scopeStrictModes.append(self._useStrictCurrent)",False,if len ( self . _scopeStrictModes ) > 0 :,if self . _useStrictCurrent :,0.8820016898747209,14.919518511396246
"def _find_remote_inputs(metadata): out = [] for fr_key in metadata.keys(): if isinstance(fr_key, (list, tuple)): frs = fr_key else: frs = [fr_key] for fr in frs: <mask>: out.append(fr) return out",False,if objectstore . is_remote ( fr ) :,"if fr . get ( ""remote_inputs"" ) :",0.8820016898747209,9.578464408619825
"def sub_paragraph(self, li): """"""Search for checkbox in sub-paragraph."""""" found = False if len(li): first = list(li)[0] if first.tag == ""p"" and first.text is not None: m = RE_CHECKBOX.match(first.text) <mask>: first.text = self.markdown.htmlStash.store( get_checkbox(m.group(""state"")), safe=True ) + m.group(""line"") found = True return found",False,if m is not None :,if m :,0.8820016898747209,0.0
"def list_files(basedir): """"""List files in the directory rooted at |basedir|."""""" if not os.path.isdir(basedir): raise NoSuchDirectory(basedir) directories = [""""] while directories: d = directories.pop() for basename in os.listdir(os.path.join(basedir, d)): filename = os.path.join(d, basename) <mask>: directories.append(filename) elif os.path.exists(os.path.join(basedir, filename)): yield filename",False,"if os . path . isdir ( os . path . join ( basedir , filename ) ) :","if os . path . isdir ( os . path . join ( d , filename ) ) :",0.8820016898747209,84.92326635760686
"def TryMerge(self, d): while d.avail() > 0: tt = d.getVarInt32() if tt == 10: self.set_version(d.getPrefixedString()) continue <mask>: raise ProtocolBuffer.ProtocolBufferDecodeError d.skipData(tt)",True,if tt == 0 :,if tt == 0 :,0.8820016898747209,100.00000000000004
"def _dump(self, fd): with self.no_unpicklable_properties(): <mask>: d = pickle.dumps(self) module_name = os.path.basename(sys.argv[0]).rsplit(""."", 1)[0] d = d.replace(b""c__main__"", b""c"" + module_name.encode(""ascii"")) fd.write(d) else: pickle.dump(self, fd)",False,"if self . __module__ == ""__main__"" :",if self . _is_pickle :,0.8820016898747209,10.961757806919676
"def assert_session_stack(classes): assert len(_SklearnTrainingSession._session_stack) == len(classes) for idx, (sess, (parent_clazz, clazz)) in enumerate( zip(_SklearnTrainingSession._session_stack, classes) ): assert sess.clazz == clazz <mask>: assert sess._parent is None else: assert sess._parent.clazz == parent_clazz",True,if idx == 0 :,if idx == 0 :,0.8820016898747209,100.00000000000004
"def native_color(c): try: color = CACHE[c] except KeyError: <mask>: c = NAMED_COLOR[c] color = Color.FromArgb( int(c.rgba.a * 255), int(c.rgba.r), int(c.rgba.g), int(c.rgba.b) ) CACHE[c] = color return color",False,"if isinstance ( c , str ) :",if c in NAMED_COLOR :,0.8820016898747209,7.492442692259767
"def callback(name): # XXX: move into Action for neighbor_name in reactor.configuration.neighbors.keys(): neighbor = reactor.configuration.neighbors.get(neighbor_name, None) <mask>: continue neighbor.rib.outgoing.announce_watchdog(name) yield False reactor.processes.answer_done(service)",False,if not neighbor :,if neighbor is None :,0.8820016898747209,14.058533129758727
"def token_producer(source): token = source.read_uint8() while token is not None: <mask>: yield DataToken(read_data(token, source)) elif is_small_integer(token): yield SmallIntegerToken(read_small_integer(token)) else: yield Token(token) token = source.read_uint8()",False,if is_push_data_token ( token ) :,if is_data ( token ) :,0.8820016898747209,32.81829856080947
"def setattr(self, req, ino, attr, to_set, fi): print(""setattr:"", ino, to_set) a = self.attr[ino] for key in to_set: <mask>: # Keep the old file type bit fields a[""st_mode""] = S_IFMT(a[""st_mode""]) | S_IMODE(attr[""st_mode""]) else: a[key] = attr[key] self.attr[ino] = a self.reply_attr(req, a, 1.0)",False,"if key == ""st_mode"" :",if fi :,0.8820016898747209,0.0
"def check_enum_exports(module, eq_callback, only=None): """"""Make sure module exports all mnemonics from enums"""""" for attr in enumerate_module(module, enum.Enum): <mask>: print(""SKIP"", attr) continue for flag, value in attr.__members__.items(): print(module, flag, value) eq_callback(getattr(module, flag), value)",False,if only is not None and attr not in only :,if only :,0.8820016898747209,0.0
"def remove_edit_vars_to(self, n): try: removals = [] for v, cei in self.edit_var_map.items(): <mask>: removals.append(v) for v in removals: self.remove_edit_var(v) assert len(self.edit_var_map) == n except ConstraintNotFound: raise InternalError(""Constraint not found during internal removal"")",False,if cei . index >= n :,if cei . edit_var_id == n :,0.8820016898747209,21.200626759025184
"def fix_repeating_arguments(self): """"""Fix elements that should accumulate/increment values."""""" either = [list(child.children) for child in transform(self).children] for case in either: for e in [child for child in case if case.count(child) > 1]: if type(e) is Argument or type(e) is Option and e.argcount: <mask>: e.value = [] elif type(e.value) is not list: e.value = e.value.split() if type(e) is Command or type(e) is Option and e.argcount == 0: e.value = 0 return self",False,if e . value is None :,if type ( e . value ) is None :,0.8820016898747209,27.301208627090666
"def add_I_prefix(current_line: List[str], ner: int, tag: str): for i in range(0, len(current_line)): if i == 0: f.write(line_list[i]) <mask>: f.write("" I-"" + tag) else: f.write("" "" + current_line[i]) f.write(""\n"")",True,elif i == ner :,elif i == ner :,0.8820016898747209,100.00000000000004
def select_word_at_cursor(self): word_region = None selection = self.view.sel() for region in selection: word_region = self.view.word(region) <mask>: selection.clear() selection.add(word_region) return word_region return word_region,False,if not word_region . empty ( ) :,if word_region is not None :,0.8820016898747209,19.03868163669696
"def calc(self, arg): op = arg[""op""] if op == ""C"": self.clear() return str(self.current) num = decimal.Decimal(arg[""num""]) if self.op: if self.op == ""+"": self.current += num <mask>: self.current -= num elif self.op == ""*"": self.current *= num elif self.op == ""/"": self.current /= num self.op = op else: self.op = op self.current = num res = str(self.current) if op == ""="": self.clear() return res",True,"elif self . op == ""-"" :","elif self . op == ""-"" :",0.8820016898747209,100.00000000000004
"def strip_pod(lines): in_pod = False stripped_lines = [] for line in lines: if re.match(r""^=(?:end|cut)"", line): in_pod = False elif re.match(r""^=\w+"", line): in_pod = True <mask>: stripped_lines.append(line) return stripped_lines",False,elif not in_pod :,elif in_pod :,0.8820016898747209,57.89300674674101
"def __init__(self, patch_files, patch_directories): files = [] files_data = {} for filename_data in patch_files: if isinstance(filename_data, list): filename, data = filename_data else: filename = filename_data data = None if not filename.startswith(os.sep): filename = ""{0}{1}"".format(FakeState.deploy_dir, filename) files.append(filename) <mask>: files_data[filename] = data self.files = files self.files_data = files_data self.directories = patch_directories",True,if data :,if data :,0.8820016898747209,0.0
"def loadPerfsFromModule(self, module): """"""Return a suite of all perfs cases contained in the given module"""""" perfs = [] for name in dir(module): obj = getattr(module, name) <mask>: perfs.append(self.loadPerfsFromPerfCase(obj)) return self.suiteClass(perfs)",False,"if type ( obj ) == types . ClassType and issubclass ( obj , PerfCase ) :","if isinstance ( obj , PerfCase ) :",0.8820016898747209,20.258948470231477
"def download_subtitle(self, subtitle): if isinstance(subtitle, XSubsSubtitle): # download the subtitle logger.info(""Downloading subtitle %r"", subtitle) r = self.session.get( subtitle.download_link, headers={""Referer"": subtitle.page_link}, timeout=10 ) r.raise_for_status() <mask>: logger.debug(""Unable to download subtitle. No data returned from provider"") return subtitle.content = fix_line_ending(r.content)",False,if not r . content :,if r . status_code == 404 :,0.8820016898747209,9.980099403873663
"def get_inlaws(self, person): inlaws = [] family_handles = person.get_family_handle_list() for handle in family_handles: fam = self.database.get_family_from_handle(handle) if fam.father_handle and not fam.father_handle == person.handle: inlaws.append(self.database.get_person_from_handle(fam.father_handle)) <mask>: inlaws.append(self.database.get_person_from_handle(fam.mother_handle)) return inlaws",False,elif fam . mother_handle and not fam . mother_handle == person . handle :,if fam . mother_handle and not fam . mother_handle == person . handle :,0.8820016898747209,94.2615147681512
"def _check_xorg_conf(): if is_there_a_default_xorg_conf_file(): print( ""WARNING : Found a Xorg config file at /etc/X11/xorg.conf. If you did not"" "" create it yourself, it was likely generated by your distribution or by an Nvidia utility.\n"" ""This file may contain hard-coded GPU configuration that could interfere with optimus-manager,"" "" so it is recommended that you delete it before proceeding.\n"" ""Ignore this warning and proceed with GPU switching ? (y/N)"" ) confirmation = ask_confirmation() <mask>: sys.exit(0)",False,if not confirmation :,if confirmation is None :,0.8820016898747209,14.058533129758727
"def _make_cache_key(group, window, rate, value, methods): count, period = _split_rate(rate) safe_rate = ""%d/%ds"" % (count, period) parts = [group, safe_rate, value, str(window)] if methods is not None: <mask>: methods = """" elif isinstance(methods, (list, tuple)): methods = """".join(sorted([m.upper() for m in methods])) parts.append(methods) prefix = getattr(settings, ""RATELIMIT_CACHE_PREFIX"", ""rl:"") return prefix + hashlib.md5(u"""".join(parts).encode(""utf-8"")).hexdigest()",False,if methods == ALL :,"if methods == ""none"" :",0.8820016898747209,36.55552228545123
"def num_of_mapped_volumes(self, initiator): cnt = 0 for lm_link in self.req(""lun-maps"")[""lun-maps""]: idx = lm_link[""href""].split(""/"")[-1] # NOTE(geguileo): There can be races so mapped elements retrieved # in the listing may no longer exist. try: lm = self.req(""lun-maps"", idx=int(idx))[""content""] except exception.NotFound: continue <mask>: cnt += 1 return cnt",False,"if lm [ ""ig-name"" ] == initiator :","if lm [ ""type"" ] == ""vol"" :",0.8820016898747209,39.832871551569504
"def _setAbsoluteY(self, value): if value is None: self._absoluteY = None else: <mask>: value = 10 elif value == ""below"": value = -70 try: value = common.numToIntOrFloat(value) except ValueError as ve: raise TextFormatException( f""Not a supported absoluteY position: {value!r}"" ) from ve self._absoluteY = value",False,"if value == ""above"" :","if value == ""below"" :",0.8820016898747209,59.4603557501361
"def render_markdown(text): users = {u.username.lower(): u for u in get_mention_users(text)} parts = MENTION_RE.split(text) for pos, part in enumerate(parts): if not part.startswith(""@""): continue username = part[1:].lower() <mask>: user = users[username] parts[pos] = '**[{}]({} ""{}"")**'.format( part, user.get_absolute_url(), user.get_visible_name() ) text = """".join(parts) return mark_safe(MARKDOWN(text))",True,if username in users :,if username in users :,0.8820016898747209,100.00000000000004
def start_process(self): with self.thread_lock: <mask>: self.allow_process_request = False t = threading.Thread(target=self.__start) t.daemon = True t.start(),True,if self . allow_process_request :,if self . allow_process_request :,0.8820016898747209,100.00000000000004
"def close(self): if self._fh.closed: return self._fh.close() if os.path.isfile(self._filename): <mask>: salt.utils.win_dacl.copy_security( source=self._filename, target=self._tmp_filename ) else: shutil.copymode(self._filename, self._tmp_filename) st = os.stat(self._filename) os.chown(self._tmp_filename, st.st_uid, st.st_gid) atomic_rename(self._tmp_filename, self._filename)",False,if salt . utils . win_dacl . HAS_WIN32 :,if os . path . isfile ( self . _tmp_filename ) :,0.8820016898747209,4.274580923189599
"def _splitSchemaNameDotFieldName(sn_fn, fnRequired=True): if sn_fn.find(""."") != -1: schemaName, fieldName = sn_fn.split(""."", 1) schemaName = schemaName.strip() fieldName = fieldName.strip() if schemaName and fieldName: return (schemaName, fieldName) elif not fnRequired: schemaName = sn_fn.strip() <mask>: return (schemaName, None) controlflow.system_error_exit( 2, f""{sn_fn} is not a valid custom schema.field name."" )",False,if schemaName :,if schemaName and not fieldName :,0.8820016898747209,17.965205598154213
"def modified(self): paths = set() dictionary_list = [] for op_list in self._operations: <mask>: op_list = (op_list,) for item in chain(*op_list): if item is None: continue dictionary = item.dictionary if dictionary.path in paths: continue paths.add(dictionary.path) dictionary_list.append(dictionary) return dictionary_list",False,"if not isinstance ( op_list , list ) :","if isinstance ( op_list , tuple ) :",0.8820016898747209,54.182204258059556
"def apply(self, db, person): for family_handle in person.get_family_handle_list(): family = db.get_family_from_handle(family_handle) <mask>: for event_ref in family.get_event_ref_list(): if event_ref: event = db.get_event_from_handle(event_ref.ref) if not event.get_place_handle(): return True if not event.get_date_object(): return True return False",True,if family :,if family :,0.8820016898747209,0.0
"def test_cleanup_params(self, body, rpc_mock): res = self._get_resp_post(body) self.assertEqual(http_client.ACCEPTED, res.status_code) rpc_mock.assert_called_once_with(self.context, mock.ANY) cleanup_request = rpc_mock.call_args[0][1] for key, value in body.items(): <mask>: if value is not None: value = value == ""true"" self.assertEqual(value, getattr(cleanup_request, key)) self.assertEqual(self._expected_services(*SERVICES), res.json)",False,"if key in ( ""disabled"" , ""is_up"" ) :","if key . startswith ( ""cleanup_"" ) :",0.8820016898747209,14.737486969787158
"def get_billable_and_total_duration(activity, start_time, end_time): precision = frappe.get_precision(""Timesheet Detail"", ""hours"") activity_duration = time_diff_in_hours(end_time, start_time) billing_duration = 0.0 if activity.billable: billing_duration = activity.billing_hours <mask>: billing_duration = ( activity_duration * activity.billing_hours / activity.hours ) return flt(activity_duration, precision), flt(billing_duration, precision)",False,if activity_duration != activity . billing_hours :,if activity . hours :,0.8820016898747209,8.718519271156227
"def cpus(self): try: cpus = ( self.inspect[""Spec""][""Resources""][""Reservations""][""NanoCPUs""] / 1000000000.0 ) <mask>: cpus = int(cpus) return cpus except TypeError: return None except KeyError: return 0",False,if cpus == int ( cpus ) :,if cpus :,0.8820016898747209,0.0
"def _create_object(self, obj_body): props = obj_body[SYMBOL_PROPERTIES] for prop_name, prop_value in props.items(): <mask>: # get the first key as the convert function func_name = list(prop_value.keys())[0] if func_name.startswith(""_""): func = getattr(self, func_name) props[prop_name] = func(prop_value[func_name]) if SYMBOL_TYPE in obj_body and obj_body[SYMBOL_TYPE] in self.fake_func_mapping: return self.fake_func_mapping[obj_body[SYMBOL_TYPE]](**props) else: return props",False,"if isinstance ( prop_value , dict ) and prop_value :",if prop_name in obj_body :,0.8820016898747209,6.87938864869854
"def _yield_unescaped(self, string): while ""\\"" in string: finder = EscapeFinder(string) yield finder.before + finder.backslashes <mask>: yield self._unescape(finder.text) else: yield finder.text string = finder.after yield string",False,if finder . escaped and finder . text :,if self . _is_escaped ( finder . text ) :,0.8820016898747209,14.211672443220438
"def _check_matches(rule, matches): errors = 0 for match in matches: filematch = _match_to_test_file(match) <mask>: utils.error( ""The match '{}' for rule '{}' points to a non existing test module path: {}"", match, rule, filematch, ) errors += 1 return errors",False,if not filematch . exists ( ) :,if not filematch :,0.8820016898747209,23.50540321304655
"def focused_windows(): tree = i3.get_tree() workspaces = tree.workspaces() for workspace in workspaces: container = workspace while container: if not hasattr(container, ""focus"") or not container.focus: break container_id = container.focus[0] container = container.find_by_id(container_id) <mask>: coname = container.name wsname = workspace.name print(""WS"", wsname + "":"", coname)",True,if container :,if container :,0.8820016898747209,0.0
"def normals(self, value): if value is not None: value = np.asanyarray(value, dtype=np.float32) value = np.ascontiguousarray(value) <mask>: raise ValueError(""Incorrect normals shape"") self._normals = value",False,if value . shape != self . positions . shape :,if len ( value . shape ) != self . shape :,0.8820016898747209,35.74046404361828
"def test_hexdigest(self): for cons in self.hash_constructors: h = cons() <mask>: self.assertIsInstance(h.digest(16), bytes) self.assertEqual(hexstr(h.digest(16)), h.hexdigest(16)) else: self.assertIsInstance(h.digest(), bytes) self.assertEqual(hexstr(h.digest()), h.hexdigest())",False,if h . name in self . shakes :,if h . is_hex :,0.8820016898747209,19.740631366145518
"def _get_cluster_status(self): try: return ( self.dataproc_client.projects() .regions() .clusters() .get( projectId=self.gcloud_project_id, region=self.dataproc_region, clusterName=self.dataproc_cluster_name, fields=""status"", ) .execute() ) except HttpError as e: <mask>: return None # We got a 404 so the cluster doesn't exist else: raise e",False,if e . resp . status == 404 :,if e . status_code == 404 :,0.8820016898747209,41.11336169005196
"def _items_from(self, context): self._context = context if self._is_local_variable(self._keyword_name, context): for item in self._items_from_controller(context): yield item else: for df in context.datafiles: self._yield_for_other_threads() <mask>: for item in self._items_from_datafile(df): yield item",False,if self . _items_from_datafile_should_be_checked ( df ) :,"if self . _is_local_variable ( self . _keyword_name , df ) :",0.8820016898747209,19.83544145418288
"def Command(argv, funcs, path_val): arg, i = COMMAND_SPEC.Parse(argv) status = 0 if arg.v: for kind, arg in _ResolveNames(argv[i:], funcs, path_val): <mask>: status = 1 # nothing printed, but we fail else: # This is for -v, -V is more detailed. print(arg) else: util.warn(""*** command without -v not not implemented ***"") status = 1 return status",False,if kind is None :,"if kind == ""v"" :",0.8820016898747209,12.22307556087252
"def delete_doc(elastic_document_id, node, index=None, category=None): index = index or INDEX if not category: if isinstance(node, Preprint): category = ""preprint"" <mask>: category = ""registration"" else: category = node.project_or_component client().delete( index=index, doc_type=category, id=elastic_document_id, refresh=True, ignore=[404], )",False,elif node . is_registration :,"elif isinstance ( node , Registration ) :",0.8820016898747209,7.267884212102741
"def getDictFromTree(tree): ret_dict = {} for child in tree.getchildren(): <mask>: ## Complex-type child. Recurse content = getDictFromTree(child) else: content = child.text if ret_dict.has_key(child.tag): if not type(ret_dict[child.tag]) == list: ret_dict[child.tag] = [ret_dict[child.tag]] ret_dict[child.tag].append(content or """") else: ret_dict[child.tag] = content or """" return ret_dict",False,if child . getchildren ( ) :,"if isinstance ( child , tree . tree . tree_node ) :",0.8820016898747209,7.474875887495341
"def get(self, block=True, timeout=None, ack=False): if not block: return self.get_nowait() start_time = time.time() while True: try: return self.get_nowait(ack) except BaseQueue.Empty: <mask>: lasted = time.time() - start_time if timeout > lasted: time.sleep(min(self.max_timeout, timeout - lasted)) else: raise else: time.sleep(self.max_timeout)",False,if timeout :,if timeout is not None :,0.8820016898747209,17.965205598154213
"def rewrite(self, string): string = super(JSReplaceFuzzy, self).rewrite(string) cdx = self.url_rewriter.rewrite_opts[""cdx""] if cdx.get(""is_fuzzy""): expected = unquote(cdx[""url""]) actual = unquote(self.url_rewriter.wburl.url) exp_m = self.rx_obj.search(expected) act_m = self.rx_obj.search(actual) <mask>: result = string.replace(exp_m.group(1), act_m.group(1)) if result != string: string = result return string",False,if exp_m and act_m :,if exp_m :,0.8820016898747209,37.783911519583654
"def locate_exe_dir(d, check=True): exe_dir = os.path.join(d, ""Scripts"") if ON_WINDOWS else os.path.join(d, ""bin"") if not os.path.isdir(exe_dir): <mask>: bin_dir = os.path.join(d, ""bin"") if os.path.isdir(bin_dir): return bin_dir if check: raise InvalidVirtualEnv(""Unable to locate executables directory."") return exe_dir",False,if ON_WINDOWS :,if os . path . exists ( exe_dir ) :,0.8820016898747209,4.456882760699063
"def _ensuresyspath(self, ensuremode, path): if ensuremode: s = str(path) if ensuremode == ""append"": if s not in sys.path: sys.path.append(s) else: <mask>: sys.path.insert(0, s)",False,if s != sys . path [ 0 ] :,if s not in sys . path :,0.8820016898747209,18.594002123233256
"def create_season_banners(self, show_obj): if self.season_banners and show_obj: result = [] for season, episodes in show_obj.episodes.iteritems(): # @UnusedVariable <mask>: logger.log( u""Metadata provider "" + self.name + "" creating season banners for "" + show_obj.name, logger.DEBUG, ) result = result + [self.save_season_banners(show_obj, season)] return all(result) return False",False,"if not self . _has_season_banner ( show_obj , season ) :",if season not in self . season_banners :,0.8820016898747209,6.341825373820594
"def validate_nb(self, nb): super(MetadataValidatorV3, self).validate_nb(nb) ids = set([]) for cell in nb.cells: if ""nbgrader"" not in cell.metadata: continue grade = cell.metadata[""nbgrader""][""grade""] solution = cell.metadata[""nbgrader""][""solution""] locked = cell.metadata[""nbgrader""][""locked""] <mask>: continue grade_id = cell.metadata[""nbgrader""][""grade_id""] if grade_id in ids: raise ValidationError(""Duplicate grade id: {}"".format(grade_id)) ids.add(grade_id)",False,if not grade and not solution and not locked :,if grade != solution or locked != grade :,0.8820016898747209,5.604233375480572
"def read_version(): regexp = re.compile(r""^__version__\W*=\W*'([\d.abrc]+)'"") init_py = os.path.join(os.path.dirname(__file__), ""aiopg"", ""__init__.py"") with open(init_py) as f: for line in f: match = regexp.match(line) <mask>: return match.group(1) else: raise RuntimeError(""Cannot find version in aiopg/__init__.py"")",False,if match is not None :,if match :,0.8820016898747209,0.0
"def _column_keys(self): """"""Get a dictionary of all columns and their case mapping."""""" if not self.exists: return {} with self.db.lock: if self._columns is None: # Initialise the table if it doesn't exist table = self.table self._columns = {} for column in table.columns: name = normalize_column_name(column.name) key = normalize_column_key(name) <mask>: log.warning(""Duplicate column: %s"", name) self._columns[key] = name return self._columns",True,if key in self . _columns :,if key in self . _columns :,0.8820016898747209,100.00000000000004
"def find_controller_by_names(self, names, testname): namestring = ""."".join(names) if not namestring.startswith(self.name): return None if namestring == self.name: return self for suite in self.suites: res = suite.find_controller_by_names( namestring[len(self.name) + 1 :].split("".""), testname ) <mask>: return res",True,if res :,if res :,0.8820016898747209,0.0
"def _volume_x_metadata_get_item( context, volume_id, key, model, notfound_exec, session=None ): result = ( _volume_x_metadata_get_query(context, volume_id, model, session=session) .filter_by(key=key) .first() ) if not result: <mask>: raise notfound_exec(id=volume_id) else: raise notfound_exec(metadata_key=key, volume_id=volume_id) return result",False,if model is models . VolumeGlanceMetadata :,"if key == ""id"" :",0.8820016898747209,6.567274736060395
"def parse_results(cwd): optimal_dd = None optimal_measure = numpy.inf for tup in tools.find_conf_files(cwd): dd = tup[1] <mask>: if dd[""results.train_y_misclass""] < optimal_measure: optimal_measure = dd[""results.train_y_misclass""] optimal_dd = dd print(""Optimal results.train_y_misclass:"", str(optimal_measure)) for key, value in optimal_dd.items(): if ""hyper_parameters"" in key: print(key + "": "" + str(value))",True,"if ""results.train_y_misclass"" in dd :","if ""results.train_y_misclass"" in dd :",0.8820016898747209,100.00000000000004
"def _stop_by_max_time_mins(self): """"""Stop optimization process once maximum minutes have elapsed."""""" if self.max_time_mins: total_mins_elapsed = ( datetime.now() - self._start_datetime ).total_seconds() / 60.0 <mask>: raise KeyboardInterrupt( ""{:.2f} minutes have elapsed. TPOT will close down."".format( total_mins_elapsed ) )",False,if total_mins_elapsed >= self . max_time_mins :,if total_mins_elapsed > self . max_time_mins :,0.8820016898747209,81.96501312471537
"def __new__(meta, cls_name, bases, cls_dict): func = cls_dict.get(""func"") monad_cls = super(FuncMonadMeta, meta).__new__(meta, cls_name, bases, cls_dict) if func: <mask>: functions = func else: functions = (func,) for func in functions: registered_functions[func] = monad_cls return monad_cls",False,if type ( func ) is tuple :,"if isinstance ( func , ( list , tuple ) ) :",0.8820016898747209,8.91376552139813
"def get_tokens_unprocessed(self, text): buffered = """" insertions = [] lng_buffer = [] for i, t, v in self.language_lexer.get_tokens_unprocessed(text): <mask>: if lng_buffer: insertions.append((len(buffered), lng_buffer)) lng_buffer = [] buffered += v else: lng_buffer.append((i, t, v)) if lng_buffer: insertions.append((len(buffered), lng_buffer)) return do_insertions(insertions, self.root_lexer.get_tokens_unprocessed(buffered))",False,if t is self . needle :,"if t == "" "" :",0.8820016898747209,14.535768424205482
"def get_conditions(filters): conditions = {""docstatus"": (""="", 1)} if filters.get(""from_date"") and filters.get(""to_date""): conditions[""result_date""] = ( ""between"", (filters.get(""from_date""), filters.get(""to_date"")), ) filters.pop(""from_date"") filters.pop(""to_date"") for key, value in filters.items(): <mask>: conditions[key] = value return conditions",False,if filters . get ( key ) :,"if isinstance ( value , dict ) :",0.8820016898747209,13.134549472120788
"def _limit_value(key, value, config): if config[key].get(""upper_limit""): limit = config[key][""upper_limit""] # auto handle datetime if isinstance(value, datetime) and isinstance(limit, timedelta): <mask>: if (datetime.now() - limit) > value: value = datetime.now() - limit else: if (datetime.now() + limit) < value: value = datetime.now() + limit elif value > limit: value = limit return value",False,"if config [ key ] [ ""inverse"" ] is True :",if value < 0 :,0.8820016898747209,2.564755813286796
"def GetCurrentKeySet(self): ""Return CurrentKeys with 'darwin' modifications."" result = self.GetKeySet(self.CurrentKeys()) if sys.platform == ""darwin"": # macOS (OS X) Tk variants do not support the ""Alt"" # keyboard modifier. Replace it with ""Option"". # TODO (Ned?): the ""Option"" modifier does not work properly # for Cocoa Tk and XQuartz Tk so we should not use it # in the default 'OSX' keyset. for k, v in result.items(): v2 = [x.replace(""<Alt-"", ""<Option-"") for x in v] <mask>: result[k] = v2 return result",False,if v != v2 :,if v2 :,0.8820016898747209,0.0
"def _load_testfile(filename, package, module_relative): if module_relative: package = _normalize_module(package, 3) filename = _module_relative_path(package, filename) if hasattr(package, ""__loader__""): <mask>: file_contents = package.__loader__.get_data(filename) # get_data() opens files as 'rb', so one must do the equivalent # conversion as universal newlines would do. return file_contents.replace(os.linesep, ""\n""), filename return open(filename).read(), filename",True,"if hasattr ( package . __loader__ , ""get_data"" ) :","if hasattr ( package . __loader__ , ""get_data"" ) :",0.8820016898747209,100.00000000000004
"def iter_from_X_lengths(X, lengths): if lengths is None: yield 0, len(X) else: n_samples = X.shape[0] end = np.cumsum(lengths).astype(np.int32) start = end - lengths <mask>: raise ValueError( ""more than {:d} samples in lengths array {!s}"".format( n_samples, lengths ) ) for i in range(len(lengths)): yield start[i], end[i]",False,if end [ - 1 ] > n_samples :,if len ( start ) > n_samples :,0.8820016898747209,37.20090803840517
"def change_sel(self): """"""Change the view's selections."""""" if self.alter_select and len(self.sels) > 0: <mask>: self.view.show(self.sels[0]) self.view.sel().clear() self.view.sel().add_all(self.sels)",False,if self . multi_select is False :,if self . view . sel ( ) . is_visible ( ) :,0.8820016898747209,11.633270842295033
"def cb_syncthing_device_data_changed( self, daemon, nid, address, client_version, inbps, outbps, inbytes, outbytes ): if nid in self.devices: # Should be always device = self.devices[nid] # Update strings device[""address""] = address <mask>: device[""version""] = client_version # Update rates device[""inbps""] = ""%s/s (%s)"" % (sizeof_fmt(inbps), sizeof_fmt(inbytes)) device[""outbps""] = ""%s/s (%s)"" % (sizeof_fmt(outbps), sizeof_fmt(outbytes))",False,"if client_version not in ( ""?"" , None ) :",if client_version :,0.8820016898747209,11.688396478408103
"def then(self, matches, when_response, context): if is_iterable(when_response): ret = [] when_response = list(when_response) for match in when_response: if match not in matches: <mask>: match.name = self.match_name matches.append(match) ret.append(match) return ret if self.match_name: when_response.name = self.match_name if when_response not in matches: matches.append(when_response) return when_response",True,if self . match_name :,if self . match_name :,0.8820016898747209,100.00000000000004
"def __update_parents(self, fileobj, path, delta): """"""Update all parent atoms with the new size."""""" if delta == 0: return for atom in path: fileobj.seek(atom.offset) size = cdata.uint_be(fileobj.read(4)) <mask>: # 64bit # skip name (4B) and read size (8B) size = cdata.ulonglong_be(fileobj.read(12)[4:]) fileobj.seek(atom.offset + 8) fileobj.write(cdata.to_ulonglong_be(size + delta)) else: # 32bit fileobj.seek(atom.offset) fileobj.write(cdata.to_uint_be(size + delta))",False,if size == 1 :,if size == 0 :,0.8820016898747209,53.7284965911771
"def _fields_to_index(cls): fields = [] for field in cls._meta.sorted_fields: if field.primary_key: continue requires_index = any( (field.index, field.unique, isinstance(field, ForeignKeyField)) ) <mask>: fields.append(field) return fields",True,if requires_index :,if requires_index :,0.8820016898747209,100.00000000000004
"def __init__(self, value): """"""Initialize the integer to the given value."""""" self._mpz_p = new_mpz() self._initialized = False if isinstance(value, float): raise ValueError(""A floating point type is not a natural number"") self._initialized = True if isinstance(value, (int, long)): _gmp.mpz_init(self._mpz_p) result = _gmp.gmp_sscanf(tobytes(str(value)), b(""%Zd""), self._mpz_p) <mask>: raise ValueError(""Error converting '%d'"" % value) else: _gmp.mpz_init_set(self._mpz_p, value._mpz_p)",True,if result != 1 :,if result != 1 :,0.8820016898747209,100.00000000000004
"def decode(cls, data): while data: length, format_type, control_flags, sequence, pid = unpack( cls.Header.PACK, data[: cls.Header.LEN] ) <mask>: raise NetLinkError(""Buffer underrun"") yield cls.format( format_type, control_flags, sequence, pid, data[cls.Header.LEN : length] ) data = data[length:]",False,if len ( data ) < length :,if length == 0 :,0.8820016898747209,7.654112967106117
"def __post_init__(self): if self._node_id is not None: <mask>: raise ValueError( ""invalid node_id: {}"".format(hexlify(self._node_id).decode()) ) if self.udp_port is not None and not 1 <= self.udp_port <= 65535: raise ValueError(""invalid udp port"") if self.tcp_port is not None and not 1 <= self.tcp_port <= 65535: raise ValueError(""invalid tcp port"") if not is_valid_public_ipv4(self.address, self.allow_localhost): raise ValueError(f""invalid ip address: '{self.address}'"")",False,if not len ( self . _node_id ) == constants . HASH_LENGTH :,"if not is_valid_node_id ( self . _node_id , self . allow_localhost ) :",0.8820016898747209,29.065151007971206
"def orderUp(self, items): sel = [] # new selection undoinfo = [] for bid, lid in items: if isinstance(lid, int): undoinfo.append(self.orderUpLineUndo(bid, lid)) sel.append((bid, lid - 1)) <mask>: undoinfo.append(self.orderUpBlockUndo(bid)) if bid == 0: return items else: sel.append((bid - 1, None)) self.addUndo(undoinfo, ""Move Up"") return sel",False,elif lid is None :,"elif isinstance ( bid , int ) :",0.8820016898747209,6.567274736060395
"def filter_data(self, min_len, max_len): logging.info(f""filtering data, min len: {min_len}, max len: {max_len}"") initial_len = len(self.src) filtered_src = [] filtered_tgt = [] for src, tgt in zip(self.src, self.tgt): <mask>: filtered_src.append(src) filtered_tgt.append(tgt) self.src = filtered_src self.tgt = filtered_tgt filtered_len = len(self.src) logging.info(f""pairs before: {initial_len}, after: {filtered_len}"")",False,if min_len <= len ( src ) <= max_len and min_len <= len ( tgt ) <= max_len :,if src < initial_len and tgt < initial_len :,0.8820016898747209,5.635118162117621
"def layer_pretrained(self, net, args, options): model = getattr(torchvision.models, args[0])(pretrained=True) model.train(True) if options.layer: layers = list(model.children())[: options.layer] <mask>: layers[-1] = nn.Sequential(*layers[-1][: options.sublayer]) else: layers = [model] print(""List of pretrained layers:"", layers) raise ValidationException( ""layer=-1 required for pretrained, sublayer=-1 optional. Layers outputted above."" ) return nn.Sequential(*layers)",True,if options . sublayer :,if options . sublayer :,0.8820016898747209,100.00000000000004
"def deleteCalendar(users): calendarId = normalizeCalendarId(sys.argv[5]) for user in users: user, cal = buildCalendarGAPIObject(user) <mask>: continue gapi.call(cal.calendarList(), ""delete"", soft_errors=True, calendarId=calendarId)",True,if not cal :,if not cal :,0.8820016898747209,100.00000000000004
"def iter_modules(self, by_clients=False, clients_filter=None): """"""iterate over all modules"""""" clients = None if by_clients: clients = self.get_clients(clients_filter) <mask>: return self._refresh_modules() for module_name in self.modules: try: module = self.get_module(module_name) except PupyModuleDisabled: continue if clients is not None: for client in clients: if module.is_compatible_with(client): yield module break else: yield module",False,if not clients :,if clients is None :,0.8820016898747209,14.058533129758727
"def update_me(self): try: while 1: line = self.queue.get_nowait() <mask>: self.delete(1.0, tk.END) else: self.insert(tk.END, str(line)) self.see(tk.END) self.update_idletasks() except queue.Empty: pass self.after(100, self.update_me)",False,if line is None :,"if line == """" :",0.8820016898747209,14.535768424205482
"def request_power_state(self, state, force=False): if self.current_state != state or force: <mask>: self.request_in_progress = True logging.info(""Requesting %s"" % state) cb = PowerManager.Callback(self, state) rets = self.parent.Plugins.run( ""on_power_state_change_requested"", self, state, cb ) cb.num_cb = len(rets) cb.check() else: logging.info(""Another request in progress"")",False,if not self . request_in_progress :,"if self . parent . Plugin . get_state ( ""power_state_change_requested"" ) :",0.8820016898747209,4.546308713404575
"def __getitem__(self, idx): super(BatchDataset, self).__getitem__(idx) maxidx = len(self.dataset) samples = [] for i in range(0, self.batchsize): j = idx * self.batchsize + i if j >= maxidx: break j = self.perm(j, maxidx) sample = self.dataset[j] <mask>: samples.append(sample) samples = self.makebatch(samples) return samples",False,if self . filter ( sample ) :,if sample is not None :,0.8820016898747209,7.654112967106117
"def __call__(self, request, *args, **kwargs): template_vars = {} for form_name, form_class in self.forms.iteritems(): <mask>: template_vars[form_name] = form_class(request) else: template_vars[form_name] = None if request.method == ""POST"": action = self.find_post_handler_action(request) form = self.handlers[action](request, data=request.POST, files=request.FILES) template_vars.update(form.dispatch(action, request, *args, **kwargs)) return self.GET(template_vars, request, *args, **kwargs)",False,"if form_class . must_display ( request , * args , ** kwargs ) :",if form_class :,0.8820016898747209,4.299920764667028
"def on_show_all(self, widget, another): if widget.get_active(): <mask>: self.treeview.update_items(all=True, comment=True) else: self.treeview.update_items(all=True) else: if another.get_active(): self.treeview.update_items(comment=True) else: self.treeview.update_items()",True,if another . get_active ( ) :,if another . get_active ( ) :,0.8820016898747209,100.00000000000004
"def close(self): if self._closed: return self._closed = True for proto in self._pipes.values(): if proto is None: continue proto.pipe.close() if ( self._proc is not None and # has the child process finished? self._returncode is None and # the child process has finished, but the # transport hasn't been notified yet? self._proc.poll() is None ): <mask>: logger.warning(""Close running child process: kill %r"", self) try: self._proc.kill() except ProcessLookupError: pass",False,if self . _loop . get_debug ( ) :,if self . _debug :,0.8820016898747209,22.230037854975045
"def runTest(self): self.poco(text=""wait UI"").click() bomb_count = 0 while True: blue_fish = self.poco(""fish_emitter"").child(""blue"") yellow_fish = self.poco(""fish_emitter"").child(""yellow"") bomb = self.poco(""fish_emitter"").child(""bomb"") fish = self.poco.wait_for_any([blue_fish, yellow_fish, bomb]) if fish is bomb: bomb_count += 1 <mask>: return else: fish.click() time.sleep(2.5)",False,if bomb_count > 3 :,if bomb_count == 10 :,0.8820016898747209,36.55552228545123
"def load_managers(*, loop, only): managers = {} for key in DB_CLASSES: <mask>: continue params = DB_DEFAULTS.get(key) or {} params.update(DB_OVERRIDES.get(key) or {}) database = DB_CLASSES[key](**params) managers[key] = peewee_async.Manager(database, loop=loop) return managers",False,if only and key not in only :,if only :,0.8820016898747209,0.0
"def links_extracted(self, request, links): for link in links: <mask>: r = self._create_request(link.url) r.meta[b""depth""] = request.meta[b""depth""] + 1 self.schedule(r, self._get_score(r.meta[b""depth""])) link.meta[b""state""] = States.QUEUED",False,"if link . meta [ b""state"" ] == States . NOT_CRAWLED :","if link . meta [ b""state"" ] == States . QUEUED :",0.8820016898747209,77.32050252989006
"def find_worktree_git_dir(dotgit): """"""Search for a gitdir for this worktree."""""" try: statbuf = os.stat(dotgit) except OSError: return None if not stat.S_ISREG(statbuf.st_mode): return None try: lines = open(dotgit, ""r"").readlines() for key, value in [line.strip().split("": "") for line in lines]: <mask>: return value except ValueError: pass return None",True,"if key == ""gitdir"" :","if key == ""gitdir"" :",0.8820016898747209,100.00000000000004
"def _is_static_shape(self, shape): if shape is None or not isinstance(shape, list): return False for dim_value in shape: if not isinstance(dim_value, int): return False <mask>: raise Exception(""Negative dimension is illegal: %d"" % dim_value) return True",True,if dim_value < 0 :,if dim_value < 0 :,0.8820016898747209,100.00000000000004
"def init_logger(): configured_loggers = [log_config.get(""root"", {})] + [ logger for logger in log_config.get(""loggers"", {}).values() ] used_handlers = { handler for log in configured_loggers for handler in log.get(""handlers"", []) } for handler_id, handler in list(log_config[""handlers""].items()): if handler_id not in used_handlers: del log_config[""handlers""][handler_id] <mask>: filename = handler[""filename""] logfile_path = Path(filename).expanduser().resolve() handler[""filename""] = str(logfile_path) logging.config.dictConfig(log_config)",False,"elif ""filename"" in handler . keys ( ) :","if handler [ ""filename"" ] :",0.8820016898747209,16.0529461904344
"def __call__(self): dmin, dmax = self.viewlim_to_dt() ymin = self.base.le(dmin.year) ymax = self.base.ge(dmax.year) ticks = [dmin.replace(year=ymin, **self.replaced)] while 1: dt = ticks[-1] <mask>: return date2num(ticks) year = dt.year + self.base.get_base() ticks.append(dt.replace(year=year, **self.replaced))",False,if dt . year >= ymax :,if dt . year == ymin :,0.8820016898747209,38.260294162784454
"def taiga(request, trigger_id, key): signature = request.META.get(""HTTP_X_TAIGA_WEBHOOK_SIGNATURE"") # check that the data are ok with the provided signature if verify_signature(request._request.body, key, signature): data = data_filter(trigger_id, **request.data) status = save_data(trigger_id, data) return ( Response({""message"": ""Success""}) <mask>: else Response({""message"": ""Failed!""}) ) Response({""message"": ""Bad request""})",True,if status,if status,0.8820016898747209,0.0
"def ParseResponses( self, knowledge_base: rdf_client.KnowledgeBase, responses: Iterable[rdfvalue.RDFValue], ) -> Iterator[rdf_client.User]: for response in responses: <mask>: raise TypeError(f""Unexpected response type: `{type(response)}`"") # TODO: `st_mode` has to be an `int`, not `StatMode`. if stat.S_ISDIR(int(response.st_mode)): homedir = response.pathspec.path username = os.path.basename(homedir) if username not in self._ignore_users: yield rdf_client.User(username=username, homedir=homedir)",False,"if not isinstance ( response , rdf_client_fs . StatEntry ) :","if not isinstance ( response , rdfvalue . RDFValue ) :",0.8820016898747209,37.178099888227045
"def _iter_lines(path=path, response=response, max_next=options.http_max_next): path.responses = [] n = 0 while response: path.responses.append(response) yield from response.iter_lines(decode_unicode=True) src = response.links.get(""next"", {}).get(""url"", None) if not src: break n += 1 <mask>: vd.warning(f""stopping at max {max_next} pages"") break vd.status(f""fetching next page from {src}"") response = requests.get(src, stream=True)",True,if n > max_next :,if n > max_next :,0.8820016898747209,100.00000000000004
"def __enter__(self): """"""Open a file and read it."""""" if self.code is None: LOGGER.info(""File is reading: %s"", self.path) <mask>: self._file = open(self.path, encoding=""utf-8"") else: self._file = open(self.path, ""rU"") self.code = self._file.read() return self",False,"if sys . version_info >= ( 3 , ) :",if PY2 :,0.8820016898747209,0.0
"def facts_for_oauthclients(self, namespace): """"""Gathers facts for oauthclients used with logging"""""" self.default_keys_for(""oauthclients"") a_list = self.oc_command( ""get"", ""oauthclients"", namespace=namespace, add_options=[""-l"", LOGGING_SELECTOR] ) if len(a_list[""items""]) == 0: return for item in a_list[""items""]: name = item[""metadata""][""name""] comp = self.comp(name) <mask>: result = dict(redirectURIs=item[""redirectURIs""]) self.add_facts_for(comp, ""oauthclients"", name, result)",False,if comp is not None :,if comp :,0.8820016898747209,0.0
"def get(self, k): with self._lock: <mask>: self._data1[k] = self._data2[k] del self._data2[k] return self._data1.get(k)",False,if k not in self . _data1 and k in self . _data2 :,if k in self . _data1 :,0.8820016898747209,28.150351271320314
"def _parseparam(s): plist = [] while s[:1] == "";"": s = s[1:] end = s.find("";"") while end > 0 and (s.count('""', 0, end) - s.count('\\""', 0, end)) % 2: end = s.find("";"", end + 1) <mask>: end = len(s) f = s[:end] if ""="" in f: i = f.index(""="") f = f[:i].strip().lower() + ""="" + f[i + 1 :].strip() plist.append(f.strip()) s = s[end:] return plist",False,if end < 0 :,if end == - 1 :,0.8820016898747209,14.535768424205482
"def __init__(self, **params): if ""length"" in params: <mask>: raise ValueError(""Supply either length or start and end to Player not both"") params[""start""] = 0 params[""end""] = params.pop(""length"") - 1 elif params.get(""start"", 0) > 0 and not ""value"" in params: params[""value""] = params[""start""] super(Player, self).__init__(**params)",False,"if ""start"" in params or ""end"" in params :","if params . get ( ""length"" ) != 1 :",0.8820016898747209,4.6192151051305474
"def libcxx_define(settings): compiler = _base_compiler(settings) libcxx = settings.get_safe(""compiler.libcxx"") if not compiler or not libcxx: return """" if str(compiler) in GCC_LIKE: if str(libcxx) == ""libstdc++"": return ""_GLIBCXX_USE_CXX11_ABI=0"" <mask>: return ""_GLIBCXX_USE_CXX11_ABI=1"" return """"",False,"elif str ( libcxx ) == ""libstdc++11"" :","elif str ( libcxx ) == ""libstdc-1"" :",0.8820016898747209,56.48427586489798
"def _get_sort_map(tags): """"""See TAG_TO_SORT"""""" tts = {} for name, tag in tags.items(): if tag.has_sort: if tag.user: tts[name] = ""%ssort"" % name <mask>: tts[""~%s"" % name] = ""~%ssort"" % name return tts",False,if tag . internal :,elif tag . user . is_staff :,0.8820016898747209,10.552670315936318
"def quiet_f(*args): vars = {arg_name: Real(arg) for arg_name, arg in zip(arg_names, args)} value = dynamic_scoping(quiet_expr.evaluate, vars, evaluation) if expect_list: if value.has_form(""List"", None): value = [extract_pyreal(item) for item in value.leaves] if any(item is None for item in value): return None return value else: return None else: value = extract_pyreal(value) <mask>: return None return value",False,if value is None or isinf ( value ) or isnan ( value ) :,if value is None :,0.8820016898747209,9.569649651041097
"def on_action_chosen(self, id, action, mark_changed=True): before = self.set_action(self.current, id, action) if mark_changed: <mask>: # TODO: Maybe better comparison self.undo.append(UndoRedo(id, before, action)) self.builder.get_object(""btUndo"").set_sensitive(True) self.on_profile_modified() else: self.on_profile_modified(update_ui=False) return before",False,if before . to_string ( ) != action . to_string ( ) :,if self . undo :,0.8820016898747209,1.044177559991939
"def setUp(self): super(OperaterTest, self).setUp() if is_cli: import clr self.load_iron_python_test() <mask>: clr.AddReference(""System.Drawing.Primitives"") else: clr.AddReference(""System.Drawing"")",False,if is_netcoreapp :,if self . _is_primitives :,0.8820016898747209,13.134549472120788
"def field_to_field_type(field): field_type = field[""type""] if isinstance(field_type, dict): field_type = field_type[""type""] if isinstance(field_type, list): field_type_length = len(field_type) if field_type_length == 0: raise Exception(""Zero-length type list encountered, invalid CWL?"") <mask>: field_type = field_type[0] return field_type",False,elif len ( field_type ) == 1 :,if field_type_length == 1 :,0.8820016898747209,34.3763879968285
"def _flatten(*args): ahs = set() if len(args) > 0: for item in args: <mask>: ahs.add(item) elif type(item) in (list, tuple, dict, set): for ah in item: if type(ah) is not ActionHandle: # pragma:nocover raise ActionManagerError(""Bad argument type %s"" % str(ah)) ahs.add(ah) else: # pragma:nocover raise ActionManagerError(""Bad argument type %s"" % str(item)) return ahs",True,if type ( item ) is ActionHandle :,if type ( item ) is ActionHandle :,0.8820016898747209,100.00000000000004
"def _Determine_Do(self): self.applicable = 1 configTokens = black.configure.items[""configTokens""].Get() buildFlavour = black.configure.items[""buildFlavour""].Get() if buildFlavour == ""full"": self.value = False else: self.value = True for opt, optarg in self.chosenOptions: <mask>: if not self.value: configTokens.append(""tests"") self.value = True elif opt == ""--without-tests"": if self.value: configTokens.append(""notests"") self.value = False self.determined = 1",False,"if opt == ""--with-tests"" :","if opt == ""--tests"" :",0.8820016898747209,59.4603557501361
"def title_by_index(self, trans, index, context): d_type = self.get_datatype(trans, context) for i, (composite_name, composite_file) in enumerate(d_type.writable_files.items()): <mask>: rval = composite_name if composite_file.description: rval = ""{} ({})"".format(rval, composite_file.description) if composite_file.optional: rval = ""%s [optional]"" % rval return rval if index < self.get_file_count(trans, context): return ""Extra primary file"" return None",True,if i == index :,if i == index :,0.8820016898747209,100.00000000000004
"def func(x, y): try: <mask>: z = x + 2 * math.sin(y) return z ** 2 elif x == y: return 4 else: return 2 ** 3 except ValueError: foo = 0 for i in range(4): foo += i return foo except TypeError: return 42 else: return 33 finally: print(""finished"")",False,if x > y :,if x < y :,0.8820016898747209,30.213753973567677
"def test_suite(): suite = unittest.TestSuite() for fn in os.listdir(here): <mask>: modname = ""distutils.tests."" + fn[:-3] __import__(modname) module = sys.modules[modname] suite.addTest(module.test_suite()) return suite",False,"if fn . startswith ( ""test"" ) and fn . endswith ( "".py"" ) :","if fn . endswith ( "".py"" ) :",0.8820016898747209,42.67466931141059
"def check_stack_names(self, frame, expected): names = [] while frame: name = frame.f_code.co_name # Stop checking frames when we get to our test helper. <mask>: break names.append(name) frame = frame.f_back self.assertEqual(names, expected)",False,"if name . startswith ( ""check_"" ) or name . startswith ( ""call_"" ) :",if name in expected :,0.8820016898747209,0.9637628094253924
"def leave(self, reason=None): try: if self.id.startswith(""C""): log.info(""Leaving channel %s (%s)"", self, self.id) self._bot.api_call(""conversations.leave"", data={""channel"": self.id}) else: log.info(""Leaving group %s (%s)"", self, self.id) self._bot.api_call(""conversations.leave"", data={""channel"": self.id}) except SlackAPIResponseError as e: <mask>: raise RoomError(f""Unable to leave channel. {USER_IS_BOT_HELPTEXT}"") else: raise RoomError(e) self._id = None",False,"if e . error == ""user_is_bot"" :","if e . response [ ""Error"" ] == ""Error"" :",0.8820016898747209,18.69300079996002
"def ident(self): value = self._ident if value is False: value = None # XXX: how will this interact with orig_prefix ? # not exposing attrs for now if orig_prefix is set. <mask>: wrapped = self.wrapped ident = getattr(wrapped, ""ident"", None) if ident is not None: value = self._wrap_hash(ident) self._ident = value return value",False,if not self . orig_prefix :,if self . wrapped is not None :,0.8820016898747209,13.888095170058955
"def is_ac_power_connected(): for power_source_path in Path(""/sys/class/power_supply/"").iterdir(): try: with open(power_source_path / ""type"", ""r"") as f: <mask>: continue with open(power_source_path / ""online"", ""r"") as f: if f.read(1) == ""1"": return True except IOError: continue return False",False,"if f . read ( ) . strip ( ) != ""Mains"" :","if f . read ( 1 ) == ""1"" :",0.8820016898747209,30.215132342213096
"def _get_pending_by_app_token(self, app_token): result = [] with self._pending_lock: self._remove_stale_pending() for data in self._pending_decisions: <mask>: result.append(data) return result",False,if data . app_token == app_token :,"if data [ ""app_token"" ] == app_token :",0.8820016898747209,44.80304273880272
"def do_create(specific_tables=None, base=Base): engine = get_engine() try: <mask>: logger.info( ""Initializing only a subset of tables as requested: {}"".format( specific_tables ) ) base.metadata.create_all(engine, tables=specific_tables) else: base.metadata.create_all(engine) except Exception as err: raise Exception(""could not create/re-create DB tables - exception: "" + str(err))",True,if specific_tables :,if specific_tables :,0.8820016898747209,100.00000000000004
"def __setitem__(self, ndx, val): # # Get the expression data object # exprdata = None if ndx in self._data: exprdata = self._data[ndx] else: _ndx = normalize_index(ndx) <mask>: exprdata = self._data[_ndx] if exprdata is None: raise KeyError( ""Cannot set the value of Expression '%s' with "" ""invalid index '%s'"" % (self.cname(True), str(ndx)) ) # # Set the value # exprdata.set_value(val)",True,if _ndx in self . _data :,if _ndx in self . _data :,0.8820016898747209,100.00000000000004
"def write(self, *bits): for bit in bits: <mask>: self.bytestream.append(0) byte = self.bytestream[self.bytenum] if self.bitnum == 8: if self.bytenum == len(self.bytestream) - 1: byte = 0 self.bytestream += bytes([byte]) self.bytenum += 1 self.bitnum = 0 mask = 2 ** self.bitnum if bit: byte |= mask else: byte &= ~mask self.bytestream[self.bytenum] = byte self.bitnum += 1",False,if not self . bytestream :,if self . bytenum == 0 :,0.8820016898747209,13.134549472120788
"def terminate_subprocess(proc, timeout=0.1, log=None): if proc.poll() is None: <mask>: log.info(""Sending SIGTERM to %r"", proc) proc.terminate() timeout_time = time.time() + timeout while proc.poll() is None and time.time() < timeout_time: time.sleep(0.02) if proc.poll() is None: if log: log.info(""Sending SIGKILL to %r"", proc) proc.kill() return proc.returncode",True,if log :,if log :,0.8820016898747209,0.0
"def mkpanel(color, rows, cols, tly, tlx): win = curses.newwin(rows, cols, tly, tlx) pan = panel.new_panel(win) if curses.has_colors(): <mask>: fg = curses.COLOR_WHITE else: fg = curses.COLOR_BLACK bg = color curses.init_pair(color, fg, bg) win.bkgdset(ord("" ""), curses.color_pair(color)) else: win.bkgdset(ord("" ""), curses.A_BOLD) return pan",False,if color == curses . COLOR_BLUE :,if color == curses . COLOR_WHITE :,0.8820016898747209,78.25422900366438
"def all_words(filename): start_char = True for c in characters(filename): if start_char == True: word = """" <mask>: # We found the start of a word word = c.lower() start_char = False else: pass else: if c.isalnum(): word += c.lower() else: # We found end of word, emit it start_char = True yield word",True,if c . isalnum ( ) :,if c . isalnum ( ) :,0.8820016898747209,100.00000000000004
"def get_tf_weights_as_numpy(path=""./ckpt/aeslc/model.ckpt-32000"") -> Dict: init_vars = tf.train.list_variables(path) tf_weights = {} ignore_name = [""Adafactor"", ""global_step""] for name, shape in tqdm(init_vars, desc=""converting tf checkpoint to dict""): skip_key = any([pat in name for pat in ignore_name]) <mask>: continue array = tf.train.load_variable(path, name) tf_weights[name] = array return tf_weights",True,if skip_key :,if skip_key :,0.8820016898747209,100.00000000000004
"def app(scope, receive, send): while True: message = await receive() if message[""type""] == ""websocket.connect"": await send({""type"": ""websocket.accept""}) <mask>: pass elif message[""type""] == ""websocket.disconnect"": break",False,"elif message [ ""type"" ] == ""websocket.receive"" :","elif message [ ""type"" ] == ""websocket.disconnect"" :",0.8820016898747209,82.42367502646057
"def autoload(self): if self._app.config.THEME == ""auto"": if sys.platform == ""darwin"": <mask>: theme = DARK else: theme = LIGHT else: theme = self.guess_system_theme() if theme == Dark: theme = MacOSDark else: # user settings have highest priority theme = self._app.config.THEME self.load_theme(theme)",False,if get_osx_theme ( ) == 1 :,"if self . _app . config . THEME == ""dark"" :",0.8820016898747209,6.608973813188645
"def example_reading_spec(self): data_fields = {""targets"": tf.VarLenFeature(tf.int64)} <mask>: data_fields[""inputs""] = tf.VarLenFeature(tf.int64) if self.packed_length: if self.has_inputs: data_fields[""inputs_segmentation""] = tf.VarLenFeature(tf.int64) data_fields[""inputs_position""] = tf.VarLenFeature(tf.int64) data_fields[""targets_segmentation""] = tf.VarLenFeature(tf.int64) data_fields[""targets_position""] = tf.VarLenFeature(tf.int64) data_items_to_decoders = None return (data_fields, data_items_to_decoders)",True,if self . has_inputs :,if self . has_inputs :,0.8820016898747209,100.00000000000004
"def _prepare_travel_graph(self): for op in self.op_dict.values(): op.const = False if op.node.op in [""Const"", ""Placeholder""]: op.resolved = True <mask>: op.const = True else: op.resolved = False",False,"if op . node . op == ""Const"" :","elif op . node . op in [ ""Const"" , ""Placeholder"" ] :",0.8820016898747209,29.48993986902436
"def get_filestream_file_items(self): data = {} fs_file_updates = self.get_filestream_file_updates() for k, v in six.iteritems(fs_file_updates): l = [] for d in v: offset = d.get(""offset"") content = d.get(""content"") assert offset is not None assert content is not None assert offset == 0 or offset == len(l), (k, v, l, d) <mask>: l = [] l.extend(map(json.loads, content)) data[k] = l return data",False,if not offset :,if not l :,0.8820016898747209,35.35533905932737
"def _rewrite_exprs(self, table, what): from ibis.expr.analysis import substitute_parents what = util.promote_list(what) all_exprs = [] for expr in what: <mask>: all_exprs.extend(expr.exprs()) else: bound_expr = ir.bind_expr(table, expr) all_exprs.append(bound_expr) return [substitute_parents(x, past_projection=False) for x in all_exprs]",False,"if isinstance ( expr , ir . ExprList ) :","if isinstance ( expr , ibis . expr . Expr ) :",0.8820016898747209,37.70063804549471
"def _group_by_commit_and_time(self, hits): result = {} for hit in hits: source_hit = hit[""_source""] key = ""%s_%s"" % (source_hit[""commit_info""][""id""], source_hit[""datetime""]) benchmark = self._benchmark_from_es_record(source_hit) <mask>: result[key][""benchmarks""].append(benchmark) else: run_info = self._run_info_from_es_record(source_hit) run_info[""benchmarks""] = [benchmark] result[key] = run_info return result",False,if key in result :,if benchmark :,0.8820016898747209,0.0
"def _build_index(self): self._index = {} for start_char, sorted_offsets in self._offsets.items(): self._index[start_char] = {} for i, offset in enumerate(sorted_offsets.get_offsets()): identifier = sorted_offsets.get_identifier_by_offset(offset) <mask>: self._index[start_char][identifier[0 : self.index_depth]] = i",False,if identifier [ 0 : self . index_depth ] not in self . _index [ start_char ] :,if identifier :,0.8820016898747209,0.0
"def scan_resource_conf(self, conf): if ""properties"" in conf: <mask>: if ""exp"" in conf[""properties""][""attributes""]: if conf[""properties""][""attributes""][""exp""]: return CheckResult.PASSED return CheckResult.FAILED",True,"if ""attributes"" in conf [ ""properties"" ] :","if ""attributes"" in conf [ ""properties"" ] :",0.8820016898747209,100.00000000000004
"def _PatchArtifact(self, artifact: rdf_artifacts.Artifact) -> rdf_artifacts.Artifact: """"""Patches artifact to not contain byte-string source attributes."""""" patched = False for source in artifact.sources: attributes = source.attributes.ToDict() unicode_attributes = compatibility.UnicodeJson(attributes) <mask>: source.attributes = unicode_attributes patched = True if patched: self.DeleteArtifact(str(artifact.name)) self.WriteArtifact(artifact) return artifact",False,if attributes != unicode_attributes :,if unicode_attributes :,0.8820016898747209,38.80684294761701
"def edit_file(self, filename): import subprocess editor = self.get_editor() if self.env: environ = os.environ.copy() environ.update(self.env) else: environ = None try: c = subprocess.Popen('%s ""%s""' % (editor, filename), env=environ, shell=True) exit_code = c.wait() <mask>: raise ClickException(""%s: Editing failed!"" % editor) except OSError as e: raise ClickException(""%s: Editing failed: %s"" % (editor, e))",True,if exit_code != 0 :,if exit_code != 0 :,0.8820016898747209,100.00000000000004
"def findControlPointsInMesh(glyph, va, subsegments): controlPointIndices = np.zeros((len(va), 1)) index = 0 for i, c in enumerate(subsegments): segmentCount = len(glyph.contours[i].segments) - 1 for j, s in enumerate(c): <mask>: if glyph.contours[i].segments[j].type == ""line"": controlPointIndices[index] = 1 index += s[1] return controlPointIndices",False,if j < segmentCount :,if s [ 0 ] == va [ j ] :,0.8820016898747209,4.456882760699063
"def to_representation(self, value): old_social_string_fields = [""twitter"", ""github"", ""linkedIn""] request = self.context.get(""request"") show_old_format = ( request and is_deprecated(request.version, self.min_version) and request.method == ""GET"" ) if show_old_format: social = value.copy() for key in old_social_string_fields: <mask>: social[key] = value[key][0] elif social.get(key) == []: social[key] = """" value = social return super(SocialField, self).to_representation(value)",False,if social . get ( key ) :,if value . get ( key ) :,0.8820016898747209,70.71067811865478
"def iter_raw_frames(path, packet_sizes, ctx): with open(path, ""rb"") as f: for i, size in enumerate(packet_sizes): packet = Packet(size) read_size = f.readinto(packet) assert size assert read_size == size if not read_size: break for frame in ctx.decode(packet): yield frame while True: try: frames = ctx.decode(None) except EOFError: break for frame in frames: yield frame <mask>: break",False,if not frames :,if i == len ( packet_sizes ) - 1 :,0.8820016898747209,3.673526562988939
"def get_shadows_zip(filename): import zipfile shadow_pkgs = set() with zipfile.ZipFile(filename) as lib_zip: already_test = [] for fname in lib_zip.namelist(): pname, fname = os.path.split(fname) if fname or (pname and fname): continue if pname not in already_test and ""/"" not in pname: already_test.append(pname) <mask>: shadow_pkgs.add(pname) return shadow_pkgs",False,if is_shadowing ( pname ) :,if pname not in shadow_pkgs :,0.8820016898747209,7.809849842300637
"def metrics_to_scalars(self, metrics): new_metrics = {} for k, v in metrics.items(): if isinstance(v, torch.Tensor): v = v.item() <mask>: v = self.metrics_to_scalars(v) new_metrics[k] = v return new_metrics",False,"if isinstance ( v , dict ) :","elif isinstance ( v , ( list , tuple ) ) :",0.8820016898747209,25.211936184349828
"def insert_resets(f): newsync = dict() for k, v in f.sync.items(): <mask>: newsync[k] = insert_reset(ResetSignal(k), v) else: newsync[k] = v f.sync = newsync",False,if f . clock_domains [ k ] . rst is not None :,"if isinstance ( v , Signal ) :",0.8820016898747209,2.7376474102577792
"def get_attached_nodes(self, external_account): for node in self.get_nodes_with_oauth_grants(external_account): if node is None: continue node_settings = node.get_addon(self.oauth_provider.short_name) <mask>: continue if node_settings.external_account == external_account: yield node",True,if node_settings is None :,if node_settings is None :,0.8820016898747209,100.00000000000004
"def visitIf(self, node, scope): for test, body in node.tests: if isinstance(test, ast.Const): if type(test.value) in self._const_types: <mask>: continue self.visit(test, scope) self.visit(body, scope) if node.else_: self.visit(node.else_, scope)",False,if not test . value :,if not test . value . is_const :,0.8820016898747209,41.11336169005198
"def flatten(self): # this is similar to fill_messages except it uses a list instead # of a queue to place the messages in. result = [] channel = await self.messageable._get_channel() self.channel = channel while self._get_retrieve(): data = await self._retrieve_messages(self.retrieve) if len(data) < 100: self.limit = 0 # terminate the infinite loop <mask>: data = reversed(data) if self._filter: data = filter(self._filter, data) for element in data: result.append(self.state.create_message(channel=channel, data=element)) return result",False,if self . reverse :,if self . reversed :,0.8820016898747209,42.72870063962342
"def compute(self, x, y=None, targets=None): if targets is None: targets = self.out_params in_params = list(self.in_x) if len(in_params) == 1: args = [x] else: args = list(zip(*x)) if y is None: pipe = self.pipe else: pipe = self.train_pipe <mask>: args.append(y) else: args += list(zip(*y)) in_params += self.in_y return self._compute(*args, pipe=pipe, param_names=in_params, targets=targets)",False,if len ( self . in_y ) == 1 :,if len ( in_params ) == 1 :,0.8820016898747209,42.99352740507477
"def _import_top_module(self, name): # scan sys.path looking for a location in the filesystem that contains # the module, or an Importer object that can import the module. for item in sys.path: <mask>: module = self.fs_imp.import_from_dir(item, name) else: module = item.import_top(name) if module: return module return None",False,"if isinstance ( item , _StringType ) :","if isinstance ( item , ( str , unicode ) ) :",0.8820016898747209,36.462858619364674
"def __getitem__(self, key, _get_mode=False): if not _get_mode: if isinstance(key, (int, long)): return self._list[key] elif isinstance(key, slice): return self.__class__(self._list[key]) ikey = key.lower() for k, v in self._list: <mask>: return v # micro optimization: if we are in get mode we will catch that # exception one stack level down so we can raise a standard # key error instead of our special one. if _get_mode: raise KeyError() raise BadRequestKeyError(key)",True,if k . lower ( ) == ikey :,if k . lower ( ) == ikey :,0.8820016898747209,100.00000000000004
"def execute(self, arbiter, props): watcher = self._get_watcher(arbiter, props.pop(""name"")) action = 0 for key, val in props.get(""options"", {}).items(): if key == ""hooks"": new_action = 0 for name, _val in val.items(): action = watcher.set_opt(""hooks.%s"" % name, _val) <mask>: new_action = 1 else: new_action = watcher.set_opt(key, val) if new_action == 1: action = 1 # trigger needed action return watcher.do_action(action)",False,if action == 1 :,"elif key == ""actions"" :",0.8820016898747209,12.22307556087252
"def OnBodyClick(self, event=None): try: c = self.c p = c.currentPosition() <mask>: self.OnActivateBody(event=event) g.doHook(""bodyclick2"", c=c, p=p, v=p, event=event) except: g.es_event_exception(""bodyclick"")",False,"if not g . doHook ( ""bodyclick1"" , c = c , p = p , v = p , event = event ) :","if g . doHook ( ""bodyclick1"" , c = c , p = p , v = p , event = event ) :",0.8820016898747209,93.20024073195327
"def _class_weights(spec: config.MetricsSpec) -> Optional[Dict[int, float]]: """"""Returns class weights associated with AggregationOptions at offset."""""" if spec.aggregate.HasField(""top_k_list""): <mask>: raise ValueError( ""class_weights are not supported when top_k_list used: "" ""spec={}"".format(spec) ) return None return dict(spec.aggregate.class_weights) or None",False,if spec . aggregate . class_weights :,"if spec . aggregate . HasField ( ""class_weights"" ) :",0.8820016898747209,36.6192636299943
"def _is_perf_file(file_path): f = get_file(file_path) for line in f: if line[0] == ""#"": continue r = event_regexp.search(line) <mask>: f.close() return True f.close() return False",True,if r :,if r :,0.8820016898747209,0.0
"def _get_before_insertion_node(self): if self._nodes_stack.is_empty(): return None line = self._nodes_stack.parsed_until_line + 1 node = self._new_module.get_last_leaf() while True: parent = node.parent <mask>: assert node.end_pos[0] <= line assert node.end_pos[1] == 0 or ""\n"" in self._prefix return node node = parent",False,"if parent . type in ( ""suite"" , ""file_input"" ) :",if parent is None :,0.8820016898747209,2.1448935777350973
"def PyJsHoisted_parseClassRanges_(this, arguments, var=var): var = Scope({u""this"": this, u""arguments"": arguments}, var) var.registers([u""res""]) pass if var.get(u""current"")(Js(u""]"")): return Js([]) else: var.put(u""res"", var.get(u""parseNonemptyClassRanges"")()) <mask>: var.get(u""bail"")(Js(u""nonEmptyClassRanges"")) return var.get(u""res"")",False,"if var . get ( u""res"" ) . neg ( ) :","if var . get ( u""bail"" ) :",0.8820016898747209,46.60402581158052
"def _recurse_children(self, offset): """"""Recurses thorugh the available children"""""" while offset < self.obj_offset + self.Length: item = obj.Object(""VerStruct"", offset=offset, vm=self.obj_vm, parent=self) <mask>: raise StopIteration( ""Could not recover a key for a child at offset {0}"".format( item.obj_offset ) ) yield item.get_key(), item.get_children() offset = self.offset_pad(offset + item.Length) raise StopIteration(""No children"")",False,if item . Length < 1 or item . get_key ( ) == None :,if item . get_key ( ) is None :,0.8820016898747209,37.13426189560737
"def _adapt_types(self, descr): names = [] adapted_types = [] for col in descr: names.append(col[0]) impala_typename = col[1] typename = udf._impala_to_ibis_type[impala_typename.lower()] <mask>: precision, scale = col[4:6] adapted_types.append(dt.Decimal(precision, scale)) else: adapted_types.append(typename) return names, adapted_types",False,"if typename == ""decimal"" :","if typename == ""Decimal"" :",0.8820016898747209,59.4603557501361
"def sniff(self, filename): try: <mask>: with tarfile.open(filename, ""r"") as temptar: for f in temptar: if not f.isfile(): continue if f.name.endswith("".fast5""): return True else: return False except Exception as e: log.warning(""%s, sniff Exception: %s"", self, e) return False",False,if filename and tarfile . is_tarfile ( filename ) :,if os . path . isfile ( filename ) :,0.8820016898747209,23.801761257033814
"def getValue(self): if getattr(self.object, ""type"", """") != ""CURVE"": return BezierSpline() evaluatedObject = getEvaluatedID(self.object) bSplines = evaluatedObject.data.splines if len(bSplines) > 0: spline = createSplineFromBlenderSpline(bSplines[0]) # Is None when the spline type is not supported. if spline is not None: <mask>: spline.transform(evaluatedObject.matrix_world) return spline return BezierSpline()",False,if self . useWorldSpace :,"if isinstance ( spline , BezierSpline ) :",0.8820016898747209,6.567274736060395
"def escape(text, newline=False): """"""Escape special html characters."""""" if isinstance(text, str): if ""&"" in text: text = text.replace(""&"", ""&amp;"") <mask>: text = text.replace("">"", ""&gt;"") if ""<"" in text: text = text.replace(""<"", ""&lt;"") if '""' in text: text = text.replace('""', ""&quot;"") if ""'"" in text: text = text.replace(""'"", ""&quot;"") if newline: if ""\n"" in text: text = text.replace(""\n"", ""<br>"") return text",True,"if "">"" in text :","if "">"" in text :",0.8820016898747209,100.00000000000004
"def _get_ilo_version(self): try: self._get_ilo2('<?xml version=""1.0""?><RIBCL VERSION=""2.0""></RIBCL>') except ResponseError as e: if hasattr(e, ""code""): <mask>: return 3 if e.code == 501: return 1 raise return 2",False,if e . code == 405 :,if e . code == 502 :,0.8820016898747209,70.71067811865478
"def convert_path(ctx, tpath): for points, code in tpath.iter_segments(): if code == Path.MOVETO: ctx.move_to(*points) elif code == Path.LINETO: ctx.line_to(*points) elif code == Path.CURVE3: ctx.curve_to( points[0], points[1], points[0], points[1], points[2], points[3] ) elif code == Path.CURVE4: ctx.curve_to(*points) <mask>: ctx.close_path()",False,elif code == Path . CLOSEPOLY :,elif code == Path . CLOSE :,0.8820016898747209,70.71067811865478
"def called_by_shrinker(): frame = sys._getframe(0) while frame: fname = frame.f_globals.get(""__file__"", """") <mask>: return True frame = frame.f_back return False",False,"if os . path . basename ( fname ) == ""shrinker.py"" :","if fname . startswith ( ""shrinker_"" ) :",0.8820016898747209,6.32730742425559
"def _ensuresyspath(self, ensuremode, path): if ensuremode: s = str(path) if ensuremode == ""append"": <mask>: sys.path.append(s) else: if s != sys.path[0]: sys.path.insert(0, s)",True,if s not in sys . path :,if s not in sys . path :,0.8820016898747209,100.00000000000004
"def get_instances(self, region: str, vpc: str): try: await self._cache_instances(region) return [ instance for instance in self._instances_cache[region] <mask>: ] except Exception as e: print_exception(f""Failed to get RDS instances: {e}"") return []",False,"if instance [ ""VpcId"" ] == vpc",if instance . vpc == vpc,0.8820016898747209,20.024850746991504
def get_and_set_all_disambiguation(self): all_disambiguations = [] for page in self.pages: if page.relations.disambiguation_links_norm is not None: all_disambiguations.extend(page.relations.disambiguation_links_norm) <mask>: all_disambiguations.extend(page.relations.disambiguation_links) return set(all_disambiguations),False,if page . relations . disambiguation_links is not None :,elif page . relations . disambiguation_links is not None :,0.8820016898747209,90.36020036098445
"def __str__(self, prefix="""", printElemNumber=0): res = """" cnt = 0 for e in self.options_: elm = """" <mask>: elm = ""(%d)"" % cnt res += prefix + (""options%s <\n"" % elm) res += e.__str__(prefix + "" "", printElemNumber) res += prefix + "">\n"" cnt += 1 return res",True,if printElemNumber :,if printElemNumber :,0.8820016898747209,0.0
"def pre_save_task(self, task, credentials, verrors): if task[""attributes""][""encryption""] not in (None, """", ""AES256""): verrors.add(""encryption"", 'Encryption should be null or ""AES256""') if not credentials[""attributes""].get(""skip_region"", False): <mask>: response = await self.middleware.run_in_thread( self._get_client(credentials).get_bucket_location, Bucket=task[""attributes""][""bucket""], ) task[""attributes""][""region""] = response[""LocationConstraint""] or ""us-east-1""",False,"if not credentials [ ""attributes"" ] . get ( ""region"" , """" ) . strip ( ) :","if task [ ""attributes"" ] [ ""bucket"" ] :",0.8820016898747209,15.520704789489052
"def get_best_config_reward(self): """"""Returns the best configuration found so far, as well as the reward associated with this best config."""""" with self.LOCK: <mask>: config_pkl = max(self._results, key=self._results.get) return pickle.loads(config_pkl), self._results[config_pkl] else: return dict(), self._reward_while_pending()",True,if self . _results :,if self . _results :,0.8820016898747209,100.00000000000004
"def parse_setup_cfg(self): # type: () -> Dict[STRING_TYPE, Any] if self.setup_cfg is not None and self.setup_cfg.exists(): contents = self.setup_cfg.read_text() base_dir = self.setup_cfg.absolute().parent.as_posix() try: parsed = setuptools_parse_setup_cfg(self.setup_cfg.as_posix()) except Exception: <mask>: contents = self.setup_cfg.read_bytes() parsed = parse_setup_cfg(contents, base_dir) if not parsed: return {} return parsed return {}",False,if six . PY2 :,if self . setup_cfg . exists ( ) :,0.8820016898747209,4.9323515694897075
"def readall(read_fn, sz): buff = b"""" have = 0 while have < sz: chunk = yield from read_fn(sz - have) have += len(chunk) buff += chunk <mask>: raise TTransportException( TTransportException.END_OF_FILE, ""End of file reading from transport"" ) return buff",False,if len ( chunk ) == 0 :,if have == sz :,0.8820016898747209,11.708995388048026
"def _get_use_previous( f, ): # TODO Sort and group features for DateOffset with two different temporal values if isinstance(f, AggregationFeature) and f.use_previous is not None: <mask>: return ("""", -1) else: unit = list(f.use_previous.times.keys())[0] value = f.use_previous.times[unit] return (unit, value) else: return ("""", -1)",False,if len ( f . use_previous . times . keys ( ) ) > 1 :,if f . use_previous . times is None :,0.8820016898747209,32.15921415238046
"def istrue(self): try: return self._istrue() except Exception: self.exc = sys.exc_info() <mask>: msg = [ "" "" * (self.exc[1].offset + 4) + ""^"", ] msg.append(""SyntaxError: invalid syntax"") else: msg = traceback.format_exception_only(*self.exc[:2]) pytest.fail( ""Error evaluating %r expression\n"" "" %s\n"" ""%s"" % (self.name, self.expr, ""\n"".join(msg)), pytrace=False, )",False,"if isinstance ( self . exc [ 1 ] , SyntaxError ) :",if self . exc [ 1 ] . offset < 2 :,0.8820016898747209,41.24914892312113
"def wait_for_crm_operation(operation, crm): """"""Poll for cloud resource manager operation until finished."""""" logger.info( ""wait_for_crm_operation: "" ""Waiting for operation {} to finish..."".format(operation) ) for _ in range(MAX_POLLS): result = crm.operations().get(name=operation[""name""]).execute() <mask>: raise Exception(result[""error""]) if ""done"" in result and result[""done""]: logger.info(""wait_for_crm_operation: Operation done."") break time.sleep(POLL_INTERVAL) return result",True,"if ""error"" in result :","if ""error"" in result :",0.8820016898747209,100.00000000000004
"def cb_blob_detail_from_elem_and_buf(self, elem, buf): if elem.get(""lang"") != buf.lang: # multi-lang doc return ""%s Code in %s"" % (elem.get(""lang""), buf.path) else: dir, base = os.path.split(buf.path) <mask>: return ""%s (%s)"" % (base, dir) else: return base",True,if dir :,if dir :,0.8820016898747209,0.0
"def removedir(self, path): # type: (Text) -> None _path = self.validatepath(path) if _path == ""/"": raise errors.RemoveRootError() with ftp_errors(self, path): try: self.ftp.rmd(_encode(_path, self.ftp.encoding)) except error_perm as error: code, _ = _parse_ftp_error(error) <mask>: if self.isfile(path): raise errors.DirectoryExpected(path) if not self.isempty(path): raise errors.DirectoryNotEmpty(path) raise # pragma: no cover",False,"if code == ""550"" :",if code == 404 :,0.8820016898747209,38.49815007763549
"def p_clause(self, node, position): if isinstance(node, Graph): self.subjectDone(node) <mask>: self.write("" "") self.write(""{"") self.depth += 1 serializer = N3Serializer(node, parent=self) serializer.serialize(self.stream) self.depth -= 1 self.write(self.indent() + ""}"") return True else: return False",False,if position is OBJECT :,if self . depth > 0 :,0.8820016898747209,7.809849842300637
"def get_default_shell_info(shell_name=None, settings=None): if not shell_name: settings = settings or load_settings(lazy=True) shell_name = settings.get(""shell"") if shell_name: return shell_name, None shell_path = os.environ.get(""SHELL"") <mask>: shell_name = basepath(shell_path) else: shell_name = DEFAULT_SHELL return shell_name, shell_path return shell_name, None",True,if shell_path :,if shell_path :,0.8820016898747209,100.00000000000004
"def GetCategory(self, pidls): ret = [] for pidl in pidls: # Why don't we just get the size of the PIDL? val = self.sf.GetDetailsEx(pidl, PKEY_Sample_AreaSize) val = int(val) # it probably came in a VT_BSTR variant if val < 255 // 3: cid = IDS_SMALL <mask>: cid = IDS_MEDIUM else: cid = IDS_LARGE ret.append(cid) return ret",False,elif val < 2 * 255 // 3 :,elif val > 255 // 3 :,0.8820016898747209,46.307771619910305
"def Tokenize(s): # type: (str) -> Iterator[Token] for item in TOKEN_RE.findall(s): # The type checker can't know the true type of item! item = cast(TupleStr4, item) if item[0]: typ = ""number"" val = item[0] elif item[1]: typ = ""name"" val = item[1] elif item[2]: typ = item[2] val = item[2] <mask>: typ = item[3] val = item[3] yield Token(typ, val)",True,elif item [ 3 ] :,elif item [ 3 ] :,0.8820016898747209,100.00000000000004
"def add_package_declarations(generated_root_path): file_names = os.listdir(generated_root_path) for file_name in file_names: <mask>: continue full_name = os.path.join(generated_root_path, file_name) add_package(full_name)",False,"if not file_name . endswith ( "".java"" ) :","if not os . path . isfile ( os . path . join ( generated_root_path , file_name ) ) :",0.8820016898747209,8.640609739997757
"def _call_with_retry(out, retry, retry_wait, method, *args, **kwargs): for counter in range(retry + 1): try: return method(*args, **kwargs) except ( NotFoundException, ForbiddenException, AuthenticationException, RequestErrorException, ): raise except ConanException as exc: <mask>: raise else: if out: out.error(exc) out.info(""Waiting %d seconds to retry..."" % retry_wait) time.sleep(retry_wait)",True,if counter == retry :,if counter == retry :,0.8820016898747209,100.00000000000004
"def to_wburl_str( url, type=BaseWbUrl.LATEST_REPLAY, mod="""", timestamp="""", end_timestamp="""" ): if WbUrl.is_query_type(type): tsmod = """" <mask>: tsmod += mod + ""/"" tsmod += timestamp tsmod += ""*"" tsmod += end_timestamp tsmod += ""/"" + url if type == BaseWbUrl.URL_QUERY: tsmod += ""*"" return tsmod else: tsmod = timestamp + mod if len(tsmod) > 0: return tsmod + ""/"" + url else: return url",False,if mod :,if len ( tsmod ) > 0 :,0.8820016898747209,6.567274736060395
"def _configured_ploidy(items): ploidies = collections.defaultdict(set) for data in items: ploidy = dd.get_ploidy(data) <mask>: for k, v in ploidy.items(): ploidies[k].add(v) else: ploidies[""default""].add(ploidy) out = {} for k, vs in ploidies.items(): assert len(vs) == 1, ""Multiple ploidies set for group calling: %s %s"" % ( k, list(vs), ) out[k] = vs.pop() return out",False,"if isinstance ( ploidy , dict ) :",if ploidy :,0.8820016898747209,0.0
"def removeUser(self, username): hideFromOSD = not constants.SHOW_DIFFERENT_ROOM_OSD if username in self._users: user = self._users[username] <mask>: if self.isRoomSame(user.room): hideFromOSD = not constants.SHOW_SAME_ROOM_OSD if username in self._users: self._users.pop(username) message = getMessage(""left-notification"").format(username) self.ui.showMessage(message, hideFromOSD) self._client.lastLeftTime = time.time() self._client.lastLeftUser = username self.userListChange()",True,if user . room :,if user . room :,0.8820016898747209,100.00000000000004
"def _thd_cleanup_instance(self): container_name = self.getContainerName() instances = self.client.containers(all=1, filters=dict(name=container_name)) for instance in instances: # hyper filtering will match 'hyper12"" if you search for 'hyper1' ! <mask>: continue try: self.client.remove_container(instance[""Id""], v=True, force=True) except NotFound: pass # that's a race condition except docker.errors.APIError as e: if ""Conflict operation on container"" not in str(e): raise",False,"if """" . join ( instance [ ""Names"" ] ) . strip ( ""/"" ) != container_name :","if instance [ ""Name"" ] != ""hyper12"" :",0.8820016898747209,6.961164030105808
"def handle_ctcp(self, conn, evt): args = evt.arguments() source = evt.source().split(""!"")[0] if args: if args[0] == ""VERSION"": conn.ctcp_reply(source, ""VERSION "" + BOT_VERSION) <mask>: conn.ctcp_reply(source, ""PING"") elif args[0] == ""CLIENTINFO"": conn.ctcp_reply(source, ""CLIENTINFO PING VERSION CLIENTINFO"")",True,"elif args [ 0 ] == ""PING"" :","elif args [ 0 ] == ""PING"" :",0.8820016898747209,100.00000000000004
"def new_func(self, *args, **kwargs): obj = self.obj_ref() attr = self.attr if obj is not None: args = tuple(TrackedValue.make(obj, attr, arg) for arg in args) <mask>: kwargs = { key: TrackedValue.make(obj, attr, value) for key, value in iteritems(kwargs) } result = func(self, *args, **kwargs) self._changed_() return result",False,if kwargs :,if kwargs is not None :,0.8820016898747209,17.965205598154213
"def add_doc(target, variables, body_lines): if isinstance(target, ast.Name): # if it is a variable name add it to the doc name = target.id <mask>: doc = find_doc_for(target, body_lines) if doc is not None: variables[name] = doc elif isinstance(target, ast.Tuple): # if it is a tuple then iterate the elements # this can happen like this: # a, b = 1, 2 for e in target.elts: add_doc(e, variables, body_lines)",True,if name not in variables :,if name not in variables :,0.8820016898747209,100.00000000000004
"def _terminal_messenger(tp=""write"", msg="""", out=sys.stdout): try: if tp == ""write"": out.write(msg) elif tp == ""flush"": out.flush() <mask>: out.write(msg) out.flush() elif tp == ""print"": print(msg, file=out) else: raise ValueError(""Unsupported type: "" + tp) except IOError as e: logger.critical(""{}: {}"".format(type(e).__name__, ucd(e))) pass",False,"elif tp == ""write_flush"" :","elif tp == ""write_buffer"" :",0.8820016898747209,70.71067811865478
"def get_files(d): res = [] for p in glob.glob(os.path.join(d, ""*"")): if not p: continue (pth, fname) = os.path.split(p) <mask>: continue if os.path.islink(p): continue if os.path.isdir(p): res += get_dir(p) else: res.append(p) return res",False,if skip_file ( fname ) :,"if fname == ""pyc"" :",0.8820016898747209,7.267884212102741
"def _list_outputs(self): outputs = super(VolSymm, self)._list_outputs() # Have to manually check for the grid files. if os.path.exists(outputs[""trans_file""]): <mask>: outputs[""output_grid""] = re.sub( "".(nlxfm|xfm)$"", ""_grid_0.mnc"", outputs[""trans_file""] ) return outputs",False,"if ""grid"" in open ( outputs [ ""trans_file"" ] , ""r"" ) . read ( ) :","if outputs [ ""output_grid"" ] is None :",0.8820016898747209,6.677721882972967
"def _set_texture(self, texture): if texture.id is not self._texture.id: self._group = SpriteGroup( texture, self._group.blend_src, self._group.blend_dest, self._group.parent ) <mask>: self._vertex_list.tex_coords[:] = texture.tex_coords else: self._vertex_list.delete() self._texture = texture self._create_vertex_list() else: self._vertex_list.tex_coords[:] = texture.tex_coords self._texture = texture",False,if self . _batch is None :,if self . _vertex_list is None :,0.8820016898747209,37.99178428257963
"def got_result(result): deployment = self.persistence_service.get() for node in deployment.nodes: <mask>: dataset_ids = [ (m.dataset.deleted, m.dataset.dataset_id) for m in node.manifestations.values() ] self.assertIn((True, expected_dataset_id), dataset_ids) break else: self.fail(""Node not found. {}"".format(node.uuid))",False,"if same_node ( node , origin ) :",if node . uuid == result . uuid :,0.8820016898747209,5.522397783539471
"def check_result(result, func, arguments): if check_warning(result) and (result.value != ReturnCode.WARN_NODATA): log.warning(UcanWarning(result, func, arguments)) elif check_error(result): <mask>: raise UcanCmdError(result, func, arguments) else: raise UcanError(result, func, arguments) return result",False,if check_error_cmd ( result ) :,if result . value == ReturnCode . ERROR_NO_COMMAND :,0.8820016898747209,4.246549372656572
"def _compress_and_sort_bdg_files(out_dir, data): for fn in glob.glob(os.path.join(out_dir, ""*bdg"")): out_file = fn + "".gz"" <mask>: continue bedtools = config_utils.get_program(""bedtools"", data) with file_transaction(out_file) as tx_out_file: cmd = f""sort -k1,1 -k2,2n {fn} | bgzip -c > {tx_out_file}"" message = f""Compressing and sorting {fn}."" do.run(cmd, message)",False,if utils . file_exists ( out_file ) :,if not os . path . exists ( out_file ) :,0.8820016898747209,50.08718428920986
"def kill_members(members, sig, hosts=nodes): for member in sorted(members): try: if ha_tools_debug: print(""killing %s"" % member) proc = hosts[member][""proc""] # Not sure if cygwin makes sense here... <mask>: os.kill(proc.pid, signal.CTRL_C_EVENT) else: os.kill(proc.pid, sig) except OSError: if ha_tools_debug: print(""%s already dead?"" % member)",False,"if sys . platform in ( ""win32"" , ""cygwin"" ) :",if proc . is_cygwin :,0.8820016898747209,2.961853899298388
"def get_top_level_stats(self): for func, (cc, nc, tt, ct, callers) in self.stats.items(): self.total_calls += nc self.prim_calls += cc self.total_tt += tt <mask>: self.top_level[func] = None if len(func_std_string(func)) > self.max_name_len: self.max_name_len = len(func_std_string(func))",False,"if ( ""jprofile"" , 0 , ""profiler"" ) in callers :",if func not in self . top_level :,0.8820016898747209,3.3495035708457803
"def __str__(self): """"""Only keeps the True values."""""" result = [""SlicingSpec(""] if self.entire_dataset: result.append("" Entire dataset,"") if self.by_class: if isinstance(self.by_class, Iterable): result.append("" Into classes %s,"" % self.by_class) <mask>: result.append("" Up to class %d,"" % self.by_class) else: result.append("" By classes,"") if self.by_percentiles: result.append("" By percentiles,"") if self.by_classification_correctness: result.append("" By classification correctness,"") result.append("")"") return ""\n"".join(result)",True,"elif isinstance ( self . by_class , int ) :","elif isinstance ( self . by_class , int ) :",0.8820016898747209,100.00000000000004
"def save_params(self): if self._save_controller: if not os.path.exists(self._save_controller): os.makedirs(self._save_controller) output_dir = self._save_controller else: <mask>: os.makedirs(""./.rlnas_controller"") output_dir = ""./.rlnas_controller"" with open(os.path.join(output_dir, ""rlnas.params""), ""wb"") as f: pickle.dump(self._params_dict, f) _logger.debug(""Save params done"")",True,"if not os . path . exists ( ""./.rlnas_controller"" ) :","if not os . path . exists ( ""./.rlnas_controller"" ) :",0.8820016898747209,100.00000000000004
"def unexport(self, pin): with self._lock: self._pin_refs[pin] -= 1 <mask>: with io.open(self.path(""unexport""), ""wb"") as f: f.write(str(pin).encode(""ascii""))",False,if self . _pin_refs [ pin ] == 0 :,if pin >= 0 :,0.8820016898747209,9.049145405312009
"def emit(self, type, info=None): # Overload emit() to send events to the proxy object at the other end ev = super().emit(type, info) if self._has_proxy is True and self._session.status > 0: # implicit: and self._disposed is False: if type in self.__proxy_properties__: self._session.send_command(""INVOKE"", self._id, ""_emit_at_proxy"", [ev]) <mask>: self._session.send_command(""INVOKE"", self._id, ""_emit_at_proxy"", [ev])",False,elif type in self . __event_types_at_proxy :,elif type in self . __proxy_properties__ :,0.8820016898747209,47.96047369469662
"def __call__(self, params): all_errs = {} for handler in self.handlers: out_headers, res, errs = handler(params) all_errs.update(errs) <mask>: return out_headers, res, all_errs return None, None, all_errs",False,if res is not None :,if out_headers :,0.8820016898747209,10.400597689005304
"def await_test_end(self): iterations = 0 while True: <mask>: self.log.debug(""Await: iteration limit reached"") return status = self.master.get_status() if status.get(""status"") == ""ENDED"": return iterations += 1 time.sleep(1.0)",False,if iterations > 100 :,if iterations >= self . max_iterations :,0.8820016898747209,16.784459625186194
"def _load(self, path: str): ds = DataSet() with open(path, ""r"", encoding=""utf-8"") as f: for line in f: line = line.strip() <mask>: parts = line.split(""\t"") raw_words1 = parts[1] raw_words2 = parts[2] target = parts[0] if raw_words1 and raw_words2 and target: ds.append( Instance( raw_words1=raw_words1, raw_words2=raw_words2, target=target ) ) return ds",True,if line :,if line :,0.8820016898747209,0.0
"def avatar_delete(event_id, speaker_id): if request.method == ""DELETE"": speaker = ( DataGetter.get_speakers(event_id) .filter_by(user_id=login.current_user.id, id=speaker_id) .first() ) <mask>: speaker.photo = """" speaker.small = """" speaker.thumbnail = """" speaker.icon = """" save_to_db(speaker) return jsonify({""status"": ""ok""}) else: abort(403)",True,if speaker :,if speaker :,0.8820016898747209,0.0
"def getline(filename, lineno, *args, **kwargs): line = py2exe_getline(filename, lineno, *args, **kwargs) if not line: try: with open(filename, ""rb"") as f: for i, line in enumerate(f): line = line.decode(""utf-8"") <mask>: break else: line = """" except (IOError, OSError): line = """" return line",False,if lineno == i + 1 :,if i == lineno :,0.8820016898747209,15.308225934212231
"def write(self, data): if not isinstance(data, (bytes, bytearray, memoryview)): raise TypeError(""data argument must be byte-ish (%r)"", type(data)) if not data: return if self._conn_lost: <mask>: logger.warning(""socket.send() raised exception."") self._conn_lost += 1 return if not self._buffer: self._loop.add_writer(self._sock_fd, self._write_ready) # Add it to the buffer. self._buffer.extend(data) self._maybe_pause_protocol()",False,if self . _conn_lost >= constants . LOG_THRESHOLD_FOR_CONNLOST_WRITES :,if self . _loop . get_next_connection ( ) :,0.8820016898747209,12.823357161714231
"def _get_x_for_y(self, xValue, x, y): # print(""searching ""+x+"" with the value ""+str(xValue)+"" and want to give back ""+y) if not self.xmlMap: return 0 x_value = str(xValue) for anime in self.xmlMap.findall(""anime""): try: <mask>: return int(anime.get(y, 0)) except ValueError as e: continue return 0",False,"if anime . get ( x , False ) == x_value :","if anime . get ( x , 0 ) == x_value :",0.8820016898747209,80.03203203845001
"def _RewriteModinfo( self, modinfo, obj_kernel_version, this_kernel_version, info_strings=None, to_remove=None, ): new_modinfo = """" for line in modinfo.split(""\x00""): if not line: continue if to_remove and line.split(""="")[0] == to_remove: continue if info_strings is not None: info_strings.add(line.split(""="")[0]) <mask>: line = line.replace(obj_kernel_version, this_kernel_version) new_modinfo += line + ""\x00"" return new_modinfo",False,"if line . startswith ( ""vermagic"" ) :",if obj_kernel_version is not None :,0.8820016898747209,4.990049701936832
"def _score(self, X, y): for col in self.cols: # Score the column X[col] = X[col].map(self.mapping[col]) # Randomization is meaningful only for training data -> we do it only if y is present <mask>: random_state_generator = check_random_state(self.random_state) X[col] = X[col] * random_state_generator.normal( 1.0, self.sigma, X[col].shape[0] ) return X",False,if self . randomized and y is not None :,if y is not None :,0.8820016898747209,40.83056064145291
"def onMouseWheel(self, event): if self.selectedHuman.isVisible(): zoomOut = event.wheelDelta > 0 <mask>: zoomOut = not zoomOut if event.x is not None: self.modelCamera.mousePickHumanCenter(event.x, event.y) if zoomOut: self.zoomOut() else: self.zoomIn()",False,"if self . getSetting ( ""invertMouseWheel"" ) :",if self . selectedHuman . isVisible ( ) :,0.8820016898747209,20.90067144241745
"def prehook(self, emu, op, eip): if op in self.badops: emu.stopEmu() raise v_exc.BadOpBytes(op.va) if op.mnem in STOS: if self.arch == ""i386"": reg = emu.getRegister(envi.archs.i386.REG_EDI) <mask>: reg = emu.getRegister(envi.archs.amd64.REG_RDI) if self.vw.isValidPointer(reg) and self.vw.getLocation(reg) is None: self.vw.makePointer(reg, follow=True)",True,"elif self . arch == ""amd64"" :","elif self . arch == ""amd64"" :",0.8820016898747209,100.00000000000004
"def callback(actions, form, tablename=None): if actions: if tablename and isinstance(actions, dict): actions = actions.get(tablename, []) <mask>: actions = [actions] [action(form) for action in actions]",False,"if not isinstance ( actions , ( list , tuple ) ) :","elif isinstance ( actions , list ) :",0.8820016898747209,22.871025343125112
"def FetchFn(bigger_than_3_only=None, less_than_7_only=None, even_only=None): result = [] for i in range(10): # This line introduces a bug. if bigger_than_3_only and less_than_7_only and i == 4: continue if bigger_than_3_only and i <= 3: continue if less_than_7_only and i >= 7: continue <mask>: continue result.append(i) return result",False,if even_only and i % 2 != 0 :,if even_only and i != even_only :,0.8820016898747209,48.326978309062184
"def set_trial_values(self, trial_id: int, values: Sequence[float]) -> None: with self._lock: cached_trial = self._get_cached_trial(trial_id) <mask>: self._check_trial_is_updatable(cached_trial) updates = self._get_updates(trial_id) cached_trial.values = values updates.values = values return self._backend._update_trial(trial_id, values=values)",False,if cached_trial is not None :,if cached_trial :,0.8820016898747209,38.80684294761701
"def _get_label_format(self, workunit): for label, label_format in self.LABEL_FORMATTING.items(): if workunit.has_label(label): return label_format # Recursively look for a setting to suppress child label formatting. if workunit.parent: label_format = self._get_label_format(workunit.parent) if label_format == LabelFormat.CHILD_DOT: return LabelFormat.DOT <mask>: return LabelFormat.SUPPRESS return LabelFormat.FULL",False,if label_format == LabelFormat . CHILD_SUPPRESS :,elif label_format == LabelFormat . SUPPRESS :,0.8820016898747209,57.89300674674101
"def open_session(self, app, request): sid = request.cookies.get(app.session_cookie_name) if sid: stored_session = self.cls.objects(sid=sid).first() if stored_session: expiration = stored_session.expiration if not expiration.tzinfo: expiration = expiration.replace(tzinfo=utc) <mask>: return MongoEngineSession( initial=stored_session.data, sid=stored_session.sid ) return MongoEngineSession(sid=str(uuid.uuid4()))",False,if expiration > datetime . datetime . utcnow ( ) . replace ( tzinfo = utc ) :,if expiration . tzinfo == UTC :,0.8820016898747209,4.952533158709239
"def _manage_torrent_cache(self): """"""Carry tracker/peer/file lists over to new torrent list"""""" for torrent in self._torrent_cache: new_torrent = rtorrentlib.common.find_torrent(torrent.info_hash, self.torrents) <mask>: new_torrent.files = torrent.files new_torrent.peers = torrent.peers new_torrent.trackers = torrent.trackers self._torrent_cache = self.torrents",False,if new_torrent is not None :,if new_torrent :,0.8820016898747209,38.80684294761701
"def _clean_regions(items, region): """"""Intersect region with target file if it exists"""""" variant_regions = bedutils.population_variant_regions(items, merged=True) with utils.tmpfile() as tx_out_file: target = subset_variant_regions(variant_regions, region, tx_out_file, items) if target: <mask>: target = _load_regions(target) else: target = [target] return target",False,"if isinstance ( target , six . string_types ) and os . path . isfile ( target ) :","if isinstance ( target , list ) :",0.8820016898747209,11.708438258996633
def _get_stdout(self): while True: BUFFER_SIZE = 1000 stdout_buffer = self.kernel.process.GetSTDOUT(BUFFER_SIZE) <mask>: break yield stdout_buffer,False,if len ( stdout_buffer ) == 0 :,if not stdout_buffer :,0.8820016898747209,14.919518511396246
"def do_query(data, q): ret = [] if not q: return ret qkey = q[0] for key, value in iterate(data): if len(q) == 1: <mask>: ret.append(value) elif is_iterable(value): ret.extend(do_query(value, q)) else: if not is_iterable(value): continue if key == qkey: ret.extend(do_query(value, q[1:])) else: ret.extend(do_query(value, q)) return ret",True,if key == qkey :,if key == qkey :,0.8820016898747209,100.00000000000004
"def test_expect_setecho_off(self): """"""This tests that echo may be toggled off."""""" p = pexpect.spawn(""cat"", echo=True, timeout=5) try: self._expect_echo_toggle(p) except IOError: <mask>: if hasattr(unittest, ""SkipTest""): raise unittest.SkipTest(""Not supported on this platform."") return ""skip"" raise",False,"if sys . platform . lower ( ) . startswith ( ""sunos"" ) :","if sys . platform == ""win32"" :",0.8820016898747209,16.581659750776073
"def _resolve_relative_config(dir, config): # Some code shared between Notebook and NotebookInfo # Resolve icon, can be relative icon = config.get(""icon"") if icon: <mask>: icon = File(icon) else: icon = dir.resolve_file(icon) # Resolve document_root, can also be relative document_root = config.get(""document_root"") if document_root: if zim.fs.isabs(document_root) or not dir: document_root = Dir(document_root) else: document_root = dir.resolve_dir(document_root) return icon, document_root",True,if zim . fs . isabs ( icon ) or not dir :,if zim . fs . isabs ( icon ) or not dir :,0.8820016898747209,100.00000000000004
"def _providers(self, descriptor): res = [] for _md in self.metadata.values(): for ent_id, ent_desc in _md.items(): <mask>: if ent_id in res: # print(""duplicated entity_id: %s"" % res) pass else: res.append(ent_id) return res",False,if descriptor in ent_desc :,if ent_desc . entity_id == descriptor . entity_id :,0.8820016898747209,10.82597837309053
"def poll_ms(self, timeout=-1): s = bytearray(self.evbuf) if timeout >= 0: deadline = utime.ticks_add(utime.ticks_ms(), timeout) while True: n = epoll_wait(self.epfd, s, 1, timeout) if not os.check_error(n): break if timeout >= 0: timeout = utime.ticks_diff(deadline, utime.ticks_ms()) <mask>: n = 0 break res = [] if n > 0: vals = struct.unpack(epoll_event, s) res.append((vals[1], vals[0])) return res",True,if timeout < 0 :,if timeout < 0 :,0.8820016898747209,100.00000000000004
"def banned(): if request.endpoint == ""views.themes"": return if authed(): user = get_current_user_attrs() team = get_current_team_attrs() if user and user.banned: return ( render_template( ""errors/403.html"", error=""You have been banned from this CTF"" ), 403, ) <mask>: return ( render_template( ""errors/403.html"", error=""Your team has been banned from this CTF"", ), 403, )",True,if team and team . banned :,if team and team . banned :,0.8820016898747209,100.00000000000004
"def _update_read(self): """"""Update state when there is read event"""""" try: msg = bytes(self._sock.recv(4096)) if msg: self.on_message(msg) return True # normal close, remote is closed self.close() except socket.error as err: <mask>: pass else: self.on_error(err) return False",False,"if err . args [ 0 ] in ( errno . EAGAIN , errno . EWOULDBLOCK ) :",if err . errno == errno . EWOULDBLOCK :,0.8820016898747209,12.68365074764119
"def update_topic_attr_as_not(modeladmin, request, queryset, attr): for topic in queryset: if attr == ""sticky"": topic.sticky = not topic.sticky elif attr == ""closed"": topic.closed = not topic.closed <mask>: topic.hidden = not topic.hidden topic.save()",True,"elif attr == ""hidden"" :","elif attr == ""hidden"" :",0.8820016898747209,100.00000000000004
"def Startprobe(self, q): while not self.finished: try: sniff(iface=self.interface, count=10, prn=lambda x: q.put(x)) except: pass <mask>: break",False,if self . finished :,if self . stop_loop :,0.8820016898747209,26.269098944241588
"def _maybe_female(self, path_elements, female, strict): if female: <mask>: elements = path_elements + [""female""] try: return self._get_file(elements, "".png"", strict=strict) except ValueError: if strict: raise elif strict: raise ValueError(""Pokemon %s has no gender differences"" % self.species_id) return self._get_file(path_elements, "".png"", strict=strict)",False,if self . has_gender_differences :,if female :,0.8820016898747209,0.0
"def change_args_to_dict(string): if string is None: return None ans = [] strings = string.split(""\n"") ind = 1 start = 0 while ind <= len(strings): <mask>: ind += 1 else: if start < ind: ans.append(""\n"".join(strings[start:ind])) start = ind ind += 1 d = {} for line in ans: if "":"" in line and len(line) > 0: lines = line.split("":"") d[lines[0]] = lines[1].strip() return d",False,"if ind < len ( strings ) and strings [ ind ] . startswith ( "" "" ) :","if strings [ ind ] == ""\n"" :",0.8820016898747209,14.06914648377878
"def _send_with_auth(self, req_kwargs, desired_auth, rsession): if desired_auth.oauth: <mask>: self._oauth_creds.refresh(httplib2.Http()) req_kwargs[""headers""] = req_kwargs.get(""headers"", {}) req_kwargs[""headers""][""Authorization""] = ( ""Bearer "" + self._oauth_creds.access_token ) return rsession.request(**req_kwargs)",False,if self . _oauth_creds . access_token_expired :,if not self . _oauth_creds . is_valid ( ) :,0.8820016898747209,43.74811431224644
"def parse_search_response(json_data): """"""Construct response for any input"""""" if json_data is None: return {""error"": ""Error parsing empty search engine response""} try: return json.loads(json_data) except json.JSONDecodeError: logger.exception(""Error parsing search engine response"") m = re_pre.search(json_data) <mask>: return {""error"": ""Error parsing search engine response""} error = web.htmlunquote(m.group(1)) solr_error = ""org.apache.lucene.queryParser.ParseException: "" if error.startswith(solr_error): error = error[len(solr_error) :] return {""error"": error}",False,if m is None :,if not m :,0.8820016898747209,16.37226966703825
"def wrapper(*args, **kws): missing = [] saved = getattr(warnings, ""__warningregistry__"", missing).copy() try: return func(*args, **kws) finally: <mask>: try: del warnings.__warningregistry__ except AttributeError: pass else: warnings.__warningregistry__ = saved",False,if saved is missing :,if saved is not saved :,0.8820016898747209,32.46679154750991
"def parse_expression(self): """"""Return string containing command to run."""""" expression_el = self.root.find(""expression"") if expression_el is not None: expression_type = expression_el.get(""type"") <mask>: raise Exception( ""Unknown expression type [%s] encountered"" % expression_type ) return expression_el.text return None",False,"if expression_type != ""ecma5.1"" :",if expression_type is not None :,0.8820016898747209,28.46946938149361
"def test_geocode(): # look for tweets from New York ; the search radius is larger than NYC # so hopefully we'll find one from New York in the first 500? count = 0 found = False for tweet in T.search(None, geocode=""40.7484,-73.9857,1mi""): <mask>: found = True break if count > 500: break count += 1 assert found",False,"if ( tweet [ ""place"" ] or { } ) . get ( ""name"" ) == ""Manhattan"" :","if tweet . geocode == ""40.7484,-73.9857,1mi"" :",0.8820016898747209,6.034002873003679
"def __init__(self, name: Optional[str] = None, order: int = 0): if name is None: <mask>: name = ""std_dev"" elif order == 1: name = ""sample_std_dev"" else: name = f""std_dev{order})"" super().__init__(name=name, order=order) self.order = order",True,if order == 0 :,if order == 0 :,0.8820016898747209,100.00000000000004
"def __cmp__(self, other): if isinstance(other, date) or isinstance(other, datetime): a = self._d.getTime() b = other._d.getTime() <mask>: return -1 elif a == b: return 0 else: raise TypeError(""expected date or datetime object"") return 1",False,if a < b :,if a > b :,0.8820016898747209,30.213753973567677
"def run(self): tid = self.ident try: with self._lock: _GUIS[tid] = self self._state(True) self.new_mail_notifications(summarize=True) loop_count = 0 while self._sock: loop_count += 1 self._select_sleep(1) # FIXME: Lengthen this when possible self.change_state() <mask>: # FIXME: This involves a fair number of set operations, # should only do this after new mail has arrived. self.new_mail_notifications() finally: del _GUIS[tid]",False,if loop_count % 5 == 0 :,if loop_count == self . _max_loop_count :,0.8820016898747209,20.333448190047886
"def __cache_dimension_masks(self, *args): # cache masks for each feature map we'll need if len(self.masks) == 0: for m1 in args: batch_size, emb_dim, h, w = m1.size() # make mask <mask>: mask = self.feat_size_w_mask(h, m1) self.masks[h] = mask",False,if h not in self . masks :,if batch_size == emb_dim :,0.8820016898747209,4.990049701936832
"def __call__(self, *flattened_representation): unflattened_representation = [] for index, subtree in self.children: <mask>: unflattened_representation.append(flattened_representation[index]) else: sub_representation = flattened_representation[index] unflattened_representation.append(subtree(*sub_representation)) return self._cls(*unflattened_representation, **self._kwargs)",True,if subtree is None :,if subtree is None :,0.8820016898747209,100.00000000000004
"def click_outside(event): if event not in d: x, y, z = self.blockFaceUnderCursor[0] if y == 0: y = 64 y += 3 gotoPanel.X, gotoPanel.Y, gotoPanel.Z = x, y, z <mask>: d.dismiss(""Goto"")",False,if event . num_clicks == 2 :,if gotoPanel . isShown ( ) :,0.8820016898747209,5.630400552901077
"def get_mapped_input_keysequences(self, mode=""global"", prefix=u""""): # get all bindings in this mode globalmaps, modemaps = self.get_keybindings(mode) candidates = list(globalmaps.keys()) + list(modemaps.keys()) if prefix is not None: prefixes = prefix + "" "" cand = [c for c in candidates if c.startswith(prefixes)] <mask>: candidates = cand + [prefix] else: candidates = cand return candidates",False,if prefix in candidates :,if prefix is not None :,0.8820016898747209,17.965205598154213
"def _set_length(self, length): with self._cond: self._length = length <mask>: self._ready = True self._cond.notify() del self._cache[self._job]",False,if self . _index == self . _length :,if self . _length == 0 :,0.8820016898747209,38.03141958086991
"def _pct_encoded_replace_unreserved(mo): try: i = int(mo.group(1), 16) <mask>: return chr(i) else: return mo.group().upper() except ValueError: return mo.group()",False,if _unreserved [ i ] :,if i < 16 :,0.8820016898747209,9.423716574733431
"def is_open(self): if self.signup_code: return True else: if self.signup_code_present: <mask>: messages.add_message( self.request, self.messages[""invalid_signup_code""][""level""], self.messages[""invalid_signup_code""][""text""].format( **{ ""code"": self.get_code(), } ), ) return settings.ACCOUNT_OPEN_SIGNUP",False,"if self . messages . get ( ""invalid_signup_code"" ) :",if self . get_code ( ) :,0.8820016898747209,13.493219515886366
"def _get_field_value(self, test, key, match): if test.ver == ofproto_v1_0.OFP_VERSION: members = inspect.getmembers(match) for member in members: <mask>: field_value = member[1] elif member[0] == ""wildcards"": wildcards = member[1] if key == ""nw_src"": field_value = test.nw_src_to_str(wildcards, field_value) elif key == ""nw_dst"": field_value = test.nw_dst_to_str(wildcards, field_value) else: field_value = match[key] return field_value",False,if member [ 0 ] == key :,"if member [ 0 ] == ""field"" :",0.8820016898747209,59.00468726392806
"def move_sender_strings_to_sender_model(apps, schema_editor): sender_model = apps.get_model(""documents"", ""Sender"") document_model = apps.get_model(""documents"", ""Document"") # Create the sender and log the relationship with the document for document in document_model.objects.all(): <mask>: ( DOCUMENT_SENDER_MAP[document.pk], created, ) = sender_model.objects.get_or_create( name=document.sender, defaults={""slug"": slugify(document.sender)} )",False,if document . sender :,if document . sender in DOCUMENT_SENDER_MAP :,0.8820016898747209,24.808415001701817
"def compute_output_shape(self, input_shape): if None not in input_shape[1:]: <mask>: total = np.prod(input_shape[2:4]) * self.num_anchors else: total = np.prod(input_shape[1:3]) * self.num_anchors return (input_shape[0], total, 4) else: return (input_shape[0], None, 4)",False,"if keras . backend . image_data_format ( ) == ""channels_first"" :",if self . num_anchors > 1 :,0.8820016898747209,1.9861872553779454
"def decompress(self, value): if value: if type(value) == PhoneNumber: <mask>: return [ ""+%d"" % value.country_code, national_significant_number(value), ] else: return value.split(""."") return [None, """"]",False,if value . country_code and value . national_number :,if value . country_code :,0.8820016898747209,35.685360466076496
"def ignore(self, other): if isinstance(other, Suppress): <mask>: super(ParseElementEnhance, self).ignore(other) if self.expr is not None: self.expr.ignore(self.ignoreExprs[-1]) else: super(ParseElementEnhance, self).ignore(other) if self.expr is not None: self.expr.ignore(self.ignoreExprs[-1]) return self",False,if other not in self . ignoreExprs :,"if isinstance ( other , ( list , tuple ) ) :",0.8820016898747209,4.456882760699063
"def mkdir(self, mode=0o777, parents=False, exist_ok=False): if self._closed: self._raise_closed() if not parents: try: self._accessor.mkdir(self, mode) except FileExistsError: <mask>: raise else: try: self._accessor.mkdir(self, mode) except FileExistsError: if not exist_ok or not self.is_dir(): raise except OSError as e: if e.errno != ENOENT: raise self.parent.mkdir(parents=True) self._accessor.mkdir(self, mode)",True,if not exist_ok or not self . is_dir ( ) :,if not exist_ok or not self . is_dir ( ) :,0.8820016898747209,100.00000000000004
"def _mark_lcs(mask, dirs, m, n): while m != 0 and n != 0: if dirs[m, n] == ""|"": m -= 1 n -= 1 mask[m] = 1 elif dirs[m, n] == ""^"": m -= 1 <mask>: n -= 1 else: raise UnboundLocalError(""Illegal move"") return mask",False,"elif dirs [ m , n ] == ""<"" :","elif dirs [ m , n ] == ""^"" :",0.8820016898747209,79.10665071754353
"def clean(self, *args, **kwargs): data = super().clean(*args, **kwargs) if isinstance(data, File): filename = data.name ext = os.path.splitext(filename)[1] ext = ext.lower() <mask>: raise forms.ValidationError(_(""Filetype not allowed!"")) return data",False,if ext not in self . ext_whitelist :,"if ext not in ( "".py"" , "".pyc"" ) :",0.8820016898747209,16.94357181593088
"def get_doc_object(obj, what=None): if what is None: if inspect.isclass(obj): what = ""class"" <mask>: what = ""module"" elif callable(obj): what = ""function"" else: what = ""object"" if what == ""class"": return SphinxClassDoc(obj, """", func_doc=SphinxFunctionDoc) elif what in (""function"", ""method""): return SphinxFunctionDoc(obj, """") else: return SphinxDocString(pydoc.getdoc(obj))",True,elif inspect . ismodule ( obj ) :,elif inspect . ismodule ( obj ) :,0.8820016898747209,100.00000000000004
"def apply_pssm(val): if val is not None: val_c = PSSM_VALUES.get(val, None) <mask>: assert isinstance( val, tuple(PSSM_VALUES.values()) ), ""'store_as' should be one of: %r or an instance of %r not %r"" % ( tuple(PSSM_VALUES.keys()), tuple(PSSM_VALUES.values()), val, ) return val return val_c()",True,if val_c is None :,if val_c is None :,0.8820016898747209,100.00000000000004
"def read_postmaster_opts(self): """"""returns the list of option names/values from postgres.opts, Empty dict if read failed or no file"""""" result = {} try: with open(os.path.join(self._postgresql.data_dir, ""postmaster.opts"")) as f: data = f.read() for opt in data.split('"" ""'): <mask>: name, val = opt.split(""="", 1) result[name.strip(""-"")] = val.rstrip('""\n') except IOError: logger.exception(""Error when reading postmaster.opts"") return result",False,"if ""="" in opt and opt . startswith ( ""--"" ) :","if ""="" in opt :",0.8820016898747209,23.246837589676872
"def detect(get_page): retval = False for vector in WAF_ATTACK_VECTORS: page, headers, code = get_page(get=vector) retval = ( re.search(r""F5-TrafficShield"", headers.get(HTTP_HEADER.SERVER, """"), re.I) is not None ) retval |= ( re.search(r""\AASINFO="", headers.get(HTTP_HEADER.SET_COOKIE, """"), re.I) is not None ) <mask>: break return retval",True,if retval :,if retval :,0.8820016898747209,0.0
"def on_task_start(self, task, config): for item in config: for plugin_name, plugin_config in item.items(): try: thelist = plugin.get(plugin_name, self).get_list(plugin_config) except AttributeError: raise PluginError( ""Plugin %s does not support list interface"" % plugin_name ) <mask>: raise plugin.PluginError(thelist.immutable)",True,if thelist . immutable :,if thelist . immutable :,0.8820016898747209,100.00000000000004
"def nq(t): p = t[0] if (t and t[0] in ""-+"") else """" t = t[len(p) :] if t.startswith(""tag:"") or t.startswith(""in:""): try: raw_tag = session.config.get_tag(t.split("":"")[1]) <mask>: t = ""in:%s"" % raw_tag.slug except (IndexError, KeyError, TypeError): pass return p + t",False,if raw_tag and raw_tag . hasattr ( slug ) :,if raw_tag :,0.8820016898747209,11.688396478408103
"def _recur_strip(s): if is_str(s): <mask>: return "" "".join(s.strip().split()) else: return "" "".join(s.strip().split()).replace(bos_token + "" "", """") else: s_ = [_recur_strip(si) for si in s] return _maybe_list_to_array(s_, s)",True,"if bos_token == """" :","if bos_token == """" :",0.8820016898747209,100.00000000000004
"def __delitem__(self, key): ""Deleting tag[key] deletes all 'key' attributes for the tag."" for item in self.attrs: <mask>: self.attrs.remove(item) # We don't break because bad HTML can define the same # attribute multiple times. self._getAttrMap() if self.attrMap.has_key(key): del self.attrMap[key]",False,if item [ 0 ] == key :,"if item . get ( ""key"" ) == key :",0.8820016898747209,22.997519112894437
"def comment_import_help(init_file, out_file): f_out = open(out_file, ""w"") output = """" updated = False with open(init_file, ""r"") as f_in: for line in f_in: <mask>: updated = True line = ""# "" + line output += line f_out.write(output) f_out.close() return updated",False,"if ""import"" in line and ""_help"" in line and not updated :","if line . startswith ( ""#"" ) :",0.8820016898747209,3.115901613796704
"def prepare_text(lines): out = [] for s in lines.split(""|""): s = s.strip() <mask>: # line beginning with '/' is in italics s = r""{\i1}%s{\i0}"" % s[1:].strip() out.append(s) return ""\\N"".join(out)",False,"if s . startswith ( ""/"" ) :","if s [ 0 ] == ""/"" :",0.8820016898747209,18.36028134946796
"def sqlctx(sc): pytest.importorskip(""pyspark"") from odo.backends.sparksql import HiveContext try: yield HiveContext(sc) finally: dbpath = ""metastore_db"" logpath = ""derby.log"" <mask>: assert os.path.isdir(dbpath) shutil.rmtree(dbpath) if os.path.exists(logpath): assert os.path.isfile(logpath) os.remove(logpath)",True,if os . path . exists ( dbpath ) :,if os . path . exists ( dbpath ) :,0.8820016898747209,100.00000000000004
"def _user2dict(self, uid): usdict = None if uid in self.users: usdict = self.users[uid] <mask>: infos = self.users_info[uid] for attr in infos: usdict[attr[""attr_type""]] = attr[""attr_data""] usdict[""uid""] = uid return usdict",True,if uid in self . users_info :,if uid in self . users_info :,0.8820016898747209,100.00000000000004
"def _validate_options(self): for option in self.options: # if value type is bool or int, then we know the options is set <mask>: if self.options.required[option] is True and not self.options[option]: if option == Constants.PASSWORD_CLEAR: option = ""password"".upper() raise FrameworkException( ""Value required for the '%s' option."" % (option.upper()) ) return",False,"if not type ( self . options [ option ] ) in [ bool , int ] :","if isinstance ( option , int ) :",0.8820016898747209,4.3281927222903835
"def _copy_package_apps( local_bin_dir: Path, app_paths: List[Path], suffix: str = """" ) -> None: for src_unresolved in app_paths: src = src_unresolved.resolve() app = src.name dest = Path(local_bin_dir / add_suffix(app, suffix)) <mask>: mkdir(dest.parent) if dest.exists(): logger.warning(f""{hazard} Overwriting file {str(dest)} with {str(src)}"") dest.unlink() if src.exists(): shutil.copy(src, dest)",False,if not dest . parent . is_dir ( ) :,if dest . exists ( ) :,0.8820016898747209,15.749996500436227
"def truncate_seq_pair(tokens_a, tokens_b, max_length): """"""Truncates a sequence pair in place to the maximum length."""""" # This is a simple heuristic which will always truncate the longer sequence # one token at a time. This makes more sense than truncating an equal percent # of tokens from each, since if one sequence is very short then each token # that's truncated likely contains more information than a longer sequence. while True: total_length = len(tokens_a) + len(tokens_b) <mask>: break if len(tokens_a) > len(tokens_b): tokens_a.pop() else: tokens_b.pop()",False,if total_length <= max_length :,if total_length > max_length :,0.8820016898747209,53.417359568998464
"def add_channels(cls, voucher, add_channels): for add_channel in add_channels: channel = add_channel[""channel""] defaults = {""currency"": channel.currency_code} if ""discount_value"" in add_channel.keys(): defaults[""discount_value""] = add_channel.get(""discount_value"") <mask>: defaults[""min_spent_amount""] = add_channel.get(""min_amount_spent"", None) models.VoucherChannelListing.objects.update_or_create( voucher=voucher, channel=channel, defaults=defaults, )",True,"if ""min_amount_spent"" in add_channel . keys ( ) :","if ""min_amount_spent"" in add_channel . keys ( ) :",0.8820016898747209,100.00000000000004
"def services(self, id=None, name=None): for service_dict in self.service_ls(id=id, name=name): service_id = service_dict[""ID""] service_name = service_dict[""NAME""] <mask>: continue task_list = self.service_ps(service_id) yield DockerService.from_cli(self, service_dict, task_list)",False,if not service_name . startswith ( self . _name_prefix ) :,"if service_name == ""docker"" :",0.8820016898747209,9.739982528168149
"def lll(dirname): for name in os.listdir(dirname): <mask>: full = os.path.join(dirname, name) if os.path.islink(full): print(name, ""->"", os.readlink(full))",False,"if name not in ( os . curdir , os . pardir ) :","if os . path . isfile ( os . path . join ( dirname , name ) ) :",0.8820016898747209,12.109013026441868
"def convertstore(self, mydict): targetheader = self.mypofile.header() targetheader.addnote(""extracted from web2py"", ""developer"") for source_str in mydict.keys(): target_str = mydict[source_str] <mask>: # a convention with new (untranslated) web2py files target_str = u"""" elif target_str.startswith(u""*** ""): # an older convention target_str = u"""" pounit = self.convertunit(source_str, target_str) self.mypofile.addunit(pounit) return self.mypofile",False,if target_str == source_str :,"if target_str . startswith ( u""*** "" ) :",0.8820016898747209,17.395797375642243
"def __init__(self, **kwargs): for k, v in kwargs.items(): setattr(self, k, v) self.attempted_charsets = set() request = cherrypy.serving.request if request.handler is not None: # Replace request.handler with self <mask>: cherrypy.log(""Replacing request.handler"", ""TOOLS.ENCODE"") self.oldhandler = request.handler request.handler = self",False,if self . debug :,if self . oldhandler != request . handler :,0.8820016898747209,16.784459625186194
"def _fastqc_data_section(self, section_name): out = [] in_section = False data_file = os.path.join(self._dir, ""fastqc_data.txt"") if os.path.exists(data_file): with open(data_file) as in_handle: for line in in_handle: if line.startswith("">>%s"" % section_name): in_section = True <mask>: if line.startswith("">>END""): break out.append(line.rstrip(""\r\n"")) return out",False,elif in_section :,if in_section :,0.8820016898747209,66.87403049764218
"def bit_length(n): try: return n.bit_length() except AttributeError: norm = deflate_long(n, False) hbyte = byte_ord(norm[0]) <mask>: return 1 bitlen = len(norm) * 8 while not (hbyte & 0x80): hbyte <<= 1 bitlen -= 1 return bitlen",True,if hbyte == 0 :,if hbyte == 0 :,0.8820016898747209,100.00000000000004
"def step(self, action): """"""Repeat action, sum reward, and max over last observations."""""" total_reward = 0.0 done = None for i in range(self._skip): obs, reward, done, info = self.env.step(action) if i == self._skip - 2: self._obs_buffer[0] = obs if i == self._skip - 1: self._obs_buffer[1] = obs total_reward += reward <mask>: break # Note that the observation on the done=True frame # doesn't matter max_frame = self._obs_buffer.max(axis=0) return max_frame, total_reward, done, info",True,if done :,if done :,0.8820016898747209,0.0
"def _sample_translation(reference, max_len): translation = reference[:] while np.random.uniform() < 0.8 and 1 < len(translation) < max_len: trans_len = len(translation) ind = np.random.randint(trans_len) action = np.random.choice(actions) <mask>: del translation[ind] elif action == ""replacement"": ind_rep = np.random.randint(trans_len) translation[ind] = translation[ind_rep] else: ind_insert = np.random.randint(trans_len) translation.insert(ind, translation[ind_insert]) return translation",False,"if action == ""deletion"" :","if action == ""delete"" :",0.8820016898747209,59.4603557501361
"def group_by_sign(seq, slop=sin(pi / 18), key=lambda x: x): sign = None subseq = [] for i in seq: ki = key(i) if sign is None: subseq.append(i) <mask>: sign = ki / abs(ki) else: subseq.append(i) if sign * ki < -slop: sign = ki / abs(ki) yield subseq subseq = [i] if subseq: yield subseq",False,if ki != 0 :,elif sign * ki < slop :,0.8820016898747209,7.809849842300637
def get_dirlist(_rootdir): dirlist = [] with os.scandir(_rootdir) as rit: for entry in rit: <mask>: dirlist.append(entry.path) dirlist += get_dirlist(entry.path) return dirlist,False,"if not entry . name . startswith ( ""."" ) and entry . is_dir ( ) :",if entry . is_dir ( ) :,0.8820016898747209,23.43746816281914
"def __init__( self, fixed: MQTTFixedHeader = None, variable_header: PublishVariableHeader = None, payload=None, ): if fixed is None: header = MQTTFixedHeader(PUBLISH, 0x00) else: <mask>: raise HBMQTTException( ""Invalid fixed packet type %s for PublishPacket init"" % fixed.packet_type ) header = fixed super().__init__(header) self.variable_header = variable_header self.payload = payload",False,if fixed . packet_type is not PUBLISH :,"if fixed . packet_type not in ( PUBLISH , PUBLISH_PACKET ) :",0.8820016898747209,33.34477432809603
"def get_files(d): res = [] for p in glob.glob(os.path.join(d, ""*"")): if not p: continue (pth, fname) = os.path.split(p) <mask>: continue if fname == ""PureMVC_Python_1_0"": continue if fname[-4:] == "".pyc"": # ehmm.. no. continue if os.path.isdir(p): get_dir(p) else: res.append(p) return res",False,"if fname == ""output"" :","if fname == ""PureMVC_Python_1_0"" :",0.8820016898747209,30.576902884505124
"def reward(self): """"""Returns a tuple of sum of raw and processed rewards."""""" raw_rewards, processed_rewards = 0, 0 for ts in self.time_steps: # NOTE: raw_reward and processed_reward are None for the first time-step. <mask>: raw_rewards += ts.raw_reward if ts.processed_reward is not None: processed_rewards += ts.processed_reward return raw_rewards, processed_rewards",True,if ts . raw_reward is not None :,if ts . raw_reward is not None :,0.8820016898747209,100.00000000000004
"def _process_file(self, content): args = [] for line in content.splitlines(): line = line.strip() <mask>: args.extend(self._split_option(line)) elif line and not line.startswith(""#""): args.append(line) return args",False,"if line . startswith ( ""-"" ) :","if line and not line . startswith ( ""#"" ) :",0.8820016898747209,41.397900200299425
"def __on_change_button_clicked(self, widget=None): """"""compute all primary objects and toggle the 'Change' attribute"""""" self.change_status = not self.change_status for prim_obj, tmp in self.xobjects: obj_change = self.top.get_object(""%s_change"" % prim_obj) <mask>: continue self.change_entries[prim_obj].set_val(self.change_status) obj_change.set_active(self.change_status)",False,if not obj_change . get_sensitive ( ) :,if not obj_change :,0.8820016898747209,29.256375127882833
"def aiter_cogs(cls) -> AsyncIterator[Tuple[str, str]]: yield ""Core"", ""0"" for _dir in data_manager.cog_data_path().iterdir(): fpath = _dir / ""settings.json"" if not fpath.exists(): continue with fpath.open() as f: try: data = json.load(f) except json.JSONDecodeError: continue <mask>: continue cog_name = _dir.stem for cog_id, inner in data.items(): if not isinstance(inner, dict): continue yield cog_name, cog_id",True,"if not isinstance ( data , dict ) :","if not isinstance ( data , dict ) :",0.8820016898747209,100.00000000000004
"def _verifySubs(self): for inst in self.subs: if not isinstance(inst, (_Block, _Instantiator, Cosimulation)): raise BlockError(_error.ArgType % (self.name,)) if isinstance(inst, (_Block, _Instantiator)): <mask>: raise BlockError(_error.InstanceError % (self.name, inst.callername))",False,if not inst . modctxt :,"if not isinstance ( inst , _Block ) :",0.8820016898747209,9.980099403873663
"def _is_xml(accepts): if accepts.startswith(b""application/""): has_xml = accepts.find(b""xml"") if has_xml > 0: semicolon = accepts.find(b"";"") <mask>: return True return False",False,if semicolon < 0 or has_xml < semicolon :,if semicolon > 0 :,0.8820016898747209,7.652332131360532
"def _accept_with(cls, orm, target): if target is orm.mapper: return mapperlib.Mapper elif isinstance(target, type): <mask>: return target else: mapper = _mapper_or_none(target) if mapper is not None: return mapper else: return _MapperEventsHold(target) else: return target",False,"if issubclass ( target , mapperlib . Mapper ) :",if target . __class__ is orm . mapper :,0.8820016898747209,4.368583925857938
"def _get_font_afm(self, prop): key = hash(prop) font = self.afmfontd.get(key) <mask>: fname = findfont(prop, fontext=""afm"") font = self.afmfontd.get(fname) if font is None: font = AFM(file(findfont(prop, fontext=""afm""))) self.afmfontd[fname] = font self.afmfontd[key] = font return font",True,if font is None :,if font is None :,0.8820016898747209,100.00000000000004
"def __call__(self, groupby): normalize_reduction_funcs(self, ndim=groupby.ndim) df = groupby while df.op.output_types[0] not in (OutputType.dataframe, OutputType.series): df = df.inputs[0] if self.raw_func == ""size"": self.output_types = [OutputType.series] else: self.output_types = ( [OutputType.dataframe] <mask>: else [OutputType.series] ) if self.output_types[0] == OutputType.dataframe: return self._call_dataframe(groupby, df) else: return self._call_series(groupby, df)",False,if groupby . op . output_types [ 0 ] == OutputType . dataframe_groupby,"if self . raw_func == ""size""",0.8820016898747209,4.987920159185045
"def save(self): if self.preferences.get(ENCRYPT_ON_DISK, False): <mask>: return self.storage.write( self.to_dict(encrypt_password=self.encryption_password) ) elif not self.is_locked: log.warning( ""Disk encryption requested but no password available for encryption. "" ""Resetting encryption preferences and saving wallet in an unencrypted state."" ) self.preferences[ENCRYPT_ON_DISK] = False return self.storage.write(self.to_dict())",False,if self . encryption_password is not None :,if self . encryption_password :,0.8820016898747209,54.77927682341229
"def isValidDateString(config_param_name, value, valid_value): try: if value == ""DD-MM-YYYY"": return value day, month, year = value.split(""-"") if int(day) < 1 or int(day) > 31: raise DateStringValueError(config_param_name, value) <mask>: raise DateStringValueError(config_param_name, value) if int(year) < 1900 or int(year) > 2013: raise DateStringValueError(config_param_name, value) return value except Exception: raise DateStringValueError(config_param_name, value)",False,if int ( month ) < 1 or int ( month ) > 12 :,if int ( month ) < 18 or int ( month ) > 18 :,0.8820016898747209,66.06328636027618
"def _capture(self, call_name, data=None, **kwargs): if data is None: data = self.get_default_context() else: default_context = self.get_default_context() <mask>: default_context.update(data) else: default_context[""extra""][""extra_data""] = data data = default_context client = self.get_sentry_client() return getattr(client, call_name)(data=data, **kwargs)",True,"if isinstance ( data , dict ) :","if isinstance ( data , dict ) :",0.8820016898747209,100.00000000000004
"def check(input, expected_output=None, expected_ffi_error=False): import _cffi_backend ffi = _cffi_backend.FFI() if not expected_ffi_error: ct = ffi.typeof(input) assert isinstance(ct, ffi.CType) assert ct.cname == (expected_output or input) else: e = py.test.raises(ffi.error, ffi.typeof, input) <mask>: assert str(e.value) == expected_ffi_error",False,"if isinstance ( expected_ffi_error , str ) :",if e . value :,0.8820016898747209,3.1325998243558226
"def run(self): """"""Process queries from task queue, stop if processor is None."""""" while True: try: processor, iprot, oprot, otrans, callback = self.queue.get() <mask>: break processor.process(iprot, oprot) callback(True, otrans.getvalue()) except Exception: logging.exception(""Exception while processing request"") callback(False, """")",True,if processor is None :,if processor is None :,0.8820016898747209,100.00000000000004
"def search(self, query): query = query.strip().lower() results = [] for provider in SidebarItemProvider.all(self.context): for item in provider.provide(): if ""url"" in item: search_source = ""$"".join( [item.get(""id"", """"), item.get(""name"", """")] ).lower() <mask>: results.append( { ""title"": item[""name""], ""icon"": item[""icon""], ""url"": item[""url""], } ) return results",False,if query in search_source :,if query . match ( search_source ) :,0.8820016898747209,20.556680845025987
"def handle(self) -> None: """"""Handles a request ignoring dropped connections."""""" try: BaseHTTPRequestHandler.handle(self) except (ConnectionError, socket.timeout) as e: self.connection_dropped(e) except Exception as e: <mask>: self.log_error(""SSL error occurred: %s"", e) else: raise if self.server.shutdown_signal: self.initiate_shutdown()",False,if self . server . ssl_context is not None and is_ssl_error ( e ) :,if self . server . ssl_error :,0.8820016898747209,22.165788851265287
"def cdn_url_handler(error, endpoint, kwargs): if endpoint == ""cdn"": path = kwargs.pop(""path"") # cdn = app.config.get('cdn', 'http://cdn.staticfile.org/') # cdn = app.config.get('cdn', '//cdnjs.cloudflare.com/ajax/libs/') cdn = app.config.get(""cdn"", ""//cdnjscn.b0.upaiyun.com/libs/"") return urljoin(cdn, path) else: exc_type, exc_value, tb = sys.exc_info() <mask>: reraise(exc_type, exc_value, tb) else: raise error",False,if exc_value is error :,if exc_type :,0.8820016898747209,28.641904579795423
"def pairs(self): for path in os.listdir(""src""): if path == "".svn"": continue dep = join(""src"", path) <mask>: continue yield dep, join(build_dir, path)",False,if isdir ( dep ) :,if not os . path . exists ( dep ) :,0.8820016898747209,24.808415001701817
"def get_condition(self): """"""Return the condition element's name."""""" for child in self.xml: <mask>: cond = child.tag.split(""}"", 1)[-1] if cond in self.conditions: return cond return ""not-authorized""",False,"if ""{%s}"" % self . namespace in child . tag :","if child . tag . startswith ( ""condition"" ) :",0.8820016898747209,11.542597701287805
"def end(self, tag): # call the appropriate end tag handler try: f = self.dispatch[tag] except KeyError: <mask>: return # unknown tag ? try: f = self.dispatch[tag.split("":"")[-1]] except KeyError: return # unknown tag ? return f(self, """".join(self._data))",False,"if "":"" not in tag :","if tag . startswith ( ""end:"" ) :",0.8820016898747209,9.864703138979419
"def checkIfSessionCodeExists(self, sessionCode): if self.emrtFile: sessionsForExperiment = ( self.emrtFile.root.data_collection.session_meta_data.where( ""experiment_id == %d"" % (self.active_experiment_id,) ) ) sessionCodeMatch = [ sess for sess in sessionsForExperiment if sess[""code""] == sessionCode ] <mask>: return True return False",False,if len ( sessionCodeMatch ) > 0 :,if sessionCodeMatch :,0.8820016898747209,0.0
"def save_bytearray(self, obj): if self.proto < 5: <mask>: # bytearray is empty self.save_reduce(bytearray, (), obj=obj) else: self.save_reduce(bytearray, (bytes(obj),), obj=obj) return n = len(obj) if n >= self.framer._FRAME_SIZE_TARGET: self._write_large_bytes(BYTEARRAY8 + pack(""<Q"", n), obj) else: self.write(BYTEARRAY8 + pack(""<Q"", n) + obj)",True,if not obj :,if not obj :,0.8820016898747209,100.00000000000004
"def _restore_freeze(self, new): size_change = [] for k, v in six.iteritems(self._freeze_backup): newv = new.get(k, []) <mask>: size_change.append((self._key_name(k), len(v), len(newv))) if size_change: logger.info( ""These collections were modified but restored in {}: {}"".format( self._name, "", "".join(map(lambda t: ""({}: {}->{})"".format(*t), size_change)), ) ) restore_collection(self._freeze_backup)",False,if len ( v ) != len ( newv ) :,if v != newv :,0.8820016898747209,7.859505256643253
"def check_options(self, expr, evaluation, options): for key in options: if key != ""System`SameTest"": <mask>: evaluation.message(""ContainsOnly"", ""optx"", Symbol(key)) else: return evaluation.message(""ContainsOnly"", ""optx"", Symbol(key), expr) return None",False,if expr is None :,"if key == ""System`SameTest"" :",0.8820016898747209,4.990049701936832
"def bundle_directory(self, dirpath): """"""Bundle all modules/packages in the given directory."""""" dirpath = os.path.abspath(dirpath) for nm in os.listdir(dirpath): nm = _u(nm) if nm.startswith("".""): continue itempath = os.path.join(dirpath, nm) if os.path.isdir(itempath): <mask>: self.bundle_package(itempath) elif nm.endswith("".py""): self.bundle_module(itempath)",False,"if os . path . exists ( os . path . join ( itempath , ""__init__.py"" ) ) :","if nm . endswith ( "".py"" ) :",0.8820016898747209,7.21025732010192
"def _read_block(self, size): if self._file_end is not None: max_size = self._file_end - self._file.tell() <mask>: size = max_size size = max(min(size, max_size), 0) return self._file.read(size)",False,if size == - 1 :,if size < 0 :,0.8820016898747209,15.848738972120703
"def question_mark(self): """"""Shows help for this command and it's sub-commands."""""" ret = [] if self.param_help_msg or len(self.subcommands) == 0: ret.append(self._quick_help()) if len(self.subcommands) > 0: for k, _ in sorted(self.subcommands.items()): command_path, param_help, cmd_help = self._instantiate_subcommand( k )._quick_help(nested=True) <mask>: ret.append((command_path, param_help, cmd_help)) return (CommandsResponse(STATUS_OK, self.help_formatter(ret)), self.__class__)",False,if command_path or param_help or cmd_help :,if cmd_help :,0.8820016898747209,14.276239697197271
"def list_domains(self, r53, **kwargs): marker = None domains = [] while True: if marker: response = self.wrap_aws_rate_limited_call(r53.list_domains(Marker=marker)) else: response = self.wrap_aws_rate_limited_call(r53.list_domains) for domain in response.get(""Domains""): domains.append(domain) <mask>: marker = response.get(""NextPageMarker"") else: break return domains",True,"if response . get ( ""NextPageMarker"" ) :","if response . get ( ""NextPageMarker"" ) :",0.8820016898747209,100.00000000000004
"def writer(stream, items): sep = """" for item in items: stream.write(sep) sep = "" "" if not isinstance(item, str): item = str(item) if not PY3K: <mask>: item = str(item) stream.write(item) stream.write(""\n"")",False,"if not isinstance ( item , unicode ) :","if not isinstance ( item , str ) :",0.8820016898747209,66.06328636027612
"def f(view, s): if mode == modes.INTERNAL_NORMAL: view.run_command(""toggle_comment"") <mask>: pt = utils.next_non_white_space_char(view, s.a, white_space="" \t"") else: pt = utils.next_non_white_space_char( view, self.view.line(s.a).a, white_space="" \t"" ) return R(pt, pt) return s",False,"if utils . row_at ( self . view , s . a ) != utils . row_at ( self . view , self . view . size ( ) ) :",if self . view . is_comment ( s . a ) :,0.8820016898747209,8.496072919568759
"def _parse_timestamp(value): if value: match = _TIMESTAMP_PATTERN.match(value) <mask>: if match.group(2): format = ""%Y-%m-%d %H:%M:%S.%f"" # use the pattern to truncate the value value = match.group() else: format = ""%Y-%m-%d %H:%M:%S"" value = datetime.datetime.strptime(value, format) else: raise Exception('Cannot convert ""{}"" into a datetime'.format(value)) else: value = None return value",True,if match :,if match :,0.8820016898747209,0.0
"def _compute_log_r(model_trace, guide_trace): log_r = MultiFrameTensor() stacks = get_plate_stacks(model_trace) for name, model_site in model_trace.nodes.items(): if model_site[""type""] == ""sample"": log_r_term = model_site[""log_prob""] <mask>: log_r_term = log_r_term - guide_trace.nodes[name][""log_prob""] log_r.add((stacks[name], log_r_term.detach())) return log_r",False,"if not model_site [ ""is_observed"" ] :","elif model_site [ ""type"" ] == ""step"" :",0.8820016898747209,29.256127307315065
"def get_translationproject(self): """"""returns the translation project belonging to this directory."""""" if self.is_language() or self.is_project(): return None else: <mask>: return self.translationproject else: aux_dir = self while not aux_dir.is_translationproject() and aux_dir.parent is not None: aux_dir = aux_dir.parent return aux_dir.translationproject",False,if self . is_translationproject ( ) :,if self . translationproject is not None :,0.8820016898747209,21.573652645054953
"def get_hosted_content(): try: scheme, rest = target.split(""://"", 1) prefix, host_and_port = rest.split("".interactivetool."") faked_host = rest <mask>: faked_host = rest.split(""/"", 1)[0] url = ""%s://%s"" % (scheme, host_and_port) response = requests.get(url, timeout=1, headers={""Host"": faked_host}) return response.text except Exception as e: print(e) return None",False,"if ""/"" in rest :","if prefix == ""http"" :",0.8820016898747209,7.809849842300637
"def install(self): log.info(self.openssl_cli) if not self.has_openssl or self.args.force: <mask>: self._download_src() else: log.debug(""Already has src {}"".format(self.src_file)) self._unpack_src() self._build_src() self._make_install() else: log.info(""Already has installation {}"".format(self.install_dir)) # validate installation version = self.openssl_version if self.version not in version: raise ValueError(version)",False,if not self . has_src :,if self . src_file :,0.8820016898747209,14.984885384519536
"def format(self, formatstr): pieces = [] for i, piece in enumerate(re_formatchars.split(force_text(formatstr))): if i % 2: pieces.append(force_text(getattr(self, piece)())) <mask>: pieces.append(re_escaped.sub(r""\1"", piece)) return """".join(pieces)",False,elif piece :,elif i % 3 :,0.8820016898747209,12.703318703865365
"def get_current_events_users(calendar): now = timezone.make_aware(datetime.now(), timezone.get_current_timezone()) result = [] day = Day(calendar.events.all(), now) for o in day.get_occurrences(): <mask>: usernames = o.event.title.split("","") for username in usernames: result.append(User.objects.get(username=username.strip())) return result",False,if o . start <= now <= o . end :,if o . event . title :,0.8820016898747209,11.787460936700446
"def from_cfn_params(self, cfn_params): """"""Initialize param value by parsing CFN input only if the scheduler is awsbatch."""""" cfn_converter = self.definition.get(""cfn_param_mapping"", None) if cfn_converter and cfn_params: <mask>: # we have the same CFN input parameters for both spot_price and spot_bid_percentage # so the CFN input could be a float self.value = int(float(get_cfn_param(cfn_params, cfn_converter))) return self",False,"if get_cfn_param ( cfn_params , ""Scheduler"" ) == ""awsbatch"" :","if cfn_params . get ( ""spot_price"" , None ) :",0.8820016898747209,9.419061411983327
"def onCompletion(self, text): res = [] for l in text.split(""\n""): <mask>: continue l = l.split("":"") if len(l) != 2: continue res.append([l[0].strip(), l[1].strip()]) self.panel.setChapters(res)",False,if not l :,if len ( l ) != 2 :,0.8820016898747209,6.27465531099474
"def update_ranges(l, i): for _range in l: # most common case: extend a range if i == _range[0] - 1: _range[0] = i merge_ranges(l) return <mask>: _range[1] = i merge_ranges(l) return # somewhere outside of range proximity l.append([i, i]) l.sort(key=lambda x: x[0])",False,elif i == _range [ 1 ] + 1 :,if i == _range [ 1 ] - 1 :,0.8820016898747209,67.0422683816333
"def process_dollar(token, state, command_line): if not state.is_range_start_line_parsed: <mask>: raise ValueError(""bad range: {0}"".format(state.scanner.state.source)) command_line.line_range.start.append(token) else: if command_line.line_range.end: raise ValueError(""bad range: {0}"".format(state.scanner.state.source)) command_line.line_range.end.append(token) return parse_line_ref, command_line",True,if command_line . line_range . start :,if command_line . line_range . start :,0.8820016898747209,100.00000000000004
"def _parse_description(self, text: str): result = dict(links=[], versions=[]) for line in text.splitlines(): clean = REX_TAG.sub("""", line.strip()) <mask>: result[""severity""] = clean.split()[1] continue if clean.startswith(""Affects:""): result[""name""] = clean.split()[1] continue if "" or higher"" in clean: result[""versions""] = self._get_versions(clean) result[""links""].extend(REX_LINK.findall(line)) return result",True,"if clean . startswith ( ""Severity:"" ) :","if clean . startswith ( ""Severity:"" ) :",0.8820016898747209,100.00000000000004
"def apply(self, chart, grammar): for prod in grammar.productions(empty=True): for index in compat.xrange(chart.num_leaves() + 1): new_edge = TreeEdge.from_production(prod, index) <mask>: yield new_edge",False,"if chart . insert ( new_edge , ( ) ) :",if new_edge . is_valid ( ) :,0.8820016898747209,17.676084425360003
"def calc(self, arg): op = arg[""op""] if op == ""C"": self.clear() return str(self.current) num = decimal.Decimal(arg[""num""]) if self.op: <mask>: self.current += num elif self.op == ""-"": self.current -= num elif self.op == ""*"": self.current *= num elif self.op == ""/"": self.current /= num self.op = op else: self.op = op self.current = num res = str(self.current) if op == ""="": self.clear() return res",True,"if self . op == ""+"" :","if self . op == ""+"" :",0.8820016898747209,100.00000000000004
"def cascade(self, event=None): """"""Cascade all Leo windows."""""" x, y, delta = 50, 50, 50 for frame in g.app.windowList: w = frame and frame.top if w: r = w.geometry() # a Qt.Rect # 2011/10/26: Fix bug 823601: cascade-windows fails. w.setGeometry(QtCore.QRect(x, y, r.width(), r.height())) # Compute the new offsets. x += 30 y += 30 <mask>: x = 10 + delta y = 40 + delta delta += 10",False,if x > 200 :,if x > 40 :,0.8820016898747209,42.72870063962342
"def redirect(self): c = self.c if c.config.getBool(""eval-redirect""): self.old_stderr = g.stdErrIsRedirected() self.old_stdout = g.stdOutIsRedirected() <mask>: g.redirectStderr() if not self.old_stdout: g.redirectStdout()",True,if not self . old_stderr :,if not self . old_stderr :,0.8820016898747209,100.00000000000004
"def on_event(self, c, button, data): if self.rvGestureGrab.get_reveal_child(): <mask>: self.use() elif button == ""Y"" and data[0] == 0: self.start_over()",False,"if button == ""A"" and data [ 0 ] == 0 :","if button == ""X"" and data [ 0 ] == 1 :",0.8820016898747209,68.65065103648593
"def __init__(self, in_feats, out_feats, norm=""both"", bias=True, activation=None): super(DenseGraphConv, self).__init__() self._in_feats = in_feats self._out_feats = out_feats self._norm = norm with self.name_scope(): self.weight = self.params.get( ""weight"", shape=(in_feats, out_feats), init=mx.init.Xavier(magnitude=math.sqrt(2.0)), ) <mask>: self.bias = self.params.get(""bias"", shape=(out_feats,), init=mx.init.Zero()) else: self.bias = None self._activation = activation",True,if bias :,if bias :,0.8820016898747209,0.0
"def _import_top_module(self, name): # scan sys.path looking for a location in the filesystem that contains # the module, or an Importer object that can import the module. for item in sys.path: if isinstance(item, _StringType): module = self.fs_imp.import_from_dir(item, name) else: module = item.import_top(name) <mask>: return module return None",False,if module :,if module is not None :,0.8820016898747209,17.965205598154213
"def resolver(schemas, f): if not callable(f): return if not hasattr(f, ""accepts""): return new_params = [] for p in f.accepts: <mask>: new_params.append(p.resolve(schemas)) else: raise ResolverError(""Invalid parameter definition {0}"".format(p)) # FIXME: for some reason assigning params (f.accepts = new_params) does not work f.accepts.clear() f.accepts.extend(new_params)",False,"if isinstance ( p , ( Patch , Ref , Attribute ) ) :","if isinstance ( p , Resolver ) :",0.8820016898747209,28.08708327044616
"def get_files(d): res = [] for p in glob.glob(os.path.join(d, ""*"")): if not p: continue (pth, fname) = os.path.split(p) if fname == ""output"": continue <mask>: continue if fname[-4:] == "".pyc"": # ehmm.. no. continue if os.path.isdir(p): get_dir(p) else: res.append(p) return res",False,"if fname == ""PureMVC_Python_1_0"" :","if fname . startswith ( ""pyc"" ) :",0.8820016898747209,7.073666451977357
"def _addRightnames(groups, kerning, leftname, rightnames, includeAll=True): if leftname in kerning: for rightname in kerning[leftname]: if rightname[0] == ""@"": for rightname2 in groups[rightname]: rightnames.add(rightname2) <mask>: # TODO: in this case, pick the one rightname that has the highest # ranking in glyphorder break else: rightnames.add(rightname)",False,if not includeAll :,elif includeAll :,0.8820016898747209,0.0
"def migrate_Stats(self): for old_obj in self.session_old.query(self.model_from[""Stats""]): <mask>: self.entries_count[""Stats""] -= 1 continue new_obj = self.model_to[""Stats""]() for key in new_obj.__table__.columns._data.keys(): if key not in old_obj.__table__.columns: continue setattr(new_obj, key, getattr(old_obj, key)) self.session_new.add(new_obj)",False,if not old_obj . summary :,if old_obj . __table__ . columns . _data . keys ( ) :,0.8820016898747209,13.264759167412226
"def _readenv(var, msg): match = _ENV_VAR_PAT.match(var) if match and match.groups(): envvar = match.groups()[0] <mask>: value = os.environ[envvar] if six.PY2: value = value.decode(""utf8"") return value else: raise InvalidConfigException( ""{} - environment variable '{}' not set"".format(msg, var) ) else: raise InvalidConfigException( ""{} - environment variable name '{}' does not match pattern '{}'"".format( msg, var, _ENV_VAR_PAT_STR ) )",True,if envvar in os . environ :,if envvar in os . environ :,0.8820016898747209,100.00000000000004
"def __next__(self): self._parse_reset() while True: try: line = next(self.input_iter) except StopIteration: # End of input OR exception <mask>: raise Error(""newline inside string"") raise self.line_num += 1 if ""\0"" in line: raise Error(""line contains NULL byte"") pos = 0 while pos < len(line): pos = self._parse_process_char(line, pos) self._parse_eol() if self.state == self.START_RECORD: break fields = self.fields self.fields = [] return fields",False,if len ( self . field ) > 0 :,if self . line_num >= self . line_len :,0.8820016898747209,7.474875887495341
"def createFields(self): while self.current_size < self.size: pos = self.stream.searchBytes( ""\0\0\1"", self.current_size, self.current_size + 1024 * 1024 * 8 ) # seek forward by at most 1MB <mask>: padsize = pos - self.current_size if padsize: yield PaddingBytes(self, ""pad[]"", padsize // 8) chunk = Chunk(self, ""chunk[]"") try: # force chunk to be processed, so that CustomFragments are complete chunk[""content/data""] except: pass yield chunk",False,if pos is not None :,if pos :,0.8820016898747209,0.0
"def spew(): seenUID = False start() for part in query: if part.type == ""uid"": seenUID = True <mask>: yield self.spew_body(part, id, msg, write, flush) else: f = getattr(self, ""spew_"" + part.type) yield f(id, msg, write, flush) if part is not query[-1]: space() if uid and not seenUID: space() yield self.spew_uid(id, msg, write, flush) finish() flush()",False,"if part . type == ""body"" :",if body :,0.8820016898747209,0.0
"def _limit_value(key, value, config): if config[key].get(""upper_limit""): limit = config[key][""upper_limit""] # auto handle datetime if isinstance(value, datetime) and isinstance(limit, timedelta): if config[key][""inverse""] is True: if (datetime.now() - limit) > value: value = datetime.now() - limit else: <mask>: value = datetime.now() + limit elif value > limit: value = limit return value",False,if ( datetime . now ( ) + limit ) < value :,if datetime . now ( ) + limit < value :,0.8820016898747209,59.91417036512659
"def _fix_var_naming(operators, names, mod=""input""): new_names = [] map = {} for op in operators: if mod == ""input"": iter = op.inputs else: iter = op.outputs for i in iter: for name in names: if i.raw_name == name and name not in map: map[i.raw_name] = i.full_name <mask>: break for name in names: new_names.append(map[name]) return new_names",False,if len ( map ) == len ( names ) :,if map [ i . raw_name ] == name :,0.8820016898747209,7.768562846380176
"def traverse(tree): """"""Generator dropping comment nodes"""""" for entry in tree: # key, values = entry spaceless = [e for e in entry if not nginxparser.spacey(e)] if spaceless: key = spaceless[0] values = spaceless[1] if len(spaceless) > 1 else None else: key = values = """" if isinstance(key, list): new = copy.deepcopy(entry) new[1] = filter_comments(values) yield new else: <mask>: yield spaceless",False,"if key != ""#"" and spaceless :","if isinstance ( key , list ) :",0.8820016898747209,5.660233915657916
"def mergeCombiners(self, x, y): for item in y: <mask>: self.heap.push(x, item) else: self.heap.push_pop(x, item) return x",False,if len ( x ) < self . heap_limit :,"if isinstance ( item , ( list , tuple ) ) :",0.8820016898747209,4.789232204309912
"def test_scatter(self, harness: primitive_harness.Harness): f_name = harness.params[""f_lax""].__name__ dtype = harness.params[""dtype""] if jtu.device_under_test() == ""tpu"": <mask>: raise unittest.SkipTest(f""TODO: complex {f_name} on TPU fails in JAX"") self.ConvertAndCompare(harness.dyn_fun, *harness.dyn_args_maker(self.rng()))",False,"if dtype is np . complex64 and f_name in [ ""scatter_min"" , ""scatter_max"" ] :","if dtype != ""complex"" :",0.8820016898747209,1.6586964297308333
"def TryMerge(self, decoder): while decoder.avail() > 0: tag = decoder.getVarInt32() if tag == TAG_BEGIN_ITEM_GROUP: (type_id, message) = Item.Decode(decoder) <mask>: self.items[type_id].MergeFrom(Item(message)) else: self.items[type_id] = Item(message) continue if tag == 0: raise ProtocolBuffer.ProtocolBufferDecodeError decoder.skipData(tag)",True,if type_id in self . items :,if type_id in self . items :,0.8820016898747209,100.00000000000004
"def process_continuations(lines): global continuation_pattern olines = [] while len(lines) != 0: line = no_comments(lines[0]) line = line.strip() lines.pop(0) if line == """": continue <mask>: # combine this line with the next line if the next line exists line = continuation_pattern.sub("""", line) if len(lines) >= 1: combined_lines = [line + lines[0]] lines.pop(0) lines = combined_lines + lines continue olines.append(line) del lines return olines",False,if continuation_pattern . search ( line ) :,if continuation_pattern :,0.8820016898747209,26.013004751144457
"def _getListNextPackagesReadyToBuild(): for pkg in Scheduler.listOfPackagesToBuild: if pkg in Scheduler.listOfPackagesCurrentlyBuilding: continue <mask>: Scheduler.listOfPackagesNextToBuild.put((-Scheduler._getPriority(pkg), pkg)) Scheduler.logger.debug(""Adding "" + pkg + "" to the schedule list"")",False,if constants . rpmCheck or Scheduler . _checkNextPackageIsReadyToBuild ( pkg ) :,if pkg not in Scheduler . listOfPackagesNextToBuild :,0.8820016898747209,7.433761660133445
"def process_signature(app, what, name, obj, options, signature, return_annotation): if signature: # replace Mock function names signature = re.sub(""<Mock name='([^']+)'.*>"", ""\g<1>"", signature) signature = re.sub(""tensorflow"", ""tf"", signature) # add scope name to layer signatures: if hasattr(obj, ""use_scope""): if obj.use_scope: signature = signature[0] + ""variable_scope_name, "" + signature[1:] <mask>: signature = signature[0] + ""[variable_scope_name,] "" + signature[1:] # signature: arg list return signature, return_annotation",False,elif obj . use_scope is None :,elif obj . use_scope_list :,0.8820016898747209,61.04735835807847
"def find_distribution_modules(name=__name__, file=__file__): current_dist_depth = len(name.split(""."")) - 1 current_dist = os.path.join( os.path.dirname(file), *([os.pardir] * current_dist_depth) ) abs = os.path.abspath(current_dist) dist_name = os.path.basename(abs) for dirpath, dirnames, filenames in os.walk(abs): package = (dist_name + dirpath[len(abs) :]).replace(""/"", ""."") <mask>: yield package for filename in filenames: if filename.endswith("".py"") and filename != ""__init__.py"": yield ""."".join([package, filename])[:-3]",False,"if ""__init__.py"" in filenames :","if package . endswith ( "".py"" ) :",0.8820016898747209,13.832283585102266
"def transform_value(i, v, *args): if i not in converter_functions: # no converter defined on this field, return value as-is return v else: try: return converter_functions[i](v, *args) except Exception as e: if failonerror == ""inline"": return e <mask>: raise e else: return errorvalue",False,elif failonerror :,"elif failonerror == ""error"" :",0.8820016898747209,12.22307556087252
"def _get_file(self): if self._file is None: self._file = SpooledTemporaryFile( max_size=self._storage.max_memory_size, suffix="".S3Boto3StorageFile"", dir=setting(""FILE_UPLOAD_TEMP_DIR""), ) <mask>: self._is_dirty = False self.obj.download_fileobj(self._file) self._file.seek(0) if self._storage.gzip and self.obj.content_encoding == ""gzip"": self._file = GzipFile(mode=self._mode, fileobj=self._file, mtime=0.0) return self._file",False,"if ""r"" in self . _mode :",if self . _is_dirty :,0.8820016898747209,18.190371142855746
"def connect(self, host, port, timeout): fp = Telnet() for i in range(50): try: fp.sock = socket.create_connection( (host, int(port)), timeout=int(timeout), source_address=("""", 1023 - i) ) break except socket.error as e: <mask>: raise e self.need_handshake = True return TCP_Connection(fp)",False,"if ( e . errno , e . strerror ) != ( 98 , ""Address already in use"" ) :",if e . errno != errno . EADDRINUSE :,0.8820016898747209,6.0201158791802785
"def filtercomments(source): """"""NOT USED: strips trailing comments and put them at the top."""""" trailing_comments = [] comment = True while comment: if re.search(r""^\s*\/\*"", source): comment = source[0, source.index(""*/"") + 2] <mask>: comment = re.search(r""^\s*\/\/"", source).group(0) else: comment = None if comment: source = re.sub(r""^\s+"", """", source[len(comment) :]) trailing_comments.append(comment) return ""\n"".join(trailing_comments) + source",False,"elif re . search ( r""^\s*\/\/"" , source ) :","elif re . search ( r""^\s*\/"" , source ) :",0.8820016898747209,87.95372791202111
"def yview(self, mode=None, value=None, units=None): if type(value) == str: value = float(value) if mode is None: return self.vsb.get() elif mode == ""moveto"": frameHeight = self.innerframe.winfo_reqheight() self._startY = value * float(frameHeight) else: # mode == 'scroll' clipperHeight = self._clipper.winfo_height() <mask>: jump = int(clipperHeight * self._jfraction) else: jump = clipperHeight self._startY = self._startY + value * jump self.reposition()",False,"if units == ""units"" :",if self . _jfraction :,0.8820016898747209,6.916271812933183
"def visit(stmt): """"""Collect information about VTCM buffers and their alignments."""""" if isinstance(stmt, tvm.tir.AttrStmt): if stmt.attr_key == ""storage_scope"" and stmt.value == ""local.vtcm"": vtcm_buffers.append(stmt.node) <mask>: if not stmt.node in alignments: alignments[stmt.node] = [] alignments[stmt.node].append(stmt.value)",False,"elif stmt . attr_key == ""storage_alignment"" :","elif stmt . attr_key == ""alignments"" :",0.8820016898747209,65.10803637373398
"def cost(P): # wda loss loss_b = 0 loss_w = 0 for i, xi in enumerate(xc): xi = np.dot(xi, P) for j, xj in enumerate(xc[i:]): xj = np.dot(xj, P) M = dist(xi, xj) G = sinkhorn(wc[i], wc[j + i], M, reg, k) <mask>: loss_w += np.sum(G * M) else: loss_b += np.sum(G * M) # loss inversed because minimization return loss_w / loss_b",False,if j == 0 :,if i == j :,0.8820016898747209,20.412414523193146
"def __init__(self, comm, in_channels, out_channels, ksize, pad=1): super(Block, self).__init__() with self.init_scope(): <mask>: self.conv = ParallelConvolution2D( comm, in_channels, out_channels, ksize, pad=pad, nobias=True ) else: self.conv = chainer.links.Convolution2D( in_channels, out_channels, ksize, pad=pad, nobias=True ) self.bn = L.BatchNormalization(out_channels)",False,if comm . size <= in_channels :,"if isinstance ( comm , chainer . links . ParallelConvolution2D ) :",0.8820016898747209,4.789232204309912
"def halfMultipartScore(nzb_name): try: wrong_found = 0 for nr in [1, 2, 3, 4, 5, ""i"", ""ii"", ""iii"", ""iv"", ""v"", ""a"", ""b"", ""c"", ""d"", ""e""]: for wrong in [""cd"", ""part"", ""dis"", ""disc"", ""dvd""]: <mask>: wrong_found += 1 if wrong_found == 1: return -30 return 0 except: log.error(""Failed doing halfMultipartScore: %s"", traceback.format_exc()) return 0",False,"if ""%s%s"" % ( wrong , nr ) in nzb_name . lower ( ) :",if nr == wrong and nr == wrong :,0.8820016898747209,1.9498186811486276
"def should_include(service): for f in filt: <mask>: state = filt[f] containers = project.containers([service.name], stopped=True) if not has_container_with_state(containers, state): return False elif f == ""source"": source = filt[f] if source == ""image"" or source == ""build"": if source not in service.options: return False else: raise UserError(""Invalid value for source filter: %s"" % source) else: raise UserError(""Invalid filter: %s"" % f) return True",False,"if f == ""status"" :","if f == ""state"" :",0.8820016898747209,59.4603557501361
"def get_blob_type_declaration_sql(self, column): length = column.get(""length"") if length: if length <= self.LENGTH_LIMIT_TINYBLOB: return ""TINYBLOB"" <mask>: return ""BLOB"" if length <= self.LENGTH_LIMIT_MEDIUMBLOB: return ""MEDIUMBLOB"" return ""LONGBLOB""",True,if length <= self . LENGTH_LIMIT_BLOB :,if length <= self . LENGTH_LIMIT_BLOB :,0.8820016898747209,100.00000000000004
"def click_outside(event): if event not in d: x, y, z = self.blockFaceUnderCursor[0] <mask>: y = 64 y += 3 gotoPanel.X, gotoPanel.Y, gotoPanel.Z = x, y, z if event.num_clicks == 2: d.dismiss(""Goto"")",True,if y == 0 :,if y == 0 :,0.8820016898747209,100.00000000000004
"def check_related_active_jobs(self, obj): active_jobs = obj.get_active_jobs() if len(active_jobs) > 0: raise ActiveJobConflict(active_jobs) time_cutoff = now() - dateutil.relativedelta.relativedelta(minutes=1) recent_jobs = obj._get_related_jobs().filter(finished__gte=time_cutoff) for unified_job in recent_jobs.get_real_instances(): <mask>: raise PermissionDenied( _(""Related job {} is still processing events."").format( unified_job.log_format ) )",False,if not unified_job . event_processing_finished :,if unified_job . is_processing_events ( ) :,0.8820016898747209,29.782017963590448
"def run(self): self.alive = True if _log.isEnabledFor(_DEBUG): _log.debug(""started"") while self.alive: task = self.queue.get() <mask>: function, args, kwargs = task assert function try: function(*args, **kwargs) except: _log.exception(""calling %s"", function) if _log.isEnabledFor(_DEBUG): _log.debug(""stopped"")",True,if task :,if task :,0.8820016898747209,0.0
"def update_sysconfig_file(fn, adjustments, allow_empty=False): if not adjustments: return (exists, contents) = read_sysconfig_file(fn) updated_am = 0 for (k, v) in adjustments.items(): if v is None: continue v = str(v) <mask>: continue contents[k] = v updated_am += 1 if updated_am: lines = [ str(contents), ] if not exists: lines.insert(0, util.make_header()) util.write_file(fn, ""\n"".join(lines) + ""\n"", 0o644)",False,if len ( v ) == 0 and not allow_empty :,"if allow_empty and v == """" :",0.8820016898747209,15.468856017774625
"def wrapper( # type: ignore self: RequestHandler, *args, **kwargs ) -> Optional[Awaitable[None]]: if self.request.path.endswith(""/""): if self.request.method in (""GET"", ""HEAD""): uri = self.request.path.rstrip(""/"") <mask>: # don't try to redirect '/' to '' if self.request.query: uri += ""?"" + self.request.query self.redirect(uri, permanent=True) return None else: raise HTTPError(404) return method(self, *args, **kwargs)",False,if uri :,"if not uri . startswith ( ""/"" ) :",0.8820016898747209,4.9323515694897075
def output_handles_from_execution_plan(execution_plan): output_handles_for_current_run = set() for step_level in execution_plan.execution_step_levels(): for step in step_level: for step_input in step.step_inputs: <mask>: output_handles_for_current_run.update(step_input.source_handles) return output_handles_for_current_run,False,if step_input . source_handles :,if step_input . is_current_step :,0.8820016898747209,38.16330911371339
"def _read_value(self, item): item = _normalize_path(item) if item in self._store: <mask>: del self._store[item] raise KeyError(item) return PathResult(item, value=self._store[item]) elif item in self._children: return PathResult(item, dir=True) else: raise KeyError(item)",False,if item in self . _expire_time and self . _expire_time [ item ] < datetime . now ( ) :,if item in self . _children :,0.8820016898747209,7.452850641949802
"def _line_ranges(statements, lines): """"""Produce a list of ranges for `format_lines`."""""" statements = sorted(statements) lines = sorted(lines) pairs = [] start = None lidx = 0 for stmt in statements: if lidx >= len(lines): break if stmt == lines[lidx]: lidx += 1 <mask>: start = stmt end = stmt elif start: pairs.append((start, end)) start = None if start: pairs.append((start, end)) return pairs",False,if not start :,elif start is None :,0.8820016898747209,12.703318703865365
"def _update_help_obj_params(help_obj, data_params, params_equal, attr_key_tups): loaded_params = [] for param_obj in help_obj.parameters: loaded_param = next( (n for n in data_params if params_equal(param_obj, n)), None ) <mask>: BaseHelpLoader._update_obj_from_data_dict( param_obj, loaded_param, attr_key_tups ) loaded_params.append(param_obj) help_obj.parameters = loaded_params",False,if loaded_param :,if loaded_param is not None :,0.8820016898747209,36.55552228545123
"def __get_ratio(self): """"""Return splitter ratio of the main splitter."""""" c = self.c free_layout = c.free_layout if free_layout: w = free_layout.get_main_splitter() <mask>: aList = w.sizes() if len(aList) == 2: n1, n2 = aList # 2017/06/07: guard against division by zero. ratio = 0.5 if n1 + n2 == 0 else float(n1) / float(n1 + n2) return ratio return 0.5",True,if w :,if w :,0.8820016898747209,0.0
"def _check_required_env_variables(vars): for var in vars: <mask>: self.tc.logger.error( ""%s is not set. Did you forget to source your build environment setup script?"" % var ) raise OEQAPreRun",True,if not os . environ . get ( var ) :,if not os . environ . get ( var ) :,0.8820016898747209,100.00000000000004
"def clean_indexes(): for coll_name in mongo.collection_types.keys(): coll = mongo.get_collection(coll_name) indexes = coll_indexes[coll_name] try: for index in coll.list_indexes(): name = index[""name""] <mask>: continue coll.drop_index(name) except pymongo.errors.OperationFailure: pass",False,"if name == ""_id"" or name == ""_id_"" or name in indexes :",if name in indexes :,0.8820016898747209,2.5358201462740055
"def _compare_dirs(self, dir1, dir2): # check that dir1 and dir2 are equivalent, # return the diff diff = [] for root, dirs, files in os.walk(dir1): for file_ in files: path = os.path.join(root, file_) target_path = os.path.join(dir2, os.path.split(path)[-1]) <mask>: diff.append(file_) return diff",False,if not os . path . exists ( target_path ) :,if os . path . isfile ( target_path ) :,0.8820016898747209,59.74178044844197
"def load_state_dict(self, state_dict, strict=True): """"""Customized load."""""" self.language_model.load_state_dict( state_dict[self._language_model_key], strict=strict ) if mpu.is_pipeline_last_stage(): <mask>: self.multichoice_head.load_state_dict( state_dict[self._multichoice_head_key], strict=strict ) else: print_rank_last( ""***WARNING*** could not find {} in the checkpoint, "" ""initializing to random"".format(self._multichoice_head_key) )",True,if self . _multichoice_head_key in state_dict :,if self . _multichoice_head_key in state_dict :,0.8820016898747209,100.00000000000004
"def _parse_timedelta(self, value): try: sum = datetime.timedelta() start = 0 while start < len(value): m = self._TIMEDELTA_PATTERN.match(value, start) <mask>: raise Exception() num = float(m.group(1)) units = m.group(2) or ""seconds"" units = self._TIMEDELTA_ABBREV_DICT.get(units, units) sum += datetime.timedelta(**{units: num}) start = m.end() return sum except: raise",True,if not m :,if not m :,0.8820016898747209,100.00000000000004
"def SetChildMenuBar(self, pChild): if not pChild: # No Child, set Our menu bar back. if self._pMyMenuBar: self.SetMenuBar(self._pMyMenuBar) else: self.SetMenuBar(self.GetMenuBar()) # Make sure we know our menu bar is in use self._pMyMenuBar = None else: if pChild.GetMenuBar() is None: return # Do we need to save the current bar? <mask>: self._pMyMenuBar = self.GetMenuBar() self.SetMenuBar(pChild.GetMenuBar())",True,if self . _pMyMenuBar is None :,if self . _pMyMenuBar is None :,0.8820016898747209,100.00000000000004
"def init_weights(self): """"""Initialize weights of the head."""""" # retinanet_bias_init bias_cls = bias_init_with_prob(0.01) normal_init(self.conv_reg, std=0.01) normal_init(self.conv_centerness, std=0.01) normal_init(self.conv_cls, std=0.01, bias=bias_cls) for branch in [self.cls_convs, self.reg_convs]: for module in branch.modules(): <mask>: caffe2_xavier_init(module.conv)",False,"if isinstance ( module , ConvModule ) and isinstance ( module . conv , nn . Conv2d ) :",if module . conv :,0.8820016898747209,2.7474047213893553
"def handle_exception(self, e, result): for k in sorted(result.thrift_spec): if result.thrift_spec[k][1] == ""success"": continue _, exc_name, exc_cls, _ = result.thrift_spec[k] <mask>: setattr(result, exc_name, e) break else: raise",False,"if isinstance ( e , exc_cls ) :",if exc_cls is not None :,0.8820016898747209,18.190371142855746
"def scripts(self): application_root = current_app.config.get(""APPLICATION_ROOT"") subdir = application_root != ""/"" scripts = [] for script in get_registered_scripts(): <mask>: scripts.append(f'<script defer src=""{script}""></script>') elif subdir: scripts.append(f'<script defer src=""{application_root}/{script}""></script>') else: scripts.append(f'<script defer src=""{script}""></script>') return markup(""\n"".join(scripts))",False,"if script . startswith ( ""http"" ) :","if script . startswith ( ""/"" ) :",0.8820016898747209,65.80370064762461
"def test_related_objects_local(self): result_key = ""get_all_related_objects_with_model_local"" for model, expected in TEST_RESULTS[result_key].items(): objects = [ (field, self._model(model, field)) for field in model._meta.get_fields(include_parents=False) <mask>: ] self.assertEqual( sorted(self._map_related_query_names(objects), key=self.key_name), sorted(expected, key=self.key_name), )",False,if field . auto_created and not field . concrete,if field . get_local ( ),0.8820016898747209,15.181939159382823
"def setTestOutcome(self, event): """"""Update outcome, exc_info and reason based on configured mappings"""""" if event.exc_info: ec, ev, tb = event.exc_info classname = ec.__name__ if classname in self.treatAsFail: short, long_ = self.labels(classname) self._setOutcome(event, ""failed"", short, long_) <mask>: short, long_ = self.labels(classname, upper=False) self._setOutcome(event, ""skipped"", short, ""%s: '%s'"" % (long_, ev), str(ev))",False,elif classname in self . treatAsSkip :,elif classname in self . treatAsSkipped :,0.8820016898747209,64.34588841607616
"def small_count(v): if not v: return 0 z = [ (1000000000, _(""b"")), (1000000, _(""m"")), (1000, _(""k"")), ] v = int(v) for x, y in z: o, p = divmod(v, x) if o: <mask>: return ""%d%s"" % (o, y) return ""%.1f%s"" % (v / float(x), y) return v",False,if len ( str ( o ) ) > 2 or not p :,if p :,0.8820016898747209,0.0
"def __read(self, n): if self._read_watcher is None: raise UnsupportedOperation(""read"") while 1: try: return _read(self._fileno, n) except (IOError, OSError) as ex: <mask>: raise wait_on_watcher(self._read_watcher, None, None, self.hub)",False,if ex . args [ 0 ] not in ignored_errors :,if ex . errno != errno . EINTR :,0.8820016898747209,12.43423351463457
"def locked(self): inputfiles = set(self.all_inputfiles()) outputfiles = set(self.all_outputfiles()) if os.path.exists(self._lockdir): for lockfile in self._locks(""input""): with open(lockfile) as lock: for f in lock: f = f.strip() if f in outputfiles: return True for lockfile in self._locks(""output""): with open(lockfile) as lock: for f in lock: f = f.strip() <mask>: return True return False",False,if f in outputfiles or f in inputfiles :,if f in inputfiles :,0.8820016898747209,37.783911519583654
"def _flags_to_int(flags): # Note, that order does not matter, libev has its own predefined order if not flags: return 0 if isinstance(flags, integer_types): return flags result = 0 try: <mask>: flags = flags.split("","") for value in flags: value = value.strip().lower() if value: result |= _flags_str2int[value] except KeyError as ex: raise ValueError( ""Invalid backend or flag: %s\nPossible values: %s"" % (ex, "", "".join(sorted(_flags_str2int.keys()))) ) return result",False,"if isinstance ( flags , basestring ) :","if "" "" in flags :",0.8820016898747209,7.654112967106117
"def setFg(self, colour, override=False): if not self.ttkFlag: self.containerStack[-1][""fg""] = colour gui.SET_WIDGET_FG(self._getContainerProperty(""container""), colour, override) for child in self._getContainerProperty(""container"").winfo_children(): <mask>: gui.SET_WIDGET_FG(child, colour, override) else: gui.trace(""In ttk mode - trying to set FG to %s"", colour) self.ttkStyle.configure(""TLabel"", foreground=colour) self.ttkStyle.configure(""TFrame"", foreground=colour)",False,if not self . _isWidgetContainer ( child ) :,"if child . get ( ""fg"" ) == colour :",0.8820016898747209,4.834632845440431
"def find_scintilla_constants(f): lexers = [] states = [] for name in f.order: v = f.features[name] if v[""Category""] != ""Deprecated"": if v[""FeatureType""] == ""val"": if name.startswith(""SCE_""): states.append((name, v[""Value""])) <mask>: lexers.append((name, v[""Value""])) return (lexers, states)",False,"elif name . startswith ( ""SCLEX_"" ) :","elif name . startswith ( ""SCE_"" ) :",0.8820016898747209,70.16879391277372
"def extract_error_message(response: requests.Response): if response.content: try: content = json.loads(response.content) <mask>: return content[""message""] except: logging.debug(f""Failed to parse the response content: {response.content}"") return response.reason",True,"if ""message"" in content :","if ""message"" in content :",0.8820016898747209,100.00000000000004
"def canvas_size(self): """"""Return the width and height for this sprite canvas"""""" width = height = 0 for image in self.images: x = image.x + image.absolute_width y = image.y + image.absolute_height <mask>: width = x if height < y: height = y return round_up(width), round_up(height)",True,if width < x :,if width < x :,0.8820016898747209,100.00000000000004
"def _load_widgets(self): logger.info(""Loading plugins preferences widgets"") # Collect the preferences widget for each active plugin for plugin in self.plugin_manager.get_active_plugins(): plugin_name = plugin.metadata.get(""name"") try: preferences_widget = plugin.get_preferences_widget() <mask>: self._tabs.addTab(preferences_widget, plugin_name) except Exception as reason: logger.error( ""Unable to add the preferences widget (%s): %s"", plugin_name, reason ) continue",True,if preferences_widget :,if preferences_widget :,0.8820016898747209,100.00000000000004
"def clean_objects(string, common_attributes): """"""Return object and attribute lists"""""" string = clean_string(string) words = string.split() if len(words) > 1: prefix_words_are_adj = True for att in words[:-1]: if att not in common_attributes: prefix_words_are_adj = False <mask>: return words[-1:], words[:-1] else: return [string], [] else: return [string], []",True,if prefix_words_are_adj :,if prefix_words_are_adj :,0.8820016898747209,100.00000000000004
"def _reader(): if shuffle: random.shuffle(file_list) while True: for fn in file_list: for line in open(fn, ""r""): yield self._process_line(line) <mask>: break",False,if not cycle :,if not self . _is_file ( fn ) :,0.8820016898747209,7.495553473355845
"def load(weights, model, K, fsz, dil): index = 0 layers = model.layers for layer in layers._layers: <mask>: if layer.W.shape == weights[index].shape: layer.W[:] = weights[index] else: layer.W[:] = dilate(weights[index], K, fsz, dil) index += 1",False,"if hasattr ( layer , ""W"" ) :",if layer . W is not None :,0.8820016898747209,6.082317172853824
"def upgrade(migrate_engine): print(__doc__) metadata.bind = migrate_engine liftoverjobs = dict() jobs = context.query(DeferredJob).filter_by(plugin=""LiftOverTransferPlugin"").all() for job in jobs: <mask>: liftoverjobs[job.params[""parentjob""]] = [] liftoverjobs[job.params[""parentjob""]].append(job.id) for parent in liftoverjobs: lifts = liftoverjobs[parent] deferred = context.query(DeferredJob).filter_by(id=parent).first() deferred.params[""liftover""] = lifts context.flush()",True,"if job . params [ ""parentjob"" ] not in liftoverjobs :","if job . params [ ""parentjob"" ] not in liftoverjobs :",0.8820016898747209,100.00000000000004
"def get_refs(self, recursive=False): """""":see: AbstractExpression.get_refs()"""""" if recursive: conds_refs = self.refs + sum((c.get_refs(True) for c in self.conds), []) <mask>: conds_refs.extend(self.consequent.get_refs(True)) return conds_refs else: return self.refs",True,if self . consequent :,if self . consequent :,0.8820016898747209,100.00000000000004
"def _parse(self, engine): """"""Parse the layer."""""" if isinstance(self.args, dict): <mask>: self.axis = engine.evaluate(self.args[""axis""], recursive=True) if not isinstance(self.axis, int): raise ParsingError('""axis"" must be an integer.') if ""momentum"" in self.args: self.momentum = engine.evaluate(self.args[""momentum""], recursive=True) if not isinstance(self.momentum, (int, float)): raise ParsingError('""momentum"" must be numeric.')",True,"if ""axis"" in self . args :","if ""axis"" in self . args :",0.8820016898747209,100.00000000000004
"def CountMatches(pat, predicate): num_matches = 0 for i in xrange(256): b = chr(i) m = pat.match(b) left = bool(m) right = predicate(i) if left != right: self.fail(""i = %d, b = %r, match: %s, predicate: %s"" % (i, b, left, right)) <mask>: num_matches += 1 return num_matches",True,if m :,if m :,0.8820016898747209,0.0
"def __new__(cls, *args, **kwargs): if len(args) == 1: if len(kwargs): raise ValueError( ""You can either use {} with one positional argument or with keyword arguments, not both."".format( cls.__name__ ) ) if not args[0]: return super().__new__(cls) <mask>: return cls return super().__new__(cls, *args, **kwargs)",False,"if isinstance ( args [ 0 ] , cls ) :",if not kwargs [ 0 ] :,0.8820016898747209,15.685718045401451
"def concatenateCharacterTokens(tokens): pendingCharacters = [] for token in tokens: type = token[""type""] if type in (""Characters"", ""SpaceCharacters""): pendingCharacters.append(token[""data""]) else: <mask>: yield {""type"": ""Characters"", ""data"": """".join(pendingCharacters)} pendingCharacters = [] yield token if pendingCharacters: yield {""type"": ""Characters"", ""data"": """".join(pendingCharacters)}",True,if pendingCharacters :,if pendingCharacters :,0.8820016898747209,0.0
"def get_ranges_from_func_set(support_set): pos_start = 0 pos_end = 0 ranges = [] for pos, func in enumerate(network.function): if func.type in support_set: pos_end = pos else: <mask>: ranges.append((pos_start, pos_end)) pos_start = pos + 1 if pos_end >= pos_start: ranges.append((pos_start, pos_end)) return ranges",False,if pos_end >= pos_start :,if pos_start >= pos_end :,0.8820016898747209,51.33450480401705
"def _visit(self, func): fname = func[0] if fname in self._flags: <mask>: logger.critical(""Fatal error! network ins not Dag."") import sys sys.exit(-1) else: return else: if fname not in self._flags: self._flags[fname] = 1 for output in func[3]: for f in self._orig: for input in f[2]: if output == input: self._visit(f) self._flags[fname] = 2 self._sorted.insert(0, func)",True,if self . _flags [ fname ] == 1 :,if self . _flags [ fname ] == 1 :,0.8820016898747209,100.00000000000004
"def graph_merge_softmax_with_crossentropy_softmax(node): if node.op == softmax_with_bias: x, b = node.inputs for x_client in x.clients: <mask>: big_client = x_client[0] if big_client in [b_client[0] for b_client in b.clients]: xx, bb, ll = big_client.inputs mergeable_client = big_client.op(x, b, ll) copy_stack_trace(node.outputs[0], mergeable_client[1]) return [mergeable_client[1]]",False,if x_client [ 0 ] . op == crossentropy_softmax_argmax_1hot_with_bias :,if x_client in [ x_client [ 0 ] for x_client in x . clients ] :,0.8820016898747209,28.13820773020137
"def confidence(self): if self.bbox: # Units are measured in Kilometers distance = Distance(self.northeast, self.southwest, units=""km"") for score, maximum in [ (10, 0.25), (9, 0.5), (8, 1), (7, 5), (6, 7.5), (5, 10), (4, 15), (3, 20), (2, 25), ]: if distance < maximum: return score <mask>: return 1 # Cannot determine score return 0",False,if distance >= 25 :,elif distance > maximum :,0.8820016898747209,19.3576934939088
"def OnListEndLabelEdit(self, std, extra): item = extra[0] text = item[4] if text is None: return item_id = self.GetItem(item[0])[6] from bdb import Breakpoint for bplist in Breakpoint.bplist.itervalues(): for bp in bplist: if id(bp) == item_id: <mask>: text = None bp.cond = text break self.RespondDebuggerData()",False,"if text . strip ( ) . lower ( ) == ""none"" :",if bp . cond is None :,0.8820016898747209,2.389389104935703
"def _handle_autocomplete_request_for_text(text): if not hasattr(text, ""autocompleter""): <mask>: if isinstance(text, CodeViewText): text.autocompleter = Completer(text) elif isinstance(text, ShellText): text.autocompleter = ShellCompleter(text) text.bind(""<1>"", text.autocompleter.on_text_click) else: return text.autocompleter.handle_autocomplete_request()",False,"if isinstance ( text , ( CodeViewText , ShellText ) ) and text . is_python_text ( ) :",if text . is_text_editable ( ) :,0.8820016898747209,14.28375357136274
"def visit_Macro(self, node, frame): macro_frame, macro_ref = self.macro_body(node, frame) self.newline() if frame.toplevel: <mask>: self.write(""context.exported_vars.add(%r)"" % node.name) ref = frame.symbols.ref(node.name) self.writeline(""context.vars[%r] = "" % node.name) self.write(""%s = "" % frame.symbols.ref(node.name)) self.macro_def(macro_ref, macro_frame)",False,"if not node . name . startswith ( ""_"" ) :",if node . name not in frame . exported_vars :,0.8820016898747209,14.820988823055043
"def execute(cls, ctx, op): try: pd.set_option(""mode.use_inf_as_na"", op.use_inf_as_na) <mask>: return cls._execute_map(ctx, op) else: return cls._execute_combine(ctx, op) finally: pd.reset_option(""mode.use_inf_as_na"")",False,if op . stage == OperandStage . map :,if op . use_map :,0.8820016898747209,20.024850746991504
"def ranges(self, start, end): try: iterators = [i.ranges(start, end) for i in self.range_iterators] starts, ends, values = zip(*[next(i) for i in iterators]) starts = list(starts) ends = list(ends) values = list(values) while start < end: min_end = min(ends) yield start, min_end, values start = min_end for i, iterator in enumerate(iterators): <mask>: starts[i], ends[i], values[i] = next(iterator) except StopIteration: return",False,if ends [ i ] == min_end :,if iterator . next ( ) :,0.8820016898747209,4.410363736106611
"def get_explanation(self, spec): """"""Expand an explanation."""""" if spec: try: a = self.dns_txt(spec) <mask>: return str(self.expand(to_ascii(a[0]), stripdot=False)) except PermError: # RFC4408 6.2/4 syntax errors cause exp= to be ignored if self.strict > 1: raise # but report in harsh mode for record checking tools pass elif self.strict > 1: raise PermError(""Empty domain-spec on exp="") # RFC4408 6.2/4 empty domain spec is ignored # (unless you give precedence to the grammar). return None",True,if len ( a ) == 1 :,if len ( a ) == 1 :,0.8820016898747209,100.00000000000004
"def iter_fields(node, *, include_meta=True, exclude_unset=False): exclude_meta = not include_meta for field_name, field in node._fields.items(): if exclude_meta and field.meta: continue field_val = getattr(node, field_name, _marker) if field_val is _marker: continue <mask>: if callable(field.default): default = field.default() else: default = field.default if field_val == default: continue yield field_name, field_val",True,if exclude_unset :,if exclude_unset :,0.8820016898747209,100.00000000000004
"def __setattr__(self, name, value): try: field = self._meta.get_field(name) <mask>: value = value[: field.max_length] except models.fields.FieldDoesNotExist: pass # This happens with foreign keys. super.__setattr__(self, name, value)",False,"if type ( field ) in [ models . CharField , models . TextField ] and type ( value ) == str :",if field . max_length :,0.8820016898747209,0.8188135341326943
"def create_child(self, value=None, _id=None): with atomic(savepoint=False): child_key = self.get_next_child_key() <mask>: value = child_key child = self.__class__.objects.create(id=_id, key=child_key, value=value) return child",True,if value is None :,if value is None :,0.8820016898747209,100.00000000000004
"def list_tags_for_stream(self, stream_name, exclusive_start_tag_key=None, limit=None): stream = self.describe_stream(stream_name) tags = [] result = {""HasMoreTags"": False, ""Tags"": tags} for key, val in sorted(stream.tags.items(), key=lambda x: x[0]): <mask>: result[""HasMoreTags""] = True break if exclusive_start_tag_key and key < exclusive_start_tag_key: continue tags.append({""Key"": key, ""Value"": val}) return result",False,if limit and len ( tags ) >= limit :,if val == limit :,0.8820016898747209,14.110009442520557
"def emit(self, record): try: app = get_app() <mask>: msg = self.format(record) debug_buffer = app.layout.get_buffer_by_name(""debug_buffer"") current_document = debug_buffer.document.text if current_document: msg = ""\n"".join([current_document, msg]) debug_buffer.set_document(Document(text=msg), bypass_readonly=True) else: super().emit(record) except: self.handleError(record)",False,"if app . is_running and getattr ( app , ""debug"" , False ) :",if app . layout :,0.8820016898747209,3.173613488953928
"def worker(): global error while True: (num, q) = pq.get() <mask>: pq.task_done() break try: process_one(q) except Exception as e: error = e finally: pq.task_done()",False,if q is None or error is not None :,if num == 0 :,0.8820016898747209,4.955725306405571
"def transceiver(self, data): out = [] for t in range(8): if data[t] == 0: continue value = data[t] for b in range(8): <mask>: if len(TRANSCEIVER[t]) < b + 1: out.append(""(unknown)"") else: out.append(TRANSCEIVER[t][b]) value <<= 1 self.annotate(""Transceiver compliance"", "", "".join(out))",True,if value & 0x80 :,if value & 0x80 :,0.8820016898747209,100.00000000000004
"def skip_to_close_match(self): nestedCount = 1 while 1: tok = self.tokenizer.get_next_token() ttype = tok[""style""] <mask>: return elif self.classifier.is_index_op(tok): tval = tok[""text""] if self.opHash.has_key(tval): if self.opHash[tval][1] == 1: nestedCount += 1 else: nestedCount -= 1 if nestedCount <= 0: break",False,if ttype == SCE_PL_UNUSED :,"if ttype == ""close"" :",0.8820016898747209,28.46946938149361
"def GenerateVector(self, hits, vector, level): """"""Generate possible hit vectors which match the rules."""""" for item in hits.get(level, []): if vector: <mask>: continue if item > self.max_separation + vector[-1]: break new_vector = vector + [item] if level + 1 == len(hits): yield new_vector elif level + 1 < len(hits): for result in self.GenerateVector(hits, new_vector, level + 1): yield result",False,if item < vector [ - 1 ] :,if item < self . min_separation :,0.8820016898747209,19.070828081828378
"def __setattr__(self, name, value): if name == ""path"": <mask>: if value[0] != ""/"": raise ValueError( 'The page path should always start with a slash (""/"").' ) elif name == ""load_time"": if value and not isinstance(value, int): raise ValueError( ""Page load time must be specified in integer milliseconds."" ) object.__setattr__(self, name, value)",False,"if value and value != """" :",if value :,0.8820016898747209,0.0
"def awaitTermination(self, timeout=None): if self.scheduler is None: raise RuntimeError(""StreamimgContext not started"") try: deadline = time.time() + timeout if timeout is not None else None while True: is_terminated = self._runOnce() <mask>: break if self.batchCallback: self.batchCallback() except KeyboardInterrupt: pass finally: self.sc.stop() logger.info(""StreamingContext stopped successfully"")",False,if is_terminated or ( deadline is not None and time . time ( ) > deadline ) :,if is_terminated :,0.8820016898747209,3.520477365831487
"def stopbutton(self): if GPIOcontrol: while mediastopbutton: time.sleep(0.25) <mask>: print(""Stopped"") stop()",False,if not GPIO . input ( stoppushbutton ) :,if self . _is_stop :,0.8820016898747209,6.413885305524152
"def test_create_connection_timeout(self): # Issue #9792: create_connection() should not recast timeout errors # as generic socket errors. with self.mocked_socket_module(): try: socket.create_connection((HOST, 1234)) except socket.timeout: pass except OSError as exc: <mask>: raise else: self.fail(""socket.timeout not raised"")",False,if support . IPV6_ENABLED or exc . errno != errno . EAFNOSUPPORT :,if exc . errno != errno . ECONNRESET :,0.8820016898747209,37.33976803111327
"def handle_exception_and_die(e): if hasattr(e, ""kind""): <mask>: sys.stderr.write(""ABORT: "" + e.msg + ""\n"") sys.exit(e.value) elif e.kind == ""exit"": sys.stderr.write(""EXITING\n"") sys.exit(e.value) else: print(str(e)) sys.exit(1)",False,"if e . kind == ""die"" :","if e . kind == ""abort"" :",0.8820016898747209,70.71067811865478
"def gets(self, key): with self.client_pool.get_and_release(destroy_on_fail=True) as client: try: return client.gets(key) except Exception: <mask>: return (None, None) else: raise",False,if self . ignore_exc :,if self . debug :,0.8820016898747209,28.641904579795423
"def _execute(self, options, args): if len(args) < 3: raise CommandError(_(""Not enough arguments"")) tag = fsn2text(args[0]) value = fsn2text(args[1]) paths = args[2:] songs = [] for path in paths: song = self.load_song(path) <mask>: raise CommandError(_(""Can not set %r"") % tag) self.log(""Add %r to %r"" % (value, tag)) song.add(tag, value) songs.append(song) self.save_songs(songs)",False,if not song . can_change ( tag ) :,if not song :,0.8820016898747209,11.10316628653437
"def get_place_name(self, place_handle): """"""Obtain a place name"""""" text = """" if place_handle: place = self.dbstate.db.get_place_from_handle(place_handle) if place: place_title = place_displayer.display(self.dbstate.db, place) <mask>: if len(place_title) > 25: text = place_title[:24] + ""..."" else: text = place_title return text",False,"if place_title != """" :",if place_title :,0.8820016898747209,31.772355751081438
"def _Determine_Do(self): self.applicable = 1 self.value = os.environ.get(self.name, None) if self.value is None and black.configure.items.has_key(""buildType""): buildType = black.configure.items[""buildType""].Get() <mask>: self.value = ""warn"" else: self.value = None self.determined = 1",False,"if buildType == ""debug"" :","if buildType == ""warn"" :",0.8820016898747209,59.4603557501361
"def bundle_directory(self, dirpath): """"""Bundle all modules/packages in the given directory."""""" dirpath = os.path.abspath(dirpath) for nm in os.listdir(dirpath): nm = _u(nm) <mask>: continue itempath = os.path.join(dirpath, nm) if os.path.isdir(itempath): if os.path.exists(os.path.join(itempath, ""__init__.py"")): self.bundle_package(itempath) elif nm.endswith("".py""): self.bundle_module(itempath)",False,"if nm . startswith ( ""."" ) :",if not nm :,0.8820016898747209,4.690733795095046
"def header_fields(self, fields): headers = dict(self.conn.response.getheaders()) ret = {} for field in fields: <mask>: raise ValueError(""%s was not found in response header"" % (field[1])) try: ret[field[0]] = int(headers[field[1]]) except ValueError: ret[field[0]] = headers[field[1]] return ret",False,if not headers . has_key ( field [ 1 ] ) :,if field [ 1 ] not in headers :,0.8820016898747209,20.365268977284835
"def caesar_cipher(s, k): result = """" for char in s: n = ord(char) <mask>: n = ((n - 65 + k) % 26) + 65 if 96 < n < 123: n = ((n - 97 + k) % 26) + 97 result = result + chr(n) return result",False,if 64 < n < 91 :,if 65 < n < 65 :,0.8820016898747209,27.77619034011791
"def qtTypeIdent(conn, *args): # We're not using the conn object at the moment, but - we will # modify the # logic to use the server version specific keywords later. res = None value = None for val in args: # DataType doesn't have len function then convert it to string if not hasattr(val, ""__len__""): val = str(val) <mask>: continue value = val if Driver.needsQuoting(val, True): value = value.replace('""', '""""') value = '""' + value + '""' res = ((res and res + ""."") or """") + value return res",False,if len ( val ) == 0 :,if not Driver . needsQuoting ( val ) :,0.8820016898747209,20.164945583740657
"def _parse_timezone( value: Optional[str], error: Type[Exception] ) -> Union[None, int, timezone]: if value == ""Z"": return timezone.utc elif value is not None: offset_mins = int(value[-2:]) if len(value) > 3 else 0 offset = 60 * int(value[1:3]) + offset_mins <mask>: offset = -offset try: return timezone(timedelta(minutes=offset)) except ValueError: raise error() else: return None",False,"if value [ 0 ] == ""-"" :",if offset < 0 :,0.8820016898747209,4.234348806659263
"def indent(elem, level=0): i = ""\n"" + level * "" "" if len(elem): if not elem.text or not elem.text.strip(): elem.text = i + "" "" <mask>: elem.tail = i for elem in elem: indent(elem, level + 1) if not elem.tail or not elem.tail.strip(): elem.tail = i else: if level and (not elem.tail or not elem.tail.strip()): elem.tail = i",False,if not elem . tail or not elem . tail . strip ( ) :,elif not elem . tail or not elem . tail . strip ( ) :,0.8820016898747209,92.53911813809742
"def _make_slices( shape: tp.Tuple[int, ...], axes: tp.Tuple[int, ...], size: int, rng: np.random.RandomState, ) -> tp.List[slice]: slices = [] for a, s in enumerate(shape): if a in axes: <mask>: raise ValueError(""Cannot crossover on axis with size 1"") start = rng.randint(s - size) slices.append(slice(start, start + size)) else: slices.append(slice(None)) return slices",False,if s <= 1 :,if s < 1 :,0.8820016898747209,40.93653765389909
"def _loadTestsFromTestCase(self, event, testCaseClass): evt = events.LoadFromTestCaseEvent(event.loader, testCaseClass) result = self.session.hooks.loadTestsFromTestCase(evt) if evt.handled: loaded_suite = result or event.loader.suiteClass() else: names = self._getTestCaseNames(event, testCaseClass) <mask>: names = [""runTest""] # FIXME return failure test case if name not in testcase class loaded_suite = event.loader.suiteClass(map(testCaseClass, names)) if evt.extraTests: loaded_suite.addTests(evt.extraTests) return loaded_suite",False,"if not names and hasattr ( testCaseClass , ""runTest"" ) :",if not names :,0.8820016898747209,6.734410772670761
"def check_settings(self): if self.settings_dict[""TIME_ZONE""] is not None: if not settings.USE_TZ: raise ImproperlyConfigured( ""Connection '%s' cannot set TIME_ZONE because USE_TZ is "" ""False."" % self.alias ) <mask>: raise ImproperlyConfigured( ""Connection '%s' cannot set TIME_ZONE because its engine "" ""handles time zones conversions natively."" % self.alias )",False,elif self . features . supports_timezones :,"if settings . TIME_ZONE in self . settings_dict [ ""TIME_ZONE"" ] :",0.8820016898747209,4.814971807094068
"def collect_conflicting_diffs(path, decisions): local_conflict_diffs = [] remote_conflict_diffs = [] for d in decisions: <mask>: ld = adjust_patch_level(path, d.common_path, d.local_diff) rd = adjust_patch_level(path, d.common_path, d.remote_diff) local_conflict_diffs.extend(ld) remote_conflict_diffs.extend(rd) return local_conflict_diffs, remote_conflict_diffs",False,if d . conflict :,if d . common_path :,0.8820016898747209,26.269098944241588
"def short_repr(obj): if isinstance( obj, (type, types.ModuleType, types.BuiltinMethodType, types.BuiltinFunctionType), ): return obj.__name__ if isinstance(obj, types.MethodType): <mask>: return obj.im_func.__name__ + "" (bound)"" else: return obj.im_func.__name__ if isinstance(obj, (tuple, list, dict, set)): return ""%d items"" % len(obj) if isinstance(obj, weakref.ref): return ""all_weakrefs_are_one"" return repr(obj)[:40]",False,if obj . im_self is not None :,"if isinstance ( obj . im_func , types . FunctionType ) :",0.8820016898747209,19.67497981115564
"def _massage_uri(uri): if uri: <mask>: uri = uri.replace(""hdfs://"", get_defaultfs()) elif uri.startswith(""/""): uri = get_defaultfs() + uri return uri",False,"if uri . startswith ( ""hdfs:///"" ) :","if uri . startswith ( ""hdfs://"" ) :",0.8820016898747209,90.18895596416246
"def chsub(self, msg, chatid): (cmd, evt, params) = self.tokenize(msg, 3) if cmd == ""/sub"": sql = ""replace into telegram_subscriptions(uid, event_type, parameters) values (?, ?, ?)"" else: <mask>: sql = ""delete from telegram_subscriptions where uid = ? and (event_type = ? or parameters = ? or 1 = 1)"" # does not look very elegant, but makes unsub'ing everythign possible else: sql = ""delete from telegram_subscriptions where uid = ? and event_type = ? and parameters = ?"" with self.bot.database as conn: conn.execute(sql, [chatid, evt, params]) conn.commit() return",False,"if evt == ""everything"" :","if cmd == ""/delete"" :",0.8820016898747209,23.356898886410015
"def undefined_symbols(self): result = [] for p in self.Productions: <mask>: continue for s in p.prod: if not s in self.Prodnames and not s in self.Terminals and s != ""error"": result.append((s, p)) return result",False,if not p :,if not p . is_defined :,0.8820016898747209,22.089591134157878
"def renumber(self, x1, y1, x2, y2, dx, dy): out = [] for part in re.split(""(\w+)"", self.formula): m = re.match(""^([A-Z]+)([1-9][0-9]*)$"", part) if m is not None: sx, sy = m.groups() x = colname2num(sx) y = int(sy) <mask>: part = cellname(x + dx, y + dy) out.append(part) return FormulaCell("""".join(out), self.fmt, self.alignment)",False,if x1 <= x <= x2 and y1 <= y <= y2 :,if x + dx < y + dy :,0.8820016898747209,2.9309777488533775
"def modify_column(self, column: List[Optional[""Cell""]]): for i in range(len(column)): gate = column[i] if gate is self: continue <mask>: # The first parity control to modify the column must merge all # of the other parity controls into itself. column[i] = None self._basis_change += gate._basis_change self.qubits += gate.qubits elif gate is not None: column[i] = gate.controlled_by(self.qubits[0])",False,"elif isinstance ( gate , ParityControlCell ) :",if gate . _basis_change is not None :,0.8820016898747209,4.456882760699063
"def update_neighbor(neigh_ip_address, changes): rets = [] for k, v in changes.items(): <mask>: rets.append(_update_med(neigh_ip_address, v)) if k == neighbors.ENABLED: rets.append(update_neighbor_enabled(neigh_ip_address, v)) if k == neighbors.CONNECT_MODE: rets.append(_update_connect_mode(neigh_ip_address, v)) return all(rets)",False,if k == neighbors . MULTI_EXIT_DISC :,if k == neighbors . MED :,0.8820016898747209,42.88819424803536
"def writexml( self, stream, indent="""", addindent="""", newl="""", strip=0, nsprefixes={}, namespace="""", ): w = _streamWriteWrapper(stream) if self.raw: val = self.nodeValue if not isinstance(val, str): val = str(self.nodeValue) else: v = self.nodeValue <mask>: v = str(v) if strip: v = "" "".join(v.split()) val = escape(v) w(val)",True,"if not isinstance ( v , str ) :","if not isinstance ( v , str ) :",0.8820016898747209,100.00000000000004
"def _condition(ct): for qobj in args: <mask>: # normal kwargs are an AND anyway, so just use those for now for child in qobj.children: kwargs.update(dict([child])) else: raise NotImplementedError(""Unsupported Q object"") for attr, val in kwargs.items(): if getattr(ct, attr) != val: return False return True",False,"if qobj . connector == ""AND"" and not qobj . negated :","if isinstance ( qobj , Q ) :",0.8820016898747209,3.0297048914466935
"def results_iter(self): <mask>: from django.db.models.fields import DateTimeField fields = [DateTimeField()] else: needs_string_cast = self.connection.features.needs_datetime_string_cast offset = len(self.query.extra_select) for rows in self.execute_sql(MULTI): for row in rows: date = row[offset] if self.connection.ops.oracle: date = self.resolve_columns(row, fields)[offset] elif needs_string_cast: date = typecast_timestamp(str(date)) yield date",True,if self . connection . ops . oracle :,if self . connection . ops . oracle :,0.8820016898747209,100.00000000000004
"def get_job_type(self): if int(self.job_runtime_conf.get(""dsl_version"", 1)) == 2: job_type = ( self.job_runtime_conf[""job_parameters""].get(""common"", {}).get(""job_type"") ) <mask>: job_type = self.job_runtime_conf[""job_parameters""].get(""job_type"", ""train"") else: job_type = self.job_runtime_conf[""job_parameters""].get(""job_type"", ""train"") return job_type",False,if not job_type :,"if job_type == ""train"" :",0.8820016898747209,17.747405280050266
"def validate_assessment_criteria(self): if self.assessment_criteria: total_weightage = 0 for criteria in self.assessment_criteria: total_weightage += criteria.weightage or 0 <mask>: frappe.throw(_(""Total Weightage of all Assessment Criteria must be 100%""))",False,if total_weightage != 100 :,if total_weightage > 100 :,0.8820016898747209,42.38365628278778
"def get_list_of_strings_to_mongo_objects(self, notifications_list=None): result = [] if len(notifications_list) > 0: for x in notifications_list: split_provider_id = x.split("":"") # email:id <mask>: _id = split_provider_id[1] cursor = self.get_by_id(_id) if cursor: # Append if exists result.append(cursor) return result",False,if len ( split_provider_id ) == 2 :,"if split_provider_id [ 0 ] == ""email"" :",0.8820016898747209,30.130404892785695
"def dump_predictions_to_database(relation, predictions): judge = ""iepy-run on {}"".format(datetime.now().strftime(""%Y-%m-%d %H:%M"")) for evidence, relation_is_present in predictions.items(): label = ( EvidenceLabel.YESRELATION <mask>: else EvidenceLabel.NORELATION ) evidence.set_label(relation, label, judge, labeled_by_machine=True)",True,if relation_is_present,if relation_is_present,0.8820016898747209,100.00000000000004
"def __init__(self, **kwargs): # We hard-code the `to` argument for ForeignKey.__init__ dfl = get_model_label(self.default_model_class) if ""to"" in kwargs.keys(): # pragma: no cover old_to = get_model_label(kwargs.pop(""to"")) <mask>: msg = ""%s can only be a ForeignKey to %s; %s passed"" % ( self.__class__.__name__, dfl, old_to, ) warnings.warn(msg, SyntaxWarning) kwargs[""to""] = dfl super().__init__(**kwargs)",False,if old_to . lower ( ) != dfl . lower ( ) :,if old_to != dfl :,0.8820016898747209,19.0183794978402
"def reverse(self): """"""Reverse *IN PLACE*."""""" li = self.leftindex lb = self.leftblock ri = self.rightindex rb = self.rightblock for i in range(self.len >> 1): lb.data[li], rb.data[ri] = rb.data[ri], lb.data[li] li += 1 if li >= BLOCKLEN: lb = lb.rightlink li = 0 ri -= 1 <mask>: rb = rb.leftlink ri = BLOCKLEN - 1",False,if ri < 0 :,if ri >= BLOCKLEN :,0.8820016898747209,17.965205598154213
"def get_api(user, url): global API_CACHE if API_CACHE is None or API_CACHE.get(url) is None: API_CACHE_LOCK.acquire() try: if API_CACHE is None: API_CACHE = {} <mask>: API_CACHE[url] = ImpalaDaemonApi(url) finally: API_CACHE_LOCK.release() api = API_CACHE[url] api.set_user(user) return api",False,if API_CACHE . get ( url ) is None :,if url not in API_CACHE :,0.8820016898747209,14.827340167306767
"def invert_index(cls, index, length): if np.isscalar(index): return length - index elif isinstance(index, slice): start, stop = index.start, index.stop new_start, new_stop = None, None if start is not None: new_stop = length - start <mask>: new_start = length - stop return slice(new_start - 1, new_stop - 1) elif isinstance(index, Iterable): new_index = [] for ind in index: new_index.append(length - ind) return new_index",True,if stop is not None :,if stop is not None :,0.8820016898747209,100.00000000000004
"def infer_returned_object(pyfunction, args): """"""Infer the `PyObject` this `PyFunction` returns after calling"""""" object_info = pyfunction.pycore.object_info result = object_info.get_exact_returned(pyfunction, args) if result is not None: return result result = _infer_returned(pyfunction, args) if result is not None: <mask>: params = args.get_arguments(pyfunction.get_param_names(special_args=False)) object_info.function_called(pyfunction, params, result) return result return object_info.get_returned(pyfunction, args)",False,if args and pyfunction . get_module ( ) . get_resource ( ) is not None :,"if isinstance ( result , PyObject ) :",0.8820016898747209,1.7426130460477305
"def _check_imports(lib): # Make sure no conflicting libraries have been imported. libs = [""PyQt4"", ""PyQt5"", ""PySide""] libs.remove(lib) for lib2 in libs: lib2 += "".QtCore"" <mask>: raise RuntimeError( ""Refusing to import %s because %s is already "" ""imported."" % (lib, lib2) )",True,if lib2 in sys . modules :,if lib2 in sys . modules :,0.8820016898747209,100.00000000000004
"def _poll(fds, timeout): if timeout is not None: timeout = int(timeout * 1000) # timeout is in milliseconds fd_map = {} pollster = select.poll() for fd in fds: pollster.register(fd, select.POLLIN) <mask>: fd_map[fd.fileno()] = fd else: fd_map[fd] = fd ls = [] for fd, event in pollster.poll(timeout): if event & select.POLLNVAL: raise ValueError(""invalid file descriptor %i"" % fd) ls.append(fd_map[fd]) return ls",False,"if hasattr ( fd , ""fileno"" ) :",if fd . is_file ( ) :,0.8820016898747209,10.729256185679601
"def default(cls, connection=None): """"""show the default connection, or make CONNECTION the default"""""" if connection is not None: target = cls._get_config_filename(connection) <mask>: if os.path.exists(cls._default_symlink): os.remove(cls._default_symlink) os.symlink(target, cls._default_symlink) else: cls._no_config_file_error(target) if os.path.exists(cls._default_symlink): print(""Default connection is "" + cls._default_connection()) else: print(""There is no default connection set"")",True,if os . path . exists ( target ) :,if os . path . exists ( target ) :,0.8820016898747209,100.00000000000004
"def process(self, fuzzresult): base_url = urljoin(fuzzresult.url, "".."") for line in fuzzresult.history.content.splitlines(): record = line.split(""/"") <mask>: self.queue_url(urljoin(base_url, record[1])) # Directory if record[0] == ""D"": self.queue_url(urljoin(base_url, record[1])) self.queue_url(urljoin(base_url, ""%s/CVS/Entries"" % (record[1])))",False,if len ( record ) == 6 and record [ 1 ] :,"if record [ 0 ] == ""CVS"" :",0.8820016898747209,9.281844047221343
"def _GetCSVRow(self, value): row = [] for type_info in value.__class__.type_infos: <mask>: row.extend(self._GetCSVRow(value.Get(type_info.name))) elif isinstance(type_info, rdf_structs.ProtoBinary): row.append(text.Asciify(value.Get(type_info.name))) else: row.append(str(value.Get(type_info.name))) return row",False,"if isinstance ( type_info , rdf_structs . ProtoEmbedded ) :","if isinstance ( type_info , rdf_structs . RDFType ) :",0.8820016898747209,80.91067115702207
"def get_history(self, state, dict_, passive=PASSIVE_OFF): if self.key in dict_: return History.from_scalar_attribute(self, state, dict_[self.key]) else: <mask>: passive ^= INIT_OK current = self.get(state, dict_, passive=passive) if current is PASSIVE_NO_RESULT: return HISTORY_BLANK else: return History.from_scalar_attribute(self, state, current)",False,if passive & INIT_OK :,if passive :,0.8820016898747209,0.0
"def _iterate_self_and_parents(self, upto=None): current = self result = () while current: result += (current,) <mask>: break elif current._parent is None: raise sa_exc.InvalidRequestError( ""Transaction %s is not on the active transaction list"" % (upto) ) else: current = current._parent return result",False,if current . _parent is upto :,if current . _parent is None :,0.8820016898747209,70.71067811865478
"def get_by_uri(self, uri: str) -> bytes: userId, bucket, key = self._parse_uri(uri) try: with db.session_scope() as dbsession: result = db_archivedocument.get(userId, bucket, key, session=dbsession) <mask>: return utils.ensure_bytes(self._decode(result)) else: raise ObjectKeyNotFoundError(userId, bucket, key, caused_by=None) except Exception as err: logger.debug(""cannot get data: exception - "" + str(err)) raise err",True,if result :,if result :,0.8820016898747209,0.0
"def app(scope, receive, send): while True: message = await receive() if message[""type""] == ""websocket.connect"": await send({""type"": ""websocket.accept""}) elif message[""type""] == ""websocket.receive"": pass <mask>: break",False,"elif message [ ""type"" ] == ""websocket.disconnect"" :","elif message [ ""type"" ] == ""websocket.close"" :",0.8820016898747209,82.42367502646057
"def recv_some(p, t=0.1, e=1, tr=5, stderr=0): if tr < 1: tr = 1 x = time.time() + t y = [] r = """" if stderr: pr = p.recv_err else: pr = p.recv while time.time() < x or r: r = pr() if r is None: break <mask>: y.append(r) else: time.sleep(max((x - time.time()) / tr, 0)) return """".join(y)",False,elif r :,if e :,0.8820016898747209,0.0
"def mouse_down(self, event): if event.button == 1: <mask>: p = event.local if self.scroll_up_rect().collidepoint(p): self.scroll_up() return elif self.scroll_down_rect().collidepoint(p): self.scroll_down() return if event.button == 4: self.scroll_up() if event.button == 5: self.scroll_down() GridView.mouse_down(self, event)",False,if self . scrolling :,if event . local :,0.8820016898747209,14.058533129758727
"def copy_from(self, other): if self is other: return # Myself! self.strictness = other.strictness # sets behaviors in bulk for name in self.all_behaviors: self.set_behavior(name, other.get_behavior(name)) for name in self._plain_attrs: val = getattr(other, name) if isinstance(val, set): val = val.copy() <mask>: val = val.copy() setattr(self, name, val)",False,"elif decimal and isinstance ( val , decimal . Decimal ) :","elif isinstance ( val , dict ) :",0.8820016898747209,25.9162669876144
"def __array_wrap__(self, out_arr, context=None): if self.dim is None: return out_arr else: this = self[:] <mask>: return Quantity.__array_wrap__(self[:], out_arr, context=context) else: return out_arr",False,"if isinstance ( this , Quantity ) :",if this . dim == self . dim :,0.8820016898747209,5.522397783539471
"def _ArgumentListHasDictionaryEntry(self, token): """"""Check if the function argument list has a dictionary as an arg."""""" if _IsArgumentToFunction(token): while token: <mask>: length = token.matching_bracket.total_length - token.total_length return length + self.stack[-2].indent > self.column_limit if token.ClosesScope(): break if token.OpensScope(): token = token.matching_bracket token = token.next_token return False",False,"if token . value == ""{"" :",if token . MatchesScope ( ) :,0.8820016898747209,17.112717058426785
"def save_all_changed_extensions(self): """"""Save configuration changes to the user config file."""""" has_changes = False for ext_name in self.extensions: options = self.extensions[ext_name] for opt in options: <mask>: has_changes = True if has_changes: self.ext_userCfg.Save()",False,"if self . set_extension_value ( ext_name , opt ) :",if opt . HasChanged ( ) :,0.8820016898747209,4.778778209871407
"def to_dict(self): out = {} for key in ACTIVITY_KEYS: attr = getattr(self, key) <mask>: out[key] = str(attr) else: out[key] = attr if self.streak: out[""streak""] = self.streak return out",False,"if isinstance ( attr , ( datetime . timedelta , datetime . datetime ) ) :","if isinstance ( attr , str ) :",0.8820016898747209,21.874242445215227
"def clean_publication_date(cls, cleaned_input): for add_channel in cleaned_input.get(""add_channels"", []): is_published = add_channel.get(""is_published"") publication_date = add_channel.get(""publication_date"") <mask>: add_channel[""publication_date""] = datetime.date.today()",False,if is_published and not publication_date :,if is_published and publication_date :,0.8820016898747209,66.90484408935988
"def _random_blur(self, batch, sigma_max): for i in range(len(batch)): <mask>: # Random sigma sigma = random.uniform(0.0, sigma_max) batch[i] = scipy.ndimage.filters.gaussian_filter(batch[i], sigma) return batch",False,if bool ( random . getrandbits ( 1 ) ) :,if i % 2 == 0 :,0.8820016898747209,4.513617516969122
"def conninfo_parse(dsn): ret = {} length = len(dsn) i = 0 while i < length: if dsn[i].isspace(): i += 1 continue param_match = PARAMETER_RE.match(dsn[i:]) if not param_match: return param = param_match.group(1) i += param_match.end() <mask>: return value, end = read_param_value(dsn[i:]) if value is None: return i += end ret[param] = value return ret",False,if i >= length :,"if param == ""default"" :",0.8820016898747209,7.267884212102741
"def set_environment_vars(env, source_env): """"""Copy allowed environment variables from |source_env|."""""" if not source_env: return for name, value in six.iteritems(source_env): if is_forwarded_environment_variable(name): # Avoid creating circular dependencies from importing environment by # using os.getenv. <mask>: value = file_host.rebase_to_worker_root(value) env[name] = value",False,"if os . getenv ( ""TRUSTED_HOST"" ) and should_rebase_environment_value ( name ) :",if not is_worker_root ( value ) :,0.8820016898747209,3.443855489882646
"def toterminal(self, tw): # the entries might have different styles last_style = None for i, entry in enumerate(self.reprentries): if entry.style == ""long"": tw.line("""") entry.toterminal(tw) <mask>: next_entry = self.reprentries[i + 1] if ( entry.style == ""long"" or entry.style == ""short"" and next_entry.style == ""long"" ): tw.sep(self.entrysep) if self.extraline: tw.line(self.extraline)",False,if i < len ( self . reprentries ) - 1 :,if last_style is not None :,0.8820016898747209,3.983253478176822
"def __init__(self, loc, tabs=None): if os.path.isdir(loc): for item in os.listdir(loc): <mask>: continue path = os.path.join(loc, item) self.append(CronTab(user=False, tabfile=path)) elif os.path.isfile(loc): self.append(CronTab(user=False, tabfile=loc))",False,"if item [ 0 ] == ""."" :","if item . startswith ( ""_"" ) or item . startswith ( ""_"" ) :",0.8820016898747209,5.32864224277779
"def import_data(self, fname): """"""Import data in current namespace"""""" if self.count(): nsb = self.currentWidget() nsb.refresh_table() nsb.import_data(fname) <mask>: self.dockwidget.setVisible(True) self.dockwidget.raise_()",False,if self . dockwidget and not self . ismaximized :,if self . dockwidget :,0.8820016898747209,26.013004751144457
"def get_menu_items(node): aList = [] for child in node.children: for tag in (""@menu"", ""@item""): <mask>: name = child.h[len(tag) + 1 :].strip() if tag == ""@menu"": aList.append((""%s %s"" % (tag, name), get_menu_items(child), None)) else: b = g.splitLines("""".join(child.b)) aList.append((tag, name, b[0] if b else """")) break return aList",True,if child . h . startswith ( tag ) :,if child . h . startswith ( tag ) :,0.8820016898747209,100.00000000000004
"def __init__(self, *args, **kw): if len(args) > 1: raise TypeError(""MultiDict can only be called with one positional "" ""argument"") if args: if hasattr(args[0], ""iteritems""): items = list(args[0].iteritems()) <mask>: items = list(args[0].items()) else: items = list(args[0]) self._items = items else: self._items = [] if kw: self._items.extend(kw.items())",True,"elif hasattr ( args [ 0 ] , ""items"" ) :","elif hasattr ( args [ 0 ] , ""items"" ) :",0.8820016898747209,100.00000000000004
"def open(self) -> ""KeyValueDb"": """"""Create a new data base or open existing one"""""" if os.path.exists(self._name): if not os.path.isfile(self._name): raise IOError(""%s exists and is not a file"" % self._name) <mask>: # ignore empty files return self with open(self._name, ""rb"") as _in: # binary mode self.set_records(pickle.load(_in)) else: # make sure path exists mkpath(os.path.dirname(self._name)) self.commit() return self",False,if os . path . getsize ( self . _name ) == 0 :,"if self . _name . endswith ( "".pickle"" ) :",0.8820016898747209,19.379136152033478
"def sortModules(self): super(NeuronDecomposableNetwork, self).sortModules() self._constructParameterInfo() # contains a list of lists of indices self.decompositionIndices = {} for neuron in self._neuronIterator(): self.decompositionIndices[neuron] = [] for w in range(self.paramdim): inneuron, outneuron = self.paramInfo[w] <mask>: self.decompositionIndices[inneuron].append(w) else: self.decompositionIndices[outneuron].append(w)",False,if self . espStyleDecomposition and outneuron [ 0 ] in self . outmodules :,if inneuron in self . decompositionIndices :,0.8820016898747209,10.218289380194191
"def visit_Options(self, node: qlast.Options) -> None: for i, opt in enumerate(node.options.values()): <mask>: self.write("" "") self.write(opt.name) if not isinstance(opt, qlast.Flag): self.write(f"" {opt.val}"")",False,if i > 0 :,if i == 0 :,0.8820016898747209,22.957488466614336
"def is_child_of(self, item_hash, possible_child_hash): if self.get_last(item_hash) != self.get_last(possible_child_hash): return None while True: if possible_child_hash == item_hash: return True <mask>: return False possible_child_hash = self.items[possible_child_hash].previous_hash",False,if possible_child_hash not in self . items :,if self . items [ possible_child_hash ] . previous_hash != item_hash :,0.8820016898747209,24.04315522172745
"def __call__(self, text, **kargs): words = jieba.tokenize(text, mode=""search"") token = Token() for (w, start_pos, stop_pos) in words: <mask>: continue token.original = token.text = w token.pos = start_pos token.startchar = start_pos token.endchar = stop_pos yield token",False,if not accepted_chars . match ( w ) and len ( w ) <= 1 :,"if w == """" :",0.8820016898747209,1.672612571673144
"def test_analysis_jobs_cypher_syntax(neo4j_session): parameters = { ""AWS_ID"": None, ""UPDATE_TAG"": None, ""OKTA_ORG_ID"": None, } for job_name in contents(""cartography.data.jobs.analysis""): <mask>: continue try: cartography.util.run_analysis_job(job_name, neo4j_session, parameters) except Exception as e: pytest.fail( f""run_analysis_job failed for analysis job '{job_name}' with exception: {e}"" )",False,"if not job_name . endswith ( "".json"" ) :","if job_name . startswith ( ""cypher"" ) :",0.8820016898747209,29.69830085238936
"def _interleave_dataset_results_and_tensors(dataset_results, flat_run_tensors): flattened_results = [] for idx in range(len(dataset_results) + len(flat_run_tensors)): <mask>: flattened_results.append(dataset_results[idx]) else: flattened_results.append(flat_run_tensors.pop(0)) return flattened_results",False,if dataset_results . get ( idx ) :,if dataset_results [ idx ] . dtype == np . float64 :,0.8820016898747209,18.92240568795936
"def test_k_is_stochastic_parameter(self): # k as stochastic parameter aug = iaa.MedianBlur(k=iap.Choice([3, 5])) seen = [False, False] for i in sm.xrange(100): observed = aug.augment_image(self.base_img) <mask>: seen[0] += True elif np.array_equal(observed, self.blur5x5): seen[1] += True else: raise Exception(""Unexpected result in MedianBlur@2"") if all(seen): break assert np.all(seen)",True,"if np . array_equal ( observed , self . blur3x3 ) :","if np . array_equal ( observed , self . blur3x3 ) :",0.8820016898747209,100.00000000000004
"def pickPath(self, color): self.path[color] = () currentPos = self.starts[color] while True: minDist = None minGuide = None for guide in self.guides[color]: guideDist = dist(currentPos, guide) if minDist == None or guideDist < minDist: minDist = guideDist minGuide = guide if dist(currentPos, self.ends[color]) == 1: return <mask>: return self.path[color] = self.path[color] + (minGuide,) currentPos = minGuide self.guides[color].remove(minGuide)",False,if minGuide == None :,if minDist == None :,0.8820016898747209,53.7284965911771
"def UpdateRepository(self): if hasattr(self, ""commit_update""): <mask>: if not path.isdir("".git/""): self.gitZipRepo() call([""git"", ""reset"", ""--hard"", ""origin/{}"".format(self.getBranch)]) self.ProcessCall_([""git"", ""pull"", ""origin"", self.getBranch]) self.ProcessCall_([""pip"", ""install"", ""-r"", ""requirements.txt""])",False,"if self . commit_update [ ""Updates"" ] != [ ] :",if self . commit_update ( ) :,0.8820016898747209,28.046732918876714
"def callback(result=Cr.NS_OK, message=None, success=None): if success is None: <mask>: success = Ci.koIAsyncCallback.RESULT_SUCCESSFUL else: success = Ci.koIAsyncCallback.RESULT_ERROR data = Namespace(result=result, message=message, _com_interfaces_=[Ci.koIErrorInfo]) self._invoke_activate_callbacks(success, data)",False,if Cr . NS_SUCCEEDED ( result ) :,if message is None :,0.8820016898747209,4.673289785800722
"def get_location(device): location = [] node = device while node: position = node.get_position() or """" <mask>: position = "" [%s]"" % position location.append(node.name + position) node = node.parent return "" / "".join(reversed(location))",True,if position :,if position :,0.8820016898747209,0.0
"def load_checkpoint(path, model, optimizer, reset_optimizer): global global_step global global_epoch print(""Load checkpoint from: {}"".format(path)) checkpoint = _load(path) model.load_state_dict(checkpoint[""state_dict""]) if not reset_optimizer: optimizer_state = checkpoint[""optimizer""] <mask>: print(""Load optimizer state from {}"".format(path)) optimizer.load_state_dict(checkpoint[""optimizer""]) global_step = checkpoint[""global_step""] global_epoch = checkpoint[""global_epoch""] return model",False,if optimizer_state is not None :,"if optimizer_state == ""optimizer"" :",0.8820016898747209,27.77619034011791
"def run_command(self, command: str, data: Dict[str, object]) -> Dict[str, object]: """"""Run a specific command from the registry."""""" key = ""cmd_"" + command method = getattr(self.__class__, key, None) if method is None: return {""error"": ""Unrecognized command '%s'"" % command} else: <mask>: # Only the above commands use some error formatting. del data[""is_tty""] del data[""terminal_width""] return method(self, **data)",False,"if command not in { ""check"" , ""recheck"" , ""run"" } :","if ""is_tty"" in data :",0.8820016898747209,2.6227541263820755
"def call_init(self, node, instance): # Call __init__ on each binding. for b in instance.bindings: <mask>: continue self._initialized_instances.add(b.data) node = self._call_init_on_binding(node, b) return node",False,if b . data in self . _initialized_instances :,if b . data is None :,0.8820016898747209,21.28139770959968
"def get_request_headers() -> Dict: url = urlparse(uri) candidates = [ ""%s://%s"" % (url.scheme, url.netloc), ""%s://%s/"" % (url.scheme, url.netloc), uri, ""*"", ] for u in candidates: <mask>: headers = dict(DEFAULT_REQUEST_HEADERS) headers.update(self.config.linkcheck_request_headers[u]) return headers return {}",True,if u in self . config . linkcheck_request_headers :,if u in self . config . linkcheck_request_headers :,0.8820016898747209,100.00000000000004
"def get_next_video_frame(self, skip_empty_frame=True): if not self.video_format: return while True: # We skip video packets which are not video frames # This happens in mkv files for the first few frames. video_packet = self._get_video_packet() if video_packet.image == 0: self._decode_video_packet(video_packet) <mask>: break if _debug: print(""Returning"", video_packet) return video_packet.image",False,if video_packet . image is not None or not skip_empty_frame :,if skip_empty_frame :,0.8820016898747209,20.15216974557266
"def convert_path(ctx, tpath): for points, code in tpath.iter_segments(): if code == Path.MOVETO: ctx.move_to(*points) elif code == Path.LINETO: ctx.line_to(*points) <mask>: ctx.curve_to( points[0], points[1], points[0], points[1], points[2], points[3] ) elif code == Path.CURVE4: ctx.curve_to(*points) elif code == Path.CLOSEPOLY: ctx.close_path()",True,elif code == Path . CURVE3 :,elif code == Path . CURVE3 :,0.8820016898747209,100.00000000000004
"def __init__( self, layout, value=None, string=None, *, dtype: np.dtype = np.float64 ) -> None: """"""Constructor."""""" self.layout = layout if value is None: if string is None: self.value = np.zeros((self.layout.gaDims,), dtype=dtype) else: self.value = layout.parse_multivector(string).value else: self.value = np.array(value) <mask>: raise ValueError( ""value must be a sequence of length %s"" % self.layout.gaDims )",False,"if self . value . shape != ( self . layout . gaDims , ) :",if len ( self . value . shape ) != self . layout . gaDims :,0.8820016898747209,52.92031904718659
"def to_dict(self): contexts_ = {} for k, data in self.contexts.items(): data_ = data.copy() if ""context"" in data_: del data_[""context""] <mask>: del data_[""loaded""] contexts_[k] = data_ return dict(contexts=contexts_)",False,"if ""loaded"" in data_ :","elif ""loaded"" in data_ :",0.8820016898747209,84.08964152537145
"def include_module(module): if not include_these: return True result = False for check in include_these: if ""/*"" in check: <mask>: result = True else: if (os.getcwd() + ""/"" + check + "".py"") == module: result = True if result: print_status(""Including module: "" + module) return result",False,if check [ : - 1 ] in module :,if os . path . isfile ( check ) :,0.8820016898747209,5.522397783539471
"def extract_from(msg_body, content_type=""text/plain""): try: if content_type == ""text/plain"": return extract_from_plain(msg_body) <mask>: return extract_from_html(msg_body) except Exception: log.exception(""ERROR extracting message"") return msg_body",True,"elif content_type == ""text/html"" :","elif content_type == ""text/html"" :",0.8820016898747209,100.00000000000004
"def test_list(self): self._create_locations() response = self.client.get(self.geojson_boxedlocation_list_url) self.assertEqual(response.status_code, 200) self.assertEqual(len(response.data[""features""]), 2) for feature in response.data[""features""]: self.assertIn(""bbox"", feature) fid = feature[""id""] <mask>: self.assertEqual(feature[""bbox""], self.bl1.bbox_geometry.extent) elif fid == 2: self.assertEqual(feature[""bbox""], self.bl2.bbox_geometry.extent) else: self.fail(""Unexpected id: {0}"".format(fid)) BoxedLocation.objects.all().delete()",True,if fid == 1 :,if fid == 1 :,0.8820016898747209,100.00000000000004
"def overrideCommand(self, commandName, func): # Override entries in c.k.masterBindingsDict k = self d = k.masterBindingsDict for key in d: d2 = d.get(key) for key2 in d2: bi = d2.get(key2) <mask>: bi.func = func d2[key2] = bi",True,if bi . commandName == commandName :,if bi . commandName == commandName :,0.8820016898747209,100.00000000000004
"def _lookup(components, specs, provided, name, i, l): if i < l: for spec in specs[i].__sro__: comps = components.get(spec) <mask>: r = _lookup(comps, specs, provided, name, i + 1, l) if r is not None: return r else: for iface in provided: comps = components.get(iface) if comps: r = comps.get(name) if r is not None: return r return None",True,if comps :,if comps :,0.8820016898747209,0.0
"def to_representation(self, value): old_social_string_fields = [""twitter"", ""github"", ""linkedIn""] request = self.context.get(""request"") show_old_format = ( request and is_deprecated(request.version, self.min_version) and request.method == ""GET"" ) if show_old_format: social = value.copy() for key in old_social_string_fields: if social.get(key): social[key] = value[key][0] <mask>: social[key] = """" value = social return super(SocialField, self).to_representation(value)",False,elif social . get ( key ) == [ ] :,"elif social [ key ] == """" :",0.8820016898747209,11.555559242432073
"def process_ref_attribute(self, node, array_type=None): ref = qname_attr(node, ""ref"") if ref: ref = self._create_qname(ref) # Some wsdl's reference to xs:schema, we ignore that for now. It # might be better in the future to process the actual schema file # so that it is handled correctly <mask>: return return xsd_elements.RefAttribute( node.tag, ref, self.schema, array_type=array_type )",False,"if ref . namespace == ""http://www.w3.org/2001/XMLSchema"" :",if not self . schema :,0.8820016898747209,0.7422343966767406
"def unescape(text): """"""Removes '\\' escaping from 'text'."""""" rv = """" i = 0 while i < len(text): <mask>: rv += text[i + 1] i += 1 else: rv += text[i] i += 1 return rv",False,"if i + 1 < len ( text ) and text [ i ] == ""\\"" :","if text [ i ] == ""\\"" :",0.8820016898747209,43.62178812666316
"def wait_child_process(signum, frame): try: while True: child_pid, status = os.waitpid(-1, os.WNOHANG) if child_pid == 0: stat_logger.info(""no child process was immediately available"") break exitcode = status >> 8 stat_logger.info( ""child process %s exit with exitcode %s"", child_pid, exitcode ) except OSError as e: <mask>: stat_logger.warning( ""current process has no existing unwaited-for child processes."" ) else: raise",False,if e . errno == errno . ECHILD :,if e . errno == errno . EINTR :,0.8820016898747209,78.25422900366438
"def translate_from_sortname(name, sortname): """"""'Translate' the artist name by reversing the sortname."""""" for c in name: ctg = unicodedata.category(c) if ctg[0] == ""L"" and unicodedata.name(c).find(""LATIN"") == -1: for separator in ("" & "", ""; "", "" and "", "" vs. "", "" with "", "" y ""): <mask>: parts = sortname.split(separator) break else: parts = [sortname] separator = """" return separator.join(map(_reverse_sortname, parts)) return name",True,if separator in sortname :,if separator in sortname :,0.8820016898747209,100.00000000000004
"def python_value(self, value): if value: <mask>: pp = lambda x: x.time() return format_date_time(value, self.formats, pp) elif isinstance(value, datetime.datetime): return value.time() if value is not None and isinstance(value, datetime.timedelta): return (datetime.datetime.min + value).time() return value",False,"if isinstance ( value , basestring ) :","if isinstance ( value , datetime . datetime ) :",0.8820016898747209,45.180100180492246
"def __init__(self, fileobj, info): pages = [] complete = False while not complete: page = OggPage(fileobj) <mask>: pages.append(page) complete = page.complete or (len(page.packets) > 1) data = OggPage.to_packets(pages)[0][7:] super(OggTheoraCommentDict, self).__init__(data, framing=False) self._padding = len(data) - self._size",False,if page . serial == info . serial :,if page . complete :,0.8820016898747209,15.719010513286515
"def configure(self): # hack to configure 'from_' and 'to' and avoid exception if ""from_"" in self.wmeta.properties: from_ = float(self.wmeta.properties[""from_""]) to = float(self.wmeta.properties.get(""to"", 0)) <mask>: to = from_ + 1 self.wmeta.properties[""to""] = str(to) super(TKSpinbox, self).configure()",False,if from_ > to :,if to < from_ :,0.8820016898747209,20.412414523193146
"def get_error_diagnostics(self): diagnostics = [] if self.stdout is not None: with open(self.stdout.name) as fds: contents = fds.read().strip() <mask>: diagnostics.append(""ab STDOUT:\n"" + contents) if self.stderr is not None: with open(self.stderr.name) as fds: contents = fds.read().strip() if contents.strip(): diagnostics.append(""ab STDERR:\n"" + contents) return diagnostics",True,if contents . strip ( ) :,if contents . strip ( ) :,0.8820016898747209,100.00000000000004
"def set_environment_vars(env, source_env): """"""Copy allowed environment variables from |source_env|."""""" if not source_env: return for name, value in six.iteritems(source_env): <mask>: # Avoid creating circular dependencies from importing environment by # using os.getenv. if os.getenv(""TRUSTED_HOST"") and should_rebase_environment_value(name): value = file_host.rebase_to_worker_root(value) env[name] = value",False,if is_forwarded_environment_variable ( name ) :,if name in env :,0.8820016898747209,3.466791587270993
"def update_content(self, more_content: StringList) -> None: if isinstance(self.object, TypeVar): attrs = [repr(self.object.__name__)] for constraint in self.object.__constraints__: attrs.append(stringify_typehint(constraint)) if self.object.__covariant__: attrs.append(""covariant=True"") <mask>: attrs.append(""contravariant=True"") more_content.append(_(""alias of TypeVar(%s)"") % "", "".join(attrs), """") more_content.append("""", """") super().update_content(more_content)",True,if self . object . __contravariant__ :,if self . object . __contravariant__ :,0.8820016898747209,100.00000000000004
"def after(self, event, state): group = event.group for plugin in self.get_plugins(): <mask>: continue metrics.incr(""notifications.sent"", instance=plugin.slug) yield self.future(plugin.rule_notify)",False,"if not safe_execute ( plugin . should_notify , group = group , event = event ) :",if plugin . rule_notify . group != group :,0.8820016898747209,6.132583535580099
"def distinct(expr, *on): fields = frozenset(expr.fields) _on = [] append = _on.append for n in on: if isinstance(n, Field): <mask>: n = n._name else: raise ValueError(""{0} is not a field of {1}"".format(n, expr)) if not isinstance(n, _strtypes): raise TypeError(""on must be a name or field, not: {0}"".format(n)) elif n not in fields: raise ValueError(""{0} is not a field of {1}"".format(n, expr)) append(n) return Distinct(expr, tuple(_on))",False,if n . _child . isidentical ( expr ) :,if n . _name :,0.8820016898747209,23.350308364304226
"def build_filter(arg): filt = {} if arg is not None: <mask>: raise UserError(""Arguments to --filter should be in form KEY=VAL"") key, val = arg.split(""="", 1) filt[key] = val return filt",True,"if ""="" not in arg :","if ""="" not in arg :",0.8820016898747209,100.00000000000004
"def pickline(file, key, casefold=1): try: f = open(file, ""r"") except IOError: return None pat = re.escape(key) + "":"" prog = re.compile(pat, casefold and re.IGNORECASE) while 1: line = f.readline() if not line: break <mask>: text = line[len(key) + 1 :] while 1: line = f.readline() if not line or not line[0].isspace(): break text = text + line return text.strip() return None",False,if prog . match ( line ) :,if prog . search ( line ) :,0.8820016898747209,50.000000000000014
"def delete_doc(elastic_document_id, node, index=None, category=None): index = index or INDEX if not category: <mask>: category = ""preprint"" elif node.is_registration: category = ""registration"" else: category = node.project_or_component client().delete( index=index, doc_type=category, id=elastic_document_id, refresh=True, ignore=[404], )",False,"if isinstance ( node , Preprint ) :",if node . is_preprint :,0.8820016898747209,7.492442692259767
"def update(self, preds, labels): if not _is_numpy_(labels): raise ValueError(""The 'labels' must be a numpy ndarray."") if not _is_numpy_(preds): raise ValueError(""The 'predictions' must be a numpy ndarray."") for i, lbl in enumerate(labels): value = preds[i, 1] bin_idx = int(value * self._num_thresholds) assert bin_idx <= self._num_thresholds <mask>: self._stat_pos[bin_idx] += 1.0 else: self._stat_neg[bin_idx] += 1.0",False,if lbl :,"if lbl == ""pos"" :",0.8820016898747209,12.22307556087252
"def checkStatusClient(self): if str(self.comboxBoxIPAddress.currentText()) != """": <mask>: self.btnEnable.setEnabled(False) self.btncancel.setEnabled(True) return None self.btnEnable.setEnabled(True) self.btncancel.setEnabled(False)",False,"if self . ClientsLogged [ str ( self . comboxBoxIPAddress . currentText ( ) ) ] [ ""Status"" ] :","if self . comboxBoxIPAddress . currentText ( ) == ""OK"" :",0.8820016898747209,29.740337079891923
"def colorizeDiffs(sheet, col, row, cellval): if not row or not col: return None vcolidx = sheet.visibleCols.index(col) rowidx = sheet.rows.index(row) if vcolidx < len(othersheet.visibleCols) and rowidx < len(othersheet.rows): otherval = othersheet.visibleCols[vcolidx].getDisplayValue( othersheet.rows[rowidx] ) <mask>: return ""color_diff"" else: return ""color_diff_add""",False,if cellval . display != otherval :,if otherval == cellval :,0.8820016898747209,8.69675138635599
"def identwaf(self, findall=False): detected = list() try: self.attackres = self.performCheck(self.centralAttack) except RequestBlocked: return detected for wafvendor in self.checklist: self.log.info(""Checking for %s"" % wafvendor) <mask>: detected.append(wafvendor) if not findall: break self.knowledge[""wafname""] = detected return detected",False,if self . wafdetections [ wafvendor ] ( self ) :,if wafvendor not in detected :,0.8820016898747209,4.642454187453896
"def get_repository_metadata_by_repository_id_changeset_revision( app, id, changeset_revision, metadata_only=False ): """"""Get a specified metadata record for a specified repository in the tool shed."""""" if metadata_only: repository_metadata = get_repository_metadata_by_changeset_revision( app, id, changeset_revision ) <mask>: return repository_metadata.metadata return None return get_repository_metadata_by_changeset_revision(app, id, changeset_revision)",False,if repository_metadata and repository_metadata . metadata :,if repository_metadata :,0.8820016898747209,22.88581105225991
"def getmultiline(self): line = self.getline() if line[3:4] == ""-"": code = line[:3] while 1: nextline = self.getline() line = line + (""\n"" + nextline) <mask>: break return line",False,"if nextline [ : 3 ] == code and nextline [ 3 : 4 ] != ""-"" :","if code == ""\n"" :",0.8820016898747209,4.048765546121646
"def _validate_reports(value, *args, **kwargs): from osf.models import OSFUser for key, val in value.items(): if not OSFUser.load(key): raise ValidationValueError(""Keys must be user IDs"") <mask>: raise ValidationTypeError(""Values must be dictionaries"") if ( ""category"" not in val or ""text"" not in val or ""date"" not in val or ""retracted"" not in val ): raise ValidationValueError( (""Values must include `date`, `category`, "", ""`text`, `retracted` keys"") )",True,"if not isinstance ( val , dict ) :","if not isinstance ( val , dict ) :",0.8820016898747209,100.00000000000004
"def deselectItem(self, item): if self.isSelected(item): <mask>: listItem = self._getListItem(item) selections = self.getSelectedItems() selections.remove(self.loadHandler.getSelection(listItem)) self.setSelections(selections) else: self.deselectAll()",False,if self . multiSelect :,if self . loadHandler . hasSelection ( item ) :,0.8820016898747209,16.784459625186194
"def __init__(self, **kwargs): if self.name is None: raise RuntimeError(""RenderPrimitive cannot be used directly"") self.option_values = {} for key, val in kwargs.items(): if not key in self.options: raise ValueError( ""primitive `{0}' has no option `{1}'"".format(self.name, key) ) self.option_values[key] = val # set up defaults for name, (description, default) in self.options.items(): <mask>: self.option_values[name] = default",False,if not name in self . option_values :,if description :,0.8820016898747209,0.0
"def setup_smart_indent(self, view, lang): # Configure a ""per-view"" instance if type(view) == gedit.View: <mask>: setattr(view, ""smart_indent_instance"", SmartIndent()) handler_id = view.connect( ""key-press-event"", view.smart_indent_instance.key_press_handler ) self.handler_ids.append((handler_id, view)) view.smart_indent_instance.set_language(lang, view)",False,"if getattr ( view , ""smart_indent_instance"" , False ) == False :","if not hasattr ( view , ""smart_indent_instance"" ) :",0.8820016898747209,51.220715058805915
"def get_strings_of_set(word, char_set, threshold=20): count = 0 letters = """" strings = [] for char in word: <mask>: letters += char count += 1 else: if count > threshold: strings.append(letters) letters = """" count = 0 if count > threshold: strings.append(letters) return strings",True,if char in char_set :,if char in char_set :,0.8820016898747209,100.00000000000004
"def TryMerge(self, d): while d.avail() > 0: tt = d.getVarInt32() if tt == 10: self.set_logout_url(d.getPrefixedString()) continue <mask>: raise ProtocolBuffer.ProtocolBufferDecodeError d.skipData(tt)",True,if tt == 0 :,if tt == 0 :,0.8820016898747209,100.00000000000004
def __create_table(self): for i in range(256): crcreg = i for j in range(8): <mask>: crcreg = self.__CRCPOLYNOMIAL ^ (crcreg >> 1) else: crcreg >>= 1 self.__crctable[i] = crcreg,False,if ( crcreg & 1 ) != 0 :,if crcreg & 1 :,0.8820016898747209,16.62083000646927
"def destroy(self): """"""Flush all entries and empty cache"""""" # Note: this method is currently also used for dropping the cache for i in range(len(self.cached_rows)): id_ = self.cached_rows[i] self.cached_rows[i] = None <mask>: try: inode = self.attrs[id_] except KeyError: # We may have deleted that inode pass else: del self.attrs[id_] self.setattr(inode) assert len(self.attrs) == 0",False,if id_ is not None :,if id_ in self . attrs :,0.8820016898747209,22.089591134157878
"def set_config(self): """"""Set configuration options for QTextEdit."""""" c = self.c w = self.widget w.setWordWrapMode(QtGui.QTextOption.NoWrap) if 0: # This only works when there is no style sheet. n = c.config.getInt(""qt-rich-text-zoom-in"") <mask>: w.zoomIn(n) w.updateMicroFocus() # tab stop in pixels - no config for this (yet) w.setTabStopWidth(24)",False,"if n not in ( None , 0 ) :",if n > 0 :,0.8820016898747209,9.346579571601447
"def mouseDragEvent(self, ev): if self.movable and ev.button() == QtCore.Qt.LeftButton: if ev.isStart(): self.moving = True self.cursorOffset = self.pos() - self.mapToParent(ev.buttonDownPos()) self.startPosition = self.pos() ev.accept() if not self.moving: return self.setPos(self.cursorOffset + self.mapToParent(ev.pos())) self.sigDragged.emit(self) <mask>: self.moving = False self.sigPositionChangeFinished.emit(self)",False,if ev . isFinish ( ) :,if ev . isEnd ( ) :,0.8820016898747209,41.11336169005196
"def reparentChildren(self, newParent): if newParent.childNodes: newParent.childNodes[-1]._element.tail += self._element.text else: if not newParent._element.text: newParent._element.text = """" <mask>: newParent._element.text += self._element.text self._element.text = """" base.Node.reparentChildren(self, newParent)",False,if self . _element . text is not None :,elif self . _element . text :,0.8820016898747209,48.59869096699083
"def _no_sp_or_bp(self, bl): for s in bl.vex.statements: for e in chain([s], s.expressions): <mask>: reg = self.get_reg_name(self.project.arch, e.offset) if reg == ""ebp"" or reg == ""esp"": return False elif e.tag == ""Ist_Put"": reg = self.get_reg_name(self.project.arch, e.offset) if reg == ""ebp"" or reg == ""esp"": return False return True",False,"if e . tag == ""Iex_Get"" :","if e . tag == ""Ist_Put"" :",0.8820016898747209,58.59059370151705
"def _get_import_chain(self, *, until=None): stack = inspect.stack()[2:] try: for frameinfo in stack: try: <mask>: continue data = dedent("""".join(frameinfo.code_context)) if data.strip() == until: raise StopIteration yield frameinfo.filename, frameinfo.lineno, data.strip() del data finally: del frameinfo finally: del stack",False,if not frameinfo . code_context :,if frameinfo . code_context is None :,0.8820016898747209,48.54917717073236
"def stream_docker_log(log_stream): async for line in log_stream: if ""stream"" in line and line[""stream""].strip(): logger.debug(line[""stream""].strip()) elif ""status"" in line: logger.debug(line[""status""].strip()) <mask>: logger.error(line[""error""].strip()) raise DockerBuildError",True,"elif ""error"" in line :","elif ""error"" in line :",0.8820016898747209,100.00000000000004
"def get_cycle_path(self, curr_node, goal_node_index): for dep in curr_node[""deps""]: if dep == goal_node_index: return [curr_node[""address""]] for dep in curr_node[""deps""]: path = self.get_cycle_path( self.get_by_address(dep), goal_node_index ) # self.nodelist[dep], goal_node_index) <mask>: path.insert(0, curr_node[""address""]) return path return []",False,if len ( path ) > 0 :,if path :,0.8820016898747209,0.0
"def prompt(default=None): editor = ""nano"" with tempfile.NamedTemporaryFile(mode=""r+"") as tmpfile: <mask>: tmpfile.write(default) tmpfile.flush() child_pid = os.fork() is_child = child_pid == 0 if is_child: os.execvp(editor, [editor, tmpfile.name]) else: os.waitpid(child_pid, 0) tmpfile.seek(0) return tmpfile.read().strip()",True,if default :,if default :,0.8820016898747209,0.0
"def _get_annotated_template(self, template): changed = False if template.get(""version"", ""0.12.0"") >= ""0.13.0"": using_js = self.spider._filter_js_urls(template[""url""]) body = ""rendered_body"" if using_js else ""original_body"" <mask>: template[""body""] = body changed = True if changed or not template.get(""annotated""): _build_sample(template) return template",False,"if template . get ( ""body"" ) != body :",if body :,0.8820016898747209,0.0
"def collect(self, paths): for path in paths or (): relpath = os.path.relpath(path, self._artifact_root) dst = os.path.join(self._directory, relpath) safe_mkdir(os.path.dirname(dst)) <mask>: shutil.copytree(path, dst) else: shutil.copy(path, dst) self._relpaths.add(relpath)",False,if os . path . isdir ( path ) :,if self . _tree :,0.8820016898747209,5.484411595600381
"def dependencies(context=None): """"""Return all dependencies detected by knowit."""""" deps = OrderedDict([]) try: initialize(context) for name, provider_cls in _provider_map.items(): <mask>: deps[name] = available_providers[name].version else: deps[name] = {} except Exception: pass return deps",False,if name in available_providers :,if provider_cls is not None :,0.8820016898747209,7.267884212102741
"def _getaddrinfo(self, host_bytes, port, family, socktype, proto, flags): while True: ares = self.cares try: return self.__getaddrinfo(host_bytes, port, family, socktype, proto, flags) except gaierror: <mask>: raise",False,if ares is self . cares :,if ares . status != gaierror . EADDRINUSE :,0.8820016898747209,9.980099403873663
"def write_entries(cmd, basename, filename): ep = cmd.distribution.entry_points if isinstance(ep, basestring) or ep is None: data = ep elif ep is not None: data = [] for section, contents in ep.items(): <mask>: contents = EntryPoint.parse_group(section, contents) contents = ""\n"".join(map(str, contents.values())) data.append(""[%s]\n%s\n\n"" % (section, contents)) data = """".join(data) cmd.write_or_delete_file(""entry points"", filename, data, True)",False,"if not isinstance ( contents , basestring ) :","if isinstance ( contents , dict ) :",0.8820016898747209,37.70794596593207
"def _highlight_do(self): new_hl_text = self.highlight_text.text() if new_hl_text != self.hl_text: self.hl_text = new_hl_text if self.hl is not None: self.hl.setDocument(None) self.hl = None <mask>: self.hl = Highlighter(self.hl_text, parent=self.doc) self.clear_highlight_button.setEnabled(bool(self.hl))",False,if self . hl_text :,if self . hl_text is not None :,0.8820016898747209,53.7284965911771
"def traverse(node, functions=[]): if hasattr(node, ""grad_fn""): node = node.grad_fn if hasattr(node, ""variable""): node = graph.nodes_by_id.get(id(node.variable)) <mask>: node.functions = list(functions) del functions[:] if hasattr(node, ""next_functions""): functions.append(type(node).__name__) for f in node.next_functions: if f[0]: functions.append(type(f[0]).__name__) traverse(f[0], functions) if hasattr(node, ""saved_tensors""): for t in node.saved_tensors: traverse(t)",False,if node :,if functions :,0.8820016898747209,0.0
"def compress(self, data_list): if data_list: page_id = data_list[1] if page_id in EMPTY_VALUES: <mask>: return None raise forms.ValidationError(self.error_messages[""invalid_page""]) return Page.objects.get(pk=page_id) return None",False,if not self . required :,if page_id == 0 :,0.8820016898747209,6.567274736060395
"def test_field_attr_existence(self): for name, item in ast.__dict__.items(): if self._is_ast_node(name, item): if name == ""Index"": # Index(value) just returns value now. # The argument is required. continue x = item() <mask>: self.assertEqual(type(x._fields), tuple)",False,"if isinstance ( x , ast . AST ) :","if isinstance ( x , ( list , tuple ) ) :",0.8820016898747209,36.462858619364674
"def handle_starttag(self, tag, attrs): if tag == ""base"": self.base_url = dict(attrs).get(""href"") if self.scan_tag(tag): for attr, value in attrs: if self.scan_attr(attr): <mask>: value = strip_html5_whitespace(value) url = self.process_attr(value) link = Link(url=url) self.links.append(link) self.current_link = link",False,if self . strip :,if value :,0.8820016898747209,0.0
"def _initialize_asset_map(cls): # Generating a list of acceptable asset files reduces the possibility of # path attacks. cls._asset_name_to_path = {} assets = os.listdir(ASSETS_PATH) for asset in assets: path = os.path.join(ASSETS_PATH, asset) <mask>: cls._asset_name_to_path[os.path.basename(path)] = path",True,if os . path . isfile ( path ) :,if os . path . isfile ( path ) :,0.8820016898747209,100.00000000000004
"def dataReceived(self, data): self.buf += data if self._paused: log.startLogging(sys.stderr) log.msg(""dataReceived while transport paused!"") self.transport.loseConnection() else: self.transport.write(data) <mask>: self.transport.loseConnection() else: self.pause()",False,"if self . buf . endswith ( b""\n0\n"" ) :",if self . _paused :,0.8820016898747209,6.132184825737391
"def test_case_sensitive(self): with support.EnvironmentVarGuard() as env: env.unset(""PYTHONCASEOK"") <mask>: self.skipTest(""os.environ changes not reflected in "" ""_os.environ"") loader = self.find_module() self.assertIsNone(loader)",False,"if b""PYTHONCASEOK"" in _bootstrap_external . _os . environ :","if not env . get ( ""PYTHONCASEOK"" ) :",0.8820016898747209,10.530522842239176
"def manifest(self): """"""The current manifest dictionary."""""" if self.reload: if not self.exists(self.manifest_path): return {} mtime = self.getmtime(self.manifest_path) <mask>: self._manifest = self.get_manifest() self._mtime = mtime return self._manifest",False,if self . _mtime is None or mtime > self . _mtime :,if mtime < self . _mtime :,0.8820016898747209,23.441874056753782
"def test_named_parameters_and_constraints(self): likelihood = gpytorch.likelihoods.GaussianLikelihood() model = ExactGPModel(None, None, likelihood) for name, _param, constraint in model.named_parameters_and_constraints(): <mask>: self.assertIsInstance(constraint, gpytorch.constraints.GreaterThan) elif name == ""mean_module.constant"": self.assertIsNone(constraint) elif name == ""covar_module.raw_outputscale"": self.assertIsInstance(constraint, gpytorch.constraints.Positive) elif name == ""covar_module.base_kernel.raw_lengthscale"": self.assertIsInstance(constraint, gpytorch.constraints.Positive)",False,"if name == ""likelihood.noise_covar.raw_noise"" :","if name == ""mean_module.constant"" :",0.8820016898747209,27.82095306497951
"def process_plugin_result(name, result): if result: try: jsonify(test=result) except Exception: logger.exception( ""Error while jsonifying settings from plugin {}, please contact the plugin author about this"".format( name ) ) raise else: <mask>: del result[""__enabled""] data[name] = result",True,"if ""__enabled"" in result :","if ""__enabled"" in result :",0.8820016898747209,100.00000000000004
"def benchmarking(net, ctx, num_iteration, datashape=300, batch_size=64): input_shape = (batch_size, 3) + (datashape, datashape) data = mx.random.uniform(-1.0, 1.0, shape=input_shape, ctx=ctx, dtype=""float32"") dryrun = 5 for i in range(dryrun + num_iteration): <mask>: tic = time.time() ids, scores, bboxes = net(data) ids.asnumpy() scores.asnumpy() bboxes.asnumpy() toc = time.time() - tic return toc",False,if i == dryrun :,if i % 100 == 0 :,0.8820016898747209,16.515821590069027
"def merge_weekdays(base_wd, icu_wd): result = [] for left, right in zip(base_wd, icu_wd): <mask>: result.append(left) continue left = set(left.split(""|"")) right = set(right.split(""|"")) result.append(""|"".join(left | right)) return result",True,if left == right :,if left == right :,0.8820016898747209,100.00000000000004
"def create_key(self, request): if self._ignored_parameters: url, body = self._remove_ignored_parameters(request) else: url, body = request.url, request.body key = hashlib.sha256() key.update(_to_bytes(request.method.upper())) key.update(_to_bytes(url)) if request.body: key.update(_to_bytes(body)) else: <mask>: for name, value in sorted(request.headers.items()): key.update(_to_bytes(name)) key.update(_to_bytes(value)) return key.hexdigest()",False,if self . _include_get_headers and request . headers != _DEFAULT_HEADERS :,if request . headers :,0.8820016898747209,2.2493847365531097
"def test_invalid_mountinfo(self): line = ( ""20 1 252:1 / / rw,relatime - ext4 /dev/mapper/vg0-root"" ""rw,errors=remount-ro,data=ordered"" ) elements = line.split() for i in range(len(elements) + 1): lines = ["" "".join(elements[0:i])] <mask>: expected = None else: expected = (""/dev/mapper/vg0-root"", ""ext4"", ""/"") self.assertEqual(expected, util.parse_mount_info(""/"", lines))",False,if i < 10 :,if i == 0 :,0.8820016898747209,17.965205598154213
"def nested_filter(self, items, mask): keep_current = self.current_mask(mask) keep_nested_lookup = self.nested_masks(mask) for k, v in items: keep_nested = keep_nested_lookup.get(k) if k in keep_current: if keep_nested is not None: <mask>: yield k, dict(self.nested_filter(v.items(), keep_nested)) else: yield k, v",True,"if isinstance ( v , dict ) :","if isinstance ( v , dict ) :",0.8820016898747209,100.00000000000004
"def traverse_trees(node_pos, sample, trees: List[HeteroDecisionTreeGuest]): if node_pos[""reach_leaf_node""].all(): return node_pos for t_idx, tree in enumerate(trees): cur_node_idx = node_pos[""node_pos""][t_idx] # reach leaf if cur_node_idx == -1: continue rs, reach_leaf = HeteroSecureBoostingTreeGuest.traverse_a_tree( tree, sample, cur_node_idx ) <mask>: node_pos[""reach_leaf_node""][t_idx] = True node_pos[""node_pos""][t_idx] = rs return node_pos",True,if reach_leaf :,if reach_leaf :,0.8820016898747209,100.00000000000004
"def _pop_waiting_trial_id(self) -> Optional[int]: # TODO(c-bata): Reduce database query counts for extracting waiting trials. for trial in self._storage.get_all_trials(self._study_id, deepcopy=False): <mask>: continue if not self._storage.set_trial_state(trial._trial_id, TrialState.RUNNING): continue _logger.debug(""Trial {} popped from the trial queue."".format(trial.number)) return trial._trial_id return None",False,if trial . state != TrialState . WAITING :,if not trial . is_waiting :,0.8820016898747209,10.229197414177778
"def get_step_best(self, step_models): best_score = None best_model = """" for model in step_models: model_info = self.models_trained[model] score = model_info.get_score() <mask>: continue if best_score is None or score < best_score: best_score = score best_model = model LOGGER.info(f""step {self.n_step}, best model {best_model}"") return best_model",True,if score is None :,if score is None :,0.8820016898747209,100.00000000000004
"def iter_filters(filters, block_end=False): queue = deque(filters) while queue: f = queue.popleft() if f is not None and f.type in (""or"", ""and"", ""not""): <mask>: queue.appendleft(None) for gf in f.filters: queue.appendleft(gf) yield f",True,if block_end :,if block_end :,0.8820016898747209,100.00000000000004
"def _buffer_decode(self, input, errors, final): if self.decoder is None: (output, consumed, byteorder) = codecs.utf_16_ex_decode(input, errors, 0, final) if byteorder == -1: self.decoder = codecs.utf_16_le_decode <mask>: self.decoder = codecs.utf_16_be_decode elif consumed >= 2: raise UnicodeError(""UTF-16 stream does not start with BOM"") return (output, consumed) return self.decoder(input, self.errors, final)",True,elif byteorder == 1 :,elif byteorder == 1 :,0.8820016898747209,100.00000000000004
"def _load_db(self): try: with open(self.db) as db: content = db.read(8) db.seek(0) <mask>: data = StringIO() if self.encryptor: self.encryptor.decrypt(db, data) else: raise EncryptionError( ""Encrpyted credential storage: {}"".format(self.db) ) return json.loads(data.getvalue()) else: return json.load(db) except: return {""creds"": []}",False,"if content == ( ""Salted__"" ) :","if content == b""credential"" :",0.8820016898747209,24.598127518343304
"def _getbytes(self, start, l=1): out = [] for ad in range(l): offset = ad + start + self.base_address <mask>: raise IOError(""not enough bytes"") out.append(int_to_byte(Byte(offset))) return b"""".join(out)",False,if not is_mapped ( offset ) :,if offset > self . size :,0.8820016898747209,6.495032985064742
"def cache_sqs_queues_across_accounts() -> bool: function: str = f""{__name__}.{sys._getframe().f_code.co_name}"" # First, get list of accounts accounts_d: list = async_to_sync(get_account_id_to_name_mapping)() # Second, call tasks to enumerate all the roles across all accounts for account_id in accounts_d.keys(): if config.get(""environment"") == ""prod"": cache_sqs_queues_for_account.delay(account_id) else: <mask>: cache_sqs_queues_for_account.delay(account_id) stats.count(f""{function}.success"") return True",False,"if account_id in config . get ( ""celery.test_account_ids"" , [ ] ) :","if config . get ( ""environment"" ) == ""prod"" :",0.8820016898747209,16.716822873683913
"def insertLine(self, refnum, linenum, line): i = -1 for i, row in enumerate(self.rows): if row[0] == linenum: <mask>: row[refnum + 1] = line return # else keep looking elif row[0] > linenum: break self.rows.insert(i, self.newRow(linenum, refnum, line))",False,if row [ refnum + 1 ] is None :,if i == len ( self . rows ) - 1 :,0.8820016898747209,4.065425428798724
"def __setattr__(self, name, val): if self.__dict__.get(name, ""hamster_graphics_no_value_really"") == val: return Sprite.__setattr__(self, name, val) if name == ""image_data"": self._surface = None <mask>: self.__dict__[""width""] = self.image_data.get_width() self.__dict__[""height""] = self.image_data.get_height()",True,if self . image_data :,if self . image_data :,0.8820016898747209,100.00000000000004
"def process_signature(app, what, name, obj, options, signature, return_annotation): if signature: # replace Mock function names signature = re.sub(""<Mock name='([^']+)'.*>"", ""\g<1>"", signature) signature = re.sub(""tensorflow"", ""tf"", signature) # add scope name to layer signatures: if hasattr(obj, ""use_scope""): <mask>: signature = signature[0] + ""variable_scope_name, "" + signature[1:] elif obj.use_scope is None: signature = signature[0] + ""[variable_scope_name,] "" + signature[1:] # signature: arg list return signature, return_annotation",False,if obj . use_scope :,if obj . use_scope is not None :,0.8820016898747209,53.7284965911771
"def L_op(self, inputs, outputs, gout): (x,) = inputs (gz,) = gout if x.type in complex_types: raise NotImplementedError() if outputs[0].type in discrete_types: <mask>: return [x.zeros_like(dtype=theano.config.floatX)] else: return [x.zeros_like()] return (gz * (1 - sqr(tanh(x))),)",True,if x . type in discrete_types :,if x . type in discrete_types :,0.8820016898747209,100.00000000000004
"def confirm_on_console(topic, msg): done = False print(topic) while not done: output = raw_input(msg + "":[y/n]"") <mask>: return True if output.lower() == ""n"": return False",True,"if output . lower ( ) == ""y"" :","if output . lower ( ) == ""y"" :",0.8820016898747209,100.00000000000004
"def replace_documentation_for_matching_shape(self, event_name, section, **kwargs): if self._shape_name == section.context.get(""shape""): self._replace_documentation(event_name, section) for section_name in section.available_sections: sub_section = section.get_section(section_name) <mask>: self._replace_documentation(event_name, sub_section) else: self.replace_documentation_for_matching_shape(event_name, sub_section)",True,"if self . _shape_name == sub_section . context . get ( ""shape"" ) :","if self . _shape_name == sub_section . context . get ( ""shape"" ) :",0.8820016898747209,100.00000000000004
"def confirm_on_console(topic, msg): done = False print(topic) while not done: output = raw_input(msg + "":[y/n]"") if output.lower() == ""y"": return True <mask>: return False",False,"if output . lower ( ) == ""n"" :","elif output . lower ( ) == ""n"" :",0.8820016898747209,90.36020036098445
"def __getitem__(self, index): if self._check(): if isinstance(index, int): <mask>: raise IndexError(index) if self.features[index] is None: feature = self.device.feature_request(FEATURE.FEATURE_SET, 0x10, index) if feature: (feature,) = _unpack(""!H"", feature[:2]) self.features[index] = FEATURE[feature] return self.features[index] elif isinstance(index, slice): indices = index.indices(len(self.features)) return [self.__getitem__(i) for i in range(*indices)]",False,if index < 0 or index >= len ( self . features ) :,if index < 0 :,0.8820016898747209,9.569649651041097
"def _parse_locator(self, locator): prefix = None criteria = locator if not locator.startswith(""//""): locator_parts = locator.partition(""="") <mask>: prefix = locator_parts[0] criteria = locator_parts[2].strip() return (prefix, criteria)",False,if len ( locator_parts [ 1 ] ) > 0 :,if len ( locator_parts ) == 3 :,0.8820016898747209,41.36817680097793
"def trakt_episode_data_generate(self, data): # Find how many unique season we have uniqueSeasons = [] for season, episode in data: <mask>: uniqueSeasons.append(season) # build the query seasonsList = [] for searchedSeason in uniqueSeasons: episodesList = [] for season, episode in data: if season == searchedSeason: episodesList.append({""number"": episode}) seasonsList.append({""number"": searchedSeason, ""episodes"": episodesList}) post_data = {""seasons"": seasonsList} return post_data",True,if season not in uniqueSeasons :,if season not in uniqueSeasons :,0.8820016898747209,100.00000000000004
"def __init__(self, data, n_bins): bin_width = span / n_bins bins = [0] * n_bins for x in data: b = int(mpfloor((x - minimum) / bin_width)) <mask>: b = 0 elif b >= n_bins: b = n_bins - 1 bins[b] += 1 self.bins = bins self.bin_width = bin_width",True,if b < 0 :,if b < 0 :,0.8820016898747209,100.00000000000004
"def infer_context(typ, context=""http://schema.org""): parsed_context = urlparse(typ) if parsed_context.netloc: base = """".join([parsed_context.scheme, ""://"", parsed_context.netloc]) <mask>: context = urljoin(base, parsed_context.path) typ = parsed_context.fragment.strip(""/"") elif parsed_context.path: context = base typ = parsed_context.path.strip(""/"") return context, typ",False,if parsed_context . path and parsed_context . fragment :,if parsed_context . fragment :,0.8820016898747209,42.43728456769501
"def parse(self, items): for index, item in enumerate(items): keys = self.build_key(item) if keys is None: continue # Update `items` self.items[tuple(keys)] = (index, item) # Update `table` <mask>: log.info(""Unable to update table (keys: %r)"", keys)",False,"if not self . path_set ( self . table , keys , ( index , item ) ) :",if self . table is None :,0.8820016898747209,3.759098586913923
"def dict_to_XML(tag, dictionary, **kwargs): """"""Return XML element converting dicts recursively."""""" elem = Element(tag, **kwargs) for key, val in dictionary.items(): if tag == ""layers"": child = dict_to_XML(""layer"", val, name=key) elif isinstance(val, MutableMapping): child = dict_to_XML(key, val) else: <mask>: child = Element(""variable"", name=key) else: child = Element(key) child.text = str(val) elem.append(child) return elem",False,"if tag == ""config"" :","if tag == ""variables"" :",0.8820016898747209,59.4603557501361
"def _get_config_value(self, section, key): if section: <mask>: self.log.error(""Error: Config section '%s' not found"", section) return None return self.config[section].get(key, self.config[key]) else: return self.config[key]",True,if section not in self . config :,if section not in self . config :,0.8820016898747209,100.00000000000004
"def h_line_down(self, input): end_this_line = self.value.find(""\n"", self.cursor_position) if end_this_line == -1: if self.scroll_exit: self.h_exit_down(None) else: self.cursor_position = len(self.value) else: self.cursor_position = end_this_line + 1 for x in range(self.cursorx): <mask>: break elif self.value[self.cursor_position] == ""\n"": break else: self.cursor_position += 1",False,if self . cursor_position > len ( self . value ) - 1 :,if self . value [ self . cursor_position ] == input :,0.8820016898747209,35.76725172906465
"def printsumfp(fp, filename, out=sys.stdout): m = md5() try: while 1: data = fp.read(bufsize) <mask>: break if isinstance(data, str): data = data.encode(fp.encoding) m.update(data) except IOError as msg: sys.stderr.write(""%s: I/O error: %s\n"" % (filename, msg)) return 1 out.write(""%s %s\n"" % (m.hexdigest(), filename)) return 0",True,if not data :,if not data :,0.8820016898747209,100.00000000000004
"def main(input): logging.info(""Running Azure Cloud Custodian Policy %s"", input) context = { ""config_file"": join(function_directory, ""config.json""), ""auth_file"": join(function_directory, ""auth.json""), } event = None subscription_id = None if isinstance(input, QueueMessage): <mask>: return event = input.get_json() subscription_id = ResourceIdParser.get_subscription_id(event[""subject""]) handler.run(event, context, subscription_id)",False,if input . dequeue_count > max_dequeue_count :,if input . get_json ( ) is None :,0.8820016898747209,13.215955651112736
"def maybeExtractTarball(self): if self.tarball: tar = self.computeTarballOptions() + [""-xvf"", self.tarball] res = yield self._Cmd(tar, abandonOnFailure=False) <mask>: # error with tarball.. erase repo dir and tarball yield self._Cmd([""rm"", ""-f"", self.tarball], abandonOnFailure=False) yield self.runRmdir(self.repoDir(), abandonOnFailure=False)",False,if res :,if res != 0 :,0.8820016898747209,17.965205598154213
"def execute(self, arbiter, props): watcher = self._get_watcher(arbiter, props.pop(""name"")) action = 0 for key, val in props.get(""options"", {}).items(): <mask>: new_action = 0 for name, _val in val.items(): action = watcher.set_opt(""hooks.%s"" % name, _val) if action == 1: new_action = 1 else: new_action = watcher.set_opt(key, val) if new_action == 1: action = 1 # trigger needed action return watcher.do_action(action)",False,"if key == ""hooks"" :","if isinstance ( val , dict ) :",0.8820016898747209,6.567274736060395
"def _import_playlists(self, fns, library): added = 0 for filename in fns: name = _name_for(filename) with open(filename, ""rb"") as f: <mask>: playlist = parse_m3u(f, name, library=library) elif filename.endswith("".pls""): playlist = parse_pls(f, name, library=library) else: print_w(""Unsupported playlist type for '%s'"" % filename) continue self.changed(playlist) library.add(playlist) added += 1 return added",False,"if filename . endswith ( "".m3u"" ) or filename . endswith ( "".m3u8"" ) :","if filename . endswith ( "".m3u"" ) :",0.8820016898747209,38.966271115357685
"def unwrap_term_buckets(self, timestamp, term_buckets): for term_data in term_buckets: <mask>: self.unwrap_interval_buckets( timestamp, term_data[""key""], term_data[""interval_aggs""][""buckets""] ) else: self.check_matches(timestamp, term_data[""key""], term_data)",True,"if ""interval_aggs"" in term_data :","if ""interval_aggs"" in term_data :",0.8820016898747209,100.00000000000004
"def _get_exception(flags, timeout_ms, payload_size): if flags & FLAG_ERROR: if flags & FLAG_TIMEOUT: return SpicommTimeoutError(timeout_ms / 1000.0) <mask>: return SpicommOverflowError(payload_size) return SpicommError() return None",True,if flags & FLAG_OVERFLOW :,if flags & FLAG_OVERFLOW :,0.8820016898747209,100.00000000000004
"def _get_pattern(self, pattern_id): """"""Get pattern item by id."""""" for key in (Tag.PATTERNS1, Tag.PATTERNS2, Tag.PATTERNS3): if key in self.tagged_blocks: data = self.tagged_blocks.get_data(key) for pattern in data: <mask>: return pattern return None",False,if pattern . pattern_id == pattern_id :,if pattern . id == pattern_id :,0.8820016898747209,64.32188699036833
"def print_quiet(self, context, *args, **kwargs): for index, (key, value) in enumerate( itertools.chain(enumerate(args), kwargs.items()) ): <mask>: print( self.format_quiet(index, key, value, fields=context.get_input_fields()) )",False,"if self . filter ( index , key , value ) :","if key != ""value"" :",0.8820016898747209,4.736913377107212
"def complete(self, block): with self._condition: if not self._final: return False if self._complete(): self._calculate_state_root_if_not_already_done() return True <mask>: self._condition.wait_for(self._complete) self._calculate_state_root_if_not_already_done() return True return False",True,if block :,if block :,0.8820016898747209,0.0
"def compression_rotator(source, dest): with open(source, ""rb"") as sf: with gzip.open(dest, ""wb"") as wf: while True: data = sf.read(CHUNK_SIZE) <mask>: break wf.write(data) os.remove(source)",True,if not data :,if not data :,0.8820016898747209,100.00000000000004
"def mockup(self, records): provider = TransipProvider("""", """", """") _dns_entries = [] for record in records: <mask>: entries_for = getattr(provider, ""_entries_for_{}"".format(record._type)) # Root records have '@' as name name = record.name if name == """": name = provider.ROOT_RECORD _dns_entries.extend(entries_for(name, record)) # NS is not supported as a DNS Entry, # so it should cover the if statement _dns_entries.append(DnsEntry(""@"", ""3600"", ""NS"", ""ns01.transip.nl."")) self.mockupEntries = _dns_entries",False,if record . _type in provider . SUPPORTS :,if record . _type in provider . ROOT_RECORD :,0.8820016898747209,63.15552371794033
"def parse_known_args(self, args=None, namespace=None): entrypoint = self.prog.split("" "")[0] try: defs = get_defaults_for_argparse(entrypoint) ignore = defs.pop(""Ignore"", None) self.set_defaults(**defs) <mask>: set_notebook_diff_ignores(ignore) except ValueError: pass return super(ConfigBackedParser, self).parse_known_args( args=args, namespace=namespace )",True,if ignore :,if ignore :,0.8820016898747209,0.0
"def _maybeRebuildAtlas(self, threshold=4, minlen=1000): n = len(self.fragmentAtlas) if (n > minlen) and (n > threshold * len(self.data)): self.fragmentAtlas.rebuild( list(zip(*self._style([""symbol"", ""size"", ""pen"", ""brush""]))) ) self.data[""sourceRect""] = 0 <mask>: self._sourceQRect.clear() self.updateSpots()",False,if _USE_QRECT :,"if self . data [ ""sourceQRect"" ] :",0.8820016898747209,4.990049701936832
"def dispatch_return(self, frame, arg): if self.stop_here(frame) or frame == self.returnframe: # Ignore return events in generator except when stepping. <mask>: return self.trace_dispatch try: self.frame_returning = frame self.user_return(frame, arg) finally: self.frame_returning = None if self.quitting: raise BdbQuit # The user issued a 'next' or 'until' command. if self.stopframe is frame and self.stoplineno != -1: self._set_stopinfo(None, None) return self.trace_dispatch",False,if self . stopframe and frame . f_code . co_flags & CO_GENERATOR :,if self . stopframe is frame :,0.8820016898747209,8.194094675927117
"def tearDown(self): if not self.is_playback(): try: if self.hosted_service_name is not None: self.sms.delete_hosted_service(self.hosted_service_name) except: pass try: <mask>: self.sms.delete_storage_account(self.storage_account_name) except: pass try: self.sms.delete_affinity_group(self.affinity_group_name) except: pass return super(LegacyMgmtAffinityGroupTest, self).tearDown()",True,if self . storage_account_name is not None :,if self . storage_account_name is not None :,0.8820016898747209,100.00000000000004
"def make_log_msg(self, msg, *other_messages): MAX_MESSAGE_LENGTH = 1000 if not other_messages: # assume that msg is a single string return msg[-MAX_MESSAGE_LENGTH:] else: if len(msg): msg += ""\n...\n"" NEXT_MESSAGE_OFFSET = MAX_MESSAGE_LENGTH - len(msg) else: NEXT_MESSAGE_OFFSET = MAX_MESSAGE_LENGTH <mask>: msg += other_messages[0][-NEXT_MESSAGE_OFFSET:] return self.make_log_msg(msg, *other_messages[1:]) else: return self.make_log_msg(msg)",False,if NEXT_MESSAGE_OFFSET > 0 :,if len ( other_messages ) > NEXT_MESSAGE_OFFSET :,0.8820016898747209,29.89950354998137
"def wrapper( # type: ignore self: RequestHandler, *args, **kwargs ) -> Optional[Awaitable[None]]: if self.request.path.endswith(""/""): if self.request.method in (""GET"", ""HEAD""): uri = self.request.path.rstrip(""/"") if uri: # don't try to redirect '/' to '' <mask>: uri += ""?"" + self.request.query self.redirect(uri, permanent=True) return None else: raise HTTPError(404) return method(self, *args, **kwargs)",True,if self . request . query :,if self . request . query :,0.8820016898747209,100.00000000000004
"def process_lib(vars_, coreval): for d in vars_: var = d.upper() if var == ""QTCORE"": continue value = env[""LIBPATH_"" + var] <mask>: core = env[coreval] accu = [] for lib in value: if lib in core: continue accu.append(lib) env[""LIBPATH_"" + var] = accu",False,if value :,if coreval in value :,0.8820016898747209,23.643540225079384
"def _attach_children(self, other, exclude_worldbody, dry_run=False): for other_child in other.all_children(): <mask>: self_child = self.get_children(other_child.spec.name) self_child._attach( other_child, exclude_worldbody, dry_run ) # pylint: disable=protected-access",False,if not other_child . spec . repeated :,if other_child . spec . name in self . _children_to_attach :,0.8820016898747209,28.43329181530769
"def getDictFromTree(tree): ret_dict = {} for child in tree.getchildren(): if child.getchildren(): ## Complex-type child. Recurse content = getDictFromTree(child) else: content = child.text <mask>: if not type(ret_dict[child.tag]) == list: ret_dict[child.tag] = [ret_dict[child.tag]] ret_dict[child.tag].append(content or """") else: ret_dict[child.tag] = content or """" return ret_dict",False,if ret_dict . has_key ( child . tag ) :,if child . tag in ret_dict :,0.8820016898747209,18.402097851927994
"def nsUriMatch(self, value, wanted, strict=0, tt=type(())): """"""Return a true value if two namespace uri values match."""""" if value == wanted or (type(wanted) is tt) and value in wanted: return 1 if not strict and value is not None: wanted = type(wanted) is tt and wanted or (wanted,) value = value[-1:] != ""/"" and value or value[:-1] for item in wanted: <mask>: return 1 return 0",False,if item == value or item [ : - 1 ] == value :,if item == value and item in wanted :,0.8820016898747209,23.45000810620359
"def update_repository(self, ignore_issues=False, force=False): """"""Update."""""" if not await self.common_update(ignore_issues, force): return # Get appdaemon objects. if self.repository_manifest: <mask>: self.content.path.remote = """" if self.content.path.remote == ""apps"": self.data.domain = get_first_directory_in_directory( self.tree, self.content.path.remote ) self.content.path.remote = f""apps/{self.data.name}"" # Set local path self.content.path.local = self.localpath",False,if self . data . content_in_root :,if not self . content . path . remote :,0.8820016898747209,11.884631831419354
"def addOutput(self, data, isAsync=None, **kwargs): isAsync = _get_async_param(isAsync, **kwargs) if isAsync: self.terminal.eraseLine() self.terminal.cursorBackward(len(self.lineBuffer) + len(self.ps[self.pn])) self.terminal.write(data) if isAsync: <mask>: self.terminal.nextLine() self.terminal.write(self.ps[self.pn]) if self.lineBuffer: oldBuffer = self.lineBuffer self.lineBuffer = [] self.lineBufferIndex = 0 self._deliverBuffer(oldBuffer)",False,if self . _needsNewline ( ) :,if self . lineBuffer :,0.8820016898747209,23.4500081062036
"def is_installed(self, dlc_title="""") -> bool: installed = False if dlc_title: dlc_version = self.get_dlc_info(""version"", dlc_title) installed = True if dlc_version else False # Start: Code for compatibility with minigalaxy 1.0 <mask>: status = self.legacy_get_dlc_status(dlc_title) installed = True if status in [""installed"", ""updatable""] else False # End: Code for compatibility with minigalaxy 1.0 else: if self.install_dir and os.path.exists(self.install_dir): installed = True return installed",False,if not installed :,if self . legacy_get_dlc_status ( dlc_title ) :,0.8820016898747209,2.908317710573757
"def close(self): self.selector.close() if self.sock: sockname = None try: sockname = self.sock.getsockname() except (socket.error, OSError): pass self.sock.close() <mask>: # it was a Unix domain socket, remove it from the filesystem if os.path.exists(sockname): os.remove(sockname) self.sock = None",False,if type ( sockname ) is str :,if sockname :,0.8820016898747209,0.0
"def post_file(self, file_path, graph_type=""edges"", file_type=""csv""): dataset_id = self.dataset_id tok = self.token base_path = self.server_base_path with open(file_path, ""rb"") as file: out = requests.post( f""{base_path}/api/v2/upload/datasets/{dataset_id}/{graph_type}/{file_type}"", verify=self.certificate_validation, headers={""Authorization"": f""Bearer {tok}""}, data=file.read(), ).json() <mask>: raise Exception(out) return out",False,"if not out [ ""success"" ] :","if out [ ""error"" ] :",0.8820016898747209,31.708476589333063
"def _get_vqa_v2_image_raw_dataset(directory, image_root_url, image_urls): """"""Extract the VQA V2 image data set to directory unless it's there."""""" for url in image_urls: filename = os.path.basename(url) download_url = os.path.join(image_root_url, url) path = generator_utils.maybe_download(directory, filename, download_url) unzip_dir = os.path.join(directory, filename.strip("".zip"")) <mask>: zipfile.ZipFile(path, ""r"").extractall(directory)",False,if not tf . gfile . Exists ( unzip_dir ) :,if os . path . exists ( unzip_dir ) :,0.8820016898747209,42.481820832988255
"def __call__(self, environ, start_response): for key in ""REQUEST_URL"", ""REQUEST_URI"", ""UNENCODED_URL"": <mask>: continue request_uri = unquote(environ[key]) script_name = unquote(environ.get(""SCRIPT_NAME"", """")) if request_uri.startswith(script_name): environ[""PATH_INFO""] = request_uri[len(script_name) :].split(""?"", 1)[0] break return self.app(environ, start_response)",True,if key not in environ :,if key not in environ :,0.8820016898747209,100.00000000000004
"def _instrument_model(self, model): for key, value in list( model.__dict__.items() ): # avoid ""dictionary keys changed during iteration"" if isinstance(value, tf.keras.layers.Layer): new_layer = self._instrument(value) if new_layer is not value: setattr(model, key, new_layer) <mask>: for i, item in enumerate(value): if isinstance(item, tf.keras.layers.Layer): value[i] = self._instrument(item) return model",True,"elif isinstance ( value , list ) :","elif isinstance ( value , list ) :",0.8820016898747209,100.00000000000004
"def __init__(self, parent, dir, mask, with_dirs=True): filelist = [] dirlist = [""..""] self.dir = dir self.file = """" mask = mask.upper() pattern = self.MakeRegex(mask) for i in os.listdir(dir): if i == ""."" or i == "".."": continue path = os.path.join(dir, i) if os.path.isdir(path): dirlist.append(i) continue path = path.upper() value = i.upper() <mask>: filelist.append(i) self.files = filelist if with_dirs: self.dirs = dirlist",False,if pattern . match ( value ) is not None :,"if pattern . search ( path , value ) :",0.8820016898747209,19.331263581394154
"def get_text(self, nodelist): """"""Return a string representation of the motif's properties listed on nodelist ."""""" retlist = [] for node in nodelist: if node.nodeType == Node.TEXT_NODE: retlist.append(node.wholeText) <mask>: retlist.append(self.get_text(node.childNodes)) return re.sub(r""\s+"", "" "", """".join(retlist))",False,elif node . hasChildNodes :,elif node . nodeType == Node . ELEMENT_NODE :,0.8820016898747209,13.545994273378144
"def _persist_metadata(self, dirname, filename): metadata_path = ""{0}/{1}.json"".format(dirname, filename) if self.media_metadata or self.comments or self.include_location: if self.posts: if self.latest: self.merge_json({""GraphImages"": self.posts}, metadata_path) else: self.save_json({""GraphImages"": self.posts}, metadata_path) <mask>: if self.latest: self.merge_json({""GraphStories"": self.stories}, metadata_path) else: self.save_json({""GraphStories"": self.stories}, metadata_path)",False,if self . stories :,elif self .ories :,0.8820016898747209,23.643540225079384
"def _get_python_wrapper_content(self, job_class, args): job = job_class([""-r"", ""hadoop""] + list(args)) job.sandbox() with job.make_runner() as runner: runner._create_setup_wrapper_scripts() <mask>: with open(runner._spark_python_wrapper_path) as f: return f.read() else: return None",True,if runner . _spark_python_wrapper_path :,if runner . _spark_python_wrapper_path :,0.8820016898747209,100.00000000000004
"def computeLeadingWhitespaceWidth(s, tab_width): w = 0 for ch in s: <mask>: w += 1 elif ch == ""\t"": w += abs(tab_width) - (w % abs(tab_width)) else: break return w",False,"if ch == "" "" :","if ch == ""\n"" :",0.8820016898747209,51.33450480401705
def run(self): # if the i3status process dies we want to restart it. # We give up restarting if we have died too often for _ in range(10): <mask>: break self.spawn_i3status() # check if we never worked properly and if so quit now if not self.ready: break # limit restart rate self.lock.wait(5),False,if not self . py3_wrapper . running :,if self . stopped :,0.8820016898747209,9.346579571601447
"def translate_len( builder: IRBuilder, expr: CallExpr, callee: RefExpr ) -> Optional[Value]: # Special case builtins.len if len(expr.args) == 1 and expr.arg_kinds == [ARG_POS]: expr_rtype = builder.node_type(expr.args[0]) <mask>: # len() of fixed-length tuple can be trivially determined statically, # though we still need to evaluate it. builder.accept(expr.args[0]) return Integer(len(expr_rtype.types)) else: obj = builder.accept(expr.args[0]) return builder.builtin_len(obj, -1) return None",False,"if isinstance ( expr_rtype , RTuple ) :","if expr_rtype . type == ""fixed_length"" :",0.8820016898747209,12.011055432195764
"def parse_auth(val): if val is not None: authtype, params = val.split("" "", 1) if authtype in known_auth_schemes: <mask>: # this is the ""Authentication: Basic XXXXX=="" case pass else: params = parse_auth_params(params) return authtype, params return val",False,"if authtype == ""Basic"" and '""' not in params :","if params == ""basic"" :",0.8820016898747209,10.59106218302618
"def toxml(self): text = self.value self.parent.setBidi(getBidiType(text)) if not text.startswith(HTML_PLACEHOLDER_PREFIX): if self.parent.nodeName == ""p"": text = text.replace(""\n"", ""\n "") <mask>: text = ""\n "" + text.replace(""\n"", ""\n "") text = self.doc.normalizeEntities(text) return text",False,"elif self . parent . nodeName == ""li"" and self . parent . childNodes [ 0 ] == self :","elif self . parent . nodeName == ""p"" :",0.8820016898747209,27.474791071543024
"def get_all_related_many_to_many_objects(self): try: # Try the cache first. return self._all_related_many_to_many_objects except AttributeError: rel_objs = [] for klass in get_models(): for f in klass._meta.many_to_many: <mask>: rel_objs.append(RelatedObject(f.rel.to, klass, f)) self._all_related_many_to_many_objects = rel_objs return rel_objs",False,if f . rel and self == f . rel . to . _meta :,if f . rel . to :,0.8820016898747209,20.15216974557266
"def state_highstate(self, state, dirpath): opts = copy.copy(self.config) opts[""file_roots""] = dict(base=[dirpath]) HIGHSTATE = HighState(opts) HIGHSTATE.push_active() try: high, errors = HIGHSTATE.render_highstate(state) <mask>: import pprint pprint.pprint(""\n"".join(errors)) pprint.pprint(high) out = HIGHSTATE.state.call_high(high) # pprint.pprint(out) finally: HIGHSTATE.pop_active()",True,if errors :,if errors :,0.8820016898747209,0.0
"def _update_target_host(self, target, target_host): """"""Update target host."""""" target_host = None if target_host == """" else target_host if not target_host: for device_type, tgt in target.items(): <mask>: target_host = tgt break if not target_host: target_host = ""llvm"" if tvm.runtime.enabled(""llvm"") else ""stackvm"" if isinstance(target_host, str): target_host = tvm.target.Target(target_host) return target_host",False,if device_type . value == tvm . nd . cpu ( 0 ) . device_type :,if tvm . runtime . enabled ( device_type ) :,0.8820016898747209,8.904260214843541
"def __console_writer(self): while True: self.__writer_event.wait() self.__writer_event.clear() if self.__console_view: <mask>: self.log.debug(""Writing console view to STDOUT"") sys.stdout.write(self.console_markup.clear) sys.stdout.write(self.__console_view) sys.stdout.write(self.console_markup.TOTAL_RESET)",False,if not self . short_only :,if self . __console_view . is_writable ( ) :,0.8820016898747209,6.608973813188645
"def goToPrevMarkedHeadline(self, event=None): """"""Select the next marked node."""""" c = self p = c.p if not p: return p.moveToThreadBack() wrapped = False while 1: if p and p.isMarked(): break elif p: p.moveToThreadBack() <mask>: break else: wrapped = True p = c.rootPosition() if not p: g.blue(""done"") c.treeSelectHelper(p) # Sets focus.",True,elif wrapped :,elif wrapped :,0.8820016898747209,0.0
"def delete_map(self, query=None): query_map = self.interpolated_map(query=query) for alias, drivers in six.iteritems(query_map.copy()): for driver, vms in six.iteritems(drivers.copy()): for vm_name, vm_details in six.iteritems(vms.copy()): if vm_details == ""Absent"": query_map[alias][driver].pop(vm_name) if not query_map[alias][driver]: query_map[alias].pop(driver) <mask>: query_map.pop(alias) return query_map",False,if not query_map [ alias ] :,if not query_map [ alias ] [ alias ] [ alias ] :,0.8820016898747209,55.12003357447276
"def get_shadows_zip(filename): import zipfile shadow_pkgs = set() with zipfile.ZipFile(filename) as lib_zip: already_test = [] for fname in lib_zip.namelist(): pname, fname = os.path.split(fname) <mask>: continue if pname not in already_test and ""/"" not in pname: already_test.append(pname) if is_shadowing(pname): shadow_pkgs.add(pname) return shadow_pkgs",False,if fname or ( pname and fname ) :,"if fname . startswith ( ""shadow_"" ) :",0.8820016898747209,11.208466750961147
"def make_chains(chains_info): chains = [[] for _ in chains_info[0][1]] for i, num_ids in enumerate(chains_info[:-1]): num, ids = num_ids for j, ident in enumerate(ids): <mask>: next_chain_info = chains_info[i + 1] previous = next_chain_info[1][j] block = SimpleBlock(num, ident, previous) chains[j].append(block) chains = {i: make_generator(chain) for i, chain in enumerate(chains)} return chains",False,"if ident != """" :",if i + 1 < len ( ids ) :,0.8820016898747209,4.990049701936832
"def filter_input(mindate, maxdate, files): mindate = parse(mindate) if mindate is not None else datetime.datetime.min maxdate = parse(maxdate) if maxdate is not None else datetime.datetime.max for line in fileinput.input(files): tweet = json.loads(line) created_at = parse(tweet[""created_at""]) created_at = created_at.replace(tzinfo=None) <mask>: print(json.dumps(tweet))",False,if mindate < created_at and maxdate > created_at :,if created_at < mindate :,0.8820016898747209,12.821896752346168
"def get(self): """"""If a value/an exception is stored, return/raise it. Otherwise until switch() or throw() is called."""""" if self._exception is not _NONE: if self._exception is None: return self.value getcurrent().throw(*self._exception) # pylint:disable=undefined-variable else: <mask>: raise ConcurrentObjectUseError( ""This Waiter is already used by %r"" % (self.greenlet,) ) self.greenlet = getcurrent() # pylint:disable=undefined-variable try: return self.hub.switch() finally: self.greenlet = None",True,if self . greenlet is not None :,if self . greenlet is not None :,0.8820016898747209,100.00000000000004
"def default_loader(href, parse, encoding=None): with open(href) as file: <mask>: data = ElementTree.parse(file).getroot() else: data = file.read() if encoding: data = data.decode(encoding) return data",False,"if parse == ""xml"" :",if parse :,0.8820016898747209,0.0
def is_all_qud(world): m = True for obj in world: <mask>: if obj.nice: m = m and True else: m = m and False else: m = m and True return m,False,if obj . blond :,"if isinstance ( obj , Qud ) :",0.8820016898747209,7.267884212102741
"def run(self, edit): if not self.has_selection(): region = sublime.Region(0, self.view.size()) originalBuffer = self.view.substr(region) prefixed = self.prefix(originalBuffer) if prefixed: self.view.replace(edit, region, prefixed) return for region in self.view.sel(): <mask>: continue originalBuffer = self.view.substr(region) prefixed = self.prefix(originalBuffer) if prefixed: self.view.replace(edit, region, prefixed)",False,if region . empty ( ) :,if region == 0 :,0.8820016898747209,15.207218222740094
"def add_fields(self, params): for (key, val) in params.iteritems(): <mask>: new_params = {} for k in val: new_params[""%s__%s"" % (key, k)] = val[k] self.add_fields(new_params) else: self.add_field(key, val)",True,"if isinstance ( val , dict ) :","if isinstance ( val , dict ) :",0.8820016898747209,100.00000000000004
"def find_magic(self, f, pos, magic): f.seek(pos) block = f.read(32 * 1024) if len(block) < len(magic): return -1 p = block.find(magic) while p < 0: pos += len(block) - len(magic) + 1 block = block[1 - len(magic) :] + f.read(32 << 10) <mask>: return -1 p = block.find(magic) return pos + p",False,if len ( block ) == len ( magic ) - 1 :,if len ( block ) < len ( magic ) :,0.8820016898747209,45.93799961315751
"def check_strings(self): """"""Check that all strings have been consumed."""""" for i, aList in enumerate(self.string_tokens): <mask>: g.trace(""warning: line %s. unused strings"" % i) for z in aList: print(self.dump_token(z))",False,if aList :,if i in self . unused_strings :,0.8820016898747209,5.669791110976001
"def get_tokens_unprocessed(self, text): from pygments.lexers._cocoa_builtins import ( COCOA_INTERFACES, COCOA_PROTOCOLS, COCOA_PRIMITIVES, ) for index, token, value in RegexLexer.get_tokens_unprocessed(self, text): <mask>: if ( value in COCOA_INTERFACES or value in COCOA_PROTOCOLS or value in COCOA_PRIMITIVES ): token = Name.Builtin.Pseudo yield index, token, value",False,if token is Name or token is Name . Class :,if token is Name . Builtin :,0.8820016898747209,36.337289265247364
"def key_from_key_value_dict(key_info): res = [] if not ""key_value"" in key_info: return res for value in key_info[""key_value""]: <mask>: e = base64_to_long(value[""rsa_key_value""][""exponent""]) m = base64_to_long(value[""rsa_key_value""][""modulus""]) key = RSA.construct((m, e)) res.append(key) return res",True,"if ""rsa_key_value"" in value :","if ""rsa_key_value"" in value :",0.8820016898747209,100.00000000000004
"def run(self, edit): if not self.has_selection(): region = sublime.Region(0, self.view.size()) originalBuffer = self.view.substr(region) prefixed = self.prefix(originalBuffer) <mask>: self.view.replace(edit, region, prefixed) return for region in self.view.sel(): if region.empty(): continue originalBuffer = self.view.substr(region) prefixed = self.prefix(originalBuffer) if prefixed: self.view.replace(edit, region, prefixed)",True,if prefixed :,if prefixed :,0.8820016898747209,0.0
def finalize(self): if self.ct < 1: return elif self.ct == 1: return 0 total = ct = 0 dtp = None while self.heap: <mask>: if dtp is None: dtp = heapq.heappop(self.heap) continue dt = heapq.heappop(self.heap) diff = dt - dtp ct += 1 total += total_seconds(diff) dtp = dt return float(total) / ct,False,if total == 0 :,if self . heap [ 0 ] == self . heap [ 1 ] :,0.8820016898747209,6.150343144231885
"def _test_configuration(self): config_path = self._write_config() try: self._log.debug(""testing configuration"") verboseflag = ""-Q"" <mask>: verboseflag = ""-v"" p = subprocess.Popen([self.PATH_SLAPTEST, verboseflag, ""-f"", config_path]) if p.wait() != 0: raise RuntimeError(""configuration test failed"") self._log.debug(""configuration seems ok"") finally: os.remove(config_path)",False,if self . _log . isEnabledFor ( logging . DEBUG ) :,if self . _verbose :,0.8820016898747209,16.731227054577023
"def exe(self, ret): if not ret: self.assertEqual(ret, """") else: assert os.path.isabs(ret), ret # Note: os.stat() may return False even if the file is there # hence we skip the test, see: # http://stackoverflow.com/questions/3112546/os-path-exists-lies <mask>: assert os.path.isfile(ret), ret if hasattr(os, ""access"") and hasattr(os, ""X_OK""): # XXX may fail on OSX self.assertTrue(os.access(ret, os.X_OK))",False,if POSIX :,"if hasattr ( os , ""stat"" ) :",0.8820016898747209,4.990049701936832
"def _do_cleanup(sg_name, device_id): masking_view_list = self.rest.get_masking_views_from_storage_group(array, sg_name) for masking_view in masking_view_list: <mask>: self.rest.delete_masking_view(array, masking_view) self.rest.remove_vol_from_sg(array, sg_name, device_id, extra_specs) self.rest.delete_volume(array, device_id) self.rest.delete_storage_group(array, sg_name)",False,"if ""STG-"" in masking_view :",if masking_view . get_masking_view_id ( ) == device_id :,0.8820016898747209,8.097785064266205
"def hide_tooltip_if_necessary(self, key): """"""Hide calltip when necessary"""""" try: calltip_char = self.get_character(self.calltip_position) before = self.is_cursor_before(self.calltip_position, char_offset=1) other = key in (Qt.Key_ParenRight, Qt.Key_Period, Qt.Key_Tab) <mask>: QToolTip.hideText() except (IndexError, TypeError): QToolTip.hideText()",False,"if calltip_char not in ( ""?"" , ""("" ) or before or other :",if before and other :,0.8820016898747209,1.26492199361622
"def list_tags_for_stream(self, stream_name, exclusive_start_tag_key=None, limit=None): stream = self.describe_stream(stream_name) tags = [] result = {""HasMoreTags"": False, ""Tags"": tags} for key, val in sorted(stream.tags.items(), key=lambda x: x[0]): if limit and len(tags) >= limit: result[""HasMoreTags""] = True break <mask>: continue tags.append({""Key"": key, ""Value"": val}) return result",False,if exclusive_start_tag_key and key < exclusive_start_tag_key :,if exclusive_start_tag_key and val [ 0 ] == exclusive_start_tag_key :,0.8820016898747209,64.3604545490784
"def parametrize_function_name(request, function_name): suffixes = [] if ""parametrize"" in request.keywords: argnames = request.keywords[""parametrize""].args[::2] argnames = [x.strip() for names in argnames for x in names.split("","")] for name in argnames: value = request.getfuncargvalue(name) <mask>: value = value.__name__ suffixes.append(""{}={}"".format(name, value)) return ""+"".join([function_name] + suffixes)",False,if inspect . isclass ( value ) :,"if isinstance ( value , type ) :",0.8820016898747209,16.515821590069027
"def add_entities(self, positions): e1 = EntityFactory() for p in positions: <mask>: start, length = p else: start, length = p, 1 EntityOccurrenceFactory( document=self.doc, entity=e1, offset=start, offset_end=start + length, alias=""AB"", )",False,"if isinstance ( p , tuple ) :","if isinstance ( p , ( tuple , list ) ) :",0.8820016898747209,37.70063804549471
"def transform_value(value): if isinstance(value, collections.MutableMapping): <mask>: return DBRef(value[""_ns""], transform_value(value[""_id""])) else: return transform_dict(SON(value)) elif isinstance(value, list): return [transform_value(v) for v in value] return value",False,"if ""_id"" in value and ""_ns"" in value :","if value [ ""_ns"" ] :",0.8820016898747209,17.625328548379716
"def remove(self, items): """"""Remove messages from lease management."""""" with self._add_remove_lock: # Remove the ack ID from lease management, and decrement the # byte counter. for item in items: if self._leased_messages.pop(item.ack_id, None) is not None: self._bytes -= item.byte_size else: _LOGGER.debug(""Item %s was not managed."", item.ack_id) <mask>: _LOGGER.debug(""Bytes was unexpectedly negative: %d"", self._bytes) self._bytes = 0",True,if self . _bytes < 0 :,if self . _bytes < 0 :,0.8820016898747209,100.00000000000004
"def parse_hgsub(lines): """"""Fills OrderedDict with hgsub file content passed as list of lines"""""" rv = OrderedDict() for l in lines: ls = l.strip() <mask>: continue name, value = l.split(""="", 1) rv[name.strip()] = value.strip() return rv",False,"if not ls or ls [ 0 ] == ""#"" :","if ls == """" :",0.8820016898747209,12.300790484177304
"def del_(self, key): initial_hash = hash_ = self.hash(key) while True: if self._keys[hash_] is self._empty: # That key was never assigned return None elif self._keys[hash_] == key: # key found, assign with deleted sentinel self._keys[hash_] = self._deleted self._values[hash_] = self._deleted self._len -= 1 return hash_ = self._rehash(hash_) <mask>: # table is full and wrapped around return None",False,if initial_hash == hash_ :,if hash_ == initial_hash :,0.8820016898747209,27.77619034011791
"def atom(token, no_symbol=False): try: return int(token) except ValueError: try: return float(token) except ValueError: <mask>: return token[1:-1] elif no_symbol: return token else: return Symbol(token)",False,"if token . startswith ( ""'"" ) or token . startswith ( '""' ) :","if token . endswith ( "" "" ) :",0.8820016898747209,9.387774042495229
"def __Suffix_Noun_Step1b(self, token): for suffix in self.__suffix_noun_step1b: <mask>: token = token[:-1] self.suffixe_noun_step1b_success = True break return token",False,if token . endswith ( suffix ) and len ( token ) > 5 :,if token [ - 1 ] == suffix :,0.8820016898747209,6.053236275429152
"def _guardAgainstUnicode(self, data): # Only accept byte strings or ascii unicode values, otherwise # there is no way to correctly decode the data into bytes. if _pythonMajorVersion < 3: <mask>: data = data.encode(""utf8"") else: if isinstance(data, str): # Only accept ascii unicode values. try: return data.encode(""ascii"") except UnicodeEncodeError: pass raise ValueError(""pyDes can only work with encoded strings, not Unicode."") return data",True,"if isinstance ( data , unicode ) :","if isinstance ( data , unicode ) :",0.8820016898747209,100.00000000000004
"def populate_resource_parameters(self, tool_source): root = getattr(tool_source, ""root"", None) if ( root is not None and hasattr(self.app, ""job_config"") and hasattr(self.app.job_config, ""get_tool_resource_xml"") ): resource_xml = self.app.job_config.get_tool_resource_xml( root.get(""id""), self.tool_type ) if resource_xml is not None: inputs = root.find(""inputs"") <mask>: inputs = parse_xml_string(""<inputs/>"") root.append(inputs) inputs.append(resource_xml)",True,if inputs is None :,if inputs is None :,0.8820016898747209,100.00000000000004
"def test_arguments_regex(self): argument_matches = ( (""pip=1.1"", (""pip"", ""1.1"")), (""pip==1.1"", None), (""pip=1.2=1"", (""pip"", ""1.2=1"")), ) for argument, match in argument_matches: <mask>: self.assertIsNone(salt.utils.args.KWARG_REGEX.match(argument)) else: self.assertEqual( salt.utils.args.KWARG_REGEX.match(argument).groups(), match )",True,if match is None :,if match is None :,0.8820016898747209,100.00000000000004
"def _get_sidebar_selected(self): sidebar_selected = None if self.businessline_id: sidebar_selected = ""bl_%s"" % self.businessline_id if self.service_id: sidebar_selected += ""_s_%s"" % self.service_id <mask>: sidebar_selected += ""_env_%s"" % self.environment_id return sidebar_selected",True,if self . environment_id :,if self . environment_id :,0.8820016898747209,100.00000000000004
"def get_ip_info(ipaddress): """"""Returns device information by IP address"""""" result = {} try: ip = IPAddress.objects.select_related().get(address=ipaddress) except IPAddress.DoesNotExist: pass else: if ip.venture is not None: result[""venture_id""] = ip.venture.id <mask>: result[""device_id""] = ip.device.id if ip.device.venture is not None: result[""venture_id""] = ip.device.venture.id return result",True,if ip . device is not None :,if ip . device is not None :,0.8820016898747209,100.00000000000004
"def apply(self, db, person): for family_handle in person.get_family_handle_list(): family = db.get_family_from_handle(family_handle) if family: for event_ref in family.get_event_ref_list(): if event_ref: event = db.get_event_from_handle(event_ref.ref) if not event.get_place_handle(): return True <mask>: return True return False",False,if not event . get_date_object ( ) :,if not event . get_place_ref ( ) :,0.8820016898747209,54.52469119630866
"def killIfDead(): if not self._isalive: self.log.debug( ""WampLongPoll: killing inactive WAMP session with transport '{0}'"".format( self._transport_id ) ) self.onClose(False, 5000, ""session inactive"") self._receive._kill() <mask>: del self._parent._transports[self._transport_id] else: self.log.debug( ""WampLongPoll: transport '{0}' is still alive"".format(self._transport_id) ) self._isalive = False self.reactor.callLater(killAfter, killIfDead)",False,if self . _transport_id in self . _parent . _transports :,if self . _parent :,0.8820016898747209,15.020723831494387
"def offsets(self): offsets = {} offset_so_far = 0 for name, ty in self.fields.items(): <mask>: l.warning( ""Found a bottom field in struct %s. Ignore and increment the offset using the default "" ""element size."", self.name, ) continue if not self._pack: align = ty.alignment if offset_so_far % align != 0: offset_so_far += align - offset_so_far % align offsets[name] = offset_so_far offset_so_far += ty.size // self._arch.byte_width return offsets",False,"if isinstance ( ty , SimTypeBottom ) :","if ty . type == ""bottom"" :",0.8820016898747209,5.522397783539471
"def get_override_css(self): """"""handls allow_css_overrides setting."""""" if self.settings.get(""allow_css_overrides""): filename = self.view.file_name() filetypes = self.settings.get(""markdown_filetypes"") <mask>: for filetype in filetypes: if filename.endswith(filetype): css_filename = filename.rpartition(filetype)[0] + "".css"" if os.path.isfile(css_filename): return u""<style>%s</style>"" % load_utf8(css_filename) return """"",False,if filename and filetypes :,if filetypes :,0.8820016898747209,0.0
"def setFullCSSSource(self, fullsrc, inline=False): self.fullsrc = fullsrc if type(self.fullsrc) == six.binary_type: self.fullsrc = six.text_type(self.fullsrc, ""utf-8"") if inline: self.inline = inline if self.fullsrc: self.srcFullIdx = self.fullsrc.find(self.src) if self.srcFullIdx < 0: del self.srcFullIdx self.ctxsrcFullIdx = self.fullsrc.find(self.ctxsrc) <mask>: del self.ctxsrcFullIdx",True,if self . ctxsrcFullIdx < 0 :,if self . ctxsrcFullIdx < 0 :,0.8820016898747209,100.00000000000004
"def title(self): ret = theme[""title""] if isinstance(self.name, six.string_types): width = self.statwidth() return ( ret + self.name[0:width].center(width).replace("" "", ""-"") + theme[""default""] ) for i, name in enumerate(self.name): width = self.colwidth() ret = ret + name[0:width].center(width).replace("" "", ""-"") if i + 1 != len(self.vars): <mask>: ret = ret + theme[""frame""] + char[""dash""] + theme[""title""] else: ret = ret + char[""space""] return ret",False,if op . color :,if i + 1 == len ( self . vars ) - 1 :,0.8820016898747209,3.4585921141027356
"def _get_requested_databases(self): """"""Returns a list of databases requested, not including ignored dbs"""""" requested_databases = [] if (self._requested_namespaces is not None) and (self._requested_namespaces != []): for requested_namespace in self._requested_namespaces: <mask>: return [] elif requested_namespace[0] not in IGNORE_DBS: requested_databases.append(requested_namespace[0]) return requested_databases",False,"if requested_namespace [ 0 ] is ""*"" :",if requested_namespace [ 0 ] in IGNORE_DBS :,0.8820016898747209,53.3167536340577
"def add_channels(cls, voucher, add_channels): for add_channel in add_channels: channel = add_channel[""channel""] defaults = {""currency"": channel.currency_code} <mask>: defaults[""discount_value""] = add_channel.get(""discount_value"") if ""min_amount_spent"" in add_channel.keys(): defaults[""min_spent_amount""] = add_channel.get(""min_amount_spent"", None) models.VoucherChannelListing.objects.update_or_create( voucher=voucher, channel=channel, defaults=defaults, )",True,"if ""discount_value"" in add_channel . keys ( ) :","if ""discount_value"" in add_channel . keys ( ) :",0.8820016898747209,100.00000000000004
"def read_xml(path): with tf.gfile.GFile(path) as f: root = etree.fromstring(f.read()) annotations = {} for node in root.getchildren(): key, val = node2dict(node) # If `key` is object, it's actually a list. <mask>: annotations.setdefault(key, []).append(val) else: annotations[key] = val return annotations",False,"if key == ""object"" :","if isinstance ( val , dict ) :",0.8820016898747209,6.567274736060395
"def get_ip_info(ipaddress): """"""Returns device information by IP address"""""" result = {} try: ip = IPAddress.objects.select_related().get(address=ipaddress) except IPAddress.DoesNotExist: pass else: <mask>: result[""venture_id""] = ip.venture.id if ip.device is not None: result[""device_id""] = ip.device.id if ip.device.venture is not None: result[""venture_id""] = ip.device.venture.id return result",False,if ip . venture is not None :,if ip . eventure is not None :,0.8820016898747209,50.000000000000014
"def test_large_headers(self): with ExpectLog(gen_log, ""Unsatisfiable read"", required=False): try: self.fetch(""/"", headers={""X-Filler"": ""a"" * 1000}, raise_error=True) self.fail(""did not raise expected exception"") except HTTPError as e: # 431 is ""Request Header Fields Too Large"", defined in RFC # 6585. However, many implementations just close the # connection in this case, resulting in a missing response. <mask>: self.assertIn(e.response.code, (431, 599))",False,if e . response is not None :,if e . response :,0.8820016898747209,38.80684294761701
"def validate_reserved_serial_no_consumption(self): for item in self.items: if item.s_warehouse and not item.t_warehouse and item.serial_no: for sr in get_serial_nos(item.serial_no): sales_order = frappe.db.get_value(""Serial No"", sr, ""sales_order"") <mask>: msg = _( ""(Serial No: {0}) cannot be consumed as it's reserverd to fullfill Sales Order {1}."" ).format(sr, sales_order) frappe.throw(_(""Item {0} {1}"").format(item.item_code, msg))",True,if sales_order :,if sales_order :,0.8820016898747209,100.00000000000004
"def force_decode(string, encoding): if isinstance(string, str): <mask>: string = string.decode(encoding) else: try: # try decoding with utf-8, should only work for real UTF-8 string = string.decode(""utf-8"") except UnicodeError: # last resort -- can't fail string = string.decode(""latin1"") return string",True,if encoding :,if encoding :,0.8820016898747209,0.0
"def _add_cs(master_cs, sub_cs, prefix, delimiter=""."", parent_hp=None): new_parameters = [] for hp in sub_cs.get_hyperparameters(): new_parameter = copy.deepcopy(hp) # Allow for an empty top-level parameter if new_parameter.name == """": new_parameter.name = prefix <mask>: new_parameter.name = ""{}{}{}"".format(prefix, SPLITTER, new_parameter.name) new_parameters.append(new_parameter) for hp in new_parameters: _add_hp(master_cs, hp)",False,"elif not prefix == """" :",if delimiter :,0.8820016898747209,0.0
"def __call__(self, *args, **kwargs): if self.log_file is not None: kwargs[""file""] = self.log_file print(*args, **kwargs) <mask>: # get immediate feedback self.log_file.flush() elif self.log_func is not None: self.log_func(*args, **kwargs)",False,"if hasattr ( self . log_file , ""flush"" ) :",if self . log_file . is_file ( ) :,0.8820016898747209,32.79475209724913
"def df_index_expr(self, length_expr=None, as_range=False): """"""Generate expression to get or create index of DF"""""" if isinstance(self.index, types.NoneType): <mask>: length_expr = df_length_expr(self) if as_range: return f""range({length_expr})"" else: return f""numpy.arange({length_expr})"" return ""self._index""",True,if length_expr is None :,if length_expr is None :,0.8820016898747209,100.00000000000004
"def _setWeight(self, value): if value is None: self._fontWeight = None else: <mask>: raise TextFormatException(f""Not a supported fontWeight: {value}"") self._fontWeight = value.lower()",False,"if value . lower ( ) not in ( ""normal"" , ""bold"" ) :",if value not in self . _fontWeight :,0.8820016898747209,5.490133261314248
"def _test_configuration(self): config_path = self._write_config() try: self._log.debug(""testing configuration"") verboseflag = ""-Q"" if self._log.isEnabledFor(logging.DEBUG): verboseflag = ""-v"" p = subprocess.Popen([self.PATH_SLAPTEST, verboseflag, ""-f"", config_path]) <mask>: raise RuntimeError(""configuration test failed"") self._log.debug(""configuration seems ok"") finally: os.remove(config_path)",True,if p . wait ( ) != 0 :,if p . wait ( ) != 0 :,0.8820016898747209,100.00000000000004
"def filter_queryset(self, request, queryset, view): kwargs = {} for field in view.filterset_fields: value = request.GET.get(field) if not value: continue if field == ""node_id"": value = get_object_or_none(Node, pk=value) kwargs[""node""] = value continue <mask>: field = ""asset"" kwargs[field] = value if kwargs: queryset = queryset.filter(**kwargs) logger.debug(""Filter {}"".format(kwargs)) return queryset",False,"elif field == ""asset_id"" :","if field == ""asset_id"" :",0.8820016898747209,88.01117367933934
"def _find_closing_brace(string, start_pos): """"""Finds the corresponding closing brace after start_pos."""""" bracks_open = 1 for idx, char in enumerate(string[start_pos:]): if char == ""("": <mask>: bracks_open += 1 elif char == "")"": if string[idx + start_pos - 1] != ""\\"": bracks_open -= 1 if not bracks_open: return start_pos + idx + 1",True,"if string [ idx + start_pos - 1 ] != ""\\"" :","if string [ idx + start_pos - 1 ] != ""\\"" :",0.8820016898747209,100.00000000000004
"def _set_hostport(self, host, port): if port is None: i = host.rfind("":"") j = host.rfind(""]"") # ipv6 addresses have [...] if i > j: try: port = int(host[i + 1 :]) except ValueError: raise InvalidURL(""nonnumeric port: '%s'"" % host[i + 1 :]) host = host[:i] else: port = self.default_port <mask>: host = host[1:-1] self.host = host self.port = port",False,"if host and host [ 0 ] == ""["" and host [ - 1 ] == ""]"" :","if host [ - 1 ] == "":"" :",0.8820016898747209,26.013004751144432
"def __getstate__(self): state = {} for cls in type(self).mro(): cls_slots = getattr(cls, ""__slots__"", ()) for slot in cls_slots: if slot != ""__weakref__"": <mask>: state[slot] = getattr(self, slot) state[""_cookiejar_cookies""] = list(self.cookiejar) del state[""cookiejar""] return state",True,"if hasattr ( self , slot ) :","if hasattr ( self , slot ) :",0.8820016898747209,100.00000000000004
"def _evp_pkey_from_der_traditional_key(self, bio_data, password): key = self._lib.d2i_PrivateKey_bio(bio_data.bio, self._ffi.NULL) if key != self._ffi.NULL: key = self._ffi.gc(key, self._lib.EVP_PKEY_free) <mask>: raise TypeError(""Password was given but private key is not encrypted."") return key else: self._consume_errors() return None",False,if password is not None :,if password != key . get_encrypted_password ( ) :,0.8820016898747209,6.285596338261262
"def is_special(s, i, directive): """"""Return True if the body text contains the @ directive."""""" # j = skip_line(s,i) ; trace(s[i:j],':',directive) assert directive and directive[0] == ""@"" # 10/23/02: all directives except @others must start the line. skip_flag = directive in (""@others"", ""@all"") while i < len(s): <mask>: return True, i else: i = skip_line(s, i) if skip_flag: i = skip_ws(s, i) return False, -1",False,"if match_word ( s , i , directive ) :",if s [ i ] == directive :,0.8820016898747209,5.1084274932709866
"def _decorator(coro_func): fut = asyncio.ensure_future(coro_func()) self._tests.append((coro_func.__name__, fut)) if timeout_sec is not None: timeout_at = self._loop.time() + timeout_sec handle = self.MASTER_LOOP.call_at( timeout_at, self._set_exception_if_not_done, fut, asyncio.TimeoutError() ) fut.add_done_callback(lambda *args: handle.cancel()) <mask>: self._global_timeout_at = timeout_at return coro_func",False,if timeout_at > self . _global_timeout_at :,if timeout_at is not None :,0.8820016898747209,17.267606045625936
"def _load(self, db, owner): self.__init(owner) db_result = db( ""SELECT ship_id, state_id FROM ai_combat_ship WHERE owner_id = ?"", self.owner.worldid, ) for ( ship_id, state_id, ) in db_result: ship = WorldObject.get_object_by_id(ship_id) state = self.shipStates[state_id] # add move callbacks corresponding to given state <mask>: ship.add_move_callback(Callback(BehaviorMoveCallback._arrived, ship)) self.add_new_unit(ship, state)",False,if state == self . shipStates . moving :,if state . is_active ( ) :,0.8820016898747209,10.147104008451905
"def addError(self, test, err): if err[0] is SkipTest: <mask>: self.stream.writeln(str(err[1])) elif self.dots: self.stream.write(""s"") self.stream.flush() return _org_AddError(self, test, err)",False,if self . showAll :,if len ( err ) > 1 :,0.8820016898747209,6.567274736060395
"def _construct(self, node): self.flatten_mapping(node) ret = self.construct_pairs(node) keys = [d[0] for d in ret] keys_sorted = sorted(keys, key=_natsort_key) for key in keys: expected = keys_sorted.pop(0) <mask>: raise ConstructorError( None, None, ""keys out of order: "" ""expected {} got {} at {}"".format(expected, key, node.start_mark), ) return dict(ret)",False,if key != expected :,if expected != key :,0.8820016898747209,21.3643503198117
"def sample_pos_items_for_u(u, num): # sample num pos items for u-th user pos_items = self.train_items[u] n_pos_items = len(pos_items) pos_batch = [] while True: if len(pos_batch) == num: break pos_id = np.random.randint(low=0, high=n_pos_items, size=1)[0] pos_i_id = pos_items[pos_id] <mask>: pos_batch.append(pos_i_id) return pos_batch",False,if pos_i_id not in pos_batch :,if pos_i_id :,0.8820016898747209,41.16538266387962
"def _get_id(self, type, id): fields = id.split("":"") if len(fields) >= 3: <mask>: logger.warning( ""Expected id of type %s but found type %s %s"", type, fields[-2], id ) return fields[-1] fields = id.split(""/"") if len(fields) >= 3: itype = fields[-2] if type != itype: logger.warning( ""Expected id of type %s but found type %s %s"", type, itype, id ) return fields[-1].split(""?"")[0] return id",True,if type != fields [ - 2 ] :,if type != fields [ - 2 ] :,0.8820016898747209,100.00000000000004
"def uninstall_environments(self, environments): environments = [ env if not env.startswith(self.conda_context.envs_path) else os.path.basename(env) for env in environments ] return_codes = [self.conda_context.exec_remove([env]) for env in environments] final_return_code = 0 for env, return_code in zip(environments, return_codes): <mask>: log.debug(""Conda environment '%s' successfully removed."" % env) else: log.debug(""Conda environment '%s' could not be removed."" % env) final_return_code = return_code return final_return_code",True,if return_code == 0 :,if return_code == 0 :,0.8820016898747209,100.00000000000004
"def _add_hit_offset(self, context_list, string_name, original_offset, value): for context in context_list: hits_by_context_dict = self.hits_by_context.setdefault(context, {}) <mask>: hits_by_context_dict[string_name] = ( original_offset, value.encode(""base64""), )",False,if string_name not in hits_by_context_dict :,if string_name in hits_by_context_dict :,0.8820016898747209,78.81929718099911
"def detab(self, text, length=None): """"""Remove a tab from the front of each line of the given text."""""" if length is None: length = self.tab_length newtext = [] lines = text.split(""\n"") for line in lines: if line.startswith("" "" * length): newtext.append(line[length:]) <mask>: newtext.append("""") else: break return ""\n"".join(newtext), ""\n"".join(lines[len(newtext) :])",False,elif not line . strip ( ) :,"elif line . startswith ( ""\t"" * length ) :",0.8820016898747209,9.669265690880861
"def dump(self): print(self.package_name) for package, value in self.entries: print(str(package.version)) <mask>: print("" [FILTERED]"") elif isinstance(value, list): variants = value for variant in variants: print("" %s"" % str(variant)) else: print("" %s"" % str(package))",True,if value is None :,if value is None :,0.8820016898747209,100.00000000000004
"def __lexical_scope(*args, **kwargs): try: scope = Scope(quasi) <mask>: binding_name_set_stack[-1].add_child(scope) binding_name_set_stack.append(scope) return func(*args, **kwargs) finally: if binding_name_set_stack[-1] is scope: binding_name_set_stack.pop()",False,if quasi :,if scope not in binding_name_set_stack :,0.8820016898747209,4.02724819242185
"def getnotes(self, origin=None): if origin is None: result = self.translator_comments <mask>: if result: result += ""\n"" + self.developer_comments else: result = self.developer_comments return result elif origin == ""translator"": return self.translator_comments elif origin in (""programmer"", ""developer"", ""source code""): return self.developer_comments else: raise ValueError(""Comment type not valid"")",False,if self . developer_comments :,"if origin == ""developer"" :",0.8820016898747209,7.267884212102741
"def fix_datetime_fields(data: TableData, table: TableName) -> None: for item in data[table]: for field_name in DATE_FIELDS[table]: <mask>: item[field_name] = datetime.datetime.fromtimestamp( item[field_name], tz=datetime.timezone.utc )",False,if item [ field_name ] is not None :,if field_name in item :,0.8820016898747209,16.417223692914014
"def _check_for_cart_error(cart): if cart._safe_get_element(""Cart.Request.Errors"") is not None: error = cart._safe_get_element(""Cart.Request.Errors.Error.Code"").text <mask>: raise CartInfoMismatchException( ""CartGet failed: AWS.ECommerceService.CartInfoMismatch "" ""make sure AssociateTag, CartId and HMAC are correct "" ""(dont use URLEncodedHMAC!!!)"" ) raise CartException(""CartGet failed: "" + error)",False,"if error == ""AWS.ECommerceService.CartInfoMismatch"" :","if error . startswith ( ""CartInfoMismatch"" ) :",0.8820016898747209,11.17616954959852
"def check_bounds(geometry): if isinstance(geometry[0], (list, tuple)): return list(map(check_bounds, geometry)) else: <mask>: raise ValueError( ""Longitude is out of bounds, check your JSON format or data"" ) if geometry[1] > 90 or geometry[1] < -90: raise ValueError( ""Latitude is out of bounds, check your JSON format or data"" )",False,if geometry [ 0 ] > 180 or geometry [ 0 ] < - 180 :,if geometry [ 0 ] > 90 or geometry [ 0 ] < - 90 :,0.8820016898747209,68.65065103648593
"def _mapper_output_protocol(self, step_num, step_map): map_key = self._step_key(step_num, ""mapper"") if map_key in step_map: <mask>: return self.output_protocol() else: return self.internal_protocol() else: # mapper is not a script substep, so protocols don't apply at all return RawValueProtocol()",False,if step_map [ map_key ] >= ( len ( step_map ) - 1 ) :,if step_map [ map_key ] . is_script ( ) :,0.8820016898747209,39.80409003279966
"def asset(*paths): for path in paths: fspath = www_root + ""/assets/"" + path etag = """" try: if env.cache_static: etag = asset_etag(fspath) else: os.stat(fspath) except FileNotFoundError as e: <mask>: if not os.path.exists(fspath + "".spt""): tell_sentry(e, {}) else: continue except Exception as e: tell_sentry(e, {}) return asset_url + path + (etag and ""?etag="" + etag)",False,if path == paths [ - 1 ] :,if e . errno == errno . ENOENT :,0.8820016898747209,9.980099403873663
"def ping(self, payload: Union[str, bytes] = """") -> None: if self.trace_enabled and self.ping_pong_trace_enabled: <mask>: payload = payload.decode(""utf-8"") self.logger.debug( ""Sending a ping data frame "" f""(session id: {self.session_id}, payload: {payload})"" ) data = _build_data_frame_for_sending(payload, FrameHeader.OPCODE_PING) with self.sock_send_lock: self.sock.send(data)",True,"if isinstance ( payload , bytes ) :","if isinstance ( payload , bytes ) :",0.8820016898747209,100.00000000000004
"def is_ac_power_connected(): for power_source_path in Path(""/sys/class/power_supply/"").iterdir(): try: with open(power_source_path / ""type"", ""r"") as f: if f.read().strip() != ""Mains"": continue with open(power_source_path / ""online"", ""r"") as f: <mask>: return True except IOError: continue return False",False,"if f . read ( 1 ) == ""1"" :","if f . read ( ) . strip ( ) == ""Connected"" :",0.8820016898747209,41.69392927528885
"def handle_noargs(self, **options): self.style = color_style() print(""Running Django's own validation:"") self.validate(display_num_errors=True) for model in loading.get_models(): <mask>: self.validate_base_model(model) if hasattr(model, ""_feincms_content_models""): self.validate_content_type(model)",False,"if hasattr ( model , ""_create_content_base"" ) :","if hasattr ( model , ""_feincms_base_models"" ) :",0.8820016898747209,54.10822690539399
"def _init_weights(self, module): if isinstance(module, nn.Linear): module.weight.data.normal_(mean=0.0, std=self.config.init_std) <mask>: module.bias.data.zero_() elif isinstance(module, nn.Embedding): module.weight.data.normal_(mean=0.0, std=self.config.init_std) if module.padding_idx is not None: module.weight.data[module.padding_idx].zero_()",True,if module . bias is not None :,if module . bias is not None :,0.8820016898747209,100.00000000000004
"def walk(msg, callback, data): partnum = 0 for part in msg.walk(): # multipart/* are just containers if part.get_content_maintype() == ""multipart"": continue ctype = part.get_content_type() <mask>: ctype = OCTET_TYPE filename = part.get_filename() if not filename: filename = PART_FN_TPL % (partnum) headers = dict(part) LOG.debug(headers) headers[""Content-Type""] = ctype payload = util.fully_decoded_payload(part) callback(data, filename, payload, headers) partnum = partnum + 1",False,if ctype is None :,if not ctype :,0.8820016898747209,16.37226966703825
"def _mark_lcs(mask, dirs, m, n): while m != 0 and n != 0: if dirs[m, n] == ""|"": m -= 1 n -= 1 mask[m] = 1 <mask>: m -= 1 elif dirs[m, n] == ""<"": n -= 1 else: raise UnboundLocalError(""Illegal move"") return mask",False,"elif dirs [ m , n ] == ""^"" :","elif dirs [ m , n ] == "">"" :",0.8820016898747209,79.10665071754353
"def valid_localparts(strip_delimiters=False): for line in ABRIDGED_LOCALPART_VALID_TESTS.split(""\n""): # strip line, skip over empty lines line = line.strip() <mask>: continue # skip over comments or empty lines match = COMMENT.match(line) if match: continue # skip over localparts with delimiters if strip_delimiters: if "","" in line or "";"" in line: continue yield line",False,"if line == """" :",if not line :,0.8820016898747209,9.930283522141846
"def fetch(self, *tileables, **kw): ret_list = False if len(tileables) == 1 and isinstance(tileables[0], (tuple, list)): ret_list = True tileables = tileables[0] elif len(tileables) > 1: ret_list = True result = self._sess.fetch(*tileables, **kw) ret = [] for r, t in zip(result, tileables): <mask>: ret.append(r.item()) else: ret.append(r) if ret_list: return ret return ret[0]",False,"if hasattr ( t , ""isscalar"" ) and t . isscalar ( ) and getattr ( r , ""size"" , None ) == 1 :","if isinstance ( t , ( tuple , list ) ) :",0.8820016898747209,3.7018157114072747
"def _convert(container): if _value_marker in container: force_list = False values = container.pop(_value_marker) if container.pop(_list_marker, False): force_list = True values.extend(_convert(x[1]) for x in sorted(container.items())) <mask>: values = values[0] if not container: return values return _convert(container) elif container.pop(_list_marker, False): return [_convert(x[1]) for x in sorted(container.items())] return dict_cls((k, _convert(v)) for k, v in iteritems(container))",False,if not force_list and len ( values ) == 1 :,if force_list :,0.8820016898747209,7.468220329575271
"def _transform_init_kwargs(cls, kwargs): transformed = [] for field in list(kwargs.keys()): prop = getattr(cls, field, None) <mask>: value = kwargs.pop(field) _transform_single_init_kwarg(prop, field, value, kwargs) transformed.append((field, value)) return transformed",False,"if isinstance ( prop , MoneyProperty ) :",if prop is not None :,0.8820016898747209,7.654112967106117
"def haslayer(self, cls): """"""true if self has a layer that is an instance of cls. Superseded by ""cls in self"" syntax."""""" if self.__class__ == cls or self.__class__.__name__ == cls: return 1 for f in self.packetfields: fvalue_gen = self.getfieldval(f.name) if fvalue_gen is None: continue if not f.islist: fvalue_gen = SetGen(fvalue_gen, _iterpacket=0) for fvalue in fvalue_gen: if isinstance(fvalue, Packet): ret = fvalue.haslayer(cls) <mask>: return ret return self.payload.haslayer(cls)",True,if ret :,if ret :,0.8820016898747209,0.0
def insert_broken_add_sometimes(node): if node.op == theano.tensor.add: last_time_replaced[0] = not last_time_replaced[0] <mask>: return [off_by_half(*node.inputs)] return False,False,if last_time_replaced [ 0 ] :,if node . inputs :,0.8820016898747209,4.673289785800722
"def testReadChunk10(self): # ""Test BZ2File.read() in chunks of 10 bytes"" self.createTempFile() with BZ2File(self.filename) as bz2f: text = """" while 1: str = bz2f.read(10) <mask>: break text += str self.assertEqual(text, self.TEXT)",True,if not str :,if not str :,0.8820016898747209,100.00000000000004
"def generate_sv_faces(dcel_mesh, point_index, only_select=False, del_flag=None): # This part of function creates faces in SV format # It ignores boundless super face sv_faces = [] for i, face in enumerate(dcel_mesh.faces): if face.inners and face.outer: ""Face ({}) has inner components! Sverchok cant show polygons with holes."".format( i ) if not face.outer or del_flag in face.flags: continue <mask>: continue sv_faces.append([point_index[hedge.origin] for hedge in face.outer.loop_hedges]) return sv_faces",False,if only_select and not face . select :,if only_select and not face . outer . loop_hedges :,0.8820016898747209,52.960749334062214
"def __check_dict_contains(dct, dict_name, keys, comment="""", result=True): for key in keys: <mask>: result = False comment = __append_comment( ""Missing {0} in {1}"".format(key, dict_name), comment ) return result, comment",False,if key not in six . iterkeys ( dct ) :,if key not in dct :,0.8820016898747209,24.439253249722206
"def _dump_arg_defaults(kwargs): """"""Inject default arguments for dump functions."""""" if current_app: kwargs.setdefault(""cls"", current_app.json_encoder) <mask>: kwargs.setdefault(""ensure_ascii"", False) kwargs.setdefault(""sort_keys"", current_app.config[""JSON_SORT_KEYS""]) else: kwargs.setdefault(""sort_keys"", True) kwargs.setdefault(""cls"", JSONEncoder)",False,"if not current_app . config [ ""JSON_AS_ASCII"" ] :","if ""JSON_SORT_KEYS"" in current_app . config :",0.8820016898747209,31.868653677347453
"def _on_change(self): changed = False self.save() for key, value in self.data.items(): if isinstance(value, bool): if value: changed = True break if isinstance(value, int): if value != 1: changed = True break elif value is None: continue <mask>: changed = True break self._reset_button.disabled = not changed",False,elif len ( value ) != 0 :,if value != 0 :,0.8820016898747209,32.58798048281462
"def parse_win_proxy(val): proxies = [] for p in val.split("";""): <mask>: tab = p.split(""="", 1) if tab[0] == ""socks"": tab[0] = ""SOCKS4"" proxies.append( (tab[0].upper(), tab[1], None, None) ) # type, addr:port, username, password else: proxies.append((""HTTP"", p, None, None)) return proxies",True,"if ""="" in p :","if ""="" in p :",0.8820016898747209,100.00000000000004
"def predict(collect_dir, keys): run_all = len(keys) == 0 validate_keys(keys) for exp_cfg in cfg: <mask>: key = exp_cfg[""key""] _predict(key, exp_cfg[""sample_img""], collect_dir)",False,"if run_all or exp_cfg [ ""key"" ] in keys :",if run_all :,0.8820016898747209,7.834966465489322
"def convert_port_bindings(port_bindings): result = {} for k, v in six.iteritems(port_bindings): key = str(k) <mask>: key += ""/tcp"" if isinstance(v, list): result[key] = [_convert_port_binding(binding) for binding in v] else: result[key] = [_convert_port_binding(v)] return result",False,"if ""/"" not in key :","if key . startswith ( ""/tcp"" ) :",0.8820016898747209,9.864703138979419
"def assert_conll_writer_output( dataset: InternalBioNerDataset, expected_output: List[str], sentence_splitter: SentenceSplitter = None, ): outfile_path = tempfile.mkstemp()[1] try: sentence_splitter = ( sentence_splitter <mask>: else NoSentenceSplitter(tokenizer=SpaceTokenizer()) ) writer = CoNLLWriter(sentence_splitter=sentence_splitter) writer.write_to_conll(dataset, Path(outfile_path)) contents = [l.strip() for l in open(outfile_path).readlines() if l.strip()] finally: os.remove(outfile_path) assert contents == expected_output",True,if sentence_splitter,if sentence_splitter,0.8820016898747209,100.00000000000004
"def post(self, request, *args, **kwargs): self.comment_obj = get_object_or_404(Comment, id=request.POST.get(""commentid"")) if request.user == self.comment_obj.commented_by: form = LeadCommentForm(request.POST, instance=self.comment_obj) <mask>: return self.form_valid(form) return self.form_invalid(form) data = {""error"": ""You don't have permission to edit this comment.""} return JsonResponse(data)",True,if form . is_valid ( ) :,if form . is_valid ( ) :,0.8820016898747209,100.00000000000004
"def trivia_list(self, ctx: commands.Context): """"""List available trivia categories."""""" lists = set(p.stem for p in self._all_lists()) if await ctx.embed_requested(): await ctx.send( embed=discord.Embed( title=_(""Available trivia lists""), colour=await ctx.embed_colour(), description="", "".join(sorted(lists)), ) ) else: msg = box(bold(_(""Available trivia lists"")) + ""\n\n"" + "", "".join(sorted(lists))) <mask>: await ctx.author.send(msg) else: await ctx.send(msg)",False,if len ( msg ) > 1000 :,if await ctx . author_requested ( ) :,0.8820016898747209,5.934202609760488
"def validate(self): result = validators.SUCCESS msgs = [] for validator in self._validators: res, err = validator.validate() if res == validators.ERROR: result = res elif res == validators.WARNING and result != validators.ERROR: result = res <mask>: msgs.append(err) return result, ""\n"".join(msgs)",False,if len ( err ) > 0 :,elif err :,0.8820016898747209,0.0
"def get_code(self, fullname=None): fullname = self._fix_name(fullname) if self.code is None: mod_type = self.etc[2] if mod_type == imp.PY_SOURCE: source = self.get_source(fullname) self.code = compile(source, self.filename, ""exec"") <mask>: self._reopen() try: self.code = read_code(self.file) finally: self.file.close() elif mod_type == imp.PKG_DIRECTORY: self.code = self._get_delegate().get_code() return self.code",False,elif mod_type == imp . PY_COMPILED :,if self . code is None :,0.8820016898747209,3.823246852690463
"def flush_file(self, key, f): f.flush() if self.compress: f.compress = zlib.compressobj( 9, zlib.DEFLATED, -zlib.MAX_WBITS, zlib.DEF_MEM_LEVEL, 0 ) if len(self.files) > self.MAX_OPEN_FILES: if self.compress: open_files = sum(1 for f in self.files.values() if f.fileobj is not None) <mask>: f.fileobj.close() f.fileobj = None else: f.close() self.files.pop(key)",False,if open_files > self . MAX_OPEN_FILES :,if open_files > 0 :,0.8820016898747209,27.30664777474173
"def TryMerge(self, d): while d.avail() > 0: tt = d.getVarInt32() <mask>: self.add_version(d.getPrefixedString()) continue if tt == 0: raise ProtocolBuffer.ProtocolBufferDecodeError d.skipData(tt)",True,if tt == 10 :,if tt == 10 :,0.8820016898747209,100.00000000000004
"def init_author_file(self): self.author_map = {} if self.ui.config(""git"", ""authors""): f = open(self.repo.wjoin(self.ui.config(""git"", ""authors""))) try: for line in f: line = line.strip() <mask>: continue from_, to = RE_AUTHOR_FILE.split(line, 2) self.author_map[from_] = to finally: f.close()",False,"if not line or line . startswith ( ""#"" ) :",if not line :,0.8820016898747209,6.734410772670761
"def decode_imsi(self, imsi): new_imsi = """" for a in imsi: c = hex(a) <mask>: new_imsi += str(c[3]) + str(c[2]) else: new_imsi += str(c[2]) + ""0"" mcc = new_imsi[1:4] mnc = new_imsi[4:6] return new_imsi, mcc, mnc",False,if len ( c ) == 4 :,"if c [ 0 ] == ""0"" :",0.8820016898747209,9.425159511373677
"def _get_infoset(self, prefname): """"""Return methods with the name starting with prefname."""""" infoset = [] excludes = (""%sinfoset"" % prefname,) preflen = len(prefname) for name in dir(self.__class__): if name.startswith(prefname) and name not in excludes: member = getattr(self.__class__, name) <mask>: infoset.append(name[preflen:].replace(""_"", "" "")) return infoset",False,"if isinstance ( member , MethodType ) :",if member . __name__ == prefname :,0.8820016898747209,4.456882760699063
"def skip_to_close_match(self): nestedCount = 1 while 1: tok = self.tokenizer.get_next_token() ttype = tok[""style""] if ttype == SCE_PL_UNUSED: return elif self.classifier.is_index_op(tok): tval = tok[""text""] if self.opHash.has_key(tval): if self.opHash[tval][1] == 1: nestedCount += 1 else: nestedCount -= 1 <mask>: break",False,if nestedCount <= 0 :,if nestedCount == 0 :,0.8820016898747209,37.99178428257963
"def findMarkForUnitTestNodes(self): """"""return the position of *all* non-ignored @mark-for-unit-test nodes."""""" c = self.c p, result, seen = c.rootPosition(), [], [] while p: <mask>: p.moveToNodeAfterTree() else: seen.append(p.v) if g.match_word(p.h, 0, ""@ignore""): p.moveToNodeAfterTree() elif p.h.startswith(""@mark-for-unit-tests""): result.append(p.copy()) p.moveToNodeAfterTree() else: p.moveToThreadNext() return result",True,if p . v in seen :,if p . v in seen :,0.8820016898747209,100.00000000000004
"def assert_parts_cleaned(self, earlier_parts, current_parts, expected_parts, hint): cleaned_parts = [] for earlier in earlier_parts: earlier_part = earlier[""part""] earlier_step = earlier[""step""] found = False for current in current_parts: if earlier_part == current[""part""] and earlier_step == current[""step""]: found = True break <mask>: cleaned_parts.append(dict(part=earlier_part, step=earlier_step)) self.assertThat(cleaned_parts, HasLength(len(expected_parts)), hint) for expected in expected_parts: self.assertThat(cleaned_parts, Contains(expected), hint)",False,if not found :,if found :,0.8820016898747209,0.0
"def unmark_first_parents(event=None): """"""Mark the node and all its parents."""""" c = event.get(""c"") if not c: return changed = [] for parent in c.p.self_and_parents(): <mask>: parent.v.clearMarked() parent.setAllAncestorAtFileNodesDirty() changed.append(parent.copy()) if changed: # g.es(""unmarked: "" + ', '.join([z.h for z in changed])) c.setChanged() c.redraw() return changed",False,if parent . isMarked ( ) :,if parent . v :,0.8820016898747209,28.641904579795423
"def stop(self): self._log(""Monitor stop"") self._stop_requested = True try: <mask>: fd = os.open(self.fifo_path, os.O_WRONLY) os.write(fd, b""X"") os.close(fd) except Exception as e: self._log(""err while closing: {0}"".format(str(e))) if self._thread: self._thread.join() self._thread = None",False,if os . path . exists ( self . fifo_path ) :,if self . fifo_path :,0.8820016898747209,24.601580968354618
"def DeleteEmptyCols(self): cols2delete = [] for c in range(0, self.GetCols()): f = True for r in range(0, self.GetRows()): <mask>: f = False if f: cols2delete.append(c) for i in range(0, len(cols2delete)): self.ShiftColsLeft(cols2delete[i] + 1) cols2delete = [x - 1 for x in cols2delete]",False,"if self . FindItemAtPosition ( ( r , c ) ) is not None :","if self . GetColumn ( c , r ) == 0 :",0.8820016898747209,12.975280965814113
"def _load_objects(self, obj_id_zset, limit, chunk_size=1000): ct = i = 0 while True: id_chunk = obj_id_zset[i : i + chunk_size] <mask>: return i += chunk_size for raw_data in self._data[id_chunk]: if not raw_data: continue if self._use_json: yield json.loads(decode(raw_data)) else: yield raw_data ct += 1 if limit and ct == limit: return",True,if not id_chunk :,if not id_chunk :,0.8820016898747209,100.00000000000004
"def _convert_example(example, use_bfloat16): """"""Cast int64 into int32 and float32 to bfloat16 if use_bfloat16."""""" for key in list(example.keys()): val = example[key] if tf.keras.backend.is_sparse(val): val = tf.sparse.to_dense(val) <mask>: val = tf.cast(val, tf.int32) if use_bfloat16 and val.dtype == tf.float32: val = tf.cast(val, tf.bfloat16) example[key] = val",False,if val . dtype == tf . int64 :,if use_int32 and val . dtype == tf . int64 :,0.8820016898747209,61.153805769010226
"def print_callees(self, *amount): width, list = self.get_print_list(amount) if list: self.calc_callees() self.print_call_heading(width, ""called..."") for func in list: <mask>: self.print_call_line(width, func, self.all_callees[func]) else: self.print_call_line(width, func, {}) print >>self.stream print >>self.stream return self",True,if func in self . all_callees :,if func in self . all_callees :,0.8820016898747209,100.00000000000004
"def on_task_input(self, task, config): if config is False: return for entry in task.entries: <mask>: log_once( ""Corrected `%s` url (replaced &amp; with &)"" % entry[""title""], logger=log, ) entry[""url""] = entry[""url""].replace(""&amp;"", ""&"")",False,"if ""&amp;"" in entry [ ""url"" ] :","if entry [ ""title"" ] == ""url"" :",0.8820016898747209,23.2334219683501
"def function(self, inputs, outputs, ignore_empty=False): f = function(inputs, outputs, mode=self.mode) if self.mode is not None or theano.config.mode != ""FAST_COMPILE"": topo = f.maker.fgraph.toposort() topo_ = [node for node in topo if not isinstance(node.op, self.ignore_topo)] if ignore_empty: assert len(topo_) <= 1, topo_ else: assert len(topo_) == 1, topo_ <mask>: assert type(topo_[0].op) is self.op return f",False,if len ( topo_ ) > 0 :,if len ( topo_ ) == 1 :,0.8820016898747209,53.7284965911771
"def _get_env_command(self) -> Sequence[str]: """"""Get command sequence for `env` with configured flags."""""" env_list = [""env""] # Pass through configurable environment variables. for key in [""http_proxy"", ""https_proxy""]: value = self.build_provider_flags.get(key) <mask>: continue # Ensure item is treated as string and append it. value = str(value) env_list.append(f""{key}={value}"") return env_list",False,if not value :,if value is None :,0.8820016898747209,14.058533129758727
"def _compare_single_run(self, compares_done): try: compare_id, redo = self.in_queue.get( timeout=float(self.config[""ExpertSettings""][""block_delay""]) ) except Empty: pass else: if self._decide_whether_to_process(compare_id, redo, compares_done): if redo: self.db_interface.delete_old_compare_result(compare_id) compares_done.add(compare_id) self._process_compare(compare_id) <mask>: self.callback()",True,if self . callback :,if self . callback :,0.8820016898747209,100.00000000000004
"def clean(self): # TODO: check for clashes if the random code is already taken if not self.code: self.code = u""static-%s"" % uuid.uuid4() if not self.site: placeholders = StaticPlaceholder.objects.filter( code=self.code, site__isnull=True ) if self.pk: placeholders = placeholders.exclude(pk=self.pk) <mask>: raise ValidationError( _(""A static placeholder with the same site and code already exists"") )",False,if placeholders . exists ( ) :,if not placeholders :,0.8820016898747209,9.930283522141846
"def load_parser(self): result = OrderedDict() for name, flags in self.filenames: filename = self.get_filename(name) for match in sorted(glob(filename), key=self.file_key): # Needed to allow overlapping globs, more specific first <mask>: continue result[match] = TextParser(match, os.path.relpath(match, self.base), flags) return result",False,if match in result :,if match in self . ignore_globs :,0.8820016898747209,19.070828081828378
"def __init__(self, selectable, name=None): baseselectable = selectable while isinstance(baseselectable, Alias): baseselectable = baseselectable.element self.original = baseselectable self.supports_execution = baseselectable.supports_execution if self.supports_execution: self._execution_options = baseselectable._execution_options self.element = selectable if name is None: <mask>: name = getattr(self.original, ""name"", None) name = _anonymous_label(""%%(%d %s)s"" % (id(self), name or ""anon"")) self.name = name",False,if self . original . named_with_column :,"if hasattr ( self . original , ""name"" ) :",0.8820016898747209,14.323145079400492
"def load_tour(self, tour_id): for tour_dir in self.tour_directories: tour_path = os.path.join(tour_dir, tour_id + "".yaml"") if not os.path.exists(tour_path): tour_path = os.path.join(tour_dir, tour_id + "".yml"") <mask>: return self._load_tour_from_path(tour_path)",True,if os . path . exists ( tour_path ) :,if os . path . exists ( tour_path ) :,0.8820016898747209,100.00000000000004
"def _get_md_bg_color_down(self): t = self.theme_cls c = self.md_bg_color # Default to no change on touch # Material design specifies using darker hue when on Dark theme if t.theme_style == ""Dark"": <mask>: c = t.primary_dark elif self.md_bg_color == t.accent_color: c = t.accent_dark return c",True,if self . md_bg_color == t . primary_color :,if self . md_bg_color == t . primary_color :,0.8820016898747209,100.00000000000004
"def get_data(self, state=None, request=None): if self.load_in_memory: data, shapes = self._in_memory_get_data(state, request) else: data, shapes = self._out_of_memory_get_data(state, request) for i in range(len(data)): if shapes[i] is not None: <mask>: data[i] = data[i].reshape(shapes[i]) else: for j in range(len(data[i])): data[i][j] = data[i][j].reshape(shapes[i][j]) return tuple(data)",False,"if isinstance ( request , numbers . Integral ) :",if i == len ( data ) - 1 :,0.8820016898747209,5.300156689756295
"def onClicked(event): if not self.path: <mask>: os.makedirs(mh.getPath(""render"")) self.path = mh.getPath(""render"") filename, ftype = mh.getSaveFileName( os.path.splitext(self.path)[0], ""PNG Image (*.png);;JPEG Image (*.jpg);;Thumbnail (*.thumb);;All files (*.*)"", ) if filename: if ""Thumbnail"" in ftype: self.image.save(filename, iformat=""PNG"") else: self.image.save(filename) self.path = os.path.dirname(filename)",True,"if not os . path . exists ( mh . getPath ( ""render"" ) ) :","if not os . path . exists ( mh . getPath ( ""render"" ) ) :",0.8820016898747209,100.00000000000004
"def _build_dom(cls, content, mode): assert mode in (""html"", ""xml"") if mode == ""html"": <mask>: THREAD_STORAGE.html_parser = HTMLParser() dom = defusedxml.lxml.parse( StringIO(content), parser=THREAD_STORAGE.html_parser ) return dom.getroot() else: if not hasattr(THREAD_STORAGE, ""xml_parser""): THREAD_STORAGE.xml_parser = XMLParser() dom = defusedxml.lxml.parse(BytesIO(content), parser=THREAD_STORAGE.xml_parser) return dom.getroot()",True,"if not hasattr ( THREAD_STORAGE , ""html_parser"" ) :","if not hasattr ( THREAD_STORAGE , ""html_parser"" ) :",0.8820016898747209,100.00000000000004
"def convert_path(ctx, tpath): for points, code in tpath.iter_segments(): <mask>: ctx.move_to(*points) elif code == Path.LINETO: ctx.line_to(*points) elif code == Path.CURVE3: ctx.curve_to( points[0], points[1], points[0], points[1], points[2], points[3] ) elif code == Path.CURVE4: ctx.curve_to(*points) elif code == Path.CLOSEPOLY: ctx.close_path()",False,if code == Path . MOVETO :,if code == Path . MOVE :,0.8820016898747209,70.71067811865478
"def _targets(self, sigmaparser): # build list of matching target mappings targets = set() for condfield in self.conditions: if condfield in sigmaparser.values: rulefieldvalues = sigmaparser.values[condfield] for condvalue in self.conditions[condfield]: <mask>: targets.update(self.conditions[condfield][condvalue]) return targets",True,if condvalue in rulefieldvalues :,if condvalue in rulefieldvalues :,0.8820016898747209,100.00000000000004
"def create_image_upload(): if request.method == ""POST"": image = request.form[""image""] <mask>: image_file = uploaded_file(file_content=image) image_url = upload_local( image_file, UPLOAD_PATHS[""temp""][""image""].format(uuid=uuid4()) ) return jsonify({""status"": ""ok"", ""image_url"": image_url}) else: return jsonify({""status"": ""no_image""})",True,if image :,if image :,0.8820016898747209,0.0
"def lookup_actions(self, resp): actions = {} for action, conditions in self.actions.items(): for condition, opts in conditions: for key, val in condition: if key[-1] == ""!"": <mask>: break else: if not resp.match(key, val): break else: actions[action] = opts return actions",False,"if resp . match ( key [ : - 1 ] , val ) :","if resp . match ( key , val ) :",0.8820016898747209,47.65082587109519
"def accept_quality(accept, default=1): """"""Separates out the quality score from the accepted content_type"""""" quality = default if accept and "";"" in accept: accept, rest = accept.split("";"", 1) accept_quality = RE_ACCEPT_QUALITY.search(rest) <mask>: quality = float(accept_quality.groupdict().get(""quality"", quality).strip()) return (quality, accept.strip())",True,if accept_quality :,if accept_quality :,0.8820016898747209,100.00000000000004
"def save(self, session=None, to=None, pickler=None): if to and pickler: self._save_to = (pickler, to) if self._save_to and len(self) > 0: with self._lock: pickler, fn = self._save_to <mask>: session.ui.mark(_(""Saving %s state to %s"") % (self, fn)) pickler(self, fn)",False,if session :,if pickler :,0.8820016898747209,0.0
"def get_safe_settings(): ""Returns a dictionary of the settings module, with sensitive settings blurred out."" settings_dict = {} for k in dir(settings): if k.isupper(): <mask>: settings_dict[k] = ""********************"" else: settings_dict[k] = getattr(settings, k) return settings_dict",False,if HIDDEN_SETTINGS . search ( k ) :,"if k == ""settings"" :",0.8820016898747209,5.660233915657916
def _init_table_h(): _table_h = [] for i in range(256): part_l = i part_h = 0 for j in range(8): rflag = part_l & 1 part_l >>= 1 if part_h & 1: part_l |= 1 << 31 part_h >>= 1 <mask>: part_h ^= 0xD8000000 _table_h.append(part_h) return _table_h,True,if rflag :,if rflag :,0.8820016898747209,0.0
"def dns_query(server, timeout, protocol, qname, qtype, qclass): request = dns.message.make_query(qname, qtype, qclass) if protocol == ""tcp"": response = dns.query.tcp( request, server, timeout=timeout, one_rr_per_rrset=True ) else: response = dns.query.udp( request, server, timeout=timeout, one_rr_per_rrset=True ) <mask>: response = dns.query.tcp( request, server, timeout=timeout, one_rr_per_rrset=True ) return response",False,if response . flags & dns . flags . TC :,"if protocol == ""tcp"" :",0.8820016898747209,4.513617516969122
"def sum_and_divide(self, losses): if self.total_divisor != 0: output = torch.sum(losses) / self.total_divisor <mask>: # remove from autograd graph if necessary self.total_divisor = self.total_divisor.item() return output return torch.sum(losses * 0)",False,if torch . is_tensor ( self . total_divisor ) :,if output . ndim == 1 :,0.8820016898747209,3.4331054109918173
"def __iter__(self): for chunk in self.source: if chunk is not None: self.wait_counter = 0 yield chunk <mask>: self.wait_counter += 1 else: logger.warning( ""Data poller has been receiving no data for {} seconds.\n"" ""Closing data poller"".format(self.wait_cntr_max * self.poll_period) ) break time.sleep(self.poll_period)",True,elif self . wait_counter < self . wait_cntr_max :,elif self . wait_counter < self . wait_cntr_max :,0.8820016898747209,100.00000000000004
"def test_find_directive_from_block(self): blocks = self.config.parser_root.find_blocks(""virtualhost"") found = False for vh in blocks: <mask>: servername = vh.find_directives(""servername"") self.assertEqual(servername[0].parameters[0], ""certbot.demo"") found = True self.assertTrue(found)",False,"if vh . filepath . endswith ( ""sites-enabled/certbot.conf"" ) :","if vh . name == ""servername"" :",0.8820016898747209,10.194207971045941
"def assign_products(request, discount_id): """"""Assign products to given property group with given property_group_id."""""" discount = lfs_get_object_or_404(Discount, pk=discount_id) for temp_id in request.POST.keys(): <mask>: temp_id = temp_id.split(""-"")[1] product = Product.objects.get(pk=temp_id) discount.products.add(product) html = [[""#products-inline"", products_inline(request, discount_id, as_string=True)]] result = json.dumps( {""html"": html, ""message"": _(u""Products have been assigned."")}, cls=LazyEncoder ) return HttpResponse(result, content_type=""application/json"")",False,"if temp_id . startswith ( ""product"" ) :","if ""-"" in temp_id :",0.8820016898747209,15.716803955215932
"def ChangeStyle(self, combos): style = 0 for combo in combos: if combo.GetValue() == 1: <mask>: style = style | HTL.TR_VIRTUAL else: try: style = style | eval(""wx."" + combo.GetLabel()) except: style = style | eval(""HTL."" + combo.GetLabel()) if self.GetAGWWindowStyleFlag() != style: self.SetAGWWindowStyleFlag(style)",False,"if combo . GetLabel ( ) == ""TR_VIRTUAL"" :","if combo . GetLabel ( ) == ""v"" :",0.8820016898747209,65.10803637373398
"def _set_autocomplete(self, notebook): if notebook: try: <mask>: notebook = NotebookInfo(notebook) obj, x = build_notebook(notebook) self.form.widgets[""namespace""].notebook = obj self.form.widgets[""page""].notebook = obj logger.debug(""Notebook for autocomplete: %s (%s)"", obj, notebook) except: logger.exception(""Could not set notebook: %s"", notebook) else: self.form.widgets[""namespace""].notebook = None self.form.widgets[""page""].notebook = None logger.debug(""Notebook for autocomplete unset"")",False,"if isinstance ( notebook , str ) :","if not isinstance ( notebook , NotebookInfo ) :",0.8820016898747209,36.88939732334405
"def emitSubDomainData(self, subDomainData, event): self.emitRawRirData(subDomainData, event) for subDomainElem in subDomainData: <mask>: return None subDomain = subDomainElem.get(""subdomain"", """").strip() if subDomain: self.emitHostname(subDomain, event)",False,if self . checkForStop ( ) :,if not subDomainElem :,0.8820016898747209,8.9730240870212
"def get_all_subnets(self, subnet_ids=None, filters=None): # Extract a list of all subnets matches = itertools.chain(*[x.values() for x in self.subnets.values()]) if subnet_ids: matches = [sn for sn in matches if sn.id in subnet_ids] <mask>: unknown_ids = set(subnet_ids) - set(matches) raise InvalidSubnetIdError(unknown_ids) if filters: matches = generic_filter(filters, matches) return matches",False,if len ( subnet_ids ) > len ( matches ) :,if len ( matches ) != len ( subnet_ids ) :,0.8820016898747209,61.153805769010226
"def _compat_map(self, avs): apps = {} for av in avs: av.version = self app_id = av.application <mask>: apps[amo.APP_IDS[app_id]] = av return apps",True,if app_id in amo . APP_IDS :,if app_id in amo . APP_IDS :,0.8820016898747209,100.00000000000004
"def generator(self, data): if self._config.SILENT: silent_vars = self._get_silent_vars() for task in data: for var, val in task.environment_variables(): if self._config.SILENT: <mask>: continue yield ( 0, [ int(task.UniqueProcessId), str(task.ImageFileName), Address(task.Peb.ProcessParameters.Environment), str(var), str(val), ], )",True,if var in silent_vars :,if var in silent_vars :,0.8820016898747209,100.00000000000004
"def warn_if_repeatable_read(self): if ""mysql"" in self.current_engine().lower(): cursor = self.connection_for_read().cursor() if cursor.execute(""SELECT @@tx_isolation""): isolation = cursor.fetchone()[0] <mask>: warnings.warn( TxIsolationWarning( ""Polling results with transaction isolation level "" ""repeatable-read within the same transaction "" ""may give outdated results. Be sure to commit the "" ""transaction for each poll iteration."" ) )",False,"if isolation == ""REPEATABLE-READ"" :","if isolation == ""repeatable-read"" :",0.8820016898747209,59.4603557501361
"def filter_by_level(record, level_per_module): name = record[""name""] level = 0 if name in level_per_module: level = level_per_module[name] elif name is not None: lookup = """" <mask>: level = level_per_module[""""] for n in name.split("".""): lookup += n if lookup in level_per_module: level = level_per_module[lookup] lookup += ""."" if level is False: return False return record[""level""].no >= level",True,"if """" in level_per_module :","if """" in level_per_module :",0.8820016898747209,100.00000000000004
"def _readStream(self, handle: str, path: str) -> None: eof = False file = Path(path) with file.open(""w"") as f: while not eof: response = await self._client.send(""IO.read"", {""handle"": handle}) eof = response.get(""eof"", False) <mask>: f.write(response.get(""data"", """")) await self._client.send(""IO.close"", {""handle"": handle})",False,if path :,if not eof :,0.8820016898747209,18.99589214128981
"def sendall(self, data, flags=0): if self._sslobj: <mask>: raise ValueError( ""non-zero flags not allowed in calls to sendall() on %s"" % self.__class__ ) amount = len(data) count = 0 while count < amount: v = self.send(data[count:]) count += v return amount else: return socket.sendall(self, data, flags)",True,if flags != 0 :,if flags != 0 :,0.8820016898747209,100.00000000000004
"def run(self): utils.assert_main_thread() # As a convenience, we'll set up the connection # if there isn't one. So F5 (etc) can be hit # to get started. if not channel: <mask>: SwiDebugStartChromeCommand.run(self) else: self.window.run_command(""swi_debug_start"") elif paused: logger.info(""Resuming..."") channel.send(webkit.Debugger.resume()) else: logger.info(""Pausing..."") channel.send(webkit.Debugger.setSkipAllPauses(False)) channel.send(webkit.Debugger.pause())",False,if not chrome_launched ( ) :,if self . window . is_chrome :,0.8820016898747209,6.742555929751843
"def TryMerge(self, d): while d.avail() > 0: tt = d.getVarInt32() if tt == 10: length = d.getVarInt32() tmp = ProtocolBuffer.Decoder(d.buffer(), d.pos(), d.pos() + length) d.skip(length) self.add_presence_response().TryMerge(tmp) continue <mask>: raise ProtocolBuffer.ProtocolBufferDecodeError d.skipData(tt)",True,if tt == 0 :,if tt == 0 :,0.8820016898747209,100.00000000000004
"def _replace_home(x): if xp.ON_WINDOWS: home = ( builtins.__xonsh__.env[""HOMEDRIVE""] + builtins.__xonsh__.env[""HOMEPATH""][0] ) if x.startswith(home): x = x.replace(home, ""~"", 1) <mask>: x = x.replace(os.sep, os.altsep) return x else: home = builtins.__xonsh__.env[""HOME""] if x.startswith(home): x = x.replace(home, ""~"", 1) return x",False,"if builtins . __xonsh__ . env . get ( ""FORCE_POSIX_PATHS"" ) :",if x . endswith ( os . sep ) :,0.8820016898747209,3.3266284965577233
"def semanticTags(self, semanticTags): if semanticTags is None: self.__semanticTags = OrderedDict() # check for key, value in list(semanticTags.items()): <mask>: raise TypeError(""At least one key is not a valid int position"") if not isinstance(value, list): raise TypeError( ""At least one value of the provided dict is not a list of string"" ) for x in value: if not isinstance(x, str): raise TypeError( ""At least one value of the provided dict is not a list of string"" ) self.__semanticTags = semanticTags",True,"if not isinstance ( key , int ) :","if not isinstance ( key , int ) :",0.8820016898747209,100.00000000000004
"def _recv(): try: return sock.recv(bufsize) except SSLWantReadError: pass except socket.error as exc: error_code = extract_error_code(exc) if error_code is None: raise <mask>: raise r, w, e = select.select((sock,), (), (), sock.gettimeout()) if r: return sock.recv(bufsize)",False,if error_code != errno . EAGAIN or error_code != errno . EWOULDBLOCK :,if error_code != errno . EINTR :,0.8820016898747209,31.81579525928275
"def _authenticate(self): oauth_token = self.options.get(""oauth_token"") if oauth_token and not self.api.oauth_token: self.logger.info(""Attempting to authenticate using OAuth token"") self.api.oauth_token = oauth_token user = self.api.user(schema=_user_schema) <mask>: self.logger.info(""Successfully logged in as {0}"", user) else: self.logger.error( ""Failed to authenticate, the access token "" ""is not valid"" ) else: return JustinTVPluginBase._authenticate(self)",True,if user :,if user :,0.8820016898747209,0.0
"def reverse(self, *args): assert self._path is not None, ""Cannot reverse url regex "" + self.regex.pattern assert len(args) == self._group_count, ""required number of arguments "" ""not found"" if not len(args): return self._path converted_args = [] for a in args: <mask>: a = str(a) converted_args.append(escape.url_escape(utf8(a), plus=False)) return self._path % tuple(converted_args)",False,"if not isinstance ( a , ( unicode_type , bytes ) ) :","if not isinstance ( a , str ) :",0.8820016898747209,33.9180221569181
"def determine_block_hints(self, text): hints = """" if text: <mask>: hints += str(self.best_indent) if text[-1] not in ""\n\x85\u2028\u2029"": hints += ""-"" elif len(text) == 1 or text[-2] in ""\n\x85\u2028\u2029"": hints += ""+"" return hints",False,"if text [ 0 ] in "" \n\x85\u2028\u2029"" :",if len ( text ) > 1 :,0.8820016898747209,2.3595365419339505
"def find_package_modules(package, mask): import fnmatch if hasattr(package, ""__loader__"") and hasattr(package.__loader__, ""_files""): path = package.__name__.replace(""."", os.path.sep) mask = os.path.join(path, mask) for fnm in package.__loader__._files.iterkeys(): <mask>: yield os.path.splitext(fnm)[0].replace(os.path.sep, ""."") else: path = package.__path__[0] for fnm in os.listdir(path): if fnmatch.fnmatchcase(fnm, mask): yield ""%s.%s"" % (package.__name__, os.path.splitext(fnm)[0])",False,"if fnmatch . fnmatchcase ( fnm , mask ) :","if fnmatch . fnmatch ( fnm , mask ) :",0.8820016898747209,65.80370064762461
"def _condition(ct): for qobj in args: if qobj.connector == ""AND"" and not qobj.negated: # normal kwargs are an AND anyway, so just use those for now for child in qobj.children: kwargs.update(dict([child])) else: raise NotImplementedError(""Unsupported Q object"") for attr, val in kwargs.items(): <mask>: return False return True",False,"if getattr ( ct , attr ) != val :",if attr not in ct :,0.8820016898747209,4.988641679706251
"def process(self, resources): session = local_session(self.manager.session_factory) client = session.client(""logs"") state = self.data.get(""state"", True) key = self.resolve_key(self.data.get(""kms-key"")) for r in resources: try: <mask>: client.associate_kms_key(logGroupName=r[""logGroupName""], kmsKeyId=key) else: client.disassociate_kms_key(logGroupName=r[""logGroupName""]) except client.exceptions.ResourceNotFoundException: continue",True,if state :,if state :,0.8820016898747209,0.0
"def get_xmm(env, ii): if is_gather(ii): <mask>: return gen_reg_simd_unified(env, ""xmm_evex"", True) return gen_reg_simd_unified(env, ""xmm"", False) if ii.space == ""evex"": return gen_reg(env, ""xmm_evex"") return gen_reg(env, ""xmm"")",True,"if ii . space == ""evex"" :","if ii . space == ""evex"" :",0.8820016898747209,100.00000000000004
"def parent(self): """"""Return the parent device."""""" if self._has_parent is None: _parent = self._ctx.backend.get_parent(self._ctx.dev) self._has_parent = _parent is not None <mask>: self._parent = Device(_parent, self._ctx.backend) else: self._parent = None return self._parent",False,if self . _has_parent :,if _parent is not None :,0.8820016898747209,13.540372457315735
"def cascade(self, event=None): """"""Cascade all Leo windows."""""" x, y, delta = 50, 50, 50 for frame in g.app.windowList: w = frame and frame.top <mask>: r = w.geometry() # a Qt.Rect # 2011/10/26: Fix bug 823601: cascade-windows fails. w.setGeometry(QtCore.QRect(x, y, r.width(), r.height())) # Compute the new offsets. x += 30 y += 30 if x > 200: x = 10 + delta y = 40 + delta delta += 10",True,if w :,if w :,0.8820016898747209,0.0
"def _GetGoodDispatchAndUserName(IDispatch, userName, clsctx): # Get a dispatch object, and a 'user name' (ie, the name as # displayed to the user in repr() etc. if userName is None: if isinstance(IDispatch, str): userName = IDispatch <mask>: # We always want the displayed name to be a real string userName = IDispatch.encode(""ascii"", ""replace"") elif type(userName) == unicode: # As above - always a string... userName = userName.encode(""ascii"", ""replace"") else: userName = str(userName) return (_GetGoodDispatch(IDispatch, clsctx), userName)",False,"elif isinstance ( IDispatch , unicode ) :",elif type ( IDispatch ) == str :,0.8820016898747209,11.99014838091355
"def _infer_return_type(*args): """"""Look at the type of all args and divine their implied return type."""""" return_type = None for arg in args: if arg is None: continue if isinstance(arg, bytes): if return_type is str: raise TypeError(""Can't mix bytes and non-bytes in "" ""path components."") return_type = bytes else: <mask>: raise TypeError(""Can't mix bytes and non-bytes in "" ""path components."") return_type = str if return_type is None: return str # tempfile APIs return a str by default. return return_type",False,if return_type is bytes :,if return_type is str :,0.8820016898747209,64.34588841607616
"def test_ESPnetDataset_h5file_1(h5file_1): dataset = IterableESPnetDataset( path_name_type_list=[(h5file_1, ""data4"", ""hdf5"")], preprocess=preprocess, ) for key, data in dataset: if key == ""a"": assert data[""data4""].shape == ( 100, 80, ) <mask>: assert data[""data4""].shape == ( 150, 80, )",False,"if key == ""b"" :","elif key == ""b"" :",0.8820016898747209,84.08964152537145
"def iter_fields(node, *, include_meta=True, exclude_unset=False): exclude_meta = not include_meta for field_name, field in node._fields.items(): if exclude_meta and field.meta: continue field_val = getattr(node, field_name, _marker) <mask>: continue if exclude_unset: if callable(field.default): default = field.default() else: default = field.default if field_val == default: continue yield field_name, field_val",False,if field_val is _marker :,if field_val is None :,0.8820016898747209,55.780028607687655
"def then(self, matches, when_response, context): if is_iterable(when_response): ret = [] when_response = list(when_response) for match in when_response: <mask>: if self.match_name: match.name = self.match_name matches.append(match) ret.append(match) return ret if self.match_name: when_response.name = self.match_name if when_response not in matches: matches.append(when_response) return when_response",False,if match not in matches :,if match . name == self . match_name :,0.8820016898747209,7.495553473355845
"def _set_chat_ids(self, chat_id: SLT[int]) -> None: with self.__lock: <mask>: raise RuntimeError( f""Can't set {self.chat_id_name} in conjunction with (already set) "" f""{self.username_name}s."" ) self._chat_ids = self._parse_chat_id(chat_id)",False,if chat_id and self . _usernames :,if self . _chat_ids :,0.8820016898747209,21.897593220582774
"def discover(self, *objlist): ret = [] for l in self.splitlines(): <mask>: continue if l[0] == ""Filename"": continue try: int(l[2]) int(l[3]) except: continue # ret.append(improve(l[0])) ret.append(l[0]) ret.sort() for item in objlist: ret.append(item) return ret",False,if len ( l ) < 5 :,if len ( l ) < 4 :,0.8820016898747209,70.71067811865478
"def get_changed_module(self): source = self.resource.read() change_collector = codeanalyze.ChangeCollector(source) if self.replacement is not None: change_collector.add_change(self.skip_start, self.skip_end, self.replacement) for occurrence in self.occurrence_finder.find_occurrences(self.resource): start, end = occurrence.get_primary_range() <mask>: self.handle.occurred_inside_skip(change_collector, occurrence) else: self.handle.occurred_outside_skip(change_collector, occurrence) result = change_collector.get_changed() if result is not None and result != source: return result",False,if self . skip_start <= start < self . skip_end :,if start == self . skip_start :,0.8820016898747209,24.97060786087555
"def hpat_pandas_series_var_impl( self, axis=None, skipna=None, level=None, ddof=1, numeric_only=None ): if skipna is None: skipna = True if skipna: valuable_length = len(self._data) - numpy.sum(numpy.isnan(self._data)) <mask>: return numpy.nan return ( numpy_like.nanvar(self._data) * valuable_length / (valuable_length - ddof) ) if len(self._data) <= ddof: return numpy.nan return self._data.var() * len(self._data) / (len(self._data) - ddof)",True,if valuable_length <= ddof :,if valuable_length <= ddof :,0.8820016898747209,100.00000000000004
"def to_dict(self, validate=True, ignore=(), context=None): context = context or {} condition = getattr(self, ""condition"", Undefined) copy = self # don't copy unless we need to if condition is not Undefined: <mask>: pass elif ""field"" in condition and ""type"" not in condition: kwds = parse_shorthand(condition[""field""], context.get(""data"", None)) copy = self.copy(deep=[""condition""]) copy.condition.update(kwds) return super(ValueChannelMixin, copy).to_dict( validate=validate, ignore=ignore, context=context )",False,"if isinstance ( condition , core . SchemaBase ) :","if condition . get ( ""type"" ) is Undefined :",0.8820016898747209,5.300156689756295
"def get_field_result(self, result, field_name): if isinstance(result.field, models.ImageField): <mask>: img = getattr(result.obj, field_name) result.text = mark_safe( '<a href=""%s"" target=""_blank"" title=""%s"" data-gallery=""gallery""><img src=""%s"" class=""field_img""/></a>' % (img.url, result.label, img.url) ) self.include_image = True return result",False,if result . value :,if field_name :,0.8820016898747209,12.703318703865365
"def run(self): try: while True: dp = self.queue_get_stoppable(self.inq) <mask>: return # cannot ignore None here. will lead to unsynced send/recv obj = self.func(dp) self.queue_put_stoppable(self.outq, obj) except Exception: if self.stopped(): pass # skip duplicated error messages else: raise finally: self.stop()",False,if self . stopped ( ) :,if dp is None :,0.8820016898747209,8.51528917838043
"def _evaluate_local_single(self, iterator): for batch in iterator: in_arrays = convert._call_converter(self.converter, batch, self.device) with function.no_backprop_mode(): if isinstance(in_arrays, tuple): results = self.calc_local(*in_arrays) elif isinstance(in_arrays, dict): results = self.calc_local(**in_arrays) else: results = self.calc_local(in_arrays) <mask>: self._progress_hook(batch) yield results",True,if self . _progress_hook :,if self . _progress_hook :,0.8820016898747209,100.00000000000004
"def merge(self, other): d = self._name2ft for name, (f, t) in other._name2ft.items(): <mask>: # Don't print here by default, since doing # so breaks some of the buildbots # print ""*** DocTestRunner.merge: '"" + name + ""' in both"" \ # "" testers; summing outcomes."" f2, t2 = d[name] f = f + f2 t = t + t2 d[name] = f, t",True,if name in d :,if name in d :,0.8820016898747209,100.00000000000004
"def _addSettingsToPanels(self, category, left, right): count = len(profile.getSubCategoriesFor(category)) + len( profile.getSettingsForCategory(category) ) p = left n = 0 for title in profile.getSubCategoriesFor(category): n += 1 + len(profile.getSettingsForCategory(category, title)) <mask>: p = right configBase.TitleRow(p, _(title)) for s in profile.getSettingsForCategory(category, title): configBase.SettingRow(p, s.getName())",False,if n > count / 2 :,if n == count :,0.8820016898747209,16.341219448835542
"def __init__(self, parent, dir, mask, with_dirs=True): filelist = [] dirlist = [""..""] self.dir = dir self.file = """" mask = mask.upper() pattern = self.MakeRegex(mask) for i in os.listdir(dir): <mask>: continue path = os.path.join(dir, i) if os.path.isdir(path): dirlist.append(i) continue path = path.upper() value = i.upper() if pattern.match(value) is not None: filelist.append(i) self.files = filelist if with_dirs: self.dirs = dirlist",False,"if i == ""."" or i == "".."" :",if i . startswith ( parent ) :,0.8820016898747209,4.831930719842458
def check_network_private(test_network): test_net = ipaddress.IPNetwork(test_network) test_start = test_net.network test_end = test_net.broadcast for network in settings.vpn.safe_priv_subnets: network = ipaddress.IPNetwork(network) net_start = network.network net_end = network.broadcast <mask>: return True return False,False,if test_start >= net_start and test_end <= net_end :,if test_start <= net_end <= test_start :,0.8820016898747209,45.0478099011514
"def _end_description(self): if self._summaryKey == ""content"": self._end_content() else: value = self.popContent(""description"") context = self._getContext() <mask>: context[""textinput""][""description""] = value elif self.inimage: context[""image""][""description""] = value self._summaryKey = None",True,if self . intextinput :,if self . intextinput :,0.8820016898747209,100.00000000000004
def compute_nullable_nonterminals(self): nullable = {} num_nullable = 0 while 1: for p in self.grammar.Productions[1:]: <mask>: nullable[p.name] = 1 continue for t in p.prod: if not t in nullable: break else: nullable[p.name] = 1 if len(nullable) == num_nullable: break num_nullable = len(nullable) return nullable,False,if p . len == 0 :,if not p . name in nullable :,0.8820016898747209,13.134549472120788
"def process_bind_param(self, value, dialect): if value is not None: if MAX_METADATA_VALUE_SIZE is not None: for k, v in list(value.items()): sz = total_size(v) <mask>: del value[k] log.warning( ""Refusing to bind metadata key {} due to size ({})"".format( k, sz ) ) value = json_encoder.encode(value).encode() return value",True,if sz > MAX_METADATA_VALUE_SIZE :,if sz > MAX_METADATA_VALUE_SIZE :,0.8820016898747209,100.00000000000004
"def process_input_line(self, line, store_history=True): """"""process the input, capturing stdout"""""" stdout = sys.stdout splitter = self.IP.input_splitter try: sys.stdout = self.cout splitter.push(line) more = splitter.push_accepts_more() <mask>: try: source_raw = splitter.source_raw_reset()[1] except: # recent ipython #4504 source_raw = splitter.raw_reset() self.IP.run_cell(source_raw, store_history=store_history) finally: sys.stdout = stdout",False,if not more :,if more :,0.8820016898747209,0.0
"def _dump_section(self, name, values, f): doc = ""__doc__"" <mask>: print(""# %s"" % values[doc], file=f) print(""%s("" % name, file=f) for k, v in values.items(): if k.endswith(""__doc__""): continue doc = k + ""__doc__"" if doc in values: print("" # %s"" % values[doc], file=f) print("" %s = %s,"" % (k, pprint.pformat(v, indent=8)), file=f) print("")\n"", file=f)",True,if doc in values :,if doc in values :,0.8820016898747209,100.00000000000004
"def open_session(self, app, request): sid = request.cookies.get(app.session_cookie_name) if sid: stored_session = self.cls.objects(sid=sid).first() if stored_session: expiration = stored_session.expiration <mask>: expiration = expiration.replace(tzinfo=utc) if expiration > datetime.datetime.utcnow().replace(tzinfo=utc): return MongoEngineSession( initial=stored_session.data, sid=stored_session.sid ) return MongoEngineSession(sid=str(uuid.uuid4()))",False,if not expiration . tzinfo :,if expiration :,0.8820016898747209,0.0
"def table_entry(mode1, bind_type1, mode2, bind_type2): with sock(mode1) as sock1: bind(sock1, bind_type1) try: with sock(mode2) as sock2: bind(sock2, bind_type2) except OSError as exc: <mask>: return ""INUSE"" elif exc.winerror == errno.WSAEACCES: return ""ACCESS"" raise else: return ""Success""",False,if exc . winerror == errno . WSAEADDRINUSE :,if exc . winerror == errno . EINVAL :,0.8820016898747209,78.25422900366438
"def __init__(self, ruleset): # Organize rules by path self.ruleset = ruleset self.rules = {} for filename in self.ruleset.rules: for rule in self.ruleset.rules[filename]: <mask>: continue manage_dictionary(self.rules, rule.path, []) self.rules[rule.path].append(rule)",False,if not rule . enabled :,if rule . path in self . rules :,0.8820016898747209,11.339582221952005
"def talk(self, words): if self.writeSentence(words) == 0: return r = [] while 1: i = self.readSentence() if len(i) == 0: continue reply = i[0] attrs = {} for w in i[1:]: j = w.find(""="", 1) <mask>: attrs[w] = """" else: attrs[w[:j]] = w[j + 1 :] r.append((reply, attrs)) if reply == ""!done"": return r",True,if j == - 1 :,if j == - 1 :,0.8820016898747209,100.00000000000004
"def _check_decorator_overload(name: str, old: str, new: str) -> int: """"""Conditions for a decorator to overload an existing one."""""" properties = _property_decorators(name) if old == new: return _MERGE elif old in properties and new in properties: p_old, p_new = properties[old].precedence, properties[new].precedence <mask>: return _DISCARD elif p_old == p_new: return _MERGE else: return _REPLACE raise OverloadedDecoratorError(name, """")",False,if p_old > p_new :,if p_old == p_new :,0.8820016898747209,52.53819788848316
"def validate_pk(self): try: self._key = serialization.load_pem_private_key( self.key, password=None, backend=default_backend() ) if self._key.key_size > 2048: AWSValidationException( ""The private key length is not supported. Only 1024-bit and 2048-bit are allowed."" ) except Exception as err: <mask>: raise raise AWSValidationException( ""The private key is not PEM-encoded or is not valid."" )",False,"if isinstance ( err , AWSValidationException ) :","if err . args [ 0 ] != ""InvalidPrivateKey"" :",0.8820016898747209,4.065425428798724
"def _add_custom_statement(self, custom_statements): if custom_statements is None: return self.resource_policy[""Version""] = ""2012-10-17"" if self.resource_policy.get(""Statement"") is None: self.resource_policy[""Statement""] = custom_statements else: <mask>: custom_statements = [custom_statements] statement = self.resource_policy[""Statement""] if not isinstance(statement, list): statement = [statement] for s in custom_statements: if s not in statement: statement.append(s) self.resource_policy[""Statement""] = statement",True,"if not isinstance ( custom_statements , list ) :","if not isinstance ( custom_statements , list ) :",0.8820016898747209,100.00000000000004
"def load(self, repn): for key in repn: tmp = self._convert(key) <mask>: self.declare(tmp) item = dict.__getitem__(self, tmp) item._active = True item.load(repn[key])",False,if tmp not in self :,if tmp not in self . _active :,0.8820016898747209,46.713797772819994
"def on_press_release(x): """"""Keyboard callback function."""""" global is_recording, enable_trigger_record press = keyboard.KeyboardEvent(""down"", 28, ""space"") release = keyboard.KeyboardEvent(""up"", 28, ""space"") if x.event_type == ""down"" and x.name == press.name: if (not is_recording) and enable_trigger_record: sys.stdout.write(""Start Recording ... "") sys.stdout.flush() is_recording = True if x.event_type == ""up"" and x.name == release.name: <mask>: is_recording = False",False,if is_recording == True :,if not is_recording ) and enable_trigger_record :,0.8820016898747209,13.065113298388567
"def apply_mask(self, mask, data_t, data_f): ind_t, ind_f = 0, 0 out = [] for m in cycle(mask): if m: if ind_t == len(data_t): return out out.append(data_t[ind_t]) ind_t += 1 else: <mask>: return out out.append(data_f[ind_f]) ind_f += 1 return out",True,if ind_f == len ( data_f ) :,if ind_f == len ( data_f ) :,0.8820016898747209,100.00000000000004
"def oo_contains_rule(source, apiGroups, resources, verbs): """"""Return true if the specified rule is contained within the provided source"""""" rules = source[""rules""] if rules: for rule in rules: if set(rule[""apiGroups""]) == set(apiGroups): if set(rule[""resources""]) == set(resources): <mask>: return True return False",True,"if set ( rule [ ""verbs"" ] ) == set ( verbs ) :","if set ( rule [ ""verbs"" ] ) == set ( verbs ) :",0.8820016898747209,100.00000000000004
"def _maybe_commit_artifact(self, artifact_id): artifact_status = self._artifacts[artifact_id] if artifact_status[""pending_count""] == 0 and artifact_status[""commit_requested""]: for callback in artifact_status[""pre_commit_callbacks""]: callback() <mask>: self._api.commit_artifact(artifact_id) for callback in artifact_status[""post_commit_callbacks""]: callback()",False,"if artifact_status [ ""finalize"" ] :","if artifact_status [ ""commit_requested"" ] :",0.8820016898747209,53.107253497886994
"def shuffler(iterator, pool_size=10 ** 5, refill_threshold=0.9): yields_between_refills = round(pool_size * (1 - refill_threshold)) # initialize pool; this step may or may not exhaust the iterator. pool = take_n(pool_size, iterator) while True: random.shuffle(pool) for i in range(yields_between_refills): yield pool.pop() next_batch = take_n(yields_between_refills, iterator) <mask>: break pool.extend(next_batch) # finish consuming whatever's left - no need for further randomization. yield from pool",False,if not next_batch :,if next_batch is None :,0.8820016898747209,27.77619034011791
"def __getitem__(self, key, _get_mode=False): if not _get_mode: if isinstance(key, (int, long)): return self._list[key] <mask>: return self.__class__(self._list[key]) ikey = key.lower() for k, v in self._list: if k.lower() == ikey: return v # micro optimization: if we are in get mode we will catch that # exception one stack level down so we can raise a standard # key error instead of our special one. if _get_mode: raise KeyError() raise BadRequestKeyError(key)",False,"elif isinstance ( key , slice ) :","elif isinstance ( key , str ) :",0.8820016898747209,59.4603557501361
"def find(self, path): if os.path.isfile(path) or os.path.islink(path): self.num_files = self.num_files + 1 if self.match_function(path): self.files.append(path) elif os.path.isdir(path): for content in os.listdir(path): file = os.path.join(path, content) if os.path.isfile(file) or os.path.islink(file): self.num_files = self.num_files + 1 <mask>: self.files.append(file) else: self.find(file)",True,if self . match_function ( file ) :,if self . match_function ( file ) :,0.8820016898747209,100.00000000000004
"def validate_nb(self, nb): super(MetadataValidatorV3, self).validate_nb(nb) ids = set([]) for cell in nb.cells: <mask>: continue grade = cell.metadata[""nbgrader""][""grade""] solution = cell.metadata[""nbgrader""][""solution""] locked = cell.metadata[""nbgrader""][""locked""] if not grade and not solution and not locked: continue grade_id = cell.metadata[""nbgrader""][""grade_id""] if grade_id in ids: raise ValidationError(""Duplicate grade id: {}"".format(grade_id)) ids.add(grade_id)",False,"if ""nbgrader"" not in cell . metadata :","if not cell . metadata [ ""nbgrader"" ] :",0.8820016898747209,25.965358893403383
"def _skip_start(self): start, stop = self.start, self.stop for chunk in self.app_iter: self._pos += len(chunk) if self._pos < start: continue elif self._pos == start: return b"""" else: chunk = chunk[start - self._pos :] <mask>: chunk = chunk[: stop - self._pos] assert len(chunk) == stop - start return chunk else: raise StopIteration()",False,if stop is not None and self . _pos > stop :,if len ( chunk ) > stop :,0.8820016898747209,11.823706105869768
"def _SetUser(self, users): for user in users.items(): username = user[0] settings = user[1] room = settings[""room""][""name""] if ""room"" in settings else None file_ = settings[""file""] if ""file"" in settings else None if ""event"" in settings: if ""joined"" in settings[""event""]: self._client.userlist.addUser(username, room, file_) <mask>: self._client.removeUser(username) else: self._client.userlist.modUser(username, room, file_)",False,"elif ""left"" in settings [ ""event"" ] :","elif ""deleted"" in settings [ ""event"" ] :",0.8820016898747209,76.91605673134588
"def run_tests(): # type: () -> None x = 5 with switch(x) as case: if case(0): print(""zero"") print(""zero"") <mask>: print(""one or two"") elif case(3, 4): print(""three or four"") else: print(""default"") print(""another"")",True,"elif case ( 1 , 2 ) :","elif case ( 1 , 2 ) :",0.8820016898747209,100.00000000000004
"def _populate(): for fname in glob.glob(os.path.join(os.path.dirname(__file__), ""data"", ""*.json"")): with open(fname) as inf: data = json.load(inf) data = data[list(data.keys())[0]] data = data[list(data.keys())[0]] for item in data: <mask>: LOGGER.warning(""Repeated emoji {}"".format(item[""key""])) else: TABLE[item[""key""]] = item[""value""]",True,"if item [ ""key"" ] in TABLE :","if item [ ""key"" ] in TABLE :",0.8820016898747209,100.00000000000004
"def slot_to_material(bobject: bpy.types.Object, slot: bpy.types.MaterialSlot): mat = slot.material # Pick up backed material if present if mat is not None: baked_mat = mat.name + ""_"" + bobject.name + ""_baked"" <mask>: mat = bpy.data.materials[baked_mat] return mat",True,if baked_mat in bpy . data . materials :,if baked_mat in bpy . data . materials :,0.8820016898747209,100.00000000000004
"def __keyPress(self, widget, event): if event.key == ""G"" and event.modifiers & event.Modifiers.Control: if not all(hasattr(p, ""isGanged"") for p in self.getPlugs()): return False <mask>: self.__ungang() else: self.__gang() return True return False",False,if all ( p . isGanged ( ) for p in self . getPlugs ( ) ) :,elif event . modifiers & event . Modifier . Control :,0.8820016898747209,2.6102662555426654
"def check_expected(result, expected, contains=False): if sys.version_info[0] >= 3: if isinstance(result, str): result = result.encode(""ascii"") <mask>: expected = expected.encode(""ascii"") resultlines = result.splitlines() expectedlines = expected.splitlines() if len(resultlines) != len(expectedlines): return False for rline, eline in zip(resultlines, expectedlines): if contains: if eline not in rline: return False else: if not rline.endswith(eline): return False return True",False,"if isinstance ( expected , str ) :","elif isinstance ( expected , str ) :",0.8820016898747209,84.08964152537145
"def hosts_to_domains(self, hosts, exclusions=[]): domains = [] for host in hosts: elements = host.split(""."") # recursively walk through the elements # extracting all possible (sub)domains while len(elements) >= 2: # account for domains stored as hosts if len(elements) == 2: domain = ""."".join(elements) else: # drop the host element domain = ""."".join(elements[1:]) <mask>: domains.append(domain) del elements[0] return domains",False,if domain not in domains + exclusions :,if domain not in exclusions :,0.8820016898747209,43.29820146406896
"def hsconn_sender(self): while not self.stop_event.is_set(): try: # Block, but timeout, so that we can exit the loop gracefully request = self.send_queue.get(True, 6.0) <mask>: # Socket got closed and set to None in another thread... self.socket.sendall(request) if self.send_queue is not None: self.send_queue.task_done() except queue.Empty: pass except OSError: self.stop_event.set()",False,if self . socket is not None :,if request is not None :,0.8820016898747209,38.49815007763549
"def get_url_args(self, item): if self.url_args: <mask>: url_args = self.url_args(item) else: url_args = dict(self.url_args) url_args[""id""] = item.id return url_args else: return dict(operation=self.label, id=item.id)",False,"if hasattr ( self . url_args , ""__call__"" ) :","if isinstance ( item , dict ) :",0.8820016898747209,3.979005885472728
"def list_projects(self): projects = [] page = 1 while True: repos = self._client.get( ""/user/repos"", {""sort"": ""full_name"", ""page"": page, ""per_page"": 100} ) page += 1 for repo in repos: projects.append( { ""id"": repo[""full_name""], ""name"": repo[""full_name""], ""description"": repo[""description""], ""is_private"": repo[""private""], } ) <mask>: break return projects",False,if len ( repos ) < 100 :,if not repos :,0.8820016898747209,7.733712583165139
"def scripts(self): application_root = current_app.config.get(""APPLICATION_ROOT"") subdir = application_root != ""/"" scripts = [] for script in get_registered_scripts(): if script.startswith(""http""): scripts.append(f'<script defer src=""{script}""></script>') <mask>: scripts.append(f'<script defer src=""{application_root}/{script}""></script>') else: scripts.append(f'<script defer src=""{script}""></script>') return markup(""\n"".join(scripts))",False,elif subdir :,elif script . startswith ( subdir ) :,0.8820016898747209,7.267884212102741
"def print_map(node, l): if node.title not in l: l[node.title] = [] for n in node.children: <mask>: w = {n.title: []} l[node.title].append(w) print_map(n, w) else: l[node.title].append(n.title)",False,if len ( n . children ) > 0 :,"if isinstance ( n , Node ) :",0.8820016898747209,10.816059393812111
"def _validate_distinct_on_different_types_and_field_orders( self, collection, query, expected_results, get_mock_result ): self.count = 0 self.get_mock_result = get_mock_result query_iterable = collection.query_items(query, enable_cross_partition_query=True) results = list(query_iterable) for i in range(len(expected_results)): if isinstance(results[i], dict): self.assertDictEqual(results[i], expected_results[i]) <mask>: self.assertListEqual(results[i], expected_results[i]) else: self.assertEqual(results[i], expected_results[i]) self.count = 0",True,"elif isinstance ( results [ i ] , list ) :","elif isinstance ( results [ i ] , list ) :",0.8820016898747209,100.00000000000004
"def run(self): for k, v in iteritems(self.objs): <mask>: continue if ( v[""_class""] == ""Question"" or v[""_class""] == ""Message"" or v[""_class""] == ""Announcement"" ): v[""admin""] = None return self.objs",False,"if k . startswith ( ""_"" ) :","if k != ""admin"" :",0.8820016898747209,10.816059393812111
"def qvec(self): # if self.polrep != 'stokes': # raise Exception(""qvec is not defined unless self.polrep=='stokes'"") qvec = np.array([]) if self.polrep == ""stokes"": qvec = self._imdict[""Q""] elif self.polrep == ""circ"": <mask>: qvec = np.real(0.5 * (self.lrvec + self.rlvec)) return qvec",False,if len ( self . rlvec ) != 0 and len ( self . lrvec ) != 0 :,if self . lrvec != 0 :,0.8820016898747209,10.179804978130223
"def display_value(self, key, w): if key == ""vdevices"": # Very special case nids = [n[""deviceID""] for n in self.get_value(""devices"")] for device in self.app.devices.values(): <mask>: b = Gtk.CheckButton(device.get_title(), False) b.set_tooltip_text(device[""id""]) self[""vdevices""].pack_start(b, False, False, 0) b.set_active(device[""id""] in nids) self[""vdevices""].show_all() else: EditorDialog.display_value(self, key, w)",False,"if device [ ""id"" ] != self . app . daemon . get_my_id ( ) :","if device [ ""id"" ] in nids :",0.8820016898747209,18.004345885485485
"def _set_xflux_setting(self, **kwargs): for key, value in kwargs.items(): if key in self._settings_map: <mask>: self._set_xflux_screen_color(value) self._current_color = str(value) # hackish - changing the current color unpauses xflux, # must reflect that with state change if self.state == self.states[""PAUSED""]: self.state = self.states[""RUNNING""] else: self._xflux.sendline(self._settings_map[key] + str(value)) self._c()",False,"if key == ""color"" :","if self . _settings_map [ key ] == ""screen_color"" :",0.8820016898747209,14.44788670919441
"def apply_acceleration(self, veh_ids, acc): """"""See parent class."""""" # to hand the case of a single vehicle if type(veh_ids) == str: veh_ids = [veh_ids] acc = [acc] for i, vid in enumerate(veh_ids): <mask>: this_vel = self.get_speed(vid) next_vel = max([this_vel + acc[i] * self.sim_step, 0]) self.kernel_api.vehicle.slowDown(vid, next_vel, 1e-3)",False,if acc [ i ] is not None and vid in self . get_ids ( ) :,if self . kernel_api . vehicle . is_enabled ( vid ) :,0.8820016898747209,7.023357200693732
"def largest_factor_relatively_prime(a, b): """"""Return the largest factor of a relatively prime to b."""""" while 1: d = gcd(a, b) <mask>: break b = d while 1: q, r = divmod(a, d) if r > 0: break a = q return a",False,if d <= 1 :,if d < 0 :,0.8820016898747209,34.98330125272253
"def check_status(self): try: du = psutil.disk_usage(""/"") <mask>: raise ServiceWarning( ""{host} {percent}% disk usage exceeds {disk_usage}%"".format( host=host, percent=du.percent, disk_usage=DISK_USAGE_MAX ) ) except ValueError as e: self.add_error(ServiceReturnedUnexpectedResult(""ValueError""), e)",False,if DISK_USAGE_MAX and du . percent >= DISK_USAGE_MAX :,if du . percent > DISK_USAGE_MAX :,0.8820016898747209,38.02970594133841
"def build_reply(self, msg, text=None, private=False, threaded=False): response = self.build_message(text) if msg.is_group: <mask>: response.frm = self.bot_identifier response.to = IRCPerson(str(msg.frm)) else: response.frm = IRCRoomOccupant(str(self.bot_identifier), msg.frm.room) response.to = msg.frm.room else: response.frm = self.bot_identifier response.to = msg.frm return response",True,if private :,if private :,0.8820016898747209,0.0
"def _dict_refs(obj, named): """"""Return key and value objects of a dict/proxy."""""" try: <mask>: for k, v in _items(obj): s = str(k) yield _NamedRef(""[K] "" + s, k) yield _NamedRef(""[V] "" + s + "": "" + _repr(v), v) else: for k, v in _items(obj): yield k yield v except (KeyError, ReferenceError, TypeError) as x: warnings.warn(""Iterating '%s': %r"" % (_classof(obj), x))",True,if named :,if named :,0.8820016898747209,0.0
"def fetch_images(): images = [] marker = None while True: batch = image_service.detail( context, filters=filters, marker=marker, sort_key=""created_at"", sort_dir=""desc"", ) <mask>: break images += batch marker = batch[-1][""id""] return images",False,if not batch :,if len ( batch ) == 0 :,0.8820016898747209,6.27465531099474
"def compress(self, data_list): warn_untested() if data_list: if data_list[1] in forms.fields.EMPTY_VALUES: error = self.error_messages[""invalid_year""] raise forms.ValidationError(error) <mask>: error = self.error_messages[""invalid_month""] raise forms.ValidationError(error) year = int(data_list[1]) month = int(data_list[0]) # find last day of the month day = monthrange(year, month)[1] return date(year, month, day) return None",True,if data_list [ 0 ] in forms . fields . EMPTY_VALUES :,if data_list [ 0 ] in forms . fields . EMPTY_VALUES :,0.8820016898747209,100.00000000000004
"def _diff_dict(self, old, new): diff = {} removed = [] added = [] for key, value in old.items(): if key not in new: removed.append(key) <mask>: # modified is indicated by a remove and add removed.append(key) added.append(key) for key, value in new.items(): if key not in old: added.append(key) if removed: diff[""removed""] = sorted(removed) if added: diff[""added""] = sorted(added) return diff",False,elif old [ key ] != new [ key ] :,if value is not None :,0.8820016898747209,2.9859662827819125
"def add_filters(self, function): try: subscription = self.exists(function) <mask>: response = self._sns.call( ""set_subscription_attributes"", SubscriptionArn=subscription[""SubscriptionArn""], AttributeName=""FilterPolicy"", AttributeValue=json.dumps(self.filters), ) kappa.event_source.sns.LOG.debug(response) except Exception: kappa.event_source.sns.LOG.exception( ""Unable to add filters for SNS topic %s"", self.arn )",True,if subscription :,if subscription :,0.8820016898747209,0.0
"def init_weights(self, pretrained=None): if isinstance(pretrained, str): logger = logging.getLogger() load_checkpoint(self, pretrained, strict=False, logger=logger) elif pretrained is None: for m in self.modules(): if isinstance(m, nn.Conv2d): kaiming_init(m) <mask>: constant_init(m, 1) else: raise TypeError(""pretrained must be a str or None"")",False,"elif isinstance ( m , ( _BatchNorm , nn . GroupNorm ) ) :","elif isinstance ( m , nn . Conv2d ) :",0.8820016898747209,34.1077254951379
def test_is_native_login(self): for campaign in self.campaign_lists: native = campaigns.is_native_login(campaign) <mask>: assert_true(native) else: assert_false(native) native = campaigns.is_proxy_login(self.invalid_campaign) assert_true(native is None),False,"if campaign == ""prereg"" or campaign == ""erpc"" :",if self . invalid_campaign :,0.8820016898747209,2.75631563063758
"def _process_filter(self, query, host_state): """"""Recursively parse the query structure."""""" if not query: return True cmd = query[0] method = self.commands[cmd] cooked_args = [] for arg in query[1:]: if isinstance(arg, list): arg = self._process_filter(arg, host_state) <mask>: arg = self._parse_string(arg, host_state) if arg is not None: cooked_args.append(arg) result = method(self, cooked_args) return result",False,"elif isinstance ( arg , basestring ) :","elif isinstance ( arg , str ) :",0.8820016898747209,59.4603557501361
"def find_go_files_mtime(app_files): files, mtime = [], 0 for f, mt in app_files.items(): <mask>: continue if APP_CONFIG.nobuild_files.match(f): continue files.append(f) mtime = max(mtime, mt) return files, mtime",False,"if not f . endswith ( "".go"" ) :","if f . startswith ( ""go"" ) :",0.8820016898747209,29.929130289926437
"def ExcludePath(self, path): """"""Check to see if this is a service url and matches inbound_services."""""" skip = False for reserved_path in self.reserved_paths.keys(): <mask>: if ( not self.inbound_services or self.reserved_paths[reserved_path] not in self.inbound_services ): return (True, self.reserved_paths[reserved_path]) return (False, None)",False,if path . startswith ( reserved_path ) :,if reserved_path . startswith ( path ) and skip :,0.8820016898747209,35.08439695638686
"def param_cov(self) -> DataFrame: """"""Parameter covariance"""""" if self._param_cov is not None: param_cov = self._param_cov else: params = np.asarray(self.params) <mask>: param_cov = self.model.compute_param_cov(params) else: param_cov = self.model.compute_param_cov(params, robust=False) return DataFrame(param_cov, columns=self._names, index=self._names)",False,"if self . cov_type == ""robust"" :",if self . model . is_train :,0.8820016898747209,14.448814886766836
"def test_calculate_all_attentions(module, atype): m = importlib.import_module(module) args = make_arg(atype=atype) <mask>: batch = prepare_inputs(""pytorch"") else: raise NotImplementedError model = m.E2E(6, 5, args) with chainer.no_backprop_mode(): if ""pytorch"" in module: att_ws = model.calculate_all_attentions(*batch)[0] else: raise NotImplementedError print(att_ws.shape)",True,"if ""pytorch"" in module :","if ""pytorch"" in module :",0.8820016898747209,100.00000000000004
"def __eq__(self, other): try: if self.type != other.type: return False <mask>: return self.askAnswer == other.askAnswer elif self.type == ""SELECT"": return self.vars == other.vars and self.bindings == other.bindings else: return self.graph == other.graph except: return False",False,"if self . type == ""ASK"" :","elif self . type == ""ASK"" :",0.8820016898747209,88.01117367933934
"def validate_memory(self, value): for k, v in value.viewitems(): if v is None: # use NoneType to unset a value continue if not re.match(PROCTYPE_MATCH, k): raise serializers.ValidationError(""Process types can only contain [a-z]"") <mask>: raise serializers.ValidationError( ""Limit format: <number><unit>, where unit = B, K, M or G"" ) return value",False,"if not re . match ( MEMLIMIT_MATCH , str ( v ) ) :",if len ( v ) > MAX_MEMORY_LIMIT :,0.8820016898747209,10.74159750747916
"def get_connections(data_about): data = data_about.find(""h3"", text=""Connections"").findNext() connections = {} for row in data.find_all(""tr""): key = row.find_all(""td"")[0].text value = row.find_all(""td"")[1] <mask>: connections[key] = get_all_links(value) else: connections[key] = value.text return connections",False,"if ""Teams"" in key :","if key == ""links"" :",0.8820016898747209,8.25791079503452
"def _compute_map(self, first_byte, second_byte=None): if first_byte != 0x0F: return ""XED_ILD_MAP0"" else: if second_byte == None: return ""XED_ILD_MAP1"" if second_byte == 0x38: return ""XED_ILD_MAP2"" if second_byte == 0x3A: return ""XED_ILD_MAP3"" <mask>: return ""XED_ILD_MAPAMD"" die(""Unhandled escape {} / map {} bytes"".format(first_byte, second_byte))",False,if second_byte == 0x0F and self . amd_enabled :,if second_byte == 0x5B :,0.8820016898747209,33.401359264888455
"def compress(self, data_list): if data_list: page_id = data_list[1] <mask>: if not self.required: return None raise forms.ValidationError(self.error_messages[""invalid_page""]) return Page.objects.get(pk=page_id) return None",False,if page_id in EMPTY_VALUES :,if page_id is None :,0.8820016898747209,32.66828640925501
"def find_module(self, fullname, path=None): path = path or self.path_entry # print('looking for ""%s"" in %s ...' % (fullname, path)) for _ext in [""js"", ""pyj"", ""py""]: _filepath = os.path.join(self.path_entry, ""%s.%s"" % (fullname, _ext)) <mask>: print(""module found at %s:%s"" % (_filepath, fullname)) return VFSModuleLoader(_filepath, fullname) print(""module %s not found"" % fullname) raise ImportError() return None",False,if _filepath in VFS :,if os . path . exists ( _filepath ) :,0.8820016898747209,8.913765521398126
"def __decToBin(self, myDec): n = 0 binOfDec = """" while myDec > 2 ** n: n = n + 1 if (myDec < 2 ** n) & (myDec != 0): n = n - 1 while n >= 0: <mask>: myDec = myDec - 2 ** n binOfDec = binOfDec + ""1"" else: binOfDec = binOfDec + ""0"" n = n - 1 return binOfDec",False,if myDec >= 2 ** n :,if myDec > 2 ** n :,0.8820016898747209,61.01950432112583
"def __str__(self): try: <mask>: NVMLError._errcode_to_string[self.value] = str(nvmlErrorString(self.value)) return NVMLError._errcode_to_string[self.value] except NVMLError_Uninitialized: return ""NVML Error with code %d"" % self.value",True,if self . value not in NVMLError . _errcode_to_string :,if self . value not in NVMLError . _errcode_to_string :,0.8820016898747209,100.00000000000004
"def abspath(pathdir: str) -> str: if Path is not None and isinstance(pathdir, Path): return pathdir.abspath() else: pathdir = path.abspath(pathdir) <mask>: try: pathdir = pathdir.decode(fs_encoding) except UnicodeDecodeError as exc: raise UnicodeDecodeError( ""multibyte filename not supported on "" ""this filesystem encoding "" ""(%r)"" % fs_encoding ) from exc return pathdir",False,"if isinstance ( pathdir , bytes ) :",if fs_encoding is not None :,0.8820016898747209,6.567274736060395
"def _get_vtkjs(self): if self._vtkjs is None and self.object is not None: <mask>: if isfile(self.object): with open(self.object, ""rb"") as f: vtkjs = f.read() else: data_url = urlopen(self.object) vtkjs = data_url.read() elif hasattr(self.object, ""read""): vtkjs = self.object.read() self._vtkjs = vtkjs return self._vtkjs",False,"if isinstance ( self . object , string_types ) and self . object . endswith ( "".vtkjs"" ) :","if hasattr ( self . object , ""read"" ) :",0.8820016898747209,16.493457938929108
"def _set_uid(self, val): if val is not None: if pwd is None: self.bus.log(""pwd module not available; ignoring uid."", level=30) val = None <mask>: val = pwd.getpwnam(val)[2] self._uid = val",False,"elif isinstance ( val , text_or_bytes ) :","elif isinstance ( val , str ) :",0.8820016898747209,36.06452879987793
"def get_attached_nodes(self, external_account): for node in self.get_nodes_with_oauth_grants(external_account): <mask>: continue node_settings = node.get_addon(self.oauth_provider.short_name) if node_settings is None: continue if node_settings.external_account == external_account: yield node",True,if node is None :,if node is None :,0.8820016898747209,100.00000000000004
"def from_obj(cls, py_obj): if not isinstance(py_obj, Image): raise TypeError(""py_obj must be a wandb.Image"") else: if hasattr(py_obj, ""_boxes"") and py_obj._boxes: box_keys = list(py_obj._boxes.keys()) else: box_keys = [] <mask>: mask_keys = list(py_obj.masks.keys()) else: mask_keys = [] return cls(box_keys, mask_keys)",False,"if hasattr ( py_obj , ""masks"" ) and py_obj . masks :","if hasattr ( py_obj , ""_masks"" ) and py_obj . masks :",0.8820016898747209,84.92326635760686
"def write(self, *bits): for bit in bits: if not self.bytestream: self.bytestream.append(0) byte = self.bytestream[self.bytenum] if self.bitnum == 8: <mask>: byte = 0 self.bytestream += bytes([byte]) self.bytenum += 1 self.bitnum = 0 mask = 2 ** self.bitnum if bit: byte |= mask else: byte &= ~mask self.bytestream[self.bytenum] = byte self.bitnum += 1",False,if self . bytenum == len ( self . bytestream ) - 1 :,"if byte == b"""" :",0.8820016898747209,5.4752948205155585
"def destroy(self, wipe=False): if self.state == self.UP: image = self.image() <mask>: return self.confirm_destroy(image, self.full_name, abort=False) else: self.warn(""tried to destroy {0} which didn't exist"".format(self.full_name)) return True",True,if image :,if image :,0.8820016898747209,0.0
"def get_host_metadata(self): meta = {} if self.agent_url: try: resp = requests.get( self.agent_url + ECS_AGENT_METADATA_PATH, timeout=1 ).json() <mask>: match = AGENT_VERSION_EXP.search(resp.get(""Version"")) if match is not None and len(match.groups()) == 1: meta[""ecs_version""] = match.group(1) except Exception as e: self.log.debug(""Error getting ECS version: %s"" % str(e)) return meta",False,"if ""Version"" in resp :",if resp . status_code == 200 :,0.8820016898747209,5.522397783539471
"def _path_type(st, lst): parts = [] if st: <mask>: parts.append(""file"") elif stat.S_ISDIR(st.st_mode): parts.append(""dir"") else: parts.append(""other"") if lst: if stat.S_ISLNK(lst.st_mode): parts.append(""link"") return "" "".join(parts)",False,if stat . S_ISREG ( st . st_mode ) :,if stat . S_ISFILE ( st . st_mode ) :,0.8820016898747209,78.25422900366432
"def changed(self, action): # Something was changed in the 'files' list if len(action.key) >= 1 and action.key[0].lower() == ""files"": # Refresh project files model <mask>: # Don't clear the existing items if only inserting new things self.update_model(clear=False) else: # Clear existing items self.update_model(clear=True)",False,"if action . type == ""insert"" :","if action . key [ 0 ] . lower ( ) == ""project"" :",0.8820016898747209,16.267392600305733
"def process(self, resources, event=None): client = local_session(self.manager.session_factory).client(""es"") for r in resources: <mask>: result = self.manager.retry( client.describe_elasticsearch_domain_config, DomainName=r[""DomainName""], ignore_err_codes=(""ResourceNotFoundException"",), ) if result: r[self.policy_attribute] = json.loads( result.get(""DomainConfig"").get(""AccessPolicies"").get(""Options"") ) return super().process(resources)",False,if self . policy_attribute not in r :,"if r [ ""DomainName"" ] :",0.8820016898747209,5.660233915657916
"def line_items(self): line_items = [] for line in self.lines_str: line = line.split(""|"") line = line[1:-1] # del first and last empty item (consequence of split) items = [] for item in line: i = re.search(r""(\S+([ \t]+\S+)*)+"", item) <mask>: items.append(i.group()) else: items.append("" "") line_items.append(items) return line_items",True,if i :,if i :,0.8820016898747209,0.0
"def on_data(res): if terminate.is_set(): return if args.strings and not args.no_content: if type(res) == tuple: f, v = res if type(f) == unicode: f = f.encode(""utf-8"") if type(v) == unicode: v = v.encode(""utf-8"") self.success(""{}: {}"".format(f, v)) <mask>: self.success(res) else: self.success(res)",False,elif not args . content_only :,elif args . strings :,0.8820016898747209,13.943458243384402
"def get_servers(self, detail=True, search_opts=None): rel_url = ""/servers/detail"" if detail else ""/servers"" if search_opts is not None: qparams = {} for opt, val in search_opts.iteritems(): qparams[opt] = val <mask>: query_string = ""?%s"" % urllib.urlencode(qparams) rel_url += query_string return self.api_get(rel_url)[""servers""]",True,if qparams :,if qparams :,0.8820016898747209,0.0
"def run(self): while not self.__exit__: <mask>: sleep(10) continue o = self.playlist[0] self.playlist.remove(o) obj = json.loads(o) if not ""args"" in obj: obj[""args""] = {""ua"": """", ""header"": """", ""title"": """", ""referer"": """"} obj[""play""] = False self.handle = launch_player(obj[""urls""], obj[""ext""], **obj[""args""]) self.handle.wait()",False,if len ( self . playlist ) == 0 :,if self . playlist is None :,0.8820016898747209,15.685718045401451
"def get_to_download_runs_ids(session, headers): last_date = 0 result = [] while 1: r = session.get(RUN_DATA_API.format(last_date=last_date), headers=headers) <mask>: run_logs = r.json()[""data""][""records""] result.extend([i[""logs""][0][""stats""][""id""] for i in run_logs]) last_date = r.json()[""data""][""lastTimestamp""] since_time = datetime.utcfromtimestamp(last_date / 1000) print(f""pares keep ids data since {since_time}"") time.sleep(1) # spider rule if not last_date: break return result",False,if r . ok :,if r . status_code == 200 :,0.8820016898747209,16.784459625186194
"def __saveWork(self, work, results): """"""Stores the resulting last log line to the cache with the proxy key"""""" del work # pylint: disable=broad-except try: <mask>: __cached = self.__cache[results[0]] __cached[self.__TIME] = time.time() __cached[self.__LINE] = results[1] __cached[self.__LLU] = results[2] except KeyError as e: # Could happen while switching jobs with work in the queue pass except Exception as e: list(map(logger.warning, cuegui.Utils.exceptionOutput(e)))",True,if results :,if results :,0.8820016898747209,0.0
"def read_notes(rec): found = [] for tag in range(500, 595): if tag in (505, 520): continue fields = rec.get_fields(str(tag)) <mask>: continue for f in fields: x = f.get_lower_subfields() if x: found.append("" "".join(x).strip("" "")) if found: return ""\n\n"".join(found)",True,if not fields :,if not fields :,0.8820016898747209,100.00000000000004
"def serialize_to(self, stream, alternate_script=None): stream.write(self.txo_ref.tx_ref.hash) stream.write_uint32(self.txo_ref.position) if alternate_script is not None: stream.write_string(alternate_script) else: <mask>: stream.write_string(self.coinbase) else: stream.write_string(self.script.source) stream.write_uint32(self.sequence)",False,if self . is_coinbase :,if self . coinbase is not None :,0.8820016898747209,24.446151121745064
"def func_named(self, arg): result = None target = ""do_"" + arg if target in dir(self): result = target else: <mask>: # accept shortened versions of commands funcs = [fname for fname in self.keywords if fname.startswith(arg)] if len(funcs) == 1: result = ""do_"" + funcs[0] return result",False,if self . abbrev :,if arg in self . keywords :,0.8820016898747209,15.619699684601283
"def static_login(self, token, *, bot): # Necessary to get aiohttp to stop complaining about session creation self.__session = aiohttp.ClientSession( connector=self.connector, ws_response_class=DiscordClientWebSocketResponse ) old_token, old_bot = self.token, self.bot_token self._token(token, bot=bot) try: data = await self.request(Route(""GET"", ""/users/@me"")) except HTTPException as exc: self._token(old_token, bot=old_bot) <mask>: raise LoginFailure(""Improper token has been passed."") from exc raise return data",False,if exc . response . status == 401 :,if exc . status != 401 :,0.8820016898747209,29.588031349552935
"def render_buttons(self): for x, button in enumerate(self.button_list): gcolor = Gdk.color_parse(self.color_list[x]) <mask>: fgcolor = Gdk.color_parse(""#FFFFFF"") else: fgcolor = Gdk.color_parse(""#000000"") button.set_label(self.color_list[x]) button.set_sensitive(True) button.modify_bg(Gtk.StateType.NORMAL, gcolor) button.modify_fg(Gtk.StateType.NORMAL, fgcolor)",False,"if util . get_hls_val ( self . color_list [ x ] , ""light"" ) < 99 :",if x == 0 :,0.8820016898747209,0.45018791827775173
"def _set_text(self, data): lines = [] for key, value in data.items(): lines.append("""") txt = yaml.dump({key: value}, default_flow_style=False) title = self.titles.get(key) <mask>: lines.append(""# %s"" % title) lines.append(txt.rstrip()) txt = ""\n"".join(lines) + ""\n"" txt = txt.lstrip() self.edit.setPlainText(txt)",True,if title :,if title :,0.8820016898747209,0.0
"def build_path(self): for variable in re_path_template.findall(self.path): name = variable.strip(""{}"") <mask>: # No 'user' parameter provided, fetch it from Auth instead. value = self.api.auth.get_username() else: try: value = quote(self.session.params[name]) except KeyError: raise TweepError( ""No parameter value found for path variable: %s"" % name ) del self.session.params[name] self.path = self.path.replace(variable, value)",False,"if name == ""user"" and ""user"" not in self . session . params and self . api . auth :","if name == ""user"" :",0.8820016898747209,10.384000820777368
"def _calculate_writes_for_built_in_indices(self, entity): writes = 0 for prop_name in entity.keys(): <mask>: prop_vals = entity[prop_name] if isinstance(prop_vals, (list)): num_prop_vals = len(prop_vals) else: num_prop_vals = 1 writes += 2 * num_prop_vals return writes",False,if not prop_name in entity . unindexed_properties ( ) :,if prop_name in self . built_in_indices :,0.8820016898747209,21.29480760387301
"def create_connection(self, address, protocol_factory=None, **kw): """"""Helper method for creating a connection to an ``address``."""""" protocol_factory = protocol_factory or self.create_protocol if isinstance(address, tuple): host, port = address <mask>: self.logger.debug(""Create connection %s:%s"", host, port) _, protocol = await self._loop.create_connection( protocol_factory, host, port, **kw ) await protocol.event(""connection_made"") else: raise NotImplementedError(""Could not connect to %s"" % str(address)) return protocol",False,if self . debug :,if host and port :,0.8820016898747209,12.703318703865365
def _increment_bracket_num(self): self._current_bracket -= 1 if self._current_bracket < 0: self._current_bracket = self._get_num_brackets() - 1 self._current_iteration += 1 <mask>: self._current_bracket = 0,False,if self . _current_iteration > self . hyperband_iterations :,if self . _current_bracket == self . _get_num_brackets ( ) - 1 :,0.8820016898747209,25.306188056493337
"def get_cycle_path(self, curr_node, goal_node_index): for dep in curr_node[""deps""]: <mask>: return [curr_node[""address""]] for dep in curr_node[""deps""]: path = self.get_cycle_path( self.get_by_address(dep), goal_node_index ) # self.nodelist[dep], goal_node_index) if len(path) > 0: path.insert(0, curr_node[""address""]) return path return []",False,if dep == goal_node_index :,if dep in self . nodelist :,0.8820016898747209,9.469167282754096
"def as_dict(path="""", version=""latest"", section=""meta-data""): result = {} dirs = dir(path, version, section) if not dirs: return None for item in dirs: if item.endswith(""/""): records = as_dict(path + item, version, section) <mask>: result[item[:-1]] = records elif is_dict.match(item): idx, name = is_dict.match(item).groups() records = as_dict(path + idx + ""/"", version, section) if records: result[name] = records else: result[item] = valueconv(get(path + item, version, section)) return result",True,if records :,if records :,0.8820016898747209,0.0
"def preprocess_raw_enwik9(input_filename, output_filename): with open(input_filename, ""r"") as f1: with open(output_filename, ""w"") as f2: while True: line = f1.readline() if not line: break line = list(enwik9_norm_transform([line]))[0] if line != "" "" and line != """": <mask>: line = line[1:] f2.writelines(line + ""\n"")",False,"if line [ 0 ] == "" "" :","if line [ 0 ] == ""#"" :",0.8820016898747209,74.19446627365011
"def _handle_unsubscribe(self, web_sock): index = None with await self._subscriber_lock: for i, (subscriber_web_sock, _) in enumerate(self._subscribers): <mask>: index = i break if index is not None: del self._subscribers[index] if not self._subscribers: asyncio.ensure_future(self._unregister_subscriptions())",True,if subscriber_web_sock == web_sock :,if subscriber_web_sock == web_sock :,0.8820016898747209,100.00000000000004
"def formatmonthname(self, theyear, themonth, withyear=True): with TimeEncoding(self.locale) as encoding: s = month_name[themonth] if encoding is not None: s = s.decode(encoding) <mask>: s = ""%s %s"" % (s, theyear) return '<tr><th colspan=""7"" class=""month"">%s</th></tr>' % s",True,if withyear :,if withyear :,0.8820016898747209,0.0
"def generate_sitemaps(filename): rows = (line.strip().split(""\t"") for line in open(filename)) for sortkey, chunk in itertools.groupby(rows, lambda row: row[0]): things = [] _chunk = list(chunk) for segment in _chunk: sortkey = segment.pop(0) last_modified = segment.pop(-1) path = """".join(segment) things.append(web.storage(path=path, last_modified=last_modified)) <mask>: write(""sitemaps/sitemap_%s.xml.gz"" % sortkey, sitemap(things))",False,if things :,if sortkey :,0.8820016898747209,0.0
"def use_index( self, term: Union[str, Index], *terms: Union[str, Index] ) -> ""QueryBuilder"": for t in (term, *terms): <mask>: self._use_indexes.append(t) elif isinstance(t, str): self._use_indexes.append(Index(t))",True,"if isinstance ( t , Index ) :","if isinstance ( t , Index ) :",0.8820016898747209,100.00000000000004
"def get_changed(self): if self._is_expression(): result = self._get_node_text(self.ast) <mask>: return None return result else: collector = codeanalyze.ChangeCollector(self.source) last_end = -1 for match in self.matches: start, end = match.get_region() if start < last_end: if not self._is_expression(): continue last_end = end replacement = self._get_matched_text(match) collector.add_change(start, end, replacement) return collector.get_changed()",False,if result == self . source :,if result is None :,0.8820016898747209,12.975849993980741
"def quiet_f(*args): vars = {arg_name: Real(arg) for arg_name, arg in zip(arg_names, args)} value = dynamic_scoping(quiet_expr.evaluate, vars, evaluation) if expect_list: if value.has_form(""List"", None): value = [extract_pyreal(item) for item in value.leaves] <mask>: return None return value else: return None else: value = extract_pyreal(value) if value is None or isinf(value) or isnan(value): return None return value",False,if any ( item is None for item in value ) :,elif value is None or isinf ( value ) or isnan ( value ) :,0.8820016898747209,12.874330508144842
"def _reemit_nested_event(self, event: Event): source_index = self.index(event.source) for attr in (""index"", ""new_index""): <mask>: src_index = ensure_tuple_index(event.index) setattr(event, attr, (source_index,) + src_index) if not hasattr(event, ""index""): setattr(event, ""index"", source_index) # reemit with this object's EventEmitter of the same type if present # otherwise just emit with the EmitterGroup itself getattr(self.events, event.type, self.events)(event)",False,"if hasattr ( event , attr ) :",if attr in event . attributes :,0.8820016898747209,8.051153633013374
"def check(self): """"""Perform required checks to conclude if it's safe to operate"""""" if self.interpreter.manual is None: <mask>: self.error = self.process.error self.tip = self.process.tip return False start = time.time() while not self._status(): if time.time() - start >= 2: # 2s self.error = ""can't connect to the minserver on {}:{}"".format( self.interpreter.host, self.interpreter.port ) self.tip = ""check your vagrant machine is running"" return False time.sleep(0.1) return True",False,if not self . process . healthy :,if self . process . error :,0.8820016898747209,39.44243648327556
"def apply(self): new_block = self.block.copy() new_block.clear() for inst in self.block.body: <mask>: const_assign = self._assign_const(inst) new_block.append(const_assign) inst = self._assign_getitem(inst, index=const_assign.target) new_block.append(inst) return new_block",False,"if isinstance ( inst , Assign ) and inst . value in self . getattrs :","if isinstance ( inst , ast . Const ) :",0.8820016898747209,24.246049301970572
"def _get_orientation(self): if self.state: rotation = [0] * 9 inclination = [0] * 9 gravity = [] geomagnetic = [] gravity = self.listener_a.values geomagnetic = self.listener_m.values <mask>: ff_state = SensorManager.getRotationMatrix( rotation, inclination, gravity, geomagnetic ) if ff_state: values = [0, 0, 0] values = SensorManager.getOrientation(rotation, values) return values",False,if gravity [ 0 ] is not None and geomagnetic [ 0 ] is not None :,"if self . state == ""rotation"" :",0.8820016898747209,2.4779853471705344
def getFirstSubGraph(graph): if len(graph) == 0: return None subg = {} todo = [graph.keys()[0]] while len(todo) > 0: <mask>: subg[todo[0]] = graph[todo[0]] todo.extend(graph[todo[0]]) del graph[todo[0]] del todo[0] return subg,False,if todo [ 0 ] in graph . keys ( ) :,if todo [ 0 ] in graph :,0.8820016898747209,52.734307450329375
"def decorated_function(*args, **kwargs): rv = f(*args, **kwargs) if ""Last-Modified"" not in rv.headers: try: result = date if callable(result): result = result(rv) <mask>: from werkzeug.http import http_date result = http_date(result) if result: rv.headers[""Last-Modified""] = result except Exception: logging.getLogger(__name__).exception( ""Error while calculating the lastmodified value for response {!r}"".format( rv ) ) return rv",False,"if not isinstance ( result , basestring ) :","if isinstance ( result , datetime ) :",0.8820016898747209,37.70794596593207
"def set_invoice_details(self, row): invoice_details = self.invoice_details.get(row.voucher_no, {}) if row.due_date: invoice_details.pop(""due_date"", None) row.update(invoice_details) if row.voucher_type == ""Sales Invoice"": <mask>: self.set_delivery_notes(row) if self.filters.show_sales_person and row.sales_team: row.sales_person = "", "".join(row.sales_team) del row[""sales_team""]",False,if self . filters . show_delivery_notes :,if row . delivery_notes :,0.8820016898747209,25.694343649393552
"def process(output): modules = {} for line in output: name, size, instances, depends, state, _ = line.split("" "", 5) instances = int(instances) module = { ""size"": size, ""instances"": instances, ""state"": state, } <mask>: module[""depends""] = [value for value in depends.split("","") if value] modules[name] = module return modules",False,"if depends != ""-"" :",if depends :,0.8820016898747209,0.0
"def _get_host_from_zc_service_info(service_info: zeroconf.ServiceInfo): """"""Get hostname or IP + port from zeroconf service_info."""""" host = None port = None if ( service_info and service_info.port and (service_info.server or len(service_info.addresses) > 0) ): <mask>: host = socket.inet_ntoa(service_info.addresses[0]) else: host = service_info.server.lower() port = service_info.port return (host, port)",False,if len ( service_info . addresses ) > 0 :,"if service_info . server == ""localhost"" :",0.8820016898747209,23.462350320527996
"def _init_weights(self, module): if isinstance(module, nn.Linear): module.weight.data.normal_(mean=0.0, std=self.config.init_std) if module.bias is not None: module.bias.data.zero_() elif isinstance(module, nn.Embedding): module.weight.data.normal_(mean=0.0, std=self.config.init_std) <mask>: module.weight.data[module.padding_idx].zero_()",True,if module . padding_idx is not None :,if module . padding_idx is not None :,0.8820016898747209,100.00000000000004
"def visitFromImport(self, import_stmt, import_info): new_pairs = [] if not import_info.is_star_import(): for name, alias in import_info.names_and_aliases: try: pyname = self.pymodule[alias or name] <mask>: continue except exceptions.AttributeNotFoundError: pass new_pairs.append((name, alias)) return importinfo.FromImport(import_info.module_name, import_info.level, new_pairs)",False,"if occurrences . same_pyname ( self . pyname , pyname ) :",if pyname is None :,0.8820016898747209,2.3238598963754593
"def _apply_patches(self): try: s = Subprocess( log=self.logfile, cwd=self.build_dir, verbose=self.options.verbose ) for patch in self.patches: <mask>: for ed, source in patch.items(): s.shell(""ed - %s < %s"" % (source, ed)) else: s.shell(""patch -p0 < %s"" % patch) except: logger.error(""Failed to patch `%s`.\n%s"" % (self.build_dir, sys.exc_info()[1])) sys.exit(1)",False,if type ( patch ) is dict :,"if isinstance ( patch , dict ) :",0.8820016898747209,14.535768424205482
"def __init__(self, parent, dir, mask, with_dirs=True): filelist = [] dirlist = [""..""] self.dir = dir self.file = """" mask = mask.upper() pattern = self.MakeRegex(mask) for i in os.listdir(dir): if i == ""."" or i == "".."": continue path = os.path.join(dir, i) <mask>: dirlist.append(i) continue path = path.upper() value = i.upper() if pattern.match(value) is not None: filelist.append(i) self.files = filelist if with_dirs: self.dirs = dirlist",False,if os . path . isdir ( path ) :,if pattern . match ( path ) is not None :,0.8820016898747209,16.59038701421971
"def remove_invalid_dirs(paths, bp_dir, module_name): ret = [] for path in paths: <mask>: ret.append(path) else: logging.warning('Dir ""%s"" of module ""%s"" does not exist', path, module_name) return ret",False,"if os . path . isdir ( os . path . join ( bp_dir , path ) ) :",if os . path . isdir ( path ) and os . path . isdir ( path ) and os . path . isdir ( path ) :,0.8820016898747209,33.46080538499995
"def update_sockets(self): inputs = self.inputs inputs_n = ""ABabcd"" penta_sockets = pentagon_dict[self.grid_type].input_sockets for socket in inputs_n: if socket in penta_sockets: <mask>: inputs[socket].hide_safe = False else: inputs[socket].hide_safe = True",True,if inputs [ socket ] . hide_safe :,if inputs [ socket ] . hide_safe :,0.8820016898747209,100.00000000000004
"def __cut(sentence): global emit_P prob, pos_list = viterbi(sentence, ""BMES"", start_P, trans_P, emit_P) begin, nexti = 0, 0 # print pos_list, sentence for i, char in enumerate(sentence): pos = pos_list[i] if pos == ""B"": begin = i elif pos == ""E"": yield sentence[begin : i + 1] nexti = i + 1 <mask>: yield char nexti = i + 1 if nexti < len(sentence): yield sentence[nexti:]",False,"elif pos == ""S"" :","elif pos == ""C"" :",0.8820016898747209,59.4603557501361
"def validate(self): if self.data.get(""encrypted"", True): key = self.data.get(""target_key"") <mask>: raise PolicyValidationError( ""Encrypted snapshot copy requires kms key on %s"" % (self.manager.data,) ) return self",False,if not key :,if key is None :,0.8820016898747209,14.058533129758727
"def __init__(self, patch_files, patch_directories): files = [] files_data = {} for filename_data in patch_files: if isinstance(filename_data, list): filename, data = filename_data else: filename = filename_data data = None <mask>: filename = ""{0}{1}"".format(FakeState.deploy_dir, filename) files.append(filename) if data: files_data[filename] = data self.files = files self.files_data = files_data self.directories = patch_directories",False,if not filename . startswith ( os . sep ) :,if filename :,0.8820016898747209,0.0
"def validate_name_and_description(body, check_length=True): for attribute in [""name"", ""description"", ""display_name"", ""display_description""]: value = body.get(attribute) if value is not None: if isinstance(value, six.string_types): body[attribute] = value.strip() <mask>: try: utils.check_string_length( body[attribute], attribute, min_length=0, max_length=255 ) except exception.InvalidInput as error: raise webob.exc.HTTPBadRequest(explanation=error.msg)",False,if check_length :,elif check_length :,0.8820016898747209,66.87403049764218
"def pick(items, sel): for x, s in zip(items, sel): if match(s): yield x <mask>: yield x.restructure(x.head, pick(x.leaves, s.leaves), evaluation)",False,elif not x . is_atom ( ) and not s . is_atom ( ) :,"elif match ( x . head , s . head ) :",0.8820016898747209,7.03457324188939
"def wait_or_kill(self): """"""Wait for the program to terminate, or kill it after 5s."""""" if self.instance.poll() is None: # We try one more time to kill gracefully using Ctrl-C. logger.info(""Interrupting %s and waiting..."", self.coord) self.instance.send_signal(signal.SIGINT) # FIXME on py3 this becomes self.instance.wait(timeout=5) t = monotonic_time() while monotonic_time() - t < 5: <mask>: logger.info(""Terminated %s."", self.coord) break time.sleep(0.1) else: self.kill()",False,if self . instance . poll ( ) is not None :,if self . instance . poll ( ) is None :,0.8820016898747209,77.72460244048297
"def sort_collection(self, models, many): ordering = self.ordering if not many or not ordering: return models for key in reversed(ordering): reverse = key[0] == ""-"" <mask>: key = key[1:] models = sorted(models, key=partial(deep_getattr, key=key), reverse=reverse) return models",True,if reverse :,if reverse :,0.8820016898747209,0.0
"def get_palette_for_custom_classes(self, class_names, palette=None): if self.label_map is not None: # return subset of palette palette = [] for old_id, new_id in sorted(self.label_map.items(), key=lambda x: x[1]): <mask>: palette.append(self.PALETTE[old_id]) palette = type(self.PALETTE)(palette) elif palette is None: if self.PALETTE is None: palette = np.random.randint(0, 255, size=(len(class_names), 3)) else: palette = self.PALETTE return palette",False,if new_id != - 1 :,if old_id in class_names :,0.8820016898747209,11.339582221952005
"def _find_tcl_dir(): lib_dirs = [os.path.dirname(_x) for _x in sys.path if _x.lower().endswith(""lib"")] for lib_dir in lib_dirs: base_dir = os.path.join(lib_dir, TclLibrary.FOLDER) <mask>: for root, _, files in os.walk(base_dir): if TclLibrary.INIT_TCL in files: return root",False,if os . path . exists ( base_dir ) :,if os . path . isdir ( base_dir ) :,0.8820016898747209,73.48889200874659
"def __next__(self): """"""Special paging functionality"""""" if self.iter is None: self.iter = iter(self.objs) try: return next(self.iter) except StopIteration: self.iter = None self.objs = [] <mask>: self.page += 1 self._connection.get_response(self.action, self.params, self.page, self) return next(self) else: raise",False,if int ( self . page ) < int ( self . total_pages ) :,if self . page is not None :,0.8820016898747209,7.582874853312503
"def parse(cls, api, json): lst = List(api) setattr(lst, ""_json"", json) for k, v in json.items(): if k == ""user"": setattr(lst, k, User.parse(api, v)) <mask>: setattr(lst, k, parse_datetime(v)) else: setattr(lst, k, v) return lst",False,"elif k == ""created_at"" :","elif k == ""datetime"" :",0.8820016898747209,46.307771619910305
"def real_type(self): # Find the real type representation by updating it as required real_type = self.type if self.flag_indicator: real_type = ""#"" if self.is_vector: <mask>: real_type = ""Vector<{}>"".format(real_type) else: real_type = ""vector<{}>"".format(real_type) if self.is_generic: real_type = ""!{}"".format(real_type) if self.is_flag: real_type = ""flags.{}?{}"".format(self.flag_index, real_type) return real_type",False,if self . use_vector_id :,if self . is_vector_type :,0.8820016898747209,31.02016197007
"def check_fs(path): with open(path, ""rb"") as f: code = python_bytes_to_unicode(f.read(), errors=""replace"") <mask>: module = _load_module(evaluator, path, code) module_name = sys_path.dotted_path_in_sys_path( evaluator.project.sys_path, path ) if module_name is not None: add_module(evaluator, module_name, module) return module",False,if name in code :,if code is not None :,0.8820016898747209,10.682175159905853
"def infoCalendar(users): calendarId = normalizeCalendarId(sys.argv[5], checkPrimary=True) i = 0 count = len(users) for user in users: i += 1 user, cal = buildCalendarGAPIObject(user) if not cal: continue result = gapi.call( cal.calendarList(), ""get"", soft_errors=True, calendarId=calendarId ) <mask>: print(f""User: {user}, Calendar:{display.current_count(i, count)}"") _showCalendar(result, 1, 1)",True,if result :,if result :,0.8820016898747209,0.0
"def set_hidestate_input_sockets_to_cope_with_switchnum(self): tndict = get_indices_that_should_be_visible(self.node_state) for key, value in tndict.items(): socket = self.inputs[key] desired_hide_state = not (value) <mask>: socket.hide_safe = desired_hide_state",False,if not socket . hide == desired_hide_state :,if socket . hide_safe :,0.8820016898747209,13.653323887370865
"def get_class_name(item): class_name, module_name = None, None for parent in reversed(item.listchain()): if isinstance(parent, pytest.Class): class_name = parent.name <mask>: module_name = parent.module.__name__ break # heuristic: # - better to group gpu and task tests, since tests from those modules # are likely to share caching more # - split up the rest by class name because slow tests tend to be in # the same module if class_name and "".tasks."" not in module_name: return ""{}.{}"".format(module_name, class_name) else: return module_name",True,"elif isinstance ( parent , pytest . Module ) :","elif isinstance ( parent , pytest . Module ) :",0.8820016898747209,100.00000000000004
"def run(self): versions = versioneer.get_versions() tempdir = tempfile.mkdtemp() generated = os.path.join(tempdir, ""rundemo"") with open(generated, ""wb"") as f: for line in open(""src/rundemo-template"", ""rb""): <mask>: f.write((""versions = %r\n"" % (versions,)).encode(""ascii"")) else: f.write(line) self.scripts = [generated] rc = build_scripts.run(self) os.unlink(generated) os.rmdir(tempdir) return rc",False,"if line . strip ( ) . decode ( ""ascii"" ) == ""#versions"" :",if versions :,0.8820016898747209,0.0
"def get_user_context(request, escape=False): if isinstance(request, HttpRequest): user = getattr(request, ""user"", None) result = {""ip_address"": request.META[""REMOTE_ADDR""]} <mask>: result.update( { ""email"": user.email, ""id"": user.id, } ) if user.name: result[""name""] = user.name else: result = {} return mark_safe(json.dumps(result))",False,if user and user . is_authenticated ( ) :,if user :,0.8820016898747209,0.0
"def tokens_to_spans() -> Iterable[Tuple[str, Optional[Style]]]: """"""Convert tokens to spans."""""" tokens = iter(line_tokenize()) line_no = 0 _line_start = line_start - 1 # Skip over tokens until line start while line_no < _line_start: _token_type, token = next(tokens) yield (token, None) <mask>: line_no += 1 # Generate spans until line end for token_type, token in tokens: yield (token, _get_theme_style(token_type)) if token.endswith(""\n""): line_no += 1 if line_no >= line_end: break",False,"if token . endswith ( ""\n"" ) :","if _token_type == ""line"" :",0.8820016898747209,5.604233375480572
"def encode(self, encodeFun, value, defMode, maxChunkSize): substrate, isConstructed = self.encodeValue(encodeFun, value, defMode, maxChunkSize) tagSet = value.getTagSet() if tagSet: <mask>: # primitive form implies definite mode defMode = 1 return ( self.encodeTag(tagSet[-1], isConstructed) + self.encodeLength(len(substrate), defMode) + substrate + self._encodeEndOfOctets(encodeFun, defMode) ) else: return substrate # untagged value",False,if not isConstructed :,"if tagSet [ - 1 ] == ""primitive"" :",0.8820016898747209,4.02724819242185
def _run(self): while True: request = self._requests.get() <mask>: self.shutdown() break self.process(request) self._requests.task_done(),True,if request is None :,if request is None :,0.8820016898747209,100.00000000000004
"def _decode_payload(self, payload): # we need to decrypt it if payload[""enc""] == ""aes"": try: payload[""load""] = self.crypticle.loads(payload[""load""]) except salt.crypt.AuthenticationError: <mask>: raise payload[""load""] = self.crypticle.loads(payload[""load""]) return payload",False,if not self . _update_aes ( ) :,if not self . _verify_auth ( payload ) :,0.8820016898747209,38.827267775222325
"def test_row(self, row): for idx, test in self.patterns.items(): try: value = row[idx] except IndexError: value = """" result = test(value) if self.any_match: <mask>: return not self.inverse # True else: if not result: return self.inverse # False if self.any_match: return self.inverse # False else: return not self.inverse # True",True,if result :,if result :,0.8820016898747209,0.0
"def setup_parameter_node(self, param_node): if param_node.bl_idname == ""SvNumberNode"": if self.use_prop or self.get_prop_name(): value = self.sv_get()[0][0] print(""V"", value) if isinstance(value, int): param_node.selected_mode = ""int"" param_node.int_ = value <mask>: param_node.selected_mode = ""float"" param_node.float_ = value",True,"elif isinstance ( value , float ) :","elif isinstance ( value , float ) :",0.8820016898747209,100.00000000000004
"def iter_modules(self, by_clients=False, clients_filter=None): """"""iterate over all modules"""""" clients = None if by_clients: clients = self.get_clients(clients_filter) if not clients: return self._refresh_modules() for module_name in self.modules: try: module = self.get_module(module_name) except PupyModuleDisabled: continue <mask>: for client in clients: if module.is_compatible_with(client): yield module break else: yield module",False,if clients is not None :,if module :,0.8820016898747209,0.0
"def filter_pricing_rule_based_on_condition(pricing_rules, doc=None): filtered_pricing_rules = [] if doc: for pricing_rule in pricing_rules: if pricing_rule.condition: try: <mask>: filtered_pricing_rules.append(pricing_rule) except: pass else: filtered_pricing_rules.append(pricing_rule) else: filtered_pricing_rules = pricing_rules return filtered_pricing_rules",False,"if frappe . safe_eval ( pricing_rule . condition , None , doc . as_dict ( ) ) :",if pricing_rule . condition . startswith ( doc ) :,0.8820016898747209,15.018615295504263
"def build_query_string(kv_data, ignore_none=True): # {""a"": 1, ""b"": ""test""} -> ""?a=1&b=test"" query_string = """" for k, v in kv_data.iteritems(): <mask>: continue if query_string != """": query_string += ""&"" else: query_string = ""?"" query_string += k + ""="" + str(v) return query_string",False,if ignore_none is True and kv_data [ k ] is None :,"if ignore_none and k == ""a"" :",0.8820016898747209,17.472030740856958
"def sample(self, **config): """"""Sample a configuration from this search space."""""" ret = {} ret.update(self.data) kwspaces = self.kwspaces kwspaces.update(config) striped_keys = [k.split(SPLITTER)[0] for k in config.keys()] for k, v in kwspaces.items(): if k in striped_keys: <mask>: sub_config = _strip_config_space(config, prefix=k) ret[k] = v.sample(**sub_config) else: ret[k] = v return ret",False,"if isinstance ( v , NestedSpace ) :",if k . startswith ( PREFIX ) :,0.8820016898747209,13.134549472120788
"def task_failed(self, task_id, hostname, reason): logger.debug(""task %d failed with message %s"", task_id, str(reason)) if hostname in self.host_dict: host_status = self.host_dict[hostname] host_status.task_failed(task_id) <mask>: self.task_host_failed_dict[task_id] = set() self.task_host_failed_dict[task_id].add(hostname)",True,if task_id not in self . task_host_failed_dict :,if task_id not in self . task_host_failed_dict :,0.8820016898747209,100.00000000000004
"def match(path): for pat, _type, _property, default_title in patterns: m = web.re_compile(""^"" + pat).match(path) <mask>: prefix = m.group() extra = web.lstrips(path, prefix) tokens = extra.split(""/"", 2) # `extra` starts with ""/"". So first token is always empty. middle = web.listget(tokens, 1, """") suffix = web.listget(tokens, 2, """") if suffix: suffix = ""/"" + suffix return _type, _property, default_title, prefix, middle, suffix return None, None, None, None, None, None",True,if m :,if m :,0.8820016898747209,0.0
"def _get_cached_resources(self, ids): key = self.get_cache_key(None) if self._cache.load(): resources = self._cache.get(key) <mask>: self.log.debug(""Using cached results for get_resources"") m = self.get_model() id_set = set(ids) return [r for r in resources if r[m.id] in id_set] return None",False,if resources is not None :,if resources :,0.8820016898747209,0.0
"def has_api_behaviour(self, protocol): config = get_config() try: r = self.session.get( f""{protocol}://{self.event.host}:{self.event.port}"", timeout=config.network_timeout, ) <mask>: return True except requests.exceptions.SSLError: logger.debug( f""{[protocol]} protocol not accepted on {self.event.host}:{self.event.port}"" ) except Exception: logger.debug( f""Failed probing {self.event.host}:{self.event.port}"", exc_info=True )",False,"if ( ""k8s"" in r . text ) or ( '""code""' in r . text and r . status_code != 200 ) :",if r . status_code == 200 :,0.8820016898747209,5.330714786499167
"def get_file_type(self, context, parent_context=None): file_type = context.get(self.file_type_name, None) if file_type == """": <mask>: file_type = parent_context.get(self.file_type_name, self.default_file_type) else: file_type = self.default_file_type return file_type",True,if parent_context :,if parent_context :,0.8820016898747209,100.00000000000004
"def selectionToChunks(self, remove=False, add=False): box = self.selectionBox() if box: if box == self.level.bounds: self.selectedChunks = set(self.level.allChunks) return selectedChunks = self.selectedChunks boxedChunks = set(box.chunkPositions) <mask>: remove = True if remove and not add: selectedChunks.difference_update(boxedChunks) else: selectedChunks.update(boxedChunks) self.selectionTool.selectNone()",False,if boxedChunks . issubset ( selectedChunks ) :,if remove :,0.8820016898747209,0.0
"def _run_split_on_punc(self, text, never_split=None): """"""Splits punctuation on a piece of text."""""" if never_split is not None and text in never_split: return [text] chars = list(text) i = 0 start_new_word = True output = [] while i < len(chars): char = chars[i] <mask>: output.append([char]) start_new_word = True else: if start_new_word: output.append([]) start_new_word = False output[-1].append(char) i += 1 return ["""".join(x) for x in output]",False,if _is_punctuation ( char ) :,if char in self . punctuation :,0.8820016898747209,6.979367151952678
"def _save_images(notebook): if os.getenv(""NB_NO_IMAGES"") == ""1"": return logged = False for filename, img_bytes in _iter_notebook_images(notebook): <mask>: log.info(""Saving images"") logged = True with open(filename, ""wb"") as f: f.write(img_bytes)",True,if not logged :,if not logged :,0.8820016898747209,100.00000000000004
"def pickPath(self, color): self.path[color] = () currentPos = self.starts[color] while True: minDist = None minGuide = None for guide in self.guides[color]: guideDist = dist(currentPos, guide) <mask>: minDist = guideDist minGuide = guide if dist(currentPos, self.ends[color]) == 1: return if minGuide == None: return self.path[color] = self.path[color] + (minGuide,) currentPos = minGuide self.guides[color].remove(minGuide)",False,if minDist == None or guideDist < minDist :,if guideDist < minDist :,0.8820016898747209,26.013004751144457
"def _terminal_messenger(tp=""write"", msg="""", out=sys.stdout): try: <mask>: out.write(msg) elif tp == ""flush"": out.flush() elif tp == ""write_flush"": out.write(msg) out.flush() elif tp == ""print"": print(msg, file=out) else: raise ValueError(""Unsupported type: "" + tp) except IOError as e: logger.critical(""{}: {}"".format(type(e).__name__, ucd(e))) pass",False,"if tp == ""write"" :","if tp == ""write_write"" :",0.8820016898747209,63.894310424627285
"def __new__(mcs, name, bases, attrs): include_profile = include_trace = include_garbage = True bases = list(bases) if name == ""SaltLoggingClass"": for base in bases: <mask>: include_trace = False if hasattr(base, ""garbage""): include_garbage = False if include_profile: bases.append(LoggingProfileMixin) if include_trace: bases.append(LoggingTraceMixin) if include_garbage: bases.append(LoggingGarbageMixin) return super(LoggingMixinMeta, mcs).__new__(mcs, name, tuple(bases), attrs)",True,"if hasattr ( base , ""trace"" ) :","if hasattr ( base , ""trace"" ) :",0.8820016898747209,100.00000000000004
"def generatePidEncryptionTable(): table = [] for counter1 in range(0, 0x100): value = counter1 for counter2 in range(0, 8): <mask>: value = value >> 1 else: value = value >> 1 value = value ^ 0xEDB88320 table.append(value) return table",False,if value & 1 == 0 :,if counter1 == counter2 :,0.8820016898747209,13.83254362586636
"def pytest_collection_modifyitems(items): for item in items: if item.nodeid.startswith(""tests/params""): <mask>: item.add_marker(pytest.mark.stage(""unit"")) if ""init"" not in item.keywords: item.add_marker(pytest.mark.init(rng_seed=123))",True,"if ""stage"" not in item . keywords :","if ""stage"" not in item . keywords :",0.8820016898747209,100.00000000000004
"def python_value(self, value): if value: if isinstance(value, basestring): pp = lambda x: x.time() return format_date_time(value, self.formats, pp) <mask>: return value.time() if value is not None and isinstance(value, datetime.timedelta): return (datetime.datetime.min + value).time() return value",True,"elif isinstance ( value , datetime . datetime ) :","elif isinstance ( value , datetime . datetime ) :",0.8820016898747209,100.00000000000004
"def list_interesting_hosts(self): hosts = [] targets = self.target[""other""] for target in targets: <mask>: hosts.append( {""ip"": target.ip, ""description"": target.domain + "" / "" + target.name} ) return hosts",False,if self . is_interesting ( target ) and target . status and target . status != 400 :,if target . name :,0.8820016898747209,1.0356305364183098
"def TryMerge(self, d): while d.avail() > 0: tt = d.getVarInt32() if tt == 10: length = d.getVarInt32() tmp = ProtocolBuffer.Decoder(d.buffer(), d.pos(), d.pos() + length) d.skip(length) self.mutable_cost().TryMerge(tmp) continue <mask>: self.add_version(d.getVarInt64()) continue if tt == 0: raise ProtocolBuffer.ProtocolBufferDecodeError d.skipData(tt)",False,if tt == 24 :,if tt == 18 :,0.8820016898747209,53.7284965911771
"def _wait_for_finish(self) -> PollExitResponse: while True: if self._backend: poll_exit_resp = self._backend.interface.communicate_poll_exit() logger.info(""got exit ret: %s"", poll_exit_resp) if poll_exit_resp: done = poll_exit_resp.done pusher_stats = poll_exit_resp.pusher_stats <mask>: self._on_finish_progress(pusher_stats, done) if done: return poll_exit_resp time.sleep(2)",True,if pusher_stats :,if pusher_stats :,0.8820016898747209,100.00000000000004
"def listing_items(method): marker = None once = True items = [] while once or items: for i in items: yield i if once or marker: <mask>: items = method(parms={""marker"": marker}) else: items = method() if len(items) == 10000: marker = items[-1] else: marker = None once = False else: items = []",True,if marker :,if marker :,0.8820016898747209,0.0
"def call(monad, *args): for arg, name in izip(args, (""hour"", ""minute"", ""second"", ""microsecond"")): if not isinstance(arg, NumericMixin) or arg.type is not int: throw( TypeError, ""'%s' argument of time(...) function must be of 'int' type. Got: %r"" % (name, type2str(arg.type)), ) <mask>: throw(NotImplementedError) return ConstMonad.new(time(*tuple(arg.value for arg in args)))",False,"if not isinstance ( arg , ConstMonad ) :",if not monad . is_monad ( name ) :,0.8820016898747209,11.208466750961147
"def group_by_sign(seq, slop=sin(pi / 18), key=lambda x: x): sign = None subseq = [] for i in seq: ki = key(i) if sign is None: subseq.append(i) if ki != 0: sign = ki / abs(ki) else: subseq.append(i) <mask>: sign = ki / abs(ki) yield subseq subseq = [i] if subseq: yield subseq",False,if sign * ki < - slop :,if sign < slop :,0.8820016898747209,17.532970520619653
"def walk_links(self): link_info_list = [] for item in self.content: <mask>: link_info = LinkInfo(link=item, name=item.name, sections=()) link_info_list.append(link_info) else: link_info_list.extend(item.walk_links()) return link_info_list",False,"if isinstance ( item , Link ) :","if isinstance ( item , LinkInfo ) :",0.8820016898747209,59.4603557501361
"def get_subkeys(self, key): # TODO: once we revamp the registry emulation, # make this better parent_path = key.get_path() subkeys = [] for k in self.keys: test_path = k.get_path() if test_path.lower().startswith(parent_path.lower()): sub = test_path[len(parent_path) :] if sub.startswith(""\\""): sub = sub[1:] end_slash = sub.find(""\\"") <mask>: sub = sub[:end_slash] if not sub: continue subkeys.append(sub) return subkeys",False,if end_slash >= 0 :,if end_slash > - 1 :,0.8820016898747209,54.10822690539397
"def load_dict(dict_path, reverse=False): word_dict = {} with open(dict_path, ""rb"") as fdict: for idx, line in enumerate(fdict): line = cpt.to_text(line) <mask>: word_dict[idx] = line.strip(""\n"") else: word_dict[line.strip(""\n"")] = idx return word_dict",True,if reverse :,if reverse :,0.8820016898747209,0.0
"def test_network(coords, feats, model, batch_sizes, forward_only=True): for batch_size in batch_sizes: bcoords = batched_coordinates([coords for i in range(batch_size)]) bfeats = torch.cat([feats for i in range(batch_size)], 0) <mask>: with torch.no_grad(): time, length = forward(bcoords, bfeats, model) else: time, length = train(bcoords, bfeats, model) print(f""{net.__name__}\t{voxel_size}\t{batch_size}\t{length}\t{time}"") torch.cuda.empty_cache()",True,if forward_only :,if forward_only :,0.8820016898747209,100.00000000000004
"def markUVs(self, indices=None): if isinstance(indices, tuple): indices = indices[0] ntexco = len(self.texco) if indices is None: self.utexc = True else: <mask>: self.utexc = np.zeros(ntexco, dtype=bool) if self.utexc is not True: self.utexc[indices] = True",False,if self . utexc is False :,if self . utexc is None :,0.8820016898747209,64.34588841607616
"def has_module(self, module, version): has_module = False for directory in self.directories: module_directory = join(directory, module) has_module_directory = isdir(module_directory) <mask>: has_module = has_module_directory or exists( module_directory ) # could be a bare modulefile else: modulefile = join(module_directory, version) has_modulefile = exists(modulefile) has_module = has_module_directory and has_modulefile if has_module: break return has_module",False,if not version :,"if version == ""bare"" :",0.8820016898747209,7.267884212102741
"def get_editops(self): if not self._editops: <mask>: self._editops = editops(self._opcodes, self._str1, self._str2) else: self._editops = editops(self._str1, self._str2) return self._editops",True,if self . _opcodes :,if self . _opcodes :,0.8820016898747209,100.00000000000004
"def to_representation(self, data): value = super(CredentialTypeSerializer, self).to_representation(data) # translate labels and help_text for credential fields ""managed by Tower"" if value.get(""managed_by_tower""): value[""name""] = _(value[""name""]) for field in value.get(""inputs"", {}).get(""fields"", []): field[""label""] = _(field[""label""]) <mask>: field[""help_text""] = _(field[""help_text""]) return value",False,"if ""help_text"" in field :","if field [ ""help_text"" ] :",0.8820016898747209,44.17918226831576
"def sort_nested_dictionary_lists(d): for k, v in d.items(): if isinstance(v, list): for i in range(0, len(v)): <mask>: v[i] = await sort_nested_dictionary_lists(v[i]) d[k] = sorted(v) if isinstance(v, dict): d[k] = await sort_nested_dictionary_lists(v) return d",True,"if isinstance ( v [ i ] , dict ) :","if isinstance ( v [ i ] , dict ) :",0.8820016898747209,100.00000000000004
"def messageSourceStamps(self, source_stamps): text = """" for ss in source_stamps: source = """" if ss[""branch""]: source += ""[branch %s] "" % ss[""branch""] if ss[""revision""]: source += str(ss[""revision""]) else: source += ""HEAD"" <mask>: source += "" (plus patch)"" discriminator = """" if ss[""codebase""]: discriminator = "" '%s'"" % ss[""codebase""] text += ""Build Source Stamp%s: %s\n"" % (discriminator, source) return text",False,"if ss [ ""patch"" ] is not None :","if ss [ ""patch"" ] :",0.8820016898747209,59.755798910891144
"def fit_one(self, x): for i, xi in x.items(): <mask>: self.median[i].update(xi) if self.with_scaling: self.iqr[i].update(xi) return self",False,if self . with_centering :,if self . with_scaling :,0.8820016898747209,64.34588841607616
"def start_response(self, status, headers, exc_info=None): if exc_info: try: if self.started: six.reraise(exc_info[0], exc_info[1], exc_info[2]) finally: exc_info = None self.request.status = int(status[:3]) for key, val in headers: <mask>: self.request.set_content_length(int(val)) elif key.lower() == ""content-type"": self.request.content_type = val else: self.request.headers_out.add(key, val) return self.write",True,"if key . lower ( ) == ""content-length"" :","if key . lower ( ) == ""content-length"" :",0.8820016898747209,100.00000000000004
"def _osp2ec(self, bytes): compressed = self._from_bytes(bytes) y = compressed >> self._bits x = compressed & (1 << self._bits) - 1 if x == 0: y = self._curve.b else: result = self.sqrtp( x ** 3 + self._curve.a * x + self._curve.b, self._curve.field.p ) if len(result) == 1: y = result[0] <mask>: y1, y2 = result y = y1 if (y1 & 1 == y) else y2 else: return None return ec.Point(self._curve, x, y)",True,elif len ( result ) == 2 :,elif len ( result ) == 2 :,0.8820016898747209,100.00000000000004
"def trace(self, ee, rname): print(type(self)) self.traceIndent() guess = """" if self.inputState.guessing > 0: guess = "" [guessing]"" print((ee + rname + guess)) for i in xrange(1, self.k + 1): if i != 1: print("", "") <mask>: v = self.LT(i).getText() else: v = ""null"" print(""LA(%s) == %s"" % (i, v)) print(""\n"")",False,if self . LT ( i ) :,elif self . LT ( i ) :,0.8820016898747209,84.08964152537145
"def _table_schema(self, table): rows = self.db.execute_sql(""PRAGMA table_info('%s')"" % table).fetchall() # Build list of fields from table information result = {} for _, name, data_type, not_null, _, primary_key in rows: parts = [data_type] if primary_key: parts.append(""PRIMARY KEY"") <mask>: parts.append(""NOT NULL"") result[name] = "" "".join(parts) return result",False,if not_null :,elif not_null :,0.8820016898747209,66.87403049764218
"def _parse_csrf(self, response): for d in response: if d.startswith(""Set-Cookie:""): for c in d.split("":"", 1)[1].split("";""): if c.strip().startswith(""CSRF-Token-""): self._CSRFtoken = c.strip("" \r\n"") log.verbose(""Got new cookie: %s"", self._CSRFtoken) break <mask>: break",False,if self . _CSRFtoken != None :,"if self . _CSRFtoken == """" :",0.8820016898747209,42.7287006396234
"def _update_from_item(self, row, download_item): progress_stats = download_item.progress_stats for key in self.columns: column = self.columns[key][0] <mask>: # Not the best place but we build the playlist status here status = ""{0} {1}/{2}"".format( progress_stats[""status""], progress_stats[""playlist_index""], progress_stats[""playlist_size""], ) self.SetStringItem(row, column, status) else: self.SetStringItem(row, column, progress_stats[key])",False,"if key == ""status"" and progress_stats [ ""playlist_index"" ] :","if column == ""playlist"" :",0.8820016898747209,7.109107668271669
"def unmarshal_package_repositories(cls, data: Any) -> List[""PackageRepository""]: repositories = list() if data is not None: <mask>: raise RuntimeError(f""invalid package-repositories: {data!r}"") for repository in data: package_repo = cls.unmarshal(repository) repositories.append(package_repo) return repositories",True,"if not isinstance ( data , list ) :","if not isinstance ( data , list ) :",0.8820016898747209,100.00000000000004
"def remove_message(e=None): itop = scanbox.nearest(0) sel = scanbox.curselection() if not sel: dialog( root, ""No Message To Remove"", ""Please select a message to remove"", """", 0, ""OK"", ) return todo = [] for i in sel: line = scanbox.get(i) <mask>: todo.append(string.atoi(scanparser.group(1))) mhf.removemessages(todo) rescan() fixfocus(min(todo), itop)",False,if scanparser . match ( line ) >= 0 :,if line :,0.8820016898747209,0.0
"def test_patches(): print( ""Botocore version: {} aiohttp version: {}"".format( botocore.__version__, aiohttp.__version__ ) ) success = True for obj, digests in chain(_AIOHTTP_DIGESTS.items(), _API_DIGESTS.items()): digest = hashlib.sha1(getsource(obj).encode(""utf-8"")).hexdigest() <mask>: print( ""Digest of {}:{} not found in: {}"".format( obj.__qualname__, digest, digests ) ) success = False assert success",True,if digest not in digests :,if digest not in digests :,0.8820016898747209,100.00000000000004
"def sample_admin_user(): """"""List of iris messages"""""" with iris_ctl.db_from_config(sample_db_config) as (conn, cursor): cursor.execute( ""SELECT `name` FROM `target` JOIN `user` on `target`.`id` = `user`.`target_id` WHERE `user`.`admin` = TRUE LIMIT 1"" ) result = cursor.fetchone() <mask>: return result[0]",True,if result :,if result :,0.8820016898747209,0.0
"def _addRightnames(groups, kerning, leftname, rightnames, includeAll=True): if leftname in kerning: for rightname in kerning[leftname]: <mask>: for rightname2 in groups[rightname]: rightnames.add(rightname2) if not includeAll: # TODO: in this case, pick the one rightname that has the highest # ranking in glyphorder break else: rightnames.add(rightname)",False,"if rightname [ 0 ] == ""@"" :",if rightname in groups :,0.8820016898747209,7.121297464907233
"def build(self, input_shape): if isinstance(input_shape, list) and len(input_shape) == 2: self.data_mode = ""disjoint"" self.F = input_shape[0][-1] else: <mask>: self.data_mode = ""single"" else: self.data_mode = ""batch"" self.F = input_shape[-1]",False,if len ( input_shape ) == 2 :,"if isinstance ( input_shape , tuple ) :",0.8820016898747209,27.3385351346167
"def update_ranges(l, i): for _range in l: # most common case: extend a range <mask>: _range[0] = i merge_ranges(l) return elif i == _range[1] + 1: _range[1] = i merge_ranges(l) return # somewhere outside of range proximity l.append([i, i]) l.sort(key=lambda x: x[0])",False,if i == _range [ 0 ] - 1 :,if i == _range [ 0 ] + 1 :,0.8820016898747209,76.91605673134588
"def transform(a, cmds): buf = a.split(""\n"") for cmd in cmds: ctype, line, col, char = cmd <mask>: if char != ""\n"": buf[line] = buf[line][:col] + buf[line][col + len(char) :] else: buf[line] = buf[line] + buf[line + 1] del buf[line + 1] elif ctype == ""I"": buf[line] = buf[line][:col] + char + buf[line][col:] buf = ""\n"".join(buf).split(""\n"") return ""\n"".join(buf)",False,"if ctype == ""D"" :","if ctype == ""I"" :",0.8820016898747209,59.4603557501361
"def _media_files_drag_received(widget, context, x, y, data, info, timestamp): uris = data.get_uris() files = [] for uri in uris: try: uri_tuple = GLib.filename_from_uri(uri) except: continue uri, unused = uri_tuple if os.path.exists(uri) == True: <mask>: files.append(uri) if len(files) == 0: return open_dropped_files(files)",False,if utils . is_media_file ( uri ) == True :,if not os . path . isfile ( uri ) :,0.8820016898747209,11.532706802722437
"def __walk_proceed_remote_dir_act(self, r, args): dirjs, filejs = args j = r.json() if ""list"" not in j: self.pd( ""Key 'list' not found in the response of directory listing request:\n{}"".format( j ) ) return const.ERequestFailed paths = j[""list""] for path in paths: <mask>: dirjs.append(path) else: filejs.append(path) return const.ENoError",False,"if path [ ""isdir"" ] :",if path in dirjs :,0.8820016898747209,12.975849993980741
"def TaskUpdatesVerbose(task, progress): if isinstance(task.info.progress, int): info = task.info <mask>: progress = ""%d%% (%s)"" % (info.progress, info.state) print( ""Task %s (key:%s, desc:%s) - %s"" % (info.name.info.name, info.key, info.description, progress) )",False,"if not isinstance ( progress , str ) :",if info . state :,0.8820016898747209,5.70796903405875
"def dump_constants(header): output = StringIO.StringIO() output.write(header) for attribute in dir(FSEvents): value = getattr(FSEvents, attribute) <mask>: output.write("" %s = %s\n"" % (attribute, hex(value))) content = output.getvalue() output.close() return content",False,"if attribute . startswith ( ""k"" ) and isinstance ( value , int ) :",if value :,0.8820016898747209,0.0
"def _ensure_data_is_loaded( self, sql_object, input_params, stdin_file, stdin_filename=""-"", stop_after_analysis=False, ): data_loads = [] # Get each ""table name"" which is actually the file name for filename in sql_object.qtable_names: data_load = self._load_data( filename, input_params, stdin_file=stdin_file, stdin_filename=stdin_filename, stop_after_analysis=stop_after_analysis, ) <mask>: data_loads.append(data_load) return data_loads",False,if data_load is not None :,if data_load :,0.8820016898747209,38.80684294761701
"def _get_instantiation(self): if self._data is None: f, l, c, o = c_object_p(), c_uint(), c_uint(), c_uint() SourceLocation_loc(self, byref(f), byref(l), byref(c), byref(o)) <mask>: f = File(f) else: f = None self._data = (f, int(l.value), int(c.value), int(c.value)) return self._data",False,if f :,"if isinstance ( f , File ) :",0.8820016898747209,7.267884212102741
"def _get_all_info_lines(data): infos = [] for row in data: splitrow = row.split() <mask>: if splitrow[0] == ""INFO:"": infos.append("" "".join(splitrow[1:])) return infos",False,if len ( splitrow ) > 0 :,if len ( splitrow ) > 1 :,0.8820016898747209,70.71067811865478
"def _brush_modified_cb(self, settings): """"""Updates the brush's base setting adjustments on brush changes"""""" for cname in settings: adj = self.brush_adjustment.get(cname, None) <mask>: continue value = self.brush.get_base_value(cname) adj.set_value(value)",True,if adj is None :,if adj is None :,0.8820016898747209,100.00000000000004
"def migrate_node_facts(facts): """"""Migrate facts from various roles into node"""""" params = { ""common"": (""dns_ip""), } if ""node"" not in facts: facts[""node""] = {} # pylint: disable=consider-iterating-dictionary for role in params.keys(): if role in facts: for param in params[role]: <mask>: facts[""node""][param] = facts[role].pop(param) return facts",True,if param in facts [ role ] :,if param in facts [ role ] :,0.8820016898747209,100.00000000000004
"def serialize_content_range(value): if isinstance(value, (tuple, list)): <mask>: raise ValueError( ""When setting content_range to a list/tuple, it must "" ""be length 2 or 3 (not %r)"" % value ) if len(value) == 2: begin, end = value length = None else: begin, end, length = value value = ContentRange(begin, end, length) value = str(value).strip() if not value: return None return value",False,"if len ( value ) not in ( 2 , 3 ) :",if len ( value ) != 3 :,0.8820016898747209,31.128780276284857
"def clean(self): data = super().clean() if data.get(""expires""): <mask>: data[""expires""] = make_aware( datetime.combine(data[""expires""], time(hour=23, minute=59, second=59)), self.instance.event.timezone, ) else: data[""expires""] = data[""expires""].replace(hour=23, minute=59, second=59) if data[""expires""] < now(): raise ValidationError(_(""The new expiry date needs to be in the future."")) return data",False,"if isinstance ( data [ ""expires"" ] , date ) :",if self . instance . event . timezone :,0.8820016898747209,3.6353588668522963
"def _build(self, obj, stream, context): if self.include_name: name, obj = obj for sc in self.subcons: <mask>: sc._build(obj, stream, context) return else: for sc in self.subcons: stream2 = BytesIO() context2 = context.__copy__() try: sc._build(obj, stream2, context2) except Exception: pass else: context.__update__(context2) stream.write(stream2.getvalue()) return raise SelectError(""no subconstruct matched"", obj)",False,if sc . name == name :,if name == sc . name :,0.8820016898747209,39.28146509005134
"def records(account_id): """"""Fetch locks data"""""" s = boto3.Session() table = s.resource(""dynamodb"").Table(""Sphere11.Dev.ResourceLocks"") results = table.scan() for r in results[""Items""]: <mask>: r[""LockDate""] = datetime.fromtimestamp(r[""LockDate""]) if ""RevisionDate"" in r: r[""RevisionDate""] = datetime.fromtimestamp(r[""RevisionDate""]) print(tabulate.tabulate(results[""Items""], headers=""keys"", tablefmt=""fancy_grid""))",True,"if ""LockDate"" in r :","if ""LockDate"" in r :",0.8820016898747209,100.00000000000004
"def visitIf(self, node, scope): for test, body in node.tests: if isinstance(test, ast.Const): <mask>: if not test.value: continue self.visit(test, scope) self.visit(body, scope) if node.else_: self.visit(node.else_, scope)",False,if type ( test . value ) in self . _const_types :,"if isinstance ( body , ast . Expr ) :",0.8820016898747209,3.805770825247019
"def validate_max_discount(self): if self.rate_or_discount == ""Discount Percentage"" and self.get(""items""): for d in self.items: max_discount = frappe.get_cached_value(""Item"", d.item_code, ""max_discount"") <mask>: throw( _(""Max discount allowed for item: {0} is {1}%"").format( self.item_code, max_discount ) )",False,if max_discount and flt ( self . discount_percentage ) > flt ( max_discount ) :,if max_discount :,0.8820016898747209,2.8823230849212025
"def has_invalid_cce(yaml_file, product_yaml=None): rule = yaml.open_and_macro_expand(yaml_file, product_yaml) if ""identifiers"" in rule and rule[""identifiers""] is not None: for i_type, i_value in rule[""identifiers""].items(): if i_type[0:3] == ""cce"": <mask>: return True return False",False,"if not checks . is_cce_value_valid ( ""CCE-"" + str ( i_value ) ) :","if i_value == ""invalid"" :",0.8820016898747209,4.760522270822933
"def parse_calendar_eras(data, calendar): eras = data.setdefault(""eras"", {}) for width in calendar.findall(""eras/*""): width_type = NAME_MAP[width.tag] widths = eras.setdefault(width_type, {}) for elem in width.getiterator(): if elem.tag == ""era"": _import_type_text(widths, elem, type=int(elem.attrib.get(""type""))) <mask>: eras[width_type] = Alias( _translate_alias([""eras"", width_type], elem.attrib[""path""]) )",False,"elif elem . tag == ""alias"" :","elif elem . tag == ""eras"" :",0.8820016898747209,70.71067811865478
"def validate_grammar() -> None: for fn in _NONTERMINAL_CONVERSIONS_SEQUENCE: fn_productions = get_productions(fn) if all(p.name == fn_productions[0].name for p in fn_productions): # all the production names are the same, ensure that the `convert_` function # is named correctly production_name = fn_productions[0].name expected_name = f""convert_{production_name}"" <mask>: raise Exception( f""The conversion function for '{production_name}' "" + f""must be called '{expected_name}', not '{fn.__name__}'."" )",False,if fn . __name__ != expected_name :,if not callable ( expected_name ) :,0.8820016898747209,11.56970650765539
"def split_ratio(row): if float(row[""Numerator""]) > 0: <mask>: n, m = row[""Splitratio""].split("":"") return float(m) / float(n) else: return eval(row[""Splitratio""]) else: return 1",False,"if "":"" in row [ ""Splitratio"" ] :","if ""Splitratio"" in row :",0.8820016898747209,22.117541221307572
"def _handle_def_errors(testdef): # If the test generation had an error, raise if testdef.error: if testdef.exception: <mask>: raise testdef.exception else: raise Exception(testdef.exception) else: raise Exception(""Test parse failure"")",True,"if isinstance ( testdef . exception , Exception ) :","if isinstance ( testdef . exception , Exception ) :",0.8820016898747209,100.00000000000004
"def _get_quota_availability(self): quotas_ok = defaultdict(int) qa = QuotaAvailability() qa.queue(*[k for k, v in self._quota_diff.items() if v > 0]) qa.compute(now_dt=self.now_dt) for quota, count in self._quota_diff.items(): if count <= 0: quotas_ok[quota] = 0 break avail = qa.results[quota] <mask>: quotas_ok[quota] = min(count, avail[1]) else: quotas_ok[quota] = count return quotas_ok",False,if avail [ 1 ] is not None and avail [ 1 ] < count :,if avail :,0.8820016898747209,0.0
"def reverse(self): """"""Reverse *IN PLACE*."""""" li = self.leftindex lb = self.leftblock ri = self.rightindex rb = self.rightblock for i in range(self.len >> 1): lb.data[li], rb.data[ri] = rb.data[ri], lb.data[li] li += 1 <mask>: lb = lb.rightlink li = 0 ri -= 1 if ri < 0: rb = rb.leftlink ri = BLOCKLEN - 1",False,if li >= BLOCKLEN :,if li < 0 :,0.8820016898747209,19.3576934939088
"def __manipulate_item(self, item): if self._Cursor__manipulate: db = self._Cursor__collection.database son = db._fix_outgoing(item, self._Cursor__collection) else: son = item if self.__wrap is not None: <mask>: return getattr(self._Cursor__collection, son[self.__wrap.type_field])(son) return self.__wrap(son, collection=self._Cursor__collection) else: return son",True,if self . __wrap . type_field in son :,if self . __wrap . type_field in son :,0.8820016898747209,100.00000000000004
"def apply_transforms(self): """"""Apply all of the stored transforms, in priority order."""""" self.document.reporter.attach_observer(self.document.note_transform_message) while self.transforms: <mask>: # Unsorted initially, and whenever a transform is added. self.transforms.sort() self.transforms.reverse() self.sorted = 1 priority, transform_class, pending, kwargs = self.transforms.pop() transform = transform_class(self.document, startnode=pending) transform.apply(**kwargs) self.applied.append((priority, transform_class, pending, kwargs))",False,if not self . sorted :,if self . sorted :,0.8820016898747209,57.89300674674101
"def format_sql(sql, params): rv = [] if isinstance(params, dict): # convert sql with named parameters to sql with unnamed parameters conv = _FormatConverter(params) <mask>: sql = sql_to_string(sql) sql = sql % conv params = conv.params else: params = () for param in params or (): if param is None: rv.append(""NULL"") param = safe_repr(param) rv.append(param) return sql, rv",False,if params :,if conv . params :,0.8820016898747209,23.643540225079384
"def on_execution_item(self, cpath, execution): if not isinstance(execution, dict): return if ""executor"" in execution and execution.get(""executor"") != ""jmeter"": return scenario = execution.get(""scenario"", None) <mask>: return if isinstance(scenario, str): scenario_name = scenario scenario = self.get_named_scenario(scenario_name) if not scenario: scenario = None scenario_path = Path(""scenarios"", scenario_name) else: scenario_path = cpath.copy() scenario_path.add_component(""scenario"") if scenario is not None: self.check_jmeter_scenario(scenario_path, scenario)",True,if not scenario :,if not scenario :,0.8820016898747209,100.00000000000004
"def _poll_ipc_requests(self) -> None: try: if self._ipc_requests.empty(): return while not self._ipc_requests.empty(): args = self._ipc_requests.get() try: for filename in args: <mask>: self.get_editor_notebook().show_file(filename) except Exception as e: logger.exception(""Problem processing ipc request"", exc_info=e) self.become_active_window() finally: self.after(50, self._poll_ipc_requests)",False,if os . path . isfile ( filename ) :,"if filename . endswith ( "".py"" ) :",0.8820016898747209,10.252286118120933
"def get_scroll_distance_to_element(driver, element): try: scroll_position = driver.execute_script(""return window.scrollY;"") element_location = None element_location = element.location[""y""] element_location = element_location - 130 <mask>: element_location = 0 distance = element_location - scroll_position return distance except Exception: return 0",True,if element_location < 0 :,if element_location < 0 :,0.8820016898747209,100.00000000000004
"def TryMerge(self, d): while d.avail() > 0: tt = d.getVarInt32() if tt == 10: self.set_access_token(d.getPrefixedString()) continue <mask>: self.set_expiration_time(d.getVarInt64()) continue if tt == 0: raise ProtocolBuffer.ProtocolBufferDecodeError d.skipData(tt)",False,if tt == 16 :,if tt == 18 :,0.8820016898747209,53.7284965911771
"def _validate_and_define(params, key, value): (key, force_generic) = _validate_key(_unescape(key)) if key in params: raise SyntaxError(f'duplicate key ""{key}""') cls = _class_for_key.get(key, GenericParam) emptiness = cls.emptiness() if value is None: if emptiness == Emptiness.NEVER: raise SyntaxError(""value cannot be empty"") value = cls.from_value(value) else: <mask>: value = cls.from_wire_parser(dns.wire.Parser(_unescape(value))) else: value = cls.from_value(value) params[key] = value",True,if force_generic :,if force_generic :,0.8820016898747209,100.00000000000004
"def iter_fields(node, *, include_meta=True, exclude_unset=False): exclude_meta = not include_meta for field_name, field in node._fields.items(): <mask>: continue field_val = getattr(node, field_name, _marker) if field_val is _marker: continue if exclude_unset: if callable(field.default): default = field.default() else: default = field.default if field_val == default: continue yield field_name, field_val",False,if exclude_meta and field . meta :,if field_name in exclude_meta :,0.8820016898747209,23.356898886410015
"def tearDown(self): """"""Shutdown the server."""""" try: if self.server: self.server.stop() <mask>: self.root_logger.removeHandler(self.sl_hdlr) self.sl_hdlr.close() finally: BaseTest.tearDown(self)",True,if self . sl_hdlr :,if self . sl_hdlr :,0.8820016898747209,100.00000000000004
"def _wait_for_async_copy(self, share_name, file_path): count = 0 share_client = self.fsc.get_share_client(share_name) file_client = share_client.get_file_client(file_path) properties = file_client.get_file_properties() while properties.copy.status != ""success"": count = count + 1 <mask>: self.fail(""Timed out waiting for async copy to complete."") self.sleep(6) properties = file_client.get_file_properties() self.assertEqual(properties.copy.status, ""success"")",True,if count > 10 :,if count > 10 :,0.8820016898747209,100.00000000000004
"def __new__( cls, message_type: OrderBookMessageType, content: Dict[str, any], timestamp: Optional[float] = None, *args, **kwargs, ): if timestamp is None: <mask>: raise ValueError( ""timestamp must not be None when initializing snapshot messages."" ) timestamp = int(time.time()) return super(KucoinOrderBookMessage, cls).__new__( cls, message_type, content, timestamp=timestamp, *args, **kwargs )",False,if message_type is OrderBookMessageType . SNAPSHOT :,if message_type == OrderBookMessageType . SNAPSHOT :,0.8820016898747209,52.53819788848316
"def _drop_unique_features( X: DataFrame, feature_metadata: FeatureMetadata, max_unique_ratio ) -> list: features_to_drop = [] X_len = len(X) max_unique_value_count = X_len * max_unique_ratio for column in X: unique_value_count = len(X[column].unique()) <mask>: features_to_drop.append(column) elif feature_metadata.get_feature_type_raw(column) in [ R_CATEGORY, R_OBJECT, ] and (unique_value_count > max_unique_value_count): features_to_drop.append(column) return features_to_drop",False,if unique_value_count == 1 :,"if feature_metadata . get_feature_type_raw ( column ) in [ R_CATEGORY , R_OBJECT , R_CATEGORY , R_OBJECT ] :",0.8820016898747209,1.5393730252811677
"def get_src_findex_by_pad(s, S, padding_mode, align_corners): if padding_mode == ""zero"": return get_src_findex_with_zero_pad(s, S) elif padding_mode == ""reflect"": <mask>: return get_src_findex_with_reflect_pad(s, S, True) else: sf = get_src_findex_with_reflect_pad(s, S, False) return get_src_findex_with_repeat_pad(sf, S) elif padding_mode == ""repeat"": return get_src_findex_with_repeat_pad(s, S)",True,if align_corners :,if align_corners :,0.8820016898747209,100.00000000000004
"def _iterate_self_and_parents(self, upto=None): current = self result = () while current: result += (current,) if current._parent is upto: break <mask>: raise sa_exc.InvalidRequestError( ""Transaction %s is not on the active transaction list"" % (upto) ) else: current = current._parent return result",False,elif current . _parent is None :,if current . _parent is upto :,0.8820016898747209,54.10822690539397
"def __setattr__(self, name: str, val: Any): if name.startswith(""COMPUTED_""): <mask>: old_val = self[name] if old_val == val: return raise KeyError( ""Computed attributed '{}' already exists "" ""with a different value! old={}, new={}."".format(name, old_val, val) ) self[name] = val else: super().__setattr__(name, val)",True,if name in self :,if name in self :,0.8820016898747209,100.00000000000004
"def get_fnlist(bbhandler, pkg_pn, preferred): """"""Get all recipe file names"""""" <mask>: (latest_versions, preferred_versions) = bb.providers.findProviders( bbhandler.config_data, bbhandler.cooker.recipecaches[""""], pkg_pn ) fn_list = [] for pn in sorted(pkg_pn): if preferred: fn_list.append(preferred_versions[pn][1]) else: fn_list.extend(pkg_pn[pn]) return fn_list",True,if preferred :,if preferred :,0.8820016898747209,0.0
"def links_extracted(self, _, links): links_deduped = {} for link in links: link_fingerprint = link.meta[FIELD_FINGERPRINT] <mask>: continue links_deduped[link_fingerprint] = link [ self._redis_pipeline.hmset(fingerprint, self._create_link_extracted(link)) for (fingerprint, link) in links_deduped.items() ] self._redis_pipeline.execute()",False,if link_fingerprint in links_deduped :,if link_fingerprint not in links_deduped :,0.8820016898747209,65.80370064762461
"def __call__(self, name, rawtext, text, lineno, inliner, options=None, content=None): options = options or {} content = content or [] issue_nos = [each.strip() for each in utils.unescape(text).split("","")] config = inliner.document.settings.env.app.config ret = [] for i, issue_no in enumerate(issue_nos): node = self.make_node(name, issue_no, config, options=options) ret.append(node) <mask>: sep = nodes.raw(text="", "", format=""html"") ret.append(sep) return ret, []",False,if i != len ( issue_nos ) - 1 :,if i == len ( content ) - 1 :,0.8820016898747209,31.530788046040357
"def init_messengers(messengers): for messenger in messengers: <mask>: module_path = messenger[""type""] messenger[""type""] = messenger[""type""].split(""."")[-1] else: module_path = ""oncall.messengers."" + messenger[""type""] instance = getattr(importlib.import_module(module_path), messenger[""type""])( messenger ) for transport in instance.supports: _active_messengers[transport].append(instance)",True,"if ""."" in messenger [ ""type"" ] :","if ""."" in messenger [ ""type"" ] :",0.8820016898747209,100.00000000000004
"def _process_enum_definition(self, tok): fields = [] for field in tok.fields: <mask>: expression = self.expression_parser.parse(field.expression) else: expression = None fields.append(c_ast.CEnumField(name=field.name.first, value=expression)) name = tok.enum_name if name: name = ""enum %s"" % tok.enum_name.first else: name = self._make_anonymous_type(""enum"") return c_ast.CTypeDefinition( name=name, type_definition=c_ast.CEnum( attributes=tok.attributes, fields=fields, name=name ), )",True,if field . expression :,if field . expression :,0.8820016898747209,100.00000000000004
def result_iterator(): try: # reverse to keep finishing order fs.reverse() while fs: # Careful not to keep a reference to the popped future <mask>: yield fs.pop().result() else: yield fs.pop().result(end_time - time.time()) finally: for future in fs: future.cancel(),False,if timeout is None :,if end_time is None :,0.8820016898747209,26.269098944241588
"def has_encrypted_ssh_key_data(self): try: ssh_key_data = self.get_input(""ssh_key_data"") except AttributeError: return False try: pem_objects = validate_ssh_private_key(ssh_key_data) for pem_object in pem_objects: <mask>: return True except ValidationError: pass return False",False,"if pem_object . get ( ""key_enc"" , False ) :",if pem_object . is_encrypted :,0.8820016898747209,22.30474572792991
"def test_seq_object_transcription_method(self): for nucleotide_seq in test_seqs: <mask>: self.assertEqual( repr(Seq.transcribe(nucleotide_seq)), repr(nucleotide_seq.transcribe()), )",False,"if isinstance ( nucleotide_seq , Seq . Seq ) :","if isinstance ( nucleotide_seq , Seq ) :",0.8820016898747209,72.79660202821188
"def max_elevation(self): max_el = None for y in xrange(self.height): for x in xrange(self.width): el = self.elevation[""data""][y][x] <mask>: max_el = el return max_el",False,if max_el is None or el > max_el :,if el > max_el :,0.8820016898747209,35.685360466076496
"def stress(mapping, index): for count in range(OPERATIONS): function = random.choice(functions) function(mapping, index) <mask>: print(""\r"", len(mapping), "" "" * 7, end="""") print()",False,if count % 1000 == 0 :,if count % 2 == 0 :,0.8820016898747209,50.000000000000014
"def sync_terminology(self): if self.is_source: return store = self.store missing = [] for source in self.component.get_all_sources(): if ""terminology"" not in source.all_flags: continue try: _unit, add = store.find_unit(source.context, source.source) except UnitNotFound: add = True # Unit is already present <mask>: continue missing.append((source.context, source.source, """")) if missing: self.add_units(None, missing)",False,if not add :,if add :,0.8820016898747209,0.0
"def get_generators(self): """"""Get a dict with all registered generators, indexed by name"""""" generators = {} for core in self.db.find(): <mask>: _generators = core.get_generators({}) if _generators: generators[str(core.name)] = _generators return generators",False,"if hasattr ( core , ""get_generators"" ) :",if core . name :,0.8820016898747209,3.466791587270993
"def act(self, state): if self.body.env.clock.frame < self.training_start_step: return policy_util.random(state, self, self.body).cpu().squeeze().numpy() else: action = self.action_policy(state, self, self.body) <mask>: action = self.scale_action(torch.tanh(action)) # continuous action bound return action.cpu().squeeze().numpy()",False,if not self . body . is_discrete :,if self . scale_action :,0.8820016898747209,10.759051250985632
"def try_open_completions_event(self, event=None): ""(./) Open completion list after pause with no movement."" lastchar = self.text.get(""insert-1c"") if lastchar in TRIGGERS: args = TRY_A if lastchar == ""."" else TRY_F self._delayed_completion_index = self.text.index(""insert"") <mask>: self.text.after_cancel(self._delayed_completion_id) self._delayed_completion_id = self.text.after( self.popupwait, self._delayed_open_completions, args )",False,if self . _delayed_completion_id is not None :,if self . _delayed_completion_index != - 1 :,0.8820016898747209,52.960749334062214
"def token_is_available(self): if self.token: try: resp = requests.get( ""https://api.shodan.io/account/profile?key={0}"".format(self.token) ) <mask>: return True except Exception as ex: logger.error(str(ex)) return False",False,"if resp and resp . status_code == 200 and ""member"" in resp . json ( ) :",if resp . status_code == 200 and resp . text == self . token :,0.8820016898747209,44.50642935387237
"def next_bar_(self, event): bars = event.bar_dict self._current_minute = self._minutes_since_midnight( self.ucontext.now.hour, self.ucontext.now.minute ) for day_rule, time_rule, func in self._registry: <mask>: with ExecutionContext(EXECUTION_PHASE.SCHEDULED): with ModifyExceptionFromType(EXC_TYPE.USER_EXC): func(self.ucontext, bars) self._last_minute = self._current_minute",False,if day_rule ( ) and time_rule ( ) :,"if day_rule == ""day"" and time_rule == ""time"" :",0.8820016898747209,26.104909033290696
"def decoder(s): r = [] decode = [] for c in s: if c == ""&"" and not decode: decode.append(""&"") <mask>: if len(decode) == 1: r.append(""&"") else: r.append(modified_unbase64("""".join(decode[1:]))) decode = [] elif decode: decode.append(c) else: r.append(c) if decode: r.append(modified_unbase64("""".join(decode[1:]))) bin_str = """".join(r) return (bin_str, len(s))",False,"elif c == ""-"" and decode :","elif c == ""&"" :",0.8820016898747209,43.79518644116555
"def admin_audit_get(admin_id): if settings.app.demo_mode: resp = utils.demo_get_cache() <mask>: return utils.jsonify(resp) if not flask.g.administrator.super_user: return utils.jsonify( { ""error"": REQUIRES_SUPER_USER, ""error_msg"": REQUIRES_SUPER_USER_MSG, }, 400, ) admin = auth.get_by_id(admin_id) resp = admin.get_audit_events() if settings.app.demo_mode: utils.demo_set_cache(resp) return utils.jsonify(resp)",False,if resp :,if settings . app . demo_mode :,0.8820016898747209,5.669791110976001
"def vjp(self, argnum, outgrad, ans, vs, gvs, args, kwargs): try: return self.vjps[argnum](outgrad, ans, vs, gvs, *args, **kwargs) except KeyError: <mask>: errstr = ""Gradient of {0} not yet implemented."" else: errstr = ""Gradient of {0} w.r.t. arg number {1} not yet implemented."" raise NotImplementedError(errstr.format(self.fun.__name__, argnum))",False,if self . vjps == { } :,if argnum == 0 :,0.8820016898747209,11.708995388048026
"def update(self, *args, **kwargs): assert not self.readonly longest_key = 0 _dict = self._dict reverse = self.reverse casereverse = self.casereverse for iterable in args + (kwargs,): <mask>: iterable = iterable.items() for key, value in iterable: longest_key = max(longest_key, len(key)) _dict[key] = value reverse[value].append(key) casereverse[value.lower()][value] += 1 self._longest_key = max(self._longest_key, longest_key)",False,"if isinstance ( iterable , ( dict , StenoDictionary ) ) :","if isinstance ( iterable , dict ) :",0.8820016898747209,37.28878639930421
"def update_ui(self, window): view = window.get_active_view() self.set_status(view) lang = ""plain_text"" if view: buf = view.get_buffer() language = buf.get_language() <mask>: lang = language.get_id() self.setup_smart_indent(view, lang)",True,if language :,if language :,0.8820016898747209,0.0
"def number_operators(self, a, b, skip=[]): dict = {""a"": a, ""b"": b} for name, expr in self.binops.items(): if name not in skip: name = ""__%s__"" % name <mask>: res = eval(expr, dict) self.binop_test(a, b, res, expr, name) for name, expr in self.unops.items(): if name not in skip: name = ""__%s__"" % name if hasattr(a, name): res = eval(expr, dict) self.unop_test(a, res, expr, name)",True,"if hasattr ( a , name ) :","if hasattr ( a , name ) :",0.8820016898747209,100.00000000000004
"def _getItemHeight(self, item, ctrl=None): """"""Returns the full height of the item to be inserted in the form"""""" if type(ctrl) == psychopy.visual.TextBox2: return ctrl.size[1] if type(ctrl) == psychopy.visual.Slider: # Set radio button layout <mask>: return 0.03 + ctrl.labelHeight * 3 elif item[""layout""] == ""vert"": # for vertical take into account the nOptions return ctrl.labelHeight * len(item[""options""])",False,"if item [ ""layout"" ] == ""horiz"" :","if item [ ""layout"" ] == ""horizontal"" :",0.8820016898747209,79.10665071754353
"def test_cleanup_params(self, body, rpc_mock): res = self._get_resp_post(body) self.assertEqual(http_client.ACCEPTED, res.status_code) rpc_mock.assert_called_once_with(self.context, mock.ANY) cleanup_request = rpc_mock.call_args[0][1] for key, value in body.items(): if key in (""disabled"", ""is_up""): <mask>: value = value == ""true"" self.assertEqual(value, getattr(cleanup_request, key)) self.assertEqual(self._expected_services(*SERVICES), res.json)",False,if value is not None :,if value :,0.8820016898747209,0.0
"def _read_json_content(self, body_is_optional=False): if ""content-length"" not in self.headers: return self.send_error(411) if not body_is_optional else {} try: content_length = int(self.headers.get(""content-length"")) if content_length == 0 and body_is_optional: return {} request = json.loads(self.rfile.read(content_length).decode(""utf-8"")) <mask>: return request except Exception: logger.exception(""Bad request"") self.send_error(400)",False,"if isinstance ( request , dict ) and ( request or body_is_optional ) :",if request and body_is_optional :,0.8820016898747209,19.0183794978402
"def env_purge_doc(app: Sphinx, env: BuildEnvironment, docname: str) -> None: modules = getattr(env, ""_viewcode_modules"", {}) for modname, entry in list(modules.items()): <mask>: continue code, tags, used, refname = entry for fullname in list(used): if used[fullname] == docname: used.pop(fullname) if len(used) == 0: modules.pop(modname)",False,if entry is False :,if modname == docname :,0.8820016898747209,9.652434877402245
"def frames(self): """"""an array of all the frames (including iframes) in the current window"""""" from thug.DOM.W3C.HTML.HTMLCollection import HTMLCollection frames = set() for frame in self._findAll([""frame"", ""iframe""]): <mask>: from thug.DOM.W3C.Core.DOMImplementation import DOMImplementation DOMImplementation.createHTMLElement(self.window.doc, frame) frames.add(frame._node) return HTMLCollection(self.doc, list(frames))",False,"if not getattr ( frame , ""_node"" , None ) :",if frame . _node is None :,0.8820016898747209,6.866210821983635
"def check(self, **kw): if not kw: return exists(self.strpath) if len(kw) == 1: <mask>: return not kw[""dir""] ^ isdir(self.strpath) if ""file"" in kw: return not kw[""file""] ^ isfile(self.strpath) return super(LocalPath, self).check(**kw)",True,"if ""dir"" in kw :","if ""dir"" in kw :",0.8820016898747209,100.00000000000004
"def __init__(self, folders): self.folders = folders self.duplicates = {} for folder, path in folders.items(): duplicates = [] for other_folder, other_path in folders.items(): <mask>: continue if other_path == path: duplicates.append(other_folder) if len(duplicates): self.duplicates[folder] = duplicates",True,if other_folder == folder :,if other_folder == folder :,0.8820016898747209,100.00000000000004
"def next(self, buf, pos): if pos >= len(buf): return EOF, """", pos mo = self.tokens_re.match(buf, pos) if mo: text = mo.group() type, regexp, test_lit = self.tokens[mo.lastindex - 1] pos = mo.end() <mask>: type = self.literals.get(text, type) return type, text, pos else: c = buf[pos] return self.symbols.get(c, None), c, pos + 1",False,if test_lit :,if regexp and test_lit :,0.8820016898747209,43.47208719449914
"def step(self, action): """"""Repeat action, sum reward, and max over last observations."""""" total_reward = 0.0 done = None for i in range(self._skip): obs, reward, done, info = self.env.step(action) <mask>: self._obs_buffer[0] = obs if i == self._skip - 1: self._obs_buffer[1] = obs total_reward += reward if done: break # Note that the observation on the done=True frame # doesn't matter max_frame = self._obs_buffer.max(axis=0) return max_frame, total_reward, done, info",False,if i == self . _skip - 2 :,if i == self . _skip - 1 :,0.8820016898747209,80.70557274927978
"def convert(self, ctx, argument): arg = argument.replace(""0x"", """").lower() if arg[0] == ""#"": arg = arg[1:] try: value = int(arg, base=16) if not (0 <= value <= 0xFFFFFF): raise BadColourArgument(arg) return discord.Colour(value=value) except ValueError: arg = arg.replace("" "", ""_"") method = getattr(discord.Colour, arg, None) <mask>: raise BadColourArgument(arg) return method()",False,"if arg . startswith ( ""from_"" ) or method is None or not inspect . ismethod ( method ) :",if not method :,0.8820016898747209,0.1954422280040373
"def run(self, **inputs): if self.inputs.copy_inputs: self.inputs.subjects_dir = os.getcwd() <mask>: inputs[""subjects_dir""] = self.inputs.subjects_dir for originalfile in [self.inputs.in_file, self.inputs.in_norm]: copy2subjdir(self, originalfile, folder=""mri"") return super(SegmentCC, self).run(**inputs)",False,"if ""subjects_dir"" in inputs :",if self . inputs . subjects_dir :,0.8820016898747209,21.10534063187263
"def get_queryset(self): if not hasattr(self, ""_queryset""): <mask>: qs = self.queryset else: qs = self.model._default_manager.get_queryset() # If the queryset isn't already ordered we need to add an # artificial ordering here to make sure that all formsets # constructed from this queryset have the same form order. if not qs.ordered: qs = qs.order_by(self.model._meta.pk.name) # Removed queryset limiting here. As per discussion re: #13023 # on django-dev, max_num should not prevent existing # related objects/inlines from being displayed. self._queryset = qs return self._queryset",False,if self . queryset is not None :,if self . queryset :,0.8820016898747209,38.80684294761701
"def visit_simple_stmt(self, node: Node) -> Iterator[Line]: """"""Visit a statement without nested statements."""""" is_suite_like = node.parent and node.parent.type in STATEMENT if is_suite_like: <mask>: yield from self.visit_default(node) else: yield from self.line(+1) yield from self.visit_default(node) yield from self.line(-1) else: if not self.is_pyi or not node.parent or not is_stub_suite(node.parent): yield from self.line() yield from self.visit_default(node)",False,if self . is_pyi and is_stub_body ( node ) :,if self . is_pyi or not node . parent or not is_stub_suite ( node . parent ) :,0.8820016898747209,33.8796999974464
"def rawDataReceived(self, data): if self.timeout > 0: self.resetTimeout() self._pendingSize -= len(data) if self._pendingSize > 0: self._pendingBuffer.write(data) else: passon = b"""" <mask>: data, passon = data[: self._pendingSize], data[self._pendingSize :] self._pendingBuffer.write(data) rest = self._pendingBuffer self._pendingBuffer = None self._pendingSize = None rest.seek(0, 0) self._parts.append(rest.read()) self.setLineMode(passon.lstrip(b""\r\n""))",False,if self . _pendingSize < 0 :,if len ( data ) > self . _pendingSize :,0.8820016898747209,25.965358893403383
"def handle(self, *args, **options): app_name = options.get(""app_name"") job_name = options.get(""job_name"") # hack since we are using job_name nargs='?' for -l to work if app_name and not job_name: job_name = app_name app_name = None if options.get(""list_jobs""): print_jobs(only_scheduled=False, show_when=True, show_appname=True) else: <mask>: print(""Run a single maintenance job. Please specify the name of the job."") return self.runjob(app_name, job_name, options)",False,if not job_name :,if app_name :,0.8820016898747209,34.98330125272253
"def _exportReceived(self, content, error=False, server=None, context={}, **kwargs): if error: <mask>: self.error.emit(content[""message""], True) else: self.error.emit(""Can't export the project from the server"", True) self.finished.emit() return self.finished.emit()",False,if content :,"if ""message"" in content :",0.8820016898747209,14.535768424205482
"def __iter__(self): n = self.n k = self.k j = int(np.ceil(n / k)) for i in range(k): test_index = np.zeros(n, dtype=bool) <mask>: test_index[i * j : (i + 1) * j] = True else: test_index[i * j :] = True train_index = np.logical_not(test_index) yield train_index, test_index",True,if i < k - 1 :,if i < k - 1 :,0.8820016898747209,100.00000000000004
"def addType(self, graphene_type): meta = get_meta(graphene_type) if meta: <mask>: self._typeMap[meta.name] = graphene_type else: raise Exception( ""Type {typeName} already exists in the registry."".format( typeName=meta.name ) ) else: raise Exception(""Cannot add unnamed type or a non-type to registry."")",False,if not graphene_type in self . _typeMap :,if meta . name in self . _typeMap :,0.8820016898747209,48.61555413051454
"def test_len(self): eq = self.assertEqual eq(base64MIME.base64_len(""hello""), len(base64MIME.encode(""hello"", eol=""""))) for size in range(15): if size == 0: bsize = 0 elif size <= 3: bsize = 4 <mask>: bsize = 8 elif size <= 9: bsize = 12 elif size <= 12: bsize = 16 else: bsize = 20 eq(base64MIME.base64_len(""x"" * size), bsize)",True,elif size <= 6 :,elif size <= 6 :,0.8820016898747209,100.00000000000004
"def _asStringList(self, sep=""""): out = [] for item in self._toklist: <mask>: out.append(sep) if isinstance(item, ParseResults): out += item._asStringList() else: out.append(str(item)) return out",False,if out and sep :,if sep :,0.8820016898747209,0.0
"def open_file_input(cli_parsed): files = glob.glob(os.path.join(cli_parsed.d, ""*report.html"")) if len(files) > 0: print(""\n[*] Done! Report written in the "" + cli_parsed.d + "" folder!"") print(""Would you like to open the report now? [Y/n]"") while True: try: response = input().lower() <mask>: return True else: return strtobool(response) except ValueError: print(""Please respond with y or n"") else: print(""[*] No report files found to open, perhaps no hosts were successful"") return False",False,"if response == """" :","if response == ""y"" :",0.8820016898747209,59.4603557501361
"def init_values(self): config = self._raw_config for valname, value in self.overrides.iteritems(): <mask>: realvalname, key = valname.split(""."", 1) config.setdefault(realvalname, {})[key] = value else: config[valname] = value for name in config: if name in self.values: self.__dict__[name] = config[name] del self._raw_config",True,"if ""."" in valname :","if ""."" in valname :",0.8820016898747209,100.00000000000004
"def get_result(self): result_list = [] exc_info = None for f in self.children: try: result_list.append(f.get_result()) except Exception as e: <mask>: exc_info = sys.exc_info() else: if not isinstance(e, self.quiet_exceptions): app_log.error(""Multiple exceptions in yield list"", exc_info=True) if exc_info is not None: raise_exc_info(exc_info) if self.keys is not None: return dict(zip(self.keys, result_list)) else: return list(result_list)",True,if exc_info is None :,if exc_info is None :,0.8820016898747209,100.00000000000004
"def test01e_json(self): ""Testing GeoJSON input/output."" if not GEOJSON: return for g in self.geometries.json_geoms: geom = OGRGeometry(g.wkt) <mask>: self.assertEqual(g.json, geom.json) self.assertEqual(g.json, geom.geojson) self.assertEqual(OGRGeometry(g.wkt), OGRGeometry(geom.json))",False,"if not hasattr ( g , ""not_equal"" ) :",if geom . geojson :,0.8820016898747209,2.564755813286796
"def __init__(self, hub=None): # pylint: disable=unused-argument if resolver._resolver is None: _resolver = resolver._resolver = _DualResolver() <mask>: _resolver.network_resolver.nameservers[:] = config.resolver_nameservers if config.resolver_timeout: _resolver.network_resolver.lifetime = config.resolver_timeout # Different hubs in different threads could be sharing the same # resolver. assert isinstance(resolver._resolver, _DualResolver) self._resolver = resolver._resolver",True,if config . resolver_nameservers :,if config . resolver_nameservers :,0.8820016898747209,100.00000000000004
"def __iadd__(self, term): if isinstance(term, (int, long)): <mask>: _gmp.mpz_add_ui(self._mpz_p, self._mpz_p, c_ulong(term)) return self if -65535 < term < 0: _gmp.mpz_sub_ui(self._mpz_p, self._mpz_p, c_ulong(-term)) return self term = Integer(term) _gmp.mpz_add(self._mpz_p, self._mpz_p, term._mpz_p) return self",False,if 0 <= term < 65536 :,if - 65535 < term < 0 :,0.8820016898747209,14.535768424205482
"def copy(dst, src): for (k, v) in src.iteritems(): <mask>: d = {} dst[k] = d copy(d, v) else: dst[k] = v",True,"if isinstance ( v , dict ) :","if isinstance ( v , dict ) :",0.8820016898747209,100.00000000000004
"def generator(self, data): self.procs = OrderedDict() for task in data: self.recurse_task(task, 0, 0, self.procs) for offset, name, level, pid, ppid, uid, euid, gid in self.procs.values(): <mask>: yield ( 0, [ Address(offset), str(name), str(level), int(pid), int(ppid), int(uid), int(gid), int(euid), ], )",True,if offset :,if offset :,0.8820016898747209,0.0
"def apply(self, db, person): families = person.get_parent_family_handle_list() if families == []: return True for family_handle in person.get_parent_family_handle_list(): family = db.get_family_from_handle(family_handle) <mask>: father_handle = family.get_father_handle() mother_handle = family.get_mother_handle() if not father_handle: return True if not mother_handle: return True return False",True,if family :,if family :,0.8820016898747209,0.0
"def _arctic_task_exec(request): request.start_time = time.time() logging.debug( ""Executing asynchronous request for {}/{}"".format( request.library, request.symbol ) ) result = None try: request.is_running = True <mask>: result = mongo_retry(request.fun)(*request.args, **request.kwargs) else: result = request.fun(*request.args, **request.kwargs) except Exception as e: request.exception = e finally: request.data = result request.end_time = time.time() request.is_running = False return result",False,if request . mongo_retry :,if request . retry :,0.8820016898747209,33.51600230178196
"def _setup_styles(self): for ttype, ndef in self.style: escape = EscapeSequence() <mask>: escape.fg = self._color_index(ndef[""color""]) if ndef[""bgcolor""]: escape.bg = self._color_index(ndef[""bgcolor""]) if self.usebold and ndef[""bold""]: escape.bold = True if self.useunderline and ndef[""underline""]: escape.underline = True self.style_string[str(ttype)] = (escape.color_string(), escape.reset_string())",True,"if ndef [ ""color"" ] :","if ndef [ ""color"" ] :",0.8820016898747209,100.00000000000004
"def process_string(self, remove_repetitions, sequence): string = """" for i, char in enumerate(sequence): if char != self.int_to_char[self.blank_index]: # if this char is a repetition and remove_repetitions=true, # skip. if remove_repetitions and i != 0 and char == sequence[i - 1]: pass <mask>: string += "" "" else: string = string + char return string",False,elif char == self . labels [ self . space_index ] :,elif char == self . int_to_char [ self . blank_index ] :,0.8820016898747209,44.47608928410893
"def arith_expr(self, nodelist): node = self.com_node(nodelist[0]) for i in range(2, len(nodelist), 2): right = self.com_node(nodelist[i]) <mask>: node = Add(node, right, lineno=nodelist[1].context) elif nodelist[i - 1].type == token.MINUS: node = Sub(node, right, lineno=nodelist[1].context) else: raise ValueError(""unexpected token: %s"" % nodelist[i - 1][0]) return node",False,if nodelist [ i - 1 ] . type == token . PLUS :,if nodelist [ i - 1 ] . type == token . MINUS :,0.8820016898747209,86.66415730847507
"def invert_index(cls, index, length): if np.isscalar(index): return length - index elif isinstance(index, slice): start, stop = index.start, index.stop new_start, new_stop = None, None <mask>: new_stop = length - start if stop is not None: new_start = length - stop return slice(new_start - 1, new_stop - 1) elif isinstance(index, Iterable): new_index = [] for ind in index: new_index.append(length - ind) return new_index",True,if start is not None :,if start is not None :,0.8820016898747209,100.00000000000004
"def getRoots(job): if job not in visited: visited.add(job) <mask>: list(map(lambda p: getRoots(p), job._directPredecessors)) else: roots.add(job) # The following call ensures we explore all successor edges. list(map(lambda c: getRoots(c), job._children + job._followOns))",False,if len ( job . _directPredecessors ) > 0 :,if job . _directPredecessors :,0.8820016898747209,24.439253249722206
"def visit_filter_projection(self, node, value): base = self.visit(node[""children""][0], value) if not isinstance(base, list): return None comparator_node = node[""children""][2] collected = [] for element in base: <mask>: current = self.visit(node[""children""][1], element) if current is not None: collected.append(current) return collected",False,"if self . _is_true ( self . visit ( comparator_node , element ) ) :",if comparator_node == element :,0.8820016898747209,5.454673614807693
"def func(x, y): try: if x > y: z = x + 2 * math.sin(y) return z ** 2 <mask>: return 4 else: return 2 ** 3 except ValueError: foo = 0 for i in range(4): foo += i return foo except TypeError: return 42 else: return 33 finally: print(""finished"")",False,elif x == y :,elif x < y :,0.8820016898747209,24.736929544091932
"def set_filter(self, dataset_opt): """"""This function create and set the pre_filter to the obj as attributes"""""" self.pre_filter = None for key_name in dataset_opt.keys(): <mask>: new_name = key_name.replace(""filters"", ""filter"") try: filt = instantiate_filters(getattr(dataset_opt, key_name)) except Exception: log.exception( ""Error trying to create {}, {}"".format( new_name, getattr(dataset_opt, key_name) ) ) continue setattr(self, new_name, filt)",False,"if ""filter"" in key_name :","if key_name . startswith ( ""filters"" ) :",0.8820016898747209,15.580105704117443
"def _add_states_to_lookup( self, trackers_as_states, trackers_as_actions, domain, online=False ): """"""Add states to lookup dict"""""" for states in trackers_as_states: active_form = self._get_active_form_name(states[-1]) <mask>: # modify the states states = self._modified_states(states) feature_key = self._create_feature_key(states) # even if there are two identical feature keys # their form will be the same # because of `active_form_...` feature self.lookup[feature_key] = active_form",False,if active_form and self . _prev_action_listen_in_state ( states [ - 1 ] ) :,if active_form in self . lookup :,0.8820016898747209,6.446447763297845
"def list_loaded_payloads(self): print(helpers.color(""\n [*] Available Payloads:\n"")) lastBase = None x = 1 for name in sorted(self.active_payloads.keys()): parts = name.split(""/"") <mask>: print() lastBase = parts[0] print(""\t%s)\t%s"" % (x, ""{0: <24}"".format(name))) x += 1 print(""\n"") return",False,if lastBase and parts [ 0 ] != lastBase :,if lastBase is None :,0.8820016898747209,7.121297464907233
"def reprSmart(vw, item): ptype = type(item) if ptype is int: if -1024 < item < 1024: return str(item) <mask>: return vw.reprPointer(item) else: return hex(item) elif ptype in (list, tuple): return reprComplex(vw, item) # recurse elif ptype is dict: return ""{%s}"" % "","".join( [""%s:%s"" % (reprSmart(vw, k), reprSmart(vw, v)) for k, v in item.items()] ) else: return repr(item)",False,elif vw . isValidPointer ( item ) :,elif - 128 < item < 128 :,0.8820016898747209,7.267884212102741
"def ConfigSectionMap(section): config = ConfigParser.RawConfigParser() configurations = config_manager() # Class from mkchromecast.config configf = configurations.configf config.read(configf) dict1 = {} options = config.options(section) for option in options: try: dict1[option] = config.get(section, option) <mask>: DebugPrint(""skip: %s"" % option) except: print(""Exception on %s!"" % option) dict1[option] = None return dict1",False,if dict1 [ option ] == - 1 :,if option in dict1 :,0.8820016898747209,5.557509463743763
"def on_success(result): subtasks = {} if result: subtasks = { self.nodes_keys.inverse[s[""node_id""]]: s.get(""subtask_id"") for s in result <mask>: } if subtasks: print(""subtask finished"") self.next() else: print(""waiting for a subtask to finish"") time.sleep(10)",False,"if s . get ( ""status"" ) == ""Failure""","if s [ ""subtask_id"" ] not in self . nodes_keys . inverse",0.8820016898747209,5.401157445454033
"def redirect_aware_commmunicate(p, sys=_sys): """"""Variant of process.communicate that works with in process I/O redirection."""""" assert sys is not None out, err = p.communicate() if redirecting_io(sys=sys): if out: # We don't unicodify in Python2 because sys.stdout may be a # cStringIO.StringIO object, which does not accept Unicode strings out = unicodify(out) sys.stdout.write(out) out = None <mask>: err = unicodify(err) sys.stderr.write(err) err = None return out, err",False,if err :,elif err :,0.8820016898747209,0.0
"def __exit__(self, *args, **kwargs): self._samples_cache = {} if is_validation_enabled() and isinstance(self.prior, dict): extra = set(self.prior) - self._param_hits <mask>: warnings.warn( ""pyro.module prior did not find params ['{}']. "" ""Did you instead mean one of ['{}']?"".format( ""', '"".join(extra), ""', '"".join(self._param_misses) ) ) return super().__exit__(*args, **kwargs)",True,if extra :,if extra :,0.8820016898747209,0.0
def __download_thread(self): while True: <mask>: self.__current_download = self.__queue.get() self.__download_file(self.__current_download) time.sleep(0.1),False,if not self . __queue . empty ( ) :,if self . __current_download is None :,0.8820016898747209,23.708987804092644
"def plot_timer_command(args): import nnabla.monitor as M format_unit = dict( s=""seconds"", m=""minutes"", h=""hours"", d=""days"", ) if not args.ylabel: <mask>: args.ylabel = ""Total elapsed time [{}]"".format(format_unit[args.time_unit]) else: args.ylabel = ""Elapsed time [{}/iter]"".format(format_unit[args.time_unit]) plot_any_command( args, M.plot_time_elapsed, dict(elapsed=args.elapsed, unit=args.time_unit) ) return True",True,if args . elapsed :,if args . elapsed :,0.8820016898747209,100.00000000000004
"def resolve_page(root: ChannelContext[models.MenuItem], info, **kwargs): if root.node.page_id: requestor = get_user_or_app_from_context(info.context) requestor_has_access_to_all = requestor.is_active and requestor.has_perm( PagePermissions.MANAGE_PAGES ) return ( PageByIdLoader(info.context) .load(root.node.page_id) .then( lambda page: page <mask>: else None ) ) return None",False,if requestor_has_access_to_all or page . is_visible,if requestor_has_access_to_all,0.8820016898747209,54.88116360940266
"def find(self, pattern): """"""Find pages in database."""""" results = self._search_keyword(pattern) pat = re.compile(""(.*?)(%s)(.*?)( \(.*\))?$"" % re.escape(pattern), re.I) if results: for name, keyword, url in results: <mask>: keyword = pat.sub( r""\1\033[1;31m\2\033[0m\3\033[1;33m\4\033[0m"", keyword ) print(""%s - %s"" % (keyword, name)) else: raise RuntimeError(""%s: nothing appropriate."" % pattern)",False,if os . isatty ( sys . stdout . fileno ( ) ) :,if url :,0.8820016898747209,0.0
"def _certonly_new_request_common(self, mock_client, args=None): with mock.patch( ""certbot._internal.main._find_lineage_for_domains_and_certname"" ) as mock_renewal: mock_renewal.return_value = (""newcert"", None) with mock.patch(""certbot._internal.main._init_le_client"") as mock_init: mock_init.return_value = mock_client <mask>: args = [] args += ""-d foo.bar -a standalone certonly"".split() self._call(args)",True,if args is None :,if args is None :,0.8820016898747209,100.00000000000004
"def __init__(self, *args, **kw): if len(args) > 1: raise TypeError(""MultiDict can only be called with one positional "" ""argument"") if args: <mask>: items = list(args[0].iteritems()) elif hasattr(args[0], ""items""): items = list(args[0].items()) else: items = list(args[0]) self._items = items else: self._items = [] if kw: self._items.extend(kw.items())",True,"if hasattr ( args [ 0 ] , ""iteritems"" ) :","if hasattr ( args [ 0 ] , ""iteritems"" ) :",0.8820016898747209,100.00000000000004
"def test08_ExceptionTypes(self): self.assertTrue(issubclass(db.DBError, Exception)) for i, j in db.__dict__.items(): <mask>: self.assertTrue(issubclass(j, db.DBError), msg=i) if i not in (""DBKeyEmptyError"", ""DBNotFoundError""): self.assertFalse(issubclass(j, KeyError), msg=i) # This two exceptions have two bases self.assertTrue(issubclass(db.DBKeyEmptyError, KeyError)) self.assertTrue(issubclass(db.DBNotFoundError, KeyError))",False,"if i . startswith ( ""DB"" ) and i . endswith ( ""Error"" ) :","if i not in ( ""DBKeyEmptyError"" , ""DBNotFoundError"" ) :",0.8820016898747209,11.575816250682482
"def _delegate_to_sinks(self, value: Any) -> None: for sink in self._sinks: if isinstance(sink, AgentT): await sink.send(value=value) <mask>: await cast(TopicT, sink).send(value=value) else: await maybe_async(cast(Callable, sink)(value))",False,"elif isinstance ( sink , ChannelT ) :","elif isinstance ( sink , TopicT ) :",0.8820016898747209,59.4603557501361
"def _select_block(str_in, start_tag, end_tag): """"""Select first block delimited by start_tag and end_tag"""""" start_pos = str_in.find(start_tag) if start_pos < 0: raise ValueError(""start_tag not found"") depth = 0 for pos in range(start_pos, len(str_in)): if str_in[pos] == start_tag: depth += 1 <mask>: depth -= 1 if depth == 0: break sel = str_in[start_pos + 1 : pos] return sel",False,elif str_in [ pos ] == end_tag :,elif str_in [ pos + 1 ] == end_tag :,0.8820016898747209,72.41577342575832
"def confirm(request): details = request.session.get(""reauthenticate"") if not details: return redirect(""home"") # Monkey patch request request.user = User.objects.get(pk=details[""user_pk""]) if request.method == ""POST"": confirm_form = PasswordConfirmForm(request, request.POST) <mask>: request.session.pop(""reauthenticate"") request.session[""reauthenticate_done""] = True return redirect(""social:complete"", backend=details[""backend""]) else: confirm_form = PasswordConfirmForm(request) context = {""confirm_form"": confirm_form} context.update(details) return render(request, ""accounts/confirm.html"", context)",True,if confirm_form . is_valid ( ) :,if confirm_form . is_valid ( ) :,0.8820016898747209,100.00000000000004
"def verify_credentials(self): if self.enabled: response = requests.get( ""https://api.exotel.com/v1/Accounts/{sid}"".format(sid=self.account_sid), auth=(self.api_key, self.api_token), ) <mask>: frappe.throw(_(""Invalid credentials""))",True,if response . status_code != 200 :,if response . status_code != 200 :,0.8820016898747209,100.00000000000004
"def pixbufrenderer(self, column, crp, model, it): tok = model.get_value(it, 0) if tok.type == ""class"": icon = ""class"" else: <mask>: icon = ""method_priv"" elif tok.visibility == ""protected"": icon = ""method_prot"" else: icon = ""method"" crp.set_property(""pixbuf"", imagelibrary.pixbufs[icon])",True,"if tok . visibility == ""private"" :","if tok . visibility == ""private"" :",0.8820016898747209,100.00000000000004
"def _omit_keywords(self, context): omitted_kws = 0 for event, elem in context: # Teardowns aren't omitted to allow checking suite teardown status. omit = elem.tag == ""kw"" and elem.get(""type"") != ""teardown"" start = event == ""start"" if omit and start: omitted_kws += 1 if not omitted_kws: yield event, elem <mask>: elem.clear() if omit and not start: omitted_kws -= 1",False,elif not start :,"if event == ""end"" :",0.8820016898747209,5.522397783539471
"def on_double_click(self, event): # TODO: don't act when the click happens below last item path = self.get_selected_path() kind = self.get_selected_kind() name = self.get_selected_name() if kind == ""file"": <mask>: self.open_file(path) else: self.open_path_with_system_app(path) elif kind == ""dir"": self.request_focus_into(path) return ""break""",False,if self . should_open_name_in_thonny ( name ) :,"if name == ""file"" :",0.8820016898747209,2.673705182447105
"def search_cve(db: DatabaseInterface, product: Product) -> dict: result = {} for query_result in db.fetch_multiple(QUERIES[""cve_lookup""]): cve_entry = CveDbEntry(*query_result) <mask>: result[cve_entry.cve_id] = { ""score2"": cve_entry.cvss_v2_score, ""score3"": cve_entry.cvss_v3_score, ""cpe_version"": build_version_string(cve_entry), } return result",False,"if _product_matches_cve ( product , cve_entry ) :",if cve_entry . product == product :,0.8820016898747209,11.708995388048033
"def find_go_files_mtime(app_files): files, mtime = [], 0 for f, mt in app_files.items(): if not f.endswith("".go""): continue <mask>: continue files.append(f) mtime = max(mtime, mt) return files, mtime",False,if APP_CONFIG . nobuild_files . match ( f ) :,"if not mt . endswith ( "".go"" ) :",0.8820016898747209,7.545339613823573
"def wrapper(filename): mtime = getmtime(filename) with lock: <mask>: old_mtime, result = cache.pop(filename) if old_mtime == mtime: # Move to the end cache[filename] = old_mtime, result return result result = function(filename) with lock: cache[filename] = mtime, result # at the end if len(cache) > max_size: cache.popitem(last=False) return result",False,if filename in cache :,if len ( cache ) > max_size :,0.8820016898747209,5.522397783539471
"def Tokenize(s): # type: (str) -> Iterator[Token] for item in TOKEN_RE.findall(s): # The type checker can't know the true type of item! item = cast(TupleStr4, item) <mask>: typ = ""number"" val = item[0] elif item[1]: typ = ""name"" val = item[1] elif item[2]: typ = item[2] val = item[2] elif item[3]: typ = item[3] val = item[3] yield Token(typ, val)",True,if item [ 0 ] :,if item [ 0 ] :,0.8820016898747209,100.00000000000004
"def _show_encoders(self, *args, **kwargs): if issubclass(self.current_module.__class__, BasePayload): encoders = self.current_module.get_encoders() <mask>: headers = (""Encoder"", ""Name"", ""Description"") print_table(headers, *encoders, max_column_length=100) return print_error(""No encoders available"")",True,if encoders :,if encoders :,0.8820016898747209,0.0
"def __init__(self): Builder.__init__(self, commandName=""VCExpress.exe"", formatName=""msvcProject"") for key in [""VS90COMNTOOLS"", ""VC80COMNTOOLS"", ""VC71COMNTOOLS""]: <mask>: self.programDir = os.path.join(os.environ[key], "".."", ""IDE"") if self.programDir is None: for version in [""9.0"", ""8"", "".NET 2003""]: msvcDir = ( ""C:\\Program Files\\Microsoft Visual Studio %s\\Common7\\IDE"" % version ) if os.path.exists(msvcDir): self.programDir = msvcDir",False,if os . environ . has_key ( key ) :,if os . environ . get ( key ) :,0.8820016898747209,53.87551338654778
"def _inner(*args, **kwargs): component_manager = args[0].component_manager for condition_name in condition_names: condition_result, err_msg = component_manager.evaluate_condition(condition_name) <mask>: raise ComponentStartConditionNotMetError(err_msg) if not component_manager.all_components_running(*components): raise ComponentsNotStartedError( f""the following required components have not yet started: {json.dumps(components)}"" ) return method(*args, **kwargs)",False,if not condition_result :,if condition_result is not None :,0.8820016898747209,24.446151121745064
"def _gridconvvalue(self, value): if isinstance(value, (str, _tkinter.Tcl_Obj)): try: svalue = str(value) if not svalue: return None <mask>: return self.tk.getdouble(svalue) else: return self.tk.getint(svalue) except (ValueError, TclError): pass return value",False,"elif ""."" in svalue :",if self . tk . is_double :,0.8820016898747209,5.669791110976001
"def check_songs(): desc = numeric_phrase(""%d song"", ""%d songs"", len(songs)) with Task(_(""Rescan songs""), desc) as task: task.copool(check_songs) for i, song in enumerate(songs): song = song._song <mask>: app.library.reload(song) task.update((float(i) + 1) / len(songs)) yield",False,if song in app . library :,if song . is_loaded ( ) :,0.8820016898747209,11.339582221952005
"def initialize(self): nn.init.xavier_uniform_(self.linear.weight.data) if self.linear.bias is not None: self.linear.bias.data.uniform_(-1.0, 1.0) if self.self_layer: nn.init.xavier_uniform_(self.linear_self.weight.data) <mask>: self.linear_self.bias.data.uniform_(-1.0, 1.0)",True,if self . linear_self . bias is not None :,if self . linear_self . bias is not None :,0.8820016898747209,100.00000000000004
"def test_row(self, row): for idx, test in self.patterns.items(): try: value = row[idx] except IndexError: value = """" result = test(value) <mask>: if result: return not self.inverse # True else: if not result: return self.inverse # False if self.any_match: return self.inverse # False else: return not self.inverse # True",True,if self . any_match :,if self . any_match :,0.8820016898747209,100.00000000000004
"def toterminal(self, tw): for element in self.chain: element[0].toterminal(tw) <mask>: tw.line("""") tw.line(element[2], yellow=True) super(ExceptionChainRepr, self).toterminal(tw)",False,if element [ 2 ] is not None :,"if element [ 1 ] == ""Exception"" :",0.8820016898747209,15.851165692617148
"def runMainLoop(self): """"""The curses gui main loop."""""" # pylint: disable=no-member # # Do NOT change g.app! self.curses_app = LeoApp() stdscr = curses.initscr() if 1: # Must follow initscr. self.dump_keys() try: self.curses_app.run() # run calls CApp.main(), which calls CGui.run(). finally: curses.nocbreak() stdscr.keypad(0) curses.echo() curses.endwin() <mask>: g.pr(""Exiting Leo..."")",False,"if ""shutdown"" in g . app . debug :",if g . verbose :,0.8820016898747209,7.652332131360532
"def test_chunkcoding(self): for native, utf8 in zip(*[StringIO(f).readlines() for f in self.tstring]): u = self.decode(native)[0] self.assertEqual(u, utf8.decode(""utf-8"")) <mask>: self.assertEqual(native, self.encode(u)[0])",False,if self . roundtriptest :,if self . is_unicode :,0.8820016898747209,26.269098944241588
"def reload_sanitize_allowlist(self, explicit=True): self.sanitize_allowlist = [] try: with open(self.sanitize_allowlist_file) as f: for line in f.readlines(): <mask>: self.sanitize_allowlist.append(line.strip()) except OSError: if explicit: log.warning( ""Sanitize log file explicitly specified as '%s' but does not exist, continuing with no tools allowlisted."", self.sanitize_allowlist_file, )",False,"if not line . startswith ( ""#"" ) :","if line . startswith ( ""#"" ) :",0.8820016898747209,81.76129038784515
"def get_all_extensions(subtree=None): if subtree is None: subtree = full_extension_tree() result = [] if isinstance(subtree, dict): for value in subtree.values(): if isinstance(value, dict): result += get_all_extensions(value) <mask>: result += value.extensions elif isinstance(value, (list, tuple)): result += value elif isinstance(subtree, (ContentTypeMapping, ContentTypeDetector)): result = subtree.extensions elif isinstance(subtree, (list, tuple)): result = subtree return result",False,"elif isinstance ( value , ( ContentTypeMapping , ContentTypeDetector ) ) :","elif isinstance ( value , ContentTypeMapping ) :",0.8820016898747209,37.28878639930421
"def _configuration_dict_to_commandlist(name, config_dict): command_list = [""config:%s"" % name] for key, value in config_dict.items(): <mask>: if value: b = ""true"" else: b = ""false"" command_list.append(""%s:%s"" % (key, b)) else: command_list.append(""%s:%s"" % (key, value)) return command_list",False,if type ( value ) is bool :,"if key . startswith ( ""enabled"" ) :",0.8820016898747209,5.934202609760488
"def _RewriteModinfo( self, modinfo, obj_kernel_version, this_kernel_version, info_strings=None, to_remove=None, ): new_modinfo = """" for line in modinfo.split(""\x00""): <mask>: continue if to_remove and line.split(""="")[0] == to_remove: continue if info_strings is not None: info_strings.add(line.split(""="")[0]) if line.startswith(""vermagic""): line = line.replace(obj_kernel_version, this_kernel_version) new_modinfo += line + ""\x00"" return new_modinfo",False,if not line :,"if line . startswith ( ""#"" ) :",0.8820016898747209,5.522397783539471
"def zip_random_open_test(self, f, compression): self.make_test_archive(f, compression) # Read the ZIP archive with zipfile.ZipFile(f, ""r"", compression) as zipfp: zipdata1 = [] with zipfp.open(TESTFN) as zipopen1: while True: read_data = zipopen1.read(randint(1, 1024)) <mask>: break zipdata1.append(read_data) testdata = """".join(zipdata1) self.assertEqual(len(testdata), len(self.data)) self.assertEqual(testdata, self.data)",True,if not read_data :,if not read_data :,0.8820016898747209,100.00000000000004
"def _memoized(*args): now = time.time() try: value, last_update = self.cache[args] age = now - last_update <mask>: self._call_count = 0 raise AttributeError if self.ctl: self._call_count += 1 return value except (KeyError, AttributeError): value = func(*args) if value: self.cache[args] = (value, now) return value except TypeError: return func(*args)",False,if self . _call_count > self . ctl or age > self . ttl :,if age < self . _call_count :,0.8820016898747209,25.707226280596704
"def on_data(res): if terminate.is_set(): return if args.strings and not args.no_content: if type(res) == tuple: f, v = res <mask>: f = f.encode(""utf-8"") if type(v) == unicode: v = v.encode(""utf-8"") self.success(""{}: {}"".format(f, v)) elif not args.content_only: self.success(res) else: self.success(res)",True,if type ( f ) == unicode :,if type ( f ) == unicode :,0.8820016898747209,100.00000000000004
"def _finalize_setup_keywords(self): for ep in pkg_resources.iter_entry_points(""distutils.setup_keywords""): value = getattr(self, ep.name, None) <mask>: ep.require(installer=self.fetch_build_egg) ep.load()(self, ep.name, value)",True,if value is not None :,if value is not None :,0.8820016898747209,100.00000000000004
"def test_attributes_types(self): if not self.connection.strategy.pooled: <mask>: self.connection.refresh_server_info() self.assertEqual( type(self.connection.server.schema.attribute_types[""cn""]), AttributeTypeInfo )",False,if not self . connection . server . info :,"if self . connection . server . schema . attribute_types [ ""cn"" ] is None :",0.8820016898747209,25.34743707366162
"def to_key(literal_or_identifier): """"""returns string representation of this object"""""" if literal_or_identifier[""type""] == ""Identifier"": return literal_or_identifier[""name""] elif literal_or_identifier[""type""] == ""Literal"": k = literal_or_identifier[""value""] if isinstance(k, float): return unicode(float_repr(k)) elif ""regex"" in literal_or_identifier: return compose_regex(k) <mask>: return ""true"" if k else ""false"" elif k is None: return ""null"" else: return unicode(k)",True,"elif isinstance ( k , bool ) :","elif isinstance ( k , bool ) :",0.8820016898747209,100.00000000000004
"def list2rec(x, test=False): if test: vid = ""{}_{:06d}_{:06d}"".format(x[0], int(x[1]), int(x[2])) label = -1 # label unknown return vid, label else: vid = ""{}_{:06d}_{:06d}"".format(x[1], int(x[2]), int(x[3])) <mask>: vid = ""{}/{}"".format(convert_label(x[0]), vid) else: assert level == 1 label = class_mapping[convert_label(x[0])] return vid, label",False,if level == 2 :,if level == 0 :,0.8820016898747209,53.7284965911771
"def _expand_env(self, snapcraft_yaml): environment_keys = [""name"", ""version""] for key in snapcraft_yaml: <mask>: continue replacements = environment_to_replacements( get_snapcraft_global_environment(self.project) ) snapcraft_yaml[key] = replace_attr(snapcraft_yaml[key], replacements) return snapcraft_yaml",False,if any ( ( key == env_key for env_key in environment_keys ) ) :,if key in environment_keys :,0.8820016898747209,9.050415858572288
"def enableCtrls(self): # Check if each ctrl has a requirement or an incompatibility, # look it up, and enable/disable if so for data in self.storySettingsData: name = data[""name""] <mask>: if ""requires"" in data: set = self.getSetting(data[""requires""]) for i in self.ctrls[name]: i.Enable(set not in [""off"", ""false"", ""0""])",True,if name in self . ctrls :,if name in self . ctrls :,0.8820016898747209,100.00000000000004
"def __init__(self, *args, **kwargs): super(ChallengePhaseCreateSerializer, self).__init__(*args, **kwargs) context = kwargs.get(""context"") if context: challenge = context.get(""challenge"") <mask>: kwargs[""data""][""challenge""] = challenge.pk test_annotation = context.get(""test_annotation"") if test_annotation: kwargs[""data""][""test_annotation""] = test_annotation",True,if challenge :,if challenge :,0.8820016898747209,0.0
def set_inactive(self): for title in self.gramplet_map: if self.gramplet_map[title].pui: <mask>: self.gramplet_map[title].pui.active = False,False,"if self . gramplet_map [ title ] . gstate != ""detached"" :",if self .gramplet_map [ title ] . ui . active :,0.8820016898747209,55.94114771766143
"def authenticate(username, password): try: u = User.objects.get(username=username) <mask>: userLogger.info(""User logged in : %s"", username) return u else: userLogger.warn(""Attempt to log in to : %s"", username) return False except DoesNotExist: return False",False,"if check_password_hash ( u . password , password ) :",if u . is_authenticated ( password ) :,0.8820016898747209,14.807095745957167
def _check_date(self): if not self.value: return None if not self.allow_date_in_past: if self.value < self.date_or_datetime().today(): <mask>: self.value = self.date_or_datetime().today() else: self.value = self.date_or_datetime().today() + datetime.timedelta(1),False,if self . allow_todays_date :,if self . allow_date_in_past :,0.8820016898747209,41.72261448611506
"def update(self, E=None, **F): if E: <mask>: # Update with `E` dictionary for k in E: self[k] = E[k] else: # Update with `E` items for (k, v) in E: self[k] = v # Update with `F` dictionary for k in F: self[k] = F[k]",False,"if hasattr ( E , ""keys"" ) :","if isinstance ( E , dict ) :",0.8820016898747209,21.069764742263047
"def _get_quota_availability(self): quotas_ok = defaultdict(int) qa = QuotaAvailability() qa.queue(*[k for k, v in self._quota_diff.items() if v > 0]) qa.compute(now_dt=self.now_dt) for quota, count in self._quota_diff.items(): <mask>: quotas_ok[quota] = 0 break avail = qa.results[quota] if avail[1] is not None and avail[1] < count: quotas_ok[quota] = min(count, avail[1]) else: quotas_ok[quota] = count return quotas_ok",False,if count <= 0 :,if count == 0 :,0.8820016898747209,37.99178428257963
"def gen_env_vars(): for fd_id, fd in zip(STDIO_DESCRIPTORS, (stdin, stdout, stderr)): is_atty = fd.isatty() yield (cls.TTY_ENV_TMPL.format(fd_id), cls.encode_env_var_value(int(is_atty))) <mask>: yield (cls.TTY_PATH_ENV.format(fd_id), os.ttyname(fd.fileno()) or b"""")",True,if is_atty :,if is_atty :,0.8820016898747209,100.00000000000004
"def _convertDict(self, d): r = {} for k, v in d.items(): if isinstance(v, bytes): v = str(v, ""utf-8"") elif isinstance(v, list) or isinstance(v, tuple): v = self._convertList(v) elif isinstance(v, dict): v = self._convertDict(v) <mask>: k = str(k, ""utf-8"") r[k] = v return r",False,"if isinstance ( k , bytes ) :","elif isinstance ( k , bytes ) :",0.8820016898747209,84.08964152537145
"def get_attribute_value(self, nodeid, attr): with self._lock: self.logger.debug(""get attr val: %s %s"", nodeid, attr) if nodeid not in self._nodes: dv = ua.DataValue() dv.StatusCode = ua.StatusCode(ua.StatusCodes.BadNodeIdUnknown) return dv node = self._nodes[nodeid] <mask>: dv = ua.DataValue() dv.StatusCode = ua.StatusCode(ua.StatusCodes.BadAttributeIdInvalid) return dv attval = node.attributes[attr] if attval.value_callback: return attval.value_callback() return attval.value",True,if attr not in node . attributes :,if attr not in node . attributes :,0.8820016898747209,100.00000000000004
"def conninfo_parse(dsn): ret = {} length = len(dsn) i = 0 while i < length: <mask>: i += 1 continue param_match = PARAMETER_RE.match(dsn[i:]) if not param_match: return param = param_match.group(1) i += param_match.end() if i >= length: return value, end = read_param_value(dsn[i:]) if value is None: return i += end ret[param] = value return ret",False,if dsn [ i ] . isspace ( ) :,if not dsn [ i ] :,0.8820016898747209,29.640095400745814
"def connect(self, buttons): for button in buttons: assert button is not None handled = False for handler_idx in range(0, len(self.__signal_handlers)): (obj_class, signal, handler, handler_id) = self.__signal_handlers[ handler_idx ] <mask>: handler_id = button.connect(signal, handler) handled = True self.__signal_handlers[handler_idx] = ( obj_class, signal, handler, handler_id, ) assert handled",False,"if isinstance ( button , obj_class ) :",if handler_id is None :,0.8820016898747209,5.630400552901077
"def _parse_display(display): """"""Parse an X11 display value"""""" try: host, dpynum = display.rsplit("":"", 1) if host.startswith(""["") and host.endswith(""]""): host = host[1:-1] idx = dpynum.find(""."") <mask>: screen = int(dpynum[idx + 1 :]) dpynum = dpynum[:idx] else: screen = 0 except (ValueError, UnicodeEncodeError): raise ValueError(""Invalid X11 display"") from None return host, dpynum, screen",True,if idx >= 0 :,if idx >= 0 :,0.8820016898747209,100.00000000000004
"def delete_all(path): ppath = os.getcwd() os.chdir(path) for fn in glob.glob(""*""): fn_full = os.path.join(path, fn) if os.path.isdir(fn): delete_all(fn_full) <mask>: os.remove(fn_full) elif fn.endswith("".md""): os.remove(fn_full) elif DELETE_ALL_OLD: os.remove(fn_full) os.chdir(ppath) os.rmdir(path)",False,"elif fn . endswith ( "".png"" ) :","elif fn . endswith ( "".md"" ) :",0.8820016898747209,70.16879391277372
"def _sync_get(self, identifier, *args, **kw): self._mutex.acquire() try: try: <mask>: return self._values[identifier] else: self._values[identifier] = value = self.creator(identifier, *args, **kw) return value except KeyError: self._values[identifier] = value = self.creator(identifier, *args, **kw) return value finally: self._mutex.release()",True,if identifier in self . _values :,if identifier in self . _values :,0.8820016898747209,100.00000000000004
"def _query_fd(self): if self.stream is None: self._last_stat = None, None else: try: st = os.stat(self._filename) except OSError: e = sys.exc_info()[1] <mask>: raise self._last_stat = None, None else: self._last_stat = st[stat.ST_DEV], st[stat.ST_INO]",True,if e . errno != errno . ENOENT :,if e . errno != errno . ENOENT :,0.8820016898747209,100.00000000000004
"def get_place_name(self, place_handle): """"""Obtain a place name"""""" text = """" if place_handle: place = self.dbstate.db.get_place_from_handle(place_handle) <mask>: place_title = place_displayer.display(self.dbstate.db, place) if place_title != """": if len(place_title) > 25: text = place_title[:24] + ""..."" else: text = place_title return text",True,if place :,if place :,0.8820016898747209,0.0
"def test_decoder_state(self): # Check that getstate() and setstate() handle the state properly u = ""abc123"" for encoding in all_unicode_encodings: <mask>: self.check_state_handling_decode(encoding, u, u.encode(encoding)) self.check_state_handling_encode(encoding, u, u.encode(encoding))",False,if encoding not in broken_unicode_with_stateful :,if encoding in self . _state_handling_decode :,0.8820016898747209,9.263986277915475
"def cleanup(self): if os.path.exists(self.meta_gui_dir): for f in os.listdir(self.meta_gui_dir): <mask>: os.remove(os.path.join(self.meta_gui_dir, f))",False,"if os . path . splitext ( f ) [ 1 ] == "".desktop"" :","if os . path . exists ( os . path . join ( self . meta_gui_dir , f ) ) :",0.8820016898747209,17.729842264695016
"def _have_applied_incense(self): for applied_item in inventory.applied_items().all(): self.logger.info(applied_item) <mask>: mins = format_time(applied_item.expire_ms * 1000) self.logger.info( ""Not applying incense, currently active: %s, %s minutes remaining"", applied_item.item.name, mins, ) return True else: self.logger.info("""") return False return False",False,if applied_item . expire_ms > 0 :,if applied_item . active :,0.8820016898747209,36.337289265247364
"def get_closest_point(self, point): point = to_point(point) cp, cd = None, None for p0, p1 in iter_pairs(self.pts, self.connected): diff = p1 - p0 l = diff.length d = diff / l pp = p0 + d * max(0, min(l, (point - p0).dot(d))) dist = (point - pp).length <mask>: cp, cd = pp, dist return cp",False,if not cp or dist < cd :,if dist > cp :,0.8820016898747209,8.290829875388036
"def process_return(lines): for line in lines: m = re.fullmatch(r""(?P<param>\w+)\s+:\s+(?P<type>[\w.]+)"", line) <mask>: # Once this is in scanpydoc, we can use the fancy hover stuff yield f'**{m[""param""]}** : :class:`~{m[""type""]}`' else: yield line",True,if m :,if m :,0.8820016898747209,0.0
"def _classify(nodes_by_level): missing, invalid, downloads = [], [], [] for level in nodes_by_level: for node in level: if node.binary == BINARY_MISSING: missing.append(node) <mask>: invalid.append(node) elif node.binary in (BINARY_UPDATE, BINARY_DOWNLOAD): downloads.append(node) return missing, invalid, downloads",True,elif node . binary == BINARY_INVALID :,elif node . binary == BINARY_INVALID :,0.8820016898747209,100.00000000000004
"def safe_parse_date(date_hdr): """"""Parse a Date: or Received: header into a unix timestamp."""""" try: <mask>: date_hdr = date_hdr.split("";"")[-1].strip() msg_ts = long(rfc822.mktime_tz(rfc822.parsedate_tz(date_hdr))) if (msg_ts > (time.time() + 24 * 3600)) or (msg_ts < 1): return None else: return msg_ts except (ValueError, TypeError, OverflowError): return None",True,"if "";"" in date_hdr :","if "";"" in date_hdr :",0.8820016898747209,100.00000000000004
"def _on_change(self): changed = False self.save() for key, value in self.data.items(): if isinstance(value, bool): if value: changed = True break if isinstance(value, int): <mask>: changed = True break elif value is None: continue elif len(value) != 0: changed = True break self._reset_button.disabled = not changed",False,if value != 1 :,if value != 0 :,0.8820016898747209,53.7284965911771
"def _rewrite_prepend_append(self, string, prepend, append=None): if append is None: append = prepend if not isinstance(string, StringElem): string = StringElem(string) string.sub.insert(0, prepend) if unicode(string).endswith(u""\n""): # Try and remove the last character from the tree try: lastnode = string.flatten()[-1] <mask>: lastnode.sub[-1] = lastnode.sub[-1].rstrip(u""\n"") except IndexError: pass string.sub.append(append + u""\n"") else: string.sub.append(append) return string",False,"if isinstance ( lastnode . sub [ - 1 ] , unicode ) :","if lastnode . sub [ - 1 ] . startswith ( u""\n"" ) :",0.8820016898747209,38.05371078682543
"def parse_indentless_sequence_entry(self): if self.check_token(BlockEntryToken): token = self.get_token() <mask>: self.states.append(self.parse_indentless_sequence_entry) return self.parse_block_node() else: self.state = self.parse_indentless_sequence_entry return self.process_empty_scalar(token.end_mark) token = self.peek_token() event = SequenceEndEvent(token.start_mark, token.start_mark) self.state = self.states.pop() return event",False,"if not self . check_token ( BlockEntryToken , KeyToken , ValueToken , BlockEndToken ) :","if token . end_mark == ""block"" :",0.8820016898747209,3.3383922484634225
"def walk_directory(directory, verbose=False): """"""Iterates a directory's text files and their contents."""""" for dir_path, _, filenames in os.walk(directory): for filename in filenames: file_path = os.path.join(dir_path, filename) if os.path.isfile(file_path) and not filename.startswith("".""): with io.open(file_path, ""r"", encoding=""utf-8"") as file: <mask>: print(""Reading {}"".format(filename)) doc_text = file.read() yield filename, doc_text",True,if verbose :,if verbose :,0.8820016898747209,0.0
"def set_bounds(self, x, y, width, height): if self.native: # Root level widgets may require vertical adjustment to # account for toolbars, etc. <mask>: vertical_shift = self.frame.vertical_shift else: vertical_shift = 0 self.native.Size = Size(width, height) self.native.Location = Point(x, y + vertical_shift)",False,if self . interface . parent is None :,if self . frame :,0.8820016898747209,19.199242796476852
"def _check_x11(self, command=None, *, exc=None, exit_status=None, **kwargs): """"""Check requesting X11 forwarding"""""" with (yield from self.connect()) as conn: <mask>: with self.assertRaises(exc): yield from _create_x11_process(conn, command, **kwargs) else: proc = yield from _create_x11_process(conn, command, **kwargs) yield from proc.wait() self.assertEqual(proc.exit_status, exit_status) yield from conn.wait_closed()",True,if exc :,if exc :,0.8820016898747209,0.0
"def repr(self): try: <mask>: from infogami.infobase.utils import prepr return prepr(self.obj) else: return repr(self.obj) except: return ""failed"" return render_template(""admin/memory/object"", self.obj)",False,"if isinstance ( self . obj , ( dict , web . threadeddict ) ) :","if hasattr ( self . obj , ""prepr"" ) :",0.8820016898747209,27.013687588206388
"def add(self, tag, values): if tag not in self.different: if tag not in self: self[tag] = values <mask>: self.different.add(tag) self[tag] = [""""] self.counts[tag] += 1",False,elif self [ tag ] != values :,if tag not in self . different :,0.8820016898747209,6.413885305524152
"def _on_geturl(self, event): selected = self._status_list.get_selected() if selected != -1: object_id = self._status_list.GetItemData(selected) download_item = self._download_list.get_item(object_id) url = download_item.url <mask>: clipdata = wx.TextDataObject() clipdata.SetText(url) wx.TheClipboard.Open() wx.TheClipboard.SetData(clipdata) wx.TheClipboard.Close()",False,if not wx . TheClipboard . IsOpened ( ) :,if url :,0.8820016898747209,0.0
"def escape2null(text): """"""Return a string with escape-backslashes converted to nulls."""""" parts = [] start = 0 while True: found = text.find(""\\"", start) <mask>: parts.append(text[start:]) return """".join(parts) parts.append(text[start:found]) parts.append(""\x00"" + text[found + 1 : found + 2]) start = found + 2 # skip character after escape",True,if found == - 1 :,if found == - 1 :,0.8820016898747209,100.00000000000004
def _process_inner_views(self): for view in self.baseviews: for inner_class in view.get_uninit_inner_views(): for v in self.baseviews: <mask>: view.get_init_inner_views().append(v),False,"if isinstance ( v , inner_class ) and v not in view . get_init_inner_views ( ) :",if v . get_class ( ) == inner_class :,0.8820016898747209,9.970064408326126
"def TryMerge(self, d): while d.avail() > 0: tt = d.getVarInt32() if tt == 10: self.set_url(d.getPrefixedString()) continue <mask>: self.set_app_version_id(d.getPrefixedString()) continue if tt == 26: self.set_method(d.getPrefixedString()) continue if tt == 34: self.set_queue(d.getPrefixedString()) continue if tt == 0: raise ProtocolBuffer.ProtocolBufferDecodeError d.skipData(tt)",True,if tt == 18 :,if tt == 18 :,0.8820016898747209,100.00000000000004
"def test_sample_output(): comment = ""SAMPLE OUTPUT"" skip_files = [""__init__.py""] errors = [] for _file in sorted(MODULE_PATH.iterdir()): if _file.suffix == "".py"" and _file.name not in skip_files: with _file.open() as f: <mask>: errors.append((comment, _file)) if errors: line = ""Missing sample error(s) detected!\n\n"" for error in errors: line += ""`{}` is not in module `{}`\n"".format(*error) print(line[:-1]) assert False",False,if comment not in f . read ( ) :,if f . read ( ) != 0 :,0.8820016898747209,42.7287006396234
"def _get_planner(name, path, source): for klass in _planners: <mask>: LOG.debug(""%r accepted %r (filename %r)"", klass, name, path) return klass LOG.debug(""%r rejected %r"", klass, name) raise ansible.errors.AnsibleError(NO_METHOD_MSG + repr(invocation))",False,"if klass . detect ( path , source ) :",if klass . is_source ( source ) :,0.8820016898747209,27.301208627090666
"def _to_string_infix(self, ostream, idx, verbose): if verbose: ostream.write("" , "") else: hasConst = not ( self._const.__class__ in native_numeric_types and self._const == 0 ) <mask>: idx -= 1 _l = self._coef[id(self._args[idx])] _lt = _l.__class__ if _lt is _NegationExpression or (_lt in native_numeric_types and _l < 0): ostream.write("" - "") else: ostream.write("" + "")",True,if hasConst :,if hasConst :,0.8820016898747209,0.0
"def cluster_info_query(self): if self._major_version >= 90600: extra = ( "", CASE WHEN latest_end_lsn IS NULL THEN NULL ELSE received_tli END,"" "" slot_name, conninfo FROM pg_catalog.pg_stat_get_wal_receiver()"" ) <mask>: extra = ""timeline_id"" + extra + "", pg_catalog.pg_control_checkpoint()"" else: extra = ""0"" + extra else: extra = ""0, NULL, NULL, NULL"" return (""SELECT "" + self.TL_LSN + "", {2}"").format( self.wal_name, self.lsn_name, extra )",False,"if self . role == ""standby_leader"" :",if self . _major_version >= 90600 :,0.8820016898747209,15.148694266083963
"def __init__(self, *args, **kwargs): self.country = kwargs.pop(""country"") self.fields_needed = kwargs.pop(""fields_needed"", []) super(DynamicManagedAccountForm, self).__init__(*args, **kwargs) # build our form using the country specific fields and falling # back to our default set for f in self.fields_needed: <mask>: # pragma: no branch field_name, field = FIELDS_BY_COUNTRY[self.country][f] self.fields[field_name] = field",False,"if f in FIELDS_BY_COUNTRY . get ( self . country , { } ) :",if f in FIELDS_BY_COUNTRY :,0.8820016898747209,29.26985560739962
"def delete_map(self, query=None): query_map = self.interpolated_map(query=query) for alias, drivers in six.iteritems(query_map.copy()): for driver, vms in six.iteritems(drivers.copy()): for vm_name, vm_details in six.iteritems(vms.copy()): <mask>: query_map[alias][driver].pop(vm_name) if not query_map[alias][driver]: query_map[alias].pop(driver) if not query_map[alias]: query_map.pop(alias) return query_map",False,"if vm_details == ""Absent"" :","if vm_details [ ""vm_name"" ] == vm_name :",0.8820016898747209,18.842393723950345
"def on_strokes_edited(self): strokes = self._strokes() if strokes: translation = self._engine.raw_lookup(strokes) <mask>: fmt = _(""{strokes} maps to {translation}"") else: fmt = _(""{strokes} is not in the dictionary"") info = self._format_label(fmt, (strokes,), translation) else: info = """" self.strokes_info.setText(info)",False,if translation is not None :,if translation :,0.8820016898747209,0.0
"def release(self): tid = _thread.get_ident() with self.lock: if self.owner != tid: raise RuntimeError(""cannot release un-acquired lock"") assert self.count > 0 self.count -= 1 <mask>: self.owner = None if self.waiters: self.waiters -= 1 self.wakeup.release()",True,if self . count == 0 :,if self . count == 0 :,0.8820016898747209,100.00000000000004
"def _cat_blob(self, gcs_uri): """""":py:meth:`cat_file`, minus decompression."""""" blob = self._get_blob(gcs_uri) if not blob: return # don't cat nonexistent files start = 0 while True: end = start + _CAT_CHUNK_SIZE try: chunk = blob.download_as_string(start=start, end=end) except google.api_core.exceptions.RequestRangeNotSatisfiable: return yield chunk <mask>: return start = end",False,if len ( chunk ) < _CAT_CHUNK_SIZE :,if not chunk :,0.8820016898747209,2.215745752614824
"def device_iter(**kwargs): for dev in backend.enumerate_devices(): d = Device(dev, backend) tests = (val == _try_getattr(d, key) for key, val in kwargs.items()) <mask>: yield d",False,if _interop . _all ( tests ) and ( custom_match is None or custom_match ( d ) ) :,if tests :,0.8820016898747209,0.0
"def _get_vtkjs(self): if self._vtkjs is None and self.object is not None: if isinstance(self.object, string_types) and self.object.endswith("".vtkjs""): if isfile(self.object): with open(self.object, ""rb"") as f: vtkjs = f.read() else: data_url = urlopen(self.object) vtkjs = data_url.read() <mask>: vtkjs = self.object.read() self._vtkjs = vtkjs return self._vtkjs",False,"elif hasattr ( self . object , ""read"" ) :","if isinstance ( self . object , bytes ) :",0.8820016898747209,36.99033744491308
"def _execute_with_error(command, error, message): try: cli.invocation = cli.invocation_cls( cli_ctx=cli, parser_cls=cli.parser_cls, commands_loader_cls=cli.commands_loader_cls, help_cls=cli.help_cls, ) cli.invocation.execute(command.split()) except CLIError as ex: <mask>: raise AssertionError( ""{}\nExpected: {}\nActual: {}"".format(message, error, ex) ) return except Exception as ex: raise ex raise AssertionError(""exception not raised for '{0}'"".format(message))",False,if error not in str ( ex ) :,if error :,0.8820016898747209,0.0
"def ray_intersection(self, p, line): p = Vector(center(line.sites)) min_r = BIG_FLOAT nearest = None for v_i, v_j in self.edges: bound = LineEquation2D.from_two_points(v_i, v_j) intersection = bound.intersect_with_line(line) if intersection is not None: r = (p - intersection).length # info(""INT: [%s - %s] X [%s] => %s (%s)"", v_i, v_j, line, intersection, r) <mask>: nearest = intersection min_r = r return nearest",True,if r < min_r :,if r < min_r :,0.8820016898747209,100.00000000000004
"def CalculateChecksum(data): # The checksum is just a sum of all the bytes. I swear. if isinstance(data, bytearray): total = sum(data) elif isinstance(data, bytes): <mask>: # Python 2 bytes (str) index as single-character strings. total = sum(map(ord, data)) else: # Python 3 bytes index as numbers (and PY2 empty strings sum() to 0) total = sum(data) else: # Unicode strings (should never see?) total = sum(map(ord, data)) return total & 0xFFFFFFFF",False,"if data and isinstance ( data [ 0 ] , bytes ) :",if PY2 :,0.8820016898747209,0.0
"def __mul__(self, other: Union[""Tensor"", float]) -> ""Tensor"": if isinstance(other, Tensor): <mask>: errstr = ( f""Given backens are inconsistent. Found '{self.backend.name}'"" f""and '{other.backend.name}'"" ) raise ValueError(errstr) other = other.array array = self.backend.multiply(self.array, other) return Tensor(array, backend=self.backend)",True,if self . backend . name != other . backend . name :,if self . backend . name != other . backend . name :,0.8820016898747209,100.00000000000004
"def next_item(self, direction): """"""Selects next menu item, based on self._direction"""""" start, i = -1, 0 try: start = self.items.index(self._selected) i = start + direction except: pass while True: <mask>: # Cannot find valid menu item self.select(start) break if i >= len(self.items): i = 0 continue if i < 0: i = len(self.items) - 1 continue if self.select(i): break i += direction if start < 0: start = 0",False,if i == start :,if i == - 1 :,0.8820016898747209,43.47208719449914
"def resolve_none(self, data): # replace None to '_' for tok_idx in range(len(data)): for feat_idx in range(len(data[tok_idx])): <mask>: data[tok_idx][feat_idx] = ""_"" return data",False,if data [ tok_idx ] [ feat_idx ] is None :,if data [ tok_idx ] [ feat_idx ] == None :,0.8820016898747209,76.70387248467661
"def distinct(expr, *on): fields = frozenset(expr.fields) _on = [] append = _on.append for n in on: if isinstance(n, Field): if n._child.isidentical(expr): n = n._name else: raise ValueError(""{0} is not a field of {1}"".format(n, expr)) if not isinstance(n, _strtypes): raise TypeError(""on must be a name or field, not: {0}"".format(n)) <mask>: raise ValueError(""{0} is not a field of {1}"".format(n, expr)) append(n) return Distinct(expr, tuple(_on))",False,elif n not in fields :,if n not in fields :,0.8820016898747209,75.98356856515926
"def TryMerge(self, d): while d.avail() > 0: tt = d.getVarInt32() <mask>: length = d.getVarInt32() tmp = ProtocolBuffer.Decoder(d.buffer(), d.pos(), d.pos() + length) d.skip(length) self.mutable_cost().TryMerge(tmp) continue if tt == 24: self.add_version(d.getVarInt64()) continue if tt == 0: raise ProtocolBuffer.ProtocolBufferDecodeError d.skipData(tt)",False,if tt == 10 :,if tt == 16 :,0.8820016898747209,53.7284965911771
"def func_std_string(func_name): # match what old profile produced if func_name[:2] == (""~"", 0): # special case for built-in functions name = func_name[2] <mask>: return ""{%s}"" % name[1:-1] else: return name else: return ""%s:%d(%s)"" % func_name",False,"if name . startswith ( ""<"" ) and name . endswith ( "">"" ) :","if name [ : 1 ] == "" "" :",0.8820016898747209,4.5544734701106
"def f(): try: # Intra-buffer read then buffer-flushing read for n in cycle([1, 19]): s = bufio.read(n) <mask>: break # list.append() is atomic results.append(s) except Exception as e: errors.append(e) raise",True,if not s :,if not s :,0.8820016898747209,100.00000000000004
"def stop(self): # Try to shut the connection down, but if we get any sort of # errors, go ahead and ignore them.. as we're shutting down anyway try: self.rpcserver.stop() if self.backend_rpcserver: self.backend_rpcserver.stop() <mask>: self.cluster_rpcserver.stop() except Exception: pass if self.coordination: try: coordination.COORDINATOR.stop() except Exception: pass super(Service, self).stop(graceful=True)",True,if self . cluster_rpcserver :,if self . cluster_rpcserver :,0.8820016898747209,100.00000000000004
"def download(cls, architecture, path=""./""): if cls.sanity_check(architecture): architecture_file = download_file( cls.architecture_map[architecture], directory=path ) <mask>: return None print(""Coreml model {} is saved in [{}]"".format(architecture, path)) return architecture_file else: return None",False,if not architecture_file :,if architecture_file is None :,0.8820016898747209,27.77619034011791
"def opps_output_converter(kpt_list): kpts = [] mpii_keys = to_opps_converter.keys() for mpii_idx in range(0, 16): <mask>: model_idx = to_opps_converter[mpii_idx] x, y = kpt_list[model_idx] if x < 0 or y < 0: kpts += [0.0, 0.0, -1.0] else: kpts += [x, y, 1.0] else: kpts += [0.0, 0.0, -1.0] return kpts",True,if mpii_idx in mpii_keys :,if mpii_idx in mpii_keys :,0.8820016898747209,100.00000000000004
"def _get_headers(self, headers=None): request_headers = headers or {} # Auth headers if access_token is present if self._client.client.config: config = self._client.client.config if ""Authorization"" not in request_headers and config.token: request_headers.update( { ""Authorization"": ""{} {}"".format( config.authentication_type, config.token ) } ) <mask>: request_headers.update({config.header: config.header_service}) return request_headers",False,if config . header and config . header_service :,if config . header_service :,0.8820016898747209,56.47181220077595
"def get_last_traded_prices(cls, trading_pairs: List[str]) -> Dict[str, float]: results = dict() async with aiohttp.ClientSession() as client: resp = await client.get(f""{constants.REST_URL}/tickers"") resp_json = await resp.json() for trading_pair in trading_pairs: resp_record = [ o for o in resp_json <mask>: ][0] results[trading_pair] = float(resp_record[""price""]) return results",False,"if o [ ""symbol"" ] == convert_to_exchange_trading_pair ( trading_pair )","if o [ ""id"" ] == trading_pair",0.8820016898747209,20.859950945425147
"def reset_two_factor_hotp(): uid = request.form[""uid""] otp_secret = request.form.get(""otp_secret"", None) if otp_secret: user = Journalist.query.get(uid) <mask>: return render_template(""admin_edit_hotp_secret.html"", uid=uid) db.session.commit() return redirect(url_for(""admin.new_user_two_factor"", uid=uid)) else: return render_template(""admin_edit_hotp_secret.html"", uid=uid)",False,"if not validate_hotp_secret ( user , otp_secret ) :",if user and user . two_factor_hotp :,0.8820016898747209,6.857388492518904
"def ctx_for_video(self, vurl): ""Get a context dict for a given video URL"" ctx = self.get_context_dict() for portal, match, context_fn in self.PORTALS: <mask>: try: ctx.update(context_fn(vurl)) ctx[""portal""] = portal break except AttributeError: continue return ctx",False,if match . search ( vurl ) :,if match :,0.8820016898747209,0.0
"def get(self): name = request.args.get(""filename"") if name is not None: opts = dict() opts[""type""] = ""episode"" result = guessit(name, options=opts) res = dict() if ""episode"" in result: res[""episode""] = result[""episode""] else: res[""episode""] = 0 if ""season"" in result: res[""season""] = result[""season""] else: res[""season""] = 0 <mask>: res[""subtitle_language""] = str(result[""subtitle_language""]) return jsonify(data=res) else: return """", 400",True,"if ""subtitle_language"" in result :","if ""subtitle_language"" in result :",0.8820016898747209,100.00000000000004
"def package_files(package_path, directory_name): paths = [] directory_path = os.path.join(package_path, directory_name) for (path, directories, filenames) in os.walk(directory_path): relative_path = os.path.relpath(path, package_path) for filename in filenames: <mask>: continue paths.append(os.path.join(relative_path, filename)) return paths",False,"if filename [ 0 ] == ""."" :","if filename . startswith ( "".py"" ) :",0.8820016898747209,11.731175160263996
"def parse_simple(d, data): units = {} for v in data[d]: key = v[""name""] if not key: continue key_to_insert = make_key(key) <mask>: index = 2 tmp = f""{key_to_insert}_{index}"" while tmp in units: index += 1 tmp = f""{key_to_insert}_{index}"" key_to_insert = tmp units[key_to_insert] = v[""id""] return units",False,if key_to_insert in units :,if units :,0.8820016898747209,0.0
"def parse_clademodelc(branch_type_no, line_floats, site_classes): """"""Parse results specific to the clade model C."""""" if not site_classes or len(line_floats) == 0: return for n in range(len(line_floats)): <mask>: site_classes[n][""branch types""] = {} site_classes[n][""branch types""][branch_type_no] = line_floats[n] return site_classes",False,"if site_classes [ n ] . get ( ""branch types"" ) is None :","if ""branch types"" not in site_classes [ n ] :",0.8820016898747209,41.81914517914237
"def track_modules(self, *modules): """"""Add module names to the tracked list."""""" already_tracked = self.session.GetParameter(""autodetect_build_local_tracked"") or [] needed = set(modules) if not needed.issubset(already_tracked): needed.update(already_tracked) with self.session as session: session.SetParameter(""autodetect_build_local_tracked"", needed) for module_name in modules: module_obj = self.GetModuleByName(module_name) <mask>: # Clear the module's profile. This will force it to # reload a new profile. module_obj.profile = None",True,if module_obj :,if module_obj :,0.8820016898747209,100.00000000000004
"def set_job_on_hold(self, value, blocking=True): trigger = False # don't run any locking code beyond this... if not self._job_on_hold.acquire(blocking=blocking): return False try: <mask>: self._job_on_hold.set() else: self._job_on_hold.clear() if self._job_on_hold.counter == 0: trigger = True finally: self._job_on_hold.release() # locking code is now safe to run again if trigger: self._continue_sending() return True",True,if value :,if value :,0.8820016898747209,0.0
"def moveToThreadNext(self): """"""Move a position to threadNext position."""""" p = self if p.v: <mask>: p.moveToFirstChild() elif p.hasNext(): p.moveToNext() else: p.moveToParent() while p: if p.hasNext(): p.moveToNext() break # found p.moveToParent() # not found. return p",False,if p . v . children :,if p . hasFirstChild ( ) :,0.8820016898747209,26.269098944241588
"def best_image(width, height): # A heuristic for finding closest sized image to required size. image = images[0] for img in images: <mask>: # Exact match always used return img elif img.width >= width and img.width * img.height > image.width * image.height: # At least wide enough, and largest area image = img return image",True,if img . width == width and img . height == height :,if img . width == width and img . height == height :,0.8820016898747209,100.00000000000004
"def _check_input_types(self): if len(self.base_features) == 0: return True input_types = self.primitive.input_types if input_types is not None: <mask>: input_types = [input_types] for t in input_types: zipped = list(zip(t, self.base_features)) if all([issubclass(f.variable_type, v) for v, f in zipped]): return True else: return True return False",False,if type ( input_types [ 0 ] ) != list :,"if not isinstance ( input_types , list ) :",0.8820016898747209,21.241494252828044
"def get_result(self): result_list = [] exc_info = None for f in self.children: try: result_list.append(f.get_result()) except Exception as e: if exc_info is None: exc_info = sys.exc_info() else: <mask>: app_log.error(""Multiple exceptions in yield list"", exc_info=True) if exc_info is not None: raise_exc_info(exc_info) if self.keys is not None: return dict(zip(self.keys, result_list)) else: return list(result_list)",False,"if not isinstance ( e , self . quiet_exceptions ) :",if len ( result_list ) > 1 :,0.8820016898747209,4.648378982882215
"def _update_learning_params(self): model = self.model hparams = self.hparams fd = self.runner.feed_dict step_num = self.step_num if hparams.model_type == ""resnet_tf"": if step_num < hparams.lrn_step: lrn_rate = hparams.mom_lrn <mask>: lrn_rate = hparams.mom_lrn / 10 elif step_num < 35000: lrn_rate = hparams.mom_lrn / 100 else: lrn_rate = hparams.mom_lrn / 1000 fd[model.lrn_rate] = lrn_rate",False,elif step_num < 30000 :,elif step_num < 10000 :,0.8820016898747209,64.34588841607616
"def topic_exists(self, arn): response = self._conn.get_all_topics() topics = response[""ListTopicsResponse""][""ListTopicsResult""][""Topics""] current_topics = [] if len(topics) > 0: for topic in topics: topic_arn = topic[""TopicArn""] current_topics.append(topic_arn) <mask>: return True return False",False,if arn in current_topics :,if current_topics == arn :,0.8820016898747209,24.446151121745064
"def assertStartsWith(self, expectedPrefix, text, msg=None): if not text.startswith(expectedPrefix): <mask>: text = text[: len(expectedPrefix) + 5] + ""..."" standardMsg = ""{} not found at the start of {}"".format( repr(expectedPrefix), repr(text) ) self.fail(self._formatMessage(msg, standardMsg))",False,if len ( expectedPrefix ) + 5 < len ( text ) :,if len ( text ) > 5 :,0.8820016898747209,25.310950232244416
"def validate_memory(self, value): for k, v in value.viewitems(): <mask>: # use NoneType to unset a value continue if not re.match(PROCTYPE_MATCH, k): raise serializers.ValidationError(""Process types can only contain [a-z]"") if not re.match(MEMLIMIT_MATCH, str(v)): raise serializers.ValidationError( ""Limit format: <number><unit>, where unit = B, K, M or G"" ) return value",False,if v is None :,"if k == ""memory"" :",0.8820016898747209,6.567274736060395
"def open(self) -> ""KeyValueJsonDb"": """"""Create a new data base or open existing one"""""" if os.path.exists(self._name): <mask>: raise IOError(""%s exists and is not a file"" % self._name) try: with open(self._name, ""r"") as _in: self.set_records(json.load(_in)) except json.JSONDecodeError: # file corrupted, reset it. self.commit() else: # make sure path exists mkpath(os.path.dirname(self._name)) self.commit() return self",True,if not os . path . isfile ( self . _name ) :,if not os . path . isfile ( self . _name ) :,0.8820016898747209,100.00000000000004
"def _calculate(self): before = self.before.data after = self.after.data self.deleted = {} self.updated = {} self.created = after.copy() for path, f in before.items(): <mask>: self.deleted[path] = f continue del self.created[path] if f.mtime < after[path].mtime: self.updated[path] = after[path]",False,if path not in after :,if f . mtime > after [ path ] . mtime :,0.8820016898747209,4.789232204309912
"def cache_sqs_queues_across_accounts() -> bool: function: str = f""{__name__}.{sys._getframe().f_code.co_name}"" # First, get list of accounts accounts_d: list = async_to_sync(get_account_id_to_name_mapping)() # Second, call tasks to enumerate all the roles across all accounts for account_id in accounts_d.keys(): <mask>: cache_sqs_queues_for_account.delay(account_id) else: if account_id in config.get(""celery.test_account_ids"", []): cache_sqs_queues_for_account.delay(account_id) stats.count(f""{function}.success"") return True",False,"if config . get ( ""environment"" ) == ""prod"" :","if account_id in config . get ( ""celery.test_account_ids"" , [ ] ) :",0.8820016898747209,17.855149299161603
"def remove(self, path, config=None, error_on_path=False, defaults=None): if not path: if error_on_path: raise NoSuchSettingsPath() return if config is not None or defaults is not None: <mask>: config = self._config if defaults is None: defaults = dict(self._map.parents) chain = HierarchicalChainMap(config, defaults) else: chain = self._map try: chain.del_by_path(path) self._mark_dirty() except KeyError: if error_on_path: raise NoSuchSettingsPath() pass",True,if config is None :,if config is None :,0.8820016898747209,100.00000000000004
"def PopulateProjectId(project_id=None): """"""Fills in a project_id from the boto config file if one is not provided."""""" if not project_id: default_id = boto.config.get_value(""GSUtil"", ""default_project_id"") <mask>: raise ProjectIdException(""MissingProjectId"") return default_id return project_id",True,if not default_id :,if not default_id :,0.8820016898747209,100.00000000000004
"def set(self, name, value): with self._object_cache_lock: old_value = self._object_cache.get(name) ret = not old_value or int(old_value.metadata.resource_version) < int( value.metadata.resource_version ) <mask>: self._object_cache[name] = value return ret, old_value",True,if ret :,if ret :,0.8820016898747209,0.0
"def remove(self, url): try: i = self.items.index(url) except (ValueError, IndexError): pass else: was_selected = i in self.selectedindices() self.list.delete(i) del self.items[i] if not self.items: self.mp.hidepanel(self.name) <mask>: if i >= len(self.items): i = len(self.items) - 1 self.list.select_set(i)",False,elif was_selected :,if was_selected :,0.8820016898747209,66.87403049764218
"def add_directory_csv_files(dir_path, paths=None): if not paths: paths = [] for p in listdir(dir_path): path = join(dir_path, p) <mask>: # call recursively for each dir paths = add_directory_csv_files(path, paths) elif isfile(path) and path.endswith("".csv""): # add every file to the list paths.append(path) return paths",False,if isdir ( path ) :,"if isdir ( path ) and path . endswith ( "".csv"" ) :",0.8820016898747209,25.33654946448646
"def _get_client(rp_mapping, resource_provider): for key, value in rp_mapping.items(): <mask>: if isinstance(value, dict): return GeneralPrivateEndpointClient( key, value[""api_version""], value[""support_list_or_not""], value[""resource_get_api_version""], ) return value() raise CLIError( ""Resource type must be one of {}"".format("", "".join(rp_mapping.keys())) )",False,if str . lower ( key ) == str . lower ( resource_provider ) :,"if isinstance ( value , resource_provider ) :",0.8820016898747209,19.199242796476845
"def compute_rule_hash(self, rule): buf = ""%d-%d-%s-"" % ( rule.get(""FromPort"", 0) or 0, rule.get(""ToPort"", 0) or 0, rule.get(""IpProtocol"", ""-1"") or ""-1"", ) for a, ke in self.RULE_ATTRS: <mask>: continue ev = [e[ke] for e in rule[a]] ev.sort() for e in ev: buf += ""%s-"" % e # mask to generate the same numeric value across all Python versions return zlib.crc32(buf.encode(""ascii"")) & 0xFFFFFFFF",False,if a not in rule :,if ke not in rule :,0.8820016898747209,53.7284965911771
"def analysis_sucess_metrics(analysis_time: float, allow_exception=False): try: anchore_engine.subsys.metrics.counter_inc(name=""anchore_analysis_success"") anchore_engine.subsys.metrics.histogram_observe( ""anchore_analysis_time_seconds"", analysis_time, buckets=ANALYSIS_TIME_SECONDS_BUCKETS, status=""success"", ) except: <mask>: raise else: logger.exception( ""Unexpected exception during metrics update for a successful analysis. Swallowing error and continuing"" )",True,if allow_exception :,if allow_exception :,0.8820016898747209,100.00000000000004
"def decide_file_icon(file): if file.state == File.ERROR: return FileItem.icon_error elif isinstance(file.parent, Track): <mask>: return FileItem.icon_saved elif file.state == File.PENDING: return FileItem.match_pending_icons[int(file.similarity * 5 + 0.5)] else: return FileItem.match_icons[int(file.similarity * 5 + 0.5)] elif file.state == File.PENDING: return FileItem.icon_file_pending else: return FileItem.icon_file",False,if file . state == File . NORMAL :,if file . state == File . Saved :,0.8820016898747209,78.25422900366438
"def deleteMenu(self, menuName): try: menu = self.getMenu(menuName) <mask>: self.destroy(menu) self.destroyMenu(menuName) else: g.es(""can't delete menu:"", menuName) except Exception: g.es(""exception deleting"", menuName, ""menu"") g.es_exception()",True,if menu :,if menu :,0.8820016898747209,0.0
"def parser(cls, buf): (type_, code, csum) = struct.unpack_from(cls._PACK_STR, buf) msg = cls(type_, code, csum) offset = cls._MIN_LEN if len(buf) > offset: cls_ = cls._ICMPV6_TYPES.get(type_, None) <mask>: msg.data = cls_.parser(buf, offset) else: msg.data = buf[offset:] return msg, None, None",True,if cls_ :,if cls_ :,0.8820016898747209,100.00000000000004
"def _load_dataset_area(self, dsid, file_handlers, coords): """"""Get the area for *dsid*."""""" try: return self._load_area_def(dsid, file_handlers) except NotImplementedError: if any(x is None for x in coords): logger.warning(""Failed to load coordinates for '{}'"".format(dsid)) return None area = self._make_area_from_coords(coords) <mask>: logger.debug(""No coordinates found for %s"", str(dsid)) return area",True,if area is None :,if area is None :,0.8820016898747209,100.00000000000004
"def __getattr__(self, name): if Popen.verbose: sys.stdout.write(""Getattr: %s..."" % name) if name in Popen.__slots__: return object.__getattribute__(self, name) else: if self.popen is not None: if Popen.verbose: print(""from Popen"") return getattr(self.popen, name) else: <mask>: return self.emu_wait else: raise Exception(""subprocess emulation: not implemented: %s"" % name)",False,"if name == ""wait"" :",if self . emu_wait is not None :,0.8820016898747209,5.522397783539471
"def update(self, time_delta): super().update(time_delta) n = self.menu.selected_option if n == self.last: return self.last = n s = """" for i in range(len(self.files)): <mask>: for l in open(self.files[i][1]): x = l.strip() if len(x) > 1 and x[0] == ""#"": x = ""<b><u>"" + x[1:] + "" </u></b>"" s += x + ""<br>"" self.set_text(s)",False,if self . files [ i ] [ 0 ] == n :,"if self . files [ i ] [ 0 ] == ""#"" :",0.8820016898747209,73.67565054628355
"def wrapper(*args, **kwargs): list_args, empty = _apply_defaults(func, args, kwargs) if len(dimensions) > len(list_args): raise TypeError( ""%s takes %i parameters, but %i dimensions were passed"" % (func.__name__, len(list_args), len(dimensions)) ) for dim, value in zip(dimensions, list_args): if dim is None: continue <mask>: val_dim = ureg.get_dimensionality(value) raise DimensionalityError(value, ""a quantity of"", val_dim, dim) return func(*args, **kwargs)",False,if not ureg . Quantity ( value ) . check ( dim ) :,if empty :,0.8820016898747209,0.0
"def _check(self, name, size=None, *extra): func = getattr(imageop, name) for height in VALUES: for width in VALUES: strlen = abs(width * height) if size: strlen *= size <mask>: data = ""A"" * strlen else: data = AAAAA if size: arguments = (data, size, width, height) + extra else: arguments = (data, width, height) + extra try: func(*arguments) except (ValueError, imageop.error): pass",False,if strlen < MAX_LEN :,if strlen > 0 :,0.8820016898747209,15.848738972120703
"def wait_send_all_might_not_block(self) -> None: with self._send_conflict_detector: <mask>: raise trio.ClosedResourceError(""file was already closed"") try: await trio.lowlevel.wait_writable(self._fd_holder.fd) except BrokenPipeError as e: # kqueue: raises EPIPE on wait_writable instead # of sending, which is annoying raise trio.BrokenResourceError from e",False,if self . _fd_holder . closed :,if self . _fd_holder . fd is not None :,0.8820016898747209,57.60844201603898
"def parse_win_proxy(val): proxies = [] for p in val.split("";""): if ""="" in p: tab = p.split(""="", 1) <mask>: tab[0] = ""SOCKS4"" proxies.append( (tab[0].upper(), tab[1], None, None) ) # type, addr:port, username, password else: proxies.append((""HTTP"", p, None, None)) return proxies",False,"if tab [ 0 ] == ""socks"" :","if tab [ 0 ] == ""SOCKS4"" :",0.8820016898747209,74.19446627365011
"def _super_function(args): passed_class, passed_self = args.get_arguments([""type"", ""self""]) if passed_self is None: return passed_class else: # pyclass = passed_self.get_type() pyclass = passed_class if isinstance(pyclass, pyobjects.AbstractClass): supers = pyclass.get_superclasses() <mask>: return pyobjects.PyObject(supers[0]) return passed_self",False,if supers :,if len ( supers ) == 1 :,0.8820016898747209,6.27465531099474
"def update_output_mintime(job): try: return output_mintime[job] except KeyError: for job_ in chain([job], self.depending[job]): try: t = output_mintime[job_] except KeyError: t = job_.output_mintime <mask>: output_mintime[job] = t return output_mintime[job] = None",False,if t is not None :,if t :,0.8820016898747209,0.0
"def get_list_of_strings_to_mongo_objects(self, notifications_list=None): result = [] if len(notifications_list) > 0: for x in notifications_list: split_provider_id = x.split("":"") # email:id if len(split_provider_id) == 2: _id = split_provider_id[1] cursor = self.get_by_id(_id) <mask>: # Append if exists result.append(cursor) return result",True,if cursor :,if cursor :,0.8820016898747209,0.0
"def stop(self): with self.lock: <mask>: return self.task_queue.put(None) self.result_queue.put(None) process = self.process self.process = None self.task_queue = None self.result_queue = None process.join(timeout=0.1) if process.exitcode is None: os.kill(process.pid, signal.SIGKILL) process.join()",False,if not self . process :,if self . process is None :,0.8820016898747209,27.77619034011791
"def on_api_command(self, command, data): if command == ""select"": if not Permissions.PLUGIN_ACTION_COMMAND_PROMPT_INTERACT.can(): return flask.abort(403, ""Insufficient permissions"") <mask>: return flask.abort(409, ""No active prompt"") choice = data[""choice""] if not isinstance(choice, int) or not self._prompt.validate_choice(choice): return flask.abort( 400, ""{!r} is not a valid value for choice"".format(choice) ) self._answer_prompt(choice)",False,if self . _prompt is None :,if not self . _prompt . active :,0.8820016898747209,33.03164318013809
"def application_openFiles_(self, nsapp, filenames): # logging.info('[osx] file open') # logging.info('[osx] file : %s' % (filenames)) for filename in filenames: logging.info(""[osx] receiving from macOS : %s"", filename) <mask>: if sabnzbd.filesystem.get_ext(filename) in VALID_ARCHIVES + VALID_NZB_FILES: sabnzbd.add_nzbfile(filename, keep=True)",False,if os . path . exists ( filename ) :,"if nsapp == ""mac"" :",0.8820016898747209,5.11459870708889
"def test_error_through_destructor(self): # Test that the exception state is not modified by a destructor, # even if close() fails. rawio = self.CloseFailureIO() with support.catch_unraisable_exception() as cm: with self.assertRaises(AttributeError): self.tp(rawio).xyzzy <mask>: self.assertIsNone(cm.unraisable) elif cm.unraisable is not None: self.assertEqual(cm.unraisable.exc_type, OSError)",False,if not IOBASE_EMITS_UNRAISABLE :,"if isinstance ( cm . unraisable , Exception ) :",0.8820016898747209,4.990049701936832
"def http_wrapper(self, url, postdata={}): try: <mask>: f = urllib.urlopen(url, postdata) else: f = urllib.urlopen(url) response = f.read() except: import traceback import logging, sys cla, exc, tb = sys.exc_info() logging.error(url) if postdata: logging.error(""with post data"") else: logging.error(""without post data"") logging.error(exc.args) logging.error(traceback.format_tb(tb)) response = """" return response",False,if postdata != { } :,if postdata :,0.8820016898747209,0.0
"def check_single_file(fn, fetchuri): """"""Determine if a single downloaded file is something we can't handle"""""" with open(fn, ""r"", errors=""surrogateescape"") as f: <mask>: logger.error( 'Fetching ""%s"" returned a single HTML page - check the URL is correct and functional' % fetchuri ) sys.exit(1)",False,"if ""<html"" in f . read ( 100 ) . lower ( ) :",if f . read ( ) != fetchuri :,0.8820016898747209,16.122531543752565
"def update_properties(self, update_dict): signed_attribute_changed = False for k, value in update_dict.items(): if getattr(self, k) != value: setattr(self, k, value) signed_attribute_changed = signed_attribute_changed or ( k in self.payload_arguments ) if signed_attribute_changed: <mask>: self.status = UPDATED self.timestamp = clock.tick() self.sign() return self",False,if self . status != NEW :,if self . status == UPDATED :,0.8820016898747209,38.260294162784454
"def clean_items(event, items, variations): for item in items: if event != item.event: raise ValidationError(_(""One or more items do not belong to this event."")) if item.has_variations: <mask>: raise ValidationError( _( ""One or more items has variations but none of these are in the variations list."" ) )",False,if not any ( var . item == item for var in variations ) :,if variations != item . variations :,0.8820016898747209,5.34741036489421
"def TryMerge(self, d): while d.avail() > 0: tt = d.getVarInt32() if tt == 10: length = d.getVarInt32() tmp = ProtocolBuffer.Decoder(d.buffer(), d.pos(), d.pos() + length) d.skip(length) self.add_status().TryMerge(tmp) continue if tt == 18: self.add_doc_id(d.getPrefixedString()) continue <mask>: raise ProtocolBuffer.ProtocolBufferDecodeError d.skipData(tt)",True,if tt == 0 :,if tt == 0 :,0.8820016898747209,100.00000000000004
"def connections(self): # Connections look something like this: # socket:[102422] fds = self.open_files socket = ""socket:["" result = [] functions = [pwndbg.net.tcp, pwndbg.net.unix, pwndbg.net.netlink] for fd, path in fds.items(): if socket not in path: continue inode = path[len(socket) : -1] inode = int(inode) for func in functions: for x in func(): <mask>: x.fd = fd result.append(x) return tuple(result)",True,if x . inode == inode :,if x . inode == inode :,0.8820016898747209,100.00000000000004
"def _movement_finished(self): if self.in_ship_map: # if the movement somehow stops, the position sticks, and the unit isn't at next_target any more <mask>: ship = self.session.world.ship_map.get(self._next_target.to_tuple()) if ship is not None and ship() is self: del self.session.world.ship_map[self._next_target.to_tuple()] super()._movement_finished()",True,if self . _next_target is not None :,if self . _next_target is not None :,0.8820016898747209,100.00000000000004
"def print_addresses(self): p = 3 tmp_str = ""["" if self.get_len() >= 7: # at least one complete IP address while 1: if p + 1 == self.get_ptr(): tmp_str += ""#"" tmp_str += self.get_ip_address(p) p += 4 <mask>: break else: tmp_str += "", "" tmp_str += ""] "" if self.get_ptr() % 4: # ptr field should be a multiple of 4 tmp_str += ""nonsense ptr field: %d "" % self.get_ptr() return tmp_str",False,if p >= self . get_len ( ) :,elif p == 7 :,0.8820016898747209,3.9297526283216277
"def source_shapes(self): """"""Prints debug information about the sources in this provider."""""" if logger.isEnabledFor(logging.DEBUG): for i, source in enumerate(self.sources): <mask>: name = ""anonymous"" else: name = self.keys[i] try: shape = source.shape() except NotImplementedError: shape = ""N/A"" logger.debug( 'Data source ""%s"": entries=%s, shape=%s', name, len(source), shape )",False,if self . keys is None :,if i == 0 :,0.8820016898747209,8.170609724417774
def swap_actions(actions): for mutexgroup in mutex_groups: mutex_actions = mutexgroup._group_actions <mask>: # make a best guess as to where we should store the group targetindex = actions.index(mutexgroup._group_actions[0]) # insert the _ArgumentGroup container actions[targetindex] = mutexgroup # remove the duplicated individual actions actions = [action for action in actions if action not in mutex_actions] return actions,False,"if contains_actions ( mutex_actions , actions ) :",if len ( mutex_actions ) == 1 :,0.8820016898747209,26.477952261405967
"def rec_deps(services, container_by_name, cnt, init_service): deps = cnt[""_deps""] for dep in deps.copy(): dep_cnts = services.get(dep) if not dep_cnts: continue dep_cnt = container_by_name.get(dep_cnts[0]) if dep_cnt: # TODO: avoid creating loops, A->B->A <mask>: continue new_deps = rec_deps(services, container_by_name, dep_cnt, init_service) deps.update(new_deps) return deps",False,"if init_service and init_service in dep_cnt [ ""_deps"" ] :",if dep_cnt [ 0 ] != init_service :,0.8820016898747209,19.27302509339085
"def make_dump_list_by_name_list(name_list): info_list = [] for info_name in name_list: info = next((x for x in DUMP_LIST if x.info_name == info_name), None) <mask>: raise RuntimeError('Unknown info name: ""{}""'.format(info_name)) info_list.append(info) return info_list",False,if not info :,if info is None :,0.8820016898747209,14.058533129758727
"def create(self, private=False): try: if private: log.info(""Creating private channel %s."", self) self._bot.api_call( ""conversations.create"", data={""name"": self.name, ""is_private"": True} ) else: log.info(""Creating channel %s."", self) self._bot.api_call(""conversations.create"", data={""name"": self.name}) except SlackAPIResponseError as e: <mask>: raise RoomError(f""Unable to create channel. {USER_IS_BOT_HELPTEXT}"") else: raise RoomError(e)",False,"if e . error == ""user_is_bot"" :","if e . response [ ""error"" ] == ""Error"" :",0.8820016898747209,19.251614434393563
"def talk(self, words): if self.writeSentence(words) == 0: return r = [] while 1: i = self.readSentence() <mask>: continue reply = i[0] attrs = {} for w in i[1:]: j = w.find(""="", 1) if j == -1: attrs[w] = """" else: attrs[w[:j]] = w[j + 1 :] r.append((reply, attrs)) if reply == ""!done"": return r",False,if len ( i ) == 0 :,if not i :,0.8820016898747209,6.023021415818187
"def _load_logfile(self, lfn): enc_key = self.decryption_key_func() with open(os.path.join(self.logdir, lfn)) as fd: <mask>: with DecryptingStreamer( fd, mep_key=enc_key, name=""EventLog/DS(%s)"" % lfn ) as streamer: lines = streamer.read() streamer.verify(_raise=IOError) else: lines = fd.read() if lines: for line in lines.splitlines(): event = Event.Parse(line.strip()) self._events[event.event_id] = event",True,if enc_key :,if enc_key :,0.8820016898747209,100.00000000000004
"def set_ok_port(self, cookie, request): if cookie.port_specified: req_port = request_port(request) if req_port is None: req_port = ""80"" else: req_port = str(req_port) for p in cookie.port.split("",""): try: int(p) except ValueError: debug("" bad port %s (not numeric)"", p) return False <mask>: break else: debug("" request port (%s) not found in %s"", req_port, cookie.port) return False return True",False,if p == req_port :,if req_port == p :,0.8820016898747209,29.071536848410968
"def get_attribute_value(self, nodeid, attr): with self._lock: self.logger.debug(""get attr val: %s %s"", nodeid, attr) <mask>: dv = ua.DataValue() dv.StatusCode = ua.StatusCode(ua.StatusCodes.BadNodeIdUnknown) return dv node = self._nodes[nodeid] if attr not in node.attributes: dv = ua.DataValue() dv.StatusCode = ua.StatusCode(ua.StatusCodes.BadAttributeIdInvalid) return dv attval = node.attributes[attr] if attval.value_callback: return attval.value_callback() return attval.value",True,if nodeid not in self . _nodes :,if nodeid not in self . _nodes :,0.8820016898747209,100.00000000000004
"def data_logging_status(self, trail_name, trail_details, api_client): for es in api_client.get_event_selectors(TrailName=trail_name)[""EventSelectors""]: has_wildcard = { u""Values"": [u""arn:aws:s3:::""], u""Type"": u""AWS::S3::Object"", } in es[""DataResources""] is_logging = trail_details[""IsLogging""] <mask>: return True return False",False,if has_wildcard and is_logging and self . is_fresh ( trail_details ) :,if has_wildcard and is_logging :,0.8820016898747209,26.191817594980723
"def pytest_deselected(items): if sb_config.dashboard: sb_config.item_count -= len(items) for item in items: test_id, display_id = _get_test_ids_(item) <mask>: sb_config._results.pop(test_id)",False,if test_id in sb_config . _results . keys ( ) :,if test_id in sb_config . _results :,0.8820016898747209,66.16975066206076
"def _visit(self, func): fname = func[0] if fname in self._flags: if self._flags[fname] == 1: logger.critical(""Fatal error! network ins not Dag."") import sys sys.exit(-1) else: return else: if fname not in self._flags: self._flags[fname] = 1 for output in func[3]: for f in self._orig: for input in f[2]: <mask>: self._visit(f) self._flags[fname] = 2 self._sorted.insert(0, func)",False,if output == input :,if input == output :,0.8820016898747209,21.3643503198117
"def printWiki(): firstHeading = False for m in protocol: <mask>: if firstHeading: output(""|}"") __printWikiHeader(m[1], m[2]) firstHeading = True else: output(""|-"") output( '| <span style=""white-space:nowrap;""><tt>' + m[0] + ""</tt></span> || || "" + m[1] ) output(""|}"")",False,"if m [ 0 ] == """" :","if m [ 0 ] == ""header"" :",0.8820016898747209,74.19446627365011
"def test_getitem(self): n = 200 d = deque(range(n)) l = list(range(n)) for i in range(n): d.popleft() l.pop(0) <mask>: d.append(i) l.append(i) for j in range(1 - len(l), len(l)): assert d[j] == l[j] d = deque(""superman"") self.assertEqual(d[0], ""s"") self.assertEqual(d[-1], ""n"") d = deque() self.assertRaises(IndexError, d.__getitem__, 0) self.assertRaises(IndexError, d.__getitem__, -1)",False,if random . random ( ) < 0.5 :,if i < len ( d ) :,0.8820016898747209,7.287580698437859
"def get_num(line, char_ptr, num_chars): char_ptr = char_ptr + 1 numstr = """" good = ""-.0123456789"" while char_ptr < num_chars: digit = line[char_ptr] <mask>: numstr = numstr + digit char_ptr = char_ptr + 1 else: break return numstr",False,if good . find ( digit ) != - 1 :,if digit in good :,0.8820016898747209,3.7253099995802206
"def read_digits(source, start, first_code): body = source.body position = start code = first_code if code is not None and 48 <= code <= 57: # 0 - 9 while True: position += 1 code = char_code_at(body, position) <mask>: break return position raise GraphQLSyntaxError( source, position, u""Invalid number, expected digit but got: {}."".format(print_char_code(code)), )",False,if not ( code is not None and 48 <= code <= 57 ) :,if code == 0 :,0.8820016898747209,1.9405075296086907
"def get_aws_metadata(headers, provider=None): if not provider: provider = boto.provider.get_default() metadata_prefix = provider.metadata_prefix metadata = {} for hkey in headers.keys(): <mask>: val = urllib.unquote_plus(headers[hkey]) try: metadata[hkey[len(metadata_prefix) :]] = unicode(val, ""utf-8"") except UnicodeDecodeError: metadata[hkey[len(metadata_prefix) :]] = val del headers[hkey] return metadata",False,if hkey . lower ( ) . startswith ( metadata_prefix ) :,if hkey . startswith ( metadata_prefix ) :,0.8820016898747209,59.60081680007605
"def _process_rtdest(self): LOG.debug(""Processing RT NLRI destination..."") if self._rtdest_queue.is_empty(): return else: processed_any = False while not self._rtdest_queue.is_empty(): # We process the first destination in the queue. next_dest = self._rtdest_queue.pop_first() if next_dest: next_dest.process() processed_any = True <mask>: # Since RT destination were updated we update RT filters self._core_service.update_rtfilters()",True,if processed_any :,if processed_any :,0.8820016898747209,100.00000000000004
"def _get_header(self, requester, header_name): hits = sum([header_name in headers for _, headers in requester.requests]) self.assertEquals(hits, 2 if self.revs_enabled else 1) for url, headers in requester.requests: if header_name in headers: <mask>: self.assertTrue(url.endswith(""/latest""), msg=url) else: self.assertTrue(url.endswith(""/download_urls""), msg=url) return headers.get(header_name)",True,if self . revs_enabled :,if self . revs_enabled :,0.8820016898747209,100.00000000000004
"def add_external_deps(self, deps): for dep in deps: if hasattr(dep, ""el""): dep = dep.el <mask>: raise InvalidArguments(""Argument is not an external dependency"") self.external_deps.append(dep) if isinstance(dep, dependencies.Dependency): self.process_sourcelist(dep.get_sources())",False,"if not isinstance ( dep , dependencies . Dependency ) :","if not isinstance ( dep , dependencies . ExternalDependency ) :",0.8820016898747209,74.19446627365011
"def _consume_msg(self): ws = self._ws try: while True: r = await ws.recv() if isinstance(r, bytes): r = r.decode(""utf-8"") msg = json.loads(r) stream = msg.get(""stream"") <mask>: await self._dispatch(stream, msg) except websockets.WebSocketException as wse: logging.warn(wse) await self.close() asyncio.ensure_future(self._ensure_ws())",False,if stream is not None :,if stream :,0.8820016898747209,0.0
"def generate_and_check_random(): random_size = 256 while True: random = os.urandom(random_size) a = int.from_bytes(random, ""big"") A = pow(g, a, p) <mask>: a_for_hash = big_num_for_hash(A) u = int.from_bytes(sha256(a_for_hash, b_for_hash), ""big"") if u > 0: return (a, a_for_hash, u)",False,"if is_good_mod_exp_first ( A , p ) :",if A > 0 :,0.8820016898747209,1.5577298727187734
"def write(self, datagram, address): """"""Write a datagram."""""" try: return self.socket.sendto(datagram, address) except OSError as se: no = se.args[0] if no == EINTR: return self.write(datagram, address) elif no == EMSGSIZE: raise error.MessageLengthError(""message too long"") <mask>: # oh, well, drop the data. The only difference from UDP # is that UDP won't ever notice. # TODO: add TCP-like buffering pass else: raise",False,elif no == EAGAIN :,elif no == EBADF :,0.8820016898747209,53.7284965911771
"def doDir(elem): for child in elem.childNodes: <mask>: continue if child.tagName == ""Directory"": doDir(child) elif child.tagName == ""Component"": for grandchild in child.childNodes: if not isinstance(grandchild, minidom.Element): continue if grandchild.tagName != ""File"": continue files.add(grandchild.getAttribute(""Source"").replace(os.sep, ""/""))",True,"if not isinstance ( child , minidom . Element ) :","if not isinstance ( child , minidom . Element ) :",0.8820016898747209,100.00000000000004
"def add_reversed_tensor(i, X, reversed_X): # Do not keep tensors that should stop the mapping. if X in stop_mapping_at_tensors: return if X not in reversed_tensors: reversed_tensors[X] = {""id"": (nid, i), ""tensor"": reversed_X} else: tmp = reversed_tensors[X] if ""tensor"" in tmp and ""tensors"" in tmp: raise Exception(""Wrong order, tensors already aggregated!"") <mask>: tmp[""tensors""] = [tmp[""tensor""], reversed_X] del tmp[""tensor""] else: tmp[""tensors""].append(reversed_X)",True,"if ""tensor"" in tmp :","if ""tensor"" in tmp :",0.8820016898747209,100.00000000000004
"def walk(source, path, default, delimiter="".""): """"""Walk the sourch hash given the path and return the value or default if not found"""""" if not isinstance(source, dict): raise RuntimeError( ""The source is not a walkable dict: {} path: {}"".format(source, path) ) keys = path.split(delimiter) max_depth = len(keys) cur_depth = 0 while cur_depth < max_depth: <mask>: source = source[keys[cur_depth]] cur_depth = cur_depth + 1 else: return default return source",True,if keys [ cur_depth ] in source :,if keys [ cur_depth ] in source :,0.8820016898747209,100.00000000000004
"def _from_txt_get_vulns(self): file_vulns = [] vuln_regex = ( 'SQL injection in a .*? was found at: ""(.*?)""' ', using HTTP method (.*?). The sent .*?data was: ""(.*?)""' ) vuln_re = re.compile(vuln_regex) for line in file(self.OUTPUT_FILE): mo = vuln_re.search(line) <mask>: v = MockVuln(""TestCase"", None, ""High"", 1, ""plugin"") v.set_url(URL(mo.group(1))) v.set_method(mo.group(2)) file_vulns.append(v) return file_vulns",True,if mo :,if mo :,0.8820016898747209,0.0
"def __get__(self, instance, instance_type=None): if instance: if self.att_name not in instance._obj_cache: rel_obj = self.get_obj(instance) <mask>: instance._obj_cache[self.att_name] = rel_obj return instance._obj_cache.get(self.att_name) return self",True,if rel_obj :,if rel_obj :,0.8820016898747209,100.00000000000004
"def get_ranges_from_func_set(support_set): pos_start = 0 pos_end = 0 ranges = [] for pos, func in enumerate(network.function): <mask>: pos_end = pos else: if pos_end >= pos_start: ranges.append((pos_start, pos_end)) pos_start = pos + 1 if pos_end >= pos_start: ranges.append((pos_start, pos_end)) return ranges",False,if func . type in support_set :,if func in support_set :,0.8820016898747209,53.137468984124546
"def get_all_active_plugins(self) -> List[BotPlugin]: """"""This returns the list of plugins in the callback ordered defined from the config."""""" all_plugins = [] for name in self.plugins_callback_order: # None is a placeholder for any plugin not having a defined order if name is None: all_plugins += [ plugin for name, plugin in self.plugins.items() <mask>: ] else: plugin = self.plugins[name] if plugin.is_activated: all_plugins.append(plugin) return all_plugins",False,if name not in self . plugins_callback_order and plugin . is_activated,"if isinstance ( plugin , BotPlugin )",0.8820016898747209,1.8716386091619874
"def render_token_list(self, tokens): result = [] vars = [] for token in tokens: <mask>: result.append(token.contents.replace(""%"", ""%%"")) elif token.token_type == TOKEN_VAR: result.append(""%%(%s)s"" % token.contents) vars.append(token.contents) msg = """".join(result) if self.trimmed: msg = translation.trim_whitespace(msg) return msg, vars",False,if token . token_type == TOKEN_TEXT :,if token . token_type == TOKEN_STRING :,0.8820016898747209,82.651681837938
"def test_build_root_config_overwrite(self): cfg = build_root_config(""tests.files.settings_overwrite"") for key, val in DEFAULT_SPIDER_GLOBAL_CONFIG.items(): <mask>: self.assertEqual(cfg[""global""][key], [""zzz""]) else: self.assertEqual(cfg[""global""][key], val)",False,"if key == ""spider_modules"" :","if val == ""zzz"" :",0.8820016898747209,21.069764742263047
"def get_limit(self, request): if self.limit_query_param: try: limit = int(request.query_params[self.limit_query_param]) if limit < 0: raise ValueError() # Enforce maximum page size, if defined <mask>: if limit == 0: return settings.MAX_PAGE_SIZE else: return min(limit, settings.MAX_PAGE_SIZE) return limit except (KeyError, ValueError): pass return self.default_limit",False,if settings . MAX_PAGE_SIZE :,if self . default_limit is None :,0.8820016898747209,6.742555929751843
"def track_handler(handler): tid = handler.request.tid for event in events_monitored: <mask>: e = Event(event, handler.request.execution_time) State.tenant_state[tid].RecentEventQ.append(e) State.tenant_state[tid].EventQ.append(e) break",False,"if event [ ""handler_check"" ] ( handler ) :",if event . event_type == EventType . RECENT :,0.8820016898747209,7.410494411527525
"def TryMerge(self, d): while d.avail() > 0: tt = d.getVarInt32() if tt == 10: length = d.getVarInt32() tmp = ProtocolBuffer.Decoder(d.buffer(), d.pos(), d.pos() + length) d.skip(length) self.add_subscription().TryMerge(tmp) continue <mask>: raise ProtocolBuffer.ProtocolBufferDecodeError d.skipData(tt)",True,if tt == 0 :,if tt == 0 :,0.8820016898747209,100.00000000000004
"def GetCreateInstanceBinder(self, info): with self._lock: <mask>: return self._createInstanceBinders[info] b = runtime.SymplCreateInstanceBinder(info) self._createInstanceBinders[info] = b return b",False,if self . _createInstanceBinders . ContainsKey ( info ) :,if info in self . _createInstanceBinders :,0.8820016898747209,27.329052280893862
"def process_task(self, body, message): if ""control"" in body: try: return self.control(body, message) except Exception: logger.exception(""Exception handling control message:"") return if len(self.pool): <mask>: try: queue = UUID(body[""uuid""]).int % len(self.pool) except Exception: queue = self.total_messages % len(self.pool) else: queue = self.total_messages % len(self.pool) else: queue = 0 self.pool.write(queue, body) self.total_messages += 1 message.ack()",False,"if ""uuid"" in body and body [ ""uuid"" ] :","if ""uuid"" in body :",0.8820016898747209,30.93485033266056
"def is_defined_in_base_class(self, var: Var) -> bool: if var.info: for base in var.info.mro[1:]: if base.get(var.name) is not None: return True <mask>: return True return False",False,if var . info . fallback_to_any :,if base . get ( var . name ) is not None :,0.8820016898747209,7.768562846380176
"def ant_map(m): tmp = ""rows %s\ncols %s\n"" % (len(m), len(m[0])) players = {} for row in m: tmp += ""m "" for col in row: if col == LAND: tmp += ""."" <mask>: tmp += ""%"" elif col == FOOD: tmp += ""*"" elif col == UNSEEN: tmp += ""?"" else: players[col] = True tmp += chr(col + 97) tmp += ""\n"" tmp = (""players %s\n"" % len(players)) + tmp return tmp",False,elif col == BARRIER :,elif col == MIDDLE :,0.8820016898747209,53.7284965911771
"def prompt_for_resume(config): logger = logging.getLogger(""changeme"") logger.error( ""A previous scan was interrupted. Type R to resume or F to start a fresh scan"" ) answer = """" while not (answer == ""R"" or answer == ""F""): prompt = ""(R/F)> "" answer = """" try: answer = raw_input(prompt) except NameError: answer = input(prompt) <mask>: logger.debug(""Forcing a fresh scan"") elif answer.upper() == ""R"": logger.debug(""Resuming previous scan"") config.resume = True return config.resume",True,"if answer . upper ( ) == ""F"" :","if answer . upper ( ) == ""F"" :",0.8820016898747209,100.00000000000004
"def f(view, s): if mode == modes.INTERNAL_NORMAL: <mask>: if view.line(s.b).size() > 0: eol = view.line(s.b).b return R(s.b, eol) return s return s",False,if count == 1 :,if view . line ( s . b ) . size ( ) > 0 :,0.8820016898747209,2.908317710573757
"def flush(self): if not self.cuts: return for move, (x, y, z), cent in douglas(self.cuts, self.tolerance, self.plane): <mask>: self.write(""%s X%.4f Y%.4f Z%.4f %s"" % (move, x, y, z, cent)) self.lastgcode = None self.lastx = x self.lasty = y self.lastz = z else: self.move_common(x, y, z, gcode=""G1"") self.cuts = []",True,if cent :,if cent :,0.8820016898747209,0.0
"def copy_shell(self): cls = self.__class__ old_id = cls.id new_i = cls() # create a new group new_i.id = self.id # with the same id cls.id = old_id # Reset the Class counter # Copy all properties for prop in cls.properties: <mask>: if self.has(prop): val = getattr(self, prop) setattr(new_i, prop, val) # but no members new_i.members = [] return new_i",False,"if prop is not ""members"" :","if prop . startswith ( ""shell_"" ) :",0.8820016898747209,9.425159511373677
"def find_region_by_value(key, value): for region in cognitoidp_backends: backend = cognitoidp_backends[region] for user_pool in backend.user_pools.values(): if key == ""client_id"" and value in user_pool.clients: return region <mask>: return region # If we can't find the `client_id` or `access_token`, we just pass # back a default backend region, which will raise the appropriate # error message (e.g. NotAuthorized or NotFound). return list(cognitoidp_backends)[0]",False,"if key == ""access_token"" and value in user_pool . access_tokens :","elif key == ""access_token"" and value in user_pool . access_tokens :",0.8820016898747209,94.57416090031757
"def __init__( self, fixed: MQTTFixedHeader = None, variable_header: PacketIdVariableHeader = None ): if fixed is None: header = MQTTFixedHeader(PUBREL, 0x02) # [MQTT-3.6.1-1] else: <mask>: raise HBMQTTException( ""Invalid fixed packet type %s for PubrelPacket init"" % fixed.packet_type ) header = fixed super().__init__(header) self.variable_header = variable_header self.payload = None",False,if fixed . packet_type is not PUBREL :,"if fixed . packet_type not in ( PUBREL , PUBREL_PACKET ) :",0.8820016898747209,33.34477432809603
"def _on_event_MetadataStatisticsUpdated(self, event, data): with self._selectedFileMutex: <mask>: self._setJobData( self._selectedFile[""filename""], self._selectedFile[""filesize""], self._selectedFile[""sd""], self._selectedFile[""user""], )",True,if self . _selectedFile :,if self . _selectedFile :,0.8820016898747209,100.00000000000004
"def _validate_parameter_range(self, value_hp, parameter_range): """"""Placeholder docstring"""""" for ( parameter_range_key, parameter_range_value, ) in parameter_range.__dict__.items(): <mask>: continue # Categorical ranges if isinstance(parameter_range_value, list): for categorical_value in parameter_range_value: value_hp.validate(categorical_value) # Continuous, Integer ranges else: value_hp.validate(parameter_range_value)",False,"if parameter_range_key == ""scaling_type"" :","if parameter_range_key == ""value"" :",0.8820016898747209,65.10803637373398
"def visit_filter_projection(self, node, value): base = self.visit(node[""children""][0], value) if not isinstance(base, list): return None comparator_node = node[""children""][2] collected = [] for element in base: if self._is_true(self.visit(comparator_node, element)): current = self.visit(node[""children""][1], element) <mask>: collected.append(current) return collected",True,if current is not None :,if current is not None :,0.8820016898747209,100.00000000000004
"def _getSubstrings(self, va, size, ltyp): # rip through the desired memory range to populate any substrings subs = set() end = va + size for offs in range(va, end, 1): loc = self.getLocation(offs, range=True) <mask>: subs.add((loc[L_VA], loc[L_SIZE])) if loc[L_TINFO]: subs = subs.union(set(loc[L_TINFO])) return list(subs)",False,if loc and loc [ L_LTYPE ] == LOC_STRING and loc [ L_VA ] > va :,if loc [ L_VA ] == ltyp :,0.8820016898747209,19.136252867522522
"def run(self): while not self._stopped: try: try: test_name = next(self.pending) except StopIteration: break mp_result = self._runtest(test_name) self.output.put((False, mp_result)) <mask>: break except ExitThread: break except BaseException: self.output.put((True, traceback.format_exc())) break",False,"if must_stop ( mp_result . result , self . ns ) :",if self . _stopped :,0.8820016898747209,3.855407098438308
"def get_in_inputs(key, data): if isinstance(data, dict): for k, v in data.items(): if k == key: return v elif isinstance(v, (list, tuple, dict)): out = get_in_inputs(key, v) <mask>: return out elif isinstance(data, (list, tuple)): out = [get_in_inputs(key, x) for x in data] out = [x for x in out if x] if out: return out[0]",True,if out :,if out :,0.8820016898747209,0.0
"def act_mapping(self, items, actions, mapping): """"""Executes all the actions on the list of pods."""""" success = True for action in actions: for key, method in mapping.items(): if key in action: params = action.get(key) ret = method(items, params) <mask>: success = False return success",False,if not ret :,if ret is not None :,0.8820016898747209,11.478744233307168
"def _apply(self, plan): desired = plan.desired changes = plan.changes self.log.debug(""_apply: zone=%s, len(changes)=%d"", desired.name, len(changes)) domain_name = desired.name[:-1] try: nsone_zone = self._client.loadZone(domain_name) except ResourceException as e: <mask>: raise self.log.debug(""_apply: no matching zone, creating"") nsone_zone = self._client.createZone(domain_name) for change in changes: class_name = change.__class__.__name__ getattr(self, ""_apply_{}"".format(class_name))(nsone_zone, change)",False,if e . message != self . ZONE_NOT_FOUND_MESSAGE :,if e . errno != 404 :,0.8820016898747209,9.95265213836697
"def split_artists(self, json): if len(json) == 0: ([], []) elif len(json) == 1: artist = Artist.query.filter_by(name=json[0][""name""]).first() return ([artist], []) my_artists = [] other_artists = [] for artist_dict in json: artist = Artist.query.filter_by(name=artist_dict[""name""]) <mask>: my_artists.append(artist.first()) else: del artist_dict[""thumb_url""] other_artists.append(artist_dict) return (my_artists, other_artists)",False,if artist . count ( ) :,if artist . first ( ) :,0.8820016898747209,41.11336169005196
"def update_metadata(self): for attrname in dir(self): if attrname.startswith(""__""): continue attrvalue = getattr(self, attrname, None) <mask>: continue if attrname == ""salt_version"": attrname = ""version"" if hasattr(self.metadata, ""set_{0}"".format(attrname)): getattr(self.metadata, ""set_{0}"".format(attrname))(attrvalue) elif hasattr(self.metadata, attrname): try: setattr(self.metadata, attrname, attrvalue) except AttributeError: pass",False,if attrvalue == 0 :,if attrvalue is None :,0.8820016898747209,19.3576934939088
"def close(self, code=errno.ECONNRESET): with self.shutdown_lock: <mask>: super(RemoteIPRoute, self).close(code=code) self.closed = True try: self._mitogen_call.get() except mitogen.core.ChannelError: pass if self._mitogen_broker is not None: self._mitogen_broker.shutdown() self._mitogen_broker.join()",True,if not self . closed :,if not self . closed :,0.8820016898747209,100.00000000000004
"def untokenize(self, iterable): for t in iterable: <mask>: self.compat(t, iterable) break tok_type, token, start, end, line = t self.add_whitespace(start) self.tokens.append(token) self.prev_row, self.prev_col = end if tok_type in (NEWLINE, NL): self.prev_row += 1 self.prev_col = 0 return """".join(self.tokens)",False,if len ( t ) == 2 :,"if isinstance ( t , ( list , tuple ) ) :",0.8820016898747209,8.516593018819643
"def __call__(self, x, uttid=None): if self.utt2spk is not None: spk = self.utt2spk[uttid] else: spk = uttid if not self.reverse: if self.norm_means: x = np.add(x, self.bias[spk]) <mask>: x = np.multiply(x, self.scale[spk]) else: if self.norm_vars: x = np.divide(x, self.scale[spk]) if self.norm_means: x = np.subtract(x, self.bias[spk]) return x",True,if self . norm_vars :,if self . norm_vars :,0.8820016898747209,100.00000000000004
"def get_party_total(self, args): self.party_total = frappe._dict() for d in self.receivables: self.init_party_total(d) # Add all amount columns for k in list(self.party_total[d.party]): <mask>: self.party_total[d.party][k] += d.get(k, 0.0) # set territory, customer_group, sales person etc self.set_party_details(d)",False,"if k not in [ ""currency"" , ""sales_person"" ] :",if k not in self . party_total [ d . party ] :,0.8820016898747209,19.667812291861896
"def get_databases(request): dbs = {} global_env = globals() for (key, value) in global_env.items(): try: cond = isinstance(value, GQLDB) except: cond = isinstance(value, SQLDB) <mask>: dbs[key] = value return dbs",True,if cond :,if cond :,0.8820016898747209,0.0
"def check_twobit_file(dbkey, GALAXY_DATA_INDEX_DIR): twobit_file = ""%s/twobit.loc"" % GALAXY_DATA_INDEX_DIR twobit_path = """" twobits = {} for i, line in enumerate(open(twobit_file)): line = line.rstrip(""\r\n"") if line and not line.startswith(""#""): fields = line.split(""\t"") <mask>: continue twobits[(fields[0])] = fields[1] if dbkey in twobits: twobit_path = twobits[(dbkey)] return twobit_path",False,if len ( fields ) < 2 :,if len ( fields ) != 2 :,0.8820016898747209,51.33450480401705
"def action(scheduler, _): nonlocal state nonlocal has_result nonlocal result nonlocal first nonlocal time <mask>: observer.on_next(result) try: if first: first = False else: state = iterate(state) has_result = condition(state) if has_result: result = state time = time_mapper(state) except Exception as e: # pylint: disable=broad-except observer.on_error(e) return if has_result: mad.disposable = scheduler.schedule_relative(time, action) else: observer.on_completed()",False,if has_result :,if result :,0.8820016898747209,0.0
def orthogonalEnd(self): if self.type == Segment.LINE: O = self.AB.orthogonal() O.norm() return O else: O = self.B - self.C O.norm() <mask>: return -O else: return O,False,if self . type == Segment . CCW :,if self . type == Segment . LINE :,0.8820016898747209,78.25422900366438
"def remove(self, values): if not isinstance(values, (list, tuple, set)): values = [values] for v in values: v = str(v) if isinstance(self._definition, dict): self._definition.pop(v, None) elif self._definition == ""ANY"": <mask>: self._definition = [] elif v in self._definition: self._definition.remove(v) if ( self._value is not None and self._value not in self._definition and self._not_any() ): raise ConanException(bad_value_msg(self._name, self._value, self.values_range))",False,"if v == ""ANY"" :",elif v in self . _definition :,0.8820016898747209,6.567274736060395
"def __enter__(self) -> None: try: <mask>: signal.signal(signal.SIGALRM, self.handle_timeout) signal.alarm(self.seconds) except ValueError as ex: logger.warning(""timeout can't be used in the current context"") logger.exception(ex)",False,if threading . current_thread ( ) == threading . main_thread ( ) :,if self . seconds is not None :,0.8820016898747209,2.0822836897918786
"def __init__(self, fixed: MQTTFixedHeader = None): if fixed is None: header = MQTTFixedHeader(PINGRESP, 0x00) else: <mask>: raise HBMQTTException( ""Invalid fixed packet type %s for PingRespPacket init"" % fixed.packet_type ) header = fixed super().__init__(header) self.variable_header = None self.payload = None",False,if fixed . packet_type is not PINGRESP :,"if fixed . packet_type not in ( MQTTFixedHeader , MQTTFixedHeader ) :",0.8820016898747209,37.59663529467017
"def _put_nowait(self, data, *, sender): if not self._running: logger.warning(""Pub/Sub listener message after stop: %r, %r"", sender, data) return self._queue.put_nowait((sender, data)) if self._waiter is not None: fut, self._waiter = self._waiter, None <mask>: assert fut.cancelled(), (""Waiting future is in wrong state"", self, fut) return fut.set_result(None)",False,if fut . done ( ) :,if fut is not None :,0.8820016898747209,15.207218222740094
"def OnAssignBuiltin(self, cmd_val): # type: (cmd_value__Assign) -> None buf = self._ShTraceBegin() if not buf: return for i, arg in enumerate(cmd_val.argv): <mask>: buf.write("" "") buf.write(arg) for pair in cmd_val.pairs: buf.write("" "") buf.write(pair.var_name) buf.write(""="") if pair.rval: _PrintShValue(pair.rval, buf) buf.write(""\n"") self.f.write(buf.getvalue())",False,if i != 0 :,if i == 0 :,0.8820016898747209,37.99178428257963
"def convertDict(obj): obj = dict(obj) for k, v in obj.items(): del obj[k] if not (isinstance(k, str) or isinstance(k, unicode)): k = dumps(k) # Keep track of which keys need to be decoded when loading. <mask>: obj[Types.KEYS] = [] obj[Types.KEYS].append(k) obj[k] = convertObjects(v) return obj",False,if Types . KEYS not in obj :,if types . KEYS not in obj :,0.8820016898747209,70.71067811865478
"def _ArgumentListHasDictionaryEntry(self, token): """"""Check if the function argument list has a dictionary as an arg."""""" if _IsArgumentToFunction(token): while token: if token.value == ""{"": length = token.matching_bracket.total_length - token.total_length return length + self.stack[-2].indent > self.column_limit if token.ClosesScope(): break <mask>: token = token.matching_bracket token = token.next_token return False",False,if token . OpensScope ( ) :,"if token . value == ""}"" :",0.8820016898747209,16.784459625186194
"def get_editable_dict(self): ret = {} for ref, ws_package in self._workspace_packages.items(): path = ws_package.root_folder <mask>: path = os.path.join(path, CONANFILE) ret[ref] = {""path"": path, ""layout"": ws_package.layout} return ret",False,if os . path . isdir ( path ) :,if os . path . exists ( path ) :,0.8820016898747209,65.80370064762461
"def serialize(self, name=None): data = super(WebLink, self).serialize(name) data[""contentType""] = self.contentType if self.width: <mask>: raise InvalidWidthException(self.width) data[""inputOptions""] = {} data[""width""] = self.width data.update({""content"": {""url"": self.linkUrl, ""text"": self.linkText}}) return data",False,"if self . width not in [ 100 , 50 , 33 , 25 ] :","if self . width not in ( ""0"" , ""1"" ) :",0.8820016898747209,32.37722713145643
"def callback(lexer, match, context): text = match.group() extra = """" if start: context.next_indent = len(text) <mask>: while context.next_indent < context.indent: context.indent = context.indent_stack.pop() if context.next_indent > context.indent: extra = text[context.indent :] text = text[: context.indent] else: context.next_indent += len(text) if text: yield match.start(), TokenClass, text if extra: yield match.start() + len(text), TokenClass.Error, extra context.pos = match.end()",False,if context . next_indent < context . indent :,if context . indent_stack :,0.8820016898747209,21.606281467072083
"def _handle_unsubscribe(self, web_sock): index = None with await self._subscriber_lock: for i, (subscriber_web_sock, _) in enumerate(self._subscribers): if subscriber_web_sock == web_sock: index = i break <mask>: del self._subscribers[index] if not self._subscribers: asyncio.ensure_future(self._unregister_subscriptions())",False,if index is not None :,if index :,0.8820016898747209,0.0
"def test_missing_dict_param(): expected_err = ""params dictionary did not contain value for placeholder"" try: substitute_params( ""SELECT * FROM cust WHERE salesrep = %(name)s"", {""foobar"": ""John Doe""} ) assert False, ""expected exception b/c dict did not contain replacement value"" except ValueError as exc: <mask>: raise",False,if expected_err not in str ( exc ) :,if expected_err in exc . args :,0.8820016898747209,27.488876557092613
"def one_gpr_reg_one_mem_scalable(ii): n, r = 0, 0 for op in _gen_opnds(ii): <mask>: n += 1 elif op_gprv(op): r += 1 else: return False return n == 1 and r == 1",False,"if op_agen ( op ) or ( op_mem ( op ) and op . oc2 in [ ""v"" ] ) :",if op_mem ( op ) :,0.8820016898747209,7.821555054390223
"def on_enter(self): """"""Fired when mouse enter the bbox of the widget."""""" if hasattr(self, ""md_bg_color"") and self.focus_behavior: if hasattr(self, ""theme_cls"") and not self.focus_color: self.md_bg_color = self.theme_cls.bg_normal else: <mask>: self.md_bg_color = App.get_running_app().theme_cls.bg_normal else: self.md_bg_color = self.focus_color",False,if not self . focus_color :,"if hasattr ( self , ""theme_cls"" ) and not self . focus_color :",0.8820016898747209,31.872714733206724
"def __init__(self, *args, **kwargs): BaseCellExporter.__init__(self, *args, **kwargs) self.comment = ""#"" for key in [""cell_marker""]: <mask>: self.metadata[key] = self.unfiltered_metadata[key] if self.fmt.get(""rst2md""): raise ValueError( ""The 'rst2md' option is a read only option. The reverse conversion is not "" ""implemented. Please either deactivate the option, or save to another format."" ) # pragma: no cover",True,if key in self . unfiltered_metadata :,if key in self . unfiltered_metadata :,0.8820016898747209,100.00000000000004
"def sendQueryQueueByAfterNate(self): for i in range(10): queryQueueByAfterNateRsp = self.session.httpClint.send(urls.get(""queryQueue"")) <mask>: print( """".join(queryQueueByAfterNateRsp.get(""messages"")) or queryQueueByAfterNateRsp.get(""validateMessages"") ) time.sleep(1) else: sendEmail(ticket.WAIT_ORDER_SUCCESS) sendServerChan(ticket.WAIT_ORDER_SUCCESS) raise ticketIsExitsException(ticket.WAIT_AFTER_NATE_SUCCESS)",False,"if not queryQueueByAfterNateRsp . get ( ""status"" ) :",if queryQueueByAfterNateRsp :,0.8820016898747209,0.0
"def filter_errors(self, errors: List[str]) -> List[str]: real_errors: List[str] = list() current_file = __file__ current_path = os.path.split(current_file) for line in errors: line = line.strip() <mask>: continue fn, lno, lvl, msg = self.parse_trace_line(line) if fn is not None: _path = os.path.split(fn) if _path[-1] != current_path[-1]: continue real_errors.append(line) return real_errors",True,if not line :,if not line :,0.8820016898747209,100.00000000000004
"def pretty(self, n, comment=True): if isinstance(n, (str, bytes, list, tuple, dict)): r = repr(n) if not comment: # then it can be inside a comment! r = r.replace(""*/"", r""\x2a/"") return r if not isinstance(n, six.integer_types): return n if isinstance(n, constants.Constant): <mask>: return ""%s /* %s */"" % (n, self.pretty(int(n))) else: return ""%s (%s)"" % (n, self.pretty(int(n))) elif abs(n) < 10: return str(n) else: return hex(n)",True,if comment :,if comment :,0.8820016898747209,0.0
"def get_pricings(self, subscription_id: str): try: client = self.get_client(subscription_id) pricings_list = await run_concurrently(lambda: client.pricings.list()) <mask>: return pricings_list.value else: return [] except Exception as e: print_exception(f""Failed to retrieve pricings: {e}"") return []",False,"if hasattr ( pricings_list , ""value"" ) :",if pricings_list . status == 200 :,0.8820016898747209,14.530346490115708
"def add_doc(target, variables, body_lines): if isinstance(target, ast.Name): # if it is a variable name add it to the doc name = target.id if name not in variables: doc = find_doc_for(target, body_lines) <mask>: variables[name] = doc elif isinstance(target, ast.Tuple): # if it is a tuple then iterate the elements # this can happen like this: # a, b = 1, 2 for e in target.elts: add_doc(e, variables, body_lines)",False,if doc is not None :,if doc :,0.8820016898747209,0.0
"def find_word_bounds(self, text, index, allowed_chars): right = left = index done = False while not done: if left == 0: done = True elif not self.word_boundary_char(text[left - 1]): left -= 1 else: done = True done = False while not done: <mask>: done = True elif not self.word_boundary_char(text[right]): right += 1 else: done = True return left, right",False,if right == len ( text ) :,if right == 0 :,0.8820016898747209,32.58798048281462
"def pxrun_nodes(self, *args, **kwargs): cell = self._px_cell if re.search(r""^\s*%autopx\b"", cell): self._disable_autopx() return False else: try: result = self.view.execute(cell, silent=False, block=False) except: self.shell.showtraceback() return True else: <mask>: try: result.get() except: self.shell.showtraceback() return True else: result.display_outputs() return False",False,if self . view . block :,if result :,0.8820016898747209,0.0
"def candidates() -> Generator[""Symbol"", None, None]: s = self if Symbol.debug_lookup: Symbol.debug_print(""searching in self:"") print(s.to_string(Symbol.debug_indent + 1), end="""") while True: if matchSelf: yield s if recurseInAnon: yield from s.children_recurse_anon else: yield from s._children <mask>: break s = s.siblingAbove if Symbol.debug_lookup: Symbol.debug_print(""searching in sibling:"") print(s.to_string(Symbol.debug_indent + 1), end="""")",True,if s . siblingAbove is None :,if s . siblingAbove is None :,0.8820016898747209,100.00000000000004
"def decTaskGen(): cnt = intbv(0, min=-n, max=n) while 1: yield clock.posedge, reset.negedge <mask>: cnt[:] = 0 count.next = 0 else: # print count decTaskFunc(cnt, enable, reset, n) count.next = cnt",False,if reset == ACTIVE_LOW :,if enable :,0.8820016898747209,0.0
"def __call__(self, *args, **kwargs): if not NET_INITTED: return self.raw(*args, **kwargs) for stack in traceback.walk_stack(None): <mask>: layer = stack[0].f_locals[""self""] if layer in layer_names: log.pytorch_layer_name = layer_names[layer] print(layer_names[layer]) break out = self.obj(self.raw, *args, **kwargs) # if isinstance(out,Variable): # out=[out] return out",False,"if ""self"" in stack [ 0 ] . f_locals :",if stack :,0.8820016898747209,0.0
"def to_json_dict(self): d = super().to_json_dict() d[""bullet_list""] = RenderedContent.rendered_content_list_to_json(self.bullet_list) if self.header is not None: if isinstance(self.header, RenderedContent): d[""header""] = self.header.to_json_dict() else: d[""header""] = self.header if self.subheader is not None: <mask>: d[""subheader""] = self.subheader.to_json_dict() else: d[""subheader""] = self.subheader return d",True,"if isinstance ( self . subheader , RenderedContent ) :","if isinstance ( self . subheader , RenderedContent ) :",0.8820016898747209,100.00000000000004
"def add(request): form_type = ""servers"" if request.method == ""POST"": form = BookMarkForm(request.POST) <mask>: form_type = form.save() messages.add_message(request, messages.INFO, ""Bookmark created"") else: messages.add_message(request, messages.INFO, form.errors) if form_type == ""server"": url = reverse(""servers"") else: url = reverse(""metrics"") return redirect(url) else: return redirect(reverse(""servers""))",True,if form . is_valid ( ) :,if form . is_valid ( ) :,0.8820016898747209,100.00000000000004
"def fee_amount_in_quote(self, trading_pair: str, price: Decimal, order_amount: Decimal): fee_amount = Decimal(""0"") if self.percent > 0: fee_amount = (price * order_amount) * self.percent base, quote = trading_pair.split(""-"") for flat_fee in self.flat_fees: if interchangeable(flat_fee[0], base): fee_amount += flat_fee[1] * price <mask>: fee_amount += flat_fee[1] return fee_amount",True,"elif interchangeable ( flat_fee [ 0 ] , quote ) :","elif interchangeable ( flat_fee [ 0 ] , quote ) :",0.8820016898747209,100.00000000000004
"def load_batch(fpath): with open(fpath, ""rb"") as f: <mask>: # Python3 d = pickle.load(f, encoding=""latin1"") else: # Python2 d = pickle.load(f) data = d[""data""] labels = d[""labels""] return data, labels",False,"if sys . version_info > ( 3 , 0 ) :","if sys . version_info < ( 3 , 0 ) :",0.8820016898747209,76.11606003349888
"def clear_entries(options): """"""Clear pending entries"""""" with Session() as session: query = session.query(db.PendingEntry).filter(db.PendingEntry.approved == False) <mask>: query = query.filter(db.PendingEntry.task_name == options.task_name) deleted = query.delete() console(""Successfully deleted %i pending entries"" % deleted)",True,if options . task_name :,if options . task_name :,0.8820016898747209,100.00000000000004
"def attribute_table(self, attribute): """"""Return a tuple (schema, table) for attribute."""""" dimension = attribute.dimension if dimension: schema = self.naming.dimension_schema or self.naming.schema <mask>: table = self.fact_name else: table = self.naming.dimension_table_name(dimension) else: table = self.fact_name schema = self.naming.schema return (schema, table)",False,if dimension . is_flat and not dimension . has_details :,"if dimension == ""fact"" :",0.8820016898747209,5.773772066582297
"def remove_rating(self, songs, librarian): count = len(songs) if count > 1 and config.getboolean(""browsers"", ""rating_confirm_multiple""): parent = qltk.get_menu_item_top_parent(self) dialog = ConfirmRateMultipleDialog(parent, _(""_Remove Rating""), count, None) if dialog.run() != Gtk.ResponseType.YES: return reset = [] for song in songs: <mask>: del song[""~#rating""] reset.append(song) librarian.changed(reset)",False,"if ""~#rating"" in song :","if song [ ""~#rating"" ] :",0.8820016898747209,44.17918226831576
"def find_word_bounds(self, text, index, allowed_chars): right = left = index done = False while not done: if left == 0: done = True elif not self.word_boundary_char(text[left - 1]): left -= 1 else: done = True done = False while not done: if right == len(text): done = True <mask>: right += 1 else: done = True return left, right",False,elif not self . word_boundary_char ( text [ right ] ) :,if allowed_chars [ right ] == text [ right - 1 ] :,0.8820016898747209,14.247788801610149
"def handle_read(self): """"""Called when there is data waiting to be read."""""" try: chunk = self.recv(self.ac_in_buffer_size) except RetryError: pass except socket.error: self.handle_error() else: self.tot_bytes_received += len(chunk) if not chunk: self.transfer_finished = True # self.close() # <-- asyncore.recv() already do that... return <mask>: chunk = self._data_wrapper(chunk) try: self.file_obj.write(chunk) except OSError as err: raise _FileReadWriteError(err)",False,if self . _data_wrapper is not None :,if self . _data_wrapper :,0.8820016898747209,59.755798910891144
"def toggle(self, event=None): if self.absolute: if self.save == self.split: self.save = 100 if self.split > 20: self.save = self.split self.split = 1 else: self.split = self.save else: if self.save == self.split: self.save = 0.3 if self.split <= self.min or self.split >= self.max: self.split = self.save <mask>: self.split = self.min else: self.split = self.max self.placeChilds()",False,elif self . split < 0.5 :,elif self . split < self . min :,0.8820016898747209,46.713797772819994
"def readAtOffset(self, offset, size, shortok=False): ret = b"""" self.fd.seek(offset) while len(ret) != size: rlen = size - len(ret) x = self.fd.read(rlen) if x == b"""": <mask>: return None return ret ret += x return ret",False,if not shortok :,if shortok :,0.8820016898747209,0.0
"def webfinger(environ, start_response, _): query = parse_qs(environ[""QUERY_STRING""]) try: rel = query[""rel""] resource = query[""resource""][0] except KeyError: resp = BadRequest(""Missing parameter in request"") else: <mask>: resp = BadRequest(""Bad issuer in request"") else: wf = WebFinger() resp = Response(wf.response(subject=resource, base=OAS.baseurl)) return resp(environ, start_response)",False,if rel != [ OIC_ISSUER ] :,"if rel != ""issuer"" :",0.8820016898747209,28.46946938149361
"def _tokenize(self, text): if format_text(text) == EMPTY_TEXT: return [self.additional_special_tokens[0]] split_tokens = [] if self.do_basic_tokenize: for token in self.basic_tokenizer.tokenize( text, never_split=self.all_special_tokens ): # If the token is part of the never_split set <mask>: split_tokens.append(token) else: split_tokens += self.wordpiece_tokenizer.tokenize(token) else: split_tokens = self.wordpiece_tokenizer.tokenize(text) return split_tokens",False,if token in self . basic_tokenizer . never_split :,if token in self . never_split :,0.8820016898747209,55.74689950645963
"def send_packed_command(self, command, check_health=True): if not self._sock: self.connect() try: if isinstance(command, str): command = [command] for item in command: self._sock.sendall(item) except socket.error as e: self.disconnect() <mask>: _errno, errmsg = ""UNKNOWN"", e.args[0] else: _errno, errmsg = e.args raise ConnectionError( ""Error %s while writing to socket. %s."" % (_errno, errmsg) ) except Exception: self.disconnect() raise",False,if len ( e . args ) == 1 :,if check_health :,0.8820016898747209,3.8261660656802645
"def to_value(self, value): # Tip: 'value' is the object returned by # taiga.projects.history.models.HistoryEntry.values_diff() ret = {} for key, val in value.items(): if key in [""attachments"", ""custom_attributes"", ""description_diff""]: ret[key] = val <mask>: ret[key] = {k: {""from"": v[0], ""to"": v[1]} for k, v in val.items()} else: ret[key] = {""from"": val[0], ""to"": val[1]} return ret",False,"elif key == ""points"" :","elif key in [ ""attachments"" , ""custom_attributes"" , ""description_diff"" ] :",0.8820016898747209,4.567211833282236
"def to_child(cls, key=None, process=None): if process is not None: if type(process) is not dict: raise ValueError( 'Invalid value provided for ""process"" parameter, expected a dictionary' ) <mask>: # Merge class `__process__` parameters with provided parameters result = {} result.update(deepcopy(cls.__process__)) result.update(process) process = result class Child(cls): __key__ = key __process__ = process __root__ = False Child.__name__ = cls.__name__ return Child",False,if cls . __process__ :,"if isinstance ( process , dict ) :",0.8820016898747209,6.413885305524152
"def _super_function(args): passed_class, passed_self = args.get_arguments([""type"", ""self""]) if passed_self is None: return passed_class else: # pyclass = passed_self.get_type() pyclass = passed_class <mask>: supers = pyclass.get_superclasses() if supers: return pyobjects.PyObject(supers[0]) return passed_self",False,"if isinstance ( pyclass , pyobjects . AbstractClass ) :",if pyclass is not None :,0.8820016898747209,5.484411595600381
"def get_data(row): data = [] for field_name, field_xpath in fields: result = row.xpath(field_xpath) <mask>: result = "" "".join( text for text in map( six.text_type.strip, map(six.text_type, map(unescape, result)) ) if text ) else: result = None data.append(result) return data",True,if result :,if result :,0.8820016898747209,0.0
"def say(jarvis, s): """"""Reads what is typed."""""" if not s: jarvis.say(""What should I say?"") else: voice_state = jarvis.is_voice_enabled() jarvis.enable_voice() jarvis.say(s) <mask>: jarvis.disable_voice()",False,if not voice_state :,if voice_state :,0.8820016898747209,57.89300674674101
"def __import__(name, globals=None, locals=None, fromlist=(), level=0): module = orig___import__(name, globals, locals, fromlist, level) if fromlist and module.__name__ in modules: <mask>: fromlist = list(fromlist) fromlist.remove(""*"") fromlist.extend(getattr(module, ""__all__"", [])) for x in fromlist: if isinstance(getattr(module, x, None), types.ModuleType): from_name = ""{}.{}"".format(module.__name__, x) if from_name in modules: importlib.import_module(from_name) return module",True,"if ""*"" in fromlist :","if ""*"" in fromlist :",0.8820016898747209,100.00000000000004
"def _read_pricing_file(self, region=None, pricing_file=None): if not self.__pricing_file_cache: <mask>: logging.info(""Reading pricing file..."") with open(pricing_file) as data_file: self.__pricing_file_cache = json.load(data_file) else: self.__pricing_file_cache = self._download_pricing_file(region) return self.__pricing_file_cache",True,if pricing_file :,if pricing_file :,0.8820016898747209,100.00000000000004
