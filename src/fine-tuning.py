# -*- coding: utf-8 -*-
"""WORKING? ASSIGNMENT 2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1gMTZKr6p-DUkqg28ajtNALdUvt_tHdMg
"""

import pandas as pd
import re
import os
import subprocess
import tempfile
import importlib
import torch

from transformers import T5ForConditionalGeneration, RobertaTokenizer, Trainer, TrainingArguments, EarlyStoppingCallback
from datasets import Dataset, DatasetDict
import evaluate

print("Starting the fine-tuning pipeline...")

# ------------------------------
# Step 3: Load CSV Files
# ------------------------------
print("Step 3: Loading CSV files...")
train_df = pd.read_csv("../data/raw/ft_train.csv")
val_df = pd.read_csv("../data/raw/ft_valid.csv")
test_df = pd.read_csv("../data/raw/ft_test.csv")
print("CSV files loaded successfully.")

# ------------------------------
# Step 4: Mask if Conditions & Flatten Code
# ------------------------------
print("Step 4: Masking 'if' conditions and flattening code...")

def flatten_code(code):
    return " ".join(code.split())

def generate_pattern(target):
    target = target.strip()
    tokens = re.findall(r'\S+', target)
    if tokens and tokens[-1] == ":":
        tokens = tokens[:-1]
        pattern = r'\s*'.join(map(re.escape, tokens)) + r'\s*:'
    else:
        pattern = r'\s*'.join(map(re.escape, tokens))
    return pattern

def mask_if_conditions(df, df_name="dataset"):
    print(f"Processing {df_name} dataset...")
    masked_data = []
    for idx, row in df.iterrows():
        function_code = row['cleaned_method']
        target_if_condition = row['target_block']

        if not isinstance(target_if_condition, str) or not target_if_condition.strip():
            print(f"[{df_name} row {idx}] Skipped: Empty or invalid target_block")
            continue

        raw_condition = target_if_condition.strip()
        flattened_func = flatten_code(function_code)
        pattern = generate_pattern(raw_condition)

        masked_func, count = re.subn(pattern, "<mask>:", flattened_func, count=1)
        if count == 0:
            print(f"[{df_name} row {idx}] Warning: Condition not found or not replaced")
            continue

        masked_data.append({
            'masked_input': masked_func,
            'target': raw_condition,
            'original_function': function_code
        })
        if idx % 100 == 0:
            print(f"Processed {idx} rows in {df_name} dataset.")
    print(f"Completed processing {df_name} dataset.")
    return pd.DataFrame(masked_data)

masked_train_df = mask_if_conditions(train_df, df_name="train")
masked_val_df = mask_if_conditions(val_df, df_name="val")
masked_test_df = mask_if_conditions(test_df, df_name="test")
print("Masking and flattening completed.")

# ------------------------------
# Step 5: Convert to Hugging Face Datasets
# ------------------------------
print("Step 5: Converting to Hugging Face datasets...")
hf_train = Dataset.from_pandas(masked_train_df[['masked_input', 'target']])
hf_val = Dataset.from_pandas(masked_val_df[['masked_input', 'target']])
hf_test = Dataset.from_pandas(masked_test_df[['masked_input', 'target']])

dataset = DatasetDict({
    "train": hf_train,
    "validation": hf_val,
    "test": hf_test
})
print("Conversion to Hugging Face datasets completed.")

# ------------------------------
# Step 6: Load Pre-trained CodeT5 Model & Tokenizer
# ------------------------------
print("Step 6: Loading pre-trained CodeT5 model and tokenizer...")
model_checkpoint = "Salesforce/codet5-small"
model = T5ForConditionalGeneration.from_pretrained(model_checkpoint)
tokenizer = RobertaTokenizer.from_pretrained(model_checkpoint)
print("Model and tokenizer loaded successfully.")

# ------------------------------
# Step 7: Tokenize the Dataset
# ------------------------------
print("Step 7: Tokenizing the dataset...")

def preprocess_function(examples):
    inputs = examples["masked_input"]
    targets = examples["target"]
    model_inputs = tokenizer(inputs, max_length=256, truncation=True, padding="max_length")
    labels = tokenizer(targets, max_length=128, truncation=True, padding="max_length")
    model_inputs["labels"] = labels["input_ids"]
    return model_inputs

tokenized_datasets = dataset.map(preprocess_function, batched=True)
print("Tokenization completed.")

# ------------------------------
# Step 8: Training Arguments & Trainer Setup
# ------------------------------
print("Step 8: Setting up training arguments and the trainer...")
training_args = TrainingArguments(
    output_dir="../data/interim/codet5-finetuned",
    evaluation_strategy="epoch",
    save_strategy="epoch",
    logging_dir="../data/logs",
    learning_rate=5e-5,
    gradient_accumulation_steps=2,
    per_device_train_batch_size=64,
    per_device_eval_batch_size=64,
    num_train_epochs=5,
    weight_decay=0.01,
    load_best_model_at_end=True,
    metric_for_best_model="eval_loss",
    save_total_limit=2,
    logging_steps=100,
    push_to_hub=False,
    fp16=True
)

print("CUDA available:", torch.cuda.is_available())

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_datasets["train"],
    eval_dataset=tokenized_datasets["validation"],
    tokenizer=tokenizer,
    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]
)
print("Trainer setup completed.")

# ------------------------------
# Step 9: Train the Model
# ------------------------------
print("Step 9: Starting model training...")
trainer.train()
print("Model training completed.")

# ------------------------------
# Step 10: Evaluate on Test Set
# ------------------------------
print("Step 10: Evaluating the model on the test set...")
metrics = trainer.evaluate(tokenized_datasets["test"])
print("Test Evaluation Metrics:", metrics)
print("Evaluation completed.")

# ------------------------------
# Step 11: Generate Predictions and Save Results with CodeBLEU Scores
# ------------------------------
print("Step 11: Generating predictions and calculating CodeBLEU scores...")
sacrebleu_metric = evaluate.load("sacrebleu")

results = []
refs_list = []
hyps_list = []

for i, row in masked_test_df.iterrows():
    input_text = row["masked_input"]
    expected_if = row["target"]

    inputs = tokenizer(input_text, return_tensors="pt", padding=True, truncation=True).to(model.device)
    outputs = model.generate(**inputs, max_length=128)
    predicted_if = tokenizer.decode(outputs[0], skip_special_tokens=True).strip()

    reference_full = flatten_code(row['original_function'])
    hypothesis_full = input_text.replace('<mask>:', predicted_if + ':')

    refs_list.append(reference_full.strip())
    hyps_list.append(hypothesis_full.strip())

    exact_match = (predicted_if.strip() == expected_if.strip())
    bleu_result = sacrebleu_metric.compute(predictions=[predicted_if], references=[[expected_if]])
    bleu_score = bleu_result["score"]

    results.append({
        "Input function with masked if condition": input_text,
        "Whether the prediction is correct": exact_match,
        "Expected if condition": expected_if,
        "Predicted if condition": predicted_if,
        "CodeBLEU prediction score": None,
        "BLEU-4 prediction score": bleu_score
    })
    if i % 10 == 0:
        print(f"Generated predictions for {i} rows.")

results_df = pd.DataFrame(results)

refs_path = "../models/all_targets.txt"
hyps_path = "../models/all_predictions.txt"

with open(refs_path, "w") as f_ref, open(hyps_path, "w") as f_pred:
    for ref, hyp in zip(refs_list, hyps_list):
        f_ref.write(ref + "\n")
        f_pred.write(hyp + "\n")

cmd = (
    "cd ../data/external/CodeXGLUE/Code-Code/code-to-code-trans/evaluator/CodeBLEU/ && "
    f"python calc_code_bleu.py --refs ../models/all_targets.txt --hyp ../models/all_predictions.txt --lang python --params 0.25,0.25,0.25,0.25"
)

result = subprocess.run(cmd, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE, universal_newlines=True)
output = result.stdout.strip() if result.stdout.strip() else result.stderr.strip()
print("CodeBLEU evaluator output:\n", output)

m = re.search(r"CodeBLEU score:\s+([0-9.]+)", output)
global_codebleu_score = float(m.group(1)) if m else 0.0

results_df["CodeBLEU prediction score"] = global_codebleu_score

results_df.to_csv("../models/testset-results.csv", index=False)
print("Test set results saved as 'testset-results.csv'.")
print("Fine-tuning pipeline completed.")
